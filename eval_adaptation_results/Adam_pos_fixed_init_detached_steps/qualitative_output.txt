Target params: [1.1812, 0.2779]
iter 0 loss: 0.739
Actual params: [0.5941, 0.5941]
-Original Grad: 0.376, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.428, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.704
Actual params: [0.6941, 0.4941]
-Original Grad: 0.305, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.408, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 0.632
Actual params: [0.793 , 0.3942]
-Original Grad: 0.375, -lr * Pred Grad:  0.100, New P: 0.893
-Original Grad: -0.197, -lr * Pred Grad:  -0.093, New P: 0.301
iter 3 loss: 0.478
Actual params: [0.8925, 0.3008]
-Original Grad: 0.333, -lr * Pred Grad:  0.099, New P: 0.992
-Original Grad: 0.118, -lr * Pred Grad:  -0.064, New P: 0.236
iter 4 loss: 0.333
Actual params: [0.9919, 0.2365]
-Original Grad: 0.180, -lr * Pred Grad:  0.095, New P: 1.087
-Original Grad: 0.174, -lr * Pred Grad:  -0.038, New P: 0.198
iter 5 loss: 0.354
Actual params: [1.087 , 0.1984]
-Original Grad: 0.049, -lr * Pred Grad:  0.085, New P: 1.172
-Original Grad: 0.161, -lr * Pred Grad:  -0.019, New P: 0.179
iter 6 loss: 0.425
Actual params: [1.1722, 0.1791]
-Original Grad: -0.034, -lr * Pred Grad:  0.072, New P: 1.244
-Original Grad: 0.058, -lr * Pred Grad:  -0.013, New P: 0.167
iter 7 loss: 0.453
Actual params: [1.2442, 0.1665]
-Original Grad: -0.098, -lr * Pred Grad:  0.056, New P: 1.300
-Original Grad: 0.086, -lr * Pred Grad:  -0.005, New P: 0.162
iter 8 loss: 0.480
Actual params: [1.3004, 0.1618]
-Original Grad: -0.200, -lr * Pred Grad:  0.035, New P: 1.336
-Original Grad: 0.141, -lr * Pred Grad:  0.006, New P: 0.168
iter 9 loss: 0.508
Actual params: [1.3355, 0.1676]
-Original Grad: -0.306, -lr * Pred Grad:  0.011, New P: 1.346
-Original Grad: 0.264, -lr * Pred Grad:  0.022, New P: 0.190
iter 10 loss: 0.518
Actual params: [1.3464, 0.1896]
-Original Grad: -0.308, -lr * Pred Grad:  -0.008, New P: 1.338
-Original Grad: 0.168, -lr * Pred Grad:  0.030, New P: 0.219
iter 11 loss: 0.539
Actual params: [1.3384, 0.2194]
-Original Grad: -0.236, -lr * Pred Grad:  -0.020, New P: 1.319
-Original Grad: 0.150, -lr * Pred Grad:  0.036, New P: 0.255
iter 12 loss: 0.550
Actual params: [1.3189, 0.255 ]
-Original Grad: -0.171, -lr * Pred Grad:  -0.026, New P: 1.293
-Original Grad: 0.071, -lr * Pred Grad:  0.036, New P: 0.291
iter 13 loss: 0.569
Actual params: [1.2925, 0.2914]
-Original Grad: -0.106, -lr * Pred Grad:  -0.029, New P: 1.263
-Original Grad: 0.022, -lr * Pred Grad:  0.034, New P: 0.326
iter 14 loss: 0.605
Actual params: [1.2634, 0.3256]
-Original Grad: -0.079, -lr * Pred Grad:  -0.030, New P: 1.233
-Original Grad: -0.062, -lr * Pred Grad:  0.027, New P: 0.353
iter 15 loss: 0.629
Actual params: [1.2329, 0.3525]
-Original Grad: -0.055, -lr * Pred Grad:  -0.030, New P: 1.202
-Original Grad: -0.114, -lr * Pred Grad:  0.017, New P: 0.370
iter 16 loss: 0.635
Actual params: [1.2025, 0.3697]
-Original Grad: -0.035, -lr * Pred Grad:  -0.029, New P: 1.173
-Original Grad: -0.253, -lr * Pred Grad:  -0.000, New P: 0.370
iter 17 loss: 0.630
Actual params: [1.173 , 0.3695]
-Original Grad: -0.013, -lr * Pred Grad:  -0.027, New P: 1.146
-Original Grad: -0.201, -lr * Pred Grad:  -0.012, New P: 0.358
iter 18 loss: 0.614
Actual params: [1.1456, 0.3577]
-Original Grad: -0.164, -lr * Pred Grad:  -0.033, New P: 1.112
-Original Grad: -0.223, -lr * Pred Grad:  -0.023, New P: 0.335
iter 19 loss: 0.561
Actual params: [1.1122, 0.3345]
-Original Grad: -0.049, -lr * Pred Grad:  -0.033, New P: 1.079
-Original Grad: -0.115, -lr * Pred Grad:  -0.027, New P: 0.307
iter 20 loss: 0.483
Actual params: [1.0793, 0.3071]
-Original Grad: -0.138, -lr * Pred Grad:  -0.037, New P: 1.042
-Original Grad: 0.015, -lr * Pred Grad:  -0.024, New P: 0.283
Target params: [1.1812, 0.2779]
iter 0 loss: 0.312
Actual params: [0.5941, 0.5941]
-Original Grad: 0.031, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.115, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.252
Actual params: [0.6941, 0.4941]
-Original Grad: -0.010, -lr * Pred Grad:  0.041, New P: 0.735
-Original Grad: -0.009, -lr * Pred Grad:  -0.073, New P: 0.421
iter 2 loss: 0.259
Actual params: [0.7349, 0.4212]
-Original Grad: -0.025, -lr * Pred Grad:  -0.015, New P: 0.720
-Original Grad: 0.023, -lr * Pred Grad:  -0.043, New P: 0.378
iter 3 loss: 0.261
Actual params: [0.7198, 0.3782]
-Original Grad: -0.032, -lr * Pred Grad:  -0.045, New P: 0.674
-Original Grad: 0.033, -lr * Pred Grad:  -0.018, New P: 0.360
iter 4 loss: 0.256
Actual params: [0.6744, 0.36  ]
-Original Grad: -0.036, -lr * Pred Grad:  -0.063, New P: 0.612
-Original Grad: 0.025, -lr * Pred Grad:  -0.004, New P: 0.356
iter 5 loss: 0.251
Actual params: [0.6117, 0.356 ]
-Original Grad: -0.041, -lr * Pred Grad:  -0.074, New P: 0.538
-Original Grad: 0.015, -lr * Pred Grad:  0.003, New P: 0.359
iter 6 loss: 0.253
Actual params: [0.538, 0.359]
-Original Grad: -0.020, -lr * Pred Grad:  -0.075, New P: 0.463
-Original Grad: 0.003, -lr * Pred Grad:  0.004, New P: 0.363
iter 7 loss: 0.269
Actual params: [0.4629, 0.3627]
-Original Grad: -0.039, -lr * Pred Grad:  -0.081, New P: 0.381
-Original Grad: 0.001, -lr * Pred Grad:  0.004, New P: 0.366
iter 8 loss: 0.298
Actual params: [0.3814, 0.3664]
-Original Grad: 0.020, -lr * Pred Grad:  -0.060, New P: 0.322
-Original Grad: -0.023, -lr * Pred Grad:  -0.006, New P: 0.361
iter 9 loss: 0.327
Actual params: [0.3216, 0.3606]
-Original Grad: 0.050, -lr * Pred Grad:  -0.023, New P: 0.299
-Original Grad: -0.021, -lr * Pred Grad:  -0.013, New P: 0.348
iter 10 loss: 0.332
Actual params: [0.2988, 0.3479]
-Original Grad: 0.054, -lr * Pred Grad:  0.004, New P: 0.303
-Original Grad: -0.031, -lr * Pred Grad:  -0.022, New P: 0.326
iter 11 loss: 0.323
Actual params: [0.3033, 0.3256]
-Original Grad: 0.041, -lr * Pred Grad:  0.020, New P: 0.323
-Original Grad: -0.021, -lr * Pred Grad:  -0.027, New P: 0.298
iter 12 loss: 0.305
Actual params: [0.3231, 0.2985]
-Original Grad: 0.039, -lr * Pred Grad:  0.032, New P: 0.355
-Original Grad: -0.018, -lr * Pred Grad:  -0.031, New P: 0.268
iter 13 loss: 0.288
Actual params: [0.3547, 0.2678]
-Original Grad: 0.012, -lr * Pred Grad:  0.033, New P: 0.388
-Original Grad: -0.008, -lr * Pred Grad:  -0.030, New P: 0.237
iter 14 loss: 0.275
Actual params: [0.3876, 0.2373]
-Original Grad: -0.015, -lr * Pred Grad:  0.024, New P: 0.411
-Original Grad: 0.001, -lr * Pred Grad:  -0.027, New P: 0.210
iter 15 loss: 0.267
Actual params: [0.4114, 0.21  ]
-Original Grad: -0.021, -lr * Pred Grad:  0.014, New P: 0.425
-Original Grad: 0.003, -lr * Pred Grad:  -0.024, New P: 0.186
iter 16 loss: 0.262
Actual params: [0.425 , 0.1863]
-Original Grad: -0.051, -lr * Pred Grad:  -0.006, New P: 0.419
-Original Grad: 0.000, -lr * Pred Grad:  -0.021, New P: 0.165
iter 17 loss: 0.264
Actual params: [0.4185, 0.165 ]
-Original Grad: -0.007, -lr * Pred Grad:  -0.008, New P: 0.410
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: 0.146
iter 18 loss: 0.266
Actual params: [0.4103, 0.1456]
-Original Grad: -0.023, -lr * Pred Grad:  -0.015, New P: 0.395
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: 0.128
iter 19 loss: 0.271
Actual params: [0.3949, 0.1277]
-Original Grad: -0.019, -lr * Pred Grad:  -0.020, New P: 0.374
-Original Grad: -0.002, -lr * Pred Grad:  -0.017, New P: 0.111
iter 20 loss: 0.277
Actual params: [0.3745, 0.1106]
-Original Grad: -0.013, -lr * Pred Grad:  -0.023, New P: 0.352
-Original Grad: -0.002, -lr * Pred Grad:  -0.016, New P: 0.095
Target params: [1.1812, 0.2779]
iter 0 loss: 0.445
Actual params: [0.5941, 0.5941]
-Original Grad: -0.073, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.740, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.293
Actual params: [0.4941, 0.4941]
-Original Grad: -0.205, -lr * Pred Grad:  -0.092, New P: 0.402
-Original Grad: -0.423, -lr * Pred Grad:  -0.095, New P: 0.399
iter 2 loss: 0.320
Actual params: [0.4016, 0.399 ]
-Original Grad: 0.079, -lr * Pred Grad:  -0.045, New P: 0.356
-Original Grad: -0.008, -lr * Pred Grad:  -0.074, New P: 0.325
iter 3 loss: 0.339
Actual params: [0.3562, 0.3249]
-Original Grad: 0.149, -lr * Pred Grad:  0.000, New P: 0.357
-Original Grad: 0.014, -lr * Pred Grad:  -0.060, New P: 0.265
iter 4 loss: 0.338
Actual params: [0.3565, 0.2651]
-Original Grad: 0.168, -lr * Pred Grad:  0.029, New P: 0.385
-Original Grad: 0.003, -lr * Pred Grad:  -0.050, New P: 0.215
iter 5 loss: 0.330
Actual params: [0.3852, 0.2148]
-Original Grad: 0.114, -lr * Pred Grad:  0.041, New P: 0.426
-Original Grad: 0.005, -lr * Pred Grad:  -0.043, New P: 0.172
iter 6 loss: 0.322
Actual params: [0.4259, 0.1717]
-Original Grad: 0.098, -lr * Pred Grad:  0.048, New P: 0.474
-Original Grad: 0.015, -lr * Pred Grad:  -0.037, New P: 0.135
iter 7 loss: 0.316
Actual params: [0.474, 0.135]
-Original Grad: 0.029, -lr * Pred Grad:  0.046, New P: 0.520
-Original Grad: 0.008, -lr * Pred Grad:  -0.032, New P: 0.103
iter 8 loss: 0.311
Actual params: [0.5203, 0.1031]
-Original Grad: -0.002, -lr * Pred Grad:  0.041, New P: 0.561
-Original Grad: 0.004, -lr * Pred Grad:  -0.028, New P: 0.075
iter 9 loss: 0.308
Actual params: [0.5611, 0.075 ]
-Original Grad: -0.009, -lr * Pred Grad:  0.035, New P: 0.596
-Original Grad: 0.005, -lr * Pred Grad:  -0.025, New P: 0.050
iter 10 loss: 0.306
Actual params: [0.5963, 0.0501]
-Original Grad: -0.020, -lr * Pred Grad:  0.029, New P: 0.625
-Original Grad: 0.010, -lr * Pred Grad:  -0.022, New P: 0.028
iter 11 loss: 0.304
Actual params: [0.6251, 0.0285]
-Original Grad: -0.020, -lr * Pred Grad:  0.023, New P: 0.648
-Original Grad: 0.012, -lr * Pred Grad:  -0.019, New P: 0.010
iter 12 loss: 0.304
Actual params: [0.6482, 0.0096]
-Original Grad: -0.015, -lr * Pred Grad:  0.019, New P: 0.667
-Original Grad: 0.014, -lr * Pred Grad:  -0.016, New P: -0.007
iter 13 loss: 0.305
Actual params: [ 0.667 , -0.0065]
-Original Grad: -0.022, -lr * Pred Grad:  0.014, New P: 0.681
-Original Grad: 0.014, -lr * Pred Grad:  -0.014, New P: -0.020
iter 14 loss: 0.305
Actual params: [ 0.681 , -0.0204]
-Original Grad: -0.017, -lr * Pred Grad:  0.010, New P: 0.691
-Original Grad: 0.020, -lr * Pred Grad:  -0.011, New P: -0.032
iter 15 loss: 0.305
Actual params: [ 0.6913, -0.0317]
-Original Grad: -0.010, -lr * Pred Grad:  0.008, New P: 0.699
-Original Grad: 0.017, -lr * Pred Grad:  -0.009, New P: -0.041
iter 16 loss: 0.306
Actual params: [ 0.6993, -0.041 ]
-Original Grad: -0.020, -lr * Pred Grad:  0.004, New P: 0.704
-Original Grad: 0.022, -lr * Pred Grad:  -0.007, New P: -0.048
iter 17 loss: 0.306
Actual params: [ 0.7037, -0.0481]
-Original Grad: -0.014, -lr * Pred Grad:  0.002, New P: 0.706
-Original Grad: 0.019, -lr * Pred Grad:  -0.005, New P: -0.054
iter 18 loss: 0.306
Actual params: [ 0.7056, -0.0535]
-Original Grad: -0.014, -lr * Pred Grad:  -0.000, New P: 0.705
-Original Grad: 0.027, -lr * Pred Grad:  -0.003, New P: -0.057
iter 19 loss: 0.307
Actual params: [ 0.7055, -0.0568]
-Original Grad: -0.016, -lr * Pred Grad:  -0.002, New P: 0.703
-Original Grad: 0.020, -lr * Pred Grad:  -0.002, New P: -0.059
iter 20 loss: 0.307
Actual params: [ 0.7031, -0.0586]
-Original Grad: -0.012, -lr * Pred Grad:  -0.004, New P: 0.699
-Original Grad: 0.013, -lr * Pred Grad:  -0.001, New P: -0.059
Target params: [1.1812, 0.2779]
iter 0 loss: 1.269
Actual params: [0.5941, 0.5941]
-Original Grad: 0.276, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.379, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 1.194
Actual params: [0.6941, 0.4941]
-Original Grad: 0.451, -lr * Pred Grad:  0.098, New P: 0.792
-Original Grad: -0.362, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 0.997
Actual params: [0.7925, 0.3942]
-Original Grad: 0.722, -lr * Pred Grad:  0.096, New P: 0.889
-Original Grad: -0.250, -lr * Pred Grad:  -0.097, New P: 0.297
iter 3 loss: 0.705
Actual params: [0.889, 0.297]
-Original Grad: 0.383, -lr * Pred Grad:  0.096, New P: 0.985
-Original Grad: -0.057, -lr * Pred Grad:  -0.085, New P: 0.212
iter 4 loss: 0.480
Actual params: [0.9845, 0.2121]
-Original Grad: 0.150, -lr * Pred Grad:  0.088, New P: 1.073
-Original Grad: 0.009, -lr * Pred Grad:  -0.071, New P: 0.141
iter 5 loss: 0.416
Actual params: [1.0726, 0.1412]
-Original Grad: 0.076, -lr * Pred Grad:  0.080, New P: 1.152
-Original Grad: 0.009, -lr * Pred Grad:  -0.060, New P: 0.081
iter 6 loss: 0.413
Actual params: [1.1524, 0.0809]
-Original Grad: 0.043, -lr * Pred Grad:  0.072, New P: 1.224
-Original Grad: 0.018, -lr * Pred Grad:  -0.051, New P: 0.030
iter 7 loss: 0.442
Actual params: [1.2241, 0.0297]
-Original Grad: -0.015, -lr * Pred Grad:  0.062, New P: 1.287
-Original Grad: 0.025, -lr * Pred Grad:  -0.043, New P: -0.013
iter 8 loss: 0.492
Actual params: [ 1.2866, -0.0132]
-Original Grad: -0.099, -lr * Pred Grad:  0.050, New P: 1.337
-Original Grad: 0.025, -lr * Pred Grad:  -0.036, New P: -0.049
iter 9 loss: 0.529
Actual params: [ 1.3369, -0.0491]
-Original Grad: -0.102, -lr * Pred Grad:  0.040, New P: 1.377
-Original Grad: -0.010, -lr * Pred Grad:  -0.033, New P: -0.082
iter 10 loss: 0.550
Actual params: [ 1.3766, -0.082 ]
-Original Grad: -0.102, -lr * Pred Grad:  0.030, New P: 1.407
-Original Grad: 0.019, -lr * Pred Grad:  -0.028, New P: -0.110
iter 11 loss: 0.573
Actual params: [ 1.407 , -0.1099]
-Original Grad: -0.127, -lr * Pred Grad:  0.021, New P: 1.428
-Original Grad: 0.055, -lr * Pred Grad:  -0.020, New P: -0.130
iter 12 loss: 0.581
Actual params: [ 1.4281, -0.1303]
-Original Grad: -0.142, -lr * Pred Grad:  0.012, New P: 1.440
-Original Grad: 0.056, -lr * Pred Grad:  -0.014, New P: -0.144
iter 13 loss: 0.592
Actual params: [ 1.4403, -0.144 ]
-Original Grad: -0.134, -lr * Pred Grad:  0.005, New P: 1.445
-Original Grad: 0.066, -lr * Pred Grad:  -0.007, New P: -0.151
iter 14 loss: 0.596
Actual params: [ 1.4448, -0.1509]
-Original Grad: -0.131, -lr * Pred Grad:  -0.002, New P: 1.443
-Original Grad: 0.053, -lr * Pred Grad:  -0.002, New P: -0.153
iter 15 loss: 0.595
Actual params: [ 1.4427, -0.1528]
-Original Grad: -0.116, -lr * Pred Grad:  -0.007, New P: 1.435
-Original Grad: 0.055, -lr * Pred Grad:  0.003, New P: -0.150
iter 16 loss: 0.589
Actual params: [ 1.4353, -0.1499]
-Original Grad: -0.115, -lr * Pred Grad:  -0.012, New P: 1.423
-Original Grad: 0.026, -lr * Pred Grad:  0.005, New P: -0.145
iter 17 loss: 0.580
Actual params: [ 1.4233, -0.1452]
-Original Grad: -0.090, -lr * Pred Grad:  -0.015, New P: 1.408
-Original Grad: 0.059, -lr * Pred Grad:  0.009, New P: -0.136
iter 18 loss: 0.568
Actual params: [ 1.4081, -0.1361]
-Original Grad: -0.124, -lr * Pred Grad:  -0.020, New P: 1.389
-Original Grad: 0.038, -lr * Pred Grad:  0.011, New P: -0.125
iter 19 loss: 0.563
Actual params: [ 1.3885, -0.1246]
-Original Grad: -0.108, -lr * Pred Grad:  -0.023, New P: 1.366
-Original Grad: 0.026, -lr * Pred Grad:  0.013, New P: -0.112
iter 20 loss: 0.546
Actual params: [ 1.3657, -0.112 ]
-Original Grad: -0.136, -lr * Pred Grad:  -0.027, New P: 1.339
-Original Grad: 0.056, -lr * Pred Grad:  0.016, New P: -0.096
Target params: [1.1812, 0.2779]
iter 0 loss: 0.863
Actual params: [0.5941, 0.5941]
-Original Grad: 0.395, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.745, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.792
Actual params: [0.6941, 0.4941]
-Original Grad: 0.389, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.372, -lr * Pred Grad:  -0.093, New P: 0.401
iter 2 loss: 0.683
Actual params: [0.794 , 0.4009]
-Original Grad: 0.299, -lr * Pred Grad:  0.098, New P: 0.892
-Original Grad: -0.233, -lr * Pred Grad:  -0.087, New P: 0.314
iter 3 loss: 0.582
Actual params: [0.8923, 0.3143]
-Original Grad: 0.123, -lr * Pred Grad:  0.090, New P: 0.983
-Original Grad: -0.033, -lr * Pred Grad:  -0.073, New P: 0.241
iter 4 loss: 0.501
Actual params: [0.9825, 0.2412]
-Original Grad: 0.046, -lr * Pred Grad:  0.080, New P: 1.062
-Original Grad: -0.033, -lr * Pred Grad:  -0.064, New P: 0.177
iter 5 loss: 0.472
Actual params: [1.0625, 0.1774]
-Original Grad: 0.011, -lr * Pred Grad:  0.070, New P: 1.132
-Original Grad: -0.018, -lr * Pred Grad:  -0.056, New P: 0.121
iter 6 loss: 0.470
Actual params: [1.1322, 0.1213]
-Original Grad: -0.037, -lr * Pred Grad:  0.058, New P: 1.190
-Original Grad: -0.013, -lr * Pred Grad:  -0.050, New P: 0.072
iter 7 loss: 0.491
Actual params: [1.1901, 0.0716]
-Original Grad: -0.043, -lr * Pred Grad:  0.048, New P: 1.238
-Original Grad: -0.014, -lr * Pred Grad:  -0.045, New P: 0.027
iter 8 loss: 0.499
Actual params: [1.2377, 0.0269]
-Original Grad: -0.048, -lr * Pred Grad:  0.038, New P: 1.276
-Original Grad: -0.008, -lr * Pred Grad:  -0.040, New P: -0.013
iter 9 loss: 0.518
Actual params: [ 1.2761, -0.0132]
-Original Grad: -0.049, -lr * Pred Grad:  0.031, New P: 1.307
-Original Grad: -0.033, -lr * Pred Grad:  -0.038, New P: -0.051
iter 10 loss: 0.533
Actual params: [ 1.3067, -0.0508]
-Original Grad: -0.058, -lr * Pred Grad:  0.023, New P: 1.330
-Original Grad: 0.014, -lr * Pred Grad:  -0.033, New P: -0.084
iter 11 loss: 0.541
Actual params: [ 1.3297, -0.0837]
-Original Grad: -0.050, -lr * Pred Grad:  0.017, New P: 1.347
-Original Grad: 0.014, -lr * Pred Grad:  -0.029, New P: -0.113
iter 12 loss: 0.537
Actual params: [ 1.3466, -0.1125]
-Original Grad: -0.048, -lr * Pred Grad:  0.012, New P: 1.358
-Original Grad: 0.014, -lr * Pred Grad:  -0.025, New P: -0.138
iter 13 loss: 0.542
Actual params: [ 1.3583, -0.1377]
-Original Grad: -0.039, -lr * Pred Grad:  0.008, New P: 1.366
-Original Grad: 0.013, -lr * Pred Grad:  -0.022, New P: -0.160
iter 14 loss: 0.545
Actual params: [ 1.366 , -0.1597]
-Original Grad: -0.042, -lr * Pred Grad:  0.004, New P: 1.370
-Original Grad: 0.017, -lr * Pred Grad:  -0.019, New P: -0.179
iter 15 loss: 0.547
Actual params: [ 1.3698, -0.1787]
-Original Grad: -0.043, -lr * Pred Grad:  0.000, New P: 1.370
-Original Grad: 0.025, -lr * Pred Grad:  -0.016, New P: -0.194
iter 16 loss: 0.547
Actual params: [ 1.37  , -0.1945]
-Original Grad: -0.045, -lr * Pred Grad:  -0.003, New P: 1.367
-Original Grad: 0.029, -lr * Pred Grad:  -0.013, New P: -0.207
iter 17 loss: 0.545
Actual params: [ 1.3668, -0.2071]
-Original Grad: -0.045, -lr * Pred Grad:  -0.006, New P: 1.361
-Original Grad: 0.020, -lr * Pred Grad:  -0.010, New P: -0.217
iter 18 loss: 0.543
Actual params: [ 1.3605, -0.2174]
-Original Grad: -0.034, -lr * Pred Grad:  -0.008, New P: 1.352
-Original Grad: 0.010, -lr * Pred Grad:  -0.009, New P: -0.226
iter 19 loss: 0.539
Actual params: [ 1.3522, -0.2262]
-Original Grad: -0.042, -lr * Pred Grad:  -0.011, New P: 1.341
-Original Grad: 0.014, -lr * Pred Grad:  -0.007, New P: -0.233
iter 20 loss: 0.546
Actual params: [ 1.3414, -0.2333]
-Original Grad: -0.044, -lr * Pred Grad:  -0.013, New P: 1.328
-Original Grad: 0.019, -lr * Pred Grad:  -0.005, New P: -0.239
Target params: [1.1812, 0.2779]
iter 0 loss: 0.885
Actual params: [0.5941, 0.5941]
-Original Grad: 0.129, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.674, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.796
Actual params: [0.6941, 0.4941]
-Original Grad: 0.364, -lr * Pred Grad:  0.093, New P: 0.787
-Original Grad: -0.389, -lr * Pred Grad:  -0.095, New P: 0.399
iter 2 loss: 0.668
Actual params: [0.7866, 0.3988]
-Original Grad: 0.367, -lr * Pred Grad:  0.096, New P: 0.882
-Original Grad: -0.103, -lr * Pred Grad:  -0.081, New P: 0.317
iter 3 loss: 0.519
Actual params: [0.8825, 0.3175]
-Original Grad: 0.229, -lr * Pred Grad:  0.095, New P: 0.978
-Original Grad: -0.017, -lr * Pred Grad:  -0.068, New P: 0.250
iter 4 loss: 0.423
Actual params: [0.9776, 0.2496]
-Original Grad: 0.052, -lr * Pred Grad:  0.085, New P: 1.063
-Original Grad: -0.007, -lr * Pred Grad:  -0.058, New P: 0.192
iter 5 loss: 0.387
Actual params: [1.0625, 0.1917]
-Original Grad: 0.012, -lr * Pred Grad:  0.074, New P: 1.137
-Original Grad: 0.019, -lr * Pred Grad:  -0.049, New P: 0.143
iter 6 loss: 0.385
Actual params: [1.1368, 0.143 ]
-Original Grad: -0.012, -lr * Pred Grad:  0.064, New P: 1.201
-Original Grad: 0.019, -lr * Pred Grad:  -0.041, New P: 0.102
iter 7 loss: 0.397
Actual params: [1.2006, 0.1018]
-Original Grad: -0.032, -lr * Pred Grad:  0.053, New P: 1.254
-Original Grad: 0.116, -lr * Pred Grad:  -0.029, New P: 0.073
iter 8 loss: 0.403
Actual params: [1.2541, 0.0732]
-Original Grad: -0.034, -lr * Pred Grad:  0.044, New P: 1.299
-Original Grad: 0.077, -lr * Pred Grad:  -0.021, New P: 0.053
iter 9 loss: 0.413
Actual params: [1.2985, 0.0526]
-Original Grad: -0.082, -lr * Pred Grad:  0.033, New P: 1.331
-Original Grad: 0.091, -lr * Pred Grad:  -0.013, New P: 0.040
iter 10 loss: 0.425
Actual params: [1.3311, 0.0399]
-Original Grad: -0.093, -lr * Pred Grad:  0.021, New P: 1.352
-Original Grad: 0.194, -lr * Pred Grad:  0.000, New P: 0.040
iter 11 loss: 0.435
Actual params: [1.3523, 0.0401]
-Original Grad: -0.084, -lr * Pred Grad:  0.012, New P: 1.365
-Original Grad: 0.033, -lr * Pred Grad:  0.002, New P: 0.042
iter 12 loss: 0.441
Actual params: [1.3645, 0.0423]
-Original Grad: -0.062, -lr * Pred Grad:  0.006, New P: 1.370
-Original Grad: 0.051, -lr * Pred Grad:  0.005, New P: 0.047
iter 13 loss: 0.444
Actual params: [1.3705, 0.0472]
-Original Grad: -0.096, -lr * Pred Grad:  -0.002, New P: 1.368
-Original Grad: 0.030, -lr * Pred Grad:  0.006, New P: 0.053
iter 14 loss: 0.443
Actual params: [1.3682, 0.0534]
-Original Grad: -0.078, -lr * Pred Grad:  -0.008, New P: 1.360
-Original Grad: 0.034, -lr * Pred Grad:  0.008, New P: 0.061
iter 15 loss: 0.440
Actual params: [1.36  , 0.0609]
-Original Grad: -0.085, -lr * Pred Grad:  -0.014, New P: 1.346
-Original Grad: 0.146, -lr * Pred Grad:  0.015, New P: 0.076
iter 16 loss: 0.434
Actual params: [1.3458, 0.0762]
-Original Grad: -0.092, -lr * Pred Grad:  -0.020, New P: 1.326
-Original Grad: 0.035, -lr * Pred Grad:  0.016, New P: 0.092
iter 17 loss: 0.426
Actual params: [1.3259, 0.0921]
-Original Grad: -0.084, -lr * Pred Grad:  -0.024, New P: 1.301
-Original Grad: 0.084, -lr * Pred Grad:  0.019, New P: 0.111
iter 18 loss: 0.417
Actual params: [1.3015, 0.1115]
-Original Grad: -0.077, -lr * Pred Grad:  -0.028, New P: 1.273
-Original Grad: 0.064, -lr * Pred Grad:  0.021, New P: 0.133
iter 19 loss: 0.409
Actual params: [1.2733, 0.1328]
-Original Grad: -0.065, -lr * Pred Grad:  -0.031, New P: 1.243
-Original Grad: 0.117, -lr * Pred Grad:  0.026, New P: 0.159
iter 20 loss: 0.406
Actual params: [1.2427, 0.159 ]
-Original Grad: -0.034, -lr * Pred Grad:  -0.031, New P: 1.212
-Original Grad: 0.025, -lr * Pred Grad:  0.025, New P: 0.184
Target params: [1.1812, 0.2779]
iter 0 loss: 0.338
Actual params: [0.5941, 0.5941]
-Original Grad: 0.040, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.200, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.296
Actual params: [0.6941, 0.4941]
-Original Grad: 0.033, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.141, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.268
Actual params: [0.7931, 0.3964]
-Original Grad: 0.031, -lr * Pred Grad:  0.099, New P: 0.892
-Original Grad: -0.063, -lr * Pred Grad:  -0.089, New P: 0.307
iter 3 loss: 0.254
Actual params: [0.8916, 0.3074]
-Original Grad: -0.005, -lr * Pred Grad:  0.076, New P: 0.967
-Original Grad: -0.059, -lr * Pred Grad:  -0.084, New P: 0.223
iter 4 loss: 0.258
Actual params: [0.9672, 0.2232]
-Original Grad: -0.005, -lr * Pred Grad:  0.060, New P: 1.027
-Original Grad: -0.010, -lr * Pred Grad:  -0.073, New P: 0.150
iter 5 loss: 0.264
Actual params: [1.0267, 0.1499]
-Original Grad: -0.016, -lr * Pred Grad:  0.036, New P: 1.063
-Original Grad: 0.019, -lr * Pred Grad:  -0.059, New P: 0.091
iter 6 loss: 0.270
Actual params: [1.0625, 0.0906]
-Original Grad: -0.006, -lr * Pred Grad:  0.026, New P: 1.089
-Original Grad: 0.045, -lr * Pred Grad:  -0.042, New P: 0.048
iter 7 loss: 0.274
Actual params: [1.0886, 0.0484]
-Original Grad: -0.015, -lr * Pred Grad:  0.011, New P: 1.100
-Original Grad: 0.014, -lr * Pred Grad:  -0.035, New P: 0.014
iter 8 loss: 0.276
Actual params: [1.0997, 0.0137]
-Original Grad: -0.015, -lr * Pred Grad:  -0.001, New P: 1.098
-Original Grad: 0.007, -lr * Pred Grad:  -0.029, New P: -0.016
iter 9 loss: 0.277
Actual params: [ 1.0983, -0.0157]
-Original Grad: -0.006, -lr * Pred Grad:  -0.006, New P: 1.093
-Original Grad: 0.019, -lr * Pred Grad:  -0.023, New P: -0.038
iter 10 loss: 0.278
Actual params: [ 1.0928, -0.0384]
-Original Grad: -0.007, -lr * Pred Grad:  -0.010, New P: 1.083
-Original Grad: 0.030, -lr * Pred Grad:  -0.015, New P: -0.053
iter 11 loss: 0.278
Actual params: [ 1.0825, -0.0533]
-Original Grad: -0.002, -lr * Pred Grad:  -0.011, New P: 1.072
-Original Grad: 0.027, -lr * Pred Grad:  -0.008, New P: -0.062
iter 12 loss: 0.278
Actual params: [ 1.0719, -0.0617]
-Original Grad: -0.002, -lr * Pred Grad:  -0.011, New P: 1.061
-Original Grad: 0.036, -lr * Pred Grad:  -0.001, New P: -0.063
iter 13 loss: 0.278
Actual params: [ 1.0608, -0.0628]
-Original Grad: -0.016, -lr * Pred Grad:  -0.021, New P: 1.040
-Original Grad: 0.011, -lr * Pred Grad:  0.001, New P: -0.062
iter 14 loss: 0.278
Actual params: [ 1.0397, -0.0619]
-Original Grad: -0.016, -lr * Pred Grad:  -0.030, New P: 1.010
-Original Grad: 0.007, -lr * Pred Grad:  0.002, New P: -0.060
iter 15 loss: 0.278
Actual params: [ 1.0098, -0.0597]
-Original Grad: -0.010, -lr * Pred Grad:  -0.034, New P: 0.976
-Original Grad: 0.011, -lr * Pred Grad:  0.004, New P: -0.056
iter 16 loss: 0.278
Actual params: [ 0.9762, -0.0557]
-Original Grad: -0.011, -lr * Pred Grad:  -0.037, New P: 0.939
-Original Grad: 0.006, -lr * Pred Grad:  0.005, New P: -0.051
iter 17 loss: 0.278
Actual params: [ 0.9389, -0.0511]
-Original Grad: -0.005, -lr * Pred Grad:  -0.037, New P: 0.902
-Original Grad: 0.024, -lr * Pred Grad:  0.009, New P: -0.042
iter 18 loss: 0.279
Actual params: [ 0.9016, -0.0425]
-Original Grad: -0.012, -lr * Pred Grad:  -0.042, New P: 0.860
-Original Grad: 0.021, -lr * Pred Grad:  0.012, New P: -0.031
iter 19 loss: 0.281
Actual params: [ 0.8596, -0.0308]
-Original Grad: 0.008, -lr * Pred Grad:  -0.032, New P: 0.827
-Original Grad: 0.014, -lr * Pred Grad:  0.013, New P: -0.018
iter 20 loss: 0.283
Actual params: [ 0.8273, -0.0175]
-Original Grad: 0.016, -lr * Pred Grad:  -0.018, New P: 0.809
-Original Grad: 0.020, -lr * Pred Grad:  0.016, New P: -0.002
Target params: [1.1812, 0.2779]
iter 0 loss: 0.605
Actual params: [0.5941, 0.5941]
-Original Grad: 0.317, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.037, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.553
Actual params: [0.6941, 0.4941]
-Original Grad: 0.269, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: 0.015, -lr * Pred Grad:  -0.033, New P: 0.461
iter 2 loss: 0.472
Actual params: [0.7933, 0.4606]
-Original Grad: 0.172, -lr * Pred Grad:  0.095, New P: 0.889
-Original Grad: -0.008, -lr * Pred Grad:  -0.038, New P: 0.423
iter 3 loss: 0.373
Actual params: [0.8886, 0.4226]
-Original Grad: 0.130, -lr * Pred Grad:  0.091, New P: 0.980
-Original Grad: 0.004, -lr * Pred Grad:  -0.025, New P: 0.398
iter 4 loss: 0.295
Actual params: [0.9798, 0.3975]
-Original Grad: 0.055, -lr * Pred Grad:  0.083, New P: 1.063
-Original Grad: 0.017, -lr * Pred Grad:  0.001, New P: 0.399
iter 5 loss: 0.254
Actual params: [1.0627, 0.3988]
-Original Grad: 0.008, -lr * Pred Grad:  0.072, New P: 1.135
-Original Grad: -0.011, -lr * Pred Grad:  -0.011, New P: 0.388
iter 6 loss: 0.255
Actual params: [1.135 , 0.3878]
-Original Grad: -0.035, -lr * Pred Grad:  0.059, New P: 1.194
-Original Grad: 0.022, -lr * Pred Grad:  0.013, New P: 0.401
iter 7 loss: 0.262
Actual params: [1.1943, 0.4011]
-Original Grad: -0.032, -lr * Pred Grad:  0.049, New P: 1.243
-Original Grad: 0.011, -lr * Pred Grad:  0.022, New P: 0.423
iter 8 loss: 0.269
Actual params: [1.2431, 0.4229]
-Original Grad: -0.033, -lr * Pred Grad:  0.040, New P: 1.283
-Original Grad: -0.009, -lr * Pred Grad:  0.010, New P: 0.433
iter 9 loss: 0.276
Actual params: [1.2829, 0.4332]
-Original Grad: -0.015, -lr * Pred Grad:  0.034, New P: 1.317
-Original Grad: -0.016, -lr * Pred Grad:  -0.005, New P: 0.428
iter 10 loss: 0.284
Actual params: [1.3169, 0.4281]
-Original Grad: -0.037, -lr * Pred Grad:  0.027, New P: 1.344
-Original Grad: -0.005, -lr * Pred Grad:  -0.009, New P: 0.419
iter 11 loss: 0.299
Actual params: [1.3436, 0.4193]
-Original Grad: -0.064, -lr * Pred Grad:  0.017, New P: 1.361
-Original Grad: 0.026, -lr * Pred Grad:  0.013, New P: 0.433
iter 12 loss: 0.302
Actual params: [1.3608, 0.4325]
-Original Grad: -0.053, -lr * Pred Grad:  0.010, New P: 1.371
-Original Grad: 0.019, -lr * Pred Grad:  0.026, New P: 0.458
iter 13 loss: 0.298
Actual params: [1.371 , 0.4585]
-Original Grad: -0.027, -lr * Pred Grad:  0.006, New P: 1.377
-Original Grad: -0.004, -lr * Pred Grad:  0.020, New P: 0.479
iter 14 loss: 0.293
Actual params: [1.3774, 0.4789]
-Original Grad: -0.028, -lr * Pred Grad:  0.003, New P: 1.380
-Original Grad: -0.010, -lr * Pred Grad:  0.011, New P: 0.490
iter 15 loss: 0.290
Actual params: [1.3804, 0.4898]
-Original Grad: -0.026, -lr * Pred Grad:  0.000, New P: 1.380
-Original Grad: 0.009, -lr * Pred Grad:  0.017, New P: 0.506
iter 16 loss: 0.295
Actual params: [1.3804, 0.5064]
-Original Grad: -0.033, -lr * Pred Grad:  -0.003, New P: 1.377
-Original Grad: -0.011, -lr * Pred Grad:  0.006, New P: 0.513
iter 17 loss: 0.291
Actual params: [1.3771, 0.5126]
-Original Grad: -0.027, -lr * Pred Grad:  -0.006, New P: 1.371
-Original Grad: -0.006, -lr * Pred Grad:  0.001, New P: 0.514
iter 18 loss: 0.288
Actual params: [1.3714, 0.5139]
-Original Grad: -0.021, -lr * Pred Grad:  -0.007, New P: 1.364
-Original Grad: -0.011, -lr * Pred Grad:  -0.007, New P: 0.507
iter 19 loss: 0.286
Actual params: [1.3639, 0.5068]
-Original Grad: -0.032, -lr * Pred Grad:  -0.010, New P: 1.354
-Original Grad: -0.016, -lr * Pred Grad:  -0.018, New P: 0.489
iter 20 loss: 0.287
Actual params: [1.3539, 0.489 ]
-Original Grad: -0.051, -lr * Pred Grad:  -0.014, New P: 1.339
-Original Grad: -0.007, -lr * Pred Grad:  -0.021, New P: 0.467
Target params: [1.1812, 0.2779]
iter 0 loss: 0.789
Actual params: [0.5941, 0.5941]
-Original Grad: 0.562, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.115, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.720
Actual params: [0.6941, 0.4941]
-Original Grad: 0.628, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.049, -lr * Pred Grad:  -0.091, New P: 0.403
iter 2 loss: 0.606
Actual params: [0.7942, 0.4033]
-Original Grad: 0.392, -lr * Pred Grad:  0.097, New P: 0.891
-Original Grad: 0.215, -lr * Pred Grad:  0.020, New P: 0.423
iter 3 loss: 0.450
Actual params: [0.8913, 0.4234]
-Original Grad: 0.194, -lr * Pred Grad:  0.090, New P: 0.981
-Original Grad: 0.178, -lr * Pred Grad:  0.047, New P: 0.471
iter 4 loss: 0.323
Actual params: [0.9811, 0.4706]
-Original Grad: -0.064, -lr * Pred Grad:  0.072, New P: 1.053
-Original Grad: 0.087, -lr * Pred Grad:  0.053, New P: 0.524
iter 5 loss: 0.353
Actual params: [1.0531, 0.5239]
-Original Grad: -0.037, -lr * Pred Grad:  0.060, New P: 1.113
-Original Grad: -0.058, -lr * Pred Grad:  0.036, New P: 0.560
iter 6 loss: 0.411
Actual params: [1.1131, 0.5596]
-Original Grad: 0.036, -lr * Pred Grad:  0.054, New P: 1.167
-Original Grad: -0.049, -lr * Pred Grad:  0.023, New P: 0.583
iter 7 loss: 0.449
Actual params: [1.1674, 0.5828]
-Original Grad: 0.053, -lr * Pred Grad:  0.051, New P: 1.218
-Original Grad: -0.187, -lr * Pred Grad:  -0.007, New P: 0.576
iter 8 loss: 0.462
Actual params: [1.218 , 0.5759]
-Original Grad: 0.069, -lr * Pred Grad:  0.048, New P: 1.266
-Original Grad: -0.132, -lr * Pred Grad:  -0.022, New P: 0.554
iter 9 loss: 0.466
Actual params: [1.2662, 0.5541]
-Original Grad: 0.086, -lr * Pred Grad:  0.047, New P: 1.314
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: 0.534
iter 10 loss: 0.473
Actual params: [1.3135, 0.5344]
-Original Grad: 0.053, -lr * Pred Grad:  0.045, New P: 1.358
-Original Grad: -0.019, -lr * Pred Grad:  -0.020, New P: 0.515
iter 11 loss: 0.479
Actual params: [1.3585, 0.5146]
-Original Grad: 0.039, -lr * Pred Grad:  0.042, New P: 1.401
-Original Grad: -0.026, -lr * Pred Grad:  -0.021, New P: 0.494
iter 12 loss: 0.483
Actual params: [1.4009, 0.4937]
-Original Grad: 0.001, -lr * Pred Grad:  0.038, New P: 1.439
-Original Grad: -0.104, -lr * Pred Grad:  -0.030, New P: 0.463
iter 13 loss: 0.474
Actual params: [1.4391, 0.4633]
-Original Grad: 0.038, -lr * Pred Grad:  0.036, New P: 1.476
-Original Grad: 0.065, -lr * Pred Grad:  -0.020, New P: 0.444
iter 14 loss: 0.478
Actual params: [1.4755, 0.4437]
-Original Grad: -0.000, -lr * Pred Grad:  0.033, New P: 1.508
-Original Grad: -0.010, -lr * Pred Grad:  -0.019, New P: 0.425
iter 15 loss: 0.478
Actual params: [1.5085, 0.425 ]
-Original Grad: -0.001, -lr * Pred Grad:  0.030, New P: 1.538
-Original Grad: 0.043, -lr * Pred Grad:  -0.012, New P: 0.413
iter 16 loss: 0.490
Actual params: [1.5383, 0.413 ]
-Original Grad: -0.004, -lr * Pred Grad:  0.027, New P: 1.565
-Original Grad: 0.112, -lr * Pred Grad:  0.002, New P: 0.415
iter 17 loss: 0.507
Actual params: [1.5652, 0.4152]
-Original Grad: -0.025, -lr * Pred Grad:  0.023, New P: 1.588
-Original Grad: 0.094, -lr * Pred Grad:  0.013, New P: 0.428
iter 18 loss: 0.531
Actual params: [1.5882, 0.4277]
-Original Grad: -0.018, -lr * Pred Grad:  0.020, New P: 1.608
-Original Grad: 0.039, -lr * Pred Grad:  0.016, New P: 0.443
iter 19 loss: 0.546
Actual params: [1.6082, 0.4434]
-Original Grad: -0.024, -lr * Pred Grad:  0.017, New P: 1.625
-Original Grad: -0.037, -lr * Pred Grad:  0.010, New P: 0.454
iter 20 loss: 0.559
Actual params: [1.6251, 0.4536]
-Original Grad: -0.025, -lr * Pred Grad:  0.014, New P: 1.639
-Original Grad: -0.043, -lr * Pred Grad:  0.004, New P: 0.458
Target params: [1.1812, 0.2779]
iter 0 loss: 0.743
Actual params: [0.5941, 0.5941]
-Original Grad: 0.084, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.708, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.390
Actual params: [0.6941, 0.4941]
-Original Grad: 0.065, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.273, -lr * Pred Grad:  -0.089, New P: 0.405
iter 2 loss: 0.282
Actual params: [0.7926, 0.4048]
-Original Grad: 0.034, -lr * Pred Grad:  0.092, New P: 0.885
-Original Grad: -0.072, -lr * Pred Grad:  -0.075, New P: 0.330
iter 3 loss: 0.256
Actual params: [0.8848, 0.3301]
-Original Grad: -0.001, -lr * Pred Grad:  0.075, New P: 0.960
-Original Grad: -0.026, -lr * Pred Grad:  -0.063, New P: 0.267
iter 4 loss: 0.248
Actual params: [0.9598, 0.2669]
-Original Grad: 0.016, -lr * Pred Grad:  0.071, New P: 1.030
-Original Grad: -0.008, -lr * Pred Grad:  -0.054, New P: 0.213
iter 5 loss: 0.241
Actual params: [1.0304, 0.2129]
-Original Grad: 0.009, -lr * Pred Grad:  0.065, New P: 1.095
-Original Grad: -0.003, -lr * Pred Grad:  -0.047, New P: 0.166
iter 6 loss: 0.236
Actual params: [1.0952, 0.1662]
-Original Grad: 0.003, -lr * Pred Grad:  0.058, New P: 1.153
-Original Grad: -0.001, -lr * Pred Grad:  -0.041, New P: 0.125
iter 7 loss: 0.231
Actual params: [1.1532, 0.1253]
-Original Grad: 0.001, -lr * Pred Grad:  0.052, New P: 1.205
-Original Grad: 0.005, -lr * Pred Grad:  -0.036, New P: 0.090
iter 8 loss: 0.227
Actual params: [1.2048, 0.0897]
-Original Grad: 0.004, -lr * Pred Grad:  0.048, New P: 1.252
-Original Grad: 0.005, -lr * Pred Grad:  -0.031, New P: 0.058
iter 9 loss: 0.224
Actual params: [1.2524, 0.0583]
-Original Grad: 0.000, -lr * Pred Grad:  0.043, New P: 1.295
-Original Grad: 0.022, -lr * Pred Grad:  -0.027, New P: 0.032
iter 10 loss: 0.221
Actual params: [1.295 , 0.0318]
-Original Grad: -0.001, -lr * Pred Grad:  0.038, New P: 1.333
-Original Grad: 0.013, -lr * Pred Grad:  -0.023, New P: 0.009
iter 11 loss: 0.218
Actual params: [1.3325, 0.0088]
-Original Grad: 0.002, -lr * Pred Grad:  0.035, New P: 1.367
-Original Grad: 0.021, -lr * Pred Grad:  -0.019, New P: -0.011
iter 12 loss: 0.216
Actual params: [ 1.3672, -0.0106]
-Original Grad: 0.003, -lr * Pred Grad:  0.033, New P: 1.400
-Original Grad: 0.019, -lr * Pred Grad:  -0.016, New P: -0.027
iter 13 loss: 0.215
Actual params: [ 1.3997, -0.0268]
-Original Grad: 0.002, -lr * Pred Grad:  0.030, New P: 1.430
-Original Grad: 0.017, -lr * Pred Grad:  -0.014, New P: -0.040
iter 14 loss: 0.214
Actual params: [ 1.43  , -0.0404]
-Original Grad: 0.001, -lr * Pred Grad:  0.028, New P: 1.458
-Original Grad: 0.021, -lr * Pred Grad:  -0.011, New P: -0.051
iter 15 loss: 0.214
Actual params: [ 1.4578, -0.0514]
-Original Grad: 0.001, -lr * Pred Grad:  0.026, New P: 1.484
-Original Grad: 0.024, -lr * Pred Grad:  -0.008, New P: -0.060
iter 16 loss: 0.214
Actual params: [ 1.4836, -0.0597]
-Original Grad: -0.002, -lr * Pred Grad:  0.023, New P: 1.506
-Original Grad: 0.018, -lr * Pred Grad:  -0.006, New P: -0.066
iter 17 loss: 0.214
Actual params: [ 1.5062, -0.0662]
-Original Grad: 0.001, -lr * Pred Grad:  0.021, New P: 1.527
-Original Grad: 0.023, -lr * Pred Grad:  -0.004, New P: -0.070
iter 18 loss: 0.215
Actual params: [ 1.5272, -0.0705]
-Original Grad: -0.002, -lr * Pred Grad:  0.018, New P: 1.545
-Original Grad: 0.021, -lr * Pred Grad:  -0.003, New P: -0.073
iter 19 loss: 0.215
Actual params: [ 1.5452, -0.073 ]
-Original Grad: -0.003, -lr * Pred Grad:  0.015, New P: 1.560
-Original Grad: 0.011, -lr * Pred Grad:  -0.002, New P: -0.075
iter 20 loss: 0.216
Actual params: [ 1.56  , -0.0746]
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 1.574
-Original Grad: 0.026, -lr * Pred Grad:  0.000, New P: -0.074
Target params: [1.1812, 0.2779]
iter 0 loss: 0.596
Actual params: [0.5941, 0.5941]
-Original Grad: 0.547, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.142, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.395
Actual params: [0.6941, 0.6941]
-Original Grad: 0.231, -lr * Pred Grad:  0.091, New P: 0.785
-Original Grad: -0.012, -lr * Pred Grad:  0.061, New P: 0.755
iter 2 loss: 0.338
Actual params: [0.7848, 0.7546]
-Original Grad: -0.223, -lr * Pred Grad:  0.043, New P: 0.828
-Original Grad: -0.176, -lr * Pred Grad:  -0.020, New P: 0.735
iter 3 loss: 0.389
Actual params: [0.828 , 0.7345]
-Original Grad: -0.299, -lr * Pred Grad:  0.007, New P: 0.835
-Original Grad: -0.241, -lr * Pred Grad:  -0.054, New P: 0.681
iter 4 loss: 0.318
Actual params: [0.8352, 0.6809]
-Original Grad: -0.167, -lr * Pred Grad:  -0.007, New P: 0.828
-Original Grad: -0.184, -lr * Pred Grad:  -0.066, New P: 0.615
iter 5 loss: 0.294
Actual params: [0.8284, 0.6147]
-Original Grad: -0.013, -lr * Pred Grad:  -0.007, New P: 0.822
-Original Grad: -0.107, -lr * Pred Grad:  -0.069, New P: 0.546
iter 6 loss: 0.345
Actual params: [0.8217, 0.5457]
-Original Grad: 0.063, -lr * Pred Grad:  -0.001, New P: 0.820
-Original Grad: -0.042, -lr * Pred Grad:  -0.065, New P: 0.480
iter 7 loss: 0.412
Actual params: [0.8203, 0.4803]
-Original Grad: 0.067, -lr * Pred Grad:  0.003, New P: 0.824
-Original Grad: -0.050, -lr * Pred Grad:  -0.063, New P: 0.417
iter 8 loss: 0.486
Actual params: [0.8236, 0.417 ]
-Original Grad: 0.210, -lr * Pred Grad:  0.016, New P: 0.840
-Original Grad: 0.055, -lr * Pred Grad:  -0.049, New P: 0.368
iter 9 loss: 0.527
Actual params: [0.84 , 0.368]
-Original Grad: 0.155, -lr * Pred Grad:  0.024, New P: 0.864
-Original Grad: 0.062, -lr * Pred Grad:  -0.036, New P: 0.332
iter 10 loss: 0.529
Actual params: [0.8642, 0.3322]
-Original Grad: 0.172, -lr * Pred Grad:  0.032, New P: 0.896
-Original Grad: 0.099, -lr * Pred Grad:  -0.020, New P: 0.312
iter 11 loss: 0.475
Actual params: [0.8959, 0.3125]
-Original Grad: 0.093, -lr * Pred Grad:  0.034, New P: 0.930
-Original Grad: 0.078, -lr * Pred Grad:  -0.009, New P: 0.304
iter 12 loss: 0.401
Actual params: [0.9298, 0.3039]
-Original Grad: 0.069, -lr * Pred Grad:  0.035, New P: 0.964
-Original Grad: 0.081, -lr * Pred Grad:  0.001, New P: 0.305
iter 13 loss: 0.339
Actual params: [0.9644, 0.3053]
-Original Grad: 0.056, -lr * Pred Grad:  0.035, New P: 0.999
-Original Grad: 0.068, -lr * Pred Grad:  0.009, New P: 0.314
iter 14 loss: 0.302
Actual params: [0.9991, 0.314 ]
-Original Grad: 0.034, -lr * Pred Grad:  0.033, New P: 1.032
-Original Grad: 0.056, -lr * Pred Grad:  0.014, New P: 0.328
iter 15 loss: 0.278
Actual params: [1.0324, 0.328 ]
-Original Grad: 0.008, -lr * Pred Grad:  0.031, New P: 1.063
-Original Grad: -0.014, -lr * Pred Grad:  0.011, New P: 0.339
iter 16 loss: 0.268
Actual params: [1.0632, 0.3392]
-Original Grad: 0.008, -lr * Pred Grad:  0.028, New P: 1.092
-Original Grad: 0.008, -lr * Pred Grad:  0.011, New P: 0.350
iter 17 loss: 0.261
Actual params: [1.0916, 0.3502]
-Original Grad: 0.004, -lr * Pred Grad:  0.026, New P: 1.118
-Original Grad: -0.027, -lr * Pred Grad:  0.007, New P: 0.357
iter 18 loss: 0.258
Actual params: [1.1177, 0.3571]
-Original Grad: 0.002, -lr * Pred Grad:  0.024, New P: 1.142
-Original Grad: 0.009, -lr * Pred Grad:  0.007, New P: 0.364
iter 19 loss: 0.255
Actual params: [1.1415, 0.3644]
-Original Grad: 0.004, -lr * Pred Grad:  0.022, New P: 1.163
-Original Grad: -0.041, -lr * Pred Grad:  0.002, New P: 0.366
iter 20 loss: 0.256
Actual params: [1.1634, 0.3663]
-Original Grad: 0.007, -lr * Pred Grad:  0.020, New P: 1.184
-Original Grad: -0.043, -lr * Pred Grad:  -0.003, New P: 0.363
Target params: [1.1812, 0.2779]
iter 0 loss: 0.722
Actual params: [0.5941, 0.5941]
-Original Grad: 0.186, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.407, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.653
Actual params: [0.6941, 0.4941]
-Original Grad: 0.104, -lr * Pred Grad:  0.095, New P: 0.789
-Original Grad: -0.352, -lr * Pred Grad:  -0.099, New P: 0.395
iter 2 loss: 0.520
Actual params: [0.7889, 0.3947]
-Original Grad: 0.301, -lr * Pred Grad:  0.094, New P: 0.883
-Original Grad: -0.332, -lr * Pred Grad:  -0.099, New P: 0.296
iter 3 loss: 0.292
Actual params: [0.8833, 0.2958]
-Original Grad: 0.088, -lr * Pred Grad:  0.089, New P: 0.972
-Original Grad: -0.088, -lr * Pred Grad:  -0.088, New P: 0.208
iter 4 loss: 0.178
Actual params: [0.9721, 0.2075]
-Original Grad: 0.041, -lr * Pred Grad:  0.080, New P: 1.053
-Original Grad: 0.018, -lr * Pred Grad:  -0.073, New P: 0.134
iter 5 loss: 0.171
Actual params: [1.0525, 0.1345]
-Original Grad: 0.042, -lr * Pred Grad:  0.075, New P: 1.127
-Original Grad: 0.042, -lr * Pred Grad:  -0.059, New P: 0.075
iter 6 loss: 0.185
Actual params: [1.1271, 0.0751]
-Original Grad: 0.079, -lr * Pred Grad:  0.074, New P: 1.201
-Original Grad: 0.044, -lr * Pred Grad:  -0.048, New P: 0.027
iter 7 loss: 0.174
Actual params: [1.2012, 0.0268]
-Original Grad: 0.077, -lr * Pred Grad:  0.074, New P: 1.275
-Original Grad: 0.029, -lr * Pred Grad:  -0.040, New P: -0.013
iter 8 loss: 0.164
Actual params: [ 1.2748, -0.0134]
-Original Grad: 0.053, -lr * Pred Grad:  0.071, New P: 1.346
-Original Grad: 0.010, -lr * Pred Grad:  -0.035, New P: -0.048
iter 9 loss: 0.170
Actual params: [ 1.3461, -0.0483]
-Original Grad: 0.028, -lr * Pred Grad:  0.067, New P: 1.413
-Original Grad: 0.002, -lr * Pred Grad:  -0.031, New P: -0.079
iter 10 loss: 0.176
Actual params: [ 1.4128, -0.0793]
-Original Grad: 0.008, -lr * Pred Grad:  0.061, New P: 1.474
-Original Grad: -0.007, -lr * Pred Grad:  -0.028, New P: -0.108
iter 11 loss: 0.190
Actual params: [ 1.4737, -0.1076]
-Original Grad: 0.014, -lr * Pred Grad:  0.056, New P: 1.530
-Original Grad: -0.007, -lr * Pred Grad:  -0.026, New P: -0.134
iter 12 loss: 0.198
Actual params: [ 1.53  , -0.1336]
-Original Grad: 0.001, -lr * Pred Grad:  0.051, New P: 1.581
-Original Grad: -0.008, -lr * Pred Grad:  -0.024, New P: -0.158
iter 13 loss: 0.206
Actual params: [ 1.5809, -0.1576]
-Original Grad: -0.020, -lr * Pred Grad:  0.043, New P: 1.624
-Original Grad: -0.013, -lr * Pred Grad:  -0.023, New P: -0.180
iter 14 loss: 0.213
Actual params: [ 1.6244, -0.1803]
-Original Grad: -0.009, -lr * Pred Grad:  0.038, New P: 1.663
-Original Grad: -0.009, -lr * Pred Grad:  -0.021, New P: -0.202
iter 15 loss: 0.219
Actual params: [ 1.6626, -0.2016]
-Original Grad: -0.018, -lr * Pred Grad:  0.032, New P: 1.695
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.221
iter 16 loss: 0.218
Actual params: [ 1.695 , -0.2209]
-Original Grad: -0.018, -lr * Pred Grad:  0.027, New P: 1.722
-Original Grad: -0.003, -lr * Pred Grad:  -0.018, New P: -0.239
iter 17 loss: 0.222
Actual params: [ 1.7222, -0.2387]
-Original Grad: -0.032, -lr * Pred Grad:  0.021, New P: 1.743
-Original Grad: 0.005, -lr * Pred Grad:  -0.016, New P: -0.254
iter 18 loss: 0.225
Actual params: [ 1.7429, -0.2544]
-Original Grad: -0.021, -lr * Pred Grad:  0.016, New P: 1.759
-Original Grad: 0.008, -lr * Pred Grad:  -0.014, New P: -0.268
iter 19 loss: 0.226
Actual params: [ 1.759 , -0.2681]
-Original Grad: -0.027, -lr * Pred Grad:  0.011, New P: 1.770
-Original Grad: 0.011, -lr * Pred Grad:  -0.012, New P: -0.280
iter 20 loss: 0.226
Actual params: [ 1.7703, -0.2796]
-Original Grad: -0.037, -lr * Pred Grad:  0.006, New P: 1.776
-Original Grad: 0.011, -lr * Pred Grad:  -0.010, New P: -0.289
Target params: [1.1812, 0.2779]
iter 0 loss: 0.519
Actual params: [0.5941, 0.5941]
-Original Grad: 0.118, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.197, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.508
Actual params: [0.6941, 0.4941]
-Original Grad: 0.211, -lr * Pred Grad:  0.098, New P: 0.792
-Original Grad: 0.005, -lr * Pred Grad:  -0.065, New P: 0.429
iter 2 loss: 0.502
Actual params: [0.7917, 0.4288]
-Original Grad: 0.063, -lr * Pred Grad:  0.089, New P: 0.881
-Original Grad: 0.006, -lr * Pred Grad:  -0.048, New P: 0.380
iter 3 loss: 0.500
Actual params: [0.8808, 0.3803]
-Original Grad: -0.040, -lr * Pred Grad:  0.063, New P: 0.944
-Original Grad: -0.049, -lr * Pred Grad:  -0.052, New P: 0.328
iter 4 loss: 0.490
Actual params: [0.9436, 0.3279]
-Original Grad: -0.037, -lr * Pred Grad:  0.045, New P: 0.988
-Original Grad: -0.023, -lr * Pred Grad:  -0.050, New P: 0.278
iter 5 loss: 0.478
Actual params: [0.9883, 0.2776]
-Original Grad: -0.016, -lr * Pred Grad:  0.035, New P: 1.023
-Original Grad: 0.007, -lr * Pred Grad:  -0.041, New P: 0.236
iter 6 loss: 0.469
Actual params: [1.0235, 0.2363]
-Original Grad: 0.004, -lr * Pred Grad:  0.031, New P: 1.055
-Original Grad: 0.024, -lr * Pred Grad:  -0.030, New P: 0.206
iter 7 loss: 0.463
Actual params: [1.0549, 0.2063]
-Original Grad: 0.019, -lr * Pred Grad:  0.031, New P: 1.086
-Original Grad: 0.042, -lr * Pred Grad:  -0.016, New P: 0.190
iter 8 loss: 0.462
Actual params: [1.0861, 0.1904]
-Original Grad: 0.025, -lr * Pred Grad:  0.032, New P: 1.119
-Original Grad: 0.025, -lr * Pred Grad:  -0.008, New P: 0.182
iter 9 loss: 0.461
Actual params: [1.1185, 0.182 ]
-Original Grad: 0.022, -lr * Pred Grad:  0.033, New P: 1.151
-Original Grad: 0.028, -lr * Pred Grad:  -0.001, New P: 0.181
iter 10 loss: 0.466
Actual params: [1.1515, 0.181 ]
-Original Grad: 0.034, -lr * Pred Grad:  0.036, New P: 1.187
-Original Grad: 0.035, -lr * Pred Grad:  0.007, New P: 0.188
iter 11 loss: 0.473
Actual params: [1.1871, 0.1879]
-Original Grad: 0.040, -lr * Pred Grad:  0.039, New P: 1.226
-Original Grad: 0.020, -lr * Pred Grad:  0.011, New P: 0.198
iter 12 loss: 0.477
Actual params: [1.2261, 0.1984]
-Original Grad: 0.020, -lr * Pred Grad:  0.039, New P: 1.265
-Original Grad: 0.026, -lr * Pred Grad:  0.015, New P: 0.213
iter 13 loss: 0.484
Actual params: [1.2647, 0.2135]
-Original Grad: 0.002, -lr * Pred Grad:  0.035, New P: 1.300
-Original Grad: 0.026, -lr * Pred Grad:  0.019, New P: 0.233
iter 14 loss: 0.493
Actual params: [1.3001, 0.2328]
-Original Grad: 0.011, -lr * Pred Grad:  0.034, New P: 1.334
-Original Grad: 0.011, -lr * Pred Grad:  0.020, New P: 0.253
iter 15 loss: 0.499
Actual params: [1.334 , 0.2527]
-Original Grad: -0.004, -lr * Pred Grad:  0.030, New P: 1.364
-Original Grad: 0.015, -lr * Pred Grad:  0.021, New P: 0.274
iter 16 loss: 0.502
Actual params: [1.364 , 0.2741]
-Original Grad: -0.003, -lr * Pred Grad:  0.027, New P: 1.391
-Original Grad: 0.001, -lr * Pred Grad:  0.020, New P: 0.294
iter 17 loss: 0.507
Actual params: [1.3906, 0.2937]
-Original Grad: -0.013, -lr * Pred Grad:  0.022, New P: 1.412
-Original Grad: 0.013, -lr * Pred Grad:  0.021, New P: 0.314
iter 18 loss: 0.511
Actual params: [1.4123, 0.3145]
-Original Grad: -0.015, -lr * Pred Grad:  0.017, New P: 1.429
-Original Grad: -0.005, -lr * Pred Grad:  0.018, New P: 0.332
iter 19 loss: 0.513
Actual params: [1.4291, 0.3321]
-Original Grad: -0.013, -lr * Pred Grad:  0.013, New P: 1.442
-Original Grad: -0.008, -lr * Pred Grad:  0.014, New P: 0.346
iter 20 loss: 0.514
Actual params: [1.442 , 0.3464]
-Original Grad: -0.010, -lr * Pred Grad:  0.010, New P: 1.452
-Original Grad: -0.028, -lr * Pred Grad:  0.006, New P: 0.353
Target params: [1.1812, 0.2779]
iter 0 loss: 0.729
Actual params: [0.5941, 0.5941]
-Original Grad: -0.086, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.364, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.581
Actual params: [0.4941, 0.4941]
-Original Grad: -0.070, -lr * Pred Grad:  -0.099, New P: 0.395
-Original Grad: -0.319, -lr * Pred Grad:  -0.099, New P: 0.395
iter 2 loss: 0.470
Actual params: [0.395 , 0.3946]
-Original Grad: -0.005, -lr * Pred Grad:  -0.079, New P: 0.316
-Original Grad: -0.200, -lr * Pred Grad:  -0.095, New P: 0.299
iter 3 loss: 0.393
Actual params: [0.3157, 0.2992]
-Original Grad: 0.035, -lr * Pred Grad:  -0.045, New P: 0.271
-Original Grad: -0.128, -lr * Pred Grad:  -0.090, New P: 0.209
iter 4 loss: 0.368
Actual params: [0.2712, 0.2094]
-Original Grad: 0.050, -lr * Pred Grad:  -0.013, New P: 0.258
-Original Grad: -0.057, -lr * Pred Grad:  -0.081, New P: 0.128
iter 5 loss: 0.357
Actual params: [0.2584, 0.1283]
-Original Grad: 0.094, -lr * Pred Grad:  0.022, New P: 0.281
-Original Grad: -0.047, -lr * Pred Grad:  -0.074, New P: 0.054
iter 6 loss: 0.349
Actual params: [0.2808, 0.0541]
-Original Grad: 0.048, -lr * Pred Grad:  0.034, New P: 0.314
-Original Grad: -0.026, -lr * Pred Grad:  -0.067, New P: -0.013
iter 7 loss: 0.345
Actual params: [ 0.3144, -0.013 ]
-Original Grad: 0.059, -lr * Pred Grad:  0.044, New P: 0.359
-Original Grad: -0.056, -lr * Pred Grad:  -0.064, New P: -0.077
iter 8 loss: 0.341
Actual params: [ 0.3588, -0.077 ]
-Original Grad: 0.018, -lr * Pred Grad:  0.044, New P: 0.403
-Original Grad: -0.041, -lr * Pred Grad:  -0.060, New P: -0.137
iter 9 loss: 0.339
Actual params: [ 0.403 , -0.1373]
-Original Grad: 0.003, -lr * Pred Grad:  0.040, New P: 0.443
-Original Grad: -0.032, -lr * Pred Grad:  -0.057, New P: -0.194
iter 10 loss: 0.338
Actual params: [ 0.4433, -0.1938]
-Original Grad: 0.013, -lr * Pred Grad:  0.039, New P: 0.483
-Original Grad: -0.038, -lr * Pred Grad:  -0.054, New P: -0.248
iter 11 loss: 0.338
Actual params: [ 0.4827, -0.2477]
-Original Grad: 0.008, -lr * Pred Grad:  0.037, New P: 0.520
-Original Grad: -0.024, -lr * Pred Grad:  -0.051, New P: -0.298
iter 12 loss: 0.332
Actual params: [ 0.5202, -0.2982]
-Original Grad: 0.019, -lr * Pred Grad:  0.039, New P: 0.559
-Original Grad: -0.020, -lr * Pred Grad:  -0.047, New P: -0.345
iter 13 loss: 0.325
Actual params: [ 0.559 , -0.3455]
-Original Grad: 0.022, -lr * Pred Grad:  0.041, New P: 0.600
-Original Grad: -0.002, -lr * Pred Grad:  -0.043, New P: -0.388
iter 14 loss: 0.321
Actual params: [ 0.5996, -0.3884]
-Original Grad: 0.017, -lr * Pred Grad:  0.041, New P: 0.641
-Original Grad: -0.013, -lr * Pred Grad:  -0.040, New P: -0.428
iter 15 loss: 0.315
Actual params: [ 0.6408, -0.4283]
-Original Grad: 0.017, -lr * Pred Grad:  0.042, New P: 0.683
-Original Grad: 0.022, -lr * Pred Grad:  -0.034, New P: -0.463
iter 16 loss: 0.304
Actual params: [ 0.6827, -0.4625]
-Original Grad: 0.018, -lr * Pred Grad:  0.043, New P: 0.725
-Original Grad: 0.044, -lr * Pred Grad:  -0.027, New P: -0.490
iter 17 loss: 0.294
Actual params: [ 0.7254, -0.4895]
-Original Grad: 0.015, -lr * Pred Grad:  0.043, New P: 0.768
-Original Grad: 0.050, -lr * Pred Grad:  -0.020, New P: -0.509
iter 18 loss: 0.281
Actual params: [ 0.7683, -0.5095]
-Original Grad: 0.023, -lr * Pred Grad:  0.045, New P: 0.813
-Original Grad: 0.054, -lr * Pred Grad:  -0.013, New P: -0.523
iter 19 loss: 0.270
Actual params: [ 0.8133, -0.5227]
-Original Grad: 0.017, -lr * Pred Grad:  0.045, New P: 0.859
-Original Grad: 0.050, -lr * Pred Grad:  -0.007, New P: -0.530
iter 20 loss: 0.256
Actual params: [ 0.8586, -0.5301]
-Original Grad: 0.014, -lr * Pred Grad:  0.045, New P: 0.904
-Original Grad: 0.042, -lr * Pred Grad:  -0.003, New P: -0.533
Target params: [1.1812, 0.2779]
iter 0 loss: 0.763
Actual params: [0.5941, 0.5941]
-Original Grad: 0.355, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.214, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.648
Actual params: [0.6941, 0.4941]
-Original Grad: 0.446, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.037, -lr * Pred Grad:  -0.079, New P: 0.415
iter 2 loss: 0.518
Actual params: [0.794 , 0.4152]
-Original Grad: 0.130, -lr * Pred Grad:  0.090, New P: 0.884
-Original Grad: 0.049, -lr * Pred Grad:  -0.045, New P: 0.370
iter 3 loss: 0.410
Actual params: [0.8835, 0.3697]
-Original Grad: 0.077, -lr * Pred Grad:  0.080, New P: 0.964
-Original Grad: 0.078, -lr * Pred Grad:  -0.016, New P: 0.354
iter 4 loss: 0.323
Actual params: [0.9638, 0.3538]
-Original Grad: -0.013, -lr * Pred Grad:  0.067, New P: 1.031
-Original Grad: 0.077, -lr * Pred Grad:  0.004, New P: 0.358
iter 5 loss: 0.308
Actual params: [1.0305, 0.3581]
-Original Grad: -0.029, -lr * Pred Grad:  0.055, New P: 1.085
-Original Grad: 0.081, -lr * Pred Grad:  0.020, New P: 0.378
iter 6 loss: 0.304
Actual params: [1.0854, 0.3778]
-Original Grad: -0.039, -lr * Pred Grad:  0.044, New P: 1.130
-Original Grad: 0.073, -lr * Pred Grad:  0.030, New P: 0.408
iter 7 loss: 0.301
Actual params: [1.1298, 0.408 ]
-Original Grad: -0.022, -lr * Pred Grad:  0.037, New P: 1.167
-Original Grad: 0.077, -lr * Pred Grad:  0.039, New P: 0.447
iter 8 loss: 0.295
Actual params: [1.1672, 0.4473]
-Original Grad: 0.006, -lr * Pred Grad:  0.034, New P: 1.201
-Original Grad: 0.057, -lr * Pred Grad:  0.044, New P: 0.491
iter 9 loss: 0.286
Actual params: [1.2009, 0.4911]
-Original Grad: -0.003, -lr * Pred Grad:  0.030, New P: 1.231
-Original Grad: 0.015, -lr * Pred Grad:  0.042, New P: 0.533
iter 10 loss: 0.271
Actual params: [1.2307, 0.5327]
-Original Grad: -0.004, -lr * Pred Grad:  0.026, New P: 1.257
-Original Grad: 0.009, -lr * Pred Grad:  0.039, New P: 0.572
iter 11 loss: 0.260
Actual params: [1.2571, 0.5715]
-Original Grad: 0.042, -lr * Pred Grad:  0.027, New P: 1.284
-Original Grad: -0.181, -lr * Pred Grad:  0.004, New P: 0.575
iter 12 loss: 0.277
Actual params: [1.2842, 0.5753]
-Original Grad: 0.008, -lr * Pred Grad:  0.025, New P: 1.309
-Original Grad: -0.072, -lr * Pred Grad:  -0.007, New P: 0.569
iter 13 loss: 0.320
Actual params: [1.3092, 0.5686]
-Original Grad: -0.046, -lr * Pred Grad:  0.019, New P: 1.328
-Original Grad: -0.124, -lr * Pred Grad:  -0.022, New P: 0.547
iter 14 loss: 0.386
Actual params: [1.328 , 0.5465]
-Original Grad: -0.062, -lr * Pred Grad:  0.012, New P: 1.340
-Original Grad: -0.038, -lr * Pred Grad:  -0.025, New P: 0.522
iter 15 loss: 0.437
Actual params: [1.3399, 0.5218]
-Original Grad: -0.196, -lr * Pred Grad:  -0.005, New P: 1.335
-Original Grad: 0.076, -lr * Pred Grad:  -0.012, New P: 0.510
iter 16 loss: 0.441
Actual params: [1.3349, 0.5096]
-Original Grad: -0.123, -lr * Pred Grad:  -0.014, New P: 1.321
-Original Grad: 0.095, -lr * Pred Grad:  0.001, New P: 0.511
iter 17 loss: 0.416
Actual params: [1.3208, 0.511 ]
-Original Grad: -0.121, -lr * Pred Grad:  -0.022, New P: 1.299
-Original Grad: 0.047, -lr * Pred Grad:  0.007, New P: 0.518
iter 18 loss: 0.365
Actual params: [1.299 , 0.5182]
-Original Grad: -0.106, -lr * Pred Grad:  -0.028, New P: 1.271
-Original Grad: 0.040, -lr * Pred Grad:  0.012, New P: 0.530
iter 19 loss: 0.306
Actual params: [1.2714, 0.5297]
-Original Grad: -0.017, -lr * Pred Grad:  -0.026, New P: 1.245
-Original Grad: -0.030, -lr * Pred Grad:  0.007, New P: 0.536
iter 20 loss: 0.277
Actual params: [1.2449, 0.5364]
-Original Grad: -0.008, -lr * Pred Grad:  -0.025, New P: 1.220
-Original Grad: 0.046, -lr * Pred Grad:  0.012, New P: 0.548
Target params: [1.1812, 0.2779]
iter 0 loss: 0.333
Actual params: [0.5941, 0.5941]
-Original Grad: 0.084, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.017, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.402
Actual params: [0.6941, 0.6941]
-Original Grad: 0.029, -lr * Pred Grad:  0.087, New P: 0.781
-Original Grad: 0.013, -lr * Pred Grad:  0.098, New P: 0.793
iter 2 loss: 0.594
Actual params: [0.7815, 0.7925]
-Original Grad: -0.010, -lr * Pred Grad:  0.060, New P: 0.841
-Original Grad: -0.048, -lr * Pred Grad:  -0.028, New P: 0.765
iter 3 loss: 0.661
Actual params: [0.8413, 0.7649]
-Original Grad: -0.142, -lr * Pred Grad:  -0.023, New P: 0.818
-Original Grad: -0.061, -lr * Pred Grad:  -0.059, New P: 0.706
iter 4 loss: 0.583
Actual params: [0.8183, 0.7061]
-Original Grad: -0.113, -lr * Pred Grad:  -0.047, New P: 0.772
-Original Grad: -0.036, -lr * Pred Grad:  -0.068, New P: 0.639
iter 5 loss: 0.449
Actual params: [0.7717, 0.6385]
-Original Grad: -0.068, -lr * Pred Grad:  -0.055, New P: 0.717
-Original Grad: -0.006, -lr * Pred Grad:  -0.062, New P: 0.577
iter 6 loss: 0.357
Actual params: [0.717 , 0.5768]
-Original Grad: 0.014, -lr * Pred Grad:  -0.044, New P: 0.673
-Original Grad: 0.001, -lr * Pred Grad:  -0.053, New P: 0.524
iter 7 loss: 0.330
Actual params: [0.6727, 0.5236]
-Original Grad: 0.057, -lr * Pred Grad:  -0.025, New P: 0.648
-Original Grad: 0.006, -lr * Pred Grad:  -0.043, New P: 0.480
iter 8 loss: 0.323
Actual params: [0.6477, 0.4805]
-Original Grad: 0.066, -lr * Pred Grad:  -0.007, New P: 0.640
-Original Grad: 0.006, -lr * Pred Grad:  -0.035, New P: 0.446
iter 9 loss: 0.320
Actual params: [0.6404, 0.4455]
-Original Grad: 0.053, -lr * Pred Grad:  0.004, New P: 0.645
-Original Grad: 0.001, -lr * Pred Grad:  -0.030, New P: 0.415
iter 10 loss: 0.318
Actual params: [0.6449, 0.4151]
-Original Grad: 0.060, -lr * Pred Grad:  0.016, New P: 0.661
-Original Grad: 0.000, -lr * Pred Grad:  -0.027, New P: 0.388
iter 11 loss: 0.316
Actual params: [0.6607, 0.3879]
-Original Grad: 0.044, -lr * Pred Grad:  0.022, New P: 0.683
-Original Grad: -0.003, -lr * Pred Grad:  -0.026, New P: 0.362
iter 12 loss: 0.314
Actual params: [0.6831, 0.3621]
-Original Grad: 0.035, -lr * Pred Grad:  0.027, New P: 0.710
-Original Grad: -0.013, -lr * Pred Grad:  -0.030, New P: 0.332
iter 13 loss: 0.314
Actual params: [0.7099, 0.332 ]
-Original Grad: 0.041, -lr * Pred Grad:  0.032, New P: 0.742
-Original Grad: -0.008, -lr * Pred Grad:  -0.031, New P: 0.301
iter 14 loss: 0.315
Actual params: [0.7416, 0.3006]
-Original Grad: 0.053, -lr * Pred Grad:  0.038, New P: 0.780
-Original Grad: -0.008, -lr * Pred Grad:  -0.032, New P: 0.268
iter 15 loss: 0.316
Actual params: [0.7796, 0.2681]
-Original Grad: 0.014, -lr * Pred Grad:  0.037, New P: 0.817
-Original Grad: -0.025, -lr * Pred Grad:  -0.041, New P: 0.227
iter 16 loss: 0.317
Actual params: [0.8167, 0.2269]
-Original Grad: -0.010, -lr * Pred Grad:  0.032, New P: 0.848
-Original Grad: -0.028, -lr * Pred Grad:  -0.050, New P: 0.177
iter 17 loss: 0.316
Actual params: [0.8484, 0.1768]
-Original Grad: 0.007, -lr * Pred Grad:  0.030, New P: 0.879
-Original Grad: -0.034, -lr * Pred Grad:  -0.059, New P: 0.117
iter 18 loss: 0.313
Actual params: [0.8785, 0.1175]
-Original Grad: -0.003, -lr * Pred Grad:  0.027, New P: 0.905
-Original Grad: -0.035, -lr * Pred Grad:  -0.067, New P: 0.050
iter 19 loss: 0.310
Actual params: [0.9053, 0.0505]
-Original Grad: -0.038, -lr * Pred Grad:  0.017, New P: 0.922
-Original Grad: -0.012, -lr * Pred Grad:  -0.066, New P: -0.016
iter 20 loss: 0.307
Actual params: [ 0.922 , -0.0155]
-Original Grad: -0.035, -lr * Pred Grad:  0.008, New P: 0.930
-Original Grad: -0.010, -lr * Pred Grad:  -0.064, New P: -0.080
Target params: [1.1812, 0.2779]
iter 0 loss: 0.324
Actual params: [0.5941, 0.5941]
-Original Grad: 0.338, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.142, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.247
Actual params: [0.6941, 0.4941]
-Original Grad: 0.089, -lr * Pred Grad:  0.084, New P: 0.778
-Original Grad: -0.007, -lr * Pred Grad:  -0.070, New P: 0.424
iter 2 loss: 0.254
Actual params: [0.7778, 0.4237]
-Original Grad: -0.071, -lr * Pred Grad:  0.051, New P: 0.829
-Original Grad: -0.010, -lr * Pred Grad:  -0.059, New P: 0.365
iter 3 loss: 0.250
Actual params: [0.8286, 0.3648]
-Original Grad: -0.056, -lr * Pred Grad:  0.032, New P: 0.861
-Original Grad: -0.013, -lr * Pred Grad:  -0.053, New P: 0.312
iter 4 loss: 0.240
Actual params: [0.8606, 0.3115]
-Original Grad: -0.010, -lr * Pred Grad:  0.026, New P: 0.886
-Original Grad: -0.002, -lr * Pred Grad:  -0.046, New P: 0.266
iter 5 loss: 0.233
Actual params: [0.8861, 0.2659]
-Original Grad: 0.003, -lr * Pred Grad:  0.022, New P: 0.909
-Original Grad: -0.006, -lr * Pred Grad:  -0.042, New P: 0.224
iter 6 loss: 0.224
Actual params: [0.9085, 0.2242]
-Original Grad: 0.020, -lr * Pred Grad:  0.022, New P: 0.931
-Original Grad: -0.005, -lr * Pred Grad:  -0.038, New P: 0.186
iter 7 loss: 0.221
Actual params: [0.9309, 0.1862]
-Original Grad: 0.014, -lr * Pred Grad:  0.022, New P: 0.953
-Original Grad: -0.009, -lr * Pred Grad:  -0.036, New P: 0.150
iter 8 loss: 0.219
Actual params: [0.9526, 0.1498]
-Original Grad: 0.020, -lr * Pred Grad:  0.022, New P: 0.974
-Original Grad: -0.021, -lr * Pred Grad:  -0.039, New P: 0.111
iter 9 loss: 0.219
Actual params: [0.9745, 0.1105]
-Original Grad: 0.013, -lr * Pred Grad:  0.021, New P: 0.996
-Original Grad: -0.001, -lr * Pred Grad:  -0.035, New P: 0.075
iter 10 loss: 0.214
Actual params: [0.9958, 0.0751]
-Original Grad: 0.015, -lr * Pred Grad:  0.021, New P: 1.017
-Original Grad: -0.012, -lr * Pred Grad:  -0.036, New P: 0.039
iter 11 loss: 0.217
Actual params: [1.0168, 0.0395]
-Original Grad: 0.009, -lr * Pred Grad:  0.020, New P: 1.037
-Original Grad: -0.002, -lr * Pred Grad:  -0.033, New P: 0.007
iter 12 loss: 0.220
Actual params: [1.0369, 0.0069]
-Original Grad: 0.005, -lr * Pred Grad:  0.019, New P: 1.056
-Original Grad: 0.006, -lr * Pred Grad:  -0.027, New P: -0.020
iter 13 loss: 0.222
Actual params: [ 1.0557, -0.0205]
-Original Grad: 0.002, -lr * Pred Grad:  0.017, New P: 1.073
-Original Grad: 0.001, -lr * Pred Grad:  -0.024, New P: -0.045
iter 14 loss: 0.225
Actual params: [ 1.073 , -0.0448]
-Original Grad: -0.008, -lr * Pred Grad:  0.015, New P: 1.088
-Original Grad: 0.009, -lr * Pred Grad:  -0.019, New P: -0.064
iter 15 loss: 0.228
Actual params: [ 1.0875, -0.0638]
-Original Grad: -0.011, -lr * Pred Grad:  0.012, New P: 1.099
-Original Grad: 0.006, -lr * Pred Grad:  -0.015, New P: -0.079
iter 16 loss: 0.233
Actual params: [ 1.0992, -0.079 ]
-Original Grad: -0.020, -lr * Pred Grad:  0.008, New P: 1.107
-Original Grad: 0.017, -lr * Pred Grad:  -0.008, New P: -0.087
iter 17 loss: 0.235
Actual params: [ 1.1071, -0.0869]
-Original Grad: -0.017, -lr * Pred Grad:  0.005, New P: 1.112
-Original Grad: 0.012, -lr * Pred Grad:  -0.003, New P: -0.090
iter 18 loss: 0.235
Actual params: [ 1.1118, -0.0901]
-Original Grad: -0.007, -lr * Pred Grad:  0.003, New P: 1.115
-Original Grad: 0.015, -lr * Pred Grad:  0.002, New P: -0.088
iter 19 loss: 0.235
Actual params: [ 1.1152, -0.0878]
-Original Grad: -0.019, -lr * Pred Grad:  0.000, New P: 1.116
-Original Grad: 0.010, -lr * Pred Grad:  0.005, New P: -0.082
iter 20 loss: 0.235
Actual params: [ 1.1156, -0.0823]
-Original Grad: -0.023, -lr * Pred Grad:  -0.003, New P: 1.113
-Original Grad: 0.014, -lr * Pred Grad:  0.010, New P: -0.072
Target params: [1.1812, 0.2779]
iter 0 loss: 0.541
Actual params: [0.5941, 0.5941]
-Original Grad: 0.479, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.330, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.467
Actual params: [0.6941, 0.4941]
-Original Grad: 0.774, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.227, -lr * Pred Grad:  -0.097, New P: 0.397
iter 2 loss: 0.396
Actual params: [0.7926, 0.3967]
-Original Grad: 0.442, -lr * Pred Grad:  0.096, New P: 0.889
-Original Grad: -0.063, -lr * Pred Grad:  -0.084, New P: 0.312
iter 3 loss: 0.301
Actual params: [0.889 , 0.3124]
-Original Grad: 0.121, -lr * Pred Grad:  0.085, New P: 0.974
-Original Grad: 0.004, -lr * Pred Grad:  -0.068, New P: 0.244
iter 4 loss: 0.235
Actual params: [0.9744, 0.2439]
-Original Grad: 0.024, -lr * Pred Grad:  0.073, New P: 1.048
-Original Grad: 0.028, -lr * Pred Grad:  -0.054, New P: 0.190
iter 5 loss: 0.211
Actual params: [1.0478, 0.19  ]
-Original Grad: 0.012, -lr * Pred Grad:  0.064, New P: 1.112
-Original Grad: 0.034, -lr * Pred Grad:  -0.042, New P: 0.148
iter 6 loss: 0.198
Actual params: [1.1117, 0.1481]
-Original Grad: 0.021, -lr * Pred Grad:  0.057, New P: 1.169
-Original Grad: 0.046, -lr * Pred Grad:  -0.031, New P: 0.117
iter 7 loss: 0.188
Actual params: [1.1685, 0.1175]
-Original Grad: 0.013, -lr * Pred Grad:  0.051, New P: 1.219
-Original Grad: 0.049, -lr * Pred Grad:  -0.021, New P: 0.097
iter 8 loss: 0.179
Actual params: [1.2192, 0.0966]
-Original Grad: -0.010, -lr * Pred Grad:  0.045, New P: 1.264
-Original Grad: 0.066, -lr * Pred Grad:  -0.011, New P: 0.086
iter 9 loss: 0.174
Actual params: [1.2638, 0.0859]
-Original Grad: 0.027, -lr * Pred Grad:  0.041, New P: 1.305
-Original Grad: 0.058, -lr * Pred Grad:  -0.003, New P: 0.083
iter 10 loss: 0.171
Actual params: [1.3047, 0.0832]
-Original Grad: -0.043, -lr * Pred Grad:  0.035, New P: 1.339
-Original Grad: 0.078, -lr * Pred Grad:  0.006, New P: 0.090
iter 11 loss: 0.162
Actual params: [1.3394, 0.0896]
-Original Grad: -0.053, -lr * Pred Grad:  0.029, New P: 1.368
-Original Grad: 0.078, -lr * Pred Grad:  0.014, New P: 0.104
iter 12 loss: 0.162
Actual params: [1.368 , 0.1039]
-Original Grad: -0.057, -lr * Pred Grad:  0.023, New P: 1.391
-Original Grad: 0.078, -lr * Pred Grad:  0.021, New P: 0.125
iter 13 loss: 0.162
Actual params: [1.391 , 0.1251]
-Original Grad: -0.045, -lr * Pred Grad:  0.019, New P: 1.410
-Original Grad: 0.079, -lr * Pred Grad:  0.027, New P: 0.152
iter 14 loss: 0.161
Actual params: [1.4097, 0.1525]
-Original Grad: -0.057, -lr * Pred Grad:  0.014, New P: 1.424
-Original Grad: 0.078, -lr * Pred Grad:  0.033, New P: 0.185
iter 15 loss: 0.160
Actual params: [1.4238, 0.1852]
-Original Grad: -0.040, -lr * Pred Grad:  0.011, New P: 1.435
-Original Grad: 0.080, -lr * Pred Grad:  0.038, New P: 0.223
iter 16 loss: 0.159
Actual params: [1.4347, 0.2228]
-Original Grad: -0.035, -lr * Pred Grad:  0.008, New P: 1.443
-Original Grad: 0.042, -lr * Pred Grad:  0.038, New P: 0.261
iter 17 loss: 0.157
Actual params: [1.4429, 0.2612]
-Original Grad: -0.024, -lr * Pred Grad:  0.006, New P: 1.449
-Original Grad: 0.054, -lr * Pred Grad:  0.040, New P: 0.302
iter 18 loss: 0.154
Actual params: [1.4491, 0.3017]
-Original Grad: -0.025, -lr * Pred Grad:  0.004, New P: 1.454
-Original Grad: 0.042, -lr * Pred Grad:  0.041, New P: 0.343
iter 19 loss: 0.146
Actual params: [1.4535, 0.3429]
-Original Grad: -0.034, -lr * Pred Grad:  0.002, New P: 1.456
-Original Grad: 0.034, -lr * Pred Grad:  0.041, New P: 0.384
iter 20 loss: 0.144
Actual params: [1.4559, 0.3839]
-Original Grad: -0.029, -lr * Pred Grad:  0.001, New P: 1.457
-Original Grad: 0.024, -lr * Pred Grad:  0.040, New P: 0.424
Target params: [1.1812, 0.2779]
iter 0 loss: 0.743
Actual params: [0.5941, 0.5941]
-Original Grad: 0.068, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.622, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.390
Actual params: [0.6941, 0.4941]
-Original Grad: 0.067, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.236, -lr * Pred Grad:  -0.089, New P: 0.405
iter 2 loss: 0.282
Actual params: [0.794, 0.405]
-Original Grad: 0.027, -lr * Pred Grad:  0.092, New P: 0.886
-Original Grad: -0.050, -lr * Pred Grad:  -0.073, New P: 0.332
iter 3 loss: 0.257
Actual params: [0.8856, 0.3315]
-Original Grad: 0.005, -lr * Pred Grad:  0.078, New P: 0.963
-Original Grad: -0.012, -lr * Pred Grad:  -0.061, New P: 0.270
iter 4 loss: 0.250
Actual params: [0.9634, 0.2703]
-Original Grad: 0.014, -lr * Pred Grad:  0.073, New P: 1.036
-Original Grad: -0.007, -lr * Pred Grad:  -0.052, New P: 0.218
iter 5 loss: 0.243
Actual params: [1.0362, 0.218 ]
-Original Grad: 0.011, -lr * Pred Grad:  0.068, New P: 1.104
-Original Grad: -0.001, -lr * Pred Grad:  -0.045, New P: 0.173
iter 6 loss: 0.239
Actual params: [1.1042, 0.1728]
-Original Grad: 0.003, -lr * Pred Grad:  0.061, New P: 1.165
-Original Grad: 0.001, -lr * Pred Grad:  -0.039, New P: 0.133
iter 7 loss: 0.235
Actual params: [1.165 , 0.1334]
-Original Grad: -0.000, -lr * Pred Grad:  0.054, New P: 1.219
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: 0.099
iter 8 loss: 0.231
Actual params: [1.2186, 0.0986]
-Original Grad: -0.000, -lr * Pred Grad:  0.047, New P: 1.266
-Original Grad: 0.006, -lr * Pred Grad:  -0.030, New P: 0.068
iter 9 loss: 0.228
Actual params: [1.266 , 0.0683]
-Original Grad: 0.004, -lr * Pred Grad:  0.044, New P: 1.310
-Original Grad: 0.026, -lr * Pred Grad:  -0.025, New P: 0.043
iter 10 loss: 0.225
Actual params: [1.3102, 0.0431]
-Original Grad: 0.002, -lr * Pred Grad:  0.041, New P: 1.351
-Original Grad: 0.016, -lr * Pred Grad:  -0.021, New P: 0.022
iter 11 loss: 0.223
Actual params: [1.3509, 0.0217]
-Original Grad: 0.002, -lr * Pred Grad:  0.037, New P: 1.388
-Original Grad: 0.012, -lr * Pred Grad:  -0.018, New P: 0.003
iter 12 loss: 0.221
Actual params: [1.3883, 0.0033]
-Original Grad: 0.000, -lr * Pred Grad:  0.034, New P: 1.422
-Original Grad: 0.007, -lr * Pred Grad:  -0.016, New P: -0.013
iter 13 loss: 0.220
Actual params: [ 1.4223, -0.0128]
-Original Grad: 0.003, -lr * Pred Grad:  0.032, New P: 1.454
-Original Grad: 0.021, -lr * Pred Grad:  -0.013, New P: -0.026
iter 14 loss: 0.219
Actual params: [ 1.4542, -0.0258]
-Original Grad: -0.003, -lr * Pred Grad:  0.027, New P: 1.481
-Original Grad: 0.008, -lr * Pred Grad:  -0.011, New P: -0.037
iter 15 loss: 0.219
Actual params: [ 1.4814, -0.0369]
-Original Grad: -0.003, -lr * Pred Grad:  0.023, New P: 1.504
-Original Grad: 0.009, -lr * Pred Grad:  -0.009, New P: -0.046
iter 16 loss: 0.218
Actual params: [ 1.5044, -0.0464]
-Original Grad: -0.002, -lr * Pred Grad:  0.020, New P: 1.524
-Original Grad: 0.014, -lr * Pred Grad:  -0.008, New P: -0.054
iter 17 loss: 0.218
Actual params: [ 1.5241, -0.054 ]
-Original Grad: -0.002, -lr * Pred Grad:  0.017, New P: 1.541
-Original Grad: 0.018, -lr * Pred Grad:  -0.006, New P: -0.059
iter 18 loss: 0.218
Actual params: [ 1.5412, -0.0595]
-Original Grad: -0.001, -lr * Pred Grad:  0.015, New P: 1.556
-Original Grad: 0.017, -lr * Pred Grad:  -0.004, New P: -0.063
iter 19 loss: 0.219
Actual params: [ 1.5561, -0.0632]
-Original Grad: -0.002, -lr * Pred Grad:  0.013, New P: 1.569
-Original Grad: 0.022, -lr * Pred Grad:  -0.002, New P: -0.065
iter 20 loss: 0.219
Actual params: [ 1.5687, -0.0648]
-Original Grad: -0.003, -lr * Pred Grad:  0.010, New P: 1.579
-Original Grad: 0.018, -lr * Pred Grad:  -0.000, New P: -0.065
Target params: [1.1812, 0.2779]
iter 0 loss: 0.288
Actual params: [0.5941, 0.5941]
-Original Grad: 0.381, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.223, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.250
Actual params: [0.6941, 0.4941]
-Original Grad: 0.060, -lr * Pred Grad:  0.078, New P: 0.772
-Original Grad: -0.055, -lr * Pred Grad:  -0.083, New P: 0.411
iter 2 loss: 0.242
Actual params: [0.7718, 0.4111]
-Original Grad: -0.009, -lr * Pred Grad:  0.059, New P: 0.830
-Original Grad: 0.017, -lr * Pred Grad:  -0.059, New P: 0.352
iter 3 loss: 0.240
Actual params: [0.8303, 0.352 ]
-Original Grad: 0.004, -lr * Pred Grad:  0.049, New P: 0.879
-Original Grad: 0.018, -lr * Pred Grad:  -0.044, New P: 0.308
iter 4 loss: 0.240
Actual params: [0.879 , 0.3081]
-Original Grad: 0.012, -lr * Pred Grad:  0.043, New P: 0.922
-Original Grad: 0.033, -lr * Pred Grad:  -0.029, New P: 0.279
iter 5 loss: 0.240
Actual params: [0.9218, 0.279 ]
-Original Grad: 0.014, -lr * Pred Grad:  0.039, New P: 0.960
-Original Grad: 0.013, -lr * Pred Grad:  -0.022, New P: 0.257
iter 6 loss: 0.239
Actual params: [0.9604, 0.2569]
-Original Grad: 0.025, -lr * Pred Grad:  0.037, New P: 0.997
-Original Grad: 0.025, -lr * Pred Grad:  -0.014, New P: 0.243
iter 7 loss: 0.239
Actual params: [0.9974, 0.243 ]
-Original Grad: 0.028, -lr * Pred Grad:  0.036, New P: 1.033
-Original Grad: 0.027, -lr * Pred Grad:  -0.006, New P: 0.237
iter 8 loss: 0.240
Actual params: [1.0335, 0.2366]
-Original Grad: 0.026, -lr * Pred Grad:  0.035, New P: 1.069
-Original Grad: 0.026, -lr * Pred Grad:  -0.000, New P: 0.236
iter 9 loss: 0.240
Actual params: [1.0687, 0.2362]
-Original Grad: 0.018, -lr * Pred Grad:  0.034, New P: 1.102
-Original Grad: 0.010, -lr * Pred Grad:  0.002, New P: 0.238
iter 10 loss: 0.242
Actual params: [1.1023, 0.238 ]
-Original Grad: 0.015, -lr * Pred Grad:  0.032, New P: 1.134
-Original Grad: 0.015, -lr * Pred Grad:  0.005, New P: 0.243
iter 11 loss: 0.243
Actual params: [1.1342, 0.2425]
-Original Grad: 0.011, -lr * Pred Grad:  0.030, New P: 1.164
-Original Grad: 0.014, -lr * Pred Grad:  0.007, New P: 0.250
iter 12 loss: 0.244
Actual params: [1.1643, 0.2495]
-Original Grad: 0.011, -lr * Pred Grad:  0.028, New P: 1.193
-Original Grad: 0.018, -lr * Pred Grad:  0.010, New P: 0.259
iter 13 loss: 0.246
Actual params: [1.1927, 0.2594]
-Original Grad: 0.007, -lr * Pred Grad:  0.026, New P: 1.219
-Original Grad: 0.018, -lr * Pred Grad:  0.013, New P: 0.272
iter 14 loss: 0.248
Actual params: [1.2192, 0.272 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.024, New P: 1.243
-Original Grad: 0.005, -lr * Pred Grad:  0.012, New P: 0.284
iter 15 loss: 0.248
Actual params: [1.2435, 0.2843]
-Original Grad: -0.005, -lr * Pred Grad:  0.021, New P: 1.265
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.296
iter 16 loss: 0.248
Actual params: [1.2648, 0.2956]
-Original Grad: -0.009, -lr * Pred Grad:  0.018, New P: 1.283
-Original Grad: -0.016, -lr * Pred Grad:  0.007, New P: 0.303
iter 17 loss: 0.248
Actual params: [1.2831, 0.3026]
-Original Grad: -0.007, -lr * Pred Grad:  0.016, New P: 1.299
-Original Grad: -0.008, -lr * Pred Grad:  0.005, New P: 0.307
iter 18 loss: 0.249
Actual params: [1.2988, 0.3073]
-Original Grad: -0.007, -lr * Pred Grad:  0.013, New P: 1.312
-Original Grad: -0.005, -lr * Pred Grad:  0.003, New P: 0.311
iter 19 loss: 0.250
Actual params: [1.3121, 0.3105]
-Original Grad: -0.009, -lr * Pred Grad:  0.011, New P: 1.323
-Original Grad: -0.008, -lr * Pred Grad:  0.001, New P: 0.312
iter 20 loss: 0.250
Actual params: [1.3231, 0.3117]
-Original Grad: -0.009, -lr * Pred Grad:  0.009, New P: 1.332
-Original Grad: -0.017, -lr * Pred Grad:  -0.003, New P: 0.309
Target params: [1.1812, 0.2779]
iter 0 loss: 0.718
Actual params: [0.5941, 0.5941]
-Original Grad: 0.276, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.340, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.527
Actual params: [0.6941, 0.4941]
-Original Grad: 0.180, -lr * Pred Grad:  0.097, New P: 0.791
-Original Grad: -0.194, -lr * Pred Grad:  -0.095, New P: 0.399
iter 2 loss: 0.415
Actual params: [0.7908, 0.399 ]
-Original Grad: 0.026, -lr * Pred Grad:  0.080, New P: 0.870
-Original Grad: 0.012, -lr * Pred Grad:  -0.072, New P: 0.327
iter 3 loss: 0.378
Actual params: [0.8704, 0.3274]
-Original Grad: 0.015, -lr * Pred Grad:  0.068, New P: 0.938
-Original Grad: 0.052, -lr * Pred Grad:  -0.050, New P: 0.277
iter 4 loss: 0.368
Actual params: [0.9381, 0.277 ]
-Original Grad: 0.030, -lr * Pred Grad:  0.062, New P: 1.000
-Original Grad: 0.077, -lr * Pred Grad:  -0.031, New P: 0.246
iter 5 loss: 0.367
Actual params: [1.    , 0.2455]
-Original Grad: 0.026, -lr * Pred Grad:  0.057, New P: 1.057
-Original Grad: 0.040, -lr * Pred Grad:  -0.022, New P: 0.224
iter 6 loss: 0.367
Actual params: [1.0574, 0.2237]
-Original Grad: 0.034, -lr * Pred Grad:  0.055, New P: 1.112
-Original Grad: 0.085, -lr * Pred Grad:  -0.008, New P: 0.215
iter 7 loss: 0.372
Actual params: [1.1123, 0.2154]
-Original Grad: 0.024, -lr * Pred Grad:  0.052, New P: 1.164
-Original Grad: 0.037, -lr * Pred Grad:  -0.003, New P: 0.213
iter 8 loss: 0.378
Actual params: [1.1642, 0.2125]
-Original Grad: 0.017, -lr * Pred Grad:  0.048, New P: 1.213
-Original Grad: 0.018, -lr * Pred Grad:  -0.000, New P: 0.212
iter 9 loss: 0.381
Actual params: [1.2127, 0.2122]
-Original Grad: 0.015, -lr * Pred Grad:  0.045, New P: 1.258
-Original Grad: 0.046, -lr * Pred Grad:  0.005, New P: 0.217
iter 10 loss: 0.387
Actual params: [1.2581, 0.2172]
-Original Grad: 0.012, -lr * Pred Grad:  0.042, New P: 1.300
-Original Grad: 0.057, -lr * Pred Grad:  0.011, New P: 0.228
iter 11 loss: 0.396
Actual params: [1.3005, 0.2281]
-Original Grad: 0.004, -lr * Pred Grad:  0.039, New P: 1.339
-Original Grad: 0.024, -lr * Pred Grad:  0.013, New P: 0.241
iter 12 loss: 0.404
Actual params: [1.3392, 0.2407]
-Original Grad: 0.002, -lr * Pred Grad:  0.035, New P: 1.374
-Original Grad: 0.042, -lr * Pred Grad:  0.016, New P: 0.257
iter 13 loss: 0.409
Actual params: [1.3743, 0.2569]
-Original Grad: -0.003, -lr * Pred Grad:  0.031, New P: 1.406
-Original Grad: 0.033, -lr * Pred Grad:  0.018, New P: 0.275
iter 14 loss: 0.419
Actual params: [1.4056, 0.2752]
-Original Grad: -0.006, -lr * Pred Grad:  0.027, New P: 1.433
-Original Grad: 0.026, -lr * Pred Grad:  0.020, New P: 0.295
iter 15 loss: 0.431
Actual params: [1.433 , 0.2947]
-Original Grad: -0.007, -lr * Pred Grad:  0.024, New P: 1.457
-Original Grad: 0.016, -lr * Pred Grad:  0.019, New P: 0.314
iter 16 loss: 0.442
Actual params: [1.4568, 0.3142]
-Original Grad: -0.008, -lr * Pred Grad:  0.020, New P: 1.477
-Original Grad: 0.004, -lr * Pred Grad:  0.018, New P: 0.332
iter 17 loss: 0.451
Actual params: [1.4772, 0.3323]
-Original Grad: -0.013, -lr * Pred Grad:  0.017, New P: 1.494
-Original Grad: -0.016, -lr * Pred Grad:  0.015, New P: 0.347
iter 18 loss: 0.465
Actual params: [1.4937, 0.3469]
-Original Grad: -0.015, -lr * Pred Grad:  0.013, New P: 1.506
-Original Grad: -0.031, -lr * Pred Grad:  0.010, New P: 0.356
iter 19 loss: 0.475
Actual params: [1.5064, 0.3564]
-Original Grad: -0.018, -lr * Pred Grad:  0.009, New P: 1.515
-Original Grad: -0.030, -lr * Pred Grad:  0.005, New P: 0.362
iter 20 loss: 0.481
Actual params: [1.5152, 0.3616]
-Original Grad: -0.020, -lr * Pred Grad:  0.005, New P: 1.520
-Original Grad: -0.032, -lr * Pred Grad:  0.001, New P: 0.362
Target params: [1.1812, 0.2779]
iter 0 loss: 0.727
Actual params: [0.5941, 0.5941]
-Original Grad: 0.648, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.038, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.708
Actual params: [0.6941, 0.4941]
-Original Grad: 0.524, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: 0.077, -lr * Pred Grad:  0.037, New P: 0.531
iter 2 loss: 0.670
Actual params: [0.793 , 0.5312]
-Original Grad: 0.358, -lr * Pred Grad:  0.095, New P: 0.888
-Original Grad: -0.037, -lr * Pred Grad:  0.001, New P: 0.532
iter 3 loss: 0.622
Actual params: [0.8884, 0.5321]
-Original Grad: 0.275, -lr * Pred Grad:  0.092, New P: 0.980
-Original Grad: -0.100, -lr * Pred Grad:  -0.042, New P: 0.490
iter 4 loss: 0.574
Actual params: [0.9801, 0.49  ]
-Original Grad: 0.197, -lr * Pred Grad:  0.087, New P: 1.067
-Original Grad: -0.050, -lr * Pred Grad:  -0.052, New P: 0.438
iter 5 loss: 0.523
Actual params: [1.0672, 0.4379]
-Original Grad: 0.053, -lr * Pred Grad:  0.078, New P: 1.145
-Original Grad: 0.030, -lr * Pred Grad:  -0.034, New P: 0.404
iter 6 loss: 0.459
Actual params: [1.1449, 0.4042]
-Original Grad: -0.204, -lr * Pred Grad:  0.056, New P: 1.201
-Original Grad: 0.209, -lr * Pred Grad:  0.024, New P: 0.428
iter 7 loss: 0.406
Actual params: [1.201 , 0.4285]
-Original Grad: 0.010, -lr * Pred Grad:  0.050, New P: 1.251
-Original Grad: 0.120, -lr * Pred Grad:  0.040, New P: 0.469
iter 8 loss: 0.350
Actual params: [1.2508, 0.4689]
-Original Grad: 0.001, -lr * Pred Grad:  0.044, New P: 1.295
-Original Grad: -0.000, -lr * Pred Grad:  0.036, New P: 0.505
iter 9 loss: 0.425
Actual params: [1.2951, 0.5047]
-Original Grad: 0.015, -lr * Pred Grad:  0.040, New P: 1.335
-Original Grad: -0.014, -lr * Pred Grad:  0.030, New P: 0.534
iter 10 loss: 0.461
Actual params: [1.3354, 0.5343]
-Original Grad: 0.032, -lr * Pred Grad:  0.038, New P: 1.373
-Original Grad: -0.074, -lr * Pred Grad:  0.013, New P: 0.548
iter 11 loss: 0.482
Actual params: [1.373 , 0.5477]
-Original Grad: 0.084, -lr * Pred Grad:  0.038, New P: 1.411
-Original Grad: -0.147, -lr * Pred Grad:  -0.011, New P: 0.537
iter 12 loss: 0.482
Actual params: [1.4108, 0.5369]
-Original Grad: 0.046, -lr * Pred Grad:  0.036, New P: 1.447
-Original Grad: -0.097, -lr * Pred Grad:  -0.023, New P: 0.514
iter 13 loss: 0.474
Actual params: [1.4471, 0.5139]
-Original Grad: 0.056, -lr * Pred Grad:  0.035, New P: 1.483
-Original Grad: -0.021, -lr * Pred Grad:  -0.024, New P: 0.490
iter 14 loss: 0.467
Actual params: [1.4825, 0.49  ]
-Original Grad: 0.038, -lr * Pred Grad:  0.034, New P: 1.516
-Original Grad: -0.030, -lr * Pred Grad:  -0.026, New P: 0.464
iter 15 loss: 0.458
Actual params: [1.5164, 0.4642]
-Original Grad: 0.049, -lr * Pred Grad:  0.033, New P: 1.550
-Original Grad: 0.053, -lr * Pred Grad:  -0.016, New P: 0.449
iter 16 loss: 0.457
Actual params: [1.5495, 0.4486]
-Original Grad: 0.018, -lr * Pred Grad:  0.031, New P: 1.580
-Original Grad: 0.032, -lr * Pred Grad:  -0.010, New P: 0.439
iter 17 loss: 0.459
Actual params: [1.5805, 0.4391]
-Original Grad: 0.040, -lr * Pred Grad:  0.030, New P: 1.611
-Original Grad: 0.039, -lr * Pred Grad:  -0.003, New P: 0.436
iter 18 loss: 0.464
Actual params: [1.6105, 0.4361]
-Original Grad: 0.022, -lr * Pred Grad:  0.028, New P: 1.639
-Original Grad: 0.088, -lr * Pred Grad:  0.010, New P: 0.446
iter 19 loss: 0.476
Actual params: [1.6389, 0.4457]
-Original Grad: 0.014, -lr * Pred Grad:  0.027, New P: 1.665
-Original Grad: 0.037, -lr * Pred Grad:  0.014, New P: 0.460
iter 20 loss: 0.482
Actual params: [1.6655, 0.4596]
-Original Grad: 0.037, -lr * Pred Grad:  0.026, New P: 1.692
-Original Grad: 0.031, -lr * Pred Grad:  0.017, New P: 0.477
Target params: [1.1812, 0.2779]
iter 0 loss: 0.636
Actual params: [0.5941, 0.5941]
-Original Grad: 0.282, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.059, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.616
Actual params: [0.6941, 0.4941]
-Original Grad: 0.293, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: 0.018, -lr * Pred Grad:  -0.043, New P: 0.451
iter 2 loss: 0.530
Actual params: [0.7941, 0.4513]
-Original Grad: 0.462, -lr * Pred Grad:  0.099, New P: 0.893
-Original Grad: -0.013, -lr * Pred Grad:  -0.045, New P: 0.406
iter 3 loss: 0.441
Actual params: [0.8932, 0.4062]
-Original Grad: 0.387, -lr * Pred Grad:  0.100, New P: 0.993
-Original Grad: 0.090, -lr * Pred Grad:  0.026, New P: 0.432
iter 4 loss: 0.383
Actual params: [0.9928, 0.4324]
-Original Grad: 0.108, -lr * Pred Grad:  0.091, New P: 1.084
-Original Grad: -0.009, -lr * Pred Grad:  0.018, New P: 0.450
iter 5 loss: 0.350
Actual params: [1.0842, 0.45  ]
-Original Grad: 0.063, -lr * Pred Grad:  0.083, New P: 1.167
-Original Grad: -0.154, -lr * Pred Grad:  -0.034, New P: 0.416
iter 6 loss: 0.339
Actual params: [1.1671, 0.4163]
-Original Grad: 0.015, -lr * Pred Grad:  0.073, New P: 1.240
-Original Grad: -0.043, -lr * Pred Grad:  -0.040, New P: 0.376
iter 7 loss: 0.343
Actual params: [1.2405, 0.3763]
-Original Grad: -0.016, -lr * Pred Grad:  0.064, New P: 1.304
-Original Grad: 0.055, -lr * Pred Grad:  -0.020, New P: 0.356
iter 8 loss: 0.347
Actual params: [1.3041, 0.3559]
-Original Grad: -0.067, -lr * Pred Grad:  0.052, New P: 1.356
-Original Grad: 0.065, -lr * Pred Grad:  -0.002, New P: 0.354
iter 9 loss: 0.361
Actual params: [1.356 , 0.3538]
-Original Grad: -0.134, -lr * Pred Grad:  0.037, New P: 1.393
-Original Grad: 0.084, -lr * Pred Grad:  0.016, New P: 0.370
iter 10 loss: 0.375
Actual params: [1.3928, 0.3699]
-Original Grad: -0.164, -lr * Pred Grad:  0.022, New P: 1.415
-Original Grad: 0.098, -lr * Pred Grad:  0.032, New P: 0.402
iter 11 loss: 0.386
Actual params: [1.4148, 0.4022]
-Original Grad: -0.199, -lr * Pred Grad:  0.007, New P: 1.422
-Original Grad: 0.085, -lr * Pred Grad:  0.043, New P: 0.445
iter 12 loss: 0.383
Actual params: [1.4218, 0.4454]
-Original Grad: -0.199, -lr * Pred Grad:  -0.006, New P: 1.416
-Original Grad: 0.028, -lr * Pred Grad:  0.044, New P: 0.489
iter 13 loss: 0.385
Actual params: [1.4162, 0.4892]
-Original Grad: -0.230, -lr * Pred Grad:  -0.018, New P: 1.398
-Original Grad: -0.062, -lr * Pred Grad:  0.027, New P: 0.517
iter 14 loss: 0.381
Actual params: [1.3983, 0.5166]
-Original Grad: -0.156, -lr * Pred Grad:  -0.025, New P: 1.373
-Original Grad: -0.141, -lr * Pred Grad:  -0.000, New P: 0.516
iter 15 loss: 0.371
Actual params: [1.3735, 0.5162]
-Original Grad: -0.131, -lr * Pred Grad:  -0.030, New P: 1.344
-Original Grad: -0.180, -lr * Pred Grad:  -0.025, New P: 0.491
iter 16 loss: 0.364
Actual params: [1.3439, 0.491 ]
-Original Grad: -0.109, -lr * Pred Grad:  -0.033, New P: 1.311
-Original Grad: -0.117, -lr * Pred Grad:  -0.037, New P: 0.454
iter 17 loss: 0.353
Actual params: [1.3112, 0.4538]
-Original Grad: -0.022, -lr * Pred Grad:  -0.031, New P: 1.280
-Original Grad: -0.075, -lr * Pred Grad:  -0.043, New P: 0.411
iter 18 loss: 0.346
Actual params: [1.2803, 0.411 ]
-Original Grad: -0.017, -lr * Pred Grad:  -0.029, New P: 1.251
-Original Grad: -0.027, -lr * Pred Grad:  -0.042, New P: 0.369
iter 19 loss: 0.340
Actual params: [1.2512, 0.3686]
-Original Grad: -0.026, -lr * Pred Grad:  -0.028, New P: 1.223
-Original Grad: 0.056, -lr * Pred Grad:  -0.031, New P: 0.338
iter 20 loss: 0.342
Actual params: [1.2233, 0.3379]
-Original Grad: -0.002, -lr * Pred Grad:  -0.025, New P: 1.198
-Original Grad: 0.117, -lr * Pred Grad:  -0.012, New P: 0.326
Target params: [1.1812, 0.2779]
iter 0 loss: 0.473
Actual params: [0.5941, 0.5941]
-Original Grad: 0.255, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.309, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.404
Actual params: [0.6941, 0.4941]
-Original Grad: 0.211, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.273, -lr * Pred Grad:  -0.099, New P: 0.395
iter 2 loss: 0.339
Actual params: [0.7931, 0.3946]
-Original Grad: 0.200, -lr * Pred Grad:  0.099, New P: 0.892
-Original Grad: -0.202, -lr * Pred Grad:  -0.097, New P: 0.297
iter 3 loss: 0.305
Actual params: [0.8917, 0.2974]
-Original Grad: -0.031, -lr * Pred Grad:  0.076, New P: 0.968
-Original Grad: 0.063, -lr * Pred Grad:  -0.071, New P: 0.226
iter 4 loss: 0.296
Actual params: [0.9675, 0.2265]
-Original Grad: 0.047, -lr * Pred Grad:  0.070, New P: 1.038
-Original Grad: 0.085, -lr * Pred Grad:  -0.049, New P: 0.177
iter 5 loss: 0.295
Actual params: [1.0377, 0.1774]
-Original Grad: 0.084, -lr * Pred Grad:  0.070, New P: 1.108
-Original Grad: 0.069, -lr * Pred Grad:  -0.034, New P: 0.143
iter 6 loss: 0.298
Actual params: [1.1079, 0.1431]
-Original Grad: 0.042, -lr * Pred Grad:  0.066, New P: 1.174
-Original Grad: 0.074, -lr * Pred Grad:  -0.022, New P: 0.121
iter 7 loss: 0.299
Actual params: [1.1741, 0.1213]
-Original Grad: 0.022, -lr * Pred Grad:  0.061, New P: 1.235
-Original Grad: 0.036, -lr * Pred Grad:  -0.015, New P: 0.106
iter 8 loss: 0.302
Actual params: [1.2351, 0.1059]
-Original Grad: 0.020, -lr * Pred Grad:  0.057, New P: 1.292
-Original Grad: 0.036, -lr * Pred Grad:  -0.010, New P: 0.096
iter 9 loss: 0.303
Actual params: [1.2916, 0.0958]
-Original Grad: 0.023, -lr * Pred Grad:  0.053, New P: 1.345
-Original Grad: 0.060, -lr * Pred Grad:  -0.003, New P: 0.093
iter 10 loss: 0.304
Actual params: [1.3448, 0.0929]
-Original Grad: 0.023, -lr * Pred Grad:  0.050, New P: 1.395
-Original Grad: 0.048, -lr * Pred Grad:  0.002, New P: 0.095
iter 11 loss: 0.307
Actual params: [1.3952, 0.0951]
-Original Grad: 0.022, -lr * Pred Grad:  0.048, New P: 1.443
-Original Grad: 0.034, -lr * Pred Grad:  0.005, New P: 0.100
iter 12 loss: 0.310
Actual params: [1.443 , 0.1004]
-Original Grad: 0.022, -lr * Pred Grad:  0.046, New P: 1.489
-Original Grad: 0.066, -lr * Pred Grad:  0.011, New P: 0.112
iter 13 loss: 0.314
Actual params: [1.4888, 0.1117]
-Original Grad: 0.018, -lr * Pred Grad:  0.044, New P: 1.532
-Original Grad: 0.044, -lr * Pred Grad:  0.014, New P: 0.126
iter 14 loss: 0.312
Actual params: [1.5323, 0.126 ]
-Original Grad: 0.014, -lr * Pred Grad:  0.041, New P: 1.573
-Original Grad: 0.021, -lr * Pred Grad:  0.015, New P: 0.141
iter 15 loss: 0.316
Actual params: [1.5734, 0.1411]
-Original Grad: 0.012, -lr * Pred Grad:  0.039, New P: 1.612
-Original Grad: 0.012, -lr * Pred Grad:  0.015, New P: 0.156
iter 16 loss: 0.321
Actual params: [1.6121, 0.156 ]
-Original Grad: 0.005, -lr * Pred Grad:  0.036, New P: 1.648
-Original Grad: 0.005, -lr * Pred Grad:  0.014, New P: 0.170
iter 17 loss: 0.327
Actual params: [1.6478, 0.1699]
-Original Grad: 0.003, -lr * Pred Grad:  0.033, New P: 1.681
-Original Grad: -0.005, -lr * Pred Grad:  0.012, New P: 0.182
iter 18 loss: 0.332
Actual params: [1.6807, 0.1821]
-Original Grad: -0.005, -lr * Pred Grad:  0.029, New P: 1.710
-Original Grad: -0.025, -lr * Pred Grad:  0.008, New P: 0.191
iter 19 loss: 0.336
Actual params: [1.7101, 0.1906]
-Original Grad: -0.006, -lr * Pred Grad:  0.026, New P: 1.736
-Original Grad: -0.024, -lr * Pred Grad:  0.005, New P: 0.196
iter 20 loss: 0.331
Actual params: [1.736 , 0.1958]
-Original Grad: -0.009, -lr * Pred Grad:  0.022, New P: 1.758
-Original Grad: -0.031, -lr * Pred Grad:  0.002, New P: 0.197
Target params: [1.1812, 0.2779]
iter 0 loss: 0.459
Actual params: [0.5941, 0.5941]
-Original Grad: 0.303, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.357, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.313
Actual params: [0.6941, 0.4941]
-Original Grad: 0.225, -lr * Pred Grad:  0.098, New P: 0.792
-Original Grad: -0.267, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.231
Actual params: [0.7922, 0.3958]
-Original Grad: 0.030, -lr * Pred Grad:  0.081, New P: 0.873
-Original Grad: -0.028, -lr * Pred Grad:  -0.080, New P: 0.316
iter 3 loss: 0.190
Actual params: [0.8729, 0.3161]
-Original Grad: -0.021, -lr * Pred Grad:  0.063, New P: 0.936
-Original Grad: 0.014, -lr * Pred Grad:  -0.064, New P: 0.253
iter 4 loss: 0.176
Actual params: [0.9357, 0.2525]
-Original Grad: -0.007, -lr * Pred Grad:  0.052, New P: 0.988
-Original Grad: 0.006, -lr * Pred Grad:  -0.053, New P: 0.199
iter 5 loss: 0.173
Actual params: [0.9878, 0.1995]
-Original Grad: 0.043, -lr * Pred Grad:  0.051, New P: 1.038
-Original Grad: 0.018, -lr * Pred Grad:  -0.044, New P: 0.156
iter 6 loss: 0.167
Actual params: [1.0383, 0.1559]
-Original Grad: 0.023, -lr * Pred Grad:  0.047, New P: 1.085
-Original Grad: 0.018, -lr * Pred Grad:  -0.036, New P: 0.120
iter 7 loss: 0.167
Actual params: [1.0854, 0.1199]
-Original Grad: 0.024, -lr * Pred Grad:  0.045, New P: 1.130
-Original Grad: 0.036, -lr * Pred Grad:  -0.028, New P: 0.092
iter 8 loss: 0.167
Actual params: [1.1299, 0.0922]
-Original Grad: 0.025, -lr * Pred Grad:  0.043, New P: 1.173
-Original Grad: 0.011, -lr * Pred Grad:  -0.023, New P: 0.069
iter 9 loss: 0.168
Actual params: [1.1725, 0.0688]
-Original Grad: 0.017, -lr * Pred Grad:  0.040, New P: 1.213
-Original Grad: 0.042, -lr * Pred Grad:  -0.016, New P: 0.053
iter 10 loss: 0.175
Actual params: [1.2127, 0.0526]
-Original Grad: 0.022, -lr * Pred Grad:  0.039, New P: 1.251
-Original Grad: 0.036, -lr * Pred Grad:  -0.011, New P: 0.042
iter 11 loss: 0.183
Actual params: [1.2514, 0.0419]
-Original Grad: -0.001, -lr * Pred Grad:  0.035, New P: 1.286
-Original Grad: 0.032, -lr * Pred Grad:  -0.006, New P: 0.036
iter 12 loss: 0.189
Actual params: [1.2861, 0.0358]
-Original Grad: -0.019, -lr * Pred Grad:  0.029, New P: 1.315
-Original Grad: 0.005, -lr * Pred Grad:  -0.005, New P: 0.031
iter 13 loss: 0.193
Actual params: [1.3149, 0.0308]
-Original Grad: 0.004, -lr * Pred Grad:  0.027, New P: 1.341
-Original Grad: 0.019, -lr * Pred Grad:  -0.002, New P: 0.028
iter 14 loss: 0.196
Actual params: [1.3414, 0.0283]
-Original Grad: -0.002, -lr * Pred Grad:  0.024, New P: 1.365
-Original Grad: 0.016, -lr * Pred Grad:  -0.001, New P: 0.028
iter 15 loss: 0.199
Actual params: [1.3653, 0.0278]
-Original Grad: -0.007, -lr * Pred Grad:  0.021, New P: 1.386
-Original Grad: 0.003, -lr * Pred Grad:  -0.000, New P: 0.028
iter 16 loss: 0.201
Actual params: [1.3859, 0.0277]
-Original Grad: -0.006, -lr * Pred Grad:  0.018, New P: 1.404
-Original Grad: 0.016, -lr * Pred Grad:  0.002, New P: 0.029
iter 17 loss: 0.203
Actual params: [1.4039, 0.0293]
-Original Grad: -0.001, -lr * Pred Grad:  0.016, New P: 1.420
-Original Grad: 0.002, -lr * Pred Grad:  0.002, New P: 0.031
iter 18 loss: 0.205
Actual params: [1.4201, 0.031 ]
-Original Grad: 0.007, -lr * Pred Grad:  0.016, New P: 1.436
-Original Grad: 0.029, -lr * Pred Grad:  0.005, New P: 0.036
iter 19 loss: 0.207
Actual params: [1.4356, 0.0358]
-Original Grad: -0.006, -lr * Pred Grad:  0.013, New P: 1.449
-Original Grad: 0.002, -lr * Pred Grad:  0.005, New P: 0.040
iter 20 loss: 0.208
Actual params: [1.449 , 0.0403]
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 1.461
-Original Grad: 0.029, -lr * Pred Grad:  0.007, New P: 0.048
Target params: [1.1812, 0.2779]
iter 0 loss: 0.606
Actual params: [0.5941, 0.5941]
-Original Grad: 0.299, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.456, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.419
Actual params: [0.6941, 0.4941]
-Original Grad: 0.262, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.257, -lr * Pred Grad:  -0.095, New P: 0.399
iter 2 loss: 0.360
Actual params: [0.7935, 0.3992]
-Original Grad: 0.102, -lr * Pred Grad:  0.090, New P: 0.884
-Original Grad: -0.079, -lr * Pred Grad:  -0.082, New P: 0.317
iter 3 loss: 0.341
Actual params: [0.8838, 0.317 ]
-Original Grad: 0.051, -lr * Pred Grad:  0.081, New P: 0.964
-Original Grad: -0.081, -lr * Pred Grad:  -0.075, New P: 0.242
iter 4 loss: 0.332
Actual params: [0.9644, 0.2417]
-Original Grad: 0.032, -lr * Pred Grad:  0.072, New P: 1.037
-Original Grad: -0.036, -lr * Pred Grad:  -0.067, New P: 0.174
iter 5 loss: 0.322
Actual params: [1.0366, 0.1745]
-Original Grad: 0.064, -lr * Pred Grad:  0.069, New P: 1.106
-Original Grad: -0.034, -lr * Pred Grad:  -0.061, New P: 0.113
iter 6 loss: 0.322
Actual params: [1.106 , 0.1134]
-Original Grad: 0.078, -lr * Pred Grad:  0.069, New P: 1.175
-Original Grad: -0.019, -lr * Pred Grad:  -0.055, New P: 0.058
iter 7 loss: 0.330
Actual params: [1.1749, 0.0582]
-Original Grad: 0.027, -lr * Pred Grad:  0.064, New P: 1.239
-Original Grad: 0.006, -lr * Pred Grad:  -0.048, New P: 0.010
iter 8 loss: 0.335
Actual params: [1.2386, 0.0102]
-Original Grad: -0.027, -lr * Pred Grad:  0.053, New P: 1.292
-Original Grad: 0.007, -lr * Pred Grad:  -0.042, New P: -0.032
iter 9 loss: 0.347
Actual params: [ 1.292 , -0.0319]
-Original Grad: -0.043, -lr * Pred Grad:  0.043, New P: 1.334
-Original Grad: 0.006, -lr * Pred Grad:  -0.037, New P: -0.069
iter 10 loss: 0.355
Actual params: [ 1.3345, -0.0689]
-Original Grad: -0.057, -lr * Pred Grad:  0.031, New P: 1.366
-Original Grad: 0.035, -lr * Pred Grad:  -0.030, New P: -0.099
iter 11 loss: 0.364
Actual params: [ 1.3659, -0.0989]
-Original Grad: -0.069, -lr * Pred Grad:  0.020, New P: 1.386
-Original Grad: 0.045, -lr * Pred Grad:  -0.023, New P: -0.122
iter 12 loss: 0.370
Actual params: [ 1.3861, -0.1217]
-Original Grad: -0.059, -lr * Pred Grad:  0.012, New P: 1.398
-Original Grad: 0.056, -lr * Pred Grad:  -0.015, New P: -0.137
iter 13 loss: 0.374
Actual params: [ 1.3979, -0.1372]
-Original Grad: -0.059, -lr * Pred Grad:  0.004, New P: 1.402
-Original Grad: 0.066, -lr * Pred Grad:  -0.008, New P: -0.145
iter 14 loss: 0.376
Actual params: [ 1.402 , -0.1452]
-Original Grad: -0.075, -lr * Pred Grad:  -0.004, New P: 1.398
-Original Grad: 0.064, -lr * Pred Grad:  -0.002, New P: -0.147
iter 15 loss: 0.375
Actual params: [ 1.3976, -0.1468]
-Original Grad: -0.067, -lr * Pred Grad:  -0.011, New P: 1.386
-Original Grad: 0.063, -lr * Pred Grad:  0.004, New P: -0.143
iter 16 loss: 0.372
Actual params: [ 1.3864, -0.1426]
-Original Grad: -0.082, -lr * Pred Grad:  -0.019, New P: 1.368
-Original Grad: 0.058, -lr * Pred Grad:  0.009, New P: -0.134
iter 17 loss: 0.366
Actual params: [ 1.3676, -0.1337]
-Original Grad: -0.089, -lr * Pred Grad:  -0.026, New P: 1.341
-Original Grad: 0.053, -lr * Pred Grad:  0.013, New P: -0.121
iter 18 loss: 0.359
Actual params: [ 1.3415, -0.1209]
-Original Grad: -0.047, -lr * Pred Grad:  -0.029, New P: 1.313
-Original Grad: 0.046, -lr * Pred Grad:  0.016, New P: -0.105
iter 19 loss: 0.352
Actual params: [ 1.3129, -0.1053]
-Original Grad: -0.028, -lr * Pred Grad:  -0.029, New P: 1.284
-Original Grad: 0.022, -lr * Pred Grad:  0.016, New P: -0.089
iter 20 loss: 0.345
Actual params: [ 1.284 , -0.0891]
-Original Grad: -0.033, -lr * Pred Grad:  -0.030, New P: 1.254
-Original Grad: 0.022, -lr * Pred Grad:  0.017, New P: -0.072
Target params: [1.1812, 0.2779]
iter 0 loss: 0.419
Actual params: [0.5941, 0.5941]
-Original Grad: 0.073, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.170, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.338
Actual params: [0.6941, 0.4941]
-Original Grad: 0.037, -lr * Pred Grad:  0.093, New P: 0.788
-Original Grad: -0.028, -lr * Pred Grad:  -0.078, New P: 0.416
iter 2 loss: 0.314
Actual params: [0.7875, 0.416 ]
-Original Grad: 0.014, -lr * Pred Grad:  0.082, New P: 0.869
-Original Grad: -0.030, -lr * Pred Grad:  -0.070, New P: 0.346
iter 3 loss: 0.294
Actual params: [0.8693, 0.3457]
-Original Grad: -0.001, -lr * Pred Grad:  0.066, New P: 0.935
-Original Grad: -0.005, -lr * Pred Grad:  -0.059, New P: 0.286
iter 4 loss: 0.283
Actual params: [0.9355, 0.2864]
-Original Grad: -0.001, -lr * Pred Grad:  0.055, New P: 0.990
-Original Grad: 0.005, -lr * Pred Grad:  -0.049, New P: 0.238
iter 5 loss: 0.278
Actual params: [0.9904, 0.2377]
-Original Grad: -0.001, -lr * Pred Grad:  0.047, New P: 1.037
-Original Grad: 0.007, -lr * Pred Grad:  -0.040, New P: 0.198
iter 6 loss: 0.276
Actual params: [1.0372, 0.1979]
-Original Grad: 0.003, -lr * Pred Grad:  0.043, New P: 1.080
-Original Grad: 0.003, -lr * Pred Grad:  -0.034, New P: 0.164
iter 7 loss: 0.276
Actual params: [1.0798, 0.1641]
-Original Grad: 0.007, -lr * Pred Grad:  0.041, New P: 1.121
-Original Grad: 0.002, -lr * Pred Grad:  -0.029, New P: 0.135
iter 8 loss: 0.275
Actual params: [1.1212, 0.135 ]
-Original Grad: 0.007, -lr * Pred Grad:  0.041, New P: 1.162
-Original Grad: 0.006, -lr * Pred Grad:  -0.024, New P: 0.111
iter 9 loss: 0.275
Actual params: [1.1619, 0.1107]
-Original Grad: 0.005, -lr * Pred Grad:  0.039, New P: 1.201
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: 0.089
iter 10 loss: 0.274
Actual params: [1.2012, 0.0889]
-Original Grad: 0.005, -lr * Pred Grad:  0.038, New P: 1.239
-Original Grad: 0.001, -lr * Pred Grad:  -0.019, New P: 0.070
iter 11 loss: 0.274
Actual params: [1.239 , 0.0695]
-Original Grad: 0.004, -lr * Pred Grad:  0.036, New P: 1.275
-Original Grad: 0.006, -lr * Pred Grad:  -0.016, New P: 0.054
iter 12 loss: 0.273
Actual params: [1.275 , 0.0538]
-Original Grad: 0.003, -lr * Pred Grad:  0.034, New P: 1.309
-Original Grad: 0.007, -lr * Pred Grad:  -0.012, New P: 0.041
iter 13 loss: 0.278
Actual params: [1.3091, 0.0414]
-Original Grad: 0.003, -lr * Pred Grad:  0.032, New P: 1.341
-Original Grad: 0.002, -lr * Pred Grad:  -0.011, New P: 0.031
iter 14 loss: 0.278
Actual params: [1.3413, 0.0309]
-Original Grad: 0.003, -lr * Pred Grad:  0.031, New P: 1.372
-Original Grad: 0.002, -lr * Pred Grad:  -0.009, New P: 0.022
iter 15 loss: 0.278
Actual params: [1.3721, 0.0218]
-Original Grad: 0.002, -lr * Pred Grad:  0.029, New P: 1.401
-Original Grad: 0.013, -lr * Pred Grad:  -0.004, New P: 0.017
iter 16 loss: 0.278
Actual params: [1.4012, 0.0174]
-Original Grad: 0.002, -lr * Pred Grad:  0.028, New P: 1.429
-Original Grad: 0.002, -lr * Pred Grad:  -0.003, New P: 0.014
iter 17 loss: 0.278
Actual params: [1.4287, 0.014 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.026, New P: 1.455
-Original Grad: 0.022, -lr * Pred Grad:  0.003, New P: 0.017
iter 18 loss: 0.278
Actual params: [1.4548, 0.0171]
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 1.479
-Original Grad: 0.007, -lr * Pred Grad:  0.005, New P: 0.022
iter 19 loss: 0.279
Actual params: [1.4786, 0.022 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.022, New P: 1.500
-Original Grad: 0.004, -lr * Pred Grad:  0.006, New P: 0.027
iter 20 loss: 0.280
Actual params: [1.5002, 0.0275]
-Original Grad: -0.001, -lr * Pred Grad:  0.019, New P: 1.520
-Original Grad: 0.007, -lr * Pred Grad:  0.007, New P: 0.034
Target params: [1.1812, 0.2779]
iter 0 loss: 0.388
Actual params: [0.5941, 0.5941]
-Original Grad: 0.044, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.037, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.328
Actual params: [0.6941, 0.4941]
-Original Grad: 0.030, -lr * Pred Grad:  0.097, New P: 0.791
-Original Grad: -0.028, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.235
Actual params: [0.7912, 0.3958]
-Original Grad: 0.033, -lr * Pred Grad:  0.098, New P: 0.889
-Original Grad: -0.075, -lr * Pred Grad:  -0.094, New P: 0.302
iter 3 loss: 0.153
Actual params: [0.8887, 0.3015]
-Original Grad: 0.006, -lr * Pred Grad:  0.085, New P: 0.974
-Original Grad: -0.022, -lr * Pred Grad:  -0.089, New P: 0.212
iter 4 loss: 0.248
Actual params: [0.9738, 0.2123]
-Original Grad: 0.001, -lr * Pred Grad:  0.073, New P: 1.047
-Original Grad: 0.209, -lr * Pred Grad:  0.020, New P: 0.232
iter 5 loss: 0.192
Actual params: [1.0466, 0.2324]
-Original Grad: 0.024, -lr * Pred Grad:  0.077, New P: 1.124
-Original Grad: 0.229, -lr * Pred Grad:  0.049, New P: 0.282
iter 6 loss: 0.162
Actual params: [1.1239, 0.2815]
-Original Grad: -0.024, -lr * Pred Grad:  0.047, New P: 1.171
-Original Grad: -0.027, -lr * Pred Grad:  0.039, New P: 0.320
iter 7 loss: 0.210
Actual params: [1.1711, 0.3201]
-Original Grad: -0.082, -lr * Pred Grad:  -0.010, New P: 1.161
-Original Grad: -0.148, -lr * Pred Grad:  0.010, New P: 0.330
iter 8 loss: 0.215
Actual params: [1.161 , 0.3304]
-Original Grad: -0.111, -lr * Pred Grad:  -0.041, New P: 1.120
-Original Grad: -0.158, -lr * Pred Grad:  -0.011, New P: 0.319
iter 9 loss: 0.189
Actual params: [1.1199, 0.3189]
-Original Grad: -0.081, -lr * Pred Grad:  -0.055, New P: 1.065
-Original Grad: -0.093, -lr * Pred Grad:  -0.021, New P: 0.298
iter 10 loss: 0.157
Actual params: [1.0649, 0.2977]
-Original Grad: -0.004, -lr * Pred Grad:  -0.050, New P: 1.014
-Original Grad: 0.007, -lr * Pred Grad:  -0.018, New P: 0.280
iter 11 loss: 0.153
Actual params: [1.0144, 0.2795]
-Original Grad: 0.005, -lr * Pred Grad:  -0.044, New P: 0.971
-Original Grad: 0.046, -lr * Pred Grad:  -0.011, New P: 0.269
iter 12 loss: 0.170
Actual params: [0.9705, 0.2688]
-Original Grad: 0.002, -lr * Pred Grad:  -0.039, New P: 0.932
-Original Grad: 0.072, -lr * Pred Grad:  -0.001, New P: 0.268
iter 13 loss: 0.168
Actual params: [0.9315, 0.2677]
-Original Grad: 0.006, -lr * Pred Grad:  -0.034, New P: 0.898
-Original Grad: 0.077, -lr * Pred Grad:  0.008, New P: 0.276
iter 14 loss: 0.157
Actual params: [0.898 , 0.2757]
-Original Grad: -0.003, -lr * Pred Grad:  -0.031, New P: 0.867
-Original Grad: 0.029, -lr * Pred Grad:  0.011, New P: 0.286
iter 15 loss: 0.150
Actual params: [0.8669, 0.2863]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: 0.839
-Original Grad: 0.013, -lr * Pred Grad:  0.011, New P: 0.298
iter 16 loss: 0.155
Actual params: [0.8387, 0.2975]
-Original Grad: 0.015, -lr * Pred Grad:  -0.021, New P: 0.817
-Original Grad: -0.037, -lr * Pred Grad:  0.006, New P: 0.303
iter 17 loss: 0.160
Actual params: [0.8173, 0.3032]
-Original Grad: 0.016, -lr * Pred Grad:  -0.015, New P: 0.803
-Original Grad: -0.059, -lr * Pred Grad:  -0.002, New P: 0.301
iter 18 loss: 0.161
Actual params: [0.8026, 0.3014]
-Original Grad: 0.027, -lr * Pred Grad:  -0.006, New P: 0.797
-Original Grad: -0.090, -lr * Pred Grad:  -0.012, New P: 0.289
iter 19 loss: 0.155
Actual params: [0.7971, 0.2893]
-Original Grad: 0.020, -lr * Pred Grad:  0.001, New P: 0.798
-Original Grad: -0.074, -lr * Pred Grad:  -0.019, New P: 0.270
iter 20 loss: 0.150
Actual params: [0.7978, 0.2699]
-Original Grad: 0.007, -lr * Pred Grad:  0.003, New P: 0.800
-Original Grad: 0.012, -lr * Pred Grad:  -0.016, New P: 0.254
Target params: [1.1812, 0.2779]
iter 0 loss: 1.328
Actual params: [0.5941, 0.5941]
-Original Grad: 0.583, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.004, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 1.240
Actual params: [0.6941, 0.4941]
-Original Grad: 0.517, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: 0.032, -lr * Pred Grad:  0.066, New P: 0.560
iter 2 loss: 1.174
Actual params: [0.7936, 0.5602]
-Original Grad: 0.536, -lr * Pred Grad:  0.100, New P: 0.893
-Original Grad: 0.037, -lr * Pred Grad:  0.082, New P: 0.642
iter 3 loss: 1.024
Actual params: [0.8931, 0.642 ]
-Original Grad: 0.838, -lr * Pred Grad:  0.100, New P: 0.993
-Original Grad: 0.072, -lr * Pred Grad:  0.086, New P: 0.728
iter 4 loss: 0.696
Actual params: [0.9927, 0.7277]
-Original Grad: 0.944, -lr * Pred Grad:  0.100, New P: 1.093
-Original Grad: 0.105, -lr * Pred Grad:  0.088, New P: 0.816
iter 5 loss: 0.444
Actual params: [1.0928, 0.816 ]
-Original Grad: -0.054, -lr * Pred Grad:  0.084, New P: 1.177
-Original Grad: -0.216, -lr * Pred Grad:  -0.004, New P: 0.812
iter 6 loss: 0.450
Actual params: [1.1772, 0.8123]
-Original Grad: -0.056, -lr * Pred Grad:  0.072, New P: 1.249
-Original Grad: -0.171, -lr * Pred Grad:  -0.031, New P: 0.781
iter 7 loss: 0.372
Actual params: [1.2491, 0.7815]
-Original Grad: -0.049, -lr * Pred Grad:  0.062, New P: 1.311
-Original Grad: -0.191, -lr * Pred Grad:  -0.049, New P: 0.732
iter 8 loss: 0.258
Actual params: [1.3109, 0.7323]
-Original Grad: -0.007, -lr * Pred Grad:  0.055, New P: 1.366
-Original Grad: -0.092, -lr * Pred Grad:  -0.054, New P: 0.678
iter 9 loss: 0.192
Actual params: [1.3656, 0.6779]
-Original Grad: 0.009, -lr * Pred Grad:  0.049, New P: 1.415
-Original Grad: -0.067, -lr * Pred Grad:  -0.056, New P: 0.622
iter 10 loss: 0.248
Actual params: [1.4146, 0.6215]
-Original Grad: 0.009, -lr * Pred Grad:  0.044, New P: 1.459
-Original Grad: -0.009, -lr * Pred Grad:  -0.052, New P: 0.570
iter 11 loss: 0.356
Actual params: [1.4588, 0.5699]
-Original Grad: -0.015, -lr * Pred Grad:  0.039, New P: 1.498
-Original Grad: 0.025, -lr * Pred Grad:  -0.043, New P: 0.527
iter 12 loss: 0.431
Actual params: [1.4981, 0.5267]
-Original Grad: -0.021, -lr * Pred Grad:  0.035, New P: 1.533
-Original Grad: 0.039, -lr * Pred Grad:  -0.034, New P: 0.493
iter 13 loss: 0.487
Actual params: [1.5329, 0.4928]
-Original Grad: -0.037, -lr * Pred Grad:  0.030, New P: 1.563
-Original Grad: 0.046, -lr * Pred Grad:  -0.025, New P: 0.468
iter 14 loss: 0.525
Actual params: [1.5631, 0.4683]
-Original Grad: -0.032, -lr * Pred Grad:  0.026, New P: 1.590
-Original Grad: 0.047, -lr * Pred Grad:  -0.016, New P: 0.452
iter 15 loss: 0.552
Actual params: [1.5895, 0.4522]
-Original Grad: -0.059, -lr * Pred Grad:  0.022, New P: 1.612
-Original Grad: 0.072, -lr * Pred Grad:  -0.005, New P: 0.447
iter 16 loss: 0.566
Actual params: [1.6115, 0.4468]
-Original Grad: -0.046, -lr * Pred Grad:  0.019, New P: 1.630
-Original Grad: 0.069, -lr * Pred Grad:  0.004, New P: 0.450
iter 17 loss: 0.573
Actual params: [1.6301, 0.4505]
-Original Grad: -0.055, -lr * Pred Grad:  0.015, New P: 1.645
-Original Grad: 0.076, -lr * Pred Grad:  0.013, New P: 0.463
iter 18 loss: 0.572
Actual params: [1.6452, 0.4631]
-Original Grad: -0.051, -lr * Pred Grad:  0.012, New P: 1.657
-Original Grad: 0.061, -lr * Pred Grad:  0.019, New P: 0.482
iter 19 loss: 0.563
Actual params: [1.6572, 0.4819]
-Original Grad: -0.038, -lr * Pred Grad:  0.010, New P: 1.667
-Original Grad: 0.048, -lr * Pred Grad:  0.023, New P: 0.505
iter 20 loss: 0.552
Actual params: [1.667 , 0.5047]
-Original Grad: -0.034, -lr * Pred Grad:  0.008, New P: 1.675
-Original Grad: 0.058, -lr * Pred Grad:  0.028, New P: 0.532
Target params: [1.1812, 0.2779]
iter 0 loss: 0.321
Actual params: [0.5941, 0.5941]
-Original Grad: 0.013, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.071, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.328
Actual params: [0.6941, 0.4941]
-Original Grad: 0.004, -lr * Pred Grad:  0.085, New P: 0.779
-Original Grad: -0.007, -lr * Pred Grad:  -0.074, New P: 0.420
iter 2 loss: 0.325
Actual params: [0.7787, 0.4201]
-Original Grad: 0.012, -lr * Pred Grad:  0.091, New P: 0.870
-Original Grad: 0.015, -lr * Pred Grad:  -0.043, New P: 0.378
iter 3 loss: 0.321
Actual params: [0.8701, 0.3775]
-Original Grad: 0.011, -lr * Pred Grad:  0.094, New P: 0.964
-Original Grad: 0.022, -lr * Pred Grad:  -0.016, New P: 0.361
iter 4 loss: 0.313
Actual params: [0.9641, 0.3612]
-Original Grad: 0.008, -lr * Pred Grad:  0.094, New P: 1.058
-Original Grad: 0.020, -lr * Pred Grad:  0.001, New P: 0.362
iter 5 loss: 0.310
Actual params: [1.0579, 0.3618]
-Original Grad: 0.003, -lr * Pred Grad:  0.088, New P: 1.146
-Original Grad: 0.015, -lr * Pred Grad:  0.010, New P: 0.372
iter 6 loss: 0.310
Actual params: [1.1456, 0.3718]
-Original Grad: 0.004, -lr * Pred Grad:  0.084, New P: 1.229
-Original Grad: 0.010, -lr * Pred Grad:  0.015, New P: 0.387
iter 7 loss: 0.311
Actual params: [1.2291, 0.3866]
-Original Grad: 0.004, -lr * Pred Grad:  0.080, New P: 1.310
-Original Grad: 0.010, -lr * Pred Grad:  0.019, New P: 0.406
iter 8 loss: 0.312
Actual params: [1.3095, 0.4056]
-Original Grad: 0.002, -lr * Pred Grad:  0.076, New P: 1.385
-Original Grad: 0.009, -lr * Pred Grad:  0.022, New P: 0.428
iter 9 loss: 0.314
Actual params: [1.3854, 0.4276]
-Original Grad: 0.002, -lr * Pred Grad:  0.072, New P: 1.457
-Original Grad: -0.004, -lr * Pred Grad:  0.017, New P: 0.445
iter 10 loss: 0.317
Actual params: [1.4574, 0.4446]
-Original Grad: 0.000, -lr * Pred Grad:  0.065, New P: 1.522
-Original Grad: -0.009, -lr * Pred Grad:  0.010, New P: 0.454
iter 11 loss: 0.319
Actual params: [1.522 , 0.4541]
-Original Grad: -0.000, -lr * Pred Grad:  0.057, New P: 1.579
-Original Grad: -0.006, -lr * Pred Grad:  0.005, New P: 0.459
iter 12 loss: 0.321
Actual params: [1.5794, 0.4592]
-Original Grad: -0.003, -lr * Pred Grad:  0.045, New P: 1.625
-Original Grad: -0.030, -lr * Pred Grad:  -0.012, New P: 0.447
iter 13 loss: 0.321
Actual params: [1.6248, 0.4468]
-Original Grad: -0.002, -lr * Pred Grad:  0.037, New P: 1.661
-Original Grad: -0.018, -lr * Pred Grad:  -0.021, New P: 0.426
iter 14 loss: 0.320
Actual params: [1.6613, 0.4261]
-Original Grad: -0.002, -lr * Pred Grad:  0.029, New P: 1.691
-Original Grad: -0.004, -lr * Pred Grad:  -0.021, New P: 0.405
iter 15 loss: 0.318
Actual params: [1.6908, 0.4055]
-Original Grad: -0.002, -lr * Pred Grad:  0.023, New P: 1.714
-Original Grad: -0.004, -lr * Pred Grad:  -0.021, New P: 0.385
iter 16 loss: 0.317
Actual params: [1.7138, 0.3847]
-Original Grad: -0.001, -lr * Pred Grad:  0.019, New P: 1.733
-Original Grad: 0.010, -lr * Pred Grad:  -0.013, New P: 0.371
iter 17 loss: 0.316
Actual params: [1.7328, 0.3714]
-Original Grad: -0.001, -lr * Pred Grad:  0.016, New P: 1.749
-Original Grad: 0.007, -lr * Pred Grad:  -0.008, New P: 0.363
iter 18 loss: 0.316
Actual params: [1.7491, 0.3632]
-Original Grad: -0.001, -lr * Pred Grad:  0.012, New P: 1.762
-Original Grad: 0.006, -lr * Pred Grad:  -0.004, New P: 0.359
iter 19 loss: 0.316
Actual params: [1.7615, 0.3592]
-Original Grad: -0.001, -lr * Pred Grad:  0.009, New P: 1.771
-Original Grad: 0.005, -lr * Pred Grad:  -0.001, New P: 0.358
iter 20 loss: 0.316
Actual params: [1.7706, 0.3582]
-Original Grad: -0.001, -lr * Pred Grad:  0.006, New P: 1.777
-Original Grad: 0.005, -lr * Pred Grad:  0.002, New P: 0.360
Target params: [1.1812, 0.2779]
iter 0 loss: 0.383
Actual params: [0.5941, 0.5941]
-Original Grad: 0.331, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.292, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.331
Actual params: [0.6941, 0.4941]
-Original Grad: 0.008, -lr * Pred Grad:  0.069, New P: 0.763
-Original Grad: -0.035, -lr * Pred Grad:  -0.075, New P: 0.419
iter 2 loss: 0.324
Actual params: [0.7628, 0.4187]
-Original Grad: -0.003, -lr * Pred Grad:  0.052, New P: 0.815
-Original Grad: -0.012, -lr * Pred Grad:  -0.061, New P: 0.358
iter 3 loss: 0.322
Actual params: [0.8153, 0.3578]
-Original Grad: -0.011, -lr * Pred Grad:  0.041, New P: 0.856
-Original Grad: -0.005, -lr * Pred Grad:  -0.051, New P: 0.307
iter 4 loss: 0.323
Actual params: [0.8564, 0.3069]
-Original Grad: -0.002, -lr * Pred Grad:  0.034, New P: 0.891
-Original Grad: 0.000, -lr * Pred Grad:  -0.043, New P: 0.264
iter 5 loss: 0.325
Actual params: [0.8907, 0.264 ]
-Original Grad: -0.009, -lr * Pred Grad:  0.028, New P: 0.919
-Original Grad: 0.002, -lr * Pred Grad:  -0.037, New P: 0.227
iter 6 loss: 0.327
Actual params: [0.9189, 0.2273]
-Original Grad: 0.002, -lr * Pred Grad:  0.025, New P: 0.944
-Original Grad: 0.006, -lr * Pred Grad:  -0.031, New P: 0.196
iter 7 loss: 0.329
Actual params: [0.9438, 0.1963]
-Original Grad: -0.005, -lr * Pred Grad:  0.021, New P: 0.965
-Original Grad: 0.004, -lr * Pred Grad:  -0.027, New P: 0.170
iter 8 loss: 0.332
Actual params: [0.965 , 0.1698]
-Original Grad: -0.008, -lr * Pred Grad:  0.018, New P: 0.983
-Original Grad: 0.007, -lr * Pred Grad:  -0.022, New P: 0.147
iter 9 loss: 0.333
Actual params: [0.9826, 0.1474]
-Original Grad: -0.004, -lr * Pred Grad:  0.015, New P: 0.998
-Original Grad: 0.008, -lr * Pred Grad:  -0.019, New P: 0.129
iter 10 loss: 0.335
Actual params: [0.9977, 0.1288]
-Original Grad: -0.005, -lr * Pred Grad:  0.013, New P: 1.010
-Original Grad: 0.010, -lr * Pred Grad:  -0.015, New P: 0.114
iter 11 loss: 0.335
Actual params: [1.0105, 0.1139]
-Original Grad: -0.012, -lr * Pred Grad:  0.010, New P: 1.020
-Original Grad: 0.009, -lr * Pred Grad:  -0.012, New P: 0.102
iter 12 loss: 0.336
Actual params: [1.0202, 0.1019]
-Original Grad: -0.009, -lr * Pred Grad:  0.007, New P: 1.028
-Original Grad: 0.012, -lr * Pred Grad:  -0.009, New P: 0.093
iter 13 loss: 0.337
Actual params: [1.0276, 0.0931]
-Original Grad: -0.014, -lr * Pred Grad:  0.005, New P: 1.032
-Original Grad: 0.013, -lr * Pred Grad:  -0.006, New P: 0.087
iter 14 loss: 0.338
Actual params: [1.0323, 0.0873]
-Original Grad: -0.014, -lr * Pred Grad:  0.002, New P: 1.034
-Original Grad: 0.014, -lr * Pred Grad:  -0.003, New P: 0.084
iter 15 loss: 0.338
Actual params: [1.0344, 0.0844]
-Original Grad: -0.022, -lr * Pred Grad:  -0.001, New P: 1.033
-Original Grad: 0.015, -lr * Pred Grad:  -0.000, New P: 0.084
iter 16 loss: 0.338
Actual params: [1.0331, 0.0843]
-Original Grad: -0.008, -lr * Pred Grad:  -0.002, New P: 1.031
-Original Grad: 0.015, -lr * Pred Grad:  0.002, New P: 0.087
iter 17 loss: 0.337
Actual params: [1.0307, 0.0867]
-Original Grad: -0.017, -lr * Pred Grad:  -0.005, New P: 1.026
-Original Grad: 0.015, -lr * Pred Grad:  0.005, New P: 0.091
iter 18 loss: 0.337
Actual params: [1.0259, 0.0914]
-Original Grad: -0.010, -lr * Pred Grad:  -0.006, New P: 1.020
-Original Grad: 0.009, -lr * Pred Grad:  0.006, New P: 0.097
iter 19 loss: 0.336
Actual params: [1.0201, 0.0972]
-Original Grad: -0.013, -lr * Pred Grad:  -0.007, New P: 1.013
-Original Grad: 0.012, -lr * Pred Grad:  0.007, New P: 0.105
iter 20 loss: 0.335
Actual params: [1.0128, 0.1046]
-Original Grad: -0.008, -lr * Pred Grad:  -0.008, New P: 1.005
-Original Grad: 0.010, -lr * Pred Grad:  0.008, New P: 0.113
Target params: [1.1812, 0.2779]
iter 0 loss: 0.327
Actual params: [0.5941, 0.5941]
-Original Grad: -0.091, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.210, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.294
Actual params: [0.4941, 0.4941]
-Original Grad: 0.048, -lr * Pred Grad:  -0.025, New P: 0.469
-Original Grad: -0.190, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 0.301
Actual params: [0.4691, 0.3944]
-Original Grad: 0.157, -lr * Pred Grad:  0.043, New P: 0.512
-Original Grad: 0.011, -lr * Pred Grad:  -0.074, New P: 0.320
iter 3 loss: 0.302
Actual params: [0.512, 0.32 ]
-Original Grad: 0.110, -lr * Pred Grad:  0.060, New P: 0.572
-Original Grad: 0.081, -lr * Pred Grad:  -0.043, New P: 0.277
iter 4 loss: 0.291
Actual params: [0.5717, 0.2774]
-Original Grad: 0.036, -lr * Pred Grad:  0.059, New P: 0.630
-Original Grad: 0.081, -lr * Pred Grad:  -0.020, New P: 0.257
iter 5 loss: 0.288
Actual params: [0.6305, 0.2572]
-Original Grad: 0.004, -lr * Pred Grad:  0.051, New P: 0.682
-Original Grad: 0.041, -lr * Pred Grad:  -0.010, New P: 0.247
iter 6 loss: 0.288
Actual params: [0.6819, 0.247 ]
-Original Grad: -0.024, -lr * Pred Grad:  0.039, New P: 0.721
-Original Grad: 0.030, -lr * Pred Grad:  -0.004, New P: 0.243
iter 7 loss: 0.289
Actual params: [0.721 , 0.2429]
-Original Grad: -0.004, -lr * Pred Grad:  0.033, New P: 0.755
-Original Grad: 0.021, -lr * Pred Grad:  -0.000, New P: 0.243
iter 8 loss: 0.291
Actual params: [0.7545, 0.2426]
-Original Grad: -0.019, -lr * Pred Grad:  0.025, New P: 0.780
-Original Grad: 0.011, -lr * Pred Grad:  0.001, New P: 0.244
iter 9 loss: 0.293
Actual params: [0.7798, 0.244 ]
-Original Grad: -0.024, -lr * Pred Grad:  0.017, New P: 0.797
-Original Grad: -0.002, -lr * Pred Grad:  0.001, New P: 0.245
iter 10 loss: 0.294
Actual params: [0.7972, 0.245 ]
-Original Grad: -0.039, -lr * Pred Grad:  0.007, New P: 0.804
-Original Grad: -0.036, -lr * Pred Grad:  -0.005, New P: 0.240
iter 11 loss: 0.294
Actual params: [0.8041, 0.2403]
-Original Grad: -0.012, -lr * Pred Grad:  0.004, New P: 0.808
-Original Grad: 0.019, -lr * Pred Grad:  -0.001, New P: 0.239
iter 12 loss: 0.294
Actual params: [0.8077, 0.2391]
-Original Grad: -0.040, -lr * Pred Grad:  -0.005, New P: 0.803
-Original Grad: -0.029, -lr * Pred Grad:  -0.006, New P: 0.233
iter 13 loss: 0.293
Actual params: [0.8027, 0.2335]
-Original Grad: -0.029, -lr * Pred Grad:  -0.011, New P: 0.792
-Original Grad: 0.030, -lr * Pred Grad:  -0.000, New P: 0.233
iter 14 loss: 0.292
Actual params: [0.7921, 0.2331]
-Original Grad: -0.047, -lr * Pred Grad:  -0.019, New P: 0.773
-Original Grad: -0.040, -lr * Pred Grad:  -0.006, New P: 0.227
iter 15 loss: 0.291
Actual params: [0.773 , 0.2266]
-Original Grad: -0.060, -lr * Pred Grad:  -0.029, New P: 0.744
-Original Grad: 0.015, -lr * Pred Grad:  -0.004, New P: 0.223
iter 16 loss: 0.288
Actual params: [0.7442, 0.2231]
-Original Grad: -0.029, -lr * Pred Grad:  -0.032, New P: 0.713
-Original Grad: 0.014, -lr * Pred Grad:  -0.001, New P: 0.222
iter 17 loss: 0.287
Actual params: [0.7125, 0.2221]
-Original Grad: -0.021, -lr * Pred Grad:  -0.033, New P: 0.680
-Original Grad: 0.039, -lr * Pred Grad:  0.005, New P: 0.227
iter 18 loss: 0.287
Actual params: [0.6796, 0.2273]
-Original Grad: -0.017, -lr * Pred Grad:  -0.033, New P: 0.646
-Original Grad: 0.025, -lr * Pred Grad:  0.009, New P: 0.236
iter 19 loss: 0.287
Actual params: [0.6464, 0.2359]
-Original Grad: 0.003, -lr * Pred Grad:  -0.030, New P: 0.617
-Original Grad: 0.037, -lr * Pred Grad:  0.014, New P: 0.250
iter 20 loss: 0.288
Actual params: [0.6167, 0.2496]
-Original Grad: 0.038, -lr * Pred Grad:  -0.019, New P: 0.598
-Original Grad: 0.074, -lr * Pred Grad:  0.024, New P: 0.273
Target params: [1.1812, 0.2779]
iter 0 loss: 0.340
Actual params: [0.5941, 0.5941]
-Original Grad: -0.028, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.177, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.352
Actual params: [0.4941, 0.4941]
-Original Grad: 0.005, -lr * Pred Grad:  -0.053, New P: 0.441
-Original Grad: -0.032, -lr * Pred Grad:  -0.079, New P: 0.415
iter 2 loss: 0.352
Actual params: [0.4415, 0.4147]
-Original Grad: 0.009, -lr * Pred Grad:  -0.019, New P: 0.422
-Original Grad: -0.037, -lr * Pred Grad:  -0.073, New P: 0.342
iter 3 loss: 0.347
Actual params: [0.4221, 0.3418]
-Original Grad: 0.010, -lr * Pred Grad:  0.003, New P: 0.425
-Original Grad: -0.018, -lr * Pred Grad:  -0.065, New P: 0.277
iter 4 loss: 0.341
Actual params: [0.425 , 0.2767]
-Original Grad: -0.003, -lr * Pred Grad:  -0.004, New P: 0.421
-Original Grad: -0.002, -lr * Pred Grad:  -0.056, New P: 0.221
iter 5 loss: 0.337
Actual params: [0.4214, 0.2211]
-Original Grad: -0.020, -lr * Pred Grad:  -0.031, New P: 0.391
-Original Grad: 0.003, -lr * Pred Grad:  -0.047, New P: 0.174
iter 6 loss: 0.337
Actual params: [0.3909, 0.1741]
-Original Grad: -0.002, -lr * Pred Grad:  -0.029, New P: 0.362
-Original Grad: 0.006, -lr * Pred Grad:  -0.039, New P: 0.135
iter 7 loss: 0.336
Actual params: [0.362 , 0.1348]
-Original Grad: 0.010, -lr * Pred Grad:  -0.012, New P: 0.350
-Original Grad: 0.006, -lr * Pred Grad:  -0.033, New P: 0.102
iter 8 loss: 0.332
Actual params: [0.3502, 0.1018]
-Original Grad: 0.001, -lr * Pred Grad:  -0.009, New P: 0.341
-Original Grad: 0.010, -lr * Pred Grad:  -0.027, New P: 0.075
iter 9 loss: 0.328
Actual params: [0.3412, 0.0751]
-Original Grad: -0.002, -lr * Pred Grad:  -0.011, New P: 0.330
-Original Grad: 0.015, -lr * Pred Grad:  -0.020, New P: 0.055
iter 10 loss: 0.325
Actual params: [0.3304, 0.0553]
-Original Grad: -0.003, -lr * Pred Grad:  -0.014, New P: 0.316
-Original Grad: 0.024, -lr * Pred Grad:  -0.011, New P: 0.044
iter 11 loss: 0.324
Actual params: [0.3165, 0.0439]
-Original Grad: -0.007, -lr * Pred Grad:  -0.021, New P: 0.296
-Original Grad: 0.021, -lr * Pred Grad:  -0.005, New P: 0.039
iter 12 loss: 0.323
Actual params: [0.2959, 0.0391]
-Original Grad: -0.003, -lr * Pred Grad:  -0.023, New P: 0.273
-Original Grad: 0.015, -lr * Pred Grad:  -0.000, New P: 0.039
iter 13 loss: 0.324
Actual params: [0.2732, 0.0387]
-Original Grad: -0.005, -lr * Pred Grad:  -0.027, New P: 0.246
-Original Grad: 0.021, -lr * Pred Grad:  0.005, New P: 0.044
iter 14 loss: 0.326
Actual params: [0.2463, 0.0439]
-Original Grad: -0.001, -lr * Pred Grad:  -0.026, New P: 0.220
-Original Grad: 0.032, -lr * Pred Grad:  0.013, New P: 0.057
iter 15 loss: 0.330
Actual params: [0.2201, 0.0567]
-Original Grad: -0.002, -lr * Pred Grad:  -0.026, New P: 0.194
-Original Grad: 0.007, -lr * Pred Grad:  0.013, New P: 0.070
iter 16 loss: 0.334
Actual params: [0.1943, 0.0699]
-Original Grad: -0.002, -lr * Pred Grad:  -0.026, New P: 0.168
-Original Grad: 0.009, -lr * Pred Grad:  0.014, New P: 0.084
iter 17 loss: 0.337
Actual params: [0.168 , 0.0844]
-Original Grad: -0.002, -lr * Pred Grad:  -0.027, New P: 0.141
-Original Grad: 0.004, -lr * Pred Grad:  0.014, New P: 0.098
iter 18 loss: 0.340
Actual params: [0.1413, 0.0985]
-Original Grad: -0.001, -lr * Pred Grad:  -0.025, New P: 0.116
-Original Grad: 0.007, -lr * Pred Grad:  0.015, New P: 0.113
iter 19 loss: 0.343
Actual params: [0.116 , 0.1132]
-Original Grad: -0.003, -lr * Pred Grad:  -0.027, New P: 0.089
-Original Grad: -0.001, -lr * Pred Grad:  0.013, New P: 0.126
iter 20 loss: 0.346
Actual params: [0.089 , 0.1263]
-Original Grad: -0.002, -lr * Pred Grad:  -0.028, New P: 0.061
-Original Grad: -0.001, -lr * Pred Grad:  0.012, New P: 0.138
Target params: [1.1812, 0.2779]
iter 0 loss: 0.813
Actual params: [0.5941, 0.5941]
-Original Grad: 0.206, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.656, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.651
Actual params: [0.6941, 0.4941]
-Original Grad: 0.178, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.284, -lr * Pred Grad:  -0.091, New P: 0.403
iter 2 loss: 0.487
Actual params: [0.7934, 0.403 ]
-Original Grad: 0.099, -lr * Pred Grad:  0.094, New P: 0.887
-Original Grad: -0.077, -lr * Pred Grad:  -0.077, New P: 0.326
iter 3 loss: 0.404
Actual params: [0.8874, 0.3262]
-Original Grad: 0.026, -lr * Pred Grad:  0.082, New P: 0.969
-Original Grad: -0.009, -lr * Pred Grad:  -0.064, New P: 0.263
iter 4 loss: 0.367
Actual params: [0.9693, 0.2625]
-Original Grad: 0.010, -lr * Pred Grad:  0.071, New P: 1.040
-Original Grad: -0.003, -lr * Pred Grad:  -0.054, New P: 0.208
iter 5 loss: 0.369
Actual params: [1.0403, 0.2085]
-Original Grad: 0.001, -lr * Pred Grad:  0.061, New P: 1.102
-Original Grad: 0.012, -lr * Pred Grad:  -0.046, New P: 0.163
iter 6 loss: 0.384
Actual params: [1.1016, 0.1628]
-Original Grad: -0.014, -lr * Pred Grad:  0.051, New P: 1.153
-Original Grad: 0.020, -lr * Pred Grad:  -0.038, New P: 0.124
iter 7 loss: 0.398
Actual params: [1.1526, 0.1244]
-Original Grad: -0.012, -lr * Pred Grad:  0.043, New P: 1.195
-Original Grad: 0.015, -lr * Pred Grad:  -0.033, New P: 0.092
iter 8 loss: 0.410
Actual params: [1.1954, 0.0916]
-Original Grad: -0.011, -lr * Pred Grad:  0.036, New P: 1.231
-Original Grad: 0.012, -lr * Pred Grad:  -0.028, New P: 0.063
iter 9 loss: 0.410
Actual params: [1.2314, 0.0633]
-Original Grad: -0.013, -lr * Pred Grad:  0.030, New P: 1.261
-Original Grad: 0.009, -lr * Pred Grad:  -0.025, New P: 0.039
iter 10 loss: 0.415
Actual params: [1.2613, 0.0386]
-Original Grad: -0.016, -lr * Pred Grad:  0.024, New P: 1.286
-Original Grad: 0.009, -lr * Pred Grad:  -0.021, New P: 0.017
iter 11 loss: 0.420
Actual params: [1.2855, 0.0171]
-Original Grad: -0.019, -lr * Pred Grad:  0.019, New P: 1.304
-Original Grad: 0.009, -lr * Pred Grad:  -0.019, New P: -0.002
iter 12 loss: 0.424
Actual params: [ 1.3041, -0.0016]
-Original Grad: -0.018, -lr * Pred Grad:  0.014, New P: 1.318
-Original Grad: 0.009, -lr * Pred Grad:  -0.016, New P: -0.018
iter 13 loss: 0.426
Actual params: [ 1.3178, -0.0179]
-Original Grad: -0.026, -lr * Pred Grad:  0.008, New P: 1.326
-Original Grad: 0.008, -lr * Pred Grad:  -0.014, New P: -0.032
iter 14 loss: 0.427
Actual params: [ 1.3257, -0.032 ]
-Original Grad: -0.013, -lr * Pred Grad:  0.005, New P: 1.331
-Original Grad: 0.006, -lr * Pred Grad:  -0.012, New P: -0.044
iter 15 loss: 0.428
Actual params: [ 1.3308, -0.0444]
-Original Grad: -0.012, -lr * Pred Grad:  0.003, New P: 1.333
-Original Grad: 0.009, -lr * Pred Grad:  -0.011, New P: -0.055
iter 16 loss: 0.429
Actual params: [ 1.3334, -0.055 ]
-Original Grad: -0.015, -lr * Pred Grad:  -0.000, New P: 1.333
-Original Grad: 0.008, -lr * Pred Grad:  -0.009, New P: -0.064
iter 17 loss: 0.429
Actual params: [ 1.3333, -0.0641]
-Original Grad: -0.011, -lr * Pred Grad:  -0.002, New P: 1.331
-Original Grad: 0.009, -lr * Pred Grad:  -0.008, New P: -0.072
iter 18 loss: 0.429
Actual params: [ 1.3314, -0.0718]
-Original Grad: -0.013, -lr * Pred Grad:  -0.004, New P: 1.327
-Original Grad: 0.009, -lr * Pred Grad:  -0.006, New P: -0.078
iter 19 loss: 0.428
Actual params: [ 1.3274, -0.0781]
-Original Grad: -0.011, -lr * Pred Grad:  -0.006, New P: 1.322
-Original Grad: 0.008, -lr * Pred Grad:  -0.005, New P: -0.083
iter 20 loss: 0.428
Actual params: [ 1.3218, -0.0833]
-Original Grad: -0.011, -lr * Pred Grad:  -0.007, New P: 1.315
-Original Grad: 0.008, -lr * Pred Grad:  -0.004, New P: -0.087
Target params: [1.1812, 0.2779]
iter 0 loss: 0.595
Actual params: [0.5941, 0.5941]
-Original Grad: 0.196, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.088, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.587
Actual params: [0.6941, 0.4941]
-Original Grad: 0.187, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.043, -lr * Pred Grad:  -0.093, New P: 0.401
iter 2 loss: 0.570
Actual params: [0.7939, 0.4012]
-Original Grad: 0.176, -lr * Pred Grad:  0.100, New P: 0.893
-Original Grad: -0.006, -lr * Pred Grad:  -0.076, New P: 0.325
iter 3 loss: 0.551
Actual params: [0.8934, 0.3255]
-Original Grad: 0.037, -lr * Pred Grad:  0.088, New P: 0.981
-Original Grad: 0.027, -lr * Pred Grad:  -0.044, New P: 0.281
iter 4 loss: 0.537
Actual params: [0.981 , 0.2811]
-Original Grad: 0.002, -lr * Pred Grad:  0.074, New P: 1.055
-Original Grad: 0.046, -lr * Pred Grad:  -0.011, New P: 0.270
iter 5 loss: 0.531
Actual params: [1.0555, 0.2697]
-Original Grad: 0.015, -lr * Pred Grad:  0.067, New P: 1.122
-Original Grad: 0.062, -lr * Pred Grad:  0.017, New P: 0.286
iter 6 loss: 0.531
Actual params: [1.122 , 0.2864]
-Original Grad: -0.023, -lr * Pred Grad:  0.054, New P: 1.176
-Original Grad: 0.012, -lr * Pred Grad:  0.019, New P: 0.306
iter 7 loss: 0.526
Actual params: [1.1764, 0.3058]
-Original Grad: -0.030, -lr * Pred Grad:  0.043, New P: 1.220
-Original Grad: 0.053, -lr * Pred Grad:  0.035, New P: 0.340
iter 8 loss: 0.517
Actual params: [1.2195, 0.3405]
-Original Grad: -0.033, -lr * Pred Grad:  0.033, New P: 1.253
-Original Grad: -0.021, -lr * Pred Grad:  0.023, New P: 0.364
iter 9 loss: 0.512
Actual params: [1.2528, 0.3636]
-Original Grad: -0.048, -lr * Pred Grad:  0.022, New P: 1.275
-Original Grad: 0.043, -lr * Pred Grad:  0.034, New P: 0.398
iter 10 loss: 0.504
Actual params: [1.2751, 0.3976]
-Original Grad: -0.059, -lr * Pred Grad:  0.011, New P: 1.286
-Original Grad: 0.031, -lr * Pred Grad:  0.040, New P: 0.437
iter 11 loss: 0.490
Actual params: [1.2865, 0.4375]
-Original Grad: -0.049, -lr * Pred Grad:  0.003, New P: 1.290
-Original Grad: -0.085, -lr * Pred Grad:  0.007, New P: 0.445
iter 12 loss: 0.488
Actual params: [1.2896, 0.4447]
-Original Grad: -0.060, -lr * Pred Grad:  -0.006, New P: 1.284
-Original Grad: -0.035, -lr * Pred Grad:  -0.003, New P: 0.442
iter 13 loss: 0.488
Actual params: [1.2841, 0.4416]
-Original Grad: -0.050, -lr * Pred Grad:  -0.012, New P: 1.272
-Original Grad: -0.105, -lr * Pred Grad:  -0.027, New P: 0.414
iter 14 loss: 0.497
Actual params: [1.2721, 0.4142]
-Original Grad: -0.031, -lr * Pred Grad:  -0.015, New P: 1.257
-Original Grad: -0.083, -lr * Pred Grad:  -0.041, New P: 0.373
iter 15 loss: 0.510
Actual params: [1.2571, 0.3731]
-Original Grad: -0.052, -lr * Pred Grad:  -0.021, New P: 1.236
-Original Grad: 0.019, -lr * Pred Grad:  -0.033, New P: 0.340
iter 16 loss: 0.519
Actual params: [1.2364, 0.34  ]
-Original Grad: -0.032, -lr * Pred Grad:  -0.023, New P: 1.213
-Original Grad: 0.011, -lr * Pred Grad:  -0.028, New P: 0.313
iter 17 loss: 0.526
Actual params: [1.2133, 0.3125]
-Original Grad: -0.026, -lr * Pred Grad:  -0.025, New P: 1.189
-Original Grad: 0.015, -lr * Pred Grad:  -0.022, New P: 0.291
iter 18 loss: 0.531
Actual params: [1.1887, 0.2909]
-Original Grad: -0.020, -lr * Pred Grad:  -0.025, New P: 1.164
-Original Grad: 0.024, -lr * Pred Grad:  -0.014, New P: 0.277
iter 19 loss: 0.534
Actual params: [1.1635, 0.2768]
-Original Grad: -0.025, -lr * Pred Grad:  -0.026, New P: 1.137
-Original Grad: 0.045, -lr * Pred Grad:  -0.002, New P: 0.274
iter 20 loss: 0.534
Actual params: [1.1371, 0.2744]
-Original Grad: -0.014, -lr * Pred Grad:  -0.026, New P: 1.111
-Original Grad: 0.068, -lr * Pred Grad:  0.012, New P: 0.287
Target params: [1.1812, 0.2779]
iter 0 loss: 0.451
Actual params: [0.5941, 0.5941]
-Original Grad: 0.240, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.023, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.333
Actual params: [0.6941, 0.6941]
-Original Grad: 0.385, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: 0.031, -lr * Pred Grad:  0.100, New P: 0.794
iter 2 loss: 0.208
Actual params: [0.7927, 0.7937]
-Original Grad: -0.037, -lr * Pred Grad:  0.071, New P: 0.863
-Original Grad: -0.061, -lr * Pred Grad:  -0.012, New P: 0.781
iter 3 loss: 0.213
Actual params: [0.8634, 0.7813]
-Original Grad: -0.191, -lr * Pred Grad:  0.031, New P: 0.894
-Original Grad: -0.052, -lr * Pred Grad:  -0.042, New P: 0.739
iter 4 loss: 0.219
Actual params: [0.8944, 0.739 ]
-Original Grad: -0.139, -lr * Pred Grad:  0.010, New P: 0.905
-Original Grad: -0.096, -lr * Pred Grad:  -0.064, New P: 0.675
iter 5 loss: 0.193
Actual params: [0.9048, 0.6747]
-Original Grad: -0.072, -lr * Pred Grad:  0.002, New P: 0.906
-Original Grad: -0.039, -lr * Pred Grad:  -0.068, New P: 0.607
iter 6 loss: 0.189
Actual params: [0.9063, 0.6067]
-Original Grad: -0.087, -lr * Pred Grad:  -0.007, New P: 0.899
-Original Grad: -0.051, -lr * Pred Grad:  -0.073, New P: 0.533
iter 7 loss: 0.199
Actual params: [0.8992, 0.5333]
-Original Grad: -0.022, -lr * Pred Grad:  -0.008, New P: 0.891
-Original Grad: -0.023, -lr * Pred Grad:  -0.072, New P: 0.462
iter 8 loss: 0.224
Actual params: [0.8909, 0.4618]
-Original Grad: 0.067, -lr * Pred Grad:  -0.001, New P: 0.890
-Original Grad: 0.014, -lr * Pred Grad:  -0.059, New P: 0.403
iter 9 loss: 0.246
Actual params: [0.8898, 0.4033]
-Original Grad: 0.052, -lr * Pred Grad:  0.004, New P: 0.894
-Original Grad: -0.010, -lr * Pred Grad:  -0.055, New P: 0.348
iter 10 loss: 0.266
Actual params: [0.8936, 0.3478]
-Original Grad: 0.081, -lr * Pred Grad:  0.011, New P: 0.904
-Original Grad: 0.016, -lr * Pred Grad:  -0.044, New P: 0.304
iter 11 loss: 0.277
Actual params: [0.9042, 0.3036]
-Original Grad: 0.101, -lr * Pred Grad:  0.018, New P: 0.923
-Original Grad: 0.025, -lr * Pred Grad:  -0.031, New P: 0.273
iter 12 loss: 0.271
Actual params: [0.9226, 0.2726]
-Original Grad: 0.073, -lr * Pred Grad:  0.023, New P: 0.945
-Original Grad: 0.013, -lr * Pred Grad:  -0.024, New P: 0.249
iter 13 loss: 0.254
Actual params: [0.9454, 0.2489]
-Original Grad: 0.096, -lr * Pred Grad:  0.029, New P: 0.974
-Original Grad: 0.011, -lr * Pred Grad:  -0.018, New P: 0.231
iter 14 loss: 0.234
Actual params: [0.974 , 0.2311]
-Original Grad: 0.097, -lr * Pred Grad:  0.034, New P: 1.008
-Original Grad: 0.008, -lr * Pred Grad:  -0.014, New P: 0.218
iter 15 loss: 0.215
Actual params: [1.0078, 0.2175]
-Original Grad: 0.020, -lr * Pred Grad:  0.032, New P: 1.040
-Original Grad: -0.013, -lr * Pred Grad:  -0.016, New P: 0.201
iter 16 loss: 0.204
Actual params: [1.0402, 0.2011]
-Original Grad: 0.026, -lr * Pred Grad:  0.032, New P: 1.072
-Original Grad: -0.004, -lr * Pred Grad:  -0.016, New P: 0.185
iter 17 loss: 0.197
Actual params: [1.0718, 0.1848]
-Original Grad: -0.022, -lr * Pred Grad:  0.027, New P: 1.099
-Original Grad: -0.015, -lr * Pred Grad:  -0.020, New P: 0.165
iter 18 loss: 0.195
Actual params: [1.0986, 0.1653]
-Original Grad: -0.016, -lr * Pred Grad:  0.023, New P: 1.122
-Original Grad: -0.014, -lr * Pred Grad:  -0.022, New P: 0.143
iter 19 loss: 0.198
Actual params: [1.1215, 0.1429]
-Original Grad: 0.026, -lr * Pred Grad:  0.023, New P: 1.145
-Original Grad: -0.006, -lr * Pred Grad:  -0.022, New P: 0.121
iter 20 loss: 0.209
Actual params: [1.1447, 0.1206]
-Original Grad: -0.028, -lr * Pred Grad:  0.018, New P: 1.163
-Original Grad: 0.004, -lr * Pred Grad:  -0.019, New P: 0.102
Target params: [1.1812, 0.2779]
iter 0 loss: 0.334
Actual params: [0.5941, 0.5941]
-Original Grad: 0.015, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.217, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.283
Actual params: [0.6941, 0.4941]
-Original Grad: 0.004, -lr * Pred Grad:  0.083, New P: 0.777
-Original Grad: -0.072, -lr * Pred Grad:  -0.087, New P: 0.407
iter 2 loss: 0.255
Actual params: [0.7769, 0.4071]
-Original Grad: 0.003, -lr * Pred Grad:  0.076, New P: 0.853
-Original Grad: -0.040, -lr * Pred Grad:  -0.077, New P: 0.330
iter 3 loss: 0.243
Actual params: [0.8526, 0.3298]
-Original Grad: -0.005, -lr * Pred Grad:  0.040, New P: 0.893
-Original Grad: -0.011, -lr * Pred Grad:  -0.066, New P: 0.264
iter 4 loss: 0.242
Actual params: [0.8926, 0.2638]
-Original Grad: 0.002, -lr * Pred Grad:  0.040, New P: 0.933
-Original Grad: -0.003, -lr * Pred Grad:  -0.057, New P: 0.207
iter 5 loss: 0.240
Actual params: [0.9325, 0.2072]
-Original Grad: -0.007, -lr * Pred Grad:  0.013, New P: 0.946
-Original Grad: 0.005, -lr * Pred Grad:  -0.048, New P: 0.160
iter 6 loss: 0.239
Actual params: [0.9457, 0.1597]
-Original Grad: 0.015, -lr * Pred Grad:  0.041, New P: 0.987
-Original Grad: 0.000, -lr * Pred Grad:  -0.042, New P: 0.118
iter 7 loss: 0.235
Actual params: [0.9868, 0.1182]
-Original Grad: 0.022, -lr * Pred Grad:  0.060, New P: 1.047
-Original Grad: 0.008, -lr * Pred Grad:  -0.035, New P: 0.083
iter 8 loss: 0.239
Actual params: [1.0473, 0.0832]
-Original Grad: 0.032, -lr * Pred Grad:  0.073, New P: 1.120
-Original Grad: 0.015, -lr * Pred Grad:  -0.028, New P: 0.055
iter 9 loss: 0.241
Actual params: [1.1198, 0.0553]
-Original Grad: 0.008, -lr * Pred Grad:  0.072, New P: 1.192
-Original Grad: 0.018, -lr * Pred Grad:  -0.021, New P: 0.034
iter 10 loss: 0.243
Actual params: [1.1918, 0.0342]
-Original Grad: -0.002, -lr * Pred Grad:  0.062, New P: 1.254
-Original Grad: 0.022, -lr * Pred Grad:  -0.014, New P: 0.020
iter 11 loss: 0.244
Actual params: [1.2541, 0.0198]
-Original Grad: 0.001, -lr * Pred Grad:  0.057, New P: 1.311
-Original Grad: 0.026, -lr * Pred Grad:  -0.007, New P: 0.012
iter 12 loss: 0.244
Actual params: [1.3112, 0.0124]
-Original Grad: -0.003, -lr * Pred Grad:  0.048, New P: 1.359
-Original Grad: 0.022, -lr * Pred Grad:  -0.002, New P: 0.010
iter 13 loss: 0.245
Actual params: [1.3594, 0.0103]
-Original Grad: -0.005, -lr * Pred Grad:  0.038, New P: 1.397
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: 0.008
iter 14 loss: 0.245
Actual params: [1.397 , 0.0084]
-Original Grad: -0.005, -lr * Pred Grad:  0.029, New P: 1.426
-Original Grad: 0.016, -lr * Pred Grad:  0.001, New P: 0.010
iter 15 loss: 0.246
Actual params: [1.426 , 0.0099]
-Original Grad: -0.005, -lr * Pred Grad:  0.021, New P: 1.447
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.011
iter 16 loss: 0.246
Actual params: [1.4466, 0.0113]
-Original Grad: -0.006, -lr * Pred Grad:  0.012, New P: 1.458
-Original Grad: -0.003, -lr * Pred Grad:  0.001, New P: 0.012
iter 17 loss: 0.247
Actual params: [1.4585, 0.0118]
-Original Grad: -0.005, -lr * Pred Grad:  0.005, New P: 1.464
-Original Grad: 0.030, -lr * Pred Grad:  0.007, New P: 0.019
iter 18 loss: 0.247
Actual params: [1.464 , 0.0186]
-Original Grad: -0.003, -lr * Pred Grad:  0.002, New P: 1.466
-Original Grad: 0.013, -lr * Pred Grad:  0.009, New P: 0.027
iter 19 loss: 0.248
Actual params: [1.4657, 0.0275]
-Original Grad: -0.005, -lr * Pred Grad:  -0.004, New P: 1.462
-Original Grad: 0.018, -lr * Pred Grad:  0.012, New P: 0.039
iter 20 loss: 0.250
Actual params: [1.4618, 0.0393]
-Original Grad: -0.006, -lr * Pred Grad:  -0.010, New P: 1.452
-Original Grad: 0.008, -lr * Pred Grad:  0.013, New P: 0.052
Target params: [1.1812, 0.2779]
iter 0 loss: 0.244
Actual params: [0.5941, 0.5941]
-Original Grad: 0.046, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.272, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.228
Actual params: [0.6941, 0.4941]
-Original Grad: 0.020, -lr * Pred Grad:  0.091, New P: 0.785
-Original Grad: -0.072, -lr * Pred Grad:  -0.084, New P: 0.410
iter 2 loss: 0.229
Actual params: [0.785 , 0.4102]
-Original Grad: 0.015, -lr * Pred Grad:  0.086, New P: 0.871
-Original Grad: -0.034, -lr * Pred Grad:  -0.072, New P: 0.338
iter 3 loss: 0.231
Actual params: [0.8707, 0.3381]
-Original Grad: 0.016, -lr * Pred Grad:  0.084, New P: 0.954
-Original Grad: -0.009, -lr * Pred Grad:  -0.061, New P: 0.277
iter 4 loss: 0.236
Actual params: [0.9545, 0.2772]
-Original Grad: 0.003, -lr * Pred Grad:  0.074, New P: 1.029
-Original Grad: 0.003, -lr * Pred Grad:  -0.051, New P: 0.226
iter 5 loss: 0.242
Actual params: [1.0286, 0.2263]
-Original Grad: -0.002, -lr * Pred Grad:  0.062, New P: 1.091
-Original Grad: 0.005, -lr * Pred Grad:  -0.043, New P: 0.183
iter 6 loss: 0.248
Actual params: [1.091 , 0.1833]
-Original Grad: 0.002, -lr * Pred Grad:  0.056, New P: 1.147
-Original Grad: 0.005, -lr * Pred Grad:  -0.037, New P: 0.147
iter 7 loss: 0.253
Actual params: [1.1472, 0.1468]
-Original Grad: -0.004, -lr * Pred Grad:  0.046, New P: 1.193
-Original Grad: 0.007, -lr * Pred Grad:  -0.031, New P: 0.116
iter 8 loss: 0.257
Actual params: [1.1929, 0.1157]
-Original Grad: -0.009, -lr * Pred Grad:  0.032, New P: 1.225
-Original Grad: 0.014, -lr * Pred Grad:  -0.025, New P: 0.091
iter 9 loss: 0.261
Actual params: [1.2249, 0.0905]
-Original Grad: -0.013, -lr * Pred Grad:  0.017, New P: 1.242
-Original Grad: 0.031, -lr * Pred Grad:  -0.017, New P: 0.073
iter 10 loss: 0.263
Actual params: [1.242 , 0.0734]
-Original Grad: -0.013, -lr * Pred Grad:  0.005, New P: 1.247
-Original Grad: 0.020, -lr * Pred Grad:  -0.012, New P: 0.061
iter 11 loss: 0.263
Actual params: [1.2467, 0.0615]
-Original Grad: -0.018, -lr * Pred Grad:  -0.010, New P: 1.237
-Original Grad: 0.019, -lr * Pred Grad:  -0.008, New P: 0.054
iter 12 loss: 0.262
Actual params: [1.2368, 0.054 ]
-Original Grad: -0.016, -lr * Pred Grad:  -0.021, New P: 1.216
-Original Grad: 0.017, -lr * Pred Grad:  -0.004, New P: 0.050
iter 13 loss: 0.260
Actual params: [1.2159, 0.0501]
-Original Grad: -0.009, -lr * Pred Grad:  -0.026, New P: 1.190
-Original Grad: 0.015, -lr * Pred Grad:  -0.001, New P: 0.049
iter 14 loss: 0.257
Actual params: [1.1901, 0.049 ]
-Original Grad: -0.013, -lr * Pred Grad:  -0.032, New P: 1.158
-Original Grad: 0.024, -lr * Pred Grad:  0.003, New P: 0.052
iter 15 loss: 0.253
Actual params: [1.1577, 0.052 ]
-Original Grad: -0.010, -lr * Pred Grad:  -0.036, New P: 1.121
-Original Grad: 0.018, -lr * Pred Grad:  0.006, New P: 0.058
iter 16 loss: 0.249
Actual params: [1.1215, 0.0579]
-Original Grad: -0.007, -lr * Pred Grad:  -0.038, New P: 1.084
-Original Grad: 0.013, -lr * Pred Grad:  0.007, New P: 0.065
iter 17 loss: 0.245
Actual params: [1.0838, 0.0653]
-Original Grad: 0.003, -lr * Pred Grad:  -0.032, New P: 1.052
-Original Grad: 0.027, -lr * Pred Grad:  0.011, New P: 0.077
iter 18 loss: 0.242
Actual params: [1.0521, 0.0766]
-Original Grad: 0.003, -lr * Pred Grad:  -0.026, New P: 1.026
-Original Grad: 0.016, -lr * Pred Grad:  0.013, New P: 0.090
iter 19 loss: 0.240
Actual params: [1.0257, 0.0898]
-Original Grad: -0.005, -lr * Pred Grad:  -0.028, New P: 0.998
-Original Grad: 0.012, -lr * Pred Grad:  0.014, New P: 0.104
iter 20 loss: 0.238
Actual params: [0.9979, 0.1038]
-Original Grad: 0.002, -lr * Pred Grad:  -0.023, New P: 0.974
-Original Grad: 0.033, -lr * Pred Grad:  0.019, New P: 0.122
Target params: [1.1812, 0.2779]
iter 0 loss: 0.336
Actual params: [0.5941, 0.5941]
-Original Grad: 0.267, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.214, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.227
Actual params: [0.6941, 0.4941]
-Original Grad: 0.030, -lr * Pred Grad:  0.075, New P: 0.769
-Original Grad: -0.028, -lr * Pred Grad:  -0.076, New P: 0.418
iter 2 loss: 0.164
Actual params: [0.7689, 0.418 ]
-Original Grad: -0.019, -lr * Pred Grad:  0.053, New P: 0.822
-Original Grad: 0.004, -lr * Pred Grad:  -0.057, New P: 0.361
iter 3 loss: 0.148
Actual params: [0.8221, 0.3606]
-Original Grad: 0.006, -lr * Pred Grad:  0.045, New P: 0.867
-Original Grad: 0.010, -lr * Pred Grad:  -0.044, New P: 0.316
iter 4 loss: 0.144
Actual params: [0.867 , 0.3163]
-Original Grad: 0.027, -lr * Pred Grad:  0.043, New P: 0.910
-Original Grad: -0.001, -lr * Pred Grad:  -0.038, New P: 0.279
iter 5 loss: 0.139
Actual params: [0.9101, 0.2788]
-Original Grad: 0.019, -lr * Pred Grad:  0.041, New P: 0.951
-Original Grad: 0.026, -lr * Pred Grad:  -0.026, New P: 0.253
iter 6 loss: 0.136
Actual params: [0.9508, 0.2528]
-Original Grad: 0.031, -lr * Pred Grad:  0.041, New P: 0.992
-Original Grad: 0.005, -lr * Pred Grad:  -0.022, New P: 0.231
iter 7 loss: 0.129
Actual params: [0.9918, 0.2312]
-Original Grad: 0.057, -lr * Pred Grad:  0.046, New P: 1.037
-Original Grad: 0.020, -lr * Pred Grad:  -0.014, New P: 0.217
iter 8 loss: 0.123
Actual params: [1.0374, 0.2168]
-Original Grad: 0.048, -lr * Pred Grad:  0.048, New P: 1.086
-Original Grad: 0.001, -lr * Pred Grad:  -0.013, New P: 0.204
iter 9 loss: 0.119
Actual params: [1.0857, 0.2042]
-Original Grad: 0.058, -lr * Pred Grad:  0.052, New P: 1.138
-Original Grad: -0.003, -lr * Pred Grad:  -0.012, New P: 0.192
iter 10 loss: 0.117
Actual params: [1.1377, 0.1923]
-Original Grad: 0.018, -lr * Pred Grad:  0.050, New P: 1.187
-Original Grad: 0.007, -lr * Pred Grad:  -0.009, New P: 0.183
iter 11 loss: 0.118
Actual params: [1.1873, 0.1831]
-Original Grad: -0.007, -lr * Pred Grad:  0.043, New P: 1.231
-Original Grad: 0.014, -lr * Pred Grad:  -0.005, New P: 0.178
iter 12 loss: 0.117
Actual params: [1.2306, 0.1781]
-Original Grad: -0.004, -lr * Pred Grad:  0.038, New P: 1.269
-Original Grad: 0.003, -lr * Pred Grad:  -0.004, New P: 0.174
iter 13 loss: 0.119
Actual params: [1.2691, 0.1744]
-Original Grad: -0.012, -lr * Pred Grad:  0.033, New P: 1.302
-Original Grad: 0.009, -lr * Pred Grad:  -0.001, New P: 0.173
iter 14 loss: 0.122
Actual params: [1.3018, 0.1729]
-Original Grad: -0.017, -lr * Pred Grad:  0.027, New P: 1.329
-Original Grad: 0.008, -lr * Pred Grad:  0.000, New P: 0.173
iter 15 loss: 0.124
Actual params: [1.3285, 0.1732]
-Original Grad: -0.009, -lr * Pred Grad:  0.023, New P: 1.351
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: 0.174
iter 16 loss: 0.127
Actual params: [1.3513, 0.1738]
-Original Grad: -0.002, -lr * Pred Grad:  0.020, New P: 1.372
-Original Grad: 0.032, -lr * Pred Grad:  0.008, New P: 0.182
iter 17 loss: 0.129
Actual params: [1.3715, 0.1816]
-Original Grad: -0.007, -lr * Pred Grad:  0.017, New P: 1.389
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.189
iter 18 loss: 0.131
Actual params: [1.3886, 0.189 ]
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 1.404
-Original Grad: 0.004, -lr * Pred Grad:  0.008, New P: 0.197
iter 19 loss: 0.132
Actual params: [1.4043, 0.1967]
-Original Grad: -0.003, -lr * Pred Grad:  0.014, New P: 1.418
-Original Grad: 0.002, -lr * Pred Grad:  0.007, New P: 0.204
iter 20 loss: 0.134
Actual params: [1.418 , 0.2041]
-Original Grad: -0.001, -lr * Pred Grad:  0.012, New P: 1.430
-Original Grad: 0.011, -lr * Pred Grad:  0.009, New P: 0.213
Target params: [1.1812, 0.2779]
iter 0 loss: 0.252
Actual params: [0.5941, 0.5941]
-Original Grad: 0.101, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.123, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.182
Actual params: [0.6941, 0.4941]
-Original Grad: 0.063, -lr * Pred Grad:  0.096, New P: 0.790
-Original Grad: -0.093, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.167
Actual params: [0.7903, 0.3957]
-Original Grad: -0.026, -lr * Pred Grad:  0.059, New P: 0.850
-Original Grad: -0.020, -lr * Pred Grad:  -0.084, New P: 0.312
iter 3 loss: 0.149
Actual params: [0.8496, 0.312 ]
-Original Grad: -0.010, -lr * Pred Grad:  0.044, New P: 0.893
-Original Grad: -0.019, -lr * Pred Grad:  -0.075, New P: 0.237
iter 4 loss: 0.134
Actual params: [0.8932, 0.2368]
-Original Grad: 0.019, -lr * Pred Grad:  0.045, New P: 0.938
-Original Grad: -0.002, -lr * Pred Grad:  -0.064, New P: 0.172
iter 5 loss: 0.125
Actual params: [0.9379, 0.1724]
-Original Grad: 0.018, -lr * Pred Grad:  0.046, New P: 0.983
-Original Grad: 0.001, -lr * Pred Grad:  -0.055, New P: 0.117
iter 6 loss: 0.117
Actual params: [0.9835, 0.1172]
-Original Grad: 0.015, -lr * Pred Grad:  0.046, New P: 1.029
-Original Grad: 0.005, -lr * Pred Grad:  -0.047, New P: 0.070
iter 7 loss: 0.110
Actual params: [1.0291, 0.0705]
-Original Grad: 0.023, -lr * Pred Grad:  0.049, New P: 1.078
-Original Grad: 0.004, -lr * Pred Grad:  -0.040, New P: 0.031
iter 8 loss: 0.104
Actual params: [1.0777, 0.0307]
-Original Grad: 0.016, -lr * Pred Grad:  0.049, New P: 1.126
-Original Grad: 0.011, -lr * Pred Grad:  -0.032, New P: -0.001
iter 9 loss: 0.100
Actual params: [ 1.1265, -0.0012]
-Original Grad: 0.013, -lr * Pred Grad:  0.048, New P: 1.175
-Original Grad: 0.008, -lr * Pred Grad:  -0.026, New P: -0.027
iter 10 loss: 0.098
Actual params: [ 1.1746, -0.0273]
-Original Grad: 0.017, -lr * Pred Grad:  0.049, New P: 1.224
-Original Grad: 0.007, -lr * Pred Grad:  -0.021, New P: -0.048
iter 11 loss: 0.097
Actual params: [ 1.2236, -0.0484]
-Original Grad: 0.003, -lr * Pred Grad:  0.045, New P: 1.269
-Original Grad: 0.015, -lr * Pred Grad:  -0.014, New P: -0.063
iter 12 loss: 0.097
Actual params: [ 1.2687, -0.0626]
-Original Grad: 0.010, -lr * Pred Grad:  0.044, New P: 1.313
-Original Grad: 0.002, -lr * Pred Grad:  -0.012, New P: -0.075
iter 13 loss: 0.097
Actual params: [ 1.313 , -0.0747]
-Original Grad: -0.004, -lr * Pred Grad:  0.039, New P: 1.352
-Original Grad: 0.013, -lr * Pred Grad:  -0.007, New P: -0.082
iter 14 loss: 0.097
Actual params: [ 1.3516, -0.0819]
-Original Grad: 0.003, -lr * Pred Grad:  0.036, New P: 1.388
-Original Grad: 0.006, -lr * Pred Grad:  -0.004, New P: -0.086
iter 15 loss: 0.098
Actual params: [ 1.3875, -0.0864]
-Original Grad: -0.003, -lr * Pred Grad:  0.031, New P: 1.419
-Original Grad: 0.005, -lr * Pred Grad:  -0.003, New P: -0.089
iter 16 loss: 0.099
Actual params: [ 1.4189, -0.0889]
-Original Grad: -0.001, -lr * Pred Grad:  0.028, New P: 1.447
-Original Grad: 0.002, -lr * Pred Grad:  -0.002, New P: -0.091
iter 17 loss: 0.099
Actual params: [ 1.4471, -0.0907]
-Original Grad: 0.003, -lr * Pred Grad:  0.027, New P: 1.474
-Original Grad: 0.002, -lr * Pred Grad:  -0.001, New P: -0.092
iter 18 loss: 0.100
Actual params: [ 1.4739, -0.0916]
-Original Grad: -0.004, -lr * Pred Grad:  0.023, New P: 1.497
-Original Grad: 0.007, -lr * Pred Grad:  0.001, New P: -0.090
iter 19 loss: 0.100
Actual params: [ 1.4965, -0.0902]
-Original Grad: -0.007, -lr * Pred Grad:  0.018, New P: 1.514
-Original Grad: -0.002, -lr * Pred Grad:  0.001, New P: -0.089
iter 20 loss: 0.101
Actual params: [ 1.5143, -0.0894]
-Original Grad: -0.003, -lr * Pred Grad:  0.015, New P: 1.529
-Original Grad: 0.002, -lr * Pred Grad:  0.001, New P: -0.088
Target params: [1.1812, 0.2779]
iter 0 loss: 1.004
Actual params: [0.5941, 0.5941]
-Original Grad: 0.311, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.474, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.896
Actual params: [0.6941, 0.4941]
-Original Grad: 0.368, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.345, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.761
Actual params: [0.7941, 0.3961]
-Original Grad: 0.283, -lr * Pred Grad:  0.099, New P: 0.893
-Original Grad: -0.181, -lr * Pred Grad:  -0.091, New P: 0.305
iter 3 loss: 0.618
Actual params: [0.8932, 0.3049]
-Original Grad: 0.193, -lr * Pred Grad:  0.096, New P: 0.989
-Original Grad: -0.103, -lr * Pred Grad:  -0.083, New P: 0.222
iter 4 loss: 0.515
Actual params: [0.9889, 0.2216]
-Original Grad: 0.033, -lr * Pred Grad:  0.084, New P: 1.073
-Original Grad: -0.038, -lr * Pred Grad:  -0.074, New P: 0.148
iter 5 loss: 0.484
Actual params: [1.0727, 0.148 ]
-Original Grad: -0.021, -lr * Pred Grad:  0.070, New P: 1.143
-Original Grad: -0.015, -lr * Pred Grad:  -0.065, New P: 0.083
iter 6 loss: 0.494
Actual params: [1.143 , 0.0832]
-Original Grad: -0.030, -lr * Pred Grad:  0.059, New P: 1.202
-Original Grad: -0.001, -lr * Pred Grad:  -0.057, New P: 0.027
iter 7 loss: 0.520
Actual params: [1.2017, 0.0266]
-Original Grad: -0.056, -lr * Pred Grad:  0.047, New P: 1.249
-Original Grad: 0.008, -lr * Pred Grad:  -0.049, New P: -0.023
iter 8 loss: 0.546
Actual params: [ 1.2485, -0.0227]
-Original Grad: -0.055, -lr * Pred Grad:  0.037, New P: 1.285
-Original Grad: 0.010, -lr * Pred Grad:  -0.043, New P: -0.066
iter 9 loss: 0.564
Actual params: [ 1.2854, -0.0656]
-Original Grad: -0.072, -lr * Pred Grad:  0.027, New P: 1.312
-Original Grad: 0.017, -lr * Pred Grad:  -0.037, New P: -0.103
iter 10 loss: 0.580
Actual params: [ 1.3122, -0.1026]
-Original Grad: -0.047, -lr * Pred Grad:  0.020, New P: 1.332
-Original Grad: 0.014, -lr * Pred Grad:  -0.032, New P: -0.135
iter 11 loss: 0.592
Actual params: [ 1.3325, -0.1345]
-Original Grad: -0.041, -lr * Pred Grad:  0.015, New P: 1.347
-Original Grad: 0.010, -lr * Pred Grad:  -0.028, New P: -0.163
iter 12 loss: 0.599
Actual params: [ 1.3474, -0.1625]
-Original Grad: -0.043, -lr * Pred Grad:  0.010, New P: 1.357
-Original Grad: 0.016, -lr * Pred Grad:  -0.024, New P: -0.186
iter 13 loss: 0.603
Actual params: [ 1.3573, -0.1864]
-Original Grad: -0.045, -lr * Pred Grad:  0.005, New P: 1.363
-Original Grad: 0.026, -lr * Pred Grad:  -0.020, New P: -0.206
iter 14 loss: 0.605
Actual params: [ 1.3626, -0.206 ]
-Original Grad: -0.030, -lr * Pred Grad:  0.002, New P: 1.365
-Original Grad: 0.008, -lr * Pred Grad:  -0.017, New P: -0.223
iter 15 loss: 0.606
Actual params: [ 1.3651, -0.2231]
-Original Grad: -0.046, -lr * Pred Grad:  -0.001, New P: 1.364
-Original Grad: 0.025, -lr * Pred Grad:  -0.013, New P: -0.237
iter 16 loss: 0.605
Actual params: [ 1.3636, -0.2366]
-Original Grad: -0.043, -lr * Pred Grad:  -0.005, New P: 1.359
-Original Grad: 0.020, -lr * Pred Grad:  -0.011, New P: -0.247
iter 17 loss: 0.602
Actual params: [ 1.3588, -0.2472]
-Original Grad: -0.042, -lr * Pred Grad:  -0.008, New P: 1.351
-Original Grad: 0.025, -lr * Pred Grad:  -0.008, New P: -0.255
iter 18 loss: 0.598
Actual params: [ 1.351 , -0.2548]
-Original Grad: -0.047, -lr * Pred Grad:  -0.011, New P: 1.340
-Original Grad: 0.027, -lr * Pred Grad:  -0.005, New P: -0.260
iter 19 loss: 0.592
Actual params: [ 1.3401, -0.2596]
-Original Grad: -0.057, -lr * Pred Grad:  -0.015, New P: 1.325
-Original Grad: 0.031, -lr * Pred Grad:  -0.002, New P: -0.261
iter 20 loss: 0.583
Actual params: [ 1.3254, -0.2613]
-Original Grad: -0.039, -lr * Pred Grad:  -0.017, New P: 1.309
-Original Grad: 0.015, -lr * Pred Grad:  -0.000, New P: -0.262
Target params: [1.1812, 0.2779]
iter 0 loss: 0.604
Actual params: [0.5941, 0.5941]
-Original Grad: 0.082, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.444, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.450
Actual params: [0.6941, 0.4941]
-Original Grad: 0.052, -lr * Pred Grad:  0.096, New P: 0.790
-Original Grad: -0.355, -lr * Pred Grad:  -0.099, New P: 0.395
iter 2 loss: 0.357
Actual params: [0.7904, 0.3952]
-Original Grad: 0.030, -lr * Pred Grad:  0.090, New P: 0.880
-Original Grad: -0.192, -lr * Pred Grad:  -0.093, New P: 0.302
iter 3 loss: 0.294
Actual params: [0.8804, 0.3024]
-Original Grad: 0.008, -lr * Pred Grad:  0.078, New P: 0.959
-Original Grad: -0.025, -lr * Pred Grad:  -0.078, New P: 0.224
iter 4 loss: 0.267
Actual params: [0.9586, 0.224 ]
-Original Grad: 0.010, -lr * Pred Grad:  0.071, New P: 1.030
-Original Grad: 0.057, -lr * Pred Grad:  -0.061, New P: 0.163
iter 5 loss: 0.258
Actual params: [1.0296, 0.1632]
-Original Grad: 0.044, -lr * Pred Grad:  0.077, New P: 1.107
-Original Grad: 0.032, -lr * Pred Grad:  -0.050, New P: 0.114
iter 6 loss: 0.265
Actual params: [1.1066, 0.1136]
-Original Grad: 0.015, -lr * Pred Grad:  0.073, New P: 1.180
-Original Grad: 0.014, -lr * Pred Grad:  -0.042, New P: 0.071
iter 7 loss: 0.279
Actual params: [1.18  , 0.0714]
-Original Grad: 0.004, -lr * Pred Grad:  0.067, New P: 1.247
-Original Grad: 0.010, -lr * Pred Grad:  -0.036, New P: 0.035
iter 8 loss: 0.286
Actual params: [1.2466, 0.035 ]
-Original Grad: -0.000, -lr * Pred Grad:  0.059, New P: 1.306
-Original Grad: 0.010, -lr * Pred Grad:  -0.031, New P: 0.004
iter 9 loss: 0.285
Actual params: [1.3057, 0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  0.052, New P: 1.358
-Original Grad: 0.013, -lr * Pred Grad:  -0.027, New P: -0.024
iter 10 loss: 0.286
Actual params: [ 1.3578, -0.0236]
-Original Grad: -0.004, -lr * Pred Grad:  0.045, New P: 1.403
-Original Grad: 0.008, -lr * Pred Grad:  -0.024, New P: -0.047
iter 11 loss: 0.287
Actual params: [ 1.403 , -0.0472]
-Original Grad: -0.002, -lr * Pred Grad:  0.040, New P: 1.443
-Original Grad: 0.018, -lr * Pred Grad:  -0.020, New P: -0.067
iter 12 loss: 0.289
Actual params: [ 1.4429, -0.067 ]
-Original Grad: -0.002, -lr * Pred Grad:  0.035, New P: 1.478
-Original Grad: 0.012, -lr * Pred Grad:  -0.017, New P: -0.084
iter 13 loss: 0.291
Actual params: [ 1.4779, -0.0839]
-Original Grad: -0.009, -lr * Pred Grad:  0.027, New P: 1.505
-Original Grad: 0.009, -lr * Pred Grad:  -0.015, New P: -0.098
iter 14 loss: 0.292
Actual params: [ 1.5054, -0.0984]
-Original Grad: -0.008, -lr * Pred Grad:  0.021, New P: 1.527
-Original Grad: 0.013, -lr * Pred Grad:  -0.012, New P: -0.110
iter 15 loss: 0.293
Actual params: [ 1.5267, -0.1104]
-Original Grad: -0.007, -lr * Pred Grad:  0.016, New P: 1.543
-Original Grad: 0.020, -lr * Pred Grad:  -0.009, New P: -0.120
iter 16 loss: 0.293
Actual params: [ 1.543 , -0.1197]
-Original Grad: -0.008, -lr * Pred Grad:  0.011, New P: 1.554
-Original Grad: 0.010, -lr * Pred Grad:  -0.008, New P: -0.127
iter 17 loss: 0.293
Actual params: [ 1.5544, -0.1273]
-Original Grad: -0.011, -lr * Pred Grad:  0.006, New P: 1.560
-Original Grad: 0.022, -lr * Pred Grad:  -0.005, New P: -0.132
iter 18 loss: 0.293
Actual params: [ 1.56  , -0.1323]
-Original Grad: -0.007, -lr * Pred Grad:  0.002, New P: 1.562
-Original Grad: 0.029, -lr * Pred Grad:  -0.002, New P: -0.134
iter 19 loss: 0.293
Actual params: [ 1.5623, -0.1344]
-Original Grad: -0.006, -lr * Pred Grad:  -0.001, New P: 1.562
-Original Grad: 0.019, -lr * Pred Grad:  -0.000, New P: -0.135
iter 20 loss: 0.293
Actual params: [ 1.5618, -0.1346]
-Original Grad: -0.010, -lr * Pred Grad:  -0.005, New P: 1.557
-Original Grad: 0.020, -lr * Pred Grad:  0.001, New P: -0.133
Target params: [1.1812, 0.2779]
iter 0 loss: 0.267
Actual params: [0.5941, 0.5941]
-Original Grad: 0.167, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.010, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.245
Actual params: [0.6941, 0.6941]
-Original Grad: 0.173, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.013, -lr * Pred Grad:  -0.019, New P: 0.675
iter 2 loss: 0.276
Actual params: [0.7941, 0.6749]
-Original Grad: -0.054, -lr * Pred Grad:  0.062, New P: 0.856
-Original Grad: -0.056, -lr * Pred Grad:  -0.065, New P: 0.609
iter 3 loss: 0.272
Actual params: [0.8557, 0.6094]
-Original Grad: -0.153, -lr * Pred Grad:  0.012, New P: 0.868
-Original Grad: -0.060, -lr * Pred Grad:  -0.079, New P: 0.530
iter 4 loss: 0.238
Actual params: [0.8678, 0.5304]
-Original Grad: -0.135, -lr * Pred Grad:  -0.014, New P: 0.854
-Original Grad: -0.028, -lr * Pred Grad:  -0.081, New P: 0.450
iter 5 loss: 0.215
Actual params: [0.8541, 0.4498]
-Original Grad: -0.136, -lr * Pred Grad:  -0.031, New P: 0.823
-Original Grad: -0.034, -lr * Pred Grad:  -0.084, New P: 0.366
iter 6 loss: 0.213
Actual params: [0.8227, 0.3662]
-Original Grad: -0.040, -lr * Pred Grad:  -0.033, New P: 0.790
-Original Grad: -0.028, -lr * Pred Grad:  -0.084, New P: 0.282
iter 7 loss: 0.227
Actual params: [0.7898, 0.2818]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: 0.761
-Original Grad: -0.020, -lr * Pred Grad:  -0.083, New P: 0.199
iter 8 loss: 0.245
Actual params: [0.7607, 0.1991]
-Original Grad: 0.086, -lr * Pred Grad:  -0.013, New P: 0.747
-Original Grad: -0.007, -lr * Pred Grad:  -0.077, New P: 0.123
iter 9 loss: 0.253
Actual params: [0.7473, 0.1225]
-Original Grad: 0.119, -lr * Pred Grad:  0.004, New P: 0.751
-Original Grad: -0.020, -lr * Pred Grad:  -0.077, New P: 0.046
iter 10 loss: 0.251
Actual params: [0.7512, 0.0459]
-Original Grad: 0.128, -lr * Pred Grad:  0.019, New P: 0.770
-Original Grad: -0.011, -lr * Pred Grad:  -0.073, New P: -0.027
iter 11 loss: 0.242
Actual params: [ 0.77  , -0.0274]
-Original Grad: 0.027, -lr * Pred Grad:  0.020, New P: 0.790
-Original Grad: -0.006, -lr * Pred Grad:  -0.069, New P: -0.096
iter 12 loss: 0.234
Actual params: [ 0.79 , -0.096]
-Original Grad: -0.038, -lr * Pred Grad:  0.014, New P: 0.804
-Original Grad: -0.001, -lr * Pred Grad:  -0.062, New P: -0.158
iter 13 loss: 0.228
Actual params: [ 0.8035, -0.1581]
-Original Grad: -0.033, -lr * Pred Grad:  0.008, New P: 0.812
-Original Grad: -0.003, -lr * Pred Grad:  -0.057, New P: -0.216
iter 14 loss: 0.223
Actual params: [ 0.8118, -0.2155]
-Original Grad: -0.081, -lr * Pred Grad:  -0.002, New P: 0.809
-Original Grad: 0.000, -lr * Pred Grad:  -0.052, New P: -0.267
iter 15 loss: 0.223
Actual params: [ 0.8095, -0.2675]
-Original Grad: -0.082, -lr * Pred Grad:  -0.012, New P: 0.798
-Original Grad: 0.002, -lr * Pred Grad:  -0.046, New P: -0.314
iter 16 loss: 0.227
Actual params: [ 0.7979, -0.3138]
-Original Grad: -0.031, -lr * Pred Grad:  -0.014, New P: 0.784
-Original Grad: 0.003, -lr * Pred Grad:  -0.041, New P: -0.354
iter 17 loss: 0.230
Actual params: [ 0.7837, -0.3544]
-Original Grad: -0.040, -lr * Pred Grad:  -0.017, New P: 0.766
-Original Grad: -0.001, -lr * Pred Grad:  -0.037, New P: -0.392
iter 18 loss: 0.235
Actual params: [ 0.7663, -0.3916]
-Original Grad: 0.005, -lr * Pred Grad:  -0.015, New P: 0.751
-Original Grad: -0.002, -lr * Pred Grad:  -0.035, New P: -0.427
iter 19 loss: 0.239
Actual params: [ 0.7509, -0.4265]
-Original Grad: 0.102, -lr * Pred Grad:  -0.002, New P: 0.749
-Original Grad: -0.004, -lr * Pred Grad:  -0.034, New P: -0.460
iter 20 loss: 0.239
Actual params: [ 0.7494, -0.4605]
-Original Grad: 0.037, -lr * Pred Grad:  0.003, New P: 0.752
-Original Grad: -0.004, -lr * Pred Grad:  -0.033, New P: -0.493
Target params: [1.1812, 0.2779]
iter 0 loss: 0.722
Actual params: [0.5941, 0.5941]
-Original Grad: 0.120, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.540, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.653
Actual params: [0.6941, 0.4941]
-Original Grad: 0.170, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.342, -lr * Pred Grad:  -0.096, New P: 0.398
iter 2 loss: 0.522
Actual params: [0.7935, 0.3976]
-Original Grad: 0.221, -lr * Pred Grad:  0.099, New P: 0.893
-Original Grad: -0.358, -lr * Pred Grad:  -0.096, New P: 0.301
iter 3 loss: 0.290
Actual params: [0.8927, 0.3013]
-Original Grad: 0.134, -lr * Pred Grad:  0.098, New P: 0.991
-Original Grad: -0.102, -lr * Pred Grad:  -0.086, New P: 0.215
iter 4 loss: 0.174
Actual params: [0.9905, 0.2152]
-Original Grad: 0.045, -lr * Pred Grad:  0.089, New P: 1.080
-Original Grad: 0.025, -lr * Pred Grad:  -0.071, New P: 0.144
iter 5 loss: 0.171
Actual params: [1.0798, 0.1443]
-Original Grad: 0.071, -lr * Pred Grad:  0.086, New P: 1.166
-Original Grad: 0.038, -lr * Pred Grad:  -0.058, New P: 0.086
iter 6 loss: 0.173
Actual params: [1.1659, 0.0859]
-Original Grad: 0.100, -lr * Pred Grad:  0.086, New P: 1.252
-Original Grad: 0.031, -lr * Pred Grad:  -0.049, New P: 0.037
iter 7 loss: 0.163
Actual params: [1.2523, 0.0372]
-Original Grad: 0.058, -lr * Pred Grad:  0.083, New P: 1.335
-Original Grad: 0.013, -lr * Pred Grad:  -0.042, New P: -0.005
iter 8 loss: 0.174
Actual params: [ 1.3354, -0.005 ]
-Original Grad: 0.025, -lr * Pred Grad:  0.077, New P: 1.412
-Original Grad: 0.000, -lr * Pred Grad:  -0.037, New P: -0.042
iter 9 loss: 0.184
Actual params: [ 1.4124, -0.0424]
-Original Grad: 0.011, -lr * Pred Grad:  0.070, New P: 1.483
-Original Grad: -0.017, -lr * Pred Grad:  -0.034, New P: -0.077
iter 10 loss: 0.200
Actual params: [ 1.4826, -0.0769]
-Original Grad: 0.009, -lr * Pred Grad:  0.064, New P: 1.547
-Original Grad: -0.019, -lr * Pred Grad:  -0.032, New P: -0.109
iter 11 loss: 0.206
Actual params: [ 1.5467, -0.109 ]
-Original Grad: -0.005, -lr * Pred Grad:  0.057, New P: 1.604
-Original Grad: -0.017, -lr * Pred Grad:  -0.030, New P: -0.139
iter 12 loss: 0.215
Actual params: [ 1.6035, -0.1391]
-Original Grad: -0.008, -lr * Pred Grad:  0.050, New P: 1.654
-Original Grad: -0.016, -lr * Pred Grad:  -0.028, New P: -0.167
iter 13 loss: 0.223
Actual params: [ 1.6537, -0.1672]
-Original Grad: -0.016, -lr * Pred Grad:  0.043, New P: 1.697
-Original Grad: -0.009, -lr * Pred Grad:  -0.026, New P: -0.193
iter 14 loss: 0.223
Actual params: [ 1.6968, -0.1931]
-Original Grad: -0.026, -lr * Pred Grad:  0.036, New P: 1.732
-Original Grad: -0.005, -lr * Pred Grad:  -0.024, New P: -0.217
iter 15 loss: 0.228
Actual params: [ 1.7324, -0.217 ]
-Original Grad: -0.031, -lr * Pred Grad:  0.028, New P: 1.760
-Original Grad: -0.006, -lr * Pred Grad:  -0.022, New P: -0.239
iter 16 loss: 0.232
Actual params: [ 1.7603, -0.239 ]
-Original Grad: -0.032, -lr * Pred Grad:  0.021, New P: 1.781
-Original Grad: 0.011, -lr * Pred Grad:  -0.019, New P: -0.258
iter 17 loss: 0.234
Actual params: [ 1.7813, -0.2582]
-Original Grad: -0.033, -lr * Pred Grad:  0.014, New P: 1.796
-Original Grad: 0.013, -lr * Pred Grad:  -0.017, New P: -0.275
iter 18 loss: 0.233
Actual params: [ 1.7958, -0.2748]
-Original Grad: -0.033, -lr * Pred Grad:  0.009, New P: 1.804
-Original Grad: 0.010, -lr * Pred Grad:  -0.014, New P: -0.289
iter 19 loss: 0.231
Actual params: [ 1.8043, -0.2893]
-Original Grad: -0.034, -lr * Pred Grad:  0.003, New P: 1.807
-Original Grad: 0.019, -lr * Pred Grad:  -0.012, New P: -0.301
iter 20 loss: 0.229
Actual params: [ 1.8074, -0.3011]
-Original Grad: -0.034, -lr * Pred Grad:  -0.002, New P: 1.805
-Original Grad: 0.009, -lr * Pred Grad:  -0.010, New P: -0.311
Target params: [1.1812, 0.2779]
iter 0 loss: 0.260
Actual params: [0.5941, 0.5941]
-Original Grad: 0.049, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.066, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.193
Actual params: [0.6941, 0.6941]
-Original Grad: -0.039, -lr * Pred Grad:  0.006, New P: 0.700
-Original Grad: 0.038, -lr * Pred Grad:  0.095, New P: 0.789
iter 2 loss: 0.268
Actual params: [0.7003, 0.7894]
-Original Grad: -0.100, -lr * Pred Grad:  -0.052, New P: 0.649
-Original Grad: -0.031, -lr * Pred Grad:  0.044, New P: 0.833
iter 3 loss: 0.209
Actual params: [0.6487, 0.8335]
-Original Grad: -0.085, -lr * Pred Grad:  -0.068, New P: 0.580
-Original Grad: -0.002, -lr * Pred Grad:  0.035, New P: 0.868
iter 4 loss: 0.162
Actual params: [0.5803, 0.8682]
-Original Grad: -0.027, -lr * Pred Grad:  -0.067, New P: 0.513
-Original Grad: 0.002, -lr * Pred Grad:  0.031, New P: 0.899
iter 5 loss: 0.190
Actual params: [0.5135, 0.8989]
-Original Grad: 0.042, -lr * Pred Grad:  -0.041, New P: 0.472
-Original Grad: -0.006, -lr * Pred Grad:  0.022, New P: 0.921
iter 6 loss: 0.227
Actual params: [0.4724, 0.9214]
-Original Grad: 0.026, -lr * Pred Grad:  -0.027, New P: 0.446
-Original Grad: -0.006, -lr * Pred Grad:  0.016, New P: 0.937
iter 7 loss: 0.261
Actual params: [0.4457, 0.9373]
-Original Grad: 0.074, -lr * Pred Grad:  0.000, New P: 0.446
-Original Grad: -0.022, -lr * Pred Grad:  0.001, New P: 0.938
iter 8 loss: 0.261
Actual params: [0.4457, 0.9382]
-Original Grad: 0.049, -lr * Pred Grad:  0.013, New P: 0.459
-Original Grad: -0.014, -lr * Pred Grad:  -0.007, New P: 0.931
iter 9 loss: 0.247
Actual params: [0.459, 0.931]
-Original Grad: 0.022, -lr * Pred Grad:  0.018, New P: 0.477
-Original Grad: -0.013, -lr * Pred Grad:  -0.013, New P: 0.918
iter 10 loss: 0.222
Actual params: [0.4768, 0.9177]
-Original Grad: 0.064, -lr * Pred Grad:  0.031, New P: 0.508
-Original Grad: -0.014, -lr * Pred Grad:  -0.019, New P: 0.898
iter 11 loss: 0.193
Actual params: [0.508 , 0.8985]
-Original Grad: 0.029, -lr * Pred Grad:  0.035, New P: 0.543
-Original Grad: 0.005, -lr * Pred Grad:  -0.014, New P: 0.884
iter 12 loss: 0.171
Actual params: [0.5428, 0.8841]
-Original Grad: 0.021, -lr * Pred Grad:  0.036, New P: 0.579
-Original Grad: 0.001, -lr * Pred Grad:  -0.013, New P: 0.872
iter 13 loss: 0.162
Actual params: [0.5792, 0.8716]
-Original Grad: -0.080, -lr * Pred Grad:  0.012, New P: 0.591
-Original Grad: 0.010, -lr * Pred Grad:  -0.006, New P: 0.866
iter 14 loss: 0.162
Actual params: [0.5913, 0.866 ]
-Original Grad: -0.027, -lr * Pred Grad:  0.005, New P: 0.596
-Original Grad: 0.004, -lr * Pred Grad:  -0.003, New P: 0.863
iter 15 loss: 0.163
Actual params: [0.5959, 0.8631]
-Original Grad: -0.090, -lr * Pred Grad:  -0.015, New P: 0.581
-Original Grad: 0.008, -lr * Pred Grad:  0.002, New P: 0.865
iter 16 loss: 0.163
Actual params: [0.5806, 0.8649]
-Original Grad: -0.148, -lr * Pred Grad:  -0.038, New P: 0.542
-Original Grad: 0.017, -lr * Pred Grad:  0.011, New P: 0.876
iter 17 loss: 0.172
Actual params: [0.5421, 0.8759]
-Original Grad: 0.025, -lr * Pred Grad:  -0.030, New P: 0.512
-Original Grad: 0.002, -lr * Pred Grad:  0.011, New P: 0.887
iter 18 loss: 0.193
Actual params: [0.5119, 0.8871]
-Original Grad: 0.033, -lr * Pred Grad:  -0.021, New P: 0.491
-Original Grad: 0.010, -lr * Pred Grad:  0.016, New P: 0.903
iter 19 loss: 0.212
Actual params: [0.4906, 0.9027]
-Original Grad: 0.022, -lr * Pred Grad:  -0.015, New P: 0.475
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.918
iter 20 loss: 0.224
Actual params: [0.4755, 0.9177]
-Original Grad: 0.072, -lr * Pred Grad:  -0.000, New P: 0.475
-Original Grad: -0.016, -lr * Pred Grad:  0.005, New P: 0.922
Target params: [1.1812, 0.2779]
iter 0 loss: 0.358
Actual params: [0.5941, 0.5941]
-Original Grad: 0.335, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.002, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.245
Actual params: [0.6941, 0.6941]
-Original Grad: 0.057, -lr * Pred Grad:  0.079, New P: 0.773
-Original Grad: -0.010, -lr * Pred Grad:  -0.060, New P: 0.634
iter 2 loss: 0.222
Actual params: [0.7727, 0.6343]
-Original Grad: -0.130, -lr * Pred Grad:  0.034, New P: 0.807
-Original Grad: -0.086, -lr * Pred Grad:  -0.069, New P: 0.565
iter 3 loss: 0.219
Actual params: [0.8066, 0.5655]
-Original Grad: -0.074, -lr * Pred Grad:  0.016, New P: 0.822
-Original Grad: -0.053, -lr * Pred Grad:  -0.078, New P: 0.487
iter 4 loss: 0.218
Actual params: [0.8221, 0.4871]
-Original Grad: -0.076, -lr * Pred Grad:  0.002, New P: 0.824
-Original Grad: -0.032, -lr * Pred Grad:  -0.079, New P: 0.408
iter 5 loss: 0.225
Actual params: [0.8241, 0.4077]
-Original Grad: -0.035, -lr * Pred Grad:  -0.003, New P: 0.821
-Original Grad: -0.013, -lr * Pred Grad:  -0.074, New P: 0.334
iter 6 loss: 0.229
Actual params: [0.821 , 0.3335]
-Original Grad: -0.021, -lr * Pred Grad:  -0.006, New P: 0.815
-Original Grad: -0.004, -lr * Pred Grad:  -0.066, New P: 0.267
iter 7 loss: 0.232
Actual params: [0.8154, 0.267 ]
-Original Grad: -0.016, -lr * Pred Grad:  -0.007, New P: 0.808
-Original Grad: -0.004, -lr * Pred Grad:  -0.061, New P: 0.206
iter 8 loss: 0.236
Actual params: [0.8085, 0.2065]
-Original Grad: 0.002, -lr * Pred Grad:  -0.006, New P: 0.803
-Original Grad: -0.002, -lr * Pred Grad:  -0.055, New P: 0.152
iter 9 loss: 0.240
Actual params: [0.8025, 0.1519]
-Original Grad: 0.004, -lr * Pred Grad:  -0.005, New P: 0.798
-Original Grad: -0.002, -lr * Pred Grad:  -0.050, New P: 0.102
iter 10 loss: 0.247
Actual params: [0.7977, 0.1021]
-Original Grad: 0.003, -lr * Pred Grad:  -0.004, New P: 0.794
-Original Grad: -0.001, -lr * Pred Grad:  -0.045, New P: 0.057
iter 11 loss: 0.253
Actual params: [0.7938, 0.0569]
-Original Grad: 0.014, -lr * Pred Grad:  -0.002, New P: 0.792
-Original Grad: -0.001, -lr * Pred Grad:  -0.041, New P: 0.016
iter 12 loss: 0.258
Actual params: [0.792 , 0.0157]
-Original Grad: 0.018, -lr * Pred Grad:  0.001, New P: 0.793
-Original Grad: 0.001, -lr * Pred Grad:  -0.036, New P: -0.021
iter 13 loss: 0.262
Actual params: [ 0.7927, -0.0207]
-Original Grad: 0.023, -lr * Pred Grad:  0.004, New P: 0.796
-Original Grad: 0.003, -lr * Pred Grad:  -0.032, New P: -0.052
iter 14 loss: 0.263
Actual params: [ 0.7962, -0.0524]
-Original Grad: 0.022, -lr * Pred Grad:  0.006, New P: 0.802
-Original Grad: 0.003, -lr * Pred Grad:  -0.027, New P: -0.079
iter 15 loss: 0.265
Actual params: [ 0.8022, -0.0794]
-Original Grad: 0.013, -lr * Pred Grad:  0.007, New P: 0.809
-Original Grad: 0.003, -lr * Pred Grad:  -0.023, New P: -0.103
iter 16 loss: 0.266
Actual params: [ 0.8094, -0.1027]
-Original Grad: 0.013, -lr * Pred Grad:  0.008, New P: 0.818
-Original Grad: 0.004, -lr * Pred Grad:  -0.019, New P: -0.122
iter 17 loss: 0.266
Actual params: [ 0.8175, -0.1218]
-Original Grad: 0.017, -lr * Pred Grad:  0.010, New P: 0.827
-Original Grad: 0.005, -lr * Pred Grad:  -0.015, New P: -0.137
iter 18 loss: 0.265
Actual params: [ 0.8271, -0.1368]
-Original Grad: 0.019, -lr * Pred Grad:  0.011, New P: 0.838
-Original Grad: 0.006, -lr * Pred Grad:  -0.011, New P: -0.148
iter 19 loss: 0.263
Actual params: [ 0.8383, -0.1476]
-Original Grad: 0.012, -lr * Pred Grad:  0.012, New P: 0.850
-Original Grad: 0.007, -lr * Pred Grad:  -0.007, New P: -0.154
iter 20 loss: 0.261
Actual params: [ 0.8501, -0.1541]
-Original Grad: 0.003, -lr * Pred Grad:  0.011, New P: 0.861
-Original Grad: 0.002, -lr * Pred Grad:  -0.005, New P: -0.159
Target params: [1.1812, 0.2779]
iter 0 loss: 0.654
Actual params: [0.5941, 0.5941]
-Original Grad: 0.164, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.282, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.487
Actual params: [0.6941, 0.4941]
-Original Grad: 0.248, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.112, -lr * Pred Grad:  -0.090, New P: 0.404
iter 2 loss: 0.454
Actual params: [0.7931, 0.4043]
-Original Grad: 0.128, -lr * Pred Grad:  0.096, New P: 0.889
-Original Grad: 0.009, -lr * Pred Grad:  -0.067, New P: 0.337
iter 3 loss: 0.424
Actual params: [0.8887, 0.3369]
-Original Grad: 0.002, -lr * Pred Grad:  0.079, New P: 0.967
-Original Grad: 0.032, -lr * Pred Grad:  -0.049, New P: 0.288
iter 4 loss: 0.408
Actual params: [0.9672, 0.288 ]
-Original Grad: -0.014, -lr * Pred Grad:  0.064, New P: 1.031
-Original Grad: 0.035, -lr * Pred Grad:  -0.035, New P: 0.253
iter 5 loss: 0.406
Actual params: [1.0313, 0.2532]
-Original Grad: -0.014, -lr * Pred Grad:  0.053, New P: 1.084
-Original Grad: 0.020, -lr * Pred Grad:  -0.027, New P: 0.227
iter 6 loss: 0.412
Actual params: [1.0842, 0.2266]
-Original Grad: -0.003, -lr * Pred Grad:  0.046, New P: 1.130
-Original Grad: 0.038, -lr * Pred Grad:  -0.017, New P: 0.210
iter 7 loss: 0.423
Actual params: [1.13  , 0.2098]
-Original Grad: 0.019, -lr * Pred Grad:  0.043, New P: 1.173
-Original Grad: 0.025, -lr * Pred Grad:  -0.011, New P: 0.199
iter 8 loss: 0.439
Actual params: [1.1732, 0.1991]
-Original Grad: -0.002, -lr * Pred Grad:  0.038, New P: 1.211
-Original Grad: 0.043, -lr * Pred Grad:  -0.003, New P: 0.197
iter 9 loss: 0.455
Actual params: [1.2112, 0.1965]
-Original Grad: -0.028, -lr * Pred Grad:  0.030, New P: 1.241
-Original Grad: 0.051, -lr * Pred Grad:  0.005, New P: 0.202
iter 10 loss: 0.462
Actual params: [1.2408, 0.2019]
-Original Grad: -0.038, -lr * Pred Grad:  0.021, New P: 1.262
-Original Grad: 0.050, -lr * Pred Grad:  0.012, New P: 0.214
iter 11 loss: 0.470
Actual params: [1.2617, 0.2142]
-Original Grad: -0.055, -lr * Pred Grad:  0.011, New P: 1.272
-Original Grad: 0.049, -lr * Pred Grad:  0.018, New P: 0.233
iter 12 loss: 0.474
Actual params: [1.2722, 0.2325]
-Original Grad: -0.067, -lr * Pred Grad:  -0.000, New P: 1.272
-Original Grad: 0.070, -lr * Pred Grad:  0.026, New P: 0.259
iter 13 loss: 0.471
Actual params: [1.272 , 0.2587]
-Original Grad: -0.060, -lr * Pred Grad:  -0.009, New P: 1.263
-Original Grad: 0.078, -lr * Pred Grad:  0.034, New P: 0.293
iter 14 loss: 0.463
Actual params: [1.2633, 0.2928]
-Original Grad: -0.045, -lr * Pred Grad:  -0.014, New P: 1.249
-Original Grad: 0.067, -lr * Pred Grad:  0.040, New P: 0.333
iter 15 loss: 0.450
Actual params: [1.2492, 0.3325]
-Original Grad: -0.030, -lr * Pred Grad:  -0.017, New P: 1.232
-Original Grad: 0.047, -lr * Pred Grad:  0.042, New P: 0.375
iter 16 loss: 0.432
Actual params: [1.2322, 0.3747]
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: 1.217
-Original Grad: 0.061, -lr * Pred Grad:  0.046, New P: 0.421
iter 17 loss: 0.413
Actual params: [1.2167, 0.4209]
-Original Grad: 0.024, -lr * Pred Grad:  -0.011, New P: 1.206
-Original Grad: 0.046, -lr * Pred Grad:  0.048, New P: 0.469
iter 18 loss: 0.383
Actual params: [1.2062, 0.4689]
-Original Grad: 0.035, -lr * Pred Grad:  -0.004, New P: 1.202
-Original Grad: 0.008, -lr * Pred Grad:  0.045, New P: 0.514
iter 19 loss: 0.366
Actual params: [1.2017, 0.5136]
-Original Grad: 0.104, -lr * Pred Grad:  0.011, New P: 1.212
-Original Grad: -0.062, -lr * Pred Grad:  0.031, New P: 0.545
iter 20 loss: 0.371
Actual params: [1.2123, 0.5449]
-Original Grad: 0.095, -lr * Pred Grad:  0.022, New P: 1.235
-Original Grad: -0.112, -lr * Pred Grad:  0.012, New P: 0.557
Target params: [1.1812, 0.2779]
iter 0 loss: 0.710
Actual params: [0.5941, 0.5941]
-Original Grad: 0.432, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.270, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.612
Actual params: [0.6941, 0.4941]
-Original Grad: 0.096, -lr * Pred Grad:  0.082, New P: 0.776
-Original Grad: -0.082, -lr * Pred Grad:  -0.086, New P: 0.408
iter 2 loss: 0.557
Actual params: [0.7756, 0.4082]
-Original Grad: 0.018, -lr * Pred Grad:  0.066, New P: 0.841
-Original Grad: -0.024, -lr * Pred Grad:  -0.072, New P: 0.337
iter 3 loss: 0.529
Actual params: [0.8412, 0.3366]
-Original Grad: 0.014, -lr * Pred Grad:  0.056, New P: 0.897
-Original Grad: 0.002, -lr * Pred Grad:  -0.058, New P: 0.278
iter 4 loss: 0.513
Actual params: [0.8968, 0.2785]
-Original Grad: 0.005, -lr * Pred Grad:  0.048, New P: 0.944
-Original Grad: -0.001, -lr * Pred Grad:  -0.049, New P: 0.229
iter 5 loss: 0.503
Actual params: [0.9445, 0.2291]
-Original Grad: 0.007, -lr * Pred Grad:  0.042, New P: 0.986
-Original Grad: 0.031, -lr * Pred Grad:  -0.037, New P: 0.193
iter 6 loss: 0.499
Actual params: [0.9864, 0.1925]
-Original Grad: 0.006, -lr * Pred Grad:  0.037, New P: 1.024
-Original Grad: 0.041, -lr * Pred Grad:  -0.024, New P: 0.168
iter 7 loss: 0.499
Actual params: [1.0237, 0.1681]
-Original Grad: 0.020, -lr * Pred Grad:  0.035, New P: 1.059
-Original Grad: 0.041, -lr * Pred Grad:  -0.014, New P: 0.154
iter 8 loss: 0.500
Actual params: [1.0588, 0.1537]
-Original Grad: 0.012, -lr * Pred Grad:  0.032, New P: 1.091
-Original Grad: 0.053, -lr * Pred Grad:  -0.004, New P: 0.150
iter 9 loss: 0.502
Actual params: [1.0912, 0.1498]
-Original Grad: 0.005, -lr * Pred Grad:  0.029, New P: 1.121
-Original Grad: 0.041, -lr * Pred Grad:  0.003, New P: 0.153
iter 10 loss: 0.506
Actual params: [1.1206, 0.1531]
-Original Grad: 0.004, -lr * Pred Grad:  0.027, New P: 1.147
-Original Grad: 0.043, -lr * Pred Grad:  0.010, New P: 0.163
iter 11 loss: 0.510
Actual params: [1.1475, 0.1629]
-Original Grad: 0.009, -lr * Pred Grad:  0.025, New P: 1.173
-Original Grad: 0.043, -lr * Pred Grad:  0.016, New P: 0.179
iter 12 loss: 0.514
Actual params: [1.1726, 0.1785]
-Original Grad: -0.001, -lr * Pred Grad:  0.023, New P: 1.195
-Original Grad: 0.026, -lr * Pred Grad:  0.018, New P: 0.197
iter 13 loss: 0.519
Actual params: [1.1952, 0.1967]
-Original Grad: 0.014, -lr * Pred Grad:  0.022, New P: 1.217
-Original Grad: 0.035, -lr * Pred Grad:  0.022, New P: 0.219
iter 14 loss: 0.517
Actual params: [1.2171, 0.2186]
-Original Grad: 0.006, -lr * Pred Grad:  0.020, New P: 1.238
-Original Grad: 0.024, -lr * Pred Grad:  0.023, New P: 0.242
iter 15 loss: 0.523
Actual params: [1.2375, 0.2421]
-Original Grad: -0.008, -lr * Pred Grad:  0.018, New P: 1.255
-Original Grad: -0.001, -lr * Pred Grad:  0.021, New P: 0.263
iter 16 loss: 0.530
Actual params: [1.2552, 0.2631]
-Original Grad: 0.002, -lr * Pred Grad:  0.016, New P: 1.271
-Original Grad: 0.009, -lr * Pred Grad:  0.021, New P: 0.284
iter 17 loss: 0.537
Actual params: [1.2714, 0.2836]
-Original Grad: -0.010, -lr * Pred Grad:  0.014, New P: 1.285
-Original Grad: -0.008, -lr * Pred Grad:  0.017, New P: 0.301
iter 18 loss: 0.543
Actual params: [1.285, 0.301]
-Original Grad: -0.012, -lr * Pred Grad:  0.011, New P: 1.296
-Original Grad: -0.033, -lr * Pred Grad:  0.010, New P: 0.311
iter 19 loss: 0.547
Actual params: [1.2959, 0.3114]
-Original Grad: -0.013, -lr * Pred Grad:  0.008, New P: 1.304
-Original Grad: -0.019, -lr * Pred Grad:  0.006, New P: 0.318
iter 20 loss: 0.549
Actual params: [1.3044, 0.3177]
-Original Grad: -0.018, -lr * Pred Grad:  0.006, New P: 1.310
-Original Grad: -0.042, -lr * Pred Grad:  -0.001, New P: 0.316
Target params: [1.1812, 0.2779]
iter 0 loss: 0.383
Actual params: [0.5941, 0.5941]
-Original Grad: 0.301, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.263, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.331
Actual params: [0.6941, 0.4941]
-Original Grad: 0.005, -lr * Pred Grad:  0.068, New P: 0.762
-Original Grad: -0.068, -lr * Pred Grad:  -0.084, New P: 0.410
iter 2 loss: 0.324
Actual params: [0.7622, 0.4105]
-Original Grad: -0.008, -lr * Pred Grad:  0.051, New P: 0.813
-Original Grad: -0.032, -lr * Pred Grad:  -0.072, New P: 0.339
iter 3 loss: 0.322
Actual params: [0.8132, 0.3388]
-Original Grad: -0.010, -lr * Pred Grad:  0.040, New P: 0.853
-Original Grad: -0.001, -lr * Pred Grad:  -0.059, New P: 0.280
iter 4 loss: 0.323
Actual params: [0.8529, 0.2798]
-Original Grad: 0.002, -lr * Pred Grad:  0.034, New P: 0.887
-Original Grad: 0.001, -lr * Pred Grad:  -0.050, New P: 0.230
iter 5 loss: 0.325
Actual params: [0.8869, 0.2301]
-Original Grad: -0.002, -lr * Pred Grad:  0.029, New P: 0.916
-Original Grad: 0.009, -lr * Pred Grad:  -0.041, New P: 0.189
iter 6 loss: 0.327
Actual params: [0.9159, 0.1889]
-Original Grad: 0.001, -lr * Pred Grad:  0.025, New P: 0.941
-Original Grad: 0.007, -lr * Pred Grad:  -0.035, New P: 0.154
iter 7 loss: 0.329
Actual params: [0.9413, 0.1543]
-Original Grad: -0.005, -lr * Pred Grad:  0.022, New P: 0.963
-Original Grad: 0.010, -lr * Pred Grad:  -0.029, New P: 0.126
iter 8 loss: 0.332
Actual params: [0.9628, 0.1258]
-Original Grad: -0.012, -lr * Pred Grad:  0.017, New P: 0.980
-Original Grad: 0.014, -lr * Pred Grad:  -0.023, New P: 0.103
iter 9 loss: 0.334
Actual params: [0.9801, 0.103 ]
-Original Grad: -0.005, -lr * Pred Grad:  0.015, New P: 0.995
-Original Grad: 0.011, -lr * Pred Grad:  -0.018, New P: 0.085
iter 10 loss: 0.335
Actual params: [0.9947, 0.0846]
-Original Grad: -0.015, -lr * Pred Grad:  0.011, New P: 1.005
-Original Grad: 0.013, -lr * Pred Grad:  -0.014, New P: 0.070
iter 11 loss: 0.335
Actual params: [1.0053, 0.0703]
-Original Grad: -0.009, -lr * Pred Grad:  0.008, New P: 1.014
-Original Grad: 0.013, -lr * Pred Grad:  -0.011, New P: 0.060
iter 12 loss: 0.336
Actual params: [1.0135, 0.0597]
-Original Grad: -0.014, -lr * Pred Grad:  0.005, New P: 1.019
-Original Grad: 0.021, -lr * Pred Grad:  -0.006, New P: 0.054
iter 13 loss: 0.336
Actual params: [1.0187, 0.0538]
-Original Grad: -0.013, -lr * Pred Grad:  0.002, New P: 1.021
-Original Grad: 0.015, -lr * Pred Grad:  -0.003, New P: 0.051
iter 14 loss: 0.336
Actual params: [1.0212, 0.0511]
-Original Grad: -0.008, -lr * Pred Grad:  0.001, New P: 1.022
-Original Grad: 0.014, -lr * Pred Grad:  0.000, New P: 0.051
iter 15 loss: 0.336
Actual params: [1.0222, 0.0512]
-Original Grad: -0.013, -lr * Pred Grad:  -0.001, New P: 1.021
-Original Grad: 0.017, -lr * Pred Grad:  0.003, New P: 0.054
iter 16 loss: 0.336
Actual params: [1.021 , 0.0543]
-Original Grad: -0.015, -lr * Pred Grad:  -0.004, New P: 1.017
-Original Grad: 0.028, -lr * Pred Grad:  0.008, New P: 0.062
iter 17 loss: 0.336
Actual params: [1.0174, 0.062 ]
-Original Grad: -0.013, -lr * Pred Grad:  -0.005, New P: 1.012
-Original Grad: 0.023, -lr * Pred Grad:  0.011, New P: 0.073
iter 18 loss: 0.335
Actual params: [1.0122, 0.0731]
-Original Grad: -0.007, -lr * Pred Grad:  -0.006, New P: 1.006
-Original Grad: 0.009, -lr * Pred Grad:  0.012, New P: 0.085
iter 19 loss: 0.335
Actual params: [1.0063, 0.0848]
-Original Grad: -0.011, -lr * Pred Grad:  -0.007, New P: 0.999
-Original Grad: 0.012, -lr * Pred Grad:  0.013, New P: 0.098
iter 20 loss: 0.335
Actual params: [0.999 , 0.0975]
-Original Grad: -0.007, -lr * Pred Grad:  -0.008, New P: 0.991
-Original Grad: 0.011, -lr * Pred Grad:  0.014, New P: 0.111
Target params: [1.1812, 0.2779]
iter 0 loss: 0.494
Actual params: [0.5941, 0.5941]
-Original Grad: 0.133, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.530, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.334
Actual params: [0.6941, 0.4941]
-Original Grad: 0.087, -lr * Pred Grad:  0.097, New P: 0.791
-Original Grad: -0.212, -lr * Pred Grad:  -0.090, New P: 0.404
iter 2 loss: 0.187
Actual params: [0.7908, 0.4042]
-Original Grad: 0.036, -lr * Pred Grad:  0.087, New P: 0.878
-Original Grad: -0.087, -lr * Pred Grad:  -0.078, New P: 0.326
iter 3 loss: 0.172
Actual params: [0.878 , 0.3259]
-Original Grad: -0.023, -lr * Pred Grad:  0.063, New P: 0.941
-Original Grad: 0.009, -lr * Pred Grad:  -0.063, New P: 0.263
iter 4 loss: 0.187
Actual params: [0.9406, 0.2627]
-Original Grad: -0.017, -lr * Pred Grad:  0.047, New P: 0.988
-Original Grad: 0.020, -lr * Pred Grad:  -0.052, New P: 0.211
iter 5 loss: 0.195
Actual params: [0.9877, 0.2111]
-Original Grad: -0.000, -lr * Pred Grad:  0.040, New P: 1.028
-Original Grad: 0.024, -lr * Pred Grad:  -0.042, New P: 0.169
iter 6 loss: 0.192
Actual params: [1.0282, 0.1689]
-Original Grad: 0.070, -lr * Pred Grad:  0.052, New P: 1.080
-Original Grad: 0.022, -lr * Pred Grad:  -0.035, New P: 0.134
iter 7 loss: 0.175
Actual params: [1.0804, 0.134 ]
-Original Grad: 0.036, -lr * Pred Grad:  0.055, New P: 1.135
-Original Grad: 0.020, -lr * Pred Grad:  -0.029, New P: 0.105
iter 8 loss: 0.167
Actual params: [1.1355, 0.105 ]
-Original Grad: 0.024, -lr * Pred Grad:  0.055, New P: 1.190
-Original Grad: 0.013, -lr * Pred Grad:  -0.025, New P: 0.080
iter 9 loss: 0.159
Actual params: [1.1904, 0.0804]
-Original Grad: 0.015, -lr * Pred Grad:  0.053, New P: 1.243
-Original Grad: 0.013, -lr * Pred Grad:  -0.021, New P: 0.059
iter 10 loss: 0.157
Actual params: [1.243 , 0.0595]
-Original Grad: 0.009, -lr * Pred Grad:  0.049, New P: 1.292
-Original Grad: 0.011, -lr * Pred Grad:  -0.018, New P: 0.042
iter 11 loss: 0.155
Actual params: [1.2924, 0.0417]
-Original Grad: 0.003, -lr * Pred Grad:  0.045, New P: 1.338
-Original Grad: 0.008, -lr * Pred Grad:  -0.015, New P: 0.026
iter 12 loss: 0.154
Actual params: [1.3377, 0.0263]
-Original Grad: 0.004, -lr * Pred Grad:  0.042, New P: 1.379
-Original Grad: 0.008, -lr * Pred Grad:  -0.013, New P: 0.013
iter 13 loss: 0.153
Actual params: [1.3794, 0.0131]
-Original Grad: 0.003, -lr * Pred Grad:  0.039, New P: 1.418
-Original Grad: 0.009, -lr * Pred Grad:  -0.011, New P: 0.002
iter 14 loss: 0.153
Actual params: [1.418, 0.002]
-Original Grad: 0.001, -lr * Pred Grad:  0.035, New P: 1.453
-Original Grad: 0.002, -lr * Pred Grad:  -0.010, New P: -0.008
iter 15 loss: 0.152
Actual params: [ 1.453 , -0.0079]
-Original Grad: 0.002, -lr * Pred Grad:  0.032, New P: 1.485
-Original Grad: 0.004, -lr * Pred Grad:  -0.009, New P: -0.017
iter 16 loss: 0.152
Actual params: [ 1.4852, -0.0165]
-Original Grad: -0.000, -lr * Pred Grad:  0.029, New P: 1.514
-Original Grad: 0.002, -lr * Pred Grad:  -0.008, New P: -0.024
iter 17 loss: 0.153
Actual params: [ 1.5143, -0.0241]
-Original Grad: 0.004, -lr * Pred Grad:  0.027, New P: 1.542
-Original Grad: 0.003, -lr * Pred Grad:  -0.007, New P: -0.031
iter 18 loss: 0.154
Actual params: [ 1.5417, -0.0308]
-Original Grad: -0.000, -lr * Pred Grad:  0.025, New P: 1.567
-Original Grad: 0.004, -lr * Pred Grad:  -0.006, New P: -0.036
iter 19 loss: 0.155
Actual params: [ 1.5665, -0.0364]
-Original Grad: 0.001, -lr * Pred Grad:  0.023, New P: 1.589
-Original Grad: 0.003, -lr * Pred Grad:  -0.005, New P: -0.041
iter 20 loss: 0.157
Actual params: [ 1.5894, -0.0413]
-Original Grad: 0.001, -lr * Pred Grad:  0.021, New P: 1.610
-Original Grad: 0.008, -lr * Pred Grad:  -0.004, New P: -0.045
Target params: [1.1812, 0.2779]
iter 0 loss: 0.552
Actual params: [0.5941, 0.5941]
-Original Grad: 0.501, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.178, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.451
Actual params: [0.6941, 0.4941]
-Original Grad: 0.316, -lr * Pred Grad:  0.096, New P: 0.790
-Original Grad: -0.027, -lr * Pred Grad:  -0.078, New P: 0.417
iter 2 loss: 0.334
Actual params: [0.7904, 0.4165]
-Original Grad: 0.157, -lr * Pred Grad:  0.088, New P: 0.879
-Original Grad: -0.045, -lr * Pred Grad:  -0.074, New P: 0.343
iter 3 loss: 0.222
Actual params: [0.8788, 0.3428]
-Original Grad: 0.350, -lr * Pred Grad:  0.092, New P: 0.970
-Original Grad: 0.144, -lr * Pred Grad:  -0.012, New P: 0.331
iter 4 loss: 0.178
Actual params: [0.9704, 0.3307]
-Original Grad: 0.257, -lr * Pred Grad:  0.091, New P: 1.062
-Original Grad: 0.096, -lr * Pred Grad:  0.011, New P: 0.342
iter 5 loss: 0.165
Actual params: [1.0619, 0.3419]
-Original Grad: 0.183, -lr * Pred Grad:  0.089, New P: 1.151
-Original Grad: -0.089, -lr * Pred Grad:  -0.008, New P: 0.333
iter 6 loss: 0.146
Actual params: [1.1509, 0.3335]
-Original Grad: 0.168, -lr * Pred Grad:  0.087, New P: 1.238
-Original Grad: -0.042, -lr * Pred Grad:  -0.015, New P: 0.319
iter 7 loss: 0.161
Actual params: [1.2376, 0.3185]
-Original Grad: -0.018, -lr * Pred Grad:  0.075, New P: 1.313
-Original Grad: -0.005, -lr * Pred Grad:  -0.014, New P: 0.304
iter 8 loss: 0.184
Actual params: [1.3129, 0.3044]
-Original Grad: -0.047, -lr * Pred Grad:  0.064, New P: 1.377
-Original Grad: 0.010, -lr * Pred Grad:  -0.011, New P: 0.294
iter 9 loss: 0.204
Actual params: [1.3767, 0.2937]
-Original Grad: -0.070, -lr * Pred Grad:  0.052, New P: 1.429
-Original Grad: 0.005, -lr * Pred Grad:  -0.009, New P: 0.285
iter 10 loss: 0.216
Actual params: [1.4292, 0.285 ]
-Original Grad: -0.108, -lr * Pred Grad:  0.040, New P: 1.469
-Original Grad: 0.035, -lr * Pred Grad:  -0.002, New P: 0.283
iter 11 loss: 0.231
Actual params: [1.4693, 0.2834]
-Original Grad: -0.138, -lr * Pred Grad:  0.027, New P: 1.497
-Original Grad: 0.067, -lr * Pred Grad:  0.010, New P: 0.293
iter 12 loss: 0.245
Actual params: [1.4966, 0.2934]
-Original Grad: -0.102, -lr * Pred Grad:  0.018, New P: 1.515
-Original Grad: 0.017, -lr * Pred Grad:  0.012, New P: 0.305
iter 13 loss: 0.260
Actual params: [1.515 , 0.3053]
-Original Grad: -0.107, -lr * Pred Grad:  0.010, New P: 1.525
-Original Grad: 0.044, -lr * Pred Grad:  0.018, New P: 0.323
iter 14 loss: 0.271
Actual params: [1.5252, 0.3233]
-Original Grad: -0.106, -lr * Pred Grad:  0.003, New P: 1.528
-Original Grad: -0.043, -lr * Pred Grad:  0.009, New P: 0.332
iter 15 loss: 0.276
Actual params: [1.5281, 0.3321]
-Original Grad: -0.074, -lr * Pred Grad:  -0.002, New P: 1.526
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: 0.340
iter 16 loss: 0.279
Actual params: [1.5264, 0.3401]
-Original Grad: -0.079, -lr * Pred Grad:  -0.006, New P: 1.520
-Original Grad: -0.032, -lr * Pred Grad:  0.002, New P: 0.342
iter 17 loss: 0.277
Actual params: [1.5203, 0.342 ]
-Original Grad: -0.104, -lr * Pred Grad:  -0.012, New P: 1.509
-Original Grad: -0.033, -lr * Pred Grad:  -0.004, New P: 0.338
iter 18 loss: 0.269
Actual params: [1.5086, 0.338 ]
-Original Grad: -0.085, -lr * Pred Grad:  -0.016, New P: 1.493
-Original Grad: -0.097, -lr * Pred Grad:  -0.019, New P: 0.319
iter 19 loss: 0.254
Actual params: [1.493 , 0.3187]
-Original Grad: -0.115, -lr * Pred Grad:  -0.021, New P: 1.472
-Original Grad: 0.072, -lr * Pred Grad:  -0.005, New P: 0.313
iter 20 loss: 0.244
Actual params: [1.4721, 0.3132]
-Original Grad: -0.105, -lr * Pred Grad:  -0.025, New P: 1.447
-Original Grad: 0.029, -lr * Pred Grad:  -0.000, New P: 0.313
Target params: [1.1812, 0.2779]
iter 0 loss: 0.339
Actual params: [0.5941, 0.5941]
-Original Grad: 0.083, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.196, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.208
Actual params: [0.6941, 0.4941]
-Original Grad: 0.076, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.112, -lr * Pred Grad:  -0.095, New P: 0.399
iter 2 loss: 0.115
Actual params: [0.7937, 0.399 ]
-Original Grad: 0.012, -lr * Pred Grad:  0.084, New P: 0.877
-Original Grad: 0.001, -lr * Pred Grad:  -0.073, New P: 0.326
iter 3 loss: 0.100
Actual params: [0.8773, 0.3258]
-Original Grad: -0.008, -lr * Pred Grad:  0.064, New P: 0.942
-Original Grad: 0.014, -lr * Pred Grad:  -0.056, New P: 0.270
iter 4 loss: 0.113
Actual params: [0.9416, 0.2696]
-Original Grad: 0.012, -lr * Pred Grad:  0.060, New P: 1.001
-Original Grad: 0.019, -lr * Pred Grad:  -0.043, New P: 0.227
iter 5 loss: 0.114
Actual params: [1.0013, 0.2269]
-Original Grad: 0.016, -lr * Pred Grad:  0.058, New P: 1.059
-Original Grad: 0.020, -lr * Pred Grad:  -0.032, New P: 0.195
iter 6 loss: 0.110
Actual params: [1.0595, 0.1948]
-Original Grad: 0.011, -lr * Pred Grad:  0.055, New P: 1.115
-Original Grad: 0.027, -lr * Pred Grad:  -0.022, New P: 0.173
iter 7 loss: 0.108
Actual params: [1.1148, 0.173 ]
-Original Grad: 0.010, -lr * Pred Grad:  0.053, New P: 1.168
-Original Grad: 0.028, -lr * Pred Grad:  -0.013, New P: 0.160
iter 8 loss: 0.106
Actual params: [1.1678, 0.1599]
-Original Grad: 0.009, -lr * Pred Grad:  0.051, New P: 1.218
-Original Grad: 0.042, -lr * Pred Grad:  -0.003, New P: 0.157
iter 9 loss: 0.102
Actual params: [1.2185, 0.1572]
-Original Grad: 0.003, -lr * Pred Grad:  0.047, New P: 1.265
-Original Grad: 0.022, -lr * Pred Grad:  0.002, New P: 0.159
iter 10 loss: 0.100
Actual params: [1.2651, 0.1593]
-Original Grad: 0.003, -lr * Pred Grad:  0.043, New P: 1.308
-Original Grad: 0.028, -lr * Pred Grad:  0.008, New P: 0.167
iter 11 loss: 0.099
Actual params: [1.3079, 0.1669]
-Original Grad: 0.001, -lr * Pred Grad:  0.039, New P: 1.347
-Original Grad: 0.027, -lr * Pred Grad:  0.012, New P: 0.179
iter 12 loss: 0.100
Actual params: [1.3466, 0.1792]
-Original Grad: -0.001, -lr * Pred Grad:  0.035, New P: 1.381
-Original Grad: 0.014, -lr * Pred Grad:  0.014, New P: 0.193
iter 13 loss: 0.101
Actual params: [1.3812, 0.1931]
-Original Grad: -0.004, -lr * Pred Grad:  0.030, New P: 1.411
-Original Grad: 0.008, -lr * Pred Grad:  0.014, New P: 0.207
iter 14 loss: 0.103
Actual params: [1.411 , 0.2073]
-Original Grad: -0.005, -lr * Pred Grad:  0.025, New P: 1.436
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.220
iter 15 loss: 0.103
Actual params: [1.4356, 0.2201]
-Original Grad: -0.005, -lr * Pred Grad:  0.020, New P: 1.456
-Original Grad: -0.003, -lr * Pred Grad:  0.011, New P: 0.231
iter 16 loss: 0.105
Actual params: [1.4559, 0.2311]
-Original Grad: -0.004, -lr * Pred Grad:  0.017, New P: 1.473
-Original Grad: -0.006, -lr * Pred Grad:  0.009, New P: 0.240
iter 17 loss: 0.108
Actual params: [1.4727, 0.2398]
-Original Grad: -0.006, -lr * Pred Grad:  0.013, New P: 1.485
-Original Grad: -0.007, -lr * Pred Grad:  0.006, New P: 0.246
iter 18 loss: 0.111
Actual params: [1.4854, 0.2462]
-Original Grad: -0.005, -lr * Pred Grad:  0.009, New P: 1.495
-Original Grad: -0.011, -lr * Pred Grad:  0.003, New P: 0.250
iter 19 loss: 0.112
Actual params: [1.4947, 0.2497]
-Original Grad: -0.008, -lr * Pred Grad:  0.005, New P: 1.500
-Original Grad: -0.014, -lr * Pred Grad:  0.000, New P: 0.250
iter 20 loss: 0.113
Actual params: [1.4999, 0.2499]
-Original Grad: -0.005, -lr * Pred Grad:  0.003, New P: 1.502
-Original Grad: -0.020, -lr * Pred Grad:  -0.004, New P: 0.246
Target params: [1.1812, 0.2779]
iter 0 loss: 0.445
Actual params: [0.5941, 0.5941]
-Original Grad: -0.099, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -1.031, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.293
Actual params: [0.4941, 0.4941]
-Original Grad: -0.206, -lr * Pred Grad:  -0.096, New P: 0.398
-Original Grad: -0.391, -lr * Pred Grad:  -0.089, New P: 0.405
iter 2 loss: 0.320
Actual params: [0.398, 0.405]
-Original Grad: 0.054, -lr * Pred Grad:  -0.058, New P: 0.340
-Original Grad: -0.001, -lr * Pred Grad:  -0.069, New P: 0.336
iter 3 loss: 0.344
Actual params: [0.3404, 0.3361]
-Original Grad: 0.165, -lr * Pred Grad:  -0.005, New P: 0.335
-Original Grad: 0.013, -lr * Pred Grad:  -0.056, New P: 0.280
iter 4 loss: 0.343
Actual params: [0.3353, 0.2804]
-Original Grad: 0.150, -lr * Pred Grad:  0.021, New P: 0.357
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: 0.233
iter 5 loss: 0.337
Actual params: [0.3567, 0.2332]
-Original Grad: 0.151, -lr * Pred Grad:  0.039, New P: 0.395
-Original Grad: 0.002, -lr * Pred Grad:  -0.041, New P: 0.193
iter 6 loss: 0.327
Actual params: [0.3955, 0.1927]
-Original Grad: 0.095, -lr * Pred Grad:  0.046, New P: 0.441
-Original Grad: 0.001, -lr * Pred Grad:  -0.035, New P: 0.157
iter 7 loss: 0.320
Actual params: [0.4412, 0.1574]
-Original Grad: 0.058, -lr * Pred Grad:  0.047, New P: 0.489
-Original Grad: 0.008, -lr * Pred Grad:  -0.031, New P: 0.127
iter 8 loss: 0.314
Actual params: [0.4886, 0.1266]
-Original Grad: 0.029, -lr * Pred Grad:  0.046, New P: 0.534
-Original Grad: 0.008, -lr * Pred Grad:  -0.027, New P: 0.100
iter 9 loss: 0.311
Actual params: [0.5345, 0.0996]
-Original Grad: -0.011, -lr * Pred Grad:  0.039, New P: 0.574
-Original Grad: 0.004, -lr * Pred Grad:  -0.024, New P: 0.076
iter 10 loss: 0.306
Actual params: [0.574 , 0.0757]
-Original Grad: -0.004, -lr * Pred Grad:  0.035, New P: 0.609
-Original Grad: 0.007, -lr * Pred Grad:  -0.021, New P: 0.055
iter 11 loss: 0.305
Actual params: [0.6089, 0.0546]
-Original Grad: -0.014, -lr * Pred Grad:  0.030, New P: 0.638
-Original Grad: 0.004, -lr * Pred Grad:  -0.019, New P: 0.036
iter 12 loss: 0.304
Actual params: [0.6384, 0.0359]
-Original Grad: -0.021, -lr * Pred Grad:  0.024, New P: 0.662
-Original Grad: 0.008, -lr * Pred Grad:  -0.017, New P: 0.019
iter 13 loss: 0.304
Actual params: [0.6623, 0.0193]
-Original Grad: -0.013, -lr * Pred Grad:  0.020, New P: 0.682
-Original Grad: 0.012, -lr * Pred Grad:  -0.014, New P: 0.005
iter 14 loss: 0.304
Actual params: [0.6823, 0.0049]
-Original Grad: -0.011, -lr * Pred Grad:  0.017, New P: 0.699
-Original Grad: 0.018, -lr * Pred Grad:  -0.012, New P: -0.007
iter 15 loss: 0.304
Actual params: [ 0.6988, -0.0074]
-Original Grad: -0.014, -lr * Pred Grad:  0.013, New P: 0.712
-Original Grad: 0.014, -lr * Pred Grad:  -0.010, New P: -0.018
iter 16 loss: 0.305
Actual params: [ 0.712 , -0.0179]
-Original Grad: -0.013, -lr * Pred Grad:  0.010, New P: 0.722
-Original Grad: 0.011, -lr * Pred Grad:  -0.009, New P: -0.027
iter 17 loss: 0.307
Actual params: [ 0.7223, -0.0269]
-Original Grad: -0.012, -lr * Pred Grad:  0.008, New P: 0.730
-Original Grad: 0.022, -lr * Pred Grad:  -0.007, New P: -0.034
iter 18 loss: 0.307
Actual params: [ 0.73  , -0.0341]
-Original Grad: -0.011, -lr * Pred Grad:  0.006, New P: 0.736
-Original Grad: 0.022, -lr * Pred Grad:  -0.006, New P: -0.040
iter 19 loss: 0.307
Actual params: [ 0.7356, -0.0397]
-Original Grad: -0.014, -lr * Pred Grad:  0.003, New P: 0.739
-Original Grad: 0.026, -lr * Pred Grad:  -0.004, New P: -0.044
iter 20 loss: 0.307
Actual params: [ 0.7387, -0.0435]
-Original Grad: -0.010, -lr * Pred Grad:  0.001, New P: 0.740
-Original Grad: 0.019, -lr * Pred Grad:  -0.003, New P: -0.046
Target params: [1.1812, 0.2779]
iter 0 loss: 0.667
Actual params: [0.5941, 0.5941]
-Original Grad: 0.712, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.117, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.567
Actual params: [0.6941, 0.4941]
-Original Grad: 0.444, -lr * Pred Grad:  0.096, New P: 0.790
-Original Grad: 0.053, -lr * Pred Grad:  -0.030, New P: 0.464
iter 2 loss: 0.404
Actual params: [0.7903, 0.464 ]
-Original Grad: 0.171, -lr * Pred Grad:  0.086, New P: 0.876
-Original Grad: 0.058, -lr * Pred Grad:  0.005, New P: 0.469
iter 3 loss: 0.274
Actual params: [0.876 , 0.4689]
-Original Grad: 0.122, -lr * Pred Grad:  0.078, New P: 0.954
-Original Grad: 0.087, -lr * Pred Grad:  0.034, New P: 0.503
iter 4 loss: 0.222
Actual params: [0.9536, 0.503 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.066, New P: 1.019
-Original Grad: -0.038, -lr * Pred Grad:  0.016, New P: 0.519
iter 5 loss: 0.203
Actual params: [1.0194, 0.5187]
-Original Grad: 0.005, -lr * Pred Grad:  0.057, New P: 1.076
-Original Grad: -0.052, -lr * Pred Grad:  -0.002, New P: 0.516
iter 6 loss: 0.200
Actual params: [1.0765, 0.5164]
-Original Grad: 0.008, -lr * Pred Grad:  0.050, New P: 1.127
-Original Grad: -0.065, -lr * Pred Grad:  -0.019, New P: 0.497
iter 7 loss: 0.200
Actual params: [1.1267, 0.4972]
-Original Grad: -0.001, -lr * Pred Grad:  0.044, New P: 1.171
-Original Grad: -0.011, -lr * Pred Grad:  -0.020, New P: 0.477
iter 8 loss: 0.212
Actual params: [1.171 , 0.4773]
-Original Grad: 0.023, -lr * Pred Grad:  0.041, New P: 1.212
-Original Grad: -0.053, -lr * Pred Grad:  -0.030, New P: 0.447
iter 9 loss: 0.236
Actual params: [1.2116, 0.447 ]
-Original Grad: -0.045, -lr * Pred Grad:  0.034, New P: 1.245
-Original Grad: 0.032, -lr * Pred Grad:  -0.019, New P: 0.428
iter 10 loss: 0.255
Actual params: [1.2453, 0.4281]
-Original Grad: -0.060, -lr * Pred Grad:  0.027, New P: 1.272
-Original Grad: 0.085, -lr * Pred Grad:  0.003, New P: 0.432
iter 11 loss: 0.258
Actual params: [1.272 , 0.4315]
-Original Grad: -0.029, -lr * Pred Grad:  0.022, New P: 1.294
-Original Grad: 0.067, -lr * Pred Grad:  0.017, New P: 0.449
iter 12 loss: 0.262
Actual params: [1.2944, 0.4488]
-Original Grad: -0.021, -lr * Pred Grad:  0.019, New P: 1.313
-Original Grad: 0.071, -lr * Pred Grad:  0.029, New P: 0.478
iter 13 loss: 0.264
Actual params: [1.3135, 0.4781]
-Original Grad: -0.001, -lr * Pred Grad:  0.017, New P: 1.331
-Original Grad: 0.041, -lr * Pred Grad:  0.034, New P: 0.512
iter 14 loss: 0.258
Actual params: [1.3306, 0.5124]
-Original Grad: -0.016, -lr * Pred Grad:  0.015, New P: 1.345
-Original Grad: 0.023, -lr * Pred Grad:  0.036, New P: 0.548
iter 15 loss: 0.249
Actual params: [1.3453, 0.548 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.013, New P: 1.359
-Original Grad: 0.029, -lr * Pred Grad:  0.038, New P: 0.586
iter 16 loss: 0.226
Actual params: [1.3587, 0.5858]
-Original Grad: -0.008, -lr * Pred Grad:  0.012, New P: 1.370
-Original Grad: -0.072, -lr * Pred Grad:  0.019, New P: 0.605
iter 17 loss: 0.212
Actual params: [1.3703, 0.6047]
-Original Grad: -0.012, -lr * Pred Grad:  0.010, New P: 1.380
-Original Grad: -0.010, -lr * Pred Grad:  0.015, New P: 0.620
iter 18 loss: 0.205
Actual params: [1.3803, 0.62  ]
-Original Grad: -0.021, -lr * Pred Grad:  0.008, New P: 1.388
-Original Grad: -0.015, -lr * Pred Grad:  0.011, New P: 0.631
iter 19 loss: 0.193
Actual params: [1.388 , 0.6309]
-Original Grad: -0.011, -lr * Pred Grad:  0.006, New P: 1.394
-Original Grad: -0.111, -lr * Pred Grad:  -0.011, New P: 0.620
iter 20 loss: 0.208
Actual params: [1.3945, 0.6196]
-Original Grad: -0.012, -lr * Pred Grad:  0.005, New P: 1.400
-Original Grad: -0.132, -lr * Pred Grad:  -0.031, New P: 0.588
Target params: [1.1812, 0.2779]
iter 0 loss: 0.250
Actual params: [0.5941, 0.5941]
-Original Grad: 0.009, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.101, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.216
Actual params: [0.6941, 0.4941]
-Original Grad: 0.001, -lr * Pred Grad:  0.077, New P: 0.771
-Original Grad: -0.027, -lr * Pred Grad:  -0.084, New P: 0.410
iter 2 loss: 0.207
Actual params: [0.7714, 0.4101]
-Original Grad: 0.005, -lr * Pred Grad:  0.083, New P: 0.854
-Original Grad: 0.003, -lr * Pred Grad:  -0.063, New P: 0.347
iter 3 loss: 0.202
Actual params: [0.854 , 0.3472]
-Original Grad: 0.005, -lr * Pred Grad:  0.086, New P: 0.940
-Original Grad: 0.012, -lr * Pred Grad:  -0.044, New P: 0.303
iter 4 loss: 0.200
Actual params: [0.9399, 0.3027]
-Original Grad: 0.001, -lr * Pred Grad:  0.075, New P: 1.015
-Original Grad: 0.015, -lr * Pred Grad:  -0.029, New P: 0.273
iter 5 loss: 0.199
Actual params: [1.0152, 0.2732]
-Original Grad: 0.000, -lr * Pred Grad:  0.066, New P: 1.081
-Original Grad: 0.016, -lr * Pred Grad:  -0.018, New P: 0.256
iter 6 loss: 0.198
Actual params: [1.081 , 0.2557]
-Original Grad: 0.002, -lr * Pred Grad:  0.066, New P: 1.147
-Original Grad: 0.014, -lr * Pred Grad:  -0.009, New P: 0.247
iter 7 loss: 0.198
Actual params: [1.1469, 0.2468]
-Original Grad: 0.002, -lr * Pred Grad:  0.067, New P: 1.214
-Original Grad: 0.015, -lr * Pred Grad:  -0.001, New P: 0.246
iter 8 loss: 0.198
Actual params: [1.214 , 0.2456]
-Original Grad: 0.002, -lr * Pred Grad:  0.066, New P: 1.280
-Original Grad: 0.012, -lr * Pred Grad:  0.004, New P: 0.250
iter 9 loss: 0.198
Actual params: [1.2799, 0.25  ]
-Original Grad: 0.001, -lr * Pred Grad:  0.063, New P: 1.343
-Original Grad: 0.013, -lr * Pred Grad:  0.010, New P: 0.260
iter 10 loss: 0.199
Actual params: [1.3432, 0.2596]
-Original Grad: -0.000, -lr * Pred Grad:  0.056, New P: 1.399
-Original Grad: 0.010, -lr * Pred Grad:  0.013, New P: 0.273
iter 11 loss: 0.200
Actual params: [1.399 , 0.2726]
-Original Grad: 0.000, -lr * Pred Grad:  0.052, New P: 1.451
-Original Grad: 0.010, -lr * Pred Grad:  0.016, New P: 0.289
iter 12 loss: 0.201
Actual params: [1.451 , 0.2887]
-Original Grad: -0.001, -lr * Pred Grad:  0.044, New P: 1.495
-Original Grad: 0.012, -lr * Pred Grad:  0.020, New P: 0.309
iter 13 loss: 0.202
Actual params: [1.4953, 0.3085]
-Original Grad: -0.001, -lr * Pred Grad:  0.036, New P: 1.532
-Original Grad: 0.007, -lr * Pred Grad:  0.021, New P: 0.329
iter 14 loss: 0.202
Actual params: [1.5316, 0.3292]
-Original Grad: -0.000, -lr * Pred Grad:  0.032, New P: 1.564
-Original Grad: 0.012, -lr * Pred Grad:  0.024, New P: 0.353
iter 15 loss: 0.203
Actual params: [1.5638, 0.3528]
-Original Grad: -0.001, -lr * Pred Grad:  0.027, New P: 1.591
-Original Grad: 0.003, -lr * Pred Grad:  0.023, New P: 0.376
iter 16 loss: 0.203
Actual params: [1.5906, 0.3755]
-Original Grad: -0.001, -lr * Pred Grad:  0.021, New P: 1.611
-Original Grad: 0.006, -lr * Pred Grad:  0.023, New P: 0.399
iter 17 loss: 0.203
Actual params: [1.6113, 0.3986]
-Original Grad: -0.001, -lr * Pred Grad:  0.016, New P: 1.627
-Original Grad: 0.001, -lr * Pred Grad:  0.022, New P: 0.420
iter 18 loss: 0.204
Actual params: [1.6271, 0.4202]
-Original Grad: -0.000, -lr * Pred Grad:  0.013, New P: 1.640
-Original Grad: -0.009, -lr * Pred Grad:  0.015, New P: 0.436
iter 19 loss: 0.205
Actual params: [1.6397, 0.4357]
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 1.653
-Original Grad: -0.028, -lr * Pred Grad:  0.001, New P: 0.437
iter 20 loss: 0.204
Actual params: [1.6526, 0.4371]
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 1.667
-Original Grad: -0.031, -lr * Pred Grad:  -0.012, New P: 0.425
Target params: [1.1812, 0.2779]
iter 0 loss: 0.502
Actual params: [0.5941, 0.5941]
-Original Grad: 0.327, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.295, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.464
Actual params: [0.6941, 0.4941]
-Original Grad: 0.153, -lr * Pred Grad:  0.092, New P: 0.786
-Original Grad: -0.199, -lr * Pred Grad:  -0.097, New P: 0.397
iter 2 loss: 0.416
Actual params: [0.7863, 0.3968]
-Original Grad: 0.153, -lr * Pred Grad:  0.091, New P: 0.877
-Original Grad: -0.069, -lr * Pred Grad:  -0.086, New P: 0.311
iter 3 loss: 0.365
Actual params: [0.8768, 0.311 ]
-Original Grad: 0.052, -lr * Pred Grad:  0.081, New P: 0.958
-Original Grad: 0.018, -lr * Pred Grad:  -0.067, New P: 0.244
iter 4 loss: 0.329
Actual params: [0.9581, 0.2436]
-Original Grad: -0.520, -lr * Pred Grad:  -0.002, New P: 0.956
-Original Grad: 0.080, -lr * Pred Grad:  -0.044, New P: 0.200
iter 5 loss: 0.311
Actual params: [0.9561, 0.1998]
-Original Grad: -1.451, -lr * Pred Grad:  -0.048, New P: 0.908
-Original Grad: 0.225, -lr * Pred Grad:  -0.005, New P: 0.195
iter 6 loss: 0.316
Actual params: [0.9078, 0.1946]
-Original Grad: -0.101, -lr * Pred Grad:  -0.045, New P: 0.862
-Original Grad: 0.078, -lr * Pred Grad:  0.005, New P: 0.199
iter 7 loss: 0.335
Actual params: [0.8624, 0.1992]
-Original Grad: 0.088, -lr * Pred Grad:  -0.037, New P: 0.825
-Original Grad: 0.065, -lr * Pred Grad:  0.011, New P: 0.210
iter 8 loss: 0.362
Actual params: [0.8253, 0.2103]
-Original Grad: 0.227, -lr * Pred Grad:  -0.026, New P: 0.799
-Original Grad: 0.057, -lr * Pred Grad:  0.016, New P: 0.226
iter 9 loss: 0.382
Actual params: [0.7995, 0.2264]
-Original Grad: 0.201, -lr * Pred Grad:  -0.017, New P: 0.783
-Original Grad: 0.072, -lr * Pred Grad:  0.022, New P: 0.248
iter 10 loss: 0.396
Actual params: [0.7826, 0.2483]
-Original Grad: 0.139, -lr * Pred Grad:  -0.011, New P: 0.772
-Original Grad: 0.036, -lr * Pred Grad:  0.023, New P: 0.272
iter 11 loss: 0.407
Actual params: [0.7717, 0.2715]
-Original Grad: 0.200, -lr * Pred Grad:  -0.004, New P: 0.768
-Original Grad: 0.042, -lr * Pred Grad:  0.025, New P: 0.297
iter 12 loss: 0.413
Actual params: [0.7679, 0.2968]
-Original Grad: 0.253, -lr * Pred Grad:  0.004, New P: 0.772
-Original Grad: 0.029, -lr * Pred Grad:  0.026, New P: 0.323
iter 13 loss: 0.412
Actual params: [0.7718, 0.3226]
-Original Grad: 0.126, -lr * Pred Grad:  0.007, New P: 0.779
-Original Grad: -0.019, -lr * Pred Grad:  0.021, New P: 0.344
iter 14 loss: 0.411
Actual params: [0.779 , 0.3438]
-Original Grad: 0.153, -lr * Pred Grad:  0.011, New P: 0.790
-Original Grad: 0.027, -lr * Pred Grad:  0.022, New P: 0.366
iter 15 loss: 0.408
Actual params: [0.79  , 0.3659]
-Original Grad: 0.291, -lr * Pred Grad:  0.018, New P: 0.808
-Original Grad: 0.023, -lr * Pred Grad:  0.022, New P: 0.388
iter 16 loss: 0.405
Actual params: [0.8082, 0.3883]
-Original Grad: 0.106, -lr * Pred Grad:  0.020, New P: 0.828
-Original Grad: -0.002, -lr * Pred Grad:  0.020, New P: 0.408
iter 17 loss: 0.402
Actual params: [0.8278, 0.4085]
-Original Grad: 0.071, -lr * Pred Grad:  0.020, New P: 0.848
-Original Grad: -0.138, -lr * Pred Grad:  0.003, New P: 0.412
iter 18 loss: 0.401
Actual params: [0.8477, 0.4117]
-Original Grad: 0.071, -lr * Pred Grad:  0.020, New P: 0.868
-Original Grad: -0.017, -lr * Pred Grad:  0.001, New P: 0.413
iter 19 loss: 0.399
Actual params: [0.8678, 0.4129]
-Original Grad: -0.030, -lr * Pred Grad:  0.017, New P: 0.885
-Original Grad: -0.116, -lr * Pred Grad:  -0.011, New P: 0.402
iter 20 loss: 0.396
Actual params: [0.8852, 0.4019]
-Original Grad: -0.075, -lr * Pred Grad:  0.014, New P: 0.899
-Original Grad: -0.094, -lr * Pred Grad:  -0.019, New P: 0.383
Target params: [1.1812, 0.2779]
iter 0 loss: 0.450
Actual params: [0.5941, 0.5941]
-Original Grad: 0.341, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.191, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.361
Actual params: [0.6941, 0.4941]
-Original Grad: 0.265, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.140, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.261
Actual params: [0.7926, 0.396 ]
-Original Grad: 0.269, -lr * Pred Grad:  0.098, New P: 0.891
-Original Grad: -0.018, -lr * Pred Grad:  -0.080, New P: 0.315
iter 3 loss: 0.215
Actual params: [0.8911, 0.3155]
-Original Grad: 0.001, -lr * Pred Grad:  0.081, New P: 0.972
-Original Grad: 0.035, -lr * Pred Grad:  -0.057, New P: 0.259
iter 4 loss: 0.204
Actual params: [0.9719, 0.2588]
-Original Grad: 0.009, -lr * Pred Grad:  0.069, New P: 1.041
-Original Grad: 0.033, -lr * Pred Grad:  -0.040, New P: 0.219
iter 5 loss: 0.210
Actual params: [1.0411, 0.2187]
-Original Grad: -0.029, -lr * Pred Grad:  0.057, New P: 1.098
-Original Grad: 0.046, -lr * Pred Grad:  -0.024, New P: 0.195
iter 6 loss: 0.226
Actual params: [1.0977, 0.1946]
-Original Grad: -0.042, -lr * Pred Grad:  0.045, New P: 1.143
-Original Grad: 0.067, -lr * Pred Grad:  -0.007, New P: 0.187
iter 7 loss: 0.240
Actual params: [1.1428, 0.1875]
-Original Grad: -0.050, -lr * Pred Grad:  0.035, New P: 1.178
-Original Grad: 0.085, -lr * Pred Grad:  0.010, New P: 0.197
iter 8 loss: 0.255
Actual params: [1.1776, 0.1972]
-Original Grad: -0.051, -lr * Pred Grad:  0.026, New P: 1.203
-Original Grad: 0.061, -lr * Pred Grad:  0.019, New P: 0.216
iter 9 loss: 0.263
Actual params: [1.2034, 0.2165]
-Original Grad: -0.068, -lr * Pred Grad:  0.017, New P: 1.220
-Original Grad: 0.103, -lr * Pred Grad:  0.033, New P: 0.250
iter 10 loss: 0.267
Actual params: [1.2199, 0.2496]
-Original Grad: -0.047, -lr * Pred Grad:  0.010, New P: 1.230
-Original Grad: 0.072, -lr * Pred Grad:  0.040, New P: 0.290
iter 11 loss: 0.271
Actual params: [1.2303, 0.29  ]
-Original Grad: -0.039, -lr * Pred Grad:  0.006, New P: 1.236
-Original Grad: 0.059, -lr * Pred Grad:  0.045, New P: 0.335
iter 12 loss: 0.273
Actual params: [1.2359, 0.3349]
-Original Grad: -0.047, -lr * Pred Grad:  0.001, New P: 1.237
-Original Grad: 0.077, -lr * Pred Grad:  0.051, New P: 0.386
iter 13 loss: 0.278
Actual params: [1.2367, 0.3858]
-Original Grad: -0.041, -lr * Pred Grad:  -0.003, New P: 1.234
-Original Grad: 0.045, -lr * Pred Grad:  0.052, New P: 0.438
iter 14 loss: 0.280
Actual params: [1.2336, 0.4381]
-Original Grad: -0.029, -lr * Pred Grad:  -0.005, New P: 1.228
-Original Grad: 0.002, -lr * Pred Grad:  0.048, New P: 0.486
iter 15 loss: 0.287
Actual params: [1.2281, 0.4859]
-Original Grad: -0.026, -lr * Pred Grad:  -0.007, New P: 1.221
-Original Grad: -0.018, -lr * Pred Grad:  0.040, New P: 0.526
iter 16 loss: 0.295
Actual params: [1.2207, 0.5263]
-Original Grad: -0.011, -lr * Pred Grad:  -0.008, New P: 1.213
-Original Grad: -0.061, -lr * Pred Grad:  0.027, New P: 0.553
iter 17 loss: 0.304
Actual params: [1.213 , 0.5532]
-Original Grad: 0.001, -lr * Pred Grad:  -0.007, New P: 1.206
-Original Grad: -0.128, -lr * Pred Grad:  0.005, New P: 0.558
iter 18 loss: 0.302
Actual params: [1.206 , 0.5578]
-Original Grad: 0.010, -lr * Pred Grad:  -0.005, New P: 1.201
-Original Grad: -0.106, -lr * Pred Grad:  -0.011, New P: 0.547
iter 19 loss: 0.293
Actual params: [1.2007, 0.5472]
-Original Grad: -0.022, -lr * Pred Grad:  -0.007, New P: 1.194
-Original Grad: -0.065, -lr * Pred Grad:  -0.018, New P: 0.529
iter 20 loss: 0.280
Actual params: [1.1937, 0.5288]
-Original Grad: -0.003, -lr * Pred Grad:  -0.007, New P: 1.187
-Original Grad: -0.090, -lr * Pred Grad:  -0.028, New P: 0.500
Target params: [1.1812, 0.2779]
iter 0 loss: 0.617
Actual params: [0.5941, 0.5941]
-Original Grad: 0.220, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.394, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.548
Actual params: [0.6941, 0.4941]
-Original Grad: 0.073, -lr * Pred Grad:  0.087, New P: 0.781
-Original Grad: -0.111, -lr * Pred Grad:  -0.085, New P: 0.409
iter 2 loss: 0.490
Actual params: [0.7812, 0.4093]
-Original Grad: 0.024, -lr * Pred Grad:  0.074, New P: 0.855
-Original Grad: -0.009, -lr * Pred Grad:  -0.067, New P: 0.342
iter 3 loss: 0.461
Actual params: [0.8548, 0.3425]
-Original Grad: 0.012, -lr * Pred Grad:  0.063, New P: 0.918
-Original Grad: -0.008, -lr * Pred Grad:  -0.056, New P: 0.287
iter 4 loss: 0.457
Actual params: [0.9181, 0.2866]
-Original Grad: -0.000, -lr * Pred Grad:  0.053, New P: 0.972
-Original Grad: -0.001, -lr * Pred Grad:  -0.047, New P: 0.239
iter 5 loss: 0.456
Actual params: [0.9716, 0.2393]
-Original Grad: 0.001, -lr * Pred Grad:  0.046, New P: 1.018
-Original Grad: 0.021, -lr * Pred Grad:  -0.038, New P: 0.201
iter 6 loss: 0.456
Actual params: [1.0178, 0.2012]
-Original Grad: -0.004, -lr * Pred Grad:  0.039, New P: 1.057
-Original Grad: 0.011, -lr * Pred Grad:  -0.032, New P: 0.169
iter 7 loss: 0.457
Actual params: [1.0573, 0.1694]
-Original Grad: -0.003, -lr * Pred Grad:  0.034, New P: 1.091
-Original Grad: 0.017, -lr * Pred Grad:  -0.026, New P: 0.143
iter 8 loss: 0.458
Actual params: [1.0915, 0.1434]
-Original Grad: -0.001, -lr * Pred Grad:  0.030, New P: 1.122
-Original Grad: 0.020, -lr * Pred Grad:  -0.021, New P: 0.123
iter 9 loss: 0.459
Actual params: [1.1215, 0.1227]
-Original Grad: 0.003, -lr * Pred Grad:  0.027, New P: 1.149
-Original Grad: 0.014, -lr * Pred Grad:  -0.017, New P: 0.106
iter 10 loss: 0.460
Actual params: [1.149, 0.106]
-Original Grad: 0.005, -lr * Pred Grad:  0.026, New P: 1.175
-Original Grad: 0.017, -lr * Pred Grad:  -0.013, New P: 0.093
iter 11 loss: 0.461
Actual params: [1.1746, 0.0929]
-Original Grad: -0.002, -lr * Pred Grad:  0.023, New P: 1.197
-Original Grad: 0.012, -lr * Pred Grad:  -0.010, New P: 0.083
iter 12 loss: 0.462
Actual params: [1.1972, 0.0826]
-Original Grad: 0.002, -lr * Pred Grad:  0.021, New P: 1.218
-Original Grad: 0.019, -lr * Pred Grad:  -0.007, New P: 0.076
iter 13 loss: 0.463
Actual params: [1.218 , 0.0755]
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 1.237
-Original Grad: 0.018, -lr * Pred Grad:  -0.004, New P: 0.071
iter 14 loss: 0.464
Actual params: [1.2369, 0.0712]
-Original Grad: 0.002, -lr * Pred Grad:  0.017, New P: 1.254
-Original Grad: 0.006, -lr * Pred Grad:  -0.003, New P: 0.068
iter 15 loss: 0.464
Actual params: [1.2543, 0.068 ]
-Original Grad: 0.003, -lr * Pred Grad:  0.016, New P: 1.271
-Original Grad: 0.009, -lr * Pred Grad:  -0.002, New P: 0.066
iter 16 loss: 0.465
Actual params: [1.2707, 0.0663]
-Original Grad: -0.001, -lr * Pred Grad:  0.015, New P: 1.285
-Original Grad: 0.024, -lr * Pred Grad:  0.001, New P: 0.068
iter 17 loss: 0.466
Actual params: [1.2854, 0.0676]
-Original Grad: 0.004, -lr * Pred Grad:  0.014, New P: 1.300
-Original Grad: 0.007, -lr * Pred Grad:  0.002, New P: 0.070
iter 18 loss: 0.466
Actual params: [1.2995, 0.0696]
-Original Grad: -0.002, -lr * Pred Grad:  0.012, New P: 1.312
-Original Grad: 0.029, -lr * Pred Grad:  0.005, New P: 0.075
iter 19 loss: 0.468
Actual params: [1.312, 0.075]
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 1.324
-Original Grad: 0.016, -lr * Pred Grad:  0.007, New P: 0.082
iter 20 loss: 0.469
Actual params: [1.3236, 0.0819]
-Original Grad: 0.003, -lr * Pred Grad:  0.011, New P: 1.335
-Original Grad: 0.022, -lr * Pred Grad:  0.009, New P: 0.091
Target params: [1.1812, 0.2779]
iter 0 loss: 0.947
Actual params: [0.5941, 0.5941]
-Original Grad: -0.022, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.233, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.827
Actual params: [0.4941, 0.4941]
-Original Grad: -0.138, -lr * Pred Grad:  -0.084, New P: 0.410
-Original Grad: -0.377, -lr * Pred Grad:  -0.099, New P: 0.396
iter 2 loss: 0.734
Actual params: [0.4099, 0.3955]
-Original Grad: -0.076, -lr * Pred Grad:  -0.088, New P: 0.322
-Original Grad: -0.679, -lr * Pred Grad:  -0.095, New P: 0.300
iter 3 loss: 0.674
Actual params: [0.3224, 0.3004]
-Original Grad: 0.036, -lr * Pred Grad:  -0.057, New P: 0.265
-Original Grad: -0.348, -lr * Pred Grad:  -0.095, New P: 0.206
iter 4 loss: 0.642
Actual params: [0.2651, 0.2058]
-Original Grad: 0.045, -lr * Pred Grad:  -0.032, New P: 0.233
-Original Grad: -0.197, -lr * Pred Grad:  -0.090, New P: 0.116
iter 5 loss: 0.629
Actual params: [0.2329, 0.116 ]
-Original Grad: 0.112, -lr * Pred Grad:  0.006, New P: 0.239
-Original Grad: -0.083, -lr * Pred Grad:  -0.082, New P: 0.034
iter 6 loss: 0.622
Actual params: [0.2387, 0.0341]
-Original Grad: 0.151, -lr * Pred Grad:  0.034, New P: 0.273
-Original Grad: -0.011, -lr * Pred Grad:  -0.072, New P: -0.038
iter 7 loss: 0.613
Actual params: [ 0.273 , -0.0381]
-Original Grad: 0.180, -lr * Pred Grad:  0.053, New P: 0.326
-Original Grad: -0.037, -lr * Pred Grad:  -0.066, New P: -0.104
iter 8 loss: 0.603
Actual params: [ 0.3265, -0.1037]
-Original Grad: 0.154, -lr * Pred Grad:  0.064, New P: 0.391
-Original Grad: -0.007, -lr * Pred Grad:  -0.059, New P: -0.162
iter 9 loss: 0.585
Actual params: [ 0.3907, -0.1623]
-Original Grad: 0.188, -lr * Pred Grad:  0.074, New P: 0.464
-Original Grad: 0.017, -lr * Pred Grad:  -0.051, New P: -0.214
iter 10 loss: 0.559
Actual params: [ 0.4642, -0.2137]
-Original Grad: 0.065, -lr * Pred Grad:  0.073, New P: 0.537
-Original Grad: -0.002, -lr * Pred Grad:  -0.046, New P: -0.260
iter 11 loss: 0.543
Actual params: [ 0.5371, -0.2599]
-Original Grad: 0.039, -lr * Pred Grad:  0.070, New P: 0.607
-Original Grad: 0.015, -lr * Pred Grad:  -0.041, New P: -0.301
iter 12 loss: 0.526
Actual params: [ 0.607 , -0.3007]
-Original Grad: 0.003, -lr * Pred Grad:  0.063, New P: 0.670
-Original Grad: 0.023, -lr * Pred Grad:  -0.035, New P: -0.336
iter 13 loss: 0.514
Actual params: [ 0.6704, -0.3361]
-Original Grad: -0.017, -lr * Pred Grad:  0.055, New P: 0.726
-Original Grad: 0.033, -lr * Pred Grad:  -0.030, New P: -0.366
iter 14 loss: 0.502
Actual params: [ 0.7256, -0.3664]
-Original Grad: -0.022, -lr * Pred Grad:  0.047, New P: 0.773
-Original Grad: 0.041, -lr * Pred Grad:  -0.025, New P: -0.392
iter 15 loss: 0.498
Actual params: [ 0.7729, -0.3915]
-Original Grad: -0.036, -lr * Pred Grad:  0.038, New P: 0.811
-Original Grad: 0.063, -lr * Pred Grad:  -0.019, New P: -0.411
iter 16 loss: 0.498
Actual params: [ 0.8112, -0.4109]
-Original Grad: -0.038, -lr * Pred Grad:  0.030, New P: 0.841
-Original Grad: 0.067, -lr * Pred Grad:  -0.014, New P: -0.425
iter 17 loss: 0.497
Actual params: [ 0.8413, -0.4247]
-Original Grad: -0.043, -lr * Pred Grad:  0.022, New P: 0.863
-Original Grad: 0.084, -lr * Pred Grad:  -0.008, New P: -0.433
iter 18 loss: 0.499
Actual params: [ 0.8631, -0.4327]
-Original Grad: -0.034, -lr * Pred Grad:  0.016, New P: 0.879
-Original Grad: 0.067, -lr * Pred Grad:  -0.004, New P: -0.436
iter 19 loss: 0.500
Actual params: [ 0.8787, -0.4362]
-Original Grad: -0.043, -lr * Pred Grad:  0.009, New P: 0.887
-Original Grad: 0.093, -lr * Pred Grad:  0.002, New P: -0.434
iter 20 loss: 0.500
Actual params: [ 0.8874, -0.4343]
-Original Grad: -0.046, -lr * Pred Grad:  0.002, New P: 0.890
-Original Grad: 0.097, -lr * Pred Grad:  0.007, New P: -0.427
Target params: [1.1812, 0.2779]
iter 0 loss: 0.477
Actual params: [0.5941, 0.5941]
-Original Grad: -0.220, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.233, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.388
Actual params: [0.4941, 0.4941]
-Original Grad: -0.115, -lr * Pred Grad:  -0.094, New P: 0.400
-Original Grad: -0.049, -lr * Pred Grad:  -0.081, New P: 0.413
iter 2 loss: 0.349
Actual params: [0.4002, 0.4132]
-Original Grad: -0.069, -lr * Pred Grad:  -0.087, New P: 0.313
-Original Grad: 0.025, -lr * Pred Grad:  -0.055, New P: 0.358
iter 3 loss: 0.326
Actual params: [0.3132, 0.3578]
-Original Grad: -0.057, -lr * Pred Grad:  -0.082, New P: 0.231
-Original Grad: 0.029, -lr * Pred Grad:  -0.038, New P: 0.320
iter 4 loss: 0.309
Actual params: [0.231 , 0.3196]
-Original Grad: -0.047, -lr * Pred Grad:  -0.078, New P: 0.153
-Original Grad: 0.011, -lr * Pred Grad:  -0.030, New P: 0.290
iter 5 loss: 0.296
Actual params: [0.1531, 0.2897]
-Original Grad: -0.032, -lr * Pred Grad:  -0.073, New P: 0.080
-Original Grad: 0.009, -lr * Pred Grad:  -0.024, New P: 0.266
iter 6 loss: 0.287
Actual params: [0.0801, 0.266 ]
-Original Grad: -0.031, -lr * Pred Grad:  -0.069, New P: 0.011
-Original Grad: 0.012, -lr * Pred Grad:  -0.018, New P: 0.248
iter 7 loss: 0.279
Actual params: [0.0112, 0.248 ]
-Original Grad: -0.022, -lr * Pred Grad:  -0.065, New P: -0.054
-Original Grad: 0.009, -lr * Pred Grad:  -0.014, New P: 0.234
iter 8 loss: 0.272
Actual params: [-0.0535,  0.2339]
-Original Grad: -0.024, -lr * Pred Grad:  -0.062, New P: -0.115
-Original Grad: 0.010, -lr * Pred Grad:  -0.011, New P: 0.223
iter 9 loss: 0.268
Actual params: [-0.115 ,  0.2234]
-Original Grad: -0.023, -lr * Pred Grad:  -0.059, New P: -0.174
-Original Grad: 0.010, -lr * Pred Grad:  -0.008, New P: 0.216
iter 10 loss: 0.264
Actual params: [-0.1737,  0.2158]
-Original Grad: -0.024, -lr * Pred Grad:  -0.057, New P: -0.230
-Original Grad: 0.018, -lr * Pred Grad:  -0.003, New P: 0.213
iter 11 loss: 0.260
Actual params: [-0.2303,  0.2127]
-Original Grad: -0.018, -lr * Pred Grad:  -0.054, New P: -0.284
-Original Grad: 0.008, -lr * Pred Grad:  -0.001, New P: 0.211
iter 12 loss: 0.257
Actual params: [-0.2842,  0.2114]
-Original Grad: -0.013, -lr * Pred Grad:  -0.051, New P: -0.335
-Original Grad: 0.013, -lr * Pred Grad:  0.001, New P: 0.213
iter 13 loss: 0.254
Actual params: [-0.3349,  0.2128]
-Original Grad: -0.014, -lr * Pred Grad:  -0.048, New P: -0.383
-Original Grad: 0.010, -lr * Pred Grad:  0.003, New P: 0.216
iter 14 loss: 0.252
Actual params: [-0.3831,  0.2161]
-Original Grad: -0.012, -lr * Pred Grad:  -0.046, New P: -0.429
-Original Grad: 0.010, -lr * Pred Grad:  0.005, New P: 0.221
iter 15 loss: 0.251
Actual params: [-0.4288,  0.2211]
-Original Grad: -0.009, -lr * Pred Grad:  -0.043, New P: -0.472
-Original Grad: 0.009, -lr * Pred Grad:  0.006, New P: 0.227
iter 16 loss: 0.250
Actual params: [-0.4717,  0.2274]
-Original Grad: -0.009, -lr * Pred Grad:  -0.041, New P: -0.512
-Original Grad: 0.011, -lr * Pred Grad:  0.008, New P: 0.235
iter 17 loss: 0.249
Actual params: [-0.5123,  0.2353]
-Original Grad: -0.008, -lr * Pred Grad:  -0.038, New P: -0.551
-Original Grad: 0.006, -lr * Pred Grad:  0.008, New P: 0.244
iter 18 loss: 0.248
Actual params: [-0.5505,  0.2438]
-Original Grad: -0.005, -lr * Pred Grad:  -0.036, New P: -0.586
-Original Grad: 0.007, -lr * Pred Grad:  0.009, New P: 0.253
iter 19 loss: 0.248
Actual params: [-0.5862,  0.2531]
-Original Grad: -0.006, -lr * Pred Grad:  -0.033, New P: -0.620
-Original Grad: 0.009, -lr * Pred Grad:  0.010, New P: 0.263
iter 20 loss: 0.248
Actual params: [-0.6197,  0.2633]
-Original Grad: -0.005, -lr * Pred Grad:  -0.031, New P: -0.651
-Original Grad: 0.009, -lr * Pred Grad:  0.011, New P: 0.275
Target params: [1.1812, 0.2779]
iter 0 loss: 0.393
Actual params: [0.5941, 0.5941]
-Original Grad: -0.023, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.170, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.332
Actual params: [0.4941, 0.4941]
-Original Grad: -0.075, -lr * Pred Grad:  -0.091, New P: 0.403
-Original Grad: -0.134, -lr * Pred Grad:  -0.099, New P: 0.395
iter 2 loss: 0.277
Actual params: [0.4034, 0.3954]
-Original Grad: -0.080, -lr * Pred Grad:  -0.095, New P: 0.309
-Original Grad: -0.052, -lr * Pred Grad:  -0.089, New P: 0.306
iter 3 loss: 0.238
Actual params: [0.3087, 0.3062]
-Original Grad: -0.046, -lr * Pred Grad:  -0.094, New P: 0.215
-Original Grad: -0.027, -lr * Pred Grad:  -0.080, New P: 0.227
iter 4 loss: 0.219
Actual params: [0.2148, 0.2265]
-Original Grad: -0.033, -lr * Pred Grad:  -0.091, New P: 0.124
-Original Grad: -0.008, -lr * Pred Grad:  -0.069, New P: 0.157
iter 5 loss: 0.209
Actual params: [0.1239, 0.1573]
-Original Grad: -0.025, -lr * Pred Grad:  -0.087, New P: 0.037
-Original Grad: -0.006, -lr * Pred Grad:  -0.061, New P: 0.096
iter 6 loss: 0.203
Actual params: [0.0367, 0.0961]
-Original Grad: -0.025, -lr * Pred Grad:  -0.084, New P: -0.048
-Original Grad: 0.007, -lr * Pred Grad:  -0.052, New P: 0.044
iter 7 loss: 0.201
Actual params: [-0.0477,  0.0443]
-Original Grad: -0.019, -lr * Pred Grad:  -0.081, New P: -0.128
-Original Grad: 0.008, -lr * Pred Grad:  -0.044, New P: 0.001
iter 8 loss: 0.193
Actual params: [-0.1285,  0.0005]
-Original Grad: -0.008, -lr * Pred Grad:  -0.075, New P: -0.203
-Original Grad: 0.012, -lr * Pred Grad:  -0.036, New P: -0.036
iter 9 loss: 0.187
Actual params: [-0.2031, -0.0358]
-Original Grad: -0.003, -lr * Pred Grad:  -0.068, New P: -0.271
-Original Grad: 0.008, -lr * Pred Grad:  -0.031, New P: -0.066
iter 10 loss: 0.185
Actual params: [-0.2707, -0.0663]
-Original Grad: 0.001, -lr * Pred Grad:  -0.060, New P: -0.331
-Original Grad: 0.013, -lr * Pred Grad:  -0.025, New P: -0.091
iter 11 loss: 0.186
Actual params: [-0.3308, -0.0908]
-Original Grad: 0.003, -lr * Pred Grad:  -0.053, New P: -0.384
-Original Grad: 0.010, -lr * Pred Grad:  -0.020, New P: -0.111
iter 12 loss: 0.189
Actual params: [-0.3836, -0.1106]
-Original Grad: 0.005, -lr * Pred Grad:  -0.046, New P: -0.429
-Original Grad: 0.015, -lr * Pred Grad:  -0.015, New P: -0.125
iter 13 loss: 0.191
Actual params: [-0.4292, -0.1253]
-Original Grad: 0.007, -lr * Pred Grad:  -0.039, New P: -0.468
-Original Grad: 0.018, -lr * Pred Grad:  -0.009, New P: -0.135
iter 14 loss: 0.194
Actual params: [-0.4678, -0.1346]
-Original Grad: 0.006, -lr * Pred Grad:  -0.033, New P: -0.500
-Original Grad: 0.021, -lr * Pred Grad:  -0.004, New P: -0.139
iter 15 loss: 0.196
Actual params: [-0.5004, -0.1385]
-Original Grad: 0.007, -lr * Pred Grad:  -0.027, New P: -0.527
-Original Grad: 0.017, -lr * Pred Grad:  0.000, New P: -0.138
iter 16 loss: 0.197
Actual params: [-0.5273, -0.1385]
-Original Grad: 0.009, -lr * Pred Grad:  -0.021, New P: -0.548
-Original Grad: 0.021, -lr * Pred Grad:  0.005, New P: -0.134
iter 17 loss: 0.198
Actual params: [-0.5482, -0.1338]
-Original Grad: 0.008, -lr * Pred Grad:  -0.016, New P: -0.564
-Original Grad: 0.015, -lr * Pred Grad:  0.008, New P: -0.126
iter 18 loss: 0.198
Actual params: [-0.5642, -0.1262]
-Original Grad: 0.007, -lr * Pred Grad:  -0.012, New P: -0.576
-Original Grad: 0.015, -lr * Pred Grad:  0.010, New P: -0.116
iter 19 loss: 0.198
Actual params: [-0.5763, -0.116 ]
-Original Grad: 0.008, -lr * Pred Grad:  -0.008, New P: -0.584
-Original Grad: 0.018, -lr * Pred Grad:  0.013, New P: -0.103
iter 20 loss: 0.197
Actual params: [-0.5841, -0.1029]
-Original Grad: 0.007, -lr * Pred Grad:  -0.004, New P: -0.588
-Original Grad: 0.015, -lr * Pred Grad:  0.015, New P: -0.088
Target params: [1.1812, 0.2779]
iter 0 loss: 0.990
Actual params: [0.5941, 0.5941]
-Original Grad: 0.829, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.138, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.871
Actual params: [0.6941, 0.4941]
-Original Grad: 0.925, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.065, -lr * Pred Grad:  -0.092, New P: 0.402
iter 2 loss: 0.698
Actual params: [0.7942, 0.4018]
-Original Grad: 0.646, -lr * Pred Grad:  0.098, New P: 0.892
-Original Grad: 0.007, -lr * Pred Grad:  -0.068, New P: 0.333
iter 3 loss: 0.557
Actual params: [0.8924, 0.3333]
-Original Grad: 0.227, -lr * Pred Grad:  0.089, New P: 0.981
-Original Grad: -0.027, -lr * Pred Grad:  -0.066, New P: 0.268
iter 4 loss: 0.556
Actual params: [0.981 , 0.2678]
-Original Grad: 0.071, -lr * Pred Grad:  0.078, New P: 1.059
-Original Grad: -0.030, -lr * Pred Grad:  -0.065, New P: 0.203
iter 5 loss: 0.570
Actual params: [1.0586, 0.203 ]
-Original Grad: 0.023, -lr * Pred Grad:  0.068, New P: 1.126
-Original Grad: -0.020, -lr * Pred Grad:  -0.062, New P: 0.141
iter 6 loss: 0.603
Actual params: [1.1264, 0.1412]
-Original Grad: 0.022, -lr * Pred Grad:  0.060, New P: 1.186
-Original Grad: -0.041, -lr * Pred Grad:  -0.065, New P: 0.076
iter 7 loss: 0.638
Actual params: [1.1863, 0.0762]
-Original Grad: -0.003, -lr * Pred Grad:  0.053, New P: 1.239
-Original Grad: -0.013, -lr * Pred Grad:  -0.061, New P: 0.015
iter 8 loss: 0.676
Actual params: [1.239 , 0.0152]
-Original Grad: -0.022, -lr * Pred Grad:  0.046, New P: 1.285
-Original Grad: -0.002, -lr * Pred Grad:  -0.055, New P: -0.040
iter 9 loss: 0.700
Actual params: [ 1.2851, -0.0396]
-Original Grad: -0.005, -lr * Pred Grad:  0.041, New P: 1.326
-Original Grad: -0.009, -lr * Pred Grad:  -0.051, New P: -0.091
iter 10 loss: 0.731
Actual params: [ 1.326, -0.091]
-Original Grad: -0.043, -lr * Pred Grad:  0.035, New P: 1.361
-Original Grad: -0.000, -lr * Pred Grad:  -0.046, New P: -0.137
iter 11 loss: 0.759
Actual params: [ 1.3612, -0.1371]
-Original Grad: -0.031, -lr * Pred Grad:  0.031, New P: 1.392
-Original Grad: -0.008, -lr * Pred Grad:  -0.044, New P: -0.181
iter 12 loss: 0.782
Actual params: [ 1.3918, -0.1809]
-Original Grad: -0.016, -lr * Pred Grad:  0.027, New P: 1.419
-Original Grad: -0.013, -lr * Pred Grad:  -0.043, New P: -0.224
iter 13 loss: 0.803
Actual params: [ 1.4189, -0.224 ]
-Original Grad: -0.018, -lr * Pred Grad:  0.024, New P: 1.443
-Original Grad: -0.007, -lr * Pred Grad:  -0.041, New P: -0.265
iter 14 loss: 0.821
Actual params: [ 1.4427, -0.2649]
-Original Grad: -0.043, -lr * Pred Grad:  0.020, New P: 1.463
-Original Grad: -0.005, -lr * Pred Grad:  -0.039, New P: -0.303
iter 15 loss: 0.823
Actual params: [ 1.4627, -0.3034]
-Original Grad: -0.082, -lr * Pred Grad:  0.015, New P: 1.478
-Original Grad: 0.004, -lr * Pred Grad:  -0.034, New P: -0.337
iter 16 loss: 0.839
Actual params: [ 1.478 , -0.3372]
-Original Grad: -0.042, -lr * Pred Grad:  0.012, New P: 1.490
-Original Grad: 0.000, -lr * Pred Grad:  -0.031, New P: -0.368
iter 17 loss: 0.849
Actual params: [ 1.4905, -0.3678]
-Original Grad: -0.065, -lr * Pred Grad:  0.009, New P: 1.499
-Original Grad: 0.009, -lr * Pred Grad:  -0.025, New P: -0.393
iter 18 loss: 0.857
Actual params: [ 1.4994, -0.3929]
-Original Grad: -0.055, -lr * Pred Grad:  0.006, New P: 1.506
-Original Grad: 0.003, -lr * Pred Grad:  -0.022, New P: -0.415
iter 19 loss: 0.863
Actual params: [ 1.5056, -0.4149]
-Original Grad: -0.048, -lr * Pred Grad:  0.004, New P: 1.510
-Original Grad: 0.006, -lr * Pred Grad:  -0.018, New P: -0.433
iter 20 loss: 0.867
Actual params: [ 1.5096, -0.4329]
-Original Grad: -0.094, -lr * Pred Grad:  0.000, New P: 1.510
-Original Grad: 0.003, -lr * Pred Grad:  -0.016, New P: -0.448
Target params: [1.1812, 0.2779]
iter 0 loss: 0.319
Actual params: [0.5941, 0.5941]
-Original Grad: -0.054, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.014, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.303
Actual params: [0.4941, 0.4941]
-Original Grad: 0.030, -lr * Pred Grad:  -0.023, New P: 0.471
-Original Grad: 0.003, -lr * Pred Grad:  -0.053, New P: 0.441
iter 2 loss: 0.305
Actual params: [0.4708, 0.4411]
-Original Grad: 0.014, -lr * Pred Grad:  -0.003, New P: 0.468
-Original Grad: -0.019, -lr * Pred Grad:  -0.076, New P: 0.365
iter 3 loss: 0.307
Actual params: [0.4675, 0.3655]
-Original Grad: -0.008, -lr * Pred Grad:  -0.010, New P: 0.458
-Original Grad: -0.000, -lr * Pred Grad:  -0.063, New P: 0.302
iter 4 loss: 0.308
Actual params: [0.458 , 0.3025]
-Original Grad: -0.005, -lr * Pred Grad:  -0.012, New P: 0.446
-Original Grad: 0.001, -lr * Pred Grad:  -0.051, New P: 0.251
iter 5 loss: 0.308
Actual params: [0.4457, 0.2512]
-Original Grad: -0.005, -lr * Pred Grad:  -0.014, New P: 0.432
-Original Grad: 0.002, -lr * Pred Grad:  -0.039, New P: 0.212
iter 6 loss: 0.308
Actual params: [0.4315, 0.2124]
-Original Grad: -0.017, -lr * Pred Grad:  -0.025, New P: 0.406
-Original Grad: -0.001, -lr * Pred Grad:  -0.036, New P: 0.176
iter 7 loss: 0.309
Actual params: [0.4063, 0.1764]
-Original Grad: 0.001, -lr * Pred Grad:  -0.021, New P: 0.385
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: 0.144
iter 8 loss: 0.309
Actual params: [0.3851, 0.1437]
-Original Grad: -0.017, -lr * Pred Grad:  -0.030, New P: 0.355
-Original Grad: 0.010, -lr * Pred Grad:  -0.008, New P: 0.136
iter 9 loss: 0.311
Actual params: [0.3547, 0.1358]
-Original Grad: 0.039, -lr * Pred Grad:  0.001, New P: 0.355
-Original Grad: -0.003, -lr * Pred Grad:  -0.012, New P: 0.124
iter 10 loss: 0.311
Actual params: [0.3553, 0.1235]
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: 0.357
-Original Grad: -0.004, -lr * Pred Grad:  -0.018, New P: 0.106
iter 11 loss: 0.310
Actual params: [0.3568, 0.1059]
-Original Grad: -0.002, -lr * Pred Grad:  0.000, New P: 0.357
-Original Grad: 0.002, -lr * Pred Grad:  -0.013, New P: 0.093
iter 12 loss: 0.310
Actual params: [0.3568, 0.0932]
-Original Grad: 0.011, -lr * Pred Grad:  0.007, New P: 0.364
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: 0.082
iter 13 loss: 0.309
Actual params: [0.3638, 0.0818]
-Original Grad: -0.013, -lr * Pred Grad:  -0.002, New P: 0.362
-Original Grad: 0.004, -lr * Pred Grad:  -0.003, New P: 0.079
iter 14 loss: 0.309
Actual params: [0.3622, 0.0788]
-Original Grad: -0.007, -lr * Pred Grad:  -0.006, New P: 0.357
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 0.075
iter 15 loss: 0.309
Actual params: [0.3565, 0.0753]
-Original Grad: -0.001, -lr * Pred Grad:  -0.006, New P: 0.351
-Original Grad: 0.005, -lr * Pred Grad:  0.007, New P: 0.082
iter 16 loss: 0.310
Actual params: [0.3509, 0.0821]
-Original Grad: -0.017, -lr * Pred Grad:  -0.015, New P: 0.336
-Original Grad: 0.006, -lr * Pred Grad:  0.017, New P: 0.099
iter 17 loss: 0.312
Actual params: [0.3356, 0.0988]
-Original Grad: 0.012, -lr * Pred Grad:  -0.007, New P: 0.329
-Original Grad: 0.003, -lr * Pred Grad:  0.020, New P: 0.118
iter 18 loss: 0.314
Actual params: [0.3288, 0.1184]
-Original Grad: 0.001, -lr * Pred Grad:  -0.006, New P: 0.323
-Original Grad: 0.005, -lr * Pred Grad:  0.026, New P: 0.144
iter 19 loss: 0.316
Actual params: [0.3232, 0.1441]
-Original Grad: 0.018, -lr * Pred Grad:  0.006, New P: 0.329
-Original Grad: -0.002, -lr * Pred Grad:  0.020, New P: 0.164
iter 20 loss: 0.316
Actual params: [0.3291, 0.1639]
-Original Grad: 0.024, -lr * Pred Grad:  0.019, New P: 0.348
-Original Grad: -0.011, -lr * Pred Grad:  -0.003, New P: 0.161
Target params: [1.1812, 0.2779]
iter 0 loss: 0.445
Actual params: [0.5941, 0.5941]
-Original Grad: -0.072, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.907, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.293
Actual params: [0.4941, 0.4941]
-Original Grad: -0.211, -lr * Pred Grad:  -0.092, New P: 0.402
-Original Grad: -0.428, -lr * Pred Grad:  -0.092, New P: 0.402
iter 2 loss: 0.319
Actual params: [0.402 , 0.4017]
-Original Grad: 0.050, -lr * Pred Grad:  -0.056, New P: 0.346
-Original Grad: -0.024, -lr * Pred Grad:  -0.073, New P: 0.329
iter 3 loss: 0.342
Actual params: [0.3464, 0.3288]
-Original Grad: 0.125, -lr * Pred Grad:  -0.012, New P: 0.334
-Original Grad: 0.008, -lr * Pred Grad:  -0.059, New P: 0.269
iter 4 loss: 0.343
Actual params: [0.3343, 0.2695]
-Original Grad: 0.148, -lr * Pred Grad:  0.018, New P: 0.352
-Original Grad: -0.006, -lr * Pred Grad:  -0.050, New P: 0.219
iter 5 loss: 0.337
Actual params: [0.3524, 0.2191]
-Original Grad: 0.165, -lr * Pred Grad:  0.039, New P: 0.391
-Original Grad: 0.004, -lr * Pred Grad:  -0.043, New P: 0.176
iter 6 loss: 0.328
Actual params: [0.3913, 0.1758]
-Original Grad: 0.114, -lr * Pred Grad:  0.048, New P: 0.440
-Original Grad: 0.010, -lr * Pred Grad:  -0.037, New P: 0.139
iter 7 loss: 0.320
Actual params: [0.4396, 0.1386]
-Original Grad: 0.064, -lr * Pred Grad:  0.051, New P: 0.490
-Original Grad: 0.010, -lr * Pred Grad:  -0.032, New P: 0.106
iter 8 loss: 0.314
Actual params: [0.4903, 0.1062]
-Original Grad: 0.039, -lr * Pred Grad:  0.050, New P: 0.540
-Original Grad: 0.006, -lr * Pred Grad:  -0.028, New P: 0.078
iter 9 loss: 0.310
Actual params: [0.5401, 0.0778]
-Original Grad: -0.007, -lr * Pred Grad:  0.044, New P: 0.584
-Original Grad: 0.009, -lr * Pred Grad:  -0.025, New P: 0.053
iter 10 loss: 0.306
Actual params: [0.5836, 0.0528]
-Original Grad: -0.019, -lr * Pred Grad:  0.036, New P: 0.620
-Original Grad: 0.007, -lr * Pred Grad:  -0.022, New P: 0.031
iter 11 loss: 0.305
Actual params: [0.62  , 0.0308]
-Original Grad: -0.019, -lr * Pred Grad:  0.030, New P: 0.650
-Original Grad: 0.009, -lr * Pred Grad:  -0.019, New P: 0.011
iter 12 loss: 0.304
Actual params: [0.6501, 0.0114]
-Original Grad: -0.016, -lr * Pred Grad:  0.025, New P: 0.675
-Original Grad: 0.013, -lr * Pred Grad:  -0.017, New P: -0.005
iter 13 loss: 0.304
Actual params: [ 0.6752, -0.0054]
-Original Grad: -0.019, -lr * Pred Grad:  0.020, New P: 0.695
-Original Grad: 0.019, -lr * Pred Grad:  -0.014, New P: -0.020
iter 14 loss: 0.305
Actual params: [ 0.6953, -0.0197]
-Original Grad: -0.010, -lr * Pred Grad:  0.017, New P: 0.712
-Original Grad: 0.021, -lr * Pred Grad:  -0.012, New P: -0.032
iter 15 loss: 0.306
Actual params: [ 0.7122, -0.0316]
-Original Grad: -0.008, -lr * Pred Grad:  0.014, New P: 0.726
-Original Grad: 0.012, -lr * Pred Grad:  -0.010, New P: -0.042
iter 16 loss: 0.307
Actual params: [ 0.7264, -0.0417]
-Original Grad: -0.011, -lr * Pred Grad:  0.012, New P: 0.738
-Original Grad: 0.016, -lr * Pred Grad:  -0.008, New P: -0.050
iter 17 loss: 0.308
Actual params: [ 0.7379, -0.0502]
-Original Grad: -0.012, -lr * Pred Grad:  0.009, New P: 0.747
-Original Grad: 0.027, -lr * Pred Grad:  -0.006, New P: -0.056
iter 18 loss: 0.308
Actual params: [ 0.7468, -0.0565]
-Original Grad: -0.003, -lr * Pred Grad:  0.008, New P: 0.754
-Original Grad: 0.017, -lr * Pred Grad:  -0.005, New P: -0.061
iter 19 loss: 0.308
Actual params: [ 0.7543, -0.0613]
-Original Grad: -0.008, -lr * Pred Grad:  0.006, New P: 0.760
-Original Grad: 0.021, -lr * Pred Grad:  -0.003, New P: -0.065
iter 20 loss: 0.308
Actual params: [ 0.7601, -0.0647]
-Original Grad: -0.008, -lr * Pred Grad:  0.004, New P: 0.764
-Original Grad: 0.016, -lr * Pred Grad:  -0.002, New P: -0.067
Target params: [1.1812, 0.2779]
iter 0 loss: 0.492
Actual params: [0.5941, 0.5941]
-Original Grad: 0.255, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.169, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.387
Actual params: [0.6941, 0.4941]
-Original Grad: 0.345, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.116, -lr * Pred Grad:  -0.097, New P: 0.397
iter 2 loss: 0.303
Actual params: [0.7937, 0.3967]
-Original Grad: 0.170, -lr * Pred Grad:  0.095, New P: 0.889
-Original Grad: -0.063, -lr * Pred Grad:  -0.091, New P: 0.306
iter 3 loss: 0.228
Actual params: [0.8889, 0.3058]
-Original Grad: 0.069, -lr * Pred Grad:  0.086, New P: 0.975
-Original Grad: -0.026, -lr * Pred Grad:  -0.081, New P: 0.225
iter 4 loss: 0.197
Actual params: [0.9746, 0.225 ]
-Original Grad: 0.025, -lr * Pred Grad:  0.075, New P: 1.050
-Original Grad: -0.010, -lr * Pred Grad:  -0.071, New P: 0.154
iter 5 loss: 0.191
Actual params: [1.0499, 0.1542]
-Original Grad: -0.002, -lr * Pred Grad:  0.065, New P: 1.115
-Original Grad: 0.003, -lr * Pred Grad:  -0.060, New P: 0.094
iter 6 loss: 0.198
Actual params: [1.1145, 0.0937]
-Original Grad: 0.008, -lr * Pred Grad:  0.057, New P: 1.172
-Original Grad: 0.015, -lr * Pred Grad:  -0.049, New P: 0.045
iter 7 loss: 0.203
Actual params: [1.1719, 0.0447]
-Original Grad: -0.002, -lr * Pred Grad:  0.050, New P: 1.222
-Original Grad: 0.016, -lr * Pred Grad:  -0.040, New P: 0.005
iter 8 loss: 0.212
Actual params: [1.2223, 0.005 ]
-Original Grad: -0.003, -lr * Pred Grad:  0.044, New P: 1.267
-Original Grad: 0.010, -lr * Pred Grad:  -0.033, New P: -0.028
iter 9 loss: 0.221
Actual params: [ 1.2667, -0.0278]
-Original Grad: -0.010, -lr * Pred Grad:  0.039, New P: 1.305
-Original Grad: 0.016, -lr * Pred Grad:  -0.026, New P: -0.053
iter 10 loss: 0.231
Actual params: [ 1.3053, -0.0535]
-Original Grad: -0.019, -lr * Pred Grad:  0.033, New P: 1.338
-Original Grad: -0.004, -lr * Pred Grad:  -0.024, New P: -0.077
iter 11 loss: 0.241
Actual params: [ 1.3379, -0.0774]
-Original Grad: -0.024, -lr * Pred Grad:  0.027, New P: 1.365
-Original Grad: 0.008, -lr * Pred Grad:  -0.020, New P: -0.097
iter 12 loss: 0.248
Actual params: [ 1.3648, -0.0972]
-Original Grad: -0.018, -lr * Pred Grad:  0.022, New P: 1.387
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.115
iter 13 loss: 0.250
Actual params: [ 1.387 , -0.1152]
-Original Grad: -0.019, -lr * Pred Grad:  0.018, New P: 1.405
-Original Grad: 0.003, -lr * Pred Grad:  -0.016, New P: -0.131
iter 14 loss: 0.254
Actual params: [ 1.4052, -0.1309]
-Original Grad: -0.028, -lr * Pred Grad:  0.013, New P: 1.419
-Original Grad: 0.006, -lr * Pred Grad:  -0.013, New P: -0.144
iter 15 loss: 0.258
Actual params: [ 1.4187, -0.1436]
-Original Grad: -0.019, -lr * Pred Grad:  0.010, New P: 1.429
-Original Grad: -0.006, -lr * Pred Grad:  -0.013, New P: -0.157
iter 16 loss: 0.261
Actual params: [ 1.4289, -0.1565]
-Original Grad: -0.020, -lr * Pred Grad:  0.007, New P: 1.436
-Original Grad: -0.007, -lr * Pred Grad:  -0.013, New P: -0.170
iter 17 loss: 0.263
Actual params: [ 1.4362, -0.1698]
-Original Grad: -0.036, -lr * Pred Grad:  0.003, New P: 1.439
-Original Grad: 0.008, -lr * Pred Grad:  -0.010, New P: -0.180
iter 18 loss: 0.263
Actual params: [ 1.439 , -0.1802]
-Original Grad: -0.030, -lr * Pred Grad:  -0.001, New P: 1.438
-Original Grad: -0.006, -lr * Pred Grad:  -0.011, New P: -0.191
iter 19 loss: 0.263
Actual params: [ 1.4383, -0.1909]
-Original Grad: -0.034, -lr * Pred Grad:  -0.004, New P: 1.434
-Original Grad: 0.023, -lr * Pred Grad:  -0.004, New P: -0.195
iter 20 loss: 0.262
Actual params: [ 1.4341, -0.1953]
-Original Grad: -0.027, -lr * Pred Grad:  -0.007, New P: 1.427
-Original Grad: 0.006, -lr * Pred Grad:  -0.003, New P: -0.198
Target params: [1.1812, 0.2779]
iter 0 loss: 0.773
Actual params: [0.5941, 0.5941]
-Original Grad: 0.220, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.200, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.556
Actual params: [0.6941, 0.4941]
-Original Grad: 0.079, -lr * Pred Grad:  0.088, New P: 0.782
-Original Grad: -0.074, -lr * Pred Grad:  -0.089, New P: 0.405
iter 2 loss: 0.479
Actual params: [0.7822, 0.4054]
-Original Grad: 0.030, -lr * Pred Grad:  0.076, New P: 0.858
-Original Grad: -0.044, -lr * Pred Grad:  -0.080, New P: 0.325
iter 3 loss: 0.435
Actual params: [0.8579, 0.3253]
-Original Grad: 0.029, -lr * Pred Grad:  0.069, New P: 0.927
-Original Grad: -0.020, -lr * Pred Grad:  -0.071, New P: 0.255
iter 4 loss: 0.419
Actual params: [0.9265, 0.2548]
-Original Grad: 0.016, -lr * Pred Grad:  0.062, New P: 0.988
-Original Grad: -0.012, -lr * Pred Grad:  -0.062, New P: 0.192
iter 5 loss: 0.404
Actual params: [0.9882, 0.1923]
-Original Grad: -0.004, -lr * Pred Grad:  0.052, New P: 1.040
-Original Grad: -0.015, -lr * Pred Grad:  -0.057, New P: 0.135
iter 6 loss: 0.394
Actual params: [1.0405, 0.135 ]
-Original Grad: 0.012, -lr * Pred Grad:  0.048, New P: 1.089
-Original Grad: -0.010, -lr * Pred Grad:  -0.052, New P: 0.083
iter 7 loss: 0.389
Actual params: [1.0886, 0.0827]
-Original Grad: 0.008, -lr * Pred Grad:  0.044, New P: 1.133
-Original Grad: 0.007, -lr * Pred Grad:  -0.045, New P: 0.038
iter 8 loss: 0.391
Actual params: [1.1326, 0.0381]
-Original Grad: 0.000, -lr * Pred Grad:  0.039, New P: 1.172
-Original Grad: 0.009, -lr * Pred Grad:  -0.038, New P: 0.001
iter 9 loss: 0.387
Actual params: [1.1718e+00, 5.0683e-04]
-Original Grad: 0.001, -lr * Pred Grad:  0.035, New P: 1.207
-Original Grad: 0.023, -lr * Pred Grad:  -0.028, New P: -0.028
iter 10 loss: 0.385
Actual params: [ 1.2068, -0.0278]
-Original Grad: 0.000, -lr * Pred Grad:  0.031, New P: 1.238
-Original Grad: 0.023, -lr * Pred Grad:  -0.020, New P: -0.048
iter 11 loss: 0.383
Actual params: [ 1.2382, -0.0482]
-Original Grad: 0.003, -lr * Pred Grad:  0.029, New P: 1.267
-Original Grad: 0.041, -lr * Pred Grad:  -0.009, New P: -0.057
iter 12 loss: 0.383
Actual params: [ 1.2672, -0.0574]
-Original Grad: 0.003, -lr * Pred Grad:  0.027, New P: 1.294
-Original Grad: 0.022, -lr * Pred Grad:  -0.003, New P: -0.061
iter 13 loss: 0.383
Actual params: [ 1.2941, -0.0609]
-Original Grad: 0.002, -lr * Pred Grad:  0.025, New P: 1.319
-Original Grad: 0.039, -lr * Pred Grad:  0.005, New P: -0.056
iter 14 loss: 0.385
Actual params: [ 1.3187, -0.0558]
-Original Grad: 0.004, -lr * Pred Grad:  0.023, New P: 1.342
-Original Grad: 0.050, -lr * Pred Grad:  0.015, New P: -0.041
iter 15 loss: 0.388
Actual params: [ 1.3418, -0.041 ]
-Original Grad: 0.001, -lr * Pred Grad:  0.021, New P: 1.363
-Original Grad: 0.007, -lr * Pred Grad:  0.015, New P: -0.026
iter 16 loss: 0.389
Actual params: [ 1.3628, -0.0262]
-Original Grad: 0.002, -lr * Pred Grad:  0.020, New P: 1.382
-Original Grad: 0.008, -lr * Pred Grad:  0.015, New P: -0.011
iter 17 loss: 0.385
Actual params: [ 1.3823, -0.0111]
-Original Grad: -0.001, -lr * Pred Grad:  0.017, New P: 1.400
-Original Grad: 0.012, -lr * Pred Grad:  0.016, New P: 0.005
iter 18 loss: 0.388
Actual params: [1.3997, 0.005 ]
-Original Grad: -0.004, -lr * Pred Grad:  0.015, New P: 1.415
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.020
iter 19 loss: 0.391
Actual params: [1.4147, 0.0199]
-Original Grad: -0.002, -lr * Pred Grad:  0.013, New P: 1.428
-Original Grad: 0.004, -lr * Pred Grad:  0.014, New P: 0.034
iter 20 loss: 0.394
Actual params: [1.428 , 0.0343]
-Original Grad: -0.007, -lr * Pred Grad:  0.010, New P: 1.438
-Original Grad: -0.003, -lr * Pred Grad:  0.012, New P: 0.047
Target params: [1.1812, 0.2779]
iter 0 loss: 0.860
Actual params: [0.5941, 0.5941]
-Original Grad: 0.593, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.253, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.585
Actual params: [0.6941, 0.6941]
-Original Grad: 0.897, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: 0.204, -lr * Pred Grad:  0.099, New P: 0.793
iter 2 loss: 0.260
Actual params: [0.7931, 0.7929]
-Original Grad: -0.150, -lr * Pred Grad:  0.067, New P: 0.860
-Original Grad: -0.101, -lr * Pred Grad:  0.054, New P: 0.847
iter 3 loss: 0.553
Actual params: [0.86  , 0.8469]
-Original Grad: -0.312, -lr * Pred Grad:  0.037, New P: 0.897
-Original Grad: -0.200, -lr * Pred Grad:  0.009, New P: 0.856
iter 4 loss: 0.679
Actual params: [0.8967, 0.8557]
-Original Grad: -0.332, -lr * Pred Grad:  0.014, New P: 0.911
-Original Grad: -0.205, -lr * Pred Grad:  -0.019, New P: 0.837
iter 5 loss: 0.687
Actual params: [0.911 , 0.8371]
-Original Grad: -0.286, -lr * Pred Grad:  -0.000, New P: 0.911
-Original Grad: -0.257, -lr * Pred Grad:  -0.040, New P: 0.797
iter 6 loss: 0.617
Actual params: [0.9107, 0.797 ]
-Original Grad: -0.286, -lr * Pred Grad:  -0.012, New P: 0.899
-Original Grad: -0.241, -lr * Pred Grad:  -0.053, New P: 0.744
iter 7 loss: 0.486
Actual params: [0.8987, 0.7438]
-Original Grad: -0.250, -lr * Pred Grad:  -0.020, New P: 0.879
-Original Grad: -0.228, -lr * Pred Grad:  -0.062, New P: 0.682
iter 8 loss: 0.307
Actual params: [0.8786, 0.6818]
-Original Grad: -0.225, -lr * Pred Grad:  -0.026, New P: 0.852
-Original Grad: -0.197, -lr * Pred Grad:  -0.067, New P: 0.614
iter 9 loss: 0.216
Actual params: [0.8525, 0.6144]
-Original Grad: 0.090, -lr * Pred Grad:  -0.020, New P: 0.833
-Original Grad: -0.060, -lr * Pred Grad:  -0.064, New P: 0.550
iter 10 loss: 0.355
Actual params: [0.8326, 0.55  ]
-Original Grad: 0.346, -lr * Pred Grad:  -0.005, New P: 0.828
-Original Grad: 0.066, -lr * Pred Grad:  -0.052, New P: 0.498
iter 11 loss: 0.459
Actual params: [0.8279, 0.4975]
-Original Grad: 0.392, -lr * Pred Grad:  0.010, New P: 0.837
-Original Grad: 0.103, -lr * Pred Grad:  -0.039, New P: 0.459
iter 12 loss: 0.491
Actual params: [0.8375, 0.4586]
-Original Grad: 0.462, -lr * Pred Grad:  0.023, New P: 0.861
-Original Grad: 0.155, -lr * Pred Grad:  -0.023, New P: 0.435
iter 13 loss: 0.450
Actual params: [0.8609, 0.4355]
-Original Grad: 0.331, -lr * Pred Grad:  0.031, New P: 0.892
-Original Grad: 0.105, -lr * Pred Grad:  -0.013, New P: 0.422
iter 14 loss: 0.373
Actual params: [0.8922, 0.4223]
-Original Grad: 0.318, -lr * Pred Grad:  0.038, New P: 0.930
-Original Grad: 0.156, -lr * Pred Grad:  -0.001, New P: 0.422
iter 15 loss: 0.268
Actual params: [0.93  , 0.4216]
-Original Grad: 0.159, -lr * Pred Grad:  0.039, New P: 0.969
-Original Grad: 0.064, -lr * Pred Grad:  0.004, New P: 0.425
iter 16 loss: 0.191
Actual params: [0.9691, 0.4253]
-Original Grad: 0.076, -lr * Pred Grad:  0.038, New P: 1.007
-Original Grad: 0.004, -lr * Pred Grad:  0.004, New P: 0.429
iter 17 loss: 0.158
Actual params: [1.007 , 0.4291]
-Original Grad: 0.016, -lr * Pred Grad:  0.035, New P: 1.042
-Original Grad: -0.056, -lr * Pred Grad:  -0.001, New P: 0.429
iter 18 loss: 0.152
Actual params: [1.0419, 0.4285]
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: 1.074
-Original Grad: -0.080, -lr * Pred Grad:  -0.006, New P: 0.422
iter 19 loss: 0.150
Actual params: [1.0737, 0.4223]
-Original Grad: -0.022, -lr * Pred Grad:  0.028, New P: 1.102
-Original Grad: -0.114, -lr * Pred Grad:  -0.014, New P: 0.409
iter 20 loss: 0.149
Actual params: [1.1018, 0.4086]
-Original Grad: -0.021, -lr * Pred Grad:  0.025, New P: 1.127
-Original Grad: -0.093, -lr * Pred Grad:  -0.019, New P: 0.390
Target params: [1.1812, 0.2779]
iter 0 loss: 0.817
Actual params: [0.5941, 0.5941]
-Original Grad: 0.028, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.429, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.744
Actual params: [0.6941, 0.4941]
-Original Grad: 0.001, -lr * Pred Grad:  0.071, New P: 0.765
-Original Grad: -0.315, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.673
Actual params: [0.7647, 0.396 ]
-Original Grad: -0.062, -lr * Pred Grad:  -0.036, New P: 0.729
-Original Grad: -0.252, -lr * Pred Grad:  -0.096, New P: 0.300
iter 3 loss: 0.650
Actual params: [0.7286, 0.3002]
-Original Grad: -0.042, -lr * Pred Grad:  -0.056, New P: 0.673
-Original Grad: -0.204, -lr * Pred Grad:  -0.093, New P: 0.207
iter 4 loss: 0.630
Actual params: [0.6728, 0.207 ]
-Original Grad: 0.028, -lr * Pred Grad:  -0.026, New P: 0.646
-Original Grad: -0.079, -lr * Pred Grad:  -0.085, New P: 0.122
iter 5 loss: 0.606
Actual params: [0.6464, 0.1219]
-Original Grad: 0.036, -lr * Pred Grad:  -0.000, New P: 0.646
-Original Grad: -0.067, -lr * Pred Grad:  -0.078, New P: 0.044
iter 6 loss: 0.573
Actual params: [0.646 , 0.0435]
-Original Grad: 0.050, -lr * Pred Grad:  0.024, New P: 0.670
-Original Grad: -0.042, -lr * Pred Grad:  -0.072, New P: -0.028
iter 7 loss: 0.537
Actual params: [ 0.67  , -0.0282]
-Original Grad: 0.020, -lr * Pred Grad:  0.030, New P: 0.700
-Original Grad: -0.015, -lr * Pred Grad:  -0.064, New P: -0.093
iter 8 loss: 0.508
Actual params: [ 0.7002, -0.0926]
-Original Grad: 0.011, -lr * Pred Grad:  0.032, New P: 0.732
-Original Grad: -0.009, -lr * Pred Grad:  -0.058, New P: -0.150
iter 9 loss: 0.472
Actual params: [ 0.7319, -0.1504]
-Original Grad: -0.012, -lr * Pred Grad:  0.023, New P: 0.755
-Original Grad: -0.001, -lr * Pred Grad:  -0.052, New P: -0.202
iter 10 loss: 0.445
Actual params: [ 0.7546, -0.2021]
-Original Grad: -0.055, -lr * Pred Grad:  -0.004, New P: 0.751
-Original Grad: 0.033, -lr * Pred Grad:  -0.044, New P: -0.246
iter 11 loss: 0.432
Actual params: [ 0.7508, -0.2458]
-Original Grad: -0.102, -lr * Pred Grad:  -0.034, New P: 0.717
-Original Grad: 0.038, -lr * Pred Grad:  -0.036, New P: -0.282
iter 12 loss: 0.424
Actual params: [ 0.717 , -0.2822]
-Original Grad: -0.124, -lr * Pred Grad:  -0.054, New P: 0.663
-Original Grad: 0.063, -lr * Pred Grad:  -0.028, New P: -0.310
iter 13 loss: 0.418
Actual params: [ 0.6633, -0.31  ]
-Original Grad: -0.058, -lr * Pred Grad:  -0.060, New P: 0.603
-Original Grad: 0.051, -lr * Pred Grad:  -0.021, New P: -0.331
iter 14 loss: 0.414
Actual params: [ 0.6032, -0.3311]
-Original Grad: -0.027, -lr * Pred Grad:  -0.060, New P: 0.543
-Original Grad: 0.035, -lr * Pred Grad:  -0.016, New P: -0.348
iter 15 loss: 0.411
Actual params: [ 0.5432, -0.3475]
-Original Grad: -0.018, -lr * Pred Grad:  -0.058, New P: 0.485
-Original Grad: 0.020, -lr * Pred Grad:  -0.013, New P: -0.361
iter 16 loss: 0.411
Actual params: [ 0.4848, -0.3609]
-Original Grad: 0.005, -lr * Pred Grad:  -0.052, New P: 0.433
-Original Grad: 0.032, -lr * Pred Grad:  -0.010, New P: -0.370
iter 17 loss: 0.410
Actual params: [ 0.433 , -0.3705]
-Original Grad: -0.018, -lr * Pred Grad:  -0.051, New P: 0.382
-Original Grad: 0.016, -lr * Pred Grad:  -0.007, New P: -0.378
iter 18 loss: 0.409
Actual params: [ 0.3817, -0.3779]
-Original Grad: -0.014, -lr * Pred Grad:  -0.050, New P: 0.332
-Original Grad: 0.033, -lr * Pred Grad:  -0.004, New P: -0.382
iter 19 loss: 0.409
Actual params: [ 0.3319, -0.3821]
-Original Grad: -0.018, -lr * Pred Grad:  -0.049, New P: 0.282
-Original Grad: 0.038, -lr * Pred Grad:  -0.001, New P: -0.383
iter 20 loss: 0.410
Actual params: [ 0.2825, -0.3829]
-Original Grad: -0.031, -lr * Pred Grad:  -0.052, New P: 0.231
-Original Grad: 0.025, -lr * Pred Grad:  0.001, New P: -0.382
Target params: [1.1812, 0.2779]
iter 0 loss: 0.220
Actual params: [0.5941, 0.5941]
-Original Grad: -0.108, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.041, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.168
Actual params: [0.4941, 0.4941]
-Original Grad: 0.019, -lr * Pred Grad:  -0.053, New P: 0.441
-Original Grad: 0.019, -lr * Pred Grad:  -0.029, New P: 0.465
iter 2 loss: 0.254
Actual params: [0.4407, 0.4647]
-Original Grad: 0.154, -lr * Pred Grad:  0.028, New P: 0.469
-Original Grad: 0.108, -lr * Pred Grad:  0.050, New P: 0.515
iter 3 loss: 0.182
Actual params: [0.4688, 0.5149]
-Original Grad: 0.049, -lr * Pred Grad:  0.037, New P: 0.506
-Original Grad: 0.033, -lr * Pred Grad:  0.055, New P: 0.570
iter 4 loss: 0.155
Actual params: [0.5057, 0.5703]
-Original Grad: -0.055, -lr * Pred Grad:  0.015, New P: 0.521
-Original Grad: -0.018, -lr * Pred Grad:  0.038, New P: 0.609
iter 5 loss: 0.170
Actual params: [0.5209, 0.6086]
-Original Grad: -0.080, -lr * Pred Grad:  -0.007, New P: 0.514
-Original Grad: -0.012, -lr * Pred Grad:  0.028, New P: 0.636
iter 6 loss: 0.172
Actual params: [0.5139, 0.6365]
-Original Grad: -0.092, -lr * Pred Grad:  -0.025, New P: 0.489
-Original Grad: -0.010, -lr * Pred Grad:  0.020, New P: 0.657
iter 7 loss: 0.162
Actual params: [0.4886, 0.6568]
-Original Grad: -0.067, -lr * Pred Grad:  -0.035, New P: 0.454
-Original Grad: -0.007, -lr * Pred Grad:  0.015, New P: 0.672
iter 8 loss: 0.145
Actual params: [0.4535, 0.6717]
-Original Grad: -0.024, -lr * Pred Grad:  -0.036, New P: 0.418
-Original Grad: -0.004, -lr * Pred Grad:  0.012, New P: 0.683
iter 9 loss: 0.167
Actual params: [0.4177, 0.6834]
-Original Grad: 0.035, -lr * Pred Grad:  -0.025, New P: 0.393
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.694
iter 10 loss: 0.195
Actual params: [0.393 , 0.6943]
-Original Grad: 0.081, -lr * Pred Grad:  -0.006, New P: 0.387
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.705
iter 11 loss: 0.200
Actual params: [0.3869, 0.7046]
-Original Grad: 0.078, -lr * Pred Grad:  0.009, New P: 0.395
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.714
iter 12 loss: 0.189
Actual params: [0.3955, 0.7144]
-Original Grad: 0.059, -lr * Pred Grad:  0.018, New P: 0.413
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.723
iter 13 loss: 0.166
Actual params: [0.4133, 0.7233]
-Original Grad: 0.018, -lr * Pred Grad:  0.019, New P: 0.432
-Original Grad: -0.002, -lr * Pred Grad:  0.007, New P: 0.731
iter 14 loss: 0.146
Actual params: [0.4324, 0.7307]
-Original Grad: 0.033, -lr * Pred Grad:  0.023, New P: 0.455
-Original Grad: -0.002, -lr * Pred Grad:  0.006, New P: 0.737
iter 15 loss: 0.150
Actual params: [0.4552, 0.7367]
-Original Grad: 0.002, -lr * Pred Grad:  0.021, New P: 0.476
-Original Grad: -0.004, -lr * Pred Grad:  0.004, New P: 0.741
iter 16 loss: 0.163
Actual params: [0.4763, 0.7406]
-Original Grad: -0.042, -lr * Pred Grad:  0.012, New P: 0.488
-Original Grad: -0.005, -lr * Pred Grad:  0.001, New P: 0.742
iter 17 loss: 0.168
Actual params: [0.4878, 0.742 ]
-Original Grad: -0.064, -lr * Pred Grad:  -0.001, New P: 0.487
-Original Grad: -0.005, -lr * Pred Grad:  -0.001, New P: 0.741
iter 18 loss: 0.167
Actual params: [0.4871, 0.7412]
-Original Grad: -0.071, -lr * Pred Grad:  -0.013, New P: 0.475
-Original Grad: -0.006, -lr * Pred Grad:  -0.003, New P: 0.738
iter 19 loss: 0.162
Actual params: [0.4746, 0.738 ]
-Original Grad: -0.041, -lr * Pred Grad:  -0.018, New P: 0.456
-Original Grad: -0.007, -lr * Pred Grad:  -0.006, New P: 0.732
iter 20 loss: 0.151
Actual params: [0.4564, 0.7323]
-Original Grad: -0.003, -lr * Pred Grad:  -0.017, New P: 0.439
-Original Grad: -0.003, -lr * Pred Grad:  -0.006, New P: 0.726
Target params: [1.1812, 0.2779]
iter 0 loss: 0.338
Actual params: [0.5941, 0.5941]
-Original Grad: 0.113, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.197, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.296
Actual params: [0.6941, 0.4941]
-Original Grad: 0.040, -lr * Pred Grad:  0.088, New P: 0.782
-Original Grad: -0.199, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 0.270
Actual params: [0.7819, 0.394 ]
-Original Grad: 0.020, -lr * Pred Grad:  0.078, New P: 0.859
-Original Grad: -0.062, -lr * Pred Grad:  -0.089, New P: 0.305
iter 3 loss: 0.257
Actual params: [0.8595, 0.3047]
-Original Grad: -0.001, -lr * Pred Grad:  0.063, New P: 0.923
-Original Grad: -0.021, -lr * Pred Grad:  -0.077, New P: 0.228
iter 4 loss: 0.258
Actual params: [0.9227, 0.2275]
-Original Grad: -0.001, -lr * Pred Grad:  0.053, New P: 0.975
-Original Grad: -0.007, -lr * Pred Grad:  -0.067, New P: 0.161
iter 5 loss: 0.263
Actual params: [0.9754, 0.161 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.046, New P: 1.021
-Original Grad: 0.013, -lr * Pred Grad:  -0.055, New P: 0.106
iter 6 loss: 0.268
Actual params: [1.0209, 0.1061]
-Original Grad: -0.004, -lr * Pred Grad:  0.038, New P: 1.059
-Original Grad: 0.019, -lr * Pred Grad:  -0.045, New P: 0.061
iter 7 loss: 0.272
Actual params: [1.0588, 0.0614]
-Original Grad: -0.007, -lr * Pred Grad:  0.031, New P: 1.089
-Original Grad: 0.009, -lr * Pred Grad:  -0.038, New P: 0.024
iter 8 loss: 0.276
Actual params: [1.0894, 0.0236]
-Original Grad: -0.010, -lr * Pred Grad:  0.023, New P: 1.112
-Original Grad: 0.012, -lr * Pred Grad:  -0.032, New P: -0.008
iter 9 loss: 0.277
Actual params: [ 1.1124, -0.0079]
-Original Grad: -0.013, -lr * Pred Grad:  0.015, New P: 1.128
-Original Grad: 0.008, -lr * Pred Grad:  -0.027, New P: -0.035
iter 10 loss: 0.278
Actual params: [ 1.1276, -0.0347]
-Original Grad: -0.006, -lr * Pred Grad:  0.011, New P: 1.139
-Original Grad: 0.017, -lr * Pred Grad:  -0.021, New P: -0.056
iter 11 loss: 0.278
Actual params: [ 1.1388, -0.0557]
-Original Grad: -0.014, -lr * Pred Grad:  0.004, New P: 1.143
-Original Grad: 0.016, -lr * Pred Grad:  -0.016, New P: -0.072
iter 12 loss: 0.278
Actual params: [ 1.1432, -0.0718]
-Original Grad: -0.016, -lr * Pred Grad:  -0.002, New P: 1.141
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -0.086
iter 13 loss: 0.278
Actual params: [ 1.1407, -0.0863]
-Original Grad: -0.004, -lr * Pred Grad:  -0.004, New P: 1.137
-Original Grad: 0.024, -lr * Pred Grad:  -0.009, New P: -0.095
iter 14 loss: 0.278
Actual params: [ 1.1369, -0.0954]
-Original Grad: -0.009, -lr * Pred Grad:  -0.007, New P: 1.130
-Original Grad: 0.018, -lr * Pred Grad:  -0.005, New P: -0.101
iter 15 loss: 0.278
Actual params: [ 1.1299, -0.1006]
-Original Grad: -0.003, -lr * Pred Grad:  -0.008, New P: 1.122
-Original Grad: 0.023, -lr * Pred Grad:  -0.001, New P: -0.101
iter 16 loss: 0.278
Actual params: [ 1.1223, -0.1015]
-Original Grad: -0.008, -lr * Pred Grad:  -0.010, New P: 1.112
-Original Grad: 0.022, -lr * Pred Grad:  0.003, New P: -0.099
iter 17 loss: 0.278
Actual params: [ 1.1121, -0.0985]
-Original Grad: -0.007, -lr * Pred Grad:  -0.012, New P: 1.100
-Original Grad: 0.033, -lr * Pred Grad:  0.008, New P: -0.090
iter 18 loss: 0.278
Actual params: [ 1.0999, -0.0903]
-Original Grad: -0.009, -lr * Pred Grad:  -0.015, New P: 1.085
-Original Grad: 0.023, -lr * Pred Grad:  0.011, New P: -0.079
iter 19 loss: 0.278
Actual params: [ 1.0851, -0.079 ]
-Original Grad: -0.015, -lr * Pred Grad:  -0.020, New P: 1.065
-Original Grad: 0.020, -lr * Pred Grad:  0.014, New P: -0.065
iter 20 loss: 0.278
Actual params: [ 1.0655, -0.0653]
-Original Grad: -0.018, -lr * Pred Grad:  -0.025, New P: 1.041
-Original Grad: 0.010, -lr * Pred Grad:  0.014, New P: -0.051
Target params: [1.1812, 0.2779]
iter 0 loss: 0.336
Actual params: [0.5941, 0.5941]
-Original Grad: 0.326, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.242, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.227
Actual params: [0.6941, 0.4941]
-Original Grad: 0.040, -lr * Pred Grad:  0.076, New P: 0.770
-Original Grad: -0.012, -lr * Pred Grad:  -0.070, New P: 0.424
iter 2 loss: 0.165
Actual params: [0.7697, 0.4236]
-Original Grad: -0.025, -lr * Pred Grad:  0.053, New P: 0.823
-Original Grad: 0.023, -lr * Pred Grad:  -0.048, New P: 0.375
iter 3 loss: 0.148
Actual params: [0.8231, 0.3753]
-Original Grad: 0.001, -lr * Pred Grad:  0.044, New P: 0.867
-Original Grad: 0.013, -lr * Pred Grad:  -0.036, New P: 0.339
iter 4 loss: 0.142
Actual params: [0.8671, 0.3389]
-Original Grad: 0.025, -lr * Pred Grad:  0.041, New P: 0.908
-Original Grad: 0.009, -lr * Pred Grad:  -0.029, New P: 0.310
iter 5 loss: 0.138
Actual params: [0.9083, 0.3102]
-Original Grad: 0.028, -lr * Pred Grad:  0.040, New P: 0.948
-Original Grad: 0.015, -lr * Pred Grad:  -0.022, New P: 0.289
iter 6 loss: 0.134
Actual params: [0.9481, 0.2887]
-Original Grad: 0.033, -lr * Pred Grad:  0.040, New P: 0.988
-Original Grad: 0.027, -lr * Pred Grad:  -0.013, New P: 0.276
iter 7 loss: 0.129
Actual params: [0.9878, 0.2756]
-Original Grad: 0.042, -lr * Pred Grad:  0.041, New P: 1.029
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: 0.264
iter 8 loss: 0.124
Actual params: [1.0287, 0.2642]
-Original Grad: 0.042, -lr * Pred Grad:  0.042, New P: 1.071
-Original Grad: 0.009, -lr * Pred Grad:  -0.008, New P: 0.256
iter 9 loss: 0.121
Actual params: [1.0709, 0.2559]
-Original Grad: 0.049, -lr * Pred Grad:  0.044, New P: 1.115
-Original Grad: 0.003, -lr * Pred Grad:  -0.007, New P: 0.249
iter 10 loss: 0.119
Actual params: [1.1151, 0.249 ]
-Original Grad: 0.043, -lr * Pred Grad:  0.045, New P: 1.160
-Original Grad: 0.013, -lr * Pred Grad:  -0.004, New P: 0.246
iter 11 loss: 0.119
Actual params: [1.1604, 0.2455]
-Original Grad: -0.000, -lr * Pred Grad:  0.041, New P: 1.201
-Original Grad: 0.011, -lr * Pred Grad:  -0.001, New P: 0.244
iter 12 loss: 0.119
Actual params: [1.2011, 0.2445]
-Original Grad: -0.001, -lr * Pred Grad:  0.037, New P: 1.238
-Original Grad: 0.018, -lr * Pred Grad:  0.003, New P: 0.247
iter 13 loss: 0.119
Actual params: [1.2377, 0.2471]
-Original Grad: -0.002, -lr * Pred Grad:  0.033, New P: 1.271
-Original Grad: 0.035, -lr * Pred Grad:  0.009, New P: 0.256
iter 14 loss: 0.122
Actual params: [1.2705, 0.2563]
-Original Grad: -0.018, -lr * Pred Grad:  0.027, New P: 1.298
-Original Grad: -0.004, -lr * Pred Grad:  0.007, New P: 0.264
iter 15 loss: 0.126
Actual params: [1.2976, 0.2637]
-Original Grad: -0.015, -lr * Pred Grad:  0.022, New P: 1.320
-Original Grad: 0.009, -lr * Pred Grad:  0.008, New P: 0.272
iter 16 loss: 0.129
Actual params: [1.3201, 0.2722]
-Original Grad: -0.013, -lr * Pred Grad:  0.019, New P: 1.339
-Original Grad: 0.003, -lr * Pred Grad:  0.008, New P: 0.280
iter 17 loss: 0.132
Actual params: [1.3386, 0.2805]
-Original Grad: -0.010, -lr * Pred Grad:  0.015, New P: 1.354
-Original Grad: -0.006, -lr * Pred Grad:  0.006, New P: 0.287
iter 18 loss: 0.134
Actual params: [1.354 , 0.2869]
-Original Grad: -0.011, -lr * Pred Grad:  0.012, New P: 1.366
-Original Grad: 0.010, -lr * Pred Grad:  0.008, New P: 0.295
iter 19 loss: 0.136
Actual params: [1.3664, 0.2947]
-Original Grad: -0.007, -lr * Pred Grad:  0.010, New P: 1.377
-Original Grad: 0.005, -lr * Pred Grad:  0.008, New P: 0.303
iter 20 loss: 0.139
Actual params: [1.3767, 0.303 ]
-Original Grad: -0.005, -lr * Pred Grad:  0.009, New P: 1.385
-Original Grad: -0.001, -lr * Pred Grad:  0.007, New P: 0.310
Target params: [1.1812, 0.2779]
iter 0 loss: 0.118
Actual params: [0.5941, 0.5941]
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.034, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.133
Actual params: [0.4941, 0.4941]
-Original Grad: 0.022, -lr * Pred Grad:  0.071, New P: 0.565
-Original Grad: -0.024, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.116
Actual params: [0.5654, 0.3963]
-Original Grad: 0.000, -lr * Pred Grad:  0.057, New P: 0.622
-Original Grad: -0.002, -lr * Pred Grad:  -0.079, New P: 0.317
iter 3 loss: 0.110
Actual params: [0.622 , 0.3171]
-Original Grad: 0.000, -lr * Pred Grad:  0.048, New P: 0.670
-Original Grad: 0.014, -lr * Pred Grad:  -0.043, New P: 0.274
iter 4 loss: 0.111
Actual params: [0.6696, 0.2737]
-Original Grad: -0.009, -lr * Pred Grad:  0.016, New P: 0.686
-Original Grad: 0.015, -lr * Pred Grad:  -0.017, New P: 0.257
iter 5 loss: 0.119
Actual params: [0.686 , 0.2571]
-Original Grad: -0.003, -lr * Pred Grad:  0.007, New P: 0.693
-Original Grad: -0.003, -lr * Pred Grad:  -0.017, New P: 0.240
iter 6 loss: 0.120
Actual params: [0.6926, 0.2398]
-Original Grad: -0.011, -lr * Pred Grad:  -0.015, New P: 0.677
-Original Grad: 0.014, -lr * Pred Grad:  0.001, New P: 0.240
iter 7 loss: 0.117
Actual params: [0.6773, 0.2403]
-Original Grad: -0.009, -lr * Pred Grad:  -0.029, New P: 0.648
-Original Grad: 0.003, -lr * Pred Grad:  0.004, New P: 0.244
iter 8 loss: 0.109
Actual params: [0.648 , 0.2441]
-Original Grad: -0.006, -lr * Pred Grad:  -0.036, New P: 0.612
-Original Grad: 0.008, -lr * Pred Grad:  0.011, New P: 0.255
iter 9 loss: 0.110
Actual params: [0.6116, 0.2554]
-Original Grad: 0.003, -lr * Pred Grad:  -0.027, New P: 0.584
-Original Grad: 0.007, -lr * Pred Grad:  0.017, New P: 0.272
iter 10 loss: 0.112
Actual params: [0.5843, 0.2724]
-Original Grad: 0.003, -lr * Pred Grad:  -0.019, New P: 0.566
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 0.289
iter 11 loss: 0.114
Actual params: [0.5656, 0.289 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: 0.549
-Original Grad: 0.014, -lr * Pred Grad:  0.027, New P: 0.316
iter 12 loss: 0.116
Actual params: [0.5486, 0.3164]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: 0.532
-Original Grad: 0.018, -lr * Pred Grad:  0.039, New P: 0.356
iter 13 loss: 0.120
Actual params: [0.5319, 0.3557]
-Original Grad: 0.005, -lr * Pred Grad:  -0.007, New P: 0.525
-Original Grad: 0.006, -lr * Pred Grad:  0.040, New P: 0.396
iter 14 loss: 0.121
Actual params: [0.5249, 0.396 ]
-Original Grad: 0.010, -lr * Pred Grad:  0.010, New P: 0.535
-Original Grad: -0.004, -lr * Pred Grad:  0.033, New P: 0.429
iter 15 loss: 0.120
Actual params: [0.5347, 0.4289]
-Original Grad: 0.010, -lr * Pred Grad:  0.024, New P: 0.558
-Original Grad: -0.008, -lr * Pred Grad:  0.023, New P: 0.451
iter 16 loss: 0.117
Actual params: [0.5584, 0.4514]
-Original Grad: 0.007, -lr * Pred Grad:  0.032, New P: 0.590
-Original Grad: -0.016, -lr * Pred Grad:  0.006, New P: 0.458
iter 17 loss: 0.114
Actual params: [0.5902, 0.4578]
-Original Grad: 0.006, -lr * Pred Grad:  0.037, New P: 0.628
-Original Grad: -0.018, -lr * Pred Grad:  -0.009, New P: 0.448
iter 18 loss: 0.111
Actual params: [0.6276, 0.4483]
-Original Grad: 0.001, -lr * Pred Grad:  0.036, New P: 0.663
-Original Grad: -0.011, -lr * Pred Grad:  -0.017, New P: 0.431
iter 19 loss: 0.111
Actual params: [0.6632, 0.4311]
-Original Grad: -0.001, -lr * Pred Grad:  0.031, New P: 0.694
-Original Grad: -0.011, -lr * Pred Grad:  -0.024, New P: 0.407
iter 20 loss: 0.114
Actual params: [0.6939, 0.4066]
-Original Grad: -0.009, -lr * Pred Grad:  0.014, New P: 0.708
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: 0.383
Target params: [1.1812, 0.2779]
iter 0 loss: 0.551
Actual params: [0.5941, 0.5941]
-Original Grad: 0.277, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.040, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.528
Actual params: [0.6941, 0.4941]
-Original Grad: 0.525, -lr * Pred Grad:  0.097, New P: 0.791
-Original Grad: 0.105, -lr * Pred Grad:  0.045, New P: 0.539
iter 2 loss: 0.342
Actual params: [0.7911, 0.5395]
-Original Grad: 0.492, -lr * Pred Grad:  0.099, New P: 0.890
-Original Grad: 0.056, -lr * Pred Grad:  0.060, New P: 0.599
iter 3 loss: 0.296
Actual params: [0.8896, 0.5995]
-Original Grad: -0.021, -lr * Pred Grad:  0.079, New P: 0.969
-Original Grad: -0.222, -lr * Pred Grad:  -0.026, New P: 0.573
iter 4 loss: 0.258
Actual params: [0.9687, 0.5731]
-Original Grad: -0.022, -lr * Pred Grad:  0.065, New P: 1.034
-Original Grad: -0.192, -lr * Pred Grad:  -0.051, New P: 0.523
iter 5 loss: 0.195
Actual params: [1.034 , 0.5225]
-Original Grad: 0.007, -lr * Pred Grad:  0.057, New P: 1.091
-Original Grad: -0.110, -lr * Pred Grad:  -0.058, New P: 0.464
iter 6 loss: 0.189
Actual params: [1.0908, 0.4643]
-Original Grad: 0.001, -lr * Pred Grad:  0.050, New P: 1.140
-Original Grad: -0.039, -lr * Pred Grad:  -0.056, New P: 0.408
iter 7 loss: 0.255
Actual params: [1.1403, 0.4079]
-Original Grad: -0.015, -lr * Pred Grad:  0.043, New P: 1.183
-Original Grad: 0.030, -lr * Pred Grad:  -0.045, New P: 0.363
iter 8 loss: 0.331
Actual params: [1.1831, 0.3627]
-Original Grad: -0.033, -lr * Pred Grad:  0.036, New P: 1.219
-Original Grad: 0.098, -lr * Pred Grad:  -0.025, New P: 0.338
iter 9 loss: 0.378
Actual params: [1.2189, 0.3378]
-Original Grad: -0.047, -lr * Pred Grad:  0.029, New P: 1.248
-Original Grad: 0.115, -lr * Pred Grad:  -0.006, New P: 0.331
iter 10 loss: 0.394
Actual params: [1.2479, 0.3315]
-Original Grad: -0.065, -lr * Pred Grad:  0.022, New P: 1.270
-Original Grad: 0.155, -lr * Pred Grad:  0.013, New P: 0.345
iter 11 loss: 0.389
Actual params: [1.2698, 0.3448]
-Original Grad: -0.058, -lr * Pred Grad:  0.016, New P: 1.286
-Original Grad: 0.133, -lr * Pred Grad:  0.026, New P: 0.371
iter 12 loss: 0.374
Actual params: [1.2857, 0.3713]
-Original Grad: -0.061, -lr * Pred Grad:  0.011, New P: 1.296
-Original Grad: 0.128, -lr * Pred Grad:  0.037, New P: 0.408
iter 13 loss: 0.328
Actual params: [1.2963, 0.4081]
-Original Grad: -0.035, -lr * Pred Grad:  0.007, New P: 1.304
-Original Grad: 0.051, -lr * Pred Grad:  0.039, New P: 0.447
iter 14 loss: 0.282
Actual params: [1.3037, 0.4466]
-Original Grad: -0.029, -lr * Pred Grad:  0.005, New P: 1.309
-Original Grad: 0.076, -lr * Pred Grad:  0.043, New P: 0.489
iter 15 loss: 0.236
Actual params: [1.3085, 0.4891]
-Original Grad: -0.013, -lr * Pred Grad:  0.004, New P: 1.312
-Original Grad: 0.014, -lr * Pred Grad:  0.040, New P: 0.529
iter 16 loss: 0.194
Actual params: [1.3121, 0.5291]
-Original Grad: 0.008, -lr * Pred Grad:  0.004, New P: 1.316
-Original Grad: -0.033, -lr * Pred Grad:  0.033, New P: 0.562
iter 17 loss: 0.187
Actual params: [1.3159, 0.5617]
-Original Grad: 0.031, -lr * Pred Grad:  0.005, New P: 1.321
-Original Grad: -0.092, -lr * Pred Grad:  0.019, New P: 0.581
iter 18 loss: 0.197
Actual params: [1.3213, 0.5808]
-Original Grad: 0.045, -lr * Pred Grad:  0.008, New P: 1.329
-Original Grad: -0.162, -lr * Pred Grad:  -0.000, New P: 0.580
iter 19 loss: 0.195
Actual params: [1.329 , 0.5805]
-Original Grad: 0.045, -lr * Pred Grad:  0.010, New P: 1.339
-Original Grad: -0.171, -lr * Pred Grad:  -0.017, New P: 0.563
iter 20 loss: 0.189
Actual params: [1.339 , 0.5635]
-Original Grad: 0.020, -lr * Pred Grad:  0.010, New P: 1.349
-Original Grad: -0.052, -lr * Pred Grad:  -0.020, New P: 0.543
Target params: [1.1812, 0.2779]
iter 0 loss: 0.378
Actual params: [0.5941, 0.5941]
-Original Grad: 0.208, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.073, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.240
Actual params: [0.6941, 0.4941]
-Original Grad: 0.002, -lr * Pred Grad:  0.068, New P: 0.762
-Original Grad: 0.009, -lr * Pred Grad:  -0.057, New P: 0.437
iter 2 loss: 0.212
Actual params: [0.7617, 0.4371]
-Original Grad: -0.039, -lr * Pred Grad:  0.040, New P: 0.801
-Original Grad: 0.031, -lr * Pred Grad:  -0.016, New P: 0.421
iter 3 loss: 0.204
Actual params: [0.8012, 0.4211]
-Original Grad: -0.040, -lr * Pred Grad:  0.021, New P: 0.822
-Original Grad: 0.015, -lr * Pred Grad:  -0.002, New P: 0.419
iter 4 loss: 0.205
Actual params: [0.8223, 0.4188]
-Original Grad: -0.023, -lr * Pred Grad:  0.012, New P: 0.834
-Original Grad: 0.039, -lr * Pred Grad:  0.022, New P: 0.441
iter 5 loss: 0.202
Actual params: [0.8342, 0.4408]
-Original Grad: -0.019, -lr * Pred Grad:  0.006, New P: 0.840
-Original Grad: 0.018, -lr * Pred Grad:  0.029, New P: 0.470
iter 6 loss: 0.202
Actual params: [0.8398, 0.4696]
-Original Grad: -0.046, -lr * Pred Grad:  -0.006, New P: 0.834
-Original Grad: -0.000, -lr * Pred Grad:  0.025, New P: 0.494
iter 7 loss: 0.203
Actual params: [0.8342, 0.4944]
-Original Grad: -0.043, -lr * Pred Grad:  -0.014, New P: 0.820
-Original Grad: 0.004, -lr * Pred Grad:  0.024, New P: 0.518
iter 8 loss: 0.203
Actual params: [0.8199, 0.5183]
-Original Grad: -0.043, -lr * Pred Grad:  -0.022, New P: 0.798
-Original Grad: -0.004, -lr * Pred Grad:  0.019, New P: 0.537
iter 9 loss: 0.201
Actual params: [0.7982, 0.5371]
-Original Grad: -0.045, -lr * Pred Grad:  -0.028, New P: 0.770
-Original Grad: 0.010, -lr * Pred Grad:  0.022, New P: 0.559
iter 10 loss: 0.199
Actual params: [0.7701, 0.5592]
-Original Grad: -0.030, -lr * Pred Grad:  -0.031, New P: 0.739
-Original Grad: 0.013, -lr * Pred Grad:  0.026, New P: 0.586
iter 11 loss: 0.210
Actual params: [0.7389, 0.5856]
-Original Grad: -0.025, -lr * Pred Grad:  -0.033, New P: 0.706
-Original Grad: 0.014, -lr * Pred Grad:  0.031, New P: 0.616
iter 12 loss: 0.235
Actual params: [0.706 , 0.6164]
-Original Grad: 0.002, -lr * Pred Grad:  -0.029, New P: 0.677
-Original Grad: 0.007, -lr * Pred Grad:  0.031, New P: 0.648
iter 13 loss: 0.294
Actual params: [0.6768, 0.6477]
-Original Grad: 0.108, -lr * Pred Grad:  -0.004, New P: 0.673
-Original Grad: -0.024, -lr * Pred Grad:  0.015, New P: 0.663
iter 14 loss: 0.306
Actual params: [0.673 , 0.6632]
-Original Grad: 0.186, -lr * Pred Grad:  0.025, New P: 0.698
-Original Grad: -0.070, -lr * Pred Grad:  -0.017, New P: 0.646
iter 15 loss: 0.256
Actual params: [0.6984, 0.6459]
-Original Grad: 0.023, -lr * Pred Grad:  0.027, New P: 0.725
-Original Grad: -0.007, -lr * Pred Grad:  -0.019, New P: 0.627
iter 16 loss: 0.216
Actual params: [0.725 , 0.6273]
-Original Grad: -0.008, -lr * Pred Grad:  0.023, New P: 0.748
-Original Grad: -0.003, -lr * Pred Grad:  -0.018, New P: 0.609
iter 17 loss: 0.203
Actual params: [0.7478, 0.6092]
-Original Grad: -0.010, -lr * Pred Grad:  0.019, New P: 0.767
-Original Grad: 0.003, -lr * Pred Grad:  -0.015, New P: 0.594
iter 18 loss: 0.198
Actual params: [0.7668, 0.5941]
-Original Grad: -0.014, -lr * Pred Grad:  0.015, New P: 0.782
-Original Grad: 0.010, -lr * Pred Grad:  -0.009, New P: 0.585
iter 19 loss: 0.200
Actual params: [0.782 , 0.5849]
-Original Grad: -0.035, -lr * Pred Grad:  0.008, New P: 0.790
-Original Grad: 0.012, -lr * Pred Grad:  -0.003, New P: 0.582
iter 20 loss: 0.205
Actual params: [0.7902, 0.5816]
-Original Grad: -0.073, -lr * Pred Grad:  -0.004, New P: 0.786
-Original Grad: -0.004, -lr * Pred Grad:  -0.005, New P: 0.577
Target params: [1.1812, 0.2779]
iter 0 loss: 0.346
Actual params: [0.5941, 0.5941]
-Original Grad: 0.088, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.562, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.335
Actual params: [0.6941, 0.4941]
-Original Grad: 0.020, -lr * Pred Grad:  0.082, New P: 0.776
-Original Grad: -0.070, -lr * Pred Grad:  -0.076, New P: 0.418
iter 2 loss: 0.351
Actual params: [0.7758, 0.4184]
-Original Grad: 0.032, -lr * Pred Grad:  0.081, New P: 0.857
-Original Grad: 0.010, -lr * Pred Grad:  -0.057, New P: 0.361
iter 3 loss: 0.348
Actual params: [0.8567, 0.361 ]
-Original Grad: 0.034, -lr * Pred Grad:  0.082, New P: 0.939
-Original Grad: 0.032, -lr * Pred Grad:  -0.044, New P: 0.317
iter 4 loss: 0.336
Actual params: [0.9387, 0.3173]
-Original Grad: 0.042, -lr * Pred Grad:  0.085, New P: 1.024
-Original Grad: 0.033, -lr * Pred Grad:  -0.034, New P: 0.284
iter 5 loss: 0.317
Actual params: [1.0237, 0.2837]
-Original Grad: 0.039, -lr * Pred Grad:  0.086, New P: 1.110
-Original Grad: 0.030, -lr * Pred Grad:  -0.026, New P: 0.257
iter 6 loss: 0.308
Actual params: [1.11  , 0.2574]
-Original Grad: 0.034, -lr * Pred Grad:  0.087, New P: 1.197
-Original Grad: 0.038, -lr * Pred Grad:  -0.020, New P: 0.238
iter 7 loss: 0.303
Actual params: [1.1965, 0.2379]
-Original Grad: 0.022, -lr * Pred Grad:  0.084, New P: 1.281
-Original Grad: 0.024, -lr * Pred Grad:  -0.015, New P: 0.223
iter 8 loss: 0.297
Actual params: [1.2805, 0.2228]
-Original Grad: 0.019, -lr * Pred Grad:  0.081, New P: 1.362
-Original Grad: 0.033, -lr * Pred Grad:  -0.011, New P: 0.212
iter 9 loss: 0.295
Actual params: [1.3617, 0.2122]
-Original Grad: 0.013, -lr * Pred Grad:  0.077, New P: 1.439
-Original Grad: 0.023, -lr * Pred Grad:  -0.007, New P: 0.205
iter 10 loss: 0.294
Actual params: [1.4388, 0.2048]
-Original Grad: 0.008, -lr * Pred Grad:  0.072, New P: 1.511
-Original Grad: 0.019, -lr * Pred Grad:  -0.005, New P: 0.200
iter 11 loss: 0.294
Actual params: [1.5111, 0.1997]
-Original Grad: 0.007, -lr * Pred Grad:  0.068, New P: 1.579
-Original Grad: 0.024, -lr * Pred Grad:  -0.003, New P: 0.197
iter 12 loss: 0.294
Actual params: [1.5787, 0.1971]
-Original Grad: 0.009, -lr * Pred Grad:  0.064, New P: 1.643
-Original Grad: 0.035, -lr * Pred Grad:  0.001, New P: 0.198
iter 13 loss: 0.295
Actual params: [1.643 , 0.1978]
-Original Grad: 0.006, -lr * Pred Grad:  0.060, New P: 1.703
-Original Grad: 0.032, -lr * Pred Grad:  0.003, New P: 0.201
iter 14 loss: 0.296
Actual params: [1.7034, 0.201 ]
-Original Grad: 0.006, -lr * Pred Grad:  0.057, New P: 1.760
-Original Grad: 0.035, -lr * Pred Grad:  0.006, New P: 0.207
iter 15 loss: 0.294
Actual params: [1.7602, 0.207 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.052, New P: 1.812
-Original Grad: 0.015, -lr * Pred Grad:  0.007, New P: 0.214
iter 16 loss: 0.295
Actual params: [1.8125, 0.2137]
-Original Grad: -0.001, -lr * Pred Grad:  0.047, New P: 1.860
-Original Grad: 0.016, -lr * Pred Grad:  0.007, New P: 0.221
iter 17 loss: 0.297
Actual params: [1.8597, 0.2211]
-Original Grad: -0.003, -lr * Pred Grad:  0.042, New P: 1.902
-Original Grad: 0.009, -lr * Pred Grad:  0.008, New P: 0.229
iter 18 loss: 0.299
Actual params: [1.9016, 0.2286]
-Original Grad: -0.000, -lr * Pred Grad:  0.038, New P: 1.939
-Original Grad: 0.014, -lr * Pred Grad:  0.008, New P: 0.237
iter 19 loss: 0.301
Actual params: [1.9395, 0.2366]
-Original Grad: -0.002, -lr * Pred Grad:  0.034, New P: 1.973
-Original Grad: 0.017, -lr * Pred Grad:  0.009, New P: 0.245
iter 20 loss: 0.304
Actual params: [1.9732, 0.2455]
-Original Grad: -0.004, -lr * Pred Grad:  0.029, New P: 2.002
-Original Grad: 0.004, -lr * Pred Grad:  0.008, New P: 0.254
Target params: [1.1812, 0.2779]
iter 0 loss: 0.584
Actual params: [0.5941, 0.5941]
-Original Grad: 0.228, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.196, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.446
Actual params: [0.6941, 0.4941]
-Original Grad: 0.073, -lr * Pred Grad:  0.086, New P: 0.780
-Original Grad: -0.173, -lr * Pred Grad:  -0.099, New P: 0.395
iter 2 loss: 0.361
Actual params: [0.7805, 0.3946]
-Original Grad: 0.020, -lr * Pred Grad:  0.072, New P: 0.852
-Original Grad: -0.108, -lr * Pred Grad:  -0.096, New P: 0.299
iter 3 loss: 0.299
Actual params: [0.8523, 0.299 ]
-Original Grad: 0.012, -lr * Pred Grad:  0.062, New P: 0.914
-Original Grad: -0.047, -lr * Pred Grad:  -0.087, New P: 0.212
iter 4 loss: 0.268
Actual params: [0.9139, 0.2123]
-Original Grad: -0.004, -lr * Pred Grad:  0.051, New P: 0.965
-Original Grad: -0.013, -lr * Pred Grad:  -0.076, New P: 0.136
iter 5 loss: 0.249
Actual params: [0.965 , 0.1365]
-Original Grad: -0.011, -lr * Pred Grad:  0.042, New P: 1.007
-Original Grad: -0.018, -lr * Pred Grad:  -0.068, New P: 0.068
iter 6 loss: 0.246
Actual params: [1.0066, 0.068 ]
-Original Grad: -0.003, -lr * Pred Grad:  0.036, New P: 1.042
-Original Grad: -0.013, -lr * Pred Grad:  -0.062, New P: 0.006
iter 7 loss: 0.240
Actual params: [1.0422, 0.0061]
-Original Grad: 0.005, -lr * Pred Grad:  0.033, New P: 1.075
-Original Grad: -0.003, -lr * Pred Grad:  -0.055, New P: -0.049
iter 8 loss: 0.234
Actual params: [ 1.0748, -0.0491]
-Original Grad: 0.013, -lr * Pred Grad:  0.032, New P: 1.106
-Original Grad: -0.005, -lr * Pred Grad:  -0.050, New P: -0.099
iter 9 loss: 0.231
Actual params: [ 1.1063, -0.0989]
-Original Grad: 0.010, -lr * Pred Grad:  0.030, New P: 1.136
-Original Grad: -0.002, -lr * Pred Grad:  -0.045, New P: -0.144
iter 10 loss: 0.228
Actual params: [ 1.1364, -0.1437]
-Original Grad: 0.006, -lr * Pred Grad:  0.028, New P: 1.165
-Original Grad: -0.001, -lr * Pred Grad:  -0.040, New P: -0.184
iter 11 loss: 0.227
Actual params: [ 1.1645, -0.1839]
-Original Grad: 0.004, -lr * Pred Grad:  0.026, New P: 1.191
-Original Grad: 0.002, -lr * Pred Grad:  -0.036, New P: -0.220
iter 12 loss: 0.227
Actual params: [ 1.1905, -0.2198]
-Original Grad: 0.005, -lr * Pred Grad:  0.025, New P: 1.215
-Original Grad: 0.000, -lr * Pred Grad:  -0.032, New P: -0.252
iter 13 loss: 0.227
Actual params: [ 1.2151, -0.2522]
-Original Grad: 0.001, -lr * Pred Grad:  0.022, New P: 1.237
-Original Grad: 0.007, -lr * Pred Grad:  -0.028, New P: -0.280
iter 14 loss: 0.228
Actual params: [ 1.2375, -0.2802]
-Original Grad: 0.003, -lr * Pred Grad:  0.021, New P: 1.258
-Original Grad: 0.005, -lr * Pred Grad:  -0.024, New P: -0.305
iter 15 loss: 0.229
Actual params: [ 1.2583, -0.3047]
-Original Grad: -0.001, -lr * Pred Grad:  0.019, New P: 1.277
-Original Grad: 0.008, -lr * Pred Grad:  -0.021, New P: -0.325
iter 16 loss: 0.229
Actual params: [ 1.277 , -0.3255]
-Original Grad: -0.002, -lr * Pred Grad:  0.017, New P: 1.294
-Original Grad: 0.010, -lr * Pred Grad:  -0.017, New P: -0.343
iter 17 loss: 0.229
Actual params: [ 1.2936, -0.3426]
-Original Grad: -0.005, -lr * Pred Grad:  0.014, New P: 1.308
-Original Grad: 0.011, -lr * Pred Grad:  -0.014, New P: -0.356
iter 18 loss: 0.230
Actual params: [ 1.3076, -0.3563]
-Original Grad: -0.000, -lr * Pred Grad:  0.013, New P: 1.320
-Original Grad: 0.011, -lr * Pred Grad:  -0.010, New P: -0.367
iter 19 loss: 0.234
Actual params: [ 1.3204, -0.3667]
-Original Grad: -0.002, -lr * Pred Grad:  0.011, New P: 1.332
-Original Grad: 0.013, -lr * Pred Grad:  -0.007, New P: -0.374
iter 20 loss: 0.235
Actual params: [ 1.3316, -0.374 ]
-Original Grad: 0.004, -lr * Pred Grad:  0.011, New P: 1.343
-Original Grad: 0.010, -lr * Pred Grad:  -0.005, New P: -0.379
Target params: [1.1812, 0.2779]
iter 0 loss: 0.312
Actual params: [0.5941, 0.5941]
-Original Grad: 0.012, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.059, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.252
Actual params: [0.6941, 0.4941]
-Original Grad: -0.012, -lr * Pred Grad:  -0.005, New P: 0.689
-Original Grad: 0.014, -lr * Pred Grad:  -0.048, New P: 0.446
iter 2 loss: 0.249
Actual params: [0.6889, 0.4457]
-Original Grad: -0.019, -lr * Pred Grad:  -0.050, New P: 0.639
-Original Grad: 0.015, -lr * Pred Grad:  -0.021, New P: 0.425
iter 3 loss: 0.252
Actual params: [0.6388, 0.4245]
-Original Grad: -0.025, -lr * Pred Grad:  -0.070, New P: 0.569
-Original Grad: 0.006, -lr * Pred Grad:  -0.012, New P: 0.412
iter 4 loss: 0.254
Actual params: [0.5688, 0.4124]
-Original Grad: -0.018, -lr * Pred Grad:  -0.078, New P: 0.491
-Original Grad: -0.006, -lr * Pred Grad:  -0.016, New P: 0.397
iter 5 loss: 0.270
Actual params: [0.4912, 0.3967]
-Original Grad: -0.020, -lr * Pred Grad:  -0.083, New P: 0.408
-Original Grad: -0.006, -lr * Pred Grad:  -0.018, New P: 0.379
iter 6 loss: 0.292
Actual params: [0.4079, 0.3785]
-Original Grad: 0.001, -lr * Pred Grad:  -0.072, New P: 0.336
-Original Grad: -0.009, -lr * Pred Grad:  -0.023, New P: 0.356
iter 7 loss: 0.316
Actual params: [0.3361, 0.3558]
-Original Grad: 0.036, -lr * Pred Grad:  -0.017, New P: 0.319
-Original Grad: -0.022, -lr * Pred Grad:  -0.035, New P: 0.320
iter 8 loss: 0.313
Actual params: [0.3188, 0.3205]
-Original Grad: 0.027, -lr * Pred Grad:  0.007, New P: 0.326
-Original Grad: -0.017, -lr * Pred Grad:  -0.042, New P: 0.278
iter 9 loss: 0.300
Actual params: [0.3259, 0.2782]
-Original Grad: 0.018, -lr * Pred Grad:  0.020, New P: 0.346
-Original Grad: -0.007, -lr * Pred Grad:  -0.042, New P: 0.236
iter 10 loss: 0.288
Actual params: [0.3455, 0.2361]
-Original Grad: 0.005, -lr * Pred Grad:  0.022, New P: 0.367
-Original Grad: -0.003, -lr * Pred Grad:  -0.040, New P: 0.196
iter 11 loss: 0.279
Actual params: [0.367 , 0.1962]
-Original Grad: -0.009, -lr * Pred Grad:  0.013, New P: 0.380
-Original Grad: 0.002, -lr * Pred Grad:  -0.034, New P: 0.162
iter 12 loss: 0.275
Actual params: [0.3798, 0.1618]
-Original Grad: 0.003, -lr * Pred Grad:  0.014, New P: 0.393
-Original Grad: -0.001, -lr * Pred Grad:  -0.032, New P: 0.130
iter 13 loss: 0.272
Actual params: [0.3934, 0.1302]
-Original Grad: -0.019, -lr * Pred Grad:  -0.002, New P: 0.392
-Original Grad: -0.001, -lr * Pred Grad:  -0.029, New P: 0.101
iter 14 loss: 0.273
Actual params: [0.3916, 0.1011]
-Original Grad: -0.021, -lr * Pred Grad:  -0.015, New P: 0.376
-Original Grad: 0.000, -lr * Pred Grad:  -0.026, New P: 0.075
iter 15 loss: 0.277
Actual params: [0.3762, 0.0748]
-Original Grad: -0.024, -lr * Pred Grad:  -0.029, New P: 0.347
-Original Grad: 0.001, -lr * Pred Grad:  -0.023, New P: 0.052
iter 16 loss: 0.286
Actual params: [0.3474, 0.0518]
-Original Grad: 0.004, -lr * Pred Grad:  -0.023, New P: 0.324
-Original Grad: 0.002, -lr * Pred Grad:  -0.019, New P: 0.033
iter 17 loss: 0.294
Actual params: [0.3239, 0.0326]
-Original Grad: 0.028, -lr * Pred Grad:  -0.003, New P: 0.321
-Original Grad: 0.005, -lr * Pred Grad:  -0.014, New P: 0.019
iter 18 loss: 0.295
Actual params: [0.3211, 0.0189]
-Original Grad: 0.042, -lr * Pred Grad:  0.021, New P: 0.342
-Original Grad: 0.005, -lr * Pred Grad:  -0.008, New P: 0.010
iter 19 loss: 0.289
Actual params: [0.3417, 0.0104]
-Original Grad: 0.006, -lr * Pred Grad:  0.022, New P: 0.364
-Original Grad: 0.006, -lr * Pred Grad:  -0.004, New P: 0.007
iter 20 loss: 0.283
Actual params: [0.3637, 0.0068]
-Original Grad: 0.004, -lr * Pred Grad:  0.022, New P: 0.386
-Original Grad: 0.005, -lr * Pred Grad:  0.000, New P: 0.007
Target params: [1.1812, 0.2779]
iter 0 loss: 0.235
Actual params: [0.5941, 0.5941]
-Original Grad: -0.130, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.413, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.254
Actual params: [0.4941, 0.4941]
-Original Grad: 0.174, -lr * Pred Grad:  0.019, New P: 0.513
-Original Grad: -0.248, -lr * Pred Grad:  -0.096, New P: 0.398
iter 2 loss: 0.255
Actual params: [0.5134, 0.3983]
-Original Grad: 0.035, -lr * Pred Grad:  0.025, New P: 0.538
-Original Grad: -0.017, -lr * Pred Grad:  -0.076, New P: 0.322
iter 3 loss: 0.251
Actual params: [0.5384, 0.3222]
-Original Grad: 0.018, -lr * Pred Grad:  0.025, New P: 0.563
-Original Grad: -0.005, -lr * Pred Grad:  -0.063, New P: 0.259
iter 4 loss: 0.248
Actual params: [0.5633, 0.2592]
-Original Grad: -0.099, -lr * Pred Grad:  -0.003, New P: 0.560
-Original Grad: -0.005, -lr * Pred Grad:  -0.054, New P: 0.205
iter 5 loss: 0.248
Actual params: [0.5602, 0.2054]
-Original Grad: -0.039, -lr * Pred Grad:  -0.011, New P: 0.549
-Original Grad: -0.007, -lr * Pred Grad:  -0.047, New P: 0.158
iter 6 loss: 0.248
Actual params: [0.5493, 0.1582]
-Original Grad: -0.033, -lr * Pred Grad:  -0.016, New P: 0.533
-Original Grad: 0.002, -lr * Pred Grad:  -0.041, New P: 0.117
iter 7 loss: 0.248
Actual params: [0.533 , 0.1172]
-Original Grad: -0.049, -lr * Pred Grad:  -0.024, New P: 0.509
-Original Grad: 0.002, -lr * Pred Grad:  -0.036, New P: 0.081
iter 8 loss: 0.250
Actual params: [0.5093, 0.0813]
-Original Grad: 0.025, -lr * Pred Grad:  -0.016, New P: 0.493
-Original Grad: 0.003, -lr * Pred Grad:  -0.032, New P: 0.050
iter 9 loss: 0.252
Actual params: [0.4932, 0.0498]
-Original Grad: 0.063, -lr * Pred Grad:  -0.002, New P: 0.491
-Original Grad: 0.013, -lr * Pred Grad:  -0.027, New P: 0.023
iter 10 loss: 0.252
Actual params: [0.4911, 0.0229]
-Original Grad: 0.069, -lr * Pred Grad:  0.011, New P: 0.502
-Original Grad: 0.018, -lr * Pred Grad:  -0.022, New P: 0.001
iter 11 loss: 0.250
Actual params: [0.5017, 0.0007]
-Original Grad: 0.038, -lr * Pred Grad:  0.016, New P: 0.518
-Original Grad: 0.021, -lr * Pred Grad:  -0.018, New P: -0.017
iter 12 loss: 0.247
Actual params: [ 0.5178, -0.0172]
-Original Grad: 0.002, -lr * Pred Grad:  0.015, New P: 0.533
-Original Grad: 0.009, -lr * Pred Grad:  -0.015, New P: -0.032
iter 13 loss: 0.245
Actual params: [ 0.5327, -0.0324]
-Original Grad: -0.027, -lr * Pred Grad:  0.008, New P: 0.541
-Original Grad: 0.018, -lr * Pred Grad:  -0.012, New P: -0.044
iter 14 loss: 0.244
Actual params: [ 0.5411, -0.0442]
-Original Grad: -0.037, -lr * Pred Grad:  0.001, New P: 0.542
-Original Grad: 0.024, -lr * Pred Grad:  -0.008, New P: -0.053
iter 15 loss: 0.244
Actual params: [ 0.5421, -0.0525]
-Original Grad: -0.036, -lr * Pred Grad:  -0.005, New P: 0.537
-Original Grad: 0.005, -lr * Pred Grad:  -0.007, New P: -0.060
iter 16 loss: 0.244
Actual params: [ 0.5367, -0.0595]
-Original Grad: -0.010, -lr * Pred Grad:  -0.007, New P: 0.530
-Original Grad: 0.022, -lr * Pred Grad:  -0.004, New P: -0.064
iter 17 loss: 0.245
Actual params: [ 0.53  , -0.0637]
-Original Grad: -0.024, -lr * Pred Grad:  -0.010, New P: 0.520
-Original Grad: 0.015, -lr * Pred Grad:  -0.002, New P: -0.066
iter 18 loss: 0.246
Actual params: [ 0.5196, -0.0659]
-Original Grad: 0.019, -lr * Pred Grad:  -0.006, New P: 0.514
-Original Grad: 0.008, -lr * Pred Grad:  -0.001, New P: -0.067
iter 19 loss: 0.247
Actual params: [ 0.5137, -0.0671]
-Original Grad: 0.022, -lr * Pred Grad:  -0.001, New P: 0.512
-Original Grad: 0.018, -lr * Pred Grad:  0.001, New P: -0.066
iter 20 loss: 0.247
Actual params: [ 0.5122, -0.0662]
-Original Grad: 0.023, -lr * Pred Grad:  0.003, New P: 0.515
-Original Grad: 0.016, -lr * Pred Grad:  0.002, New P: -0.064
Target params: [1.1812, 0.2779]
iter 0 loss: 0.731
Actual params: [0.5941, 0.5941]
-Original Grad: 0.190, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.517, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.646
Actual params: [0.6941, 0.4941]
-Original Grad: 0.127, -lr * Pred Grad:  0.097, New P: 0.791
-Original Grad: -0.313, -lr * Pred Grad:  -0.096, New P: 0.398
iter 2 loss: 0.520
Actual params: [0.7912, 0.3982]
-Original Grad: 0.067, -lr * Pred Grad:  0.090, New P: 0.881
-Original Grad: -0.241, -lr * Pred Grad:  -0.092, New P: 0.306
iter 3 loss: 0.409
Actual params: [0.8811, 0.3057]
-Original Grad: 0.018, -lr * Pred Grad:  0.078, New P: 0.959
-Original Grad: -0.108, -lr * Pred Grad:  -0.084, New P: 0.221
iter 4 loss: 0.338
Actual params: [0.9589, 0.2215]
-Original Grad: -0.003, -lr * Pred Grad:  0.065, New P: 1.024
-Original Grad: -0.031, -lr * Pred Grad:  -0.074, New P: 0.148
iter 5 loss: 0.312
Actual params: [1.0241, 0.1477]
-Original Grad: -0.007, -lr * Pred Grad:  0.055, New P: 1.079
-Original Grad: -0.017, -lr * Pred Grad:  -0.065, New P: 0.083
iter 6 loss: 0.302
Actual params: [1.0786, 0.0828]
-Original Grad: -0.014, -lr * Pred Grad:  0.045, New P: 1.123
-Original Grad: -0.017, -lr * Pred Grad:  -0.058, New P: 0.025
iter 7 loss: 0.291
Actual params: [1.1233, 0.0249]
-Original Grad: -0.003, -lr * Pred Grad:  0.039, New P: 1.162
-Original Grad: -0.004, -lr * Pred Grad:  -0.051, New P: -0.026
iter 8 loss: 0.285
Actual params: [ 1.162 , -0.0265]
-Original Grad: -0.003, -lr * Pred Grad:  0.034, New P: 1.196
-Original Grad: 0.003, -lr * Pred Grad:  -0.045, New P: -0.072
iter 9 loss: 0.281
Actual params: [ 1.1958, -0.0719]
-Original Grad: 0.003, -lr * Pred Grad:  0.031, New P: 1.227
-Original Grad: 0.008, -lr * Pred Grad:  -0.040, New P: -0.112
iter 10 loss: 0.279
Actual params: [ 1.2265, -0.1118]
-Original Grad: 0.002, -lr * Pred Grad:  0.028, New P: 1.255
-Original Grad: 0.008, -lr * Pred Grad:  -0.035, New P: -0.147
iter 11 loss: 0.278
Actual params: [ 1.2546, -0.1469]
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 1.280
-Original Grad: 0.008, -lr * Pred Grad:  -0.031, New P: -0.178
iter 12 loss: 0.277
Actual params: [ 1.2798, -0.1779]
-Original Grad: 0.003, -lr * Pred Grad:  0.023, New P: 1.303
-Original Grad: 0.017, -lr * Pred Grad:  -0.027, New P: -0.205
iter 13 loss: 0.276
Actual params: [ 1.3031, -0.2045]
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 1.324
-Original Grad: 0.014, -lr * Pred Grad:  -0.023, New P: -0.228
iter 14 loss: 0.276
Actual params: [ 1.3241, -0.2276]
-Original Grad: 0.001, -lr * Pred Grad:  0.019, New P: 1.343
-Original Grad: 0.012, -lr * Pred Grad:  -0.020, New P: -0.248
iter 15 loss: 0.276
Actual params: [ 1.3433, -0.2476]
-Original Grad: 0.005, -lr * Pred Grad:  0.018, New P: 1.362
-Original Grad: 0.022, -lr * Pred Grad:  -0.016, New P: -0.264
iter 16 loss: 0.275
Actual params: [ 1.3617, -0.264 ]
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 1.379
-Original Grad: 0.032, -lr * Pred Grad:  -0.012, New P: -0.276
iter 17 loss: 0.275
Actual params: [ 1.3787, -0.2765]
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 1.394
-Original Grad: 0.028, -lr * Pred Grad:  -0.009, New P: -0.286
iter 18 loss: 0.275
Actual params: [ 1.3943, -0.2857]
-Original Grad: -0.001, -lr * Pred Grad:  0.014, New P: 1.408
-Original Grad: 0.034, -lr * Pred Grad:  -0.006, New P: -0.291
iter 19 loss: 0.275
Actual params: [ 1.4083, -0.2915]
-Original Grad: -0.002, -lr * Pred Grad:  0.012, New P: 1.421
-Original Grad: 0.023, -lr * Pred Grad:  -0.003, New P: -0.295
iter 20 loss: 0.275
Actual params: [ 1.4206, -0.295 ]
-Original Grad: -0.003, -lr * Pred Grad:  0.010, New P: 1.431
-Original Grad: 0.024, -lr * Pred Grad:  -0.001, New P: -0.296
Target params: [1.1812, 0.2779]
iter 0 loss: 0.710
Actual params: [0.5941, 0.5941]
-Original Grad: 0.352, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.246, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.612
Actual params: [0.6941, 0.4941]
-Original Grad: 0.118, -lr * Pred Grad:  0.087, New P: 0.781
-Original Grad: -0.070, -lr * Pred Grad:  -0.085, New P: 0.409
iter 2 loss: 0.556
Actual params: [0.7813, 0.4092]
-Original Grad: 0.073, -lr * Pred Grad:  0.078, New P: 0.860
-Original Grad: -0.022, -lr * Pred Grad:  -0.071, New P: 0.338
iter 3 loss: 0.529
Actual params: [0.8597, 0.3384]
-Original Grad: 0.014, -lr * Pred Grad:  0.066, New P: 0.926
-Original Grad: 0.009, -lr * Pred Grad:  -0.056, New P: 0.282
iter 4 loss: 0.514
Actual params: [0.9261, 0.2825]
-Original Grad: 0.028, -lr * Pred Grad:  0.060, New P: 0.986
-Original Grad: 0.024, -lr * Pred Grad:  -0.042, New P: 0.241
iter 5 loss: 0.507
Actual params: [0.9861, 0.2405]
-Original Grad: 0.004, -lr * Pred Grad:  0.052, New P: 1.038
-Original Grad: 0.016, -lr * Pred Grad:  -0.033, New P: 0.208
iter 6 loss: 0.505
Actual params: [1.0384, 0.2076]
-Original Grad: -0.011, -lr * Pred Grad:  0.044, New P: 1.083
-Original Grad: 0.018, -lr * Pred Grad:  -0.025, New P: 0.182
iter 7 loss: 0.505
Actual params: [1.0826, 0.1824]
-Original Grad: 0.015, -lr * Pred Grad:  0.041, New P: 1.123
-Original Grad: 0.051, -lr * Pred Grad:  -0.012, New P: 0.170
iter 8 loss: 0.508
Actual params: [1.1234, 0.1702]
-Original Grad: 0.014, -lr * Pred Grad:  0.038, New P: 1.161
-Original Grad: 0.046, -lr * Pred Grad:  -0.002, New P: 0.168
iter 9 loss: 0.512
Actual params: [1.1615, 0.1678]
-Original Grad: 0.010, -lr * Pred Grad:  0.035, New P: 1.197
-Original Grad: 0.031, -lr * Pred Grad:  0.004, New P: 0.171
iter 10 loss: 0.515
Actual params: [1.1967, 0.1714]
-Original Grad: 0.007, -lr * Pred Grad:  0.032, New P: 1.229
-Original Grad: 0.037, -lr * Pred Grad:  0.010, New P: 0.181
iter 11 loss: 0.519
Actual params: [1.2292, 0.1811]
-Original Grad: 0.007, -lr * Pred Grad:  0.030, New P: 1.259
-Original Grad: 0.042, -lr * Pred Grad:  0.016, New P: 0.197
iter 12 loss: 0.518
Actual params: [1.2593, 0.197 ]
-Original Grad: -0.004, -lr * Pred Grad:  0.027, New P: 1.286
-Original Grad: 0.021, -lr * Pred Grad:  0.018, New P: 0.215
iter 13 loss: 0.520
Actual params: [1.2859, 0.215 ]
-Original Grad: -0.004, -lr * Pred Grad:  0.024, New P: 1.309
-Original Grad: 0.018, -lr * Pred Grad:  0.019, New P: 0.234
iter 14 loss: 0.527
Actual params: [1.3095, 0.2344]
-Original Grad: -0.004, -lr * Pred Grad:  0.021, New P: 1.330
-Original Grad: -0.004, -lr * Pred Grad:  0.017, New P: 0.251
iter 15 loss: 0.533
Actual params: [1.3303, 0.2512]
-Original Grad: -0.014, -lr * Pred Grad:  0.017, New P: 1.347
-Original Grad: -0.019, -lr * Pred Grad:  0.012, New P: 0.263
iter 16 loss: 0.539
Actual params: [1.3473, 0.2632]
-Original Grad: -0.003, -lr * Pred Grad:  0.015, New P: 1.362
-Original Grad: -0.014, -lr * Pred Grad:  0.008, New P: 0.271
iter 17 loss: 0.543
Actual params: [1.3624, 0.2715]
-Original Grad: -0.011, -lr * Pred Grad:  0.012, New P: 1.375
-Original Grad: -0.027, -lr * Pred Grad:  0.003, New P: 0.274
iter 18 loss: 0.545
Actual params: [1.3745, 0.2741]
-Original Grad: -0.009, -lr * Pred Grad:  0.010, New P: 1.384
-Original Grad: -0.019, -lr * Pred Grad:  -0.001, New P: 0.273
iter 19 loss: 0.545
Actual params: [1.3844, 0.273 ]
-Original Grad: -0.004, -lr * Pred Grad:  0.008, New P: 1.393
-Original Grad: -0.007, -lr * Pred Grad:  -0.002, New P: 0.271
iter 20 loss: 0.546
Actual params: [1.3929, 0.2707]
-Original Grad: -0.016, -lr * Pred Grad:  0.005, New P: 1.398
-Original Grad: -0.028, -lr * Pred Grad:  -0.007, New P: 0.263
Target params: [1.1812, 0.2779]
iter 0 loss: 0.120
Actual params: [0.5941, 0.5941]
-Original Grad: -0.004, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.126, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.216
Actual params: [0.4941, 0.4941]
-Original Grad: 0.029, -lr * Pred Grad:  0.065, New P: 0.559
-Original Grad: -0.046, -lr * Pred Grad:  -0.088, New P: 0.406
iter 2 loss: 0.157
Actual params: [0.5594, 0.4057]
-Original Grad: 0.032, -lr * Pred Grad:  0.081, New P: 0.641
-Original Grad: -0.010, -lr * Pred Grad:  -0.073, New P: 0.333
iter 3 loss: 0.099
Actual params: [0.6407, 0.3326]
-Original Grad: 0.017, -lr * Pred Grad:  0.083, New P: 0.724
-Original Grad: -0.000, -lr * Pred Grad:  -0.060, New P: 0.273
iter 4 loss: 0.092
Actual params: [0.7239, 0.2726]
-Original Grad: -0.006, -lr * Pred Grad:  0.063, New P: 0.787
-Original Grad: -0.009, -lr * Pred Grad:  -0.054, New P: 0.218
iter 5 loss: 0.102
Actual params: [0.7866, 0.2184]
-Original Grad: -0.019, -lr * Pred Grad:  0.031, New P: 0.817
-Original Grad: -0.013, -lr * Pred Grad:  -0.052, New P: 0.167
iter 6 loss: 0.103
Actual params: [0.8171, 0.1668]
-Original Grad: -0.023, -lr * Pred Grad:  0.004, New P: 0.821
-Original Grad: -0.012, -lr * Pred Grad:  -0.050, New P: 0.117
iter 7 loss: 0.099
Actual params: [0.8208, 0.1173]
-Original Grad: -0.018, -lr * Pred Grad:  -0.012, New P: 0.809
-Original Grad: -0.004, -lr * Pred Grad:  -0.045, New P: 0.072
iter 8 loss: 0.093
Actual params: [0.8087, 0.0721]
-Original Grad: -0.005, -lr * Pred Grad:  -0.015, New P: 0.794
-Original Grad: 0.005, -lr * Pred Grad:  -0.038, New P: 0.034
iter 9 loss: 0.087
Actual params: [0.7938, 0.0339]
-Original Grad: -0.007, -lr * Pred Grad:  -0.019, New P: 0.775
-Original Grad: 0.008, -lr * Pred Grad:  -0.031, New P: 0.003
iter 10 loss: 0.081
Actual params: [0.7745, 0.0025]
-Original Grad: -0.004, -lr * Pred Grad:  -0.020, New P: 0.754
-Original Grad: 0.013, -lr * Pred Grad:  -0.023, New P: -0.021
iter 11 loss: 0.078
Actual params: [ 0.7542, -0.0208]
-Original Grad: 0.008, -lr * Pred Grad:  -0.012, New P: 0.743
-Original Grad: 0.006, -lr * Pred Grad:  -0.019, New P: -0.040
iter 12 loss: 0.079
Actual params: [ 0.7426, -0.0397]
-Original Grad: 0.010, -lr * Pred Grad:  -0.002, New P: 0.740
-Original Grad: 0.017, -lr * Pred Grad:  -0.011, New P: -0.051
iter 13 loss: 0.080
Actual params: [ 0.7402, -0.0507]
-Original Grad: 0.011, -lr * Pred Grad:  0.007, New P: 0.747
-Original Grad: 0.011, -lr * Pred Grad:  -0.006, New P: -0.057
iter 14 loss: 0.079
Actual params: [ 0.747 , -0.0567]
-Original Grad: 0.011, -lr * Pred Grad:  0.015, New P: 0.762
-Original Grad: 0.016, -lr * Pred Grad:  0.000, New P: -0.056
iter 15 loss: 0.078
Actual params: [ 0.7618, -0.0565]
-Original Grad: 0.005, -lr * Pred Grad:  0.017, New P: 0.779
-Original Grad: 0.010, -lr * Pred Grad:  0.004, New P: -0.053
iter 16 loss: 0.080
Actual params: [ 0.779 , -0.0527]
-Original Grad: 0.002, -lr * Pred Grad:  0.017, New P: 0.796
-Original Grad: 0.013, -lr * Pred Grad:  0.008, New P: -0.045
iter 17 loss: 0.084
Actual params: [ 0.7958, -0.0445]
-Original Grad: -0.004, -lr * Pred Grad:  0.012, New P: 0.808
-Original Grad: 0.009, -lr * Pred Grad:  0.010, New P: -0.034
iter 18 loss: 0.087
Actual params: [ 0.8081, -0.0341]
-Original Grad: -0.004, -lr * Pred Grad:  0.008, New P: 0.816
-Original Grad: 0.011, -lr * Pred Grad:  0.013, New P: -0.021
iter 19 loss: 0.089
Actual params: [ 0.816 , -0.0207]
-Original Grad: -0.008, -lr * Pred Grad:  0.001, New P: 0.817
-Original Grad: 0.016, -lr * Pred Grad:  0.018, New P: -0.003
iter 20 loss: 0.090
Actual params: [ 0.8167, -0.0029]
-Original Grad: -0.015, -lr * Pred Grad:  -0.011, New P: 0.805
-Original Grad: 0.009, -lr * Pred Grad:  0.019, New P: 0.016
Target params: [1.1812, 0.2779]
iter 0 loss: 1.013
Actual params: [0.5941, 0.5941]
-Original Grad: 0.727, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.942, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.742
Actual params: [0.6941, 0.4941]
-Original Grad: 0.904, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.691, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.485
Actual params: [0.794, 0.396]
-Original Grad: 0.275, -lr * Pred Grad:  0.090, New P: 0.884
-Original Grad: -0.139, -lr * Pred Grad:  -0.083, New P: 0.313
iter 3 loss: 0.278
Actual params: [0.884 , 0.3132]
-Original Grad: 0.140, -lr * Pred Grad:  0.080, New P: 0.964
-Original Grad: -0.021, -lr * Pred Grad:  -0.069, New P: 0.244
iter 4 loss: 0.174
Actual params: [0.964 , 0.2443]
-Original Grad: 0.001, -lr * Pred Grad:  0.068, New P: 1.032
-Original Grad: -0.011, -lr * Pred Grad:  -0.059, New P: 0.186
iter 5 loss: 0.161
Actual params: [1.0316, 0.1856]
-Original Grad: -0.001, -lr * Pred Grad:  0.058, New P: 1.090
-Original Grad: -0.013, -lr * Pred Grad:  -0.051, New P: 0.134
iter 6 loss: 0.162
Actual params: [1.0899, 0.1344]
-Original Grad: 0.004, -lr * Pred Grad:  0.051, New P: 1.141
-Original Grad: -0.008, -lr * Pred Grad:  -0.045, New P: 0.089
iter 7 loss: 0.166
Actual params: [1.141 , 0.0893]
-Original Grad: 0.004, -lr * Pred Grad:  0.045, New P: 1.186
-Original Grad: -0.007, -lr * Pred Grad:  -0.040, New P: 0.049
iter 8 loss: 0.169
Actual params: [1.1862, 0.0493]
-Original Grad: 0.004, -lr * Pred Grad:  0.040, New P: 1.226
-Original Grad: -0.006, -lr * Pred Grad:  -0.036, New P: 0.014
iter 9 loss: 0.170
Actual params: [1.2265, 0.0135]
-Original Grad: 0.001, -lr * Pred Grad:  0.036, New P: 1.263
-Original Grad: -0.008, -lr * Pred Grad:  -0.032, New P: -0.019
iter 10 loss: 0.176
Actual params: [ 1.2625, -0.0187]
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: 1.295
-Original Grad: -0.005, -lr * Pred Grad:  -0.029, New P: -0.048
iter 11 loss: 0.181
Actual params: [ 1.2948, -0.0478]
-Original Grad: 0.001, -lr * Pred Grad:  0.029, New P: 1.324
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -0.074
iter 12 loss: 0.186
Actual params: [ 1.3239, -0.074 ]
-Original Grad: -0.000, -lr * Pred Grad:  0.026, New P: 1.350
-Original Grad: 0.004, -lr * Pred Grad:  -0.023, New P: -0.097
iter 13 loss: 0.191
Actual params: [ 1.3501, -0.0975]
-Original Grad: -0.001, -lr * Pred Grad:  0.024, New P: 1.374
-Original Grad: 0.011, -lr * Pred Grad:  -0.021, New P: -0.118
iter 14 loss: 0.198
Actual params: [ 1.3737, -0.1182]
-Original Grad: -0.007, -lr * Pred Grad:  0.021, New P: 1.395
-Original Grad: 0.007, -lr * Pred Grad:  -0.018, New P: -0.137
iter 15 loss: 0.206
Actual params: [ 1.3948, -0.1367]
-Original Grad: -0.007, -lr * Pred Grad:  0.019, New P: 1.414
-Original Grad: 0.009, -lr * Pred Grad:  -0.016, New P: -0.153
iter 16 loss: 0.213
Actual params: [ 1.4137, -0.153 ]
-Original Grad: -0.006, -lr * Pred Grad:  0.017, New P: 1.431
-Original Grad: 0.015, -lr * Pred Grad:  -0.014, New P: -0.167
iter 17 loss: 0.219
Actual params: [ 1.4305, -0.1672]
-Original Grad: -0.008, -lr * Pred Grad:  0.015, New P: 1.445
-Original Grad: 0.011, -lr * Pred Grad:  -0.012, New P: -0.180
iter 18 loss: 0.224
Actual params: [ 1.4455, -0.1797]
-Original Grad: -0.004, -lr * Pred Grad:  0.013, New P: 1.459
-Original Grad: 0.015, -lr * Pred Grad:  -0.011, New P: -0.190
iter 19 loss: 0.229
Actual params: [ 1.4588, -0.1903]
-Original Grad: -0.011, -lr * Pred Grad:  0.012, New P: 1.471
-Original Grad: 0.008, -lr * Pred Grad:  -0.009, New P: -0.200
iter 20 loss: 0.228
Actual params: [ 1.4706, -0.1997]
-Original Grad: -0.009, -lr * Pred Grad:  0.010, New P: 1.481
-Original Grad: 0.008, -lr * Pred Grad:  -0.008, New P: -0.208
Target params: [1.1812, 0.2779]
iter 0 loss: 0.857
Actual params: [0.5941, 0.5941]
-Original Grad: 0.165, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.313, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.830
Actual params: [0.6941, 0.4941]
-Original Grad: 0.136, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.389, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 0.798
Actual params: [0.7931, 0.3941]
-Original Grad: 0.113, -lr * Pred Grad:  0.098, New P: 0.891
-Original Grad: -0.405, -lr * Pred Grad:  -0.100, New P: 0.294
iter 3 loss: 0.746
Actual params: [0.8906, 0.2938]
-Original Grad: 0.037, -lr * Pred Grad:  0.088, New P: 0.978
-Original Grad: -0.387, -lr * Pred Grad:  -0.100, New P: 0.193
iter 4 loss: 0.645
Actual params: [0.9784, 0.1935]
-Original Grad: 0.019, -lr * Pred Grad:  0.078, New P: 1.057
-Original Grad: -0.174, -lr * Pred Grad:  -0.095, New P: 0.099
iter 5 loss: 0.579
Actual params: [1.0565, 0.0985]
-Original Grad: 0.007, -lr * Pred Grad:  0.069, New P: 1.125
-Original Grad: -0.138, -lr * Pred Grad:  -0.090, New P: 0.009
iter 6 loss: 0.597
Actual params: [1.1254, 0.0087]
-Original Grad: -0.027, -lr * Pred Grad:  0.054, New P: 1.180
-Original Grad: -0.106, -lr * Pred Grad:  -0.085, New P: -0.076
iter 7 loss: 0.612
Actual params: [ 1.1796, -0.0758]
-Original Grad: -0.044, -lr * Pred Grad:  0.038, New P: 1.218
-Original Grad: -0.100, -lr * Pred Grad:  -0.080, New P: -0.156
iter 8 loss: 0.626
Actual params: [ 1.218 , -0.1559]
-Original Grad: -0.059, -lr * Pred Grad:  0.022, New P: 1.240
-Original Grad: -0.070, -lr * Pred Grad:  -0.075, New P: -0.231
iter 9 loss: 0.629
Actual params: [ 1.2399, -0.2311]
-Original Grad: -0.068, -lr * Pred Grad:  0.007, New P: 1.247
-Original Grad: -0.029, -lr * Pred Grad:  -0.069, New P: -0.300
iter 10 loss: 0.639
Actual params: [ 1.2465, -0.2999]
-Original Grad: -0.062, -lr * Pred Grad:  -0.005, New P: 1.241
-Original Grad: -0.023, -lr * Pred Grad:  -0.063, New P: -0.363
iter 11 loss: 0.639
Actual params: [ 1.2414, -0.3629]
-Original Grad: -0.059, -lr * Pred Grad:  -0.015, New P: 1.227
-Original Grad: -0.017, -lr * Pred Grad:  -0.058, New P: -0.421
iter 12 loss: 0.643
Actual params: [ 1.2267, -0.4206]
-Original Grad: -0.081, -lr * Pred Grad:  -0.026, New P: 1.200
-Original Grad: -0.024, -lr * Pred Grad:  -0.053, New P: -0.474
iter 13 loss: 0.639
Actual params: [ 1.2005, -0.4741]
-Original Grad: -0.068, -lr * Pred Grad:  -0.034, New P: 1.166
-Original Grad: -0.026, -lr * Pred Grad:  -0.050, New P: -0.524
iter 14 loss: 0.630
Actual params: [ 1.1665, -0.5239]
-Original Grad: -0.068, -lr * Pred Grad:  -0.041, New P: 1.126
-Original Grad: 0.057, -lr * Pred Grad:  -0.041, New P: -0.565
iter 15 loss: 0.621
Actual params: [ 1.1256, -0.5654]
-Original Grad: -0.034, -lr * Pred Grad:  -0.042, New P: 1.083
-Original Grad: 0.081, -lr * Pred Grad:  -0.032, New P: -0.598
iter 16 loss: 0.608
Actual params: [ 1.0833, -0.5979]
-Original Grad: -0.046, -lr * Pred Grad:  -0.045, New P: 1.038
-Original Grad: 0.149, -lr * Pred Grad:  -0.020, New P: -0.618
iter 17 loss: 0.587
Actual params: [ 1.0381, -0.6178]
-Original Grad: -0.026, -lr * Pred Grad:  -0.045, New P: 0.993
-Original Grad: 0.171, -lr * Pred Grad:  -0.008, New P: -0.625
iter 18 loss: 0.567
Actual params: [ 0.993 , -0.6254]
-Original Grad: -0.059, -lr * Pred Grad:  -0.050, New P: 0.943
-Original Grad: 0.198, -lr * Pred Grad:  0.005, New P: -0.620
iter 19 loss: 0.545
Actual params: [ 0.9433, -0.6204]
-Original Grad: -0.064, -lr * Pred Grad:  -0.054, New P: 0.889
-Original Grad: 0.158, -lr * Pred Grad:  0.014, New P: -0.607
iter 20 loss: 0.535
Actual params: [ 0.8891, -0.6068]
-Original Grad: -0.023, -lr * Pred Grad:  -0.053, New P: 0.836
-Original Grad: 0.196, -lr * Pred Grad:  0.023, New P: -0.583
Target params: [1.1812, 0.2779]
iter 0 loss: 0.309
Actual params: [0.5941, 0.5941]
-Original Grad: 0.182, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.139, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.284
Actual params: [0.6941, 0.4941]
-Original Grad: 0.004, -lr * Pred Grad:  0.069, New P: 0.763
-Original Grad: -0.061, -lr * Pred Grad:  -0.091, New P: 0.403
iter 2 loss: 0.290
Actual params: [0.7627, 0.4027]
-Original Grad: -0.133, -lr * Pred Grad:  0.005, New P: 0.768
-Original Grad: -0.022, -lr * Pred Grad:  -0.079, New P: 0.324
iter 3 loss: 0.291
Actual params: [0.7677, 0.3236]
-Original Grad: -0.120, -lr * Pred Grad:  -0.024, New P: 0.744
-Original Grad: -0.007, -lr * Pred Grad:  -0.067, New P: 0.256
iter 4 loss: 0.284
Actual params: [0.744 , 0.2563]
-Original Grad: -0.075, -lr * Pred Grad:  -0.035, New P: 0.709
-Original Grad: 0.004, -lr * Pred Grad:  -0.056, New P: 0.201
iter 5 loss: 0.288
Actual params: [0.7094, 0.2007]
-Original Grad: -0.032, -lr * Pred Grad:  -0.036, New P: 0.674
-Original Grad: 0.007, -lr * Pred Grad:  -0.046, New P: 0.155
iter 6 loss: 0.293
Actual params: [0.6736, 0.155 ]
-Original Grad: -0.012, -lr * Pred Grad:  -0.034, New P: 0.640
-Original Grad: 0.009, -lr * Pred Grad:  -0.037, New P: 0.118
iter 7 loss: 0.298
Actual params: [0.64  , 0.1183]
-Original Grad: 0.036, -lr * Pred Grad:  -0.023, New P: 0.617
-Original Grad: 0.015, -lr * Pred Grad:  -0.027, New P: 0.091
iter 8 loss: 0.301
Actual params: [0.6173, 0.091 ]
-Original Grad: 0.050, -lr * Pred Grad:  -0.011, New P: 0.606
-Original Grad: 0.016, -lr * Pred Grad:  -0.019, New P: 0.072
iter 9 loss: 0.302
Actual params: [0.6063, 0.072 ]
-Original Grad: 0.058, -lr * Pred Grad:  0.000, New P: 0.607
-Original Grad: 0.011, -lr * Pred Grad:  -0.014, New P: 0.058
iter 10 loss: 0.302
Actual params: [0.6068, 0.0583]
-Original Grad: 0.056, -lr * Pred Grad:  0.010, New P: 0.617
-Original Grad: 0.014, -lr * Pred Grad:  -0.008, New P: 0.050
iter 11 loss: 0.300
Actual params: [0.6167, 0.0504]
-Original Grad: 0.064, -lr * Pred Grad:  0.019, New P: 0.636
-Original Grad: 0.012, -lr * Pred Grad:  -0.004, New P: 0.047
iter 12 loss: 0.296
Actual params: [0.6358, 0.0468]
-Original Grad: 0.035, -lr * Pred Grad:  0.023, New P: 0.659
-Original Grad: 0.012, -lr * Pred Grad:  0.000, New P: 0.047
iter 13 loss: 0.293
Actual params: [0.6587, 0.0471]
-Original Grad: 0.013, -lr * Pred Grad:  0.023, New P: 0.681
-Original Grad: 0.011, -lr * Pred Grad:  0.004, New P: 0.051
iter 14 loss: 0.290
Actual params: [0.6814, 0.051 ]
-Original Grad: 0.007, -lr * Pred Grad:  0.022, New P: 0.703
-Original Grad: 0.015, -lr * Pred Grad:  0.008, New P: 0.059
iter 15 loss: 0.287
Actual params: [0.7032, 0.0589]
-Original Grad: -0.043, -lr * Pred Grad:  0.012, New P: 0.716
-Original Grad: 0.009, -lr * Pred Grad:  0.010, New P: 0.069
iter 16 loss: 0.286
Actual params: [0.7157, 0.0689]
-Original Grad: -0.049, -lr * Pred Grad:  0.003, New P: 0.719
-Original Grad: 0.009, -lr * Pred Grad:  0.012, New P: 0.081
iter 17 loss: 0.286
Actual params: [0.7189, 0.0808]
-Original Grad: -0.035, -lr * Pred Grad:  -0.003, New P: 0.716
-Original Grad: 0.009, -lr * Pred Grad:  0.014, New P: 0.095
iter 18 loss: 0.286
Actual params: [0.7161, 0.0946]
-Original Grad: -0.042, -lr * Pred Grad:  -0.009, New P: 0.707
-Original Grad: 0.017, -lr * Pred Grad:  0.018, New P: 0.112
iter 19 loss: 0.288
Actual params: [0.7067, 0.1123]
-Original Grad: -0.023, -lr * Pred Grad:  -0.012, New P: 0.694
-Original Grad: 0.015, -lr * Pred Grad:  0.021, New P: 0.133
iter 20 loss: 0.290
Actual params: [0.6944, 0.1329]
-Original Grad: -0.030, -lr * Pred Grad:  -0.016, New P: 0.678
-Original Grad: 0.011, -lr * Pred Grad:  0.022, New P: 0.155
Target params: [1.1812, 0.2779]
iter 0 loss: 0.966
Actual params: [0.5941, 0.5941]
-Original Grad: 0.189, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.275, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.828
Actual params: [0.6941, 0.4941]
-Original Grad: 0.408, -lr * Pred Grad:  0.096, New P: 0.790
-Original Grad: -0.384, -lr * Pred Grad:  -0.100, New P: 0.395
iter 2 loss: 0.611
Actual params: [0.7897, 0.3945]
-Original Grad: 0.388, -lr * Pred Grad:  0.098, New P: 0.887
-Original Grad: -0.170, -lr * Pred Grad:  -0.094, New P: 0.301
iter 3 loss: 0.401
Actual params: [0.8875, 0.3006]
-Original Grad: 0.133, -lr * Pred Grad:  0.091, New P: 0.978
-Original Grad: -0.116, -lr * Pred Grad:  -0.088, New P: 0.212
iter 4 loss: 0.279
Actual params: [0.9783, 0.2125]
-Original Grad: 0.071, -lr * Pred Grad:  0.083, New P: 1.061
-Original Grad: 0.069, -lr * Pred Grad:  -0.067, New P: 0.146
iter 5 loss: 0.259
Actual params: [1.0608, 0.1459]
-Original Grad: 0.083, -lr * Pred Grad:  0.078, New P: 1.138
-Original Grad: 0.048, -lr * Pred Grad:  -0.052, New P: 0.094
iter 6 loss: 0.278
Actual params: [1.1383, 0.0936]
-Original Grad: 0.033, -lr * Pred Grad:  0.070, New P: 1.209
-Original Grad: 0.032, -lr * Pred Grad:  -0.042, New P: 0.051
iter 7 loss: 0.294
Actual params: [1.2087, 0.0511]
-Original Grad: 0.025, -lr * Pred Grad:  0.064, New P: 1.273
-Original Grad: 0.022, -lr * Pred Grad:  -0.035, New P: 0.016
iter 8 loss: 0.303
Actual params: [1.2726, 0.0159]
-Original Grad: 0.022, -lr * Pred Grad:  0.058, New P: 1.331
-Original Grad: 0.013, -lr * Pred Grad:  -0.030, New P: -0.014
iter 9 loss: 0.313
Actual params: [ 1.3311, -0.0142]
-Original Grad: 0.018, -lr * Pred Grad:  0.054, New P: 1.385
-Original Grad: 0.011, -lr * Pred Grad:  -0.026, New P: -0.040
iter 10 loss: 0.325
Actual params: [ 1.3846, -0.0401]
-Original Grad: 0.011, -lr * Pred Grad:  0.049, New P: 1.433
-Original Grad: 0.007, -lr * Pred Grad:  -0.022, New P: -0.063
iter 11 loss: 0.332
Actual params: [ 1.4334, -0.0625]
-Original Grad: 0.006, -lr * Pred Grad:  0.044, New P: 1.478
-Original Grad: 0.004, -lr * Pred Grad:  -0.020, New P: -0.082
iter 12 loss: 0.336
Actual params: [ 1.4778, -0.0824]
-Original Grad: 0.005, -lr * Pred Grad:  0.040, New P: 1.518
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.100
iter 13 loss: 0.344
Actual params: [ 1.5182, -0.1003]
-Original Grad: -0.000, -lr * Pred Grad:  0.036, New P: 1.555
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.117
iter 14 loss: 0.351
Actual params: [ 1.5547, -0.1167]
-Original Grad: -0.001, -lr * Pred Grad:  0.033, New P: 1.588
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.131
iter 15 loss: 0.357
Actual params: [ 1.5876, -0.1315]
-Original Grad: -0.005, -lr * Pred Grad:  0.029, New P: 1.617
-Original Grad: -0.005, -lr * Pred Grad:  -0.014, New P: -0.145
iter 16 loss: 0.360
Actual params: [ 1.617 , -0.1454]
-Original Grad: -0.010, -lr * Pred Grad:  0.026, New P: 1.643
-Original Grad: -0.007, -lr * Pred Grad:  -0.013, New P: -0.159
iter 17 loss: 0.359
Actual params: [ 1.6429, -0.1586]
-Original Grad: -0.015, -lr * Pred Grad:  0.022, New P: 1.665
-Original Grad: -0.015, -lr * Pred Grad:  -0.013, New P: -0.172
iter 18 loss: 0.360
Actual params: [ 1.6652, -0.172 ]
-Original Grad: -0.018, -lr * Pred Grad:  0.019, New P: 1.684
-Original Grad: -0.002, -lr * Pred Grad:  -0.012, New P: -0.184
iter 19 loss: 0.360
Actual params: [ 1.684 , -0.1844]
-Original Grad: -0.024, -lr * Pred Grad:  0.015, New P: 1.699
-Original Grad: -0.012, -lr * Pred Grad:  -0.012, New P: -0.197
iter 20 loss: 0.359
Actual params: [ 1.6991, -0.1967]
-Original Grad: -0.035, -lr * Pred Grad:  0.011, New P: 1.710
-Original Grad: -0.017, -lr * Pred Grad:  -0.013, New P: -0.210
Target params: [1.1812, 0.2779]
iter 0 loss: 0.386
Actual params: [0.5941, 0.5941]
-Original Grad: 0.090, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.242, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.322
Actual params: [0.6941, 0.4941]
-Original Grad: 0.025, -lr * Pred Grad:  0.085, New P: 0.779
-Original Grad: -0.167, -lr * Pred Grad:  -0.097, New P: 0.397
iter 2 loss: 0.288
Actual params: [0.7788, 0.3966]
-Original Grad: 0.016, -lr * Pred Grad:  0.075, New P: 0.854
-Original Grad: -0.085, -lr * Pred Grad:  -0.090, New P: 0.307
iter 3 loss: 0.285
Actual params: [0.8542, 0.3065]
-Original Grad: 0.008, -lr * Pred Grad:  0.066, New P: 0.921
-Original Grad: -0.024, -lr * Pred Grad:  -0.078, New P: 0.228
iter 4 loss: 0.285
Actual params: [0.9206, 0.2285]
-Original Grad: 0.019, -lr * Pred Grad:  0.066, New P: 0.986
-Original Grad: -0.008, -lr * Pred Grad:  -0.067, New P: 0.161
iter 5 loss: 0.284
Actual params: [0.9864, 0.1611]
-Original Grad: -0.002, -lr * Pred Grad:  0.055, New P: 1.042
-Original Grad: 0.000, -lr * Pred Grad:  -0.058, New P: 0.103
iter 6 loss: 0.282
Actual params: [1.0418, 0.1031]
-Original Grad: -0.004, -lr * Pred Grad:  0.046, New P: 1.088
-Original Grad: 0.007, -lr * Pred Grad:  -0.050, New P: 0.054
iter 7 loss: 0.282
Actual params: [1.0881, 0.0535]
-Original Grad: -0.011, -lr * Pred Grad:  0.035, New P: 1.123
-Original Grad: 0.013, -lr * Pred Grad:  -0.042, New P: 0.012
iter 8 loss: 0.284
Actual params: [1.1231, 0.0119]
-Original Grad: -0.003, -lr * Pred Grad:  0.030, New P: 1.153
-Original Grad: 0.012, -lr * Pred Grad:  -0.035, New P: -0.023
iter 9 loss: 0.285
Actual params: [ 1.1527, -0.023 ]
-Original Grad: -0.006, -lr * Pred Grad:  0.023, New P: 1.176
-Original Grad: 0.016, -lr * Pred Grad:  -0.029, New P: -0.052
iter 10 loss: 0.287
Actual params: [ 1.1757, -0.0516]
-Original Grad: 0.005, -lr * Pred Grad:  0.023, New P: 1.199
-Original Grad: 0.020, -lr * Pred Grad:  -0.022, New P: -0.074
iter 11 loss: 0.288
Actual params: [ 1.1987, -0.074 ]
-Original Grad: -0.006, -lr * Pred Grad:  0.018, New P: 1.216
-Original Grad: 0.017, -lr * Pred Grad:  -0.017, New P: -0.091
iter 12 loss: 0.289
Actual params: [ 1.2162, -0.0914]
-Original Grad: -0.003, -lr * Pred Grad:  0.015, New P: 1.231
-Original Grad: 0.032, -lr * Pred Grad:  -0.011, New P: -0.102
iter 13 loss: 0.291
Actual params: [ 1.2308, -0.102 ]
-Original Grad: -0.005, -lr * Pred Grad:  0.011, New P: 1.241
-Original Grad: 0.029, -lr * Pred Grad:  -0.005, New P: -0.107
iter 14 loss: 0.291
Actual params: [ 1.2415, -0.107 ]
-Original Grad: -0.001, -lr * Pred Grad:  0.009, New P: 1.251
-Original Grad: 0.027, -lr * Pred Grad:  -0.000, New P: -0.107
iter 15 loss: 0.292
Actual params: [ 1.2505, -0.1074]
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 1.259
-Original Grad: 0.041, -lr * Pred Grad:  0.006, New P: -0.101
iter 16 loss: 0.291
Actual params: [ 1.2589, -0.1014]
-Original Grad: -0.005, -lr * Pred Grad:  0.005, New P: 1.264
-Original Grad: 0.029, -lr * Pred Grad:  0.010, New P: -0.091
iter 17 loss: 0.291
Actual params: [ 1.2638, -0.0915]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 1.269
-Original Grad: 0.032, -lr * Pred Grad:  0.014, New P: -0.077
iter 18 loss: 0.290
Actual params: [ 1.2685, -0.0774]
-Original Grad: -0.008, -lr * Pred Grad:  0.000, New P: 1.269
-Original Grad: 0.018, -lr * Pred Grad:  0.016, New P: -0.062
iter 19 loss: 0.289
Actual params: [ 1.2688, -0.0619]
-Original Grad: -0.007, -lr * Pred Grad:  -0.003, New P: 1.265
-Original Grad: 0.023, -lr * Pred Grad:  0.018, New P: -0.044
iter 20 loss: 0.288
Actual params: [ 1.2653, -0.0441]
-Original Grad: -0.004, -lr * Pred Grad:  -0.005, New P: 1.260
-Original Grad: 0.017, -lr * Pred Grad:  0.019, New P: -0.025
Target params: [1.1812, 0.2779]
iter 0 loss: 0.519
Actual params: [0.5941, 0.5941]
-Original Grad: 0.119, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.227, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.508
Actual params: [0.6941, 0.4941]
-Original Grad: 0.173, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.026, -lr * Pred Grad:  -0.075, New P: 0.419
iter 2 loss: 0.500
Actual params: [0.7933, 0.4191]
-Original Grad: 0.061, -lr * Pred Grad:  0.092, New P: 0.885
-Original Grad: 0.016, -lr * Pred Grad:  -0.053, New P: 0.366
iter 3 loss: 0.497
Actual params: [0.8849, 0.3656]
-Original Grad: -0.028, -lr * Pred Grad:  0.067, New P: 0.952
-Original Grad: -0.027, -lr * Pred Grad:  -0.050, New P: 0.315
iter 4 loss: 0.487
Actual params: [0.9518, 0.3153]
-Original Grad: -0.020, -lr * Pred Grad:  0.051, New P: 1.003
-Original Grad: -0.012, -lr * Pred Grad:  -0.045, New P: 0.270
iter 5 loss: 0.477
Actual params: [1.003 , 0.2698]
-Original Grad: -0.007, -lr * Pred Grad:  0.043, New P: 1.046
-Original Grad: 0.022, -lr * Pred Grad:  -0.034, New P: 0.236
iter 6 loss: 0.471
Actual params: [1.0456, 0.2358]
-Original Grad: 0.001, -lr * Pred Grad:  0.037, New P: 1.083
-Original Grad: 0.012, -lr * Pred Grad:  -0.027, New P: 0.209
iter 7 loss: 0.466
Actual params: [1.083 , 0.2087]
-Original Grad: 0.015, -lr * Pred Grad:  0.036, New P: 1.119
-Original Grad: 0.018, -lr * Pred Grad:  -0.020, New P: 0.189
iter 8 loss: 0.463
Actual params: [1.1192, 0.1887]
-Original Grad: 0.018, -lr * Pred Grad:  0.036, New P: 1.155
-Original Grad: 0.021, -lr * Pred Grad:  -0.013, New P: 0.175
iter 9 loss: 0.465
Actual params: [1.1554, 0.1754]
-Original Grad: 0.023, -lr * Pred Grad:  0.037, New P: 1.192
-Original Grad: 0.032, -lr * Pred Grad:  -0.005, New P: 0.170
iter 10 loss: 0.470
Actual params: [1.1925, 0.1701]
-Original Grad: 0.021, -lr * Pred Grad:  0.038, New P: 1.230
-Original Grad: 0.028, -lr * Pred Grad:  0.001, New P: 0.171
iter 11 loss: 0.472
Actual params: [1.2301, 0.1711]
-Original Grad: 0.007, -lr * Pred Grad:  0.035, New P: 1.265
-Original Grad: 0.044, -lr * Pred Grad:  0.010, New P: 0.181
iter 12 loss: 0.478
Actual params: [1.2654, 0.1809]
-Original Grad: 0.015, -lr * Pred Grad:  0.035, New P: 1.300
-Original Grad: 0.033, -lr * Pred Grad:  0.015, New P: 0.196
iter 13 loss: 0.487
Actual params: [1.3004, 0.1962]
-Original Grad: -0.008, -lr * Pred Grad:  0.030, New P: 1.330
-Original Grad: 0.040, -lr * Pred Grad:  0.022, New P: 0.218
iter 14 loss: 0.493
Actual params: [1.3304, 0.2177]
-Original Grad: -0.006, -lr * Pred Grad:  0.026, New P: 1.356
-Original Grad: 0.032, -lr * Pred Grad:  0.026, New P: 0.243
iter 15 loss: 0.497
Actual params: [1.3561, 0.2433]
-Original Grad: -0.004, -lr * Pred Grad:  0.022, New P: 1.378
-Original Grad: 0.018, -lr * Pred Grad:  0.027, New P: 0.270
iter 16 loss: 0.503
Actual params: [1.3784, 0.2699]
-Original Grad: -0.003, -lr * Pred Grad:  0.020, New P: 1.398
-Original Grad: 0.006, -lr * Pred Grad:  0.025, New P: 0.295
iter 17 loss: 0.508
Actual params: [1.398 , 0.2952]
-Original Grad: -0.013, -lr * Pred Grad:  0.015, New P: 1.413
-Original Grad: 0.005, -lr * Pred Grad:  0.024, New P: 0.319
iter 18 loss: 0.511
Actual params: [1.4129, 0.3192]
-Original Grad: -0.014, -lr * Pred Grad:  0.010, New P: 1.423
-Original Grad: 0.001, -lr * Pred Grad:  0.022, New P: 0.341
iter 19 loss: 0.512
Actual params: [1.4232, 0.3413]
-Original Grad: -0.010, -lr * Pred Grad:  0.007, New P: 1.430
-Original Grad: -0.020, -lr * Pred Grad:  0.016, New P: 0.357
iter 20 loss: 0.513
Actual params: [1.4303, 0.3571]
-Original Grad: -0.003, -lr * Pred Grad:  0.006, New P: 1.436
-Original Grad: -0.032, -lr * Pred Grad:  0.008, New P: 0.365
Target params: [1.1812, 0.2779]
iter 0 loss: 0.594
Actual params: [0.5941, 0.5941]
-Original Grad: 0.371, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.147, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.574
Actual params: [0.6941, 0.4941]
-Original Grad: 0.235, -lr * Pred Grad:  0.096, New P: 0.791
-Original Grad: -0.218, -lr * Pred Grad:  -0.099, New P: 0.395
iter 2 loss: 0.485
Actual params: [0.7905, 0.3949]
-Original Grad: 0.184, -lr * Pred Grad:  0.093, New P: 0.884
-Original Grad: -0.102, -lr * Pred Grad:  -0.095, New P: 0.300
iter 3 loss: 0.392
Actual params: [0.884 , 0.3004]
-Original Grad: 0.004, -lr * Pred Grad:  0.077, New P: 0.961
-Original Grad: -0.008, -lr * Pred Grad:  -0.079, New P: 0.221
iter 4 loss: 0.384
Actual params: [0.961 , 0.2214]
-Original Grad: -0.025, -lr * Pred Grad:  0.062, New P: 1.023
-Original Grad: 0.012, -lr * Pred Grad:  -0.065, New P: 0.157
iter 5 loss: 0.411
Actual params: [1.0232, 0.1569]
-Original Grad: -0.057, -lr * Pred Grad:  0.047, New P: 1.070
-Original Grad: 0.065, -lr * Pred Grad:  -0.043, New P: 0.114
iter 6 loss: 0.437
Actual params: [1.0703, 0.1143]
-Original Grad: -0.073, -lr * Pred Grad:  0.033, New P: 1.103
-Original Grad: 0.067, -lr * Pred Grad:  -0.025, New P: 0.090
iter 7 loss: 0.460
Actual params: [1.1033, 0.0896]
-Original Grad: -0.093, -lr * Pred Grad:  0.019, New P: 1.123
-Original Grad: 0.087, -lr * Pred Grad:  -0.007, New P: 0.083
iter 8 loss: 0.470
Actual params: [1.1226, 0.0826]
-Original Grad: -0.106, -lr * Pred Grad:  0.006, New P: 1.129
-Original Grad: 0.120, -lr * Pred Grad:  0.012, New P: 0.094
iter 9 loss: 0.471
Actual params: [1.1291, 0.0945]
-Original Grad: -0.105, -lr * Pred Grad:  -0.004, New P: 1.125
-Original Grad: 0.120, -lr * Pred Grad:  0.026, New P: 0.121
iter 10 loss: 0.464
Actual params: [1.1249, 0.1209]
-Original Grad: -0.077, -lr * Pred Grad:  -0.011, New P: 1.114
-Original Grad: 0.102, -lr * Pred Grad:  0.036, New P: 0.157
iter 11 loss: 0.454
Actual params: [1.114 , 0.1571]
-Original Grad: -0.070, -lr * Pred Grad:  -0.016, New P: 1.098
-Original Grad: 0.088, -lr * Pred Grad:  0.043, New P: 0.200
iter 12 loss: 0.441
Actual params: [1.0978, 0.2   ]
-Original Grad: -0.060, -lr * Pred Grad:  -0.020, New P: 1.078
-Original Grad: 0.056, -lr * Pred Grad:  0.045, New P: 0.245
iter 13 loss: 0.426
Actual params: [1.0779, 0.2452]
-Original Grad: -0.051, -lr * Pred Grad:  -0.023, New P: 1.055
-Original Grad: 0.033, -lr * Pred Grad:  0.045, New P: 0.290
iter 14 loss: 0.405
Actual params: [1.0553, 0.2902]
-Original Grad: -0.034, -lr * Pred Grad:  -0.024, New P: 1.032
-Original Grad: -0.010, -lr * Pred Grad:  0.039, New P: 0.330
iter 15 loss: 0.393
Actual params: [1.0317, 0.3296]
-Original Grad: -0.014, -lr * Pred Grad:  -0.023, New P: 1.009
-Original Grad: -0.028, -lr * Pred Grad:  0.032, New P: 0.362
iter 16 loss: 0.385
Actual params: [1.0091, 0.3617]
-Original Grad: -0.009, -lr * Pred Grad:  -0.021, New P: 0.988
-Original Grad: -0.049, -lr * Pred Grad:  0.023, New P: 0.384
iter 17 loss: 0.379
Actual params: [0.9878, 0.3844]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: 0.968
-Original Grad: -0.048, -lr * Pred Grad:  0.014, New P: 0.399
iter 18 loss: 0.371
Actual params: [0.9683, 0.3986]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: 0.951
-Original Grad: -0.042, -lr * Pred Grad:  0.007, New P: 0.406
iter 19 loss: 0.373
Actual params: [0.9505, 0.4061]
-Original Grad: 0.006, -lr * Pred Grad:  -0.016, New P: 0.935
-Original Grad: -0.083, -lr * Pred Grad:  -0.004, New P: 0.402
iter 20 loss: 0.374
Actual params: [0.9349, 0.4021]
-Original Grad: 0.008, -lr * Pred Grad:  -0.013, New P: 0.921
-Original Grad: -0.107, -lr * Pred Grad:  -0.017, New P: 0.385
Target params: [1.1812, 0.2779]
iter 0 loss: 0.818
Actual params: [0.5941, 0.5941]
-Original Grad: 0.451, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.136, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.728
Actual params: [0.6941, 0.4941]
-Original Grad: 0.517, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.056, -lr * Pred Grad:  -0.090, New P: 0.404
iter 2 loss: 0.605
Actual params: [0.7942, 0.4037]
-Original Grad: 0.449, -lr * Pred Grad:  0.100, New P: 0.894
-Original Grad: 0.030, -lr * Pred Grad:  -0.056, New P: 0.348
iter 3 loss: 0.445
Actual params: [0.8939, 0.3479]
-Original Grad: 0.334, -lr * Pred Grad:  0.098, New P: 0.992
-Original Grad: 0.078, -lr * Pred Grad:  -0.014, New P: 0.334
iter 4 loss: 0.328
Actual params: [0.9915, 0.3342]
-Original Grad: -0.075, -lr * Pred Grad:  0.078, New P: 1.069
-Original Grad: 0.039, -lr * Pred Grad:  0.001, New P: 0.335
iter 5 loss: 0.305
Actual params: [1.0691, 0.335 ]
-Original Grad: -0.066, -lr * Pred Grad:  0.063, New P: 1.132
-Original Grad: 0.054, -lr * Pred Grad:  0.016, New P: 0.351
iter 6 loss: 0.328
Actual params: [1.1319, 0.3512]
-Original Grad: -0.013, -lr * Pred Grad:  0.054, New P: 1.186
-Original Grad: 0.053, -lr * Pred Grad:  0.028, New P: 0.379
iter 7 loss: 0.346
Actual params: [1.186, 0.379]
-Original Grad: -0.030, -lr * Pred Grad:  0.046, New P: 1.232
-Original Grad: 0.062, -lr * Pred Grad:  0.039, New P: 0.418
iter 8 loss: 0.358
Actual params: [1.232 , 0.4176]
-Original Grad: -0.059, -lr * Pred Grad:  0.037, New P: 1.270
-Original Grad: 0.076, -lr * Pred Grad:  0.050, New P: 0.467
iter 9 loss: 0.362
Actual params: [1.2695, 0.4671]
-Original Grad: -0.046, -lr * Pred Grad:  0.031, New P: 1.300
-Original Grad: 0.009, -lr * Pred Grad:  0.046, New P: 0.513
iter 10 loss: 0.364
Actual params: [1.3005, 0.5134]
-Original Grad: -0.048, -lr * Pred Grad:  0.025, New P: 1.326
-Original Grad: -0.031, -lr * Pred Grad:  0.034, New P: 0.547
iter 11 loss: 0.367
Actual params: [1.3256, 0.5474]
-Original Grad: -0.011, -lr * Pred Grad:  0.022, New P: 1.348
-Original Grad: -0.058, -lr * Pred Grad:  0.017, New P: 0.564
iter 12 loss: 0.378
Actual params: [1.3476, 0.5645]
-Original Grad: -0.043, -lr * Pred Grad:  0.017, New P: 1.365
-Original Grad: -0.028, -lr * Pred Grad:  0.009, New P: 0.574
iter 13 loss: 0.388
Actual params: [1.365 , 0.5737]
-Original Grad: -0.071, -lr * Pred Grad:  0.012, New P: 1.377
-Original Grad: -0.022, -lr * Pred Grad:  0.004, New P: 0.577
iter 14 loss: 0.410
Actual params: [1.377 , 0.5772]
-Original Grad: -0.032, -lr * Pred Grad:  0.009, New P: 1.386
-Original Grad: -0.013, -lr * Pred Grad:  0.000, New P: 0.578
iter 15 loss: 0.430
Actual params: [1.386 , 0.5776]
-Original Grad: -0.049, -lr * Pred Grad:  0.006, New P: 1.391
-Original Grad: -0.025, -lr * Pred Grad:  -0.005, New P: 0.573
iter 16 loss: 0.447
Actual params: [1.3915, 0.5727]
-Original Grad: -0.031, -lr * Pred Grad:  0.003, New P: 1.395
-Original Grad: -0.046, -lr * Pred Grad:  -0.014, New P: 0.558
iter 17 loss: 0.458
Actual params: [1.3948, 0.5584]
-Original Grad: -0.044, -lr * Pred Grad:  0.000, New P: 1.395
-Original Grad: -0.030, -lr * Pred Grad:  -0.019, New P: 0.539
iter 18 loss: 0.460
Actual params: [1.3953, 0.5393]
-Original Grad: -0.050, -lr * Pred Grad:  -0.002, New P: 1.393
-Original Grad: -0.008, -lr * Pred Grad:  -0.019, New P: 0.520
iter 19 loss: 0.457
Actual params: [1.3929, 0.5201]
-Original Grad: -0.063, -lr * Pred Grad:  -0.006, New P: 1.387
-Original Grad: 0.016, -lr * Pred Grad:  -0.014, New P: 0.506
iter 20 loss: 0.450
Actual params: [1.3873, 0.5061]
-Original Grad: -0.056, -lr * Pred Grad:  -0.008, New P: 1.379
-Original Grad: 0.022, -lr * Pred Grad:  -0.008, New P: 0.498
Target params: [1.1812, 0.2779]
iter 0 loss: 0.555
Actual params: [0.5941, 0.5941]
-Original Grad: 0.139, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.318, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.437
Actual params: [0.6941, 0.4941]
-Original Grad: 0.064, -lr * Pred Grad:  0.092, New P: 0.786
-Original Grad: -0.129, -lr * Pred Grad:  -0.090, New P: 0.404
iter 2 loss: 0.356
Actual params: [0.786, 0.404]
-Original Grad: 0.033, -lr * Pred Grad:  0.083, New P: 0.869
-Original Grad: -0.027, -lr * Pred Grad:  -0.074, New P: 0.330
iter 3 loss: 0.315
Actual params: [0.8689, 0.3296]
-Original Grad: 0.004, -lr * Pred Grad:  0.069, New P: 0.938
-Original Grad: 0.010, -lr * Pred Grad:  -0.059, New P: 0.270
iter 4 loss: 0.297
Actual params: [0.9384, 0.2703]
-Original Grad: 0.000, -lr * Pred Grad:  0.059, New P: 0.997
-Original Grad: 0.051, -lr * Pred Grad:  -0.042, New P: 0.229
iter 5 loss: 0.289
Actual params: [0.9972, 0.2288]
-Original Grad: 0.006, -lr * Pred Grad:  0.053, New P: 1.050
-Original Grad: 0.048, -lr * Pred Grad:  -0.028, New P: 0.200
iter 6 loss: 0.287
Actual params: [1.0497, 0.2005]
-Original Grad: 0.005, -lr * Pred Grad:  0.047, New P: 1.097
-Original Grad: 0.033, -lr * Pred Grad:  -0.020, New P: 0.181
iter 7 loss: 0.287
Actual params: [1.097 , 0.1807]
-Original Grad: 0.005, -lr * Pred Grad:  0.043, New P: 1.140
-Original Grad: 0.026, -lr * Pred Grad:  -0.014, New P: 0.167
iter 8 loss: 0.288
Actual params: [1.1405, 0.167 ]
-Original Grad: 0.006, -lr * Pred Grad:  0.040, New P: 1.181
-Original Grad: 0.018, -lr * Pred Grad:  -0.010, New P: 0.157
iter 9 loss: 0.289
Actual params: [1.1808, 0.1573]
-Original Grad: 0.008, -lr * Pred Grad:  0.038, New P: 1.219
-Original Grad: 0.012, -lr * Pred Grad:  -0.007, New P: 0.150
iter 10 loss: 0.291
Actual params: [1.2192, 0.1504]
-Original Grad: 0.005, -lr * Pred Grad:  0.036, New P: 1.255
-Original Grad: -0.005, -lr * Pred Grad:  -0.007, New P: 0.144
iter 11 loss: 0.293
Actual params: [1.255 , 0.1436]
-Original Grad: 0.011, -lr * Pred Grad:  0.036, New P: 1.291
-Original Grad: 0.008, -lr * Pred Grad:  -0.005, New P: 0.139
iter 12 loss: 0.294
Actual params: [1.2906, 0.1386]
-Original Grad: 0.005, -lr * Pred Grad:  0.034, New P: 1.324
-Original Grad: 0.005, -lr * Pred Grad:  -0.004, New P: 0.135
iter 13 loss: 0.296
Actual params: [1.3243, 0.1347]
-Original Grad: 0.006, -lr * Pred Grad:  0.032, New P: 1.356
-Original Grad: 0.009, -lr * Pred Grad:  -0.002, New P: 0.132
iter 14 loss: 0.299
Actual params: [1.3564, 0.1324]
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: 1.386
-Original Grad: -0.003, -lr * Pred Grad:  -0.003, New P: 0.130
iter 15 loss: 0.301
Actual params: [1.3857, 0.1298]
-Original Grad: 0.004, -lr * Pred Grad:  0.028, New P: 1.413
-Original Grad: 0.002, -lr * Pred Grad:  -0.002, New P: 0.128
iter 16 loss: 0.303
Actual params: [1.4133, 0.1277]
-Original Grad: 0.001, -lr * Pred Grad:  0.025, New P: 1.438
-Original Grad: -0.003, -lr * Pred Grad:  -0.002, New P: 0.125
iter 17 loss: 0.304
Actual params: [1.4385, 0.1255]
-Original Grad: -0.003, -lr * Pred Grad:  0.022, New P: 1.460
-Original Grad: 0.001, -lr * Pred Grad:  -0.002, New P: 0.124
iter 18 loss: 0.305
Actual params: [1.4604, 0.1236]
-Original Grad: 0.001, -lr * Pred Grad:  0.020, New P: 1.481
-Original Grad: 0.003, -lr * Pred Grad:  -0.001, New P: 0.122
iter 19 loss: 0.306
Actual params: [1.4805, 0.1224]
-Original Grad: -0.006, -lr * Pred Grad:  0.017, New P: 1.497
-Original Grad: -0.009, -lr * Pred Grad:  -0.002, New P: 0.120
iter 20 loss: 0.305
Actual params: [1.497, 0.12 ]
-Original Grad: -0.002, -lr * Pred Grad:  0.014, New P: 1.511
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: 0.118
Target params: [1.1812, 0.2779]
iter 0 loss: 0.660
Actual params: [0.5941, 0.5941]
-Original Grad: 0.602, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.171, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.648
Actual params: [0.6941, 0.4941]
-Original Grad: 0.212, -lr * Pred Grad:  0.088, New P: 0.782
-Original Grad: -0.122, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.625
Actual params: [0.782 , 0.3963]
-Original Grad: 0.087, -lr * Pred Grad:  0.076, New P: 0.858
-Original Grad: -0.044, -lr * Pred Grad:  -0.087, New P: 0.309
iter 3 loss: 0.607
Actual params: [0.858 , 0.3092]
-Original Grad: 0.019, -lr * Pred Grad:  0.064, New P: 0.922
-Original Grad: -0.013, -lr * Pred Grad:  -0.075, New P: 0.234
iter 4 loss: 0.596
Actual params: [0.9219, 0.2345]
-Original Grad: 0.003, -lr * Pred Grad:  0.054, New P: 0.976
-Original Grad: 0.003, -lr * Pred Grad:  -0.062, New P: 0.172
iter 5 loss: 0.600
Actual params: [0.9762, 0.1722]
-Original Grad: -0.004, -lr * Pred Grad:  0.046, New P: 1.023
-Original Grad: 0.006, -lr * Pred Grad:  -0.052, New P: 0.120
iter 6 loss: 0.603
Actual params: [1.0227, 0.1199]
-Original Grad: -0.005, -lr * Pred Grad:  0.040, New P: 1.063
-Original Grad: 0.017, -lr * Pred Grad:  -0.042, New P: 0.078
iter 7 loss: 0.603
Actual params: [1.063 , 0.0784]
-Original Grad: -0.006, -lr * Pred Grad:  0.035, New P: 1.098
-Original Grad: 0.040, -lr * Pred Grad:  -0.027, New P: 0.051
iter 8 loss: 0.603
Actual params: [1.098 , 0.0512]
-Original Grad: -0.010, -lr * Pred Grad:  0.030, New P: 1.128
-Original Grad: 0.029, -lr * Pred Grad:  -0.017, New P: 0.034
iter 9 loss: 0.606
Actual params: [1.1283, 0.0338]
-Original Grad: -0.007, -lr * Pred Grad:  0.027, New P: 1.155
-Original Grad: 0.015, -lr * Pred Grad:  -0.012, New P: 0.022
iter 10 loss: 0.608
Actual params: [1.1549, 0.0215]
-Original Grad: -0.007, -lr * Pred Grad:  0.023, New P: 1.178
-Original Grad: 0.010, -lr * Pred Grad:  -0.009, New P: 0.013
iter 11 loss: 0.611
Actual params: [1.1782, 0.0128]
-Original Grad: -0.011, -lr * Pred Grad:  0.020, New P: 1.198
-Original Grad: 0.029, -lr * Pred Grad:  -0.001, New P: 0.011
iter 12 loss: 0.613
Actual params: [1.1983, 0.0113]
-Original Grad: -0.010, -lr * Pred Grad:  0.017, New P: 1.216
-Original Grad: 0.016, -lr * Pred Grad:  0.002, New P: 0.013
iter 13 loss: 0.616
Actual params: [1.2157, 0.0135]
-Original Grad: -0.016, -lr * Pred Grad:  0.015, New P: 1.230
-Original Grad: 0.014, -lr * Pred Grad:  0.005, New P: 0.018
iter 14 loss: 0.618
Actual params: [1.2302, 0.0183]
-Original Grad: -0.015, -lr * Pred Grad:  0.012, New P: 1.242
-Original Grad: -0.003, -lr * Pred Grad:  0.004, New P: 0.022
iter 15 loss: 0.619
Actual params: [1.2422, 0.022 ]
-Original Grad: -0.014, -lr * Pred Grad:  0.010, New P: 1.252
-Original Grad: 0.021, -lr * Pred Grad:  0.008, New P: 0.030
iter 16 loss: 0.621
Actual params: [1.252 , 0.0299]
-Original Grad: -0.016, -lr * Pred Grad:  0.008, New P: 1.260
-Original Grad: 0.006, -lr * Pred Grad:  0.008, New P: 0.038
iter 17 loss: 0.622
Actual params: [1.2596, 0.0382]
-Original Grad: -0.013, -lr * Pred Grad:  0.006, New P: 1.266
-Original Grad: 0.032, -lr * Pred Grad:  0.015, New P: 0.053
iter 18 loss: 0.624
Actual params: [1.2655, 0.0527]
-Original Grad: -0.015, -lr * Pred Grad:  0.004, New P: 1.270
-Original Grad: 0.025, -lr * Pred Grad:  0.019, New P: 0.071
iter 19 loss: 0.625
Actual params: [1.2698, 0.0713]
-Original Grad: -0.014, -lr * Pred Grad:  0.003, New P: 1.273
-Original Grad: 0.018, -lr * Pred Grad:  0.021, New P: 0.092
iter 20 loss: 0.627
Actual params: [1.2725, 0.0922]
-Original Grad: -0.019, -lr * Pred Grad:  0.001, New P: 1.273
-Original Grad: 0.015, -lr * Pred Grad:  0.022, New P: 0.114
Target params: [1.1812, 0.2779]
iter 0 loss: 0.369
Actual params: [0.5941, 0.5941]
-Original Grad: -0.036, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.087, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.318
Actual params: [0.4941, 0.4941]
-Original Grad: -0.030, -lr * Pred Grad:  -0.099, New P: 0.395
-Original Grad: -0.071, -lr * Pred Grad:  -0.099, New P: 0.395
iter 2 loss: 0.281
Actual params: [0.3949, 0.3951]
-Original Grad: -0.037, -lr * Pred Grad:  -0.100, New P: 0.295
-Original Grad: -0.056, -lr * Pred Grad:  -0.097, New P: 0.298
iter 3 loss: 0.257
Actual params: [0.2952, 0.2981]
-Original Grad: -0.024, -lr * Pred Grad:  -0.098, New P: 0.198
-Original Grad: -0.051, -lr * Pred Grad:  -0.095, New P: 0.203
iter 4 loss: 0.239
Actual params: [0.1977, 0.2026]
-Original Grad: -0.025, -lr * Pred Grad:  -0.097, New P: 0.101
-Original Grad: -0.032, -lr * Pred Grad:  -0.091, New P: 0.112
iter 5 loss: 0.231
Actual params: [0.1012, 0.1116]
-Original Grad: -0.003, -lr * Pred Grad:  -0.085, New P: 0.016
-Original Grad: -0.019, -lr * Pred Grad:  -0.085, New P: 0.027
iter 6 loss: 0.229
Actual params: [0.0159, 0.0267]
-Original Grad: 0.000, -lr * Pred Grad:  -0.075, New P: -0.059
-Original Grad: -0.010, -lr * Pred Grad:  -0.077, New P: -0.051
iter 7 loss: 0.230
Actual params: [-0.0587, -0.0508]
-Original Grad: 0.003, -lr * Pred Grad:  -0.064, New P: -0.122
-Original Grad: -0.008, -lr * Pred Grad:  -0.071, New P: -0.122
iter 8 loss: 0.232
Actual params: [-0.1222, -0.1216]
-Original Grad: 0.006, -lr * Pred Grad:  -0.052, New P: -0.174
-Original Grad: -0.005, -lr * Pred Grad:  -0.065, New P: -0.186
iter 9 loss: 0.233
Actual params: [-0.1741, -0.1864]
-Original Grad: 0.005, -lr * Pred Grad:  -0.043, New P: -0.217
-Original Grad: -0.006, -lr * Pred Grad:  -0.060, New P: -0.246
iter 10 loss: 0.235
Actual params: [-0.2171, -0.2461]
-Original Grad: 0.006, -lr * Pred Grad:  -0.034, New P: -0.251
-Original Grad: -0.006, -lr * Pred Grad:  -0.056, New P: -0.302
iter 11 loss: 0.236
Actual params: [-0.2514, -0.3017]
-Original Grad: 0.005, -lr * Pred Grad:  -0.027, New P: -0.279
-Original Grad: -0.005, -lr * Pred Grad:  -0.052, New P: -0.353
iter 12 loss: 0.238
Actual params: [-0.2789, -0.3533]
-Original Grad: 0.007, -lr * Pred Grad:  -0.019, New P: -0.298
-Original Grad: -0.004, -lr * Pred Grad:  -0.048, New P: -0.401
iter 13 loss: 0.239
Actual params: [-0.2983, -0.4011]
-Original Grad: 0.009, -lr * Pred Grad:  -0.011, New P: -0.309
-Original Grad: -0.003, -lr * Pred Grad:  -0.044, New P: -0.445
iter 14 loss: 0.240
Actual params: [-0.3092, -0.4455]
-Original Grad: 0.009, -lr * Pred Grad:  -0.003, New P: -0.313
-Original Grad: -0.002, -lr * Pred Grad:  -0.041, New P: -0.486
iter 15 loss: 0.241
Actual params: [-0.3126, -0.4864]
-Original Grad: 0.009, -lr * Pred Grad:  0.003, New P: -0.310
-Original Grad: -0.002, -lr * Pred Grad:  -0.038, New P: -0.524
iter 16 loss: 0.241
Actual params: [-0.3096, -0.5242]
-Original Grad: 0.008, -lr * Pred Grad:  0.009, New P: -0.301
-Original Grad: -0.002, -lr * Pred Grad:  -0.035, New P: -0.559
iter 17 loss: 0.240
Actual params: [-0.301 , -0.5591]
-Original Grad: 0.009, -lr * Pred Grad:  0.014, New P: -0.287
-Original Grad: -0.005, -lr * Pred Grad:  -0.033, New P: -0.593
iter 18 loss: 0.239
Actual params: [-0.2871, -0.5926]
-Original Grad: 0.009, -lr * Pred Grad:  0.018, New P: -0.269
-Original Grad: -0.004, -lr * Pred Grad:  -0.032, New P: -0.625
iter 19 loss: 0.238
Actual params: [-0.2686, -0.6245]
-Original Grad: 0.008, -lr * Pred Grad:  0.022, New P: -0.247
-Original Grad: -0.003, -lr * Pred Grad:  -0.030, New P: -0.655
iter 20 loss: 0.237
Actual params: [-0.2465, -0.6545]
-Original Grad: 0.008, -lr * Pred Grad:  0.025, New P: -0.221
-Original Grad: -0.004, -lr * Pred Grad:  -0.029, New P: -0.683
Target params: [1.1812, 0.2779]
iter 0 loss: 0.416
Actual params: [0.5941, 0.5941]
-Original Grad: 0.270, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.347
Actual params: [0.6941, 0.4941]
-Original Grad: -0.043, -lr * Pred Grad:  0.055, New P: 0.749
-Original Grad: -0.006, -lr * Pred Grad:  -0.086, New P: 0.408
iter 2 loss: 0.331
Actual params: [0.7486, 0.4085]
-Original Grad: -0.048, -lr * Pred Grad:  0.031, New P: 0.779
-Original Grad: -0.018, -lr * Pred Grad:  -0.081, New P: 0.328
iter 3 loss: 0.328
Actual params: [0.7792, 0.3276]
-Original Grad: -0.016, -lr * Pred Grad:  0.022, New P: 0.801
-Original Grad: -0.029, -lr * Pred Grad:  -0.085, New P: 0.243
iter 4 loss: 0.337
Actual params: [0.801 , 0.2427]
-Original Grad: 0.001, -lr * Pred Grad:  0.019, New P: 0.820
-Original Grad: -0.005, -lr * Pred Grad:  -0.078, New P: 0.164
iter 5 loss: 0.343
Actual params: [0.8197, 0.1644]
-Original Grad: 0.006, -lr * Pred Grad:  0.017, New P: 0.837
-Original Grad: 0.005, -lr * Pred Grad:  -0.060, New P: 0.105
iter 6 loss: 0.342
Actual params: [0.8369, 0.1046]
-Original Grad: 0.013, -lr * Pred Grad:  0.017, New P: 0.854
-Original Grad: 0.045, -lr * Pred Grad:  0.007, New P: 0.112
iter 7 loss: 0.342
Actual params: [0.8542, 0.1118]
-Original Grad: 0.010, -lr * Pred Grad:  0.017, New P: 0.871
-Original Grad: 0.018, -lr * Pred Grad:  0.021, New P: 0.133
iter 8 loss: 0.341
Actual params: [0.8712, 0.133 ]
-Original Grad: 0.009, -lr * Pred Grad:  0.017, New P: 0.888
-Original Grad: 0.043, -lr * Pred Grad:  0.044, New P: 0.177
iter 9 loss: 0.338
Actual params: [0.8879, 0.1769]
-Original Grad: 0.008, -lr * Pred Grad:  0.016, New P: 0.904
-Original Grad: -0.001, -lr * Pred Grad:  0.039, New P: 0.215
iter 10 loss: 0.336
Actual params: [0.9043, 0.2155]
-Original Grad: 0.009, -lr * Pred Grad:  0.016, New P: 0.920
-Original Grad: -0.003, -lr * Pred Grad:  0.032, New P: 0.248
iter 11 loss: 0.333
Actual params: [0.9204, 0.2479]
-Original Grad: 0.008, -lr * Pred Grad:  0.016, New P: 0.936
-Original Grad: -0.005, -lr * Pred Grad:  0.025, New P: 0.273
iter 12 loss: 0.332
Actual params: [0.9364, 0.2733]
-Original Grad: 0.007, -lr * Pred Grad:  0.016, New P: 0.952
-Original Grad: -0.018, -lr * Pred Grad:  0.011, New P: 0.284
iter 13 loss: 0.332
Actual params: [0.9521, 0.2841]
-Original Grad: 0.007, -lr * Pred Grad:  0.015, New P: 0.967
-Original Grad: -0.013, -lr * Pred Grad:  0.001, New P: 0.285
iter 14 loss: 0.334
Actual params: [0.9675, 0.2853]
-Original Grad: 0.003, -lr * Pred Grad:  0.014, New P: 0.982
-Original Grad: -0.008, -lr * Pred Grad:  -0.004, New P: 0.281
iter 15 loss: 0.337
Actual params: [0.982 , 0.2812]
-Original Grad: 0.005, -lr * Pred Grad:  0.014, New P: 0.996
-Original Grad: -0.006, -lr * Pred Grad:  -0.007, New P: 0.274
iter 16 loss: 0.340
Actual params: [0.996, 0.274]
-Original Grad: 0.004, -lr * Pred Grad:  0.013, New P: 1.009
-Original Grad: -0.010, -lr * Pred Grad:  -0.013, New P: 0.261
iter 17 loss: 0.345
Actual params: [1.0095, 0.2611]
-Original Grad: 0.002, -lr * Pred Grad:  0.013, New P: 1.022
-Original Grad: -0.004, -lr * Pred Grad:  -0.014, New P: 0.247
iter 18 loss: 0.349
Actual params: [1.0221, 0.2468]
-Original Grad: 0.003, -lr * Pred Grad:  0.012, New P: 1.034
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: 0.233
iter 19 loss: 0.353
Actual params: [1.0341, 0.2328]
-Original Grad: 0.007, -lr * Pred Grad:  0.012, New P: 1.046
-Original Grad: -0.007, -lr * Pred Grad:  -0.017, New P: 0.216
iter 20 loss: 0.358
Actual params: [1.0463, 0.2157]
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 1.057
-Original Grad: 0.001, -lr * Pred Grad:  -0.015, New P: 0.201
Target params: [1.1812, 0.2779]
iter 0 loss: 0.321
Actual params: [0.5941, 0.5941]
-Original Grad: 0.029, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.088, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.328
Actual params: [0.6941, 0.4941]
-Original Grad: 0.004, -lr * Pred Grad:  0.077, New P: 0.771
-Original Grad: -0.008, -lr * Pred Grad:  -0.074, New P: 0.420
iter 2 loss: 0.325
Actual params: [0.7712, 0.4204]
-Original Grad: 0.012, -lr * Pred Grad:  0.079, New P: 0.850
-Original Grad: 0.012, -lr * Pred Grad:  -0.047, New P: 0.373
iter 3 loss: 0.322
Actual params: [0.8503, 0.3729]
-Original Grad: 0.011, -lr * Pred Grad:  0.080, New P: 0.931
-Original Grad: 0.019, -lr * Pred Grad:  -0.026, New P: 0.347
iter 4 loss: 0.317
Actual params: [0.9307, 0.3468]
-Original Grad: 0.010, -lr * Pred Grad:  0.081, New P: 1.012
-Original Grad: 0.024, -lr * Pred Grad:  -0.007, New P: 0.339
iter 5 loss: 0.310
Actual params: [1.0115, 0.3394]
-Original Grad: 0.009, -lr * Pred Grad:  0.081, New P: 1.092
-Original Grad: 0.024, -lr * Pred Grad:  0.007, New P: 0.346
iter 6 loss: 0.309
Actual params: [1.0923, 0.3465]
-Original Grad: 0.005, -lr * Pred Grad:  0.077, New P: 1.170
-Original Grad: 0.018, -lr * Pred Grad:  0.015, New P: 0.362
iter 7 loss: 0.310
Actual params: [1.1695, 0.3617]
-Original Grad: 0.003, -lr * Pred Grad:  0.072, New P: 1.242
-Original Grad: 0.010, -lr * Pred Grad:  0.019, New P: 0.380
iter 8 loss: 0.311
Actual params: [1.2416, 0.3803]
-Original Grad: 0.004, -lr * Pred Grad:  0.069, New P: 1.310
-Original Grad: 0.011, -lr * Pred Grad:  0.022, New P: 0.402
iter 9 loss: 0.312
Actual params: [1.3105, 0.4022]
-Original Grad: 0.003, -lr * Pred Grad:  0.065, New P: 1.375
-Original Grad: 0.013, -lr * Pred Grad:  0.025, New P: 0.428
iter 10 loss: 0.314
Actual params: [1.3752, 0.4276]
-Original Grad: 0.001, -lr * Pred Grad:  0.059, New P: 1.435
-Original Grad: -0.004, -lr * Pred Grad:  0.021, New P: 0.448
iter 11 loss: 0.317
Actual params: [1.4346, 0.4485]
-Original Grad: 0.000, -lr * Pred Grad:  0.054, New P: 1.489
-Original Grad: -0.012, -lr * Pred Grad:  0.013, New P: 0.461
iter 12 loss: 0.319
Actual params: [1.4886, 0.4614]
-Original Grad: -0.001, -lr * Pred Grad:  0.048, New P: 1.537
-Original Grad: -0.008, -lr * Pred Grad:  0.008, New P: 0.469
iter 13 loss: 0.321
Actual params: [1.5366, 0.4691]
-Original Grad: -0.002, -lr * Pred Grad:  0.041, New P: 1.577
-Original Grad: -0.022, -lr * Pred Grad:  -0.004, New P: 0.466
iter 14 loss: 0.322
Actual params: [1.5771, 0.4655]
-Original Grad: -0.001, -lr * Pred Grad:  0.035, New P: 1.612
-Original Grad: -0.008, -lr * Pred Grad:  -0.007, New P: 0.459
iter 15 loss: 0.322
Actual params: [1.612 , 0.4587]
-Original Grad: -0.004, -lr * Pred Grad:  0.026, New P: 1.638
-Original Grad: -0.033, -lr * Pred Grad:  -0.021, New P: 0.438
iter 16 loss: 0.320
Actual params: [1.6378, 0.4379]
-Original Grad: -0.003, -lr * Pred Grad:  0.020, New P: 1.657
-Original Grad: -0.012, -lr * Pred Grad:  -0.024, New P: 0.414
iter 17 loss: 0.318
Actual params: [1.6573, 0.4138]
-Original Grad: -0.002, -lr * Pred Grad:  0.015, New P: 1.672
-Original Grad: -0.008, -lr * Pred Grad:  -0.025, New P: 0.389
iter 18 loss: 0.316
Actual params: [1.6725, 0.3885]
-Original Grad: -0.002, -lr * Pred Grad:  0.012, New P: 1.684
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: 0.365
iter 19 loss: 0.315
Actual params: [1.6842, 0.3648]
-Original Grad: -0.000, -lr * Pred Grad:  0.010, New P: 1.694
-Original Grad: 0.009, -lr * Pred Grad:  -0.018, New P: 0.347
iter 20 loss: 0.314
Actual params: [1.6941, 0.3473]
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 1.704
-Original Grad: 0.012, -lr * Pred Grad:  -0.011, New P: 0.337
Target params: [1.1812, 0.2779]
iter 0 loss: 0.473
Actual params: [0.5941, 0.5941]
-Original Grad: 0.228, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.333, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.404
Actual params: [0.6941, 0.4941]
-Original Grad: 0.246, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.304, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 0.339
Actual params: [0.7942, 0.3944]
-Original Grad: 0.134, -lr * Pred Grad:  0.096, New P: 0.890
-Original Grad: -0.103, -lr * Pred Grad:  -0.089, New P: 0.305
iter 3 loss: 0.306
Actual params: [0.8898, 0.3051]
-Original Grad: -0.010, -lr * Pred Grad:  0.077, New P: 0.966
-Original Grad: 0.047, -lr * Pred Grad:  -0.067, New P: 0.238
iter 4 loss: 0.296
Actual params: [0.9664, 0.2382]
-Original Grad: 0.077, -lr * Pred Grad:  0.075, New P: 1.041
-Original Grad: 0.079, -lr * Pred Grad:  -0.047, New P: 0.192
iter 5 loss: 0.296
Actual params: [1.0411, 0.1916]
-Original Grad: 0.098, -lr * Pred Grad:  0.076, New P: 1.117
-Original Grad: 0.070, -lr * Pred Grad:  -0.032, New P: 0.160
iter 6 loss: 0.298
Actual params: [1.1168, 0.1595]
-Original Grad: 0.037, -lr * Pred Grad:  0.071, New P: 1.188
-Original Grad: 0.050, -lr * Pred Grad:  -0.023, New P: 0.137
iter 7 loss: 0.302
Actual params: [1.1875, 0.137 ]
-Original Grad: 0.028, -lr * Pred Grad:  0.066, New P: 1.253
-Original Grad: 0.055, -lr * Pred Grad:  -0.014, New P: 0.123
iter 8 loss: 0.305
Actual params: [1.2533, 0.1229]
-Original Grad: 0.019, -lr * Pred Grad:  0.061, New P: 1.314
-Original Grad: 0.050, -lr * Pred Grad:  -0.007, New P: 0.116
iter 9 loss: 0.306
Actual params: [1.314 , 0.1155]
-Original Grad: 0.020, -lr * Pred Grad:  0.057, New P: 1.371
-Original Grad: 0.049, -lr * Pred Grad:  -0.002, New P: 0.114
iter 10 loss: 0.308
Actual params: [1.3707, 0.1139]
-Original Grad: 0.015, -lr * Pred Grad:  0.053, New P: 1.423
-Original Grad: 0.025, -lr * Pred Grad:  0.001, New P: 0.115
iter 11 loss: 0.311
Actual params: [1.4233, 0.1149]
-Original Grad: 0.018, -lr * Pred Grad:  0.050, New P: 1.473
-Original Grad: 0.043, -lr * Pred Grad:  0.005, New P: 0.120
iter 12 loss: 0.314
Actual params: [1.4728, 0.12  ]
-Original Grad: 0.017, -lr * Pred Grad:  0.047, New P: 1.520
-Original Grad: 0.030, -lr * Pred Grad:  0.008, New P: 0.128
iter 13 loss: 0.311
Actual params: [1.5195, 0.1276]
-Original Grad: 0.015, -lr * Pred Grad:  0.044, New P: 1.564
-Original Grad: 0.026, -lr * Pred Grad:  0.009, New P: 0.137
iter 14 loss: 0.314
Actual params: [1.5636, 0.137 ]
-Original Grad: 0.011, -lr * Pred Grad:  0.041, New P: 1.605
-Original Grad: 0.009, -lr * Pred Grad:  0.009, New P: 0.146
iter 15 loss: 0.319
Actual params: [1.6048, 0.1464]
-Original Grad: 0.015, -lr * Pred Grad:  0.039, New P: 1.644
-Original Grad: 0.023, -lr * Pred Grad:  0.011, New P: 0.157
iter 16 loss: 0.324
Actual params: [1.6442, 0.1572]
-Original Grad: 0.003, -lr * Pred Grad:  0.036, New P: 1.680
-Original Grad: -0.003, -lr * Pred Grad:  0.010, New P: 0.167
iter 17 loss: 0.329
Actual params: [1.6802, 0.1667]
-Original Grad: 0.004, -lr * Pred Grad:  0.033, New P: 1.713
-Original Grad: -0.001, -lr * Pred Grad:  0.009, New P: 0.175
iter 18 loss: 0.333
Actual params: [1.7134, 0.1753]
-Original Grad: -0.000, -lr * Pred Grad:  0.030, New P: 1.744
-Original Grad: -0.013, -lr * Pred Grad:  0.006, New P: 0.182
iter 19 loss: 0.335
Actual params: [1.7436, 0.1817]
-Original Grad: -0.009, -lr * Pred Grad:  0.026, New P: 1.770
-Original Grad: -0.024, -lr * Pred Grad:  0.003, New P: 0.185
iter 20 loss: 0.333
Actual params: [1.7698, 0.1852]
-Original Grad: -0.008, -lr * Pred Grad:  0.023, New P: 1.793
-Original Grad: -0.023, -lr * Pred Grad:  0.001, New P: 0.186
Target params: [1.1812, 0.2779]
iter 0 loss: 0.324
Actual params: [0.5941, 0.5941]
-Original Grad: 0.165, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.180, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.189
Actual params: [0.6941, 0.4941]
-Original Grad: -0.010, -lr * Pred Grad:  0.062, New P: 0.756
-Original Grad: -0.031, -lr * Pred Grad:  -0.078, New P: 0.416
iter 2 loss: 0.197
Actual params: [0.7562, 0.4156]
-Original Grad: -0.027, -lr * Pred Grad:  0.037, New P: 0.793
-Original Grad: -0.012, -lr * Pred Grad:  -0.065, New P: 0.351
iter 3 loss: 0.191
Actual params: [0.7935, 0.3509]
-Original Grad: -0.017, -lr * Pred Grad:  0.025, New P: 0.818
-Original Grad: 0.004, -lr * Pred Grad:  -0.052, New P: 0.299
iter 4 loss: 0.182
Actual params: [0.8181, 0.2991]
-Original Grad: -0.008, -lr * Pred Grad:  0.018, New P: 0.836
-Original Grad: 0.012, -lr * Pred Grad:  -0.040, New P: 0.259
iter 5 loss: 0.181
Actual params: [0.8364, 0.2591]
-Original Grad: -0.010, -lr * Pred Grad:  0.013, New P: 0.849
-Original Grad: 0.014, -lr * Pred Grad:  -0.030, New P: 0.229
iter 6 loss: 0.181
Actual params: [0.8492, 0.2287]
-Original Grad: -0.010, -lr * Pred Grad:  0.008, New P: 0.857
-Original Grad: 0.007, -lr * Pred Grad:  -0.025, New P: 0.204
iter 7 loss: 0.180
Actual params: [0.8573, 0.2041]
-Original Grad: -0.009, -lr * Pred Grad:  0.004, New P: 0.862
-Original Grad: 0.024, -lr * Pred Grad:  -0.015, New P: 0.189
iter 8 loss: 0.180
Actual params: [0.8617, 0.189 ]
-Original Grad: -0.009, -lr * Pred Grad:  0.001, New P: 0.863
-Original Grad: 0.039, -lr * Pred Grad:  -0.003, New P: 0.186
iter 9 loss: 0.180
Actual params: [0.863 , 0.1862]
-Original Grad: -0.009, -lr * Pred Grad:  -0.001, New P: 0.862
-Original Grad: 0.025, -lr * Pred Grad:  0.004, New P: 0.190
iter 10 loss: 0.180
Actual params: [0.8615, 0.19  ]
-Original Grad: -0.010, -lr * Pred Grad:  -0.004, New P: 0.857
-Original Grad: 0.056, -lr * Pred Grad:  0.017, New P: 0.207
iter 11 loss: 0.180
Actual params: [0.8573, 0.2068]
-Original Grad: -0.008, -lr * Pred Grad:  -0.006, New P: 0.851
-Original Grad: 0.023, -lr * Pred Grad:  0.020, New P: 0.227
iter 12 loss: 0.181
Actual params: [0.8512, 0.2273]
-Original Grad: -0.013, -lr * Pred Grad:  -0.009, New P: 0.842
-Original Grad: 0.023, -lr * Pred Grad:  0.024, New P: 0.251
iter 13 loss: 0.181
Actual params: [0.8422, 0.2512]
-Original Grad: -0.011, -lr * Pred Grad:  -0.011, New P: 0.831
-Original Grad: 0.022, -lr * Pred Grad:  0.027, New P: 0.278
iter 14 loss: 0.182
Actual params: [0.8309, 0.2778]
-Original Grad: -0.017, -lr * Pred Grad:  -0.015, New P: 0.816
-Original Grad: 0.014, -lr * Pred Grad:  0.027, New P: 0.305
iter 15 loss: 0.183
Actual params: [0.8159, 0.3052]
-Original Grad: -0.012, -lr * Pred Grad:  -0.017, New P: 0.799
-Original Grad: 0.008, -lr * Pred Grad:  0.027, New P: 0.332
iter 16 loss: 0.188
Actual params: [0.7988, 0.332 ]
-Original Grad: -0.013, -lr * Pred Grad:  -0.019, New P: 0.780
-Original Grad: 0.003, -lr * Pred Grad:  0.025, New P: 0.357
iter 17 loss: 0.191
Actual params: [0.7796, 0.357 ]
-Original Grad: -0.017, -lr * Pred Grad:  -0.022, New P: 0.757
-Original Grad: 0.002, -lr * Pred Grad:  0.023, New P: 0.380
iter 18 loss: 0.192
Actual params: [0.7573, 0.3802]
-Original Grad: -0.027, -lr * Pred Grad:  -0.028, New P: 0.729
-Original Grad: -0.007, -lr * Pred Grad:  0.019, New P: 0.399
iter 19 loss: 0.185
Actual params: [0.7295, 0.3994]
-Original Grad: -0.019, -lr * Pred Grad:  -0.031, New P: 0.699
-Original Grad: -0.009, -lr * Pred Grad:  0.015, New P: 0.415
iter 20 loss: 0.189
Actual params: [0.6988, 0.4147]
-Original Grad: -0.007, -lr * Pred Grad:  -0.030, New P: 0.669
-Original Grad: -0.010, -lr * Pred Grad:  0.011, New P: 0.426
Target params: [1.1812, 0.2779]
iter 0 loss: 0.724
Actual params: [0.5941, 0.5941]
-Original Grad: 0.195, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.390, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.630
Actual params: [0.6941, 0.4941]
-Original Grad: 0.152, -lr * Pred Grad:  0.099, New P: 0.793
-Original Grad: -0.287, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.558
Actual params: [0.7927, 0.396 ]
-Original Grad: -0.001, -lr * Pred Grad:  0.076, New P: 0.869
-Original Grad: -0.219, -lr * Pred Grad:  -0.095, New P: 0.301
iter 3 loss: 0.512
Actual params: [0.8686, 0.3006]
-Original Grad: -0.013, -lr * Pred Grad:  0.059, New P: 0.928
-Original Grad: -0.065, -lr * Pred Grad:  -0.085, New P: 0.216
iter 4 loss: 0.487
Actual params: [0.9276, 0.216 ]
-Original Grad: -0.020, -lr * Pred Grad:  0.045, New P: 0.973
-Original Grad: -0.033, -lr * Pred Grad:  -0.075, New P: 0.141
iter 5 loss: 0.469
Actual params: [0.9729, 0.1412]
-Original Grad: -0.019, -lr * Pred Grad:  0.035, New P: 1.008
-Original Grad: -0.008, -lr * Pred Grad:  -0.065, New P: 0.076
iter 6 loss: 0.469
Actual params: [1.0077, 0.076 ]
-Original Grad: -0.020, -lr * Pred Grad:  0.026, New P: 1.034
-Original Grad: -0.018, -lr * Pred Grad:  -0.059, New P: 0.017
iter 7 loss: 0.472
Actual params: [1.0341, 0.0173]
-Original Grad: -0.022, -lr * Pred Grad:  0.019, New P: 1.053
-Original Grad: -0.009, -lr * Pred Grad:  -0.053, New P: -0.035
iter 8 loss: 0.476
Actual params: [ 1.0529, -0.0352]
-Original Grad: -0.020, -lr * Pred Grad:  0.013, New P: 1.066
-Original Grad: -0.026, -lr * Pred Grad:  -0.049, New P: -0.084
iter 9 loss: 0.479
Actual params: [ 1.0656, -0.0842]
-Original Grad: -0.016, -lr * Pred Grad:  0.008, New P: 1.074
-Original Grad: 0.009, -lr * Pred Grad:  -0.043, New P: -0.127
iter 10 loss: 0.482
Actual params: [ 1.0739, -0.127 ]
-Original Grad: -0.018, -lr * Pred Grad:  0.004, New P: 1.078
-Original Grad: 0.019, -lr * Pred Grad:  -0.037, New P: -0.164
iter 11 loss: 0.486
Actual params: [ 1.0779, -0.1637]
-Original Grad: -0.015, -lr * Pred Grad:  0.001, New P: 1.079
-Original Grad: 0.009, -lr * Pred Grad:  -0.032, New P: -0.196
iter 12 loss: 0.490
Actual params: [ 1.0785, -0.1959]
-Original Grad: -0.015, -lr * Pred Grad:  -0.002, New P: 1.076
-Original Grad: 0.009, -lr * Pred Grad:  -0.028, New P: -0.224
iter 13 loss: 0.496
Actual params: [ 1.0763, -0.224 ]
-Original Grad: -0.014, -lr * Pred Grad:  -0.005, New P: 1.072
-Original Grad: 0.013, -lr * Pred Grad:  -0.024, New P: -0.248
iter 14 loss: 0.500
Actual params: [ 1.0716, -0.2483]
-Original Grad: -0.016, -lr * Pred Grad:  -0.007, New P: 1.064
-Original Grad: 0.015, -lr * Pred Grad:  -0.021, New P: -0.269
iter 15 loss: 0.499
Actual params: [ 1.0642, -0.2689]
-Original Grad: -0.019, -lr * Pred Grad:  -0.010, New P: 1.054
-Original Grad: 0.020, -lr * Pred Grad:  -0.017, New P: -0.286
iter 16 loss: 0.495
Actual params: [ 1.0538, -0.2856]
-Original Grad: -0.020, -lr * Pred Grad:  -0.013, New P: 1.041
-Original Grad: 0.022, -lr * Pred Grad:  -0.013, New P: -0.299
iter 17 loss: 0.496
Actual params: [ 1.0406, -0.2988]
-Original Grad: -0.013, -lr * Pred Grad:  -0.014, New P: 1.026
-Original Grad: 0.018, -lr * Pred Grad:  -0.010, New P: -0.309
iter 18 loss: 0.496
Actual params: [ 1.0261, -0.3091]
-Original Grad: -0.015, -lr * Pred Grad:  -0.016, New P: 1.010
-Original Grad: 0.020, -lr * Pred Grad:  -0.008, New P: -0.317
iter 19 loss: 0.496
Actual params: [ 1.01  , -0.3166]
-Original Grad: -0.011, -lr * Pred Grad:  -0.017, New P: 0.993
-Original Grad: 0.011, -lr * Pred Grad:  -0.006, New P: -0.322
iter 20 loss: 0.501
Actual params: [ 0.9931, -0.3224]
-Original Grad: -0.014, -lr * Pred Grad:  -0.018, New P: 0.975
-Original Grad: 0.019, -lr * Pred Grad:  -0.003, New P: -0.326
Target params: [1.1812, 0.2779]
iter 0 loss: 0.320
Actual params: [0.5941, 0.5941]
-Original Grad: 0.105, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.089, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.249
Actual params: [0.6941, 0.4941]
-Original Grad: 0.010, -lr * Pred Grad:  0.074, New P: 0.768
-Original Grad: -0.031, -lr * Pred Grad:  -0.088, New P: 0.406
iter 2 loss: 0.232
Actual params: [0.7678, 0.4062]
-Original Grad: 0.010, -lr * Pred Grad:  0.063, New P: 0.831
-Original Grad: -0.003, -lr * Pred Grad:  -0.070, New P: 0.336
iter 3 loss: 0.223
Actual params: [0.8306, 0.3362]
-Original Grad: -0.005, -lr * Pred Grad:  0.048, New P: 0.879
-Original Grad: -0.011, -lr * Pred Grad:  -0.064, New P: 0.272
iter 4 loss: 0.223
Actual params: [0.879 , 0.2724]
-Original Grad: -0.006, -lr * Pred Grad:  0.038, New P: 0.917
-Original Grad: 0.006, -lr * Pred Grad:  -0.050, New P: 0.222
iter 5 loss: 0.227
Actual params: [0.9165, 0.2222]
-Original Grad: -0.009, -lr * Pred Grad:  0.028, New P: 0.945
-Original Grad: 0.013, -lr * Pred Grad:  -0.036, New P: 0.186
iter 6 loss: 0.231
Actual params: [0.9446, 0.1864]
-Original Grad: -0.010, -lr * Pred Grad:  0.020, New P: 0.964
-Original Grad: 0.009, -lr * Pred Grad:  -0.027, New P: 0.160
iter 7 loss: 0.235
Actual params: [0.9644, 0.1598]
-Original Grad: -0.011, -lr * Pred Grad:  0.012, New P: 0.977
-Original Grad: 0.009, -lr * Pred Grad:  -0.019, New P: 0.141
iter 8 loss: 0.237
Actual params: [0.9766, 0.1409]
-Original Grad: -0.011, -lr * Pred Grad:  0.006, New P: 0.982
-Original Grad: 0.009, -lr * Pred Grad:  -0.012, New P: 0.129
iter 9 loss: 0.239
Actual params: [0.9823, 0.1287]
-Original Grad: -0.011, -lr * Pred Grad:  0.000, New P: 0.982
-Original Grad: 0.015, -lr * Pred Grad:  -0.003, New P: 0.125
iter 10 loss: 0.239
Actual params: [0.9825, 0.1252]
-Original Grad: -0.008, -lr * Pred Grad:  -0.004, New P: 0.979
-Original Grad: 0.015, -lr * Pred Grad:  0.004, New P: 0.129
iter 11 loss: 0.239
Actual params: [0.9789, 0.1293]
-Original Grad: -0.009, -lr * Pred Grad:  -0.007, New P: 0.972
-Original Grad: 0.008, -lr * Pred Grad:  0.008, New P: 0.137
iter 12 loss: 0.237
Actual params: [0.9715, 0.1369]
-Original Grad: -0.009, -lr * Pred Grad:  -0.011, New P: 0.961
-Original Grad: 0.010, -lr * Pred Grad:  0.012, New P: 0.149
iter 13 loss: 0.235
Actual params: [0.9607, 0.1486]
-Original Grad: -0.010, -lr * Pred Grad:  -0.014, New P: 0.947
-Original Grad: 0.013, -lr * Pred Grad:  0.017, New P: 0.166
iter 14 loss: 0.233
Actual params: [0.9467, 0.1655]
-Original Grad: -0.007, -lr * Pred Grad:  -0.016, New P: 0.931
-Original Grad: 0.002, -lr * Pred Grad:  0.016, New P: 0.182
iter 15 loss: 0.230
Actual params: [0.9309, 0.1819]
-Original Grad: -0.007, -lr * Pred Grad:  -0.018, New P: 0.913
-Original Grad: 0.004, -lr * Pred Grad:  0.017, New P: 0.199
iter 16 loss: 0.228
Actual params: [0.9134, 0.1989]
-Original Grad: -0.007, -lr * Pred Grad:  -0.019, New P: 0.894
-Original Grad: 0.006, -lr * Pred Grad:  0.018, New P: 0.217
iter 17 loss: 0.226
Actual params: [0.8942, 0.217 ]
-Original Grad: -0.009, -lr * Pred Grad:  -0.021, New P: 0.873
-Original Grad: 0.007, -lr * Pred Grad:  0.020, New P: 0.237
iter 18 loss: 0.223
Actual params: [0.873 , 0.2369]
-Original Grad: -0.004, -lr * Pred Grad:  -0.021, New P: 0.852
-Original Grad: 0.007, -lr * Pred Grad:  0.021, New P: 0.258
iter 19 loss: 0.221
Actual params: [0.852 , 0.2581]
-Original Grad: -0.003, -lr * Pred Grad:  -0.020, New P: 0.832
-Original Grad: 0.002, -lr * Pred Grad:  0.020, New P: 0.279
iter 20 loss: 0.221
Actual params: [0.8315, 0.2785]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: 0.813
-Original Grad: 0.002, -lr * Pred Grad:  0.019, New P: 0.298
Target params: [1.1812, 0.2779]
iter 0 loss: 0.280
Actual params: [0.5941, 0.5941]
-Original Grad: 0.197, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.027, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.232
Actual params: [0.6941, 0.4941]
-Original Grad: 0.069, -lr * Pred Grad:  0.088, New P: 0.782
-Original Grad: -0.024, -lr * Pred Grad:  -0.099, New P: 0.395
iter 2 loss: 0.210
Actual params: [0.7819, 0.3946]
-Original Grad: 0.042, -lr * Pred Grad:  0.079, New P: 0.861
-Original Grad: -0.026, -lr * Pred Grad:  -0.100, New P: 0.295
iter 3 loss: 0.218
Actual params: [0.861 , 0.2949]
-Original Grad: 0.031, -lr * Pred Grad:  0.072, New P: 0.933
-Original Grad: -0.000, -lr * Pred Grad:  -0.082, New P: 0.213
iter 4 loss: 0.234
Actual params: [0.9334, 0.2131]
-Original Grad: 0.017, -lr * Pred Grad:  0.065, New P: 0.999
-Original Grad: 0.014, -lr * Pred Grad:  -0.050, New P: 0.163
iter 5 loss: 0.251
Actual params: [0.9988, 0.1629]
-Original Grad: 0.011, -lr * Pred Grad:  0.059, New P: 1.058
-Original Grad: 0.032, -lr * Pred Grad:  -0.006, New P: 0.157
iter 6 loss: 0.249
Actual params: [1.0577, 0.1571]
-Original Grad: -0.002, -lr * Pred Grad:  0.051, New P: 1.109
-Original Grad: 0.031, -lr * Pred Grad:  0.020, New P: 0.177
iter 7 loss: 0.241
Actual params: [1.1088, 0.1768]
-Original Grad: -0.008, -lr * Pred Grad:  0.043, New P: 1.152
-Original Grad: 0.025, -lr * Pred Grad:  0.034, New P: 0.211
iter 8 loss: 0.229
Actual params: [1.1519, 0.2109]
-Original Grad: -0.005, -lr * Pred Grad:  0.037, New P: 1.189
-Original Grad: 0.012, -lr * Pred Grad:  0.038, New P: 0.249
iter 9 loss: 0.213
Actual params: [1.189 , 0.2494]
-Original Grad: -0.004, -lr * Pred Grad:  0.032, New P: 1.221
-Original Grad: 0.013, -lr * Pred Grad:  0.042, New P: 0.292
iter 10 loss: 0.199
Actual params: [1.2212, 0.2918]
-Original Grad: -0.001, -lr * Pred Grad:  0.029, New P: 1.250
-Original Grad: -0.007, -lr * Pred Grad:  0.033, New P: 0.325
iter 11 loss: 0.187
Actual params: [1.25  , 0.3249]
-Original Grad: 0.001, -lr * Pred Grad:  0.026, New P: 1.276
-Original Grad: -0.020, -lr * Pred Grad:  0.015, New P: 0.340
iter 12 loss: 0.188
Actual params: [1.276 , 0.3403]
-Original Grad: 0.006, -lr * Pred Grad:  0.025, New P: 1.301
-Original Grad: -0.027, -lr * Pred Grad:  -0.003, New P: 0.337
iter 13 loss: 0.189
Actual params: [1.3008, 0.3368]
-Original Grad: -0.001, -lr * Pred Grad:  0.022, New P: 1.323
-Original Grad: -0.016, -lr * Pred Grad:  -0.012, New P: 0.324
iter 14 loss: 0.195
Actual params: [1.323 , 0.3244]
-Original Grad: -0.004, -lr * Pred Grad:  0.019, New P: 1.342
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: 0.313
iter 15 loss: 0.202
Actual params: [1.3421, 0.313 ]
-Original Grad: -0.012, -lr * Pred Grad:  0.015, New P: 1.357
-Original Grad: 0.009, -lr * Pred Grad:  -0.005, New P: 0.308
iter 16 loss: 0.206
Actual params: [1.3567, 0.3084]
-Original Grad: -0.006, -lr * Pred Grad:  0.012, New P: 1.369
-Original Grad: 0.007, -lr * Pred Grad:  -0.000, New P: 0.308
iter 17 loss: 0.207
Actual params: [1.3686, 0.3082]
-Original Grad: -0.010, -lr * Pred Grad:  0.008, New P: 1.377
-Original Grad: 0.013, -lr * Pred Grad:  0.008, New P: 0.316
iter 18 loss: 0.204
Actual params: [1.3769, 0.3158]
-Original Grad: -0.004, -lr * Pred Grad:  0.007, New P: 1.384
-Original Grad: -0.005, -lr * Pred Grad:  0.004, New P: 0.320
iter 19 loss: 0.202
Actual params: [1.3836, 0.3198]
-Original Grad: -0.006, -lr * Pred Grad:  0.005, New P: 1.388
-Original Grad: 0.006, -lr * Pred Grad:  0.007, New P: 0.327
iter 20 loss: 0.199
Actual params: [1.3882, 0.3271]
-Original Grad: -0.006, -lr * Pred Grad:  0.003, New P: 1.391
-Original Grad: -0.012, -lr * Pred Grad:  -0.001, New P: 0.326
Target params: [1.1812, 0.2779]
iter 0 loss: 1.080
Actual params: [0.5941, 0.5941]
-Original Grad: 0.140, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.277, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 1.067
Actual params: [0.6941, 0.4941]
-Original Grad: 0.263, -lr * Pred Grad:  0.097, New P: 0.791
-Original Grad: -0.248, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 0.915
Actual params: [0.7912, 0.3945]
-Original Grad: 0.196, -lr * Pred Grad:  0.098, New P: 0.889
-Original Grad: -0.280, -lr * Pred Grad:  -0.100, New P: 0.295
iter 3 loss: 0.562
Actual params: [0.8891, 0.2946]
-Original Grad: 0.099, -lr * Pred Grad:  0.093, New P: 0.982
-Original Grad: -0.112, -lr * Pred Grad:  -0.093, New P: 0.201
iter 4 loss: 0.355
Actual params: [0.9818, 0.2014]
-Original Grad: 0.040, -lr * Pred Grad:  0.084, New P: 1.066
-Original Grad: -0.018, -lr * Pred Grad:  -0.081, New P: 0.121
iter 5 loss: 0.284
Actual params: [1.0657, 0.1207]
-Original Grad: 0.017, -lr * Pred Grad:  0.075, New P: 1.140
-Original Grad: 0.007, -lr * Pred Grad:  -0.069, New P: 0.052
iter 6 loss: 0.259
Actual params: [1.1402, 0.0519]
-Original Grad: 0.007, -lr * Pred Grad:  0.066, New P: 1.206
-Original Grad: 0.012, -lr * Pred Grad:  -0.059, New P: -0.007
iter 7 loss: 0.251
Actual params: [ 1.2062, -0.0069]
-Original Grad: 0.009, -lr * Pred Grad:  0.059, New P: 1.266
-Original Grad: 0.010, -lr * Pred Grad:  -0.051, New P: -0.058
iter 8 loss: 0.251
Actual params: [ 1.2656, -0.0576]
-Original Grad: -0.008, -lr * Pred Grad:  0.052, New P: 1.317
-Original Grad: 0.013, -lr * Pred Grad:  -0.044, New P: -0.101
iter 9 loss: 0.250
Actual params: [ 1.3173, -0.1013]
-Original Grad: -0.003, -lr * Pred Grad:  0.046, New P: 1.363
-Original Grad: 0.026, -lr * Pred Grad:  -0.036, New P: -0.138
iter 10 loss: 0.248
Actual params: [ 1.3631, -0.1377]
-Original Grad: -0.003, -lr * Pred Grad:  0.041, New P: 1.404
-Original Grad: 0.016, -lr * Pred Grad:  -0.031, New P: -0.169
iter 11 loss: 0.246
Actual params: [ 1.4037, -0.1686]
-Original Grad: -0.018, -lr * Pred Grad:  0.034, New P: 1.438
-Original Grad: 0.009, -lr * Pred Grad:  -0.027, New P: -0.195
iter 12 loss: 0.245
Actual params: [ 1.4378, -0.1955]
-Original Grad: -0.003, -lr * Pred Grad:  0.030, New P: 1.468
-Original Grad: 0.012, -lr * Pred Grad:  -0.023, New P: -0.218
iter 13 loss: 0.244
Actual params: [ 1.4681, -0.2185]
-Original Grad: 0.001, -lr * Pred Grad:  0.028, New P: 1.496
-Original Grad: 0.011, -lr * Pred Grad:  -0.020, New P: -0.238
iter 14 loss: 0.244
Actual params: [ 1.4957, -0.2381]
-Original Grad: -0.002, -lr * Pred Grad:  0.025, New P: 1.520
-Original Grad: 0.016, -lr * Pred Grad:  -0.016, New P: -0.254
iter 15 loss: 0.244
Actual params: [ 1.5204, -0.2543]
-Original Grad: 0.006, -lr * Pred Grad:  0.023, New P: 1.544
-Original Grad: 0.023, -lr * Pred Grad:  -0.012, New P: -0.267
iter 16 loss: 0.245
Actual params: [ 1.5436, -0.2666]
-Original Grad: 0.005, -lr * Pred Grad:  0.022, New P: 1.565
-Original Grad: 0.018, -lr * Pred Grad:  -0.009, New P: -0.276
iter 17 loss: 0.246
Actual params: [ 1.5652, -0.2759]
-Original Grad: 0.008, -lr * Pred Grad:  0.021, New P: 1.586
-Original Grad: 0.029, -lr * Pred Grad:  -0.005, New P: -0.281
iter 18 loss: 0.249
Actual params: [ 1.586 , -0.2813]
-Original Grad: 0.001, -lr * Pred Grad:  0.019, New P: 1.605
-Original Grad: 0.019, -lr * Pred Grad:  -0.003, New P: -0.284
iter 19 loss: 0.251
Actual params: [ 1.605 , -0.2843]
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 1.623
-Original Grad: 0.019, -lr * Pred Grad:  -0.001, New P: -0.285
iter 20 loss: 0.254
Actual params: [ 1.6225, -0.285 ]
-Original Grad: -0.002, -lr * Pred Grad:  0.016, New P: 1.638
-Original Grad: 0.012, -lr * Pred Grad:  0.001, New P: -0.284
