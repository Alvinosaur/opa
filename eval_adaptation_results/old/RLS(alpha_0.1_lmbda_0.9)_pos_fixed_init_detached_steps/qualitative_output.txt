Target params: [1.1812, 0.2779]
iter 0 loss: 0.739
Actual params: [0.5941, 0.5941]
-Original Grad: 0.051, -lr * Pred Grad:  0.065, New P: 0.659
-Original Grad: -0.803, -lr * Pred Grad:  -0.041, New P: 0.553
iter 1 loss: 0.718
Actual params: [0.6586, 0.553 ]
-Original Grad: 0.070, -lr * Pred Grad:  0.026, New P: 0.685
-Original Grad: -0.856, -lr * Pred Grad:  -0.022, New P: 0.531
iter 2 loss: 0.708
Actual params: [0.685 , 0.5308]
-Original Grad: 0.089, -lr * Pred Grad:  0.021, New P: 0.706
-Original Grad: -1.006, -lr * Pred Grad:  -0.015, New P: 0.516
iter 3 loss: 0.699
Actual params: [0.7055, 0.516 ]
-Original Grad: 0.559, -lr * Pred Grad:  0.020, New P: 0.726
-Original Grad: -0.564, -lr * Pred Grad:  -0.007, New P: 0.509
iter 4 loss: 0.693
Actual params: [0.7257, 0.5091]
-Original Grad: 0.111, -lr * Pred Grad:  0.002, New P: 0.728
-Original Grad: -0.774, -lr * Pred Grad:  -0.009, New P: 0.500
iter 5 loss: 0.691
Actual params: [0.728 , 0.5001]
-Original Grad: 0.348, -lr * Pred Grad:  0.006, New P: 0.734
-Original Grad: -0.802, -lr * Pred Grad:  -0.008, New P: 0.492
iter 6 loss: 0.686
Actual params: [0.7339, 0.4924]
-Original Grad: 0.120, -lr * Pred Grad:  0.001, New P: 0.735
-Original Grad: -0.921, -lr * Pred Grad:  -0.008, New P: 0.485
iter 7 loss: 0.685
Actual params: [0.735 , 0.4848]
-Original Grad: 0.366, -lr * Pred Grad:  0.005, New P: 0.740
-Original Grad: -0.956, -lr * Pred Grad:  -0.006, New P: 0.479
iter 8 loss: 0.680
Actual params: [0.7398, 0.4787]
-Original Grad: 0.330, -lr * Pred Grad:  0.004, New P: 0.743
-Original Grad: -0.630, -lr * Pred Grad:  -0.004, New P: 0.475
iter 9 loss: 0.677
Actual params: [0.7434, 0.4749]
-Original Grad: 0.388, -lr * Pred Grad:  0.004, New P: 0.747
-Original Grad: -0.748, -lr * Pred Grad:  -0.004, New P: 0.471
iter 10 loss: 0.674
Actual params: [0.7472, 0.4708]
-Original Grad: 0.528, -lr * Pred Grad:  0.005, New P: 0.752
-Original Grad: -0.644, -lr * Pred Grad:  -0.003, New P: 0.468
iter 11 loss: 0.671
Actual params: [0.7519, 0.4676]
-Original Grad: 0.225, -lr * Pred Grad:  0.001, New P: 0.753
-Original Grad: -0.839, -lr * Pred Grad:  -0.004, New P: 0.463
iter 12 loss: 0.670
Actual params: [0.7534, 0.4633]
-Original Grad: 0.126, -lr * Pred Grad:  0.000, New P: 0.754
-Original Grad: -1.041, -lr * Pred Grad:  -0.005, New P: 0.459
iter 13 loss: 0.670
Actual params: [0.7538, 0.4586]
-Original Grad: 0.525, -lr * Pred Grad:  0.005, New P: 0.758
-Original Grad: -0.710, -lr * Pred Grad:  -0.003, New P: 0.456
iter 14 loss: 0.673
Actual params: [0.7584, 0.456 ]
-Original Grad: 0.111, -lr * Pred Grad:  0.001, New P: 0.759
-Original Grad: -0.583, -lr * Pred Grad:  -0.003, New P: 0.453
iter 15 loss: 0.672
Actual params: [0.7589, 0.4533]
-Original Grad: 0.519, -lr * Pred Grad:  0.005, New P: 0.763
-Original Grad: -0.478, -lr * Pred Grad:  -0.002, New P: 0.452
iter 16 loss: 0.669
Actual params: [0.7635, 0.4515]
-Original Grad: 0.372, -lr * Pred Grad:  0.003, New P: 0.766
-Original Grad: -0.639, -lr * Pred Grad:  -0.003, New P: 0.449
iter 17 loss: 0.666
Actual params: [0.7661, 0.4488]
-Original Grad: 0.258, -lr * Pred Grad:  0.001, New P: 0.768
-Original Grad: -0.668, -lr * Pred Grad:  -0.003, New P: 0.446
iter 18 loss: 0.664
Actual params: [0.7676, 0.4459]
-Original Grad: 0.388, -lr * Pred Grad:  0.003, New P: 0.770
-Original Grad: -0.400, -lr * Pred Grad:  -0.002, New P: 0.444
iter 19 loss: 0.662
Actual params: [0.7703, 0.4443]
-Original Grad: 0.274, -lr * Pred Grad:  0.002, New P: 0.772
-Original Grad: -0.418, -lr * Pred Grad:  -0.002, New P: 0.442
iter 20 loss: 0.659
Actual params: [0.7718, 0.4424]
-Original Grad: 0.141, -lr * Pred Grad:  0.000, New P: 0.772
-Original Grad: -0.595, -lr * Pred Grad:  -0.003, New P: 0.439
Target params: [1.1812, 0.2779]
iter 0 loss: 0.312
Actual params: [0.5941, 0.5941]
-Original Grad: 0.081, -lr * Pred Grad:  0.005, New P: 0.600
-Original Grad: -0.408, -lr * Pred Grad:  -0.018, New P: 0.576
iter 1 loss: 0.299
Actual params: [0.5995, 0.5764]
-Original Grad: 0.069, -lr * Pred Grad:  0.010, New P: 0.609
-Original Grad: -0.300, -lr * Pred Grad:  -0.006, New P: 0.571
iter 2 loss: 0.292
Actual params: [0.6092, 0.5707]
-Original Grad: 0.044, -lr * Pred Grad:  0.018, New P: 0.627
-Original Grad: -0.148, -lr * Pred Grad:  0.001, New P: 0.571
iter 3 loss: 0.285
Actual params: [0.6274, 0.5714]
-Original Grad: 0.083, -lr * Pred Grad:  0.031, New P: 0.659
-Original Grad: -0.248, -lr * Pred Grad:  0.003, New P: 0.575
iter 4 loss: 0.273
Actual params: [0.6588, 0.5748]
-Original Grad: 0.086, -lr * Pred Grad:  0.025, New P: 0.684
-Original Grad: -0.245, -lr * Pred Grad:  0.003, New P: 0.578
iter 5 loss: 0.263
Actual params: [0.6839, 0.5781]
-Original Grad: 0.054, -lr * Pred Grad:  0.025, New P: 0.709
-Original Grad: -0.122, -lr * Pred Grad:  0.005, New P: 0.583
iter 6 loss: 0.260
Actual params: [0.709 , 0.5835]
-Original Grad: 0.046, -lr * Pred Grad:  0.029, New P: 0.738
-Original Grad: -0.070, -lr * Pred Grad:  0.008, New P: 0.591
iter 7 loss: 0.257
Actual params: [0.738, 0.591]
-Original Grad: 0.046, -lr * Pred Grad:  0.025, New P: 0.763
-Original Grad: -0.070, -lr * Pred Grad:  0.007, New P: 0.598
iter 8 loss: 0.254
Actual params: [0.7634, 0.5975]
-Original Grad: 0.067, -lr * Pred Grad:  0.022, New P: 0.786
-Original Grad: -0.137, -lr * Pred Grad:  0.005, New P: 0.602
iter 9 loss: 0.251
Actual params: [0.7858, 0.6024]
-Original Grad: 0.053, -lr * Pred Grad:  0.013, New P: 0.799
-Original Grad: -0.123, -lr * Pred Grad:  0.002, New P: 0.605
iter 10 loss: 0.250
Actual params: [0.7986, 0.6047]
-Original Grad: 0.042, -lr * Pred Grad:  0.005, New P: 0.803
-Original Grad: -0.114, -lr * Pred Grad:  -0.000, New P: 0.605
iter 11 loss: 0.249
Actual params: [0.8034, 0.6047]
-Original Grad: 0.076, -lr * Pred Grad:  0.006, New P: 0.810
-Original Grad: -0.219, -lr * Pred Grad:  -0.001, New P: 0.604
iter 12 loss: 0.248
Actual params: [0.8099, 0.6041]
-Original Grad: 0.044, -lr * Pred Grad:  0.006, New P: 0.816
-Original Grad: -0.121, -lr * Pred Grad:  0.000, New P: 0.605
iter 13 loss: 0.246
Actual params: [0.8157, 0.6045]
-Original Grad: 0.093, -lr * Pred Grad:  0.008, New P: 0.824
-Original Grad: -0.266, -lr * Pred Grad:  -0.000, New P: 0.604
iter 14 loss: 0.246
Actual params: [0.8242, 0.6044]
-Original Grad: 0.024, -lr * Pred Grad:  -0.007, New P: 0.817
-Original Grad: -0.107, -lr * Pred Grad:  -0.003, New P: 0.601
iter 15 loss: 0.246
Actual params: [0.8169, 0.601 ]
-Original Grad: 0.039, -lr * Pred Grad:  -0.008, New P: 0.809
-Original Grad: -0.166, -lr * Pred Grad:  -0.004, New P: 0.597
iter 16 loss: 0.246
Actual params: [0.8093, 0.597 ]
-Original Grad: 0.033, -lr * Pred Grad:  0.006, New P: 0.815
-Original Grad: -0.080, -lr * Pred Grad:  0.001, New P: 0.598
iter 17 loss: 0.246
Actual params: [0.8151, 0.598 ]
-Original Grad: 0.013, -lr * Pred Grad:  0.002, New P: 0.818
-Original Grad: -0.031, -lr * Pred Grad:  0.000, New P: 0.598
iter 18 loss: 0.246
Actual params: [0.8175, 0.5984]
-Original Grad: 0.066, -lr * Pred Grad:  0.006, New P: 0.824
-Original Grad: -0.187, -lr * Pred Grad:  -0.000, New P: 0.598
iter 19 loss: 0.247
Actual params: [0.8238, 0.5983]
-Original Grad: 0.044, -lr * Pred Grad:  -0.010, New P: 0.814
-Original Grad: -0.215, -lr * Pred Grad:  -0.005, New P: 0.593
iter 20 loss: 0.246
Actual params: [0.8138, 0.5934]
-Original Grad: 0.018, -lr * Pred Grad:  0.004, New P: 0.817
-Original Grad: -0.035, -lr * Pred Grad:  0.001, New P: 0.594
Target params: [1.1812, 0.2779]
iter 0 loss: 0.445
Actual params: [0.5941, 0.5941]
-Original Grad: -0.137, -lr * Pred Grad:  -0.001, New P: 0.593
-Original Grad: -2.171, -lr * Pred Grad:  -0.016, New P: 0.578
iter 1 loss: 0.405
Actual params: [0.5926, 0.5778]
-Original Grad: -0.170, -lr * Pred Grad:  -0.006, New P: 0.587
-Original Grad: -2.263, -lr * Pred Grad:  -0.008, New P: 0.570
iter 2 loss: 0.387
Actual params: [0.5867, 0.5701]
-Original Grad: -0.223, -lr * Pred Grad:  -0.019, New P: 0.567
-Original Grad: -2.338, -lr * Pred Grad:  -0.004, New P: 0.566
iter 3 loss: 0.364
Actual params: [0.5673, 0.566 ]
-Original Grad: -0.170, -lr * Pred Grad:  -0.002, New P: 0.565
-Original Grad: -1.978, -lr * Pred Grad:  -0.004, New P: 0.562
iter 4 loss: 0.357
Actual params: [0.5651, 0.5624]
-Original Grad: -0.339, -lr * Pred Grad:  -0.030, New P: 0.535
-Original Grad: -2.243, -lr * Pred Grad:  -0.000, New P: 0.562
iter 5 loss: 0.335
Actual params: [0.5348, 0.562 ]
-Original Grad: -0.300, -lr * Pred Grad:  -0.007, New P: 0.528
-Original Grad: -2.580, -lr * Pred Grad:  -0.003, New P: 0.559
iter 6 loss: 0.329
Actual params: [0.5283, 0.5593]
-Original Grad: -0.370, -lr * Pred Grad:  -0.017, New P: 0.511
-Original Grad: -2.451, -lr * Pred Grad:  -0.001, New P: 0.559
iter 7 loss: 0.316
Actual params: [0.5112, 0.5586]
-Original Grad: -0.650, -lr * Pred Grad:  -0.029, New P: 0.483
-Original Grad: -2.803, -lr * Pred Grad:  0.002, New P: 0.560
iter 8 loss: 0.298
Actual params: [0.4827, 0.5602]
-Original Grad: -0.423, -lr * Pred Grad:  -0.000, New P: 0.483
-Original Grad: -2.813, -lr * Pred Grad:  -0.002, New P: 0.558
iter 9 loss: 0.297
Actual params: [0.4826, 0.558 ]
-Original Grad: -0.568, -lr * Pred Grad:  -0.011, New P: 0.472
-Original Grad: -2.316, -lr * Pred Grad:  0.000, New P: 0.558
iter 10 loss: 0.291
Actual params: [0.4718, 0.5581]
-Original Grad: -0.538, -lr * Pred Grad:  -0.003, New P: 0.469
-Original Grad: -2.740, -lr * Pred Grad:  -0.001, New P: 0.557
iter 11 loss: 0.289
Actual params: [0.4687, 0.5568]
-Original Grad: -0.629, -lr * Pred Grad:  -0.010, New P: 0.459
-Original Grad: -2.045, -lr * Pred Grad:  0.001, New P: 0.557
iter 12 loss: 0.284
Actual params: [0.4587, 0.5575]
-Original Grad: -0.728, -lr * Pred Grad:  -0.008, New P: 0.451
-Original Grad: -2.246, -lr * Pred Grad:  0.000, New P: 0.558
iter 13 loss: 0.280
Actual params: [0.451 , 0.5578]
-Original Grad: -0.652, -lr * Pred Grad:  -0.002, New P: 0.449
-Original Grad: -2.734, -lr * Pred Grad:  -0.001, New P: 0.557
iter 14 loss: 0.279
Actual params: [0.4493, 0.5566]
-Original Grad: -0.654, -lr * Pred Grad:  -0.003, New P: 0.446
-Original Grad: -2.453, -lr * Pred Grad:  -0.001, New P: 0.556
iter 15 loss: 0.277
Actual params: [0.4464, 0.556 ]
-Original Grad: -0.698, -lr * Pred Grad:  -0.005, New P: 0.441
-Original Grad: -2.104, -lr * Pred Grad:  0.000, New P: 0.556
iter 16 loss: 0.274
Actual params: [0.4412, 0.5562]
-Original Grad: -0.762, -lr * Pred Grad:  -0.002, New P: 0.439
-Original Grad: -2.640, -lr * Pred Grad:  -0.001, New P: 0.555
iter 17 loss: 0.273
Actual params: [0.439 , 0.5554]
-Original Grad: -0.909, -lr * Pred Grad:  -0.004, New P: 0.435
-Original Grad: -2.560, -lr * Pred Grad:  -0.000, New P: 0.555
iter 18 loss: 0.271
Actual params: [0.4351, 0.5554]
-Original Grad: -0.658, -lr * Pred Grad:  0.002, New P: 0.437
-Original Grad: -2.785, -lr * Pred Grad:  -0.002, New P: 0.554
iter 19 loss: 0.271
Actual params: [0.4367, 0.5537]
-Original Grad: -0.806, -lr * Pred Grad:  -0.000, New P: 0.436
-Original Grad: -2.899, -lr * Pred Grad:  -0.001, New P: 0.553
iter 20 loss: 0.272
Actual params: [0.4363, 0.5527]
-Original Grad: -0.668, -lr * Pred Grad:  -0.004, New P: 0.432
-Original Grad: -1.496, -lr * Pred Grad:  0.001, New P: 0.553
Target params: [1.1812, 0.2779]
iter 0 loss: 1.269
Actual params: [0.5941, 0.5941]
-Original Grad: -0.055, -lr * Pred Grad:  -0.072, New P: 0.522
-Original Grad: -0.177, -lr * Pred Grad:  -0.123, New P: 0.471
iter 1 loss: 1.266
Actual params: [0.5222, 0.471 ]
-Original Grad: 0.091, -lr * Pred Grad:  0.043, New P: 0.565
-Original Grad: -0.571, -lr * Pred Grad:  -0.063, New P: 0.408
iter 2 loss: 1.234
Actual params: [0.5654, 0.4077]
-Original Grad: 0.255, -lr * Pred Grad:  0.114, New P: 0.679
-Original Grad: -0.439, -lr * Pred Grad:  -0.004, New P: 0.404
iter 3 loss: 1.170
Actual params: [0.6791, 0.4036]
-Original Grad: 0.442, -lr * Pred Grad:  0.077, New P: 0.756
-Original Grad: -0.520, -lr * Pred Grad:  0.008, New P: 0.412
iter 4 loss: 1.092
Actual params: [0.7559, 0.4115]
-Original Grad: 0.896, -lr * Pred Grad:  0.043, New P: 0.799
-Original Grad: -0.648, -lr * Pred Grad:  0.013, New P: 0.424
iter 5 loss: 1.003
Actual params: [0.7992, 0.4244]
-Original Grad: 1.062, -lr * Pred Grad:  0.026, New P: 0.825
-Original Grad: -0.666, -lr * Pred Grad:  0.011, New P: 0.435
iter 6 loss: 0.951
Actual params: [0.8253, 0.435 ]
-Original Grad: 1.030, -lr * Pred Grad:  0.017, New P: 0.843
-Original Grad: -0.592, -lr * Pred Grad:  0.008, New P: 0.443
iter 7 loss: 0.919
Actual params: [0.8426, 0.4433]
-Original Grad: 1.027, -lr * Pred Grad:  -0.004, New P: 0.838
-Original Grad: -0.979, -lr * Pred Grad:  -0.018, New P: 0.425
iter 8 loss: 0.916
Actual params: [0.8383, 0.4248]
-Original Grad: 0.998, -lr * Pred Grad:  -0.002, New P: 0.837
-Original Grad: -0.932, -lr * Pred Grad:  -0.012, New P: 0.413
iter 9 loss: 0.911
Actual params: [0.8366, 0.4129]
-Original Grad: 0.982, -lr * Pred Grad:  0.009, New P: 0.845
-Original Grad: -0.702, -lr * Pred Grad:  0.003, New P: 0.416
iter 10 loss: 0.890
Actual params: [0.8455, 0.4161]
-Original Grad: 0.745, -lr * Pred Grad:  0.011, New P: 0.856
-Original Grad: -0.419, -lr * Pred Grad:  0.008, New P: 0.425
iter 11 loss: 0.871
Actual params: [0.8561, 0.4245]
-Original Grad: 1.054, -lr * Pred Grad:  0.002, New P: 0.858
-Original Grad: -0.852, -lr * Pred Grad:  -0.005, New P: 0.420
iter 12 loss: 0.867
Actual params: [0.8583, 0.4199]
-Original Grad: 1.057, -lr * Pred Grad:  0.009, New P: 0.868
-Original Grad: -0.625, -lr * Pred Grad:  0.006, New P: 0.426
iter 13 loss: 0.847
Actual params: [0.8677, 0.4262]
-Original Grad: 1.110, -lr * Pred Grad:  0.011, New P: 0.878
-Original Grad: -0.544, -lr * Pred Grad:  0.009, New P: 0.436
iter 14 loss: 0.829
Actual params: [0.8784, 0.4357]
-Original Grad: 1.077, -lr * Pred Grad:  -0.001, New P: 0.877
-Original Grad: -0.895, -lr * Pred Grad:  -0.008, New P: 0.428
iter 15 loss: 0.826
Actual params: [0.8773, 0.4282]
-Original Grad: 1.095, -lr * Pred Grad:  0.003, New P: 0.880
-Original Grad: -0.786, -lr * Pred Grad:  -0.002, New P: 0.427
iter 16 loss: 0.820
Actual params: [0.8801, 0.4266]
-Original Grad: 1.029, -lr * Pred Grad:  0.004, New P: 0.884
-Original Grad: -0.666, -lr * Pred Grad:  0.001, New P: 0.428
iter 17 loss: 0.808
Actual params: [0.8844, 0.428 ]
-Original Grad: 0.826, -lr * Pred Grad:  0.004, New P: 0.888
-Original Grad: -0.518, -lr * Pred Grad:  0.002, New P: 0.430
iter 18 loss: 0.796
Actual params: [0.8882, 0.4296]
-Original Grad: 1.208, -lr * Pred Grad:  0.005, New P: 0.893
-Original Grad: -0.753, -lr * Pred Grad:  0.002, New P: 0.431
iter 19 loss: 0.790
Actual params: [0.8929, 0.4314]
-Original Grad: 0.862, -lr * Pred Grad:  0.004, New P: 0.897
-Original Grad: -0.481, -lr * Pred Grad:  0.003, New P: 0.434
iter 20 loss: 0.783
Actual params: [0.8973, 0.4345]
-Original Grad: 0.833, -lr * Pred Grad:  0.003, New P: 0.900
-Original Grad: -0.514, -lr * Pred Grad:  0.001, New P: 0.435
Target params: [1.1812, 0.2779]
iter 0 loss: 0.863
Actual params: [0.5941, 0.5941]
-Original Grad: 0.317, -lr * Pred Grad:  0.028, New P: 0.622
-Original Grad: -1.078, -lr * Pred Grad:  -0.028, New P: 0.566
iter 1 loss: 0.845
Actual params: [0.6223, 0.5664]
-Original Grad: 0.383, -lr * Pred Grad:  0.082, New P: 0.704
-Original Grad: -0.917, -lr * Pred Grad:  0.011, New P: 0.578
iter 2 loss: 0.804
Actual params: [0.704 , 0.5776]
-Original Grad: 0.427, -lr * Pred Grad:  0.032, New P: 0.737
-Original Grad: -1.081, -lr * Pred Grad:  -0.001, New P: 0.576
iter 3 loss: 0.782
Actual params: [0.7365, 0.5762]
-Original Grad: 0.418, -lr * Pred Grad:  0.023, New P: 0.759
-Original Grad: -1.060, -lr * Pred Grad:  -0.001, New P: 0.575
iter 4 loss: 0.765
Actual params: [0.7591, 0.5749]
-Original Grad: 0.408, -lr * Pred Grad:  0.035, New P: 0.794
-Original Grad: -0.861, -lr * Pred Grad:  0.007, New P: 0.582
iter 5 loss: 0.743
Actual params: [0.794 , 0.5816]
-Original Grad: 0.438, -lr * Pred Grad:  0.014, New P: 0.808
-Original Grad: -1.054, -lr * Pred Grad:  -0.001, New P: 0.580
iter 6 loss: 0.730
Actual params: [0.8079, 0.5803]
-Original Grad: 0.452, -lr * Pred Grad:  0.027, New P: 0.835
-Original Grad: -0.951, -lr * Pred Grad:  0.006, New P: 0.586
iter 7 loss: 0.702
Actual params: [0.8351, 0.5859]
-Original Grad: 0.548, -lr * Pred Grad:  0.024, New P: 0.859
-Original Grad: -1.120, -lr * Pred Grad:  0.005, New P: 0.591
iter 8 loss: 0.672
Actual params: [0.8592, 0.5908]
-Original Grad: 0.459, -lr * Pred Grad:  0.001, New P: 0.860
-Original Grad: -1.097, -lr * Pred Grad:  -0.004, New P: 0.587
iter 9 loss: 0.671
Actual params: [0.8601, 0.5867]
-Original Grad: 0.478, -lr * Pred Grad:  0.011, New P: 0.871
-Original Grad: -1.032, -lr * Pred Grad:  0.001, New P: 0.587
iter 10 loss: 0.669
Actual params: [0.8711, 0.5874]
-Original Grad: 0.448, -lr * Pred Grad:  0.007, New P: 0.878
-Original Grad: -0.968, -lr * Pred Grad:  -0.000, New P: 0.587
iter 11 loss: 0.660
Actual params: [0.8781, 0.587 ]
-Original Grad: 0.674, -lr * Pred Grad:  0.023, New P: 0.901
-Original Grad: -1.181, -lr * Pred Grad:  0.007, New P: 0.594
iter 12 loss: 0.633
Actual params: [0.9006, 0.5937]
-Original Grad: 0.546, -lr * Pred Grad:  0.004, New P: 0.905
-Original Grad: -1.087, -lr * Pred Grad:  -0.001, New P: 0.593
iter 13 loss: 0.628
Actual params: [0.9051, 0.5926]
-Original Grad: 0.517, -lr * Pred Grad:  -0.007, New P: 0.898
-Original Grad: -1.259, -lr * Pred Grad:  -0.007, New P: 0.586
iter 14 loss: 0.633
Actual params: [0.8976, 0.5859]
-Original Grad: 0.440, -lr * Pred Grad:  0.002, New P: 0.899
-Original Grad: -0.918, -lr * Pred Grad:  -0.002, New P: 0.584
iter 15 loss: 0.630
Actual params: [0.8993, 0.5843]
-Original Grad: 0.440, -lr * Pred Grad:  -0.003, New P: 0.896
-Original Grad: -1.019, -lr * Pred Grad:  -0.004, New P: 0.580
iter 16 loss: 0.632
Actual params: [0.8962, 0.5804]
-Original Grad: 0.505, -lr * Pred Grad:  -0.004, New P: 0.892
-Original Grad: -1.187, -lr * Pred Grad:  -0.005, New P: 0.576
iter 17 loss: 0.635
Actual params: [0.8925, 0.5759]
-Original Grad: 0.486, -lr * Pred Grad:  -0.012, New P: 0.880
-Original Grad: -1.363, -lr * Pred Grad:  -0.008, New P: 0.567
iter 18 loss: 0.648
Actual params: [0.8802, 0.5675]
-Original Grad: 0.370, -lr * Pred Grad:  0.010, New P: 0.890
-Original Grad: -0.612, -lr * Pred Grad:  0.003, New P: 0.570
iter 19 loss: 0.636
Actual params: [0.89  , 0.5705]
-Original Grad: 0.364, -lr * Pred Grad:  0.002, New P: 0.892
-Original Grad: -0.767, -lr * Pred Grad:  -0.001, New P: 0.570
iter 20 loss: 0.633
Actual params: [0.8919, 0.5695]
-Original Grad: 0.444, -lr * Pred Grad:  0.005, New P: 0.896
-Original Grad: -0.882, -lr * Pred Grad:  0.000, New P: 0.570
Target params: [1.1812, 0.2779]
iter 0 loss: 0.885
Actual params: [0.5941, 0.5941]
-Original Grad: -0.040, -lr * Pred Grad:  0.034, New P: 0.628
-Original Grad: -1.129, -lr * Pred Grad:  -0.032, New P: 0.562
iter 1 loss: 0.858
Actual params: [0.6281, 0.5618]
-Original Grad: 0.085, -lr * Pred Grad:  0.063, New P: 0.691
-Original Grad: -1.438, -lr * Pred Grad:  -0.016, New P: 0.546
iter 2 loss: 0.815
Actual params: [0.6912, 0.5459]
-Original Grad: 0.264, -lr * Pred Grad:  0.090, New P: 0.781
-Original Grad: -1.050, -lr * Pred Grad:  -0.003, New P: 0.543
iter 3 loss: 0.723
Actual params: [0.7813, 0.5431]
-Original Grad: 0.666, -lr * Pred Grad:  0.042, New P: 0.824
-Original Grad: -1.608, -lr * Pred Grad:  0.001, New P: 0.544
iter 4 loss: 0.664
Actual params: [0.8237, 0.5438]
-Original Grad: 0.418, -lr * Pred Grad:  0.006, New P: 0.830
-Original Grad: -1.519, -lr * Pred Grad:  -0.005, New P: 0.539
iter 5 loss: 0.648
Actual params: [0.83 , 0.539]
-Original Grad: 0.480, -lr * Pred Grad:  0.014, New P: 0.844
-Original Grad: -0.889, -lr * Pred Grad:  0.000, New P: 0.539
iter 6 loss: 0.621
Actual params: [0.8442, 0.5391]
-Original Grad: 0.572, -lr * Pred Grad:  0.009, New P: 0.853
-Original Grad: -1.352, -lr * Pred Grad:  -0.002, New P: 0.537
iter 7 loss: 0.596
Actual params: [0.8535, 0.5373]
-Original Grad: 0.422, -lr * Pred Grad:  -0.001, New P: 0.853
-Original Grad: -1.656, -lr * Pred Grad:  -0.005, New P: 0.533
iter 8 loss: 0.591
Actual params: [0.8527, 0.5327]
-Original Grad: 0.668, -lr * Pred Grad:  0.009, New P: 0.862
-Original Grad: -1.510, -lr * Pred Grad:  -0.001, New P: 0.532
iter 9 loss: 0.570
Actual params: [0.862 , 0.5319]
-Original Grad: 0.681, -lr * Pred Grad:  0.008, New P: 0.870
-Original Grad: -1.541, -lr * Pred Grad:  -0.001, New P: 0.531
iter 10 loss: 0.556
Actual params: [0.8699, 0.5311]
-Original Grad: 0.547, -lr * Pred Grad:  0.007, New P: 0.877
-Original Grad: -1.000, -lr * Pred Grad:  0.000, New P: 0.531
iter 11 loss: 0.541
Actual params: [0.8772, 0.5313]
-Original Grad: 0.531, -lr * Pred Grad:  0.002, New P: 0.879
-Original Grad: -1.431, -lr * Pred Grad:  -0.002, New P: 0.529
iter 12 loss: 0.534
Actual params: [0.8791, 0.5292]
-Original Grad: 0.657, -lr * Pred Grad:  0.004, New P: 0.883
-Original Grad: -1.530, -lr * Pred Grad:  -0.001, New P: 0.528
iter 13 loss: 0.525
Actual params: [0.8832, 0.5278]
-Original Grad: 0.693, -lr * Pred Grad:  0.005, New P: 0.889
-Original Grad: -1.327, -lr * Pred Grad:  -0.000, New P: 0.527
iter 14 loss: 0.514
Actual params: [0.8885, 0.5274]
-Original Grad: 0.380, -lr * Pred Grad:  0.000, New P: 0.889
-Original Grad: -1.071, -lr * Pred Grad:  -0.002, New P: 0.526
iter 15 loss: 0.514
Actual params: [0.8886, 0.5256]
-Original Grad: 0.685, -lr * Pred Grad:  0.003, New P: 0.892
-Original Grad: -1.524, -lr * Pred Grad:  -0.001, New P: 0.524
iter 16 loss: 0.507
Actual params: [0.8919, 0.5243]
-Original Grad: 0.537, -lr * Pred Grad:  0.003, New P: 0.895
-Original Grad: -1.135, -lr * Pred Grad:  -0.001, New P: 0.523
iter 17 loss: 0.502
Actual params: [0.8945, 0.5234]
-Original Grad: 0.691, -lr * Pred Grad:  0.004, New P: 0.899
-Original Grad: -1.256, -lr * Pred Grad:  -0.000, New P: 0.523
iter 18 loss: 0.493
Actual params: [0.8988, 0.5231]
-Original Grad: 0.638, -lr * Pred Grad:  0.002, New P: 0.900
-Original Grad: -1.458, -lr * Pred Grad:  -0.002, New P: 0.521
iter 19 loss: 0.491
Actual params: [0.9003, 0.5214]
-Original Grad: 0.640, -lr * Pred Grad:  0.002, New P: 0.903
-Original Grad: -1.292, -lr * Pred Grad:  -0.001, New P: 0.520
iter 20 loss: 0.488
Actual params: [0.9028, 0.5204]
-Original Grad: 0.768, -lr * Pred Grad:  0.004, New P: 0.907
-Original Grad: -1.142, -lr * Pred Grad:  -0.000, New P: 0.520
Target params: [1.1812, 0.2779]
iter 0 loss: 0.338
Actual params: [0.5941, 0.5941]
-Original Grad: -0.054, -lr * Pred Grad:  0.017, New P: 0.611
-Original Grad: -0.376, -lr * Pred Grad:  -0.050, New P: 0.544
iter 1 loss: 0.322
Actual params: [0.6106, 0.5437]
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.625
-Original Grad: -0.280, -lr * Pred Grad:  -0.026, New P: 0.518
iter 2 loss: 0.314
Actual params: [0.6252, 0.5178]
-Original Grad: 0.070, -lr * Pred Grad:  0.015, New P: 0.640
-Original Grad: -0.335, -lr * Pred Grad:  -0.021, New P: 0.497
iter 3 loss: 0.307
Actual params: [0.6404, 0.4968]
-Original Grad: 0.086, -lr * Pred Grad:  0.009, New P: 0.650
-Original Grad: -0.181, -lr * Pred Grad:  -0.011, New P: 0.486
iter 4 loss: 0.303
Actual params: [0.6496, 0.486 ]
-Original Grad: 0.011, -lr * Pred Grad:  0.005, New P: 0.655
-Original Grad: -0.335, -lr * Pred Grad:  -0.014, New P: 0.472
iter 5 loss: 0.300
Actual params: [0.655 , 0.4724]
-Original Grad: 0.081, -lr * Pred Grad:  0.007, New P: 0.662
-Original Grad: -0.250, -lr * Pred Grad:  -0.010, New P: 0.463
iter 6 loss: 0.298
Actual params: [0.6621, 0.4625]
-Original Grad: 0.018, -lr * Pred Grad:  0.004, New P: 0.666
-Original Grad: -0.269, -lr * Pred Grad:  -0.009, New P: 0.454
iter 7 loss: 0.296
Actual params: [0.6658, 0.4535]
-Original Grad: -0.023, -lr * Pred Grad:  0.001, New P: 0.667
-Original Grad: -0.236, -lr * Pred Grad:  -0.007, New P: 0.446
iter 8 loss: 0.295
Actual params: [0.6669, 0.4463]
-Original Grad: 0.020, -lr * Pred Grad:  0.004, New P: 0.671
-Original Grad: -0.315, -lr * Pred Grad:  -0.009, New P: 0.437
iter 9 loss: 0.293
Actual params: [0.671 , 0.4373]
-Original Grad: -0.045, -lr * Pred Grad:  0.001, New P: 0.672
-Original Grad: -0.361, -lr * Pred Grad:  -0.009, New P: 0.429
iter 10 loss: 0.292
Actual params: [0.6718, 0.4287]
-Original Grad: 0.132, -lr * Pred Grad:  0.007, New P: 0.679
-Original Grad: -0.155, -lr * Pred Grad:  -0.005, New P: 0.424
iter 11 loss: 0.290
Actual params: [0.6792, 0.4237]
-Original Grad: 0.029, -lr * Pred Grad:  0.003, New P: 0.682
-Original Grad: -0.187, -lr * Pred Grad:  -0.005, New P: 0.419
iter 12 loss: 0.289
Actual params: [0.682 , 0.4187]
-Original Grad: -0.038, -lr * Pred Grad:  0.000, New P: 0.682
-Original Grad: -0.268, -lr * Pred Grad:  -0.006, New P: 0.412
iter 13 loss: 0.288
Actual params: [0.6822, 0.4123]
-Original Grad: 0.033, -lr * Pred Grad:  0.003, New P: 0.686
-Original Grad: -0.228, -lr * Pred Grad:  -0.006, New P: 0.407
iter 14 loss: 0.287
Actual params: [0.6856, 0.4065]
-Original Grad: 0.074, -lr * Pred Grad:  0.004, New P: 0.690
-Original Grad: -0.156, -lr * Pred Grad:  -0.004, New P: 0.402
iter 15 loss: 0.286
Actual params: [0.6899, 0.4021]
-Original Grad: 0.069, -lr * Pred Grad:  0.004, New P: 0.694
-Original Grad: -0.128, -lr * Pred Grad:  -0.004, New P: 0.398
iter 16 loss: 0.285
Actual params: [0.6936, 0.3984]
-Original Grad: -0.027, -lr * Pred Grad:  0.000, New P: 0.694
-Original Grad: -0.229, -lr * Pred Grad:  -0.006, New P: 0.393
iter 17 loss: 0.285
Actual params: [0.6939, 0.3926]
-Original Grad: -0.041, -lr * Pred Grad:  -0.001, New P: 0.693
-Original Grad: -0.183, -lr * Pred Grad:  -0.005, New P: 0.388
iter 18 loss: 0.285
Actual params: [0.6932, 0.3881]
-Original Grad: -0.021, -lr * Pred Grad:  0.000, New P: 0.694
-Original Grad: -0.208, -lr * Pred Grad:  -0.005, New P: 0.383
iter 19 loss: 0.284
Actual params: [0.6936, 0.3829]
-Original Grad: -0.031, -lr * Pred Grad:  -0.000, New P: 0.693
-Original Grad: -0.190, -lr * Pred Grad:  -0.005, New P: 0.378
iter 20 loss: 0.284
Actual params: [0.6933, 0.3781]
-Original Grad: 0.015, -lr * Pred Grad:  0.002, New P: 0.695
-Original Grad: -0.156, -lr * Pred Grad:  -0.004, New P: 0.374
Target params: [1.1812, 0.2779]
iter 0 loss: 0.605
Actual params: [0.5941, 0.5941]
-Original Grad: 0.107, -lr * Pred Grad:  0.097, New P: 0.691
-Original Grad: -0.107, -lr * Pred Grad:  -0.017, New P: 0.577
iter 1 loss: 0.557
Actual params: [0.6914, 0.5771]
-Original Grad: 0.173, -lr * Pred Grad:  0.056, New P: 0.748
-Original Grad: -0.207, -lr * Pred Grad:  -0.013, New P: 0.564
iter 2 loss: 0.515
Actual params: [0.7475, 0.5642]
-Original Grad: 0.221, -lr * Pred Grad:  0.037, New P: 0.785
-Original Grad: 0.053, -lr * Pred Grad:  0.003, New P: 0.567
iter 3 loss: 0.483
Actual params: [0.7846, 0.5667]
-Original Grad: 0.192, -lr * Pred Grad:  0.024, New P: 0.809
-Original Grad: -0.074, -lr * Pred Grad:  -0.006, New P: 0.561
iter 4 loss: 0.455
Actual params: [0.8086, 0.561 ]
-Original Grad: 0.180, -lr * Pred Grad:  0.019, New P: 0.827
-Original Grad: -0.131, -lr * Pred Grad:  -0.007, New P: 0.554
iter 5 loss: 0.431
Actual params: [0.8274, 0.5544]
-Original Grad: 0.158, -lr * Pred Grad:  0.014, New P: 0.842
-Original Grad: -0.182, -lr * Pred Grad:  -0.006, New P: 0.548
iter 6 loss: 0.424
Actual params: [0.8416, 0.5479]
-Original Grad: 0.185, -lr * Pred Grad:  0.014, New P: 0.856
-Original Grad: -0.114, -lr * Pred Grad:  -0.004, New P: 0.544
iter 7 loss: 0.405
Actual params: [0.8555, 0.544 ]
-Original Grad: 0.183, -lr * Pred Grad:  0.012, New P: 0.867
-Original Grad: -0.174, -lr * Pred Grad:  -0.005, New P: 0.539
iter 8 loss: 0.389
Actual params: [0.8672, 0.5391]
-Original Grad: 0.187, -lr * Pred Grad:  0.011, New P: 0.878
-Original Grad: -0.103, -lr * Pred Grad:  -0.003, New P: 0.536
iter 9 loss: 0.374
Actual params: [0.8778, 0.5365]
-Original Grad: 0.176, -lr * Pred Grad:  0.009, New P: 0.887
-Original Grad: -0.105, -lr * Pred Grad:  -0.003, New P: 0.534
iter 10 loss: 0.370
Actual params: [0.887 , 0.5339]
-Original Grad: 0.115, -lr * Pred Grad:  0.005, New P: 0.892
-Original Grad: -0.445, -lr * Pred Grad:  -0.008, New P: 0.525
iter 11 loss: 0.365
Actual params: [0.8916, 0.5255]
-Original Grad: 0.130, -lr * Pred Grad:  0.006, New P: 0.897
-Original Grad: -0.322, -lr * Pred Grad:  -0.005, New P: 0.520
iter 12 loss: 0.359
Actual params: [0.8972, 0.5204]
-Original Grad: 0.223, -lr * Pred Grad:  0.010, New P: 0.908
-Original Grad: -0.053, -lr * Pred Grad:  0.000, New P: 0.521
iter 13 loss: 0.344
Actual params: [0.9077, 0.5205]
-Original Grad: 0.224, -lr * Pred Grad:  0.008, New P: 0.916
-Original Grad: -0.048, -lr * Pred Grad:  -0.000, New P: 0.520
iter 14 loss: 0.332
Actual params: [0.916 , 0.5205]
-Original Grad: 0.135, -lr * Pred Grad:  0.004, New P: 0.920
-Original Grad: -0.154, -lr * Pred Grad:  -0.003, New P: 0.518
iter 15 loss: 0.326
Actual params: [0.9204, 0.518 ]
-Original Grad: 0.133, -lr * Pred Grad:  0.005, New P: 0.925
-Original Grad: -0.146, -lr * Pred Grad:  -0.002, New P: 0.516
iter 16 loss: 0.320
Actual params: [0.925 , 0.5156]
-Original Grad: 0.157, -lr * Pred Grad:  0.005, New P: 0.930
-Original Grad: -0.009, -lr * Pred Grad:  0.000, New P: 0.516
iter 17 loss: 0.321
Actual params: [0.9302, 0.5158]
-Original Grad: 0.153, -lr * Pred Grad:  0.005, New P: 0.935
-Original Grad: -0.118, -lr * Pred Grad:  -0.002, New P: 0.514
iter 18 loss: 0.317
Actual params: [0.935 , 0.5138]
-Original Grad: 0.139, -lr * Pred Grad:  0.004, New P: 0.939
-Original Grad: -0.174, -lr * Pred Grad:  -0.003, New P: 0.511
iter 19 loss: 0.313
Actual params: [0.9392, 0.5109]
-Original Grad: 0.133, -lr * Pred Grad:  0.004, New P: 0.943
-Original Grad: -0.081, -lr * Pred Grad:  -0.001, New P: 0.510
iter 20 loss: 0.308
Actual params: [0.9431, 0.5097]
-Original Grad: 0.184, -lr * Pred Grad:  0.005, New P: 0.948
-Original Grad: -0.072, -lr * Pred Grad:  -0.001, New P: 0.509
Target params: [1.1812, 0.2779]
iter 0 loss: 0.789
Actual params: [0.5941, 0.5941]
-Original Grad: 0.555, -lr * Pred Grad:  0.052, New P: 0.646
-Original Grad: -0.052, -lr * Pred Grad:  -0.024, New P: 0.570
iter 1 loss: 0.759
Actual params: [0.6459, 0.57  ]
-Original Grad: 0.429, -lr * Pred Grad:  0.020, New P: 0.666
-Original Grad: -0.066, -lr * Pred Grad:  -0.053, New P: 0.517
iter 2 loss: 0.741
Actual params: [0.6663, 0.5169]
-Original Grad: 0.699, -lr * Pred Grad:  0.021, New P: 0.688
-Original Grad: -0.023, -lr * Pred Grad:  0.021, New P: 0.538
iter 3 loss: 0.729
Actual params: [0.6877, 0.5382]
-Original Grad: 0.542, -lr * Pred Grad:  0.009, New P: 0.697
-Original Grad: -0.072, -lr * Pred Grad:  -0.055, New P: 0.484
iter 4 loss: 0.717
Actual params: [0.6968, 0.4837]
-Original Grad: 0.689, -lr * Pred Grad:  0.011, New P: 0.708
-Original Grad: -0.056, -lr * Pred Grad:  -0.014, New P: 0.470
iter 5 loss: 0.708
Actual params: [0.708 , 0.4701]
-Original Grad: 0.773, -lr * Pred Grad:  0.011, New P: 0.719
-Original Grad: -0.044, -lr * Pred Grad:  0.006, New P: 0.476
iter 6 loss: 0.699
Actual params: [0.7187, 0.4765]
-Original Grad: 0.791, -lr * Pred Grad:  0.008, New P: 0.727
-Original Grad: -0.049, -lr * Pred Grad:  -0.000, New P: 0.476
iter 7 loss: 0.692
Actual params: [0.7271, 0.476 ]
-Original Grad: 0.695, -lr * Pred Grad:  0.004, New P: 0.731
-Original Grad: -0.081, -lr * Pred Grad:  -0.035, New P: 0.441
iter 8 loss: 0.685
Actual params: [0.7311, 0.4415]
-Original Grad: 0.503, -lr * Pred Grad:  0.007, New P: 0.738
-Original Grad: 0.024, -lr * Pred Grad:  0.048, New P: 0.490
iter 9 loss: 0.683
Actual params: [0.7385, 0.4899]
-Original Grad: 0.740, -lr * Pred Grad:  0.004, New P: 0.742
-Original Grad: -0.087, -lr * Pred Grad:  -0.028, New P: 0.462
iter 10 loss: 0.677
Actual params: [0.7422, 0.4621]
-Original Grad: 0.860, -lr * Pred Grad:  0.006, New P: 0.748
-Original Grad: -0.062, -lr * Pred Grad:  -0.001, New P: 0.461
iter 11 loss: 0.671
Actual params: [0.7479, 0.4609]
-Original Grad: 0.873, -lr * Pred Grad:  0.005, New P: 0.752
-Original Grad: -0.073, -lr * Pred Grad:  -0.007, New P: 0.454
iter 12 loss: 0.666
Actual params: [0.7524, 0.4536]
-Original Grad: 0.752, -lr * Pred Grad:  0.006, New P: 0.758
-Original Grad: -0.014, -lr * Pred Grad:  0.027, New P: 0.481
iter 13 loss: 0.663
Actual params: [0.7581, 0.4809]
-Original Grad: 0.855, -lr * Pred Grad:  0.005, New P: 0.763
-Original Grad: -0.043, -lr * Pred Grad:  0.007, New P: 0.488
iter 14 loss: 0.660
Actual params: [0.7626, 0.4878]
-Original Grad: 0.723, -lr * Pred Grad:  0.001, New P: 0.763
-Original Grad: -0.098, -lr * Pred Grad:  -0.034, New P: 0.454
iter 15 loss: 0.654
Actual params: [0.7635, 0.4536]
-Original Grad: 0.779, -lr * Pred Grad:  0.005, New P: 0.769
-Original Grad: -0.003, -lr * Pred Grad:  0.028, New P: 0.482
iter 16 loss: 0.653
Actual params: [0.7686, 0.4817]
-Original Grad: 0.873, -lr * Pred Grad:  0.002, New P: 0.771
-Original Grad: -0.091, -lr * Pred Grad:  -0.015, New P: 0.467
iter 17 loss: 0.647
Actual params: [0.771 , 0.4671]
-Original Grad: 0.820, -lr * Pred Grad:  0.003, New P: 0.774
-Original Grad: -0.069, -lr * Pred Grad:  -0.005, New P: 0.462
iter 18 loss: 0.643
Actual params: [0.7737, 0.4625]
-Original Grad: 0.887, -lr * Pred Grad:  0.004, New P: 0.777
-Original Grad: -0.044, -lr * Pred Grad:  0.008, New P: 0.471
iter 19 loss: 0.641
Actual params: [0.7773, 0.4707]
-Original Grad: 0.803, -lr * Pred Grad:  0.001, New P: 0.778
-Original Grad: -0.111, -lr * Pred Grad:  -0.022, New P: 0.449
iter 20 loss: 0.636
Actual params: [0.7783, 0.4488]
-Original Grad: 0.908, -lr * Pred Grad:  0.004, New P: 0.783
-Original Grad: -0.014, -lr * Pred Grad:  0.021, New P: 0.470
Target params: [1.1812, 0.2779]
iter 0 loss: 0.743
Actual params: [0.5941, 0.5941]
-Original Grad: 0.127, -lr * Pred Grad:  0.003, New P: 0.597
-Original Grad: -1.234, -lr * Pred Grad:  -0.019, New P: 0.575
iter 1 loss: 0.693
Actual params: [0.5968, 0.5748]
-Original Grad: 0.113, -lr * Pred Grad:  -0.006, New P: 0.591
-Original Grad: -1.133, -lr * Pred Grad:  -0.010, New P: 0.565
iter 2 loss: 0.667
Actual params: [0.591 , 0.5648]
-Original Grad: 0.081, -lr * Pred Grad:  -0.018, New P: 0.573
-Original Grad: -1.037, -lr * Pred Grad:  -0.008, New P: 0.557
iter 3 loss: 0.650
Actual params: [0.5726, 0.5569]
-Original Grad: 0.087, -lr * Pred Grad:  -0.000, New P: 0.572
-Original Grad: -0.929, -lr * Pred Grad:  -0.005, New P: 0.552
iter 4 loss: 0.634
Actual params: [0.5722, 0.5521]
-Original Grad: 0.098, -lr * Pred Grad:  0.005, New P: 0.577
-Original Grad: -0.959, -lr * Pred Grad:  -0.004, New P: 0.548
iter 5 loss: 0.618
Actual params: [0.5771, 0.5485]
-Original Grad: 0.081, -lr * Pred Grad:  -0.004, New P: 0.573
-Original Grad: -0.911, -lr * Pred Grad:  -0.004, New P: 0.545
iter 6 loss: 0.605
Actual params: [0.5726, 0.5445]
-Original Grad: 0.071, -lr * Pred Grad:  -0.008, New P: 0.564
-Original Grad: -0.857, -lr * Pred Grad:  -0.004, New P: 0.541
iter 7 loss: 0.594
Actual params: [0.5641, 0.5406]
-Original Grad: 0.084, -lr * Pred Grad:  0.010, New P: 0.575
-Original Grad: -0.784, -lr * Pred Grad:  -0.002, New P: 0.539
iter 8 loss: 0.581
Actual params: [0.5746, 0.5388]
-Original Grad: 0.086, -lr * Pred Grad:  0.013, New P: 0.587
-Original Grad: -0.777, -lr * Pred Grad:  -0.001, New P: 0.537
iter 9 loss: 0.569
Actual params: [0.5871, 0.5373]
-Original Grad: 0.080, -lr * Pred Grad:  0.006, New P: 0.593
-Original Grad: -0.776, -lr * Pred Grad:  -0.002, New P: 0.535
iter 10 loss: 0.557
Actual params: [0.5927, 0.5353]
-Original Grad: 0.078, -lr * Pred Grad:  0.004, New P: 0.597
-Original Grad: -0.777, -lr * Pred Grad:  -0.002, New P: 0.533
iter 11 loss: 0.545
Actual params: [0.5965, 0.5331]
-Original Grad: 0.098, -lr * Pred Grad:  0.023, New P: 0.620
-Original Grad: -0.752, -lr * Pred Grad:  0.000, New P: 0.533
iter 12 loss: 0.533
Actual params: [0.6197, 0.5332]
-Original Grad: 0.147, -lr * Pred Grad:  0.032, New P: 0.652
-Original Grad: -0.743, -lr * Pred Grad:  0.002, New P: 0.535
iter 13 loss: 0.525
Actual params: [0.6521, 0.5348]
-Original Grad: 0.161, -lr * Pred Grad:  0.023, New P: 0.675
-Original Grad: -0.728, -lr * Pred Grad:  0.001, New P: 0.536
iter 14 loss: 0.517
Actual params: [0.6751, 0.5357]
-Original Grad: 0.180, -lr * Pred Grad:  0.020, New P: 0.695
-Original Grad: -0.710, -lr * Pred Grad:  0.001, New P: 0.536
iter 15 loss: 0.510
Actual params: [0.6949, 0.5364]
-Original Grad: 0.075, -lr * Pred Grad:  -0.002, New P: 0.693
-Original Grad: -0.669, -lr * Pred Grad:  -0.002, New P: 0.534
iter 16 loss: 0.503
Actual params: [0.6929, 0.5342]
-Original Grad: 0.258, -lr * Pred Grad:  0.010, New P: 0.703
-Original Grad: -0.652, -lr * Pred Grad:  -0.000, New P: 0.534
iter 17 loss: 0.497
Actual params: [0.7029, 0.534 ]
-Original Grad: -0.010, -lr * Pred Grad:  -0.005, New P: 0.698
-Original Grad: -0.665, -lr * Pred Grad:  -0.002, New P: 0.531
iter 18 loss: 0.491
Actual params: [0.6979, 0.5315]
-Original Grad: 0.271, -lr * Pred Grad:  0.007, New P: 0.705
-Original Grad: -0.627, -lr * Pred Grad:  -0.001, New P: 0.531
iter 19 loss: 0.485
Actual params: [0.7052, 0.5309]
-Original Grad: 0.186, -lr * Pred Grad:  0.003, New P: 0.708
-Original Grad: -0.657, -lr * Pred Grad:  -0.001, New P: 0.529
iter 20 loss: 0.479
Actual params: [0.7078, 0.5295]
-Original Grad: 0.273, -lr * Pred Grad:  0.004, New P: 0.712
-Original Grad: -0.641, -lr * Pred Grad:  -0.001, New P: 0.528
Target params: [1.1812, 0.2779]
iter 0 loss: 0.596
Actual params: [0.5941, 0.5941]
-Original Grad: 0.595, -lr * Pred Grad:  0.028, New P: 0.622
-Original Grad: 0.212, -lr * Pred Grad:  0.018, New P: 0.612
iter 1 loss: 0.552
Actual params: [0.6218, 0.6116]
-Original Grad: 0.674, -lr * Pred Grad:  0.019, New P: 0.641
-Original Grad: 0.210, -lr * Pred Grad:  -0.009, New P: 0.603
iter 2 loss: 0.539
Actual params: [0.6405, 0.6029]
-Original Grad: 0.840, -lr * Pred Grad:  0.019, New P: 0.660
-Original Grad: 0.229, -lr * Pred Grad:  -0.030, New P: 0.573
iter 3 loss: 0.551
Actual params: [0.6596, 0.5726]
-Original Grad: 0.656, -lr * Pred Grad:  -0.008, New P: 0.652
-Original Grad: 0.251, -lr * Pred Grad:  0.045, New P: 0.618
iter 4 loss: 0.510
Actual params: [0.6517, 0.6179]
-Original Grad: 0.861, -lr * Pred Grad:  0.014, New P: 0.666
-Original Grad: 0.207, -lr * Pred Grad:  -0.031, New P: 0.587
iter 5 loss: 0.526
Actual params: [0.666 , 0.5874]
-Original Grad: 0.609, -lr * Pred Grad:  -0.009, New P: 0.657
-Original Grad: 0.256, -lr * Pred Grad:  0.042, New P: 0.630
iter 6 loss: 0.496
Actual params: [0.6569, 0.6296]
-Original Grad: 0.742, -lr * Pred Grad:  0.007, New P: 0.664
-Original Grad: 0.191, -lr * Pred Grad:  -0.011, New P: 0.618
iter 7 loss: 0.498
Actual params: [0.6639, 0.6181]
-Original Grad: 0.664, -lr * Pred Grad:  0.001, New P: 0.665
-Original Grad: 0.207, -lr * Pred Grad:  0.008, New P: 0.626
iter 8 loss: 0.490
Actual params: [0.6645, 0.6264]
-Original Grad: 0.606, -lr * Pred Grad:  0.001, New P: 0.665
-Original Grad: 0.185, -lr * Pred Grad:  0.006, New P: 0.632
iter 9 loss: 0.481
Actual params: [0.6654, 0.6323]
-Original Grad: 0.669, -lr * Pred Grad:  0.001, New P: 0.666
-Original Grad: 0.201, -lr * Pred Grad:  0.006, New P: 0.638
iter 10 loss: 0.476
Actual params: [0.6664, 0.6382]
-Original Grad: 0.618, -lr * Pred Grad:  0.001, New P: 0.667
-Original Grad: 0.182, -lr * Pred Grad:  0.005, New P: 0.643
iter 11 loss: 0.472
Actual params: [0.6673, 0.6427]
-Original Grad: 0.607, -lr * Pred Grad:  -0.000, New P: 0.667
-Original Grad: 0.194, -lr * Pred Grad:  0.009, New P: 0.652
iter 12 loss: 0.465
Actual params: [0.667 , 0.6516]
-Original Grad: 0.677, -lr * Pred Grad:  0.003, New P: 0.670
-Original Grad: 0.173, -lr * Pred Grad:  -0.002, New P: 0.650
iter 13 loss: 0.463
Actual params: [0.6697, 0.6496]
-Original Grad: 0.624, -lr * Pred Grad:  0.001, New P: 0.671
-Original Grad: 0.171, -lr * Pred Grad:  0.003, New P: 0.652
iter 14 loss: 0.459
Actual params: [0.6709, 0.6521]
-Original Grad: 0.471, -lr * Pred Grad:  -0.003, New P: 0.668
-Original Grad: 0.184, -lr * Pred Grad:  0.016, New P: 0.668
iter 15 loss: 0.449
Actual params: [0.6682, 0.6678]
-Original Grad: 0.683, -lr * Pred Grad:  0.001, New P: 0.670
-Original Grad: 0.189, -lr * Pred Grad:  0.003, New P: 0.671
iter 16 loss: 0.446
Actual params: [0.6695, 0.6706]
-Original Grad: 0.546, -lr * Pred Grad:  -0.001, New P: 0.669
-Original Grad: 0.189, -lr * Pred Grad:  0.009, New P: 0.680
iter 17 loss: 0.441
Actual params: [0.6687, 0.6796]
-Original Grad: 0.640, -lr * Pred Grad:  0.001, New P: 0.669
-Original Grad: 0.186, -lr * Pred Grad:  0.004, New P: 0.684
iter 18 loss: 0.437
Actual params: [0.6694, 0.6837]
-Original Grad: 0.774, -lr * Pred Grad:  0.003, New P: 0.672
-Original Grad: 0.171, -lr * Pred Grad:  -0.003, New P: 0.681
iter 19 loss: 0.435
Actual params: [0.672 , 0.6811]
-Original Grad: 0.543, -lr * Pred Grad:  -0.001, New P: 0.671
-Original Grad: 0.196, -lr * Pred Grad:  0.009, New P: 0.690
iter 20 loss: 0.428
Actual params: [0.6712, 0.6898]
-Original Grad: 0.578, -lr * Pred Grad:  -0.000, New P: 0.671
-Original Grad: 0.204, -lr * Pred Grad:  0.007, New P: 0.697
Target params: [1.1812, 0.2779]
iter 0 loss: 0.722
Actual params: [0.5941, 0.5941]
-Original Grad: 0.144, -lr * Pred Grad:  0.092, New P: 0.686
-Original Grad: -0.313, -lr * Pred Grad:  -0.057, New P: 0.537
iter 1 loss: 0.692
Actual params: [0.6863, 0.5368]
-Original Grad: 0.163, -lr * Pred Grad:  0.041, New P: 0.728
-Original Grad: -0.699, -lr * Pred Grad:  -0.022, New P: 0.514
iter 2 loss: 0.670
Actual params: [0.7278, 0.5143]
-Original Grad: 0.272, -lr * Pred Grad:  0.050, New P: 0.778
-Original Grad: -0.689, -lr * Pred Grad:  -0.008, New P: 0.506
iter 3 loss: 0.656
Actual params: [0.7778, 0.5062]
-Original Grad: 0.343, -lr * Pred Grad:  0.050, New P: 0.827
-Original Grad: -0.398, -lr * Pred Grad:  0.003, New P: 0.509
iter 4 loss: 0.658
Actual params: [0.8275, 0.509 ]
-Original Grad: 0.485, -lr * Pred Grad:  0.032, New P: 0.859
-Original Grad: -0.676, -lr * Pred Grad:  -0.001, New P: 0.508
iter 5 loss: 0.652
Actual params: [0.8594, 0.5082]
-Original Grad: 0.479, -lr * Pred Grad:  0.013, New P: 0.873
-Original Grad: -0.984, -lr * Pred Grad:  -0.005, New P: 0.503
iter 6 loss: 0.635
Actual params: [0.8727, 0.5028]
-Original Grad: 0.576, -lr * Pred Grad:  0.018, New P: 0.891
-Original Grad: -0.776, -lr * Pred Grad:  0.000, New P: 0.503
iter 7 loss: 0.628
Actual params: [0.8908, 0.5032]
-Original Grad: 0.513, -lr * Pred Grad:  0.009, New P: 0.900
-Original Grad: -0.887, -lr * Pred Grad:  -0.003, New P: 0.500
iter 8 loss: 0.617
Actual params: [0.8997, 0.5005]
-Original Grad: 0.551, -lr * Pred Grad:  0.007, New P: 0.907
-Original Grad: -1.036, -lr * Pred Grad:  -0.003, New P: 0.497
iter 9 loss: 0.608
Actual params: [0.9065, 0.4974]
-Original Grad: 0.375, -lr * Pred Grad:  0.002, New P: 0.908
-Original Grad: -0.857, -lr * Pred Grad:  -0.003, New P: 0.494
iter 10 loss: 0.602
Actual params: [0.9084, 0.4939]
-Original Grad: 0.573, -lr * Pred Grad:  0.009, New P: 0.917
-Original Grad: -0.914, -lr * Pred Grad:  -0.001, New P: 0.493
iter 11 loss: 0.597
Actual params: [0.917 , 0.4933]
-Original Grad: 0.565, -lr * Pred Grad:  0.004, New P: 0.921
-Original Grad: -1.126, -lr * Pred Grad:  -0.002, New P: 0.491
iter 12 loss: 0.591
Actual params: [0.9213, 0.4908]
-Original Grad: 0.489, -lr * Pred Grad:  0.002, New P: 0.923
-Original Grad: -1.081, -lr * Pred Grad:  -0.003, New P: 0.488
iter 13 loss: 0.581
Actual params: [0.9234, 0.4879]
-Original Grad: 0.562, -lr * Pred Grad:  0.004, New P: 0.928
-Original Grad: -1.116, -lr * Pred Grad:  -0.002, New P: 0.486
iter 14 loss: 0.573
Actual params: [0.9278, 0.4862]
-Original Grad: 0.572, -lr * Pred Grad:  0.007, New P: 0.935
-Original Grad: -0.947, -lr * Pred Grad:  0.000, New P: 0.486
iter 15 loss: 0.567
Actual params: [0.9351, 0.4864]
-Original Grad: 0.548, -lr * Pred Grad:  0.003, New P: 0.938
-Original Grad: -1.134, -lr * Pred Grad:  -0.002, New P: 0.484
iter 16 loss: 0.560
Actual params: [0.9377, 0.4842]
-Original Grad: 0.572, -lr * Pred Grad:  0.005, New P: 0.943
-Original Grad: -1.069, -lr * Pred Grad:  -0.001, New P: 0.483
iter 17 loss: 0.560
Actual params: [0.9425, 0.4834]
-Original Grad: 0.393, -lr * Pred Grad:  -0.002, New P: 0.940
-Original Grad: -0.996, -lr * Pred Grad:  -0.004, New P: 0.480
iter 18 loss: 0.554
Actual params: [0.9404, 0.4798]
-Original Grad: 0.347, -lr * Pred Grad:  0.001, New P: 0.942
-Original Grad: -0.751, -lr * Pred Grad:  -0.001, New P: 0.478
iter 19 loss: 0.550
Actual params: [0.9416, 0.4783]
-Original Grad: 0.471, -lr * Pred Grad:  0.001, New P: 0.943
-Original Grad: -1.039, -lr * Pred Grad:  -0.002, New P: 0.476
iter 20 loss: 0.545
Actual params: [0.9428, 0.4762]
-Original Grad: 0.535, -lr * Pred Grad:  0.003, New P: 0.946
-Original Grad: -1.089, -lr * Pred Grad:  -0.001, New P: 0.475
Target params: [1.1812, 0.2779]
iter 0 loss: 0.519
Actual params: [0.5941, 0.5941]
-Original Grad: 0.308, -lr * Pred Grad:  0.025, New P: 0.619
-Original Grad: -0.286, -lr * Pred Grad:  -0.025, New P: 0.569
iter 1 loss: 0.515
Actual params: [0.6194, 0.5689]
-Original Grad: 0.339, -lr * Pred Grad:  0.035, New P: 0.655
-Original Grad: -0.227, -lr * Pred Grad:  0.016, New P: 0.585
iter 2 loss: 0.513
Actual params: [0.6548, 0.5845]
-Original Grad: 0.359, -lr * Pred Grad:  0.023, New P: 0.678
-Original Grad: -0.228, -lr * Pred Grad:  0.013, New P: 0.597
iter 3 loss: 0.539
Actual params: [0.6782, 0.5971]
-Original Grad: 0.488, -lr * Pred Grad:  0.006, New P: 0.685
-Original Grad: -0.291, -lr * Pred Grad:  -0.006, New P: 0.591
iter 4 loss: 0.537
Actual params: [0.6845, 0.5907]
-Original Grad: 0.526, -lr * Pred Grad:  0.006, New P: 0.691
-Original Grad: -0.244, -lr * Pred Grad:  -0.001, New P: 0.589
iter 5 loss: 0.540
Actual params: [0.691 , 0.5895]
-Original Grad: 0.373, -lr * Pred Grad:  0.001, New P: 0.692
-Original Grad: -0.298, -lr * Pred Grad:  -0.006, New P: 0.583
iter 6 loss: 0.535
Actual params: [0.6924, 0.5831]
-Original Grad: 0.442, -lr * Pred Grad:  0.003, New P: 0.695
-Original Grad: -0.232, -lr * Pred Grad:  -0.002, New P: 0.581
iter 7 loss: 0.534
Actual params: [0.6953, 0.5806]
-Original Grad: 0.531, -lr * Pred Grad:  0.003, New P: 0.699
-Original Grad: -0.230, -lr * Pred Grad:  -0.001, New P: 0.580
iter 8 loss: 0.536
Actual params: [0.6988, 0.5796]
-Original Grad: 0.321, -lr * Pred Grad:  0.000, New P: 0.699
-Original Grad: -0.320, -lr * Pred Grad:  -0.005, New P: 0.574
iter 9 loss: 0.531
Actual params: [0.6991, 0.5743]
-Original Grad: 0.340, -lr * Pred Grad:  0.001, New P: 0.701
-Original Grad: -0.185, -lr * Pred Grad:  -0.002, New P: 0.572
iter 10 loss: 0.530
Actual params: [0.7005, 0.5724]
-Original Grad: 0.193, -lr * Pred Grad:  -0.000, New P: 0.700
-Original Grad: -0.359, -lr * Pred Grad:  -0.005, New P: 0.568
iter 11 loss: 0.524
Actual params: [0.7004, 0.5676]
-Original Grad: 0.496, -lr * Pred Grad:  0.002, New P: 0.703
-Original Grad: -0.174, -lr * Pred Grad:  -0.001, New P: 0.567
iter 12 loss: 0.525
Actual params: [0.7027, 0.5669]
-Original Grad: 0.490, -lr * Pred Grad:  0.002, New P: 0.705
-Original Grad: -0.172, -lr * Pred Grad:  -0.001, New P: 0.566
iter 13 loss: 0.526
Actual params: [0.7048, 0.5661]
-Original Grad: 0.219, -lr * Pred Grad:  0.000, New P: 0.705
-Original Grad: -0.267, -lr * Pred Grad:  -0.003, New P: 0.563
iter 14 loss: 0.523
Actual params: [0.7052, 0.563 ]
-Original Grad: 0.305, -lr * Pred Grad:  0.001, New P: 0.706
-Original Grad: -0.158, -lr * Pred Grad:  -0.001, New P: 0.562
iter 15 loss: 0.523
Actual params: [0.7062, 0.5616]
-Original Grad: 0.125, -lr * Pred Grad:  0.000, New P: 0.706
-Original Grad: -0.331, -lr * Pred Grad:  -0.003, New P: 0.558
iter 16 loss: 0.522
Actual params: [0.7062, 0.5582]
-Original Grad: 0.238, -lr * Pred Grad:  0.001, New P: 0.707
-Original Grad: -0.229, -lr * Pred Grad:  -0.002, New P: 0.556
iter 17 loss: 0.522
Actual params: [0.707 , 0.5562]
-Original Grad: 0.108, -lr * Pred Grad:  0.000, New P: 0.707
-Original Grad: -0.302, -lr * Pred Grad:  -0.003, New P: 0.554
iter 18 loss: 0.521
Actual params: [0.7072, 0.5536]
-Original Grad: 0.334, -lr * Pred Grad:  0.001, New P: 0.708
-Original Grad: -0.130, -lr * Pred Grad:  -0.001, New P: 0.553
iter 19 loss: 0.521
Actual params: [0.7085, 0.5527]
-Original Grad: 0.383, -lr * Pred Grad:  0.001, New P: 0.710
-Original Grad: -0.125, -lr * Pred Grad:  -0.001, New P: 0.552
iter 20 loss: 0.521
Actual params: [0.7099, 0.5518]
-Original Grad: 0.349, -lr * Pred Grad:  0.001, New P: 0.711
-Original Grad: -0.129, -lr * Pred Grad:  -0.001, New P: 0.551
Target params: [1.1812, 0.2779]
iter 0 loss: 0.729
Actual params: [0.5941, 0.5941]
-Original Grad: -0.110, -lr * Pred Grad:  -0.030, New P: 0.564
-Original Grad: -0.561, -lr * Pred Grad:  -0.043, New P: 0.551
iter 1 loss: 0.669
Actual params: [0.5639, 0.5508]
-Original Grad: -0.103, -lr * Pred Grad:  -0.043, New P: 0.521
-Original Grad: -0.400, -lr * Pred Grad:  -0.016, New P: 0.535
iter 2 loss: 0.636
Actual params: [0.5208, 0.5351]
-Original Grad: -0.100, -lr * Pred Grad:  -0.029, New P: 0.491
-Original Grad: -0.390, -lr * Pred Grad:  -0.012, New P: 0.523
iter 3 loss: 0.604
Actual params: [0.4913, 0.5227]
-Original Grad: -0.039, -lr * Pred Grad:  0.046, New P: 0.538
-Original Grad: -0.347, -lr * Pred Grad:  -0.024, New P: 0.499
iter 4 loss: 0.609
Actual params: [0.5377, 0.499 ]
-Original Grad: -0.089, -lr * Pred Grad:  -0.029, New P: 0.509
-Original Grad: -0.357, -lr * Pred Grad:  -0.008, New P: 0.491
iter 5 loss: 0.587
Actual params: [0.5092, 0.4914]
-Original Grad: -0.064, -lr * Pred Grad:  0.013, New P: 0.522
-Original Grad: -0.358, -lr * Pred Grad:  -0.015, New P: 0.477
iter 6 loss: 0.582
Actual params: [0.522 , 0.4767]
-Original Grad: -0.039, -lr * Pred Grad:  0.035, New P: 0.557
-Original Grad: -0.320, -lr * Pred Grad:  -0.016, New P: 0.460
iter 7 loss: 0.583
Actual params: [0.5569, 0.4602]
-Original Grad: -0.065, -lr * Pred Grad:  -0.017, New P: 0.540
-Original Grad: -0.296, -lr * Pred Grad:  -0.006, New P: 0.455
iter 8 loss: 0.572
Actual params: [0.5398, 0.4546]
-Original Grad: -0.061, -lr * Pred Grad:  -0.009, New P: 0.530
-Original Grad: -0.299, -lr * Pred Grad:  -0.007, New P: 0.448
iter 9 loss: 0.563
Actual params: [0.5304, 0.4478]
-Original Grad: -0.074, -lr * Pred Grad:  -0.018, New P: 0.512
-Original Grad: -0.339, -lr * Pred Grad:  -0.006, New P: 0.442
iter 10 loss: 0.552
Actual params: [0.5124, 0.4422]
-Original Grad: -0.039, -lr * Pred Grad:  0.035, New P: 0.547
-Original Grad: -0.317, -lr * Pred Grad:  -0.014, New P: 0.428
iter 11 loss: 0.556
Actual params: [0.547 , 0.4277]
-Original Grad: -0.081, -lr * Pred Grad:  -0.052, New P: 0.495
-Original Grad: -0.281, -lr * Pred Grad:  0.003, New P: 0.430
iter 12 loss: 0.537
Actual params: [0.4949, 0.4304]
-Original Grad: -0.041, -lr * Pred Grad:  0.025, New P: 0.520
-Original Grad: -0.288, -lr * Pred Grad:  -0.012, New P: 0.418
iter 13 loss: 0.539
Actual params: [0.52  , 0.4185]
-Original Grad: -0.072, -lr * Pred Grad:  -0.041, New P: 0.479
-Original Grad: -0.269, -lr * Pred Grad:  0.001, New P: 0.420
iter 14 loss: 0.523
Actual params: [0.4794, 0.4195]
-Original Grad: -0.021, -lr * Pred Grad:  0.058, New P: 0.537
-Original Grad: -0.283, -lr * Pred Grad:  -0.018, New P: 0.402
iter 15 loss: 0.535
Actual params: [0.5374, 0.402 ]
-Original Grad: -0.060, -lr * Pred Grad:  -0.009, New P: 0.528
-Original Grad: -0.305, -lr * Pred Grad:  -0.006, New P: 0.396
iter 16 loss: 0.528
Actual params: [0.5284, 0.3962]
-Original Grad: -0.059, -lr * Pred Grad:  -0.019, New P: 0.509
-Original Grad: -0.275, -lr * Pred Grad:  -0.003, New P: 0.393
iter 17 loss: 0.519
Actual params: [0.5094, 0.393 ]
-Original Grad: -0.032, -lr * Pred Grad:  0.031, New P: 0.540
-Original Grad: -0.256, -lr * Pred Grad:  -0.012, New P: 0.381
iter 18 loss: 0.523
Actual params: [0.5401, 0.3812]
-Original Grad: -0.072, -lr * Pred Grad:  -0.047, New P: 0.493
-Original Grad: -0.275, -lr * Pred Grad:  0.002, New P: 0.383
iter 19 loss: 0.507
Actual params: [0.4933, 0.3832]
-Original Grad: -0.025, -lr * Pred Grad:  0.043, New P: 0.536
-Original Grad: -0.241, -lr * Pred Grad:  -0.014, New P: 0.369
iter 20 loss: 0.514
Actual params: [0.536 , 0.3695]
-Original Grad: -0.051, -lr * Pred Grad:  -0.002, New P: 0.534
-Original Grad: -0.277, -lr * Pred Grad:  -0.006, New P: 0.363
Target params: [1.1812, 0.2779]
iter 0 loss: 0.763
Actual params: [0.5941, 0.5941]
-Original Grad: 0.602, -lr * Pred Grad:  0.053, New P: 0.647
-Original Grad: 0.017, -lr * Pred Grad:  0.001, New P: 0.595
iter 1 loss: 0.621
Actual params: [0.6468, 0.5949]
-Original Grad: 0.765, -lr * Pred Grad:  0.024, New P: 0.670
-Original Grad: 0.044, -lr * Pred Grad:  0.012, New P: 0.607
iter 2 loss: 0.540
Actual params: [0.6705, 0.6071]
-Original Grad: 0.801, -lr * Pred Grad:  0.014, New P: 0.685
-Original Grad: -0.088, -lr * Pred Grad:  -0.013, New P: 0.594
iter 3 loss: 0.537
Actual params: [0.685 , 0.5942]
-Original Grad: 0.578, -lr * Pred Grad:  0.008, New P: 0.693
-Original Grad: -0.313, -lr * Pred Grad:  -0.009, New P: 0.585
iter 4 loss: 0.538
Actual params: [0.6932, 0.585 ]
-Original Grad: 0.900, -lr * Pred Grad:  0.008, New P: 0.702
-Original Grad: 0.058, -lr * Pred Grad:  -0.001, New P: 0.584
iter 5 loss: 0.522
Actual params: [0.7015, 0.5838]
-Original Grad: 0.804, -lr * Pred Grad:  0.006, New P: 0.708
-Original Grad: -0.049, -lr * Pred Grad:  -0.003, New P: 0.581
iter 6 loss: 0.516
Actual params: [0.7077, 0.5806]
-Original Grad: 0.793, -lr * Pred Grad:  0.005, New P: 0.713
-Original Grad: -0.055, -lr * Pred Grad:  -0.003, New P: 0.578
iter 7 loss: 0.511
Actual params: [0.7129, 0.5778]
-Original Grad: 0.909, -lr * Pred Grad:  0.005, New P: 0.718
-Original Grad: 0.044, -lr * Pred Grad:  -0.001, New P: 0.577
iter 8 loss: 0.504
Actual params: [0.718 , 0.5767]
-Original Grad: 0.509, -lr * Pred Grad:  0.003, New P: 0.721
-Original Grad: -0.381, -lr * Pred Grad:  -0.005, New P: 0.572
iter 9 loss: 0.506
Actual params: [0.7212, 0.5715]
-Original Grad: 0.951, -lr * Pred Grad:  0.004, New P: 0.725
-Original Grad: 0.050, -lr * Pred Grad:  -0.001, New P: 0.571
iter 10 loss: 0.500
Actual params: [0.7255, 0.5709]
-Original Grad: 0.913, -lr * Pred Grad:  0.004, New P: 0.729
-Original Grad: 0.080, -lr * Pred Grad:  -0.000, New P: 0.571
iter 11 loss: 0.496
Actual params: [0.7291, 0.5707]
-Original Grad: 0.596, -lr * Pred Grad:  0.003, New P: 0.732
-Original Grad: -0.260, -lr * Pred Grad:  -0.003, New P: 0.567
iter 12 loss: 0.494
Actual params: [0.7319, 0.5672]
-Original Grad: 0.719, -lr * Pred Grad:  0.003, New P: 0.735
-Original Grad: -0.139, -lr * Pred Grad:  -0.002, New P: 0.565
iter 13 loss: 0.492
Actual params: [0.7347, 0.5651]
-Original Grad: 0.882, -lr * Pred Grad:  0.003, New P: 0.738
-Original Grad: 0.046, -lr * Pred Grad:  -0.000, New P: 0.565
iter 14 loss: 0.488
Actual params: [0.7379, 0.5647]
-Original Grad: 0.814, -lr * Pred Grad:  0.003, New P: 0.741
-Original Grad: -0.033, -lr * Pred Grad:  -0.001, New P: 0.564
iter 15 loss: 0.486
Actual params: [0.7407, 0.5635]
-Original Grad: 0.689, -lr * Pred Grad:  0.002, New P: 0.743
-Original Grad: -0.155, -lr * Pred Grad:  -0.002, New P: 0.561
iter 16 loss: 0.484
Actual params: [0.7431, 0.5614]
-Original Grad: 0.880, -lr * Pred Grad:  0.003, New P: 0.746
-Original Grad: 0.047, -lr * Pred Grad:  -0.000, New P: 0.561
iter 17 loss: 0.481
Actual params: [0.7458, 0.5611]
-Original Grad: 0.852, -lr * Pred Grad:  0.002, New P: 0.748
-Original Grad: 0.069, -lr * Pred Grad:  -0.000, New P: 0.561
iter 18 loss: 0.479
Actual params: [0.7483, 0.561 ]
-Original Grad: 0.873, -lr * Pred Grad:  0.003, New P: 0.751
-Original Grad: 0.048, -lr * Pred Grad:  -0.000, New P: 0.561
iter 19 loss: 0.475
Actual params: [0.7508, 0.5607]
-Original Grad: 0.639, -lr * Pred Grad:  0.002, New P: 0.753
-Original Grad: -0.148, -lr * Pred Grad:  -0.002, New P: 0.558
iter 20 loss: 0.475
Actual params: [0.7528, 0.5584]
-Original Grad: 0.756, -lr * Pred Grad:  0.002, New P: 0.755
-Original Grad: -0.035, -lr * Pred Grad:  -0.001, New P: 0.557
Target params: [1.1812, 0.2779]
iter 0 loss: 0.333
Actual params: [0.5941, 0.5941]
-Original Grad: 0.085, -lr * Pred Grad:  0.032, New P: 0.626
-Original Grad: 0.031, -lr * Pred Grad:  0.015, New P: 0.610
iter 1 loss: 0.338
Actual params: [0.6261, 0.6095]
-Original Grad: 0.061, -lr * Pred Grad:  0.018, New P: 0.644
-Original Grad: 0.026, -lr * Pred Grad:  0.008, New P: 0.617
iter 2 loss: 0.342
Actual params: [0.6441, 0.6175]
-Original Grad: 0.065, -lr * Pred Grad:  0.016, New P: 0.660
-Original Grad: 0.033, -lr * Pred Grad:  0.003, New P: 0.621
iter 3 loss: 0.346
Actual params: [0.6603, 0.6209]
-Original Grad: 0.058, -lr * Pred Grad:  0.013, New P: 0.674
-Original Grad: 0.022, -lr * Pred Grad:  0.001, New P: 0.622
iter 4 loss: 0.351
Actual params: [0.6736, 0.6218]
-Original Grad: 0.063, -lr * Pred Grad:  0.012, New P: 0.685
-Original Grad: 0.036, -lr * Pred Grad:  0.003, New P: 0.625
iter 5 loss: 0.357
Actual params: [0.6852, 0.6247]
-Original Grad: 0.059, -lr * Pred Grad:  0.011, New P: 0.696
-Original Grad: 0.018, -lr * Pred Grad:  -0.001, New P: 0.624
iter 6 loss: 0.363
Actual params: [0.6963, 0.6242]
-Original Grad: 0.038, -lr * Pred Grad:  0.003, New P: 0.700
-Original Grad: 0.046, -lr * Pred Grad:  0.004, New P: 0.628
iter 7 loss: 0.367
Actual params: [0.6998, 0.6284]
-Original Grad: 0.008, -lr * Pred Grad:  -0.001, New P: 0.699
-Original Grad: 0.041, -lr * Pred Grad:  0.004, New P: 0.632
iter 8 loss: 0.369
Actual params: [0.699 , 0.6323]
-Original Grad: 0.020, -lr * Pred Grad:  0.003, New P: 0.702
-Original Grad: -0.010, -lr * Pred Grad:  -0.002, New P: 0.630
iter 9 loss: 0.370
Actual params: [0.7016, 0.6304]
-Original Grad: -0.011, -lr * Pred Grad:  -0.002, New P: 0.699
-Original Grad: 0.037, -lr * Pred Grad:  0.003, New P: 0.634
iter 10 loss: 0.370
Actual params: [0.6991, 0.6339]
-Original Grad: 0.006, -lr * Pred Grad:  -0.001, New P: 0.698
-Original Grad: 0.040, -lr * Pred Grad:  0.003, New P: 0.637
iter 11 loss: 0.371
Actual params: [0.6982, 0.6369]
-Original Grad: 0.018, -lr * Pred Grad:  0.001, New P: 0.699
-Original Grad: 0.006, -lr * Pred Grad:  -0.000, New P: 0.637
iter 12 loss: 0.372
Actual params: [0.6995, 0.6367]
-Original Grad: -0.032, -lr * Pred Grad:  -0.002, New P: 0.697
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: 0.638
iter 13 loss: 0.371
Actual params: [0.6971, 0.6377]
-Original Grad: 0.039, -lr * Pred Grad:  0.002, New P: 0.699
-Original Grad: 0.017, -lr * Pred Grad:  -0.000, New P: 0.638
iter 14 loss: 0.372
Actual params: [0.6993, 0.6377]
-Original Grad: 0.012, -lr * Pred Grad:  0.001, New P: 0.700
-Original Grad: -0.007, -lr * Pred Grad:  -0.001, New P: 0.637
iter 15 loss: 0.372
Actual params: [0.7003, 0.6368]
-Original Grad: -0.004, -lr * Pred Grad:  -0.000, New P: 0.700
-Original Grad: 0.007, -lr * Pred Grad:  0.001, New P: 0.637
iter 16 loss: 0.372
Actual params: [0.6999, 0.6374]
-Original Grad: -0.008, -lr * Pred Grad:  -0.001, New P: 0.699
-Original Grad: 0.026, -lr * Pred Grad:  0.002, New P: 0.639
iter 17 loss: 0.372
Actual params: [0.6986, 0.6393]
-Original Grad: 0.003, -lr * Pred Grad:  0.001, New P: 0.700
-Original Grad: -0.031, -lr * Pred Grad:  -0.002, New P: 0.637
iter 18 loss: 0.372
Actual params: [0.6996, 0.6372]
-Original Grad: 0.014, -lr * Pred Grad:  0.001, New P: 0.701
-Original Grad: -0.010, -lr * Pred Grad:  -0.001, New P: 0.636
iter 19 loss: 0.372
Actual params: [0.7006, 0.6361]
-Original Grad: 0.018, -lr * Pred Grad:  0.000, New P: 0.701
-Original Grad: 0.031, -lr * Pred Grad:  0.002, New P: 0.638
iter 20 loss: 0.373
Actual params: [0.7008, 0.6377]
-Original Grad: 0.062, -lr * Pred Grad:  0.003, New P: 0.703
-Original Grad: 0.034, -lr * Pred Grad:  0.001, New P: 0.638
Target params: [1.1812, 0.2779]
iter 0 loss: 0.324
Actual params: [0.5941, 0.5941]
-Original Grad: 0.346, -lr * Pred Grad:  0.027, New P: 0.621
-Original Grad: -0.126, -lr * Pred Grad:  0.021, New P: 0.615
iter 1 loss: 0.312
Actual params: [0.6209, 0.615 ]
-Original Grad: 0.604, -lr * Pred Grad:  0.015, New P: 0.636
-Original Grad: -0.232, -lr * Pred Grad:  0.012, New P: 0.627
iter 2 loss: 0.307
Actual params: [0.6357, 0.627 ]
-Original Grad: 0.525, -lr * Pred Grad:  0.003, New P: 0.639
-Original Grad: -0.250, -lr * Pred Grad:  -0.005, New P: 0.622
iter 3 loss: 0.304
Actual params: [0.6388, 0.622 ]
-Original Grad: 0.738, -lr * Pred Grad:  0.008, New P: 0.647
-Original Grad: -0.285, -lr * Pred Grad:  0.007, New P: 0.629
iter 4 loss: 0.302
Actual params: [0.6467, 0.6295]
-Original Grad: 0.662, -lr * Pred Grad:  0.006, New P: 0.653
-Original Grad: -0.235, -lr * Pred Grad:  0.008, New P: 0.637
iter 5 loss: 0.301
Actual params: [0.6531, 0.6371]
-Original Grad: 0.439, -lr * Pred Grad:  0.001, New P: 0.654
-Original Grad: -0.184, -lr * Pred Grad:  -0.002, New P: 0.636
iter 6 loss: 0.299
Actual params: [0.6544, 0.6356]
-Original Grad: 0.431, -lr * Pred Grad:  0.001, New P: 0.656
-Original Grad: -0.178, -lr * Pred Grad:  -0.001, New P: 0.634
iter 7 loss: 0.298
Actual params: [0.6558, 0.6345]
-Original Grad: 0.651, -lr * Pred Grad:  0.004, New P: 0.659
-Original Grad: -0.231, -lr * Pred Grad:  0.003, New P: 0.638
iter 8 loss: 0.297
Actual params: [0.6593, 0.6375]
-Original Grad: 0.650, -lr * Pred Grad:  0.003, New P: 0.663
-Original Grad: -0.216, -lr * Pred Grad:  0.004, New P: 0.641
iter 9 loss: 0.296
Actual params: [0.6626, 0.6411]
-Original Grad: 0.601, -lr * Pred Grad:  0.003, New P: 0.666
-Original Grad: -0.196, -lr * Pred Grad:  0.004, New P: 0.645
iter 10 loss: 0.296
Actual params: [0.6655, 0.6446]
-Original Grad: 0.496, -lr * Pred Grad:  0.002, New P: 0.668
-Original Grad: -0.171, -lr * Pred Grad:  0.002, New P: 0.647
iter 11 loss: 0.295
Actual params: [0.6676, 0.6466]
-Original Grad: 0.687, -lr * Pred Grad:  0.002, New P: 0.670
-Original Grad: -0.246, -lr * Pred Grad:  0.002, New P: 0.648
iter 12 loss: 0.294
Actual params: [0.6698, 0.6481]
-Original Grad: 0.510, -lr * Pred Grad:  0.002, New P: 0.672
-Original Grad: -0.177, -lr * Pred Grad:  0.001, New P: 0.650
iter 13 loss: 0.293
Actual params: [0.6715, 0.6496]
-Original Grad: 0.455, -lr * Pred Grad:  0.001, New P: 0.672
-Original Grad: -0.179, -lr * Pred Grad:  -0.001, New P: 0.649
iter 14 loss: 0.293
Actual params: [0.6722, 0.6485]
-Original Grad: 0.562, -lr * Pred Grad:  0.002, New P: 0.674
-Original Grad: -0.206, -lr * Pred Grad:  0.001, New P: 0.649
iter 15 loss: 0.292
Actual params: [0.6737, 0.6493]
-Original Grad: 0.311, -lr * Pred Grad:  0.003, New P: 0.676
-Original Grad: -0.078, -lr * Pred Grad:  0.005, New P: 0.654
iter 16 loss: 0.292
Actual params: [0.6763, 0.6542]
-Original Grad: 0.441, -lr * Pred Grad:  0.004, New P: 0.680
-Original Grad: -0.099, -lr * Pred Grad:  0.007, New P: 0.662
iter 17 loss: 0.292
Actual params: [0.68  , 0.6616]
-Original Grad: 0.576, -lr * Pred Grad:  0.002, New P: 0.682
-Original Grad: -0.203, -lr * Pred Grad:  0.001, New P: 0.662
iter 18 loss: 0.291
Actual params: [0.6816, 0.6625]
-Original Grad: 0.345, -lr * Pred Grad:  -0.001, New P: 0.681
-Original Grad: -0.171, -lr * Pred Grad:  -0.004, New P: 0.659
iter 19 loss: 0.290
Actual params: [0.681 , 0.6587]
-Original Grad: 0.446, -lr * Pred Grad:  0.002, New P: 0.683
-Original Grad: -0.119, -lr * Pred Grad:  0.003, New P: 0.661
iter 20 loss: 0.290
Actual params: [0.6829, 0.6613]
-Original Grad: 0.369, -lr * Pred Grad:  0.001, New P: 0.684
-Original Grad: -0.122, -lr * Pred Grad:  -0.000, New P: 0.661
Target params: [1.1812, 0.2779]
iter 0 loss: 0.541
Actual params: [0.5941, 0.5941]
-Original Grad: 0.300, -lr * Pred Grad:  0.051, New P: 0.645
-Original Grad: -0.391, -lr * Pred Grad:  -0.053, New P: 0.541
iter 1 loss: 0.504
Actual params: [0.6448, 0.5408]
-Original Grad: 0.718, -lr * Pred Grad:  0.033, New P: 0.677
-Original Grad: -0.351, -lr * Pred Grad:  -0.005, New P: 0.536
iter 2 loss: 0.489
Actual params: [0.6773, 0.5356]
-Original Grad: 0.805, -lr * Pred Grad:  0.015, New P: 0.692
-Original Grad: -0.438, -lr * Pred Grad:  -0.009, New P: 0.526
iter 3 loss: 0.477
Actual params: [0.692 , 0.5262]
-Original Grad: 0.828, -lr * Pred Grad:  0.008, New P: 0.700
-Original Grad: -0.494, -lr * Pred Grad:  -0.010, New P: 0.516
iter 4 loss: 0.469
Actual params: [0.7003, 0.5161]
-Original Grad: 0.836, -lr * Pred Grad:  0.010, New P: 0.710
-Original Grad: -0.386, -lr * Pred Grad:  0.001, New P: 0.517
iter 5 loss: 0.465
Actual params: [0.7103, 0.5168]
-Original Grad: 0.811, -lr * Pred Grad:  0.009, New P: 0.719
-Original Grad: -0.289, -lr * Pred Grad:  0.004, New P: 0.521
iter 6 loss: 0.461
Actual params: [0.7193, 0.521 ]
-Original Grad: 0.876, -lr * Pred Grad:  0.004, New P: 0.724
-Original Grad: -0.437, -lr * Pred Grad:  -0.005, New P: 0.516
iter 7 loss: 0.456
Actual params: [0.7236, 0.5158]
-Original Grad: 0.864, -lr * Pred Grad:  0.004, New P: 0.727
-Original Grad: -0.435, -lr * Pred Grad:  -0.004, New P: 0.511
iter 8 loss: 0.454
Actual params: [0.7272, 0.5114]
-Original Grad: 0.790, -lr * Pred Grad:  0.003, New P: 0.730
-Original Grad: -0.396, -lr * Pred Grad:  -0.003, New P: 0.508
iter 9 loss: 0.452
Actual params: [0.7304, 0.508 ]
-Original Grad: 0.947, -lr * Pred Grad:  0.002, New P: 0.732
-Original Grad: -0.531, -lr * Pred Grad:  -0.006, New P: 0.502
iter 10 loss: 0.450
Actual params: [0.7324, 0.5015]
-Original Grad: 0.850, -lr * Pred Grad:  0.003, New P: 0.736
-Original Grad: -0.423, -lr * Pred Grad:  -0.002, New P: 0.500
iter 11 loss: 0.448
Actual params: [0.7356, 0.4996]
-Original Grad: 0.761, -lr * Pred Grad:  0.003, New P: 0.738
-Original Grad: -0.375, -lr * Pred Grad:  -0.001, New P: 0.498
iter 12 loss: 0.447
Actual params: [0.7384, 0.4982]
-Original Grad: 0.874, -lr * Pred Grad:  0.006, New P: 0.744
-Original Grad: -0.287, -lr * Pred Grad:  0.005, New P: 0.504
iter 13 loss: 0.446
Actual params: [0.7444, 0.5036]
-Original Grad: 0.866, -lr * Pred Grad:  0.007, New P: 0.751
-Original Grad: -0.186, -lr * Pred Grad:  0.008, New P: 0.512
iter 14 loss: 0.445
Actual params: [0.7511, 0.512 ]
-Original Grad: 0.953, -lr * Pred Grad:  0.002, New P: 0.753
-Original Grad: -0.455, -lr * Pred Grad:  -0.004, New P: 0.508
iter 15 loss: 0.443
Actual params: [0.753 , 0.5082]
-Original Grad: 1.005, -lr * Pred Grad:  0.003, New P: 0.756
-Original Grad: -0.400, -lr * Pred Grad:  0.000, New P: 0.508
iter 16 loss: 0.442
Actual params: [0.7564, 0.5084]
-Original Grad: 0.609, -lr * Pred Grad:  0.001, New P: 0.757
-Original Grad: -0.315, -lr * Pred Grad:  -0.004, New P: 0.505
iter 17 loss: 0.441
Actual params: [0.757 , 0.5047]
-Original Grad: 0.970, -lr * Pred Grad:  0.002, New P: 0.759
-Original Grad: -0.426, -lr * Pred Grad:  -0.001, New P: 0.503
iter 18 loss: 0.440
Actual params: [0.7594, 0.5033]
-Original Grad: 0.692, -lr * Pred Grad:  0.001, New P: 0.761
-Original Grad: -0.321, -lr * Pred Grad:  -0.002, New P: 0.501
iter 19 loss: 0.439
Actual params: [0.7608, 0.5013]
-Original Grad: 0.361, -lr * Pred Grad:  0.001, New P: 0.762
-Original Grad: -0.139, -lr * Pred Grad:  0.001, New P: 0.502
iter 20 loss: 0.438
Actual params: [0.7623, 0.5019]
-Original Grad: 0.685, -lr * Pred Grad:  0.002, New P: 0.764
-Original Grad: -0.279, -lr * Pred Grad:  -0.000, New P: 0.502
Target params: [1.1812, 0.2779]
iter 0 loss: 0.743
Actual params: [0.5941, 0.5941]
-Original Grad: 0.149, -lr * Pred Grad:  -0.018, New P: 0.576
-Original Grad: -1.321, -lr * Pred Grad:  -0.020, New P: 0.574
iter 1 loss: 0.700
Actual params: [0.5759, 0.5743]
-Original Grad: 0.081, -lr * Pred Grad:  -0.034, New P: 0.542
-Original Grad: -1.043, -lr * Pred Grad:  -0.013, New P: 0.562
iter 2 loss: 0.678
Actual params: [0.5417, 0.5617]
-Original Grad: 0.059, -lr * Pred Grad:  -0.037, New P: 0.504
-Original Grad: -1.060, -lr * Pred Grad:  -0.009, New P: 0.552
iter 3 loss: 0.664
Actual params: [0.5045, 0.5523]
-Original Grad: 0.063, -lr * Pred Grad:  -0.012, New P: 0.493
-Original Grad: -0.878, -lr * Pred Grad:  -0.006, New P: 0.547
iter 4 loss: 0.650
Actual params: [0.4927, 0.5468]
-Original Grad: 0.072, -lr * Pred Grad:  -0.007, New P: 0.485
-Original Grad: -0.966, -lr * Pred Grad:  -0.005, New P: 0.542
iter 5 loss: 0.635
Actual params: [0.4853, 0.542 ]
-Original Grad: 0.070, -lr * Pred Grad:  0.001, New P: 0.486
-Original Grad: -0.868, -lr * Pred Grad:  -0.003, New P: 0.539
iter 6 loss: 0.621
Actual params: [0.4861, 0.5386]
-Original Grad: 0.067, -lr * Pred Grad:  0.003, New P: 0.489
-Original Grad: -0.807, -lr * Pred Grad:  -0.003, New P: 0.536
iter 7 loss: 0.609
Actual params: [0.4887, 0.5358]
-Original Grad: 0.066, -lr * Pred Grad:  0.001, New P: 0.490
-Original Grad: -0.810, -lr * Pred Grad:  -0.003, New P: 0.533
iter 8 loss: 0.598
Actual params: [0.4898, 0.5329]
-Original Grad: 0.056, -lr * Pred Grad:  -0.003, New P: 0.487
-Original Grad: -0.712, -lr * Pred Grad:  -0.003, New P: 0.530
iter 9 loss: 0.588
Actual params: [0.4868, 0.5301]
-Original Grad: 0.075, -lr * Pred Grad:  0.023, New P: 0.510
-Original Grad: -0.770, -lr * Pred Grad:  -0.001, New P: 0.529
iter 10 loss: 0.576
Actual params: [0.5097, 0.5293]
-Original Grad: 0.064, -lr * Pred Grad:  0.001, New P: 0.511
-Original Grad: -0.771, -lr * Pred Grad:  -0.002, New P: 0.527
iter 11 loss: 0.566
Actual params: [0.5112, 0.5269]
-Original Grad: 0.056, -lr * Pred Grad:  -0.007, New P: 0.504
-Original Grad: -0.722, -lr * Pred Grad:  -0.003, New P: 0.524
iter 12 loss: 0.557
Actual params: [0.5041, 0.5239]
-Original Grad: 0.067, -lr * Pred Grad:  0.021, New P: 0.525
-Original Grad: -0.700, -lr * Pred Grad:  -0.000, New P: 0.523
iter 13 loss: 0.543
Actual params: [0.5251, 0.5234]
-Original Grad: 0.072, -lr * Pred Grad:  0.031, New P: 0.556
-Original Grad: -0.691, -lr * Pred Grad:  0.000, New P: 0.524
iter 14 loss: 0.530
Actual params: [0.5564, 0.5239]
-Original Grad: 0.073, -lr * Pred Grad:  0.019, New P: 0.576
-Original Grad: -0.747, -lr * Pred Grad:  -0.001, New P: 0.523
iter 15 loss: 0.517
Actual params: [0.5757, 0.5233]
-Original Grad: 0.073, -lr * Pred Grad:  0.036, New P: 0.612
-Original Grad: -0.665, -lr * Pred Grad:  0.001, New P: 0.524
iter 16 loss: 0.504
Actual params: [0.6115, 0.5244]
-Original Grad: 0.073, -lr * Pred Grad:  0.031, New P: 0.642
-Original Grad: -0.683, -lr * Pred Grad:  0.001, New P: 0.525
iter 17 loss: 0.493
Actual params: [0.6423, 0.525 ]
-Original Grad: 0.110, -lr * Pred Grad:  0.041, New P: 0.683
-Original Grad: -0.664, -lr * Pred Grad:  0.002, New P: 0.527
iter 18 loss: 0.482
Actual params: [0.6832, 0.527 ]
-Original Grad: 0.180, -lr * Pred Grad:  0.035, New P: 0.718
-Original Grad: -0.721, -lr * Pred Grad:  0.002, New P: 0.529
iter 19 loss: 0.472
Actual params: [0.7182, 0.5289]
-Original Grad: 0.087, -lr * Pred Grad:  0.002, New P: 0.720
-Original Grad: -0.652, -lr * Pred Grad:  -0.002, New P: 0.527
iter 20 loss: 0.466
Actual params: [0.7201, 0.5272]
-Original Grad: 0.218, -lr * Pred Grad:  0.007, New P: 0.727
-Original Grad: -0.604, -lr * Pred Grad:  -0.001, New P: 0.526
Target params: [1.1812, 0.2779]
iter 0 loss: 0.288
Actual params: [0.5941, 0.5941]
-Original Grad: 0.772, -lr * Pred Grad:  0.004, New P: 0.598
-Original Grad: -0.482, -lr * Pred Grad:  -0.021, New P: 0.573
iter 1 loss: 0.282
Actual params: [0.5984, 0.5734]
-Original Grad: 0.812, -lr * Pred Grad:  0.014, New P: 0.613
-Original Grad: -0.352, -lr * Pred Grad:  0.013, New P: 0.587
iter 2 loss: 0.280
Actual params: [0.6128, 0.5866]
-Original Grad: 0.525, -lr * Pred Grad:  -0.003, New P: 0.610
-Original Grad: -0.326, -lr * Pred Grad:  -0.014, New P: 0.572
iter 3 loss: 0.278
Actual params: [0.6097, 0.5722]
-Original Grad: 0.791, -lr * Pred Grad:  0.003, New P: 0.612
-Original Grad: -0.415, -lr * Pred Grad:  -0.004, New P: 0.568
iter 4 loss: 0.277
Actual params: [0.6124, 0.5682]
-Original Grad: 0.761, -lr * Pred Grad:  0.004, New P: 0.616
-Original Grad: -0.375, -lr * Pred Grad:  0.001, New P: 0.569
iter 5 loss: 0.276
Actual params: [0.6164, 0.5691]
-Original Grad: 0.709, -lr * Pred Grad:  -0.001, New P: 0.615
-Original Grad: -0.400, -lr * Pred Grad:  -0.008, New P: 0.561
iter 6 loss: 0.275
Actual params: [0.6152, 0.5608]
-Original Grad: 0.731, -lr * Pred Grad:  0.000, New P: 0.616
-Original Grad: -0.397, -lr * Pred Grad:  -0.004, New P: 0.556
iter 7 loss: 0.274
Actual params: [0.6155, 0.5563]
-Original Grad: 0.340, -lr * Pred Grad:  -0.000, New P: 0.615
-Original Grad: -0.192, -lr * Pred Grad:  -0.003, New P: 0.553
iter 8 loss: 0.274
Actual params: [0.6151, 0.5531]
-Original Grad: 0.676, -lr * Pred Grad:  0.003, New P: 0.618
-Original Grad: -0.322, -lr * Pred Grad:  0.002, New P: 0.555
iter 9 loss: 0.273
Actual params: [0.6182, 0.5549]
-Original Grad: 0.620, -lr * Pred Grad:  -0.002, New P: 0.616
-Original Grad: -0.376, -lr * Pred Grad:  -0.008, New P: 0.547
iter 10 loss: 0.272
Actual params: [0.616 , 0.5467]
-Original Grad: 0.642, -lr * Pred Grad:  -0.000, New P: 0.616
-Original Grad: -0.371, -lr * Pred Grad:  -0.004, New P: 0.543
iter 11 loss: 0.272
Actual params: [0.6157, 0.5425]
-Original Grad: 0.689, -lr * Pred Grad:  0.002, New P: 0.618
-Original Grad: -0.353, -lr * Pred Grad:  0.000, New P: 0.543
iter 12 loss: 0.271
Actual params: [0.6178, 0.543 ]
-Original Grad: 0.563, -lr * Pred Grad:  -0.000, New P: 0.618
-Original Grad: -0.335, -lr * Pred Grad:  -0.003, New P: 0.540
iter 13 loss: 0.271
Actual params: [0.6175, 0.5397]
-Original Grad: 0.619, -lr * Pred Grad:  0.000, New P: 0.618
-Original Grad: -0.354, -lr * Pred Grad:  -0.002, New P: 0.538
iter 14 loss: 0.271
Actual params: [0.6179, 0.5376]
-Original Grad: 0.641, -lr * Pred Grad:  0.002, New P: 0.620
-Original Grad: -0.325, -lr * Pred Grad:  0.001, New P: 0.538
iter 15 loss: 0.270
Actual params: [0.62  , 0.5384]
-Original Grad: 0.583, -lr * Pred Grad:  -0.001, New P: 0.619
-Original Grad: -0.363, -lr * Pred Grad:  -0.004, New P: 0.535
iter 16 loss: 0.270
Actual params: [0.6193, 0.5346]
-Original Grad: 0.560, -lr * Pred Grad:  0.002, New P: 0.621
-Original Grad: -0.290, -lr * Pred Grad:  0.000, New P: 0.535
iter 17 loss: 0.270
Actual params: [0.6209, 0.5349]
-Original Grad: 0.540, -lr * Pred Grad:  0.000, New P: 0.621
-Original Grad: -0.325, -lr * Pred Grad:  -0.002, New P: 0.533
iter 18 loss: 0.269
Actual params: [0.6209, 0.5327]
-Original Grad: 0.657, -lr * Pred Grad:  0.003, New P: 0.624
-Original Grad: -0.303, -lr * Pred Grad:  0.003, New P: 0.535
iter 19 loss: 0.269
Actual params: [0.6239, 0.5354]
-Original Grad: 0.528, -lr * Pred Grad:  -0.000, New P: 0.624
-Original Grad: -0.318, -lr * Pred Grad:  -0.002, New P: 0.533
iter 20 loss: 0.269
Actual params: [0.6238, 0.5329]
-Original Grad: 0.487, -lr * Pred Grad:  -0.000, New P: 0.624
-Original Grad: -0.301, -lr * Pred Grad:  -0.002, New P: 0.531
Target params: [1.1812, 0.2779]
iter 0 loss: 0.718
Actual params: [0.5941, 0.5941]
-Original Grad: 0.568, -lr * Pred Grad:  -0.000, New P: 0.594
-Original Grad: -0.700, -lr * Pred Grad:  -0.029, New P: 0.565
iter 1 loss: 0.690
Actual params: [0.5938, 0.5646]
-Original Grad: 0.464, -lr * Pred Grad:  -0.007, New P: 0.586
-Original Grad: -0.618, -lr * Pred Grad:  -0.022, New P: 0.542
iter 2 loss: 0.678
Actual params: [0.5864, 0.5424]
-Original Grad: 0.429, -lr * Pred Grad:  -0.010, New P: 0.577
-Original Grad: -0.700, -lr * Pred Grad:  -0.020, New P: 0.523
iter 3 loss: 0.670
Actual params: [0.5765, 0.5227]
-Original Grad: 0.463, -lr * Pred Grad:  0.006, New P: 0.582
-Original Grad: -0.574, -lr * Pred Grad:  -0.004, New P: 0.518
iter 4 loss: 0.660
Actual params: [0.5823, 0.5183]
-Original Grad: 0.461, -lr * Pred Grad:  -0.001, New P: 0.581
-Original Grad: -0.636, -lr * Pred Grad:  -0.009, New P: 0.510
iter 5 loss: 0.653
Actual params: [0.5812, 0.5096]
-Original Grad: 0.425, -lr * Pred Grad:  0.000, New P: 0.581
-Original Grad: -0.572, -lr * Pred Grad:  -0.006, New P: 0.503
iter 6 loss: 0.647
Actual params: [0.5813, 0.5031]
-Original Grad: 0.427, -lr * Pred Grad:  -0.002, New P: 0.580
-Original Grad: -0.593, -lr * Pred Grad:  -0.007, New P: 0.496
iter 7 loss: 0.643
Actual params: [0.5798, 0.4957]
-Original Grad: 0.443, -lr * Pred Grad:  0.004, New P: 0.584
-Original Grad: -0.559, -lr * Pred Grad:  -0.003, New P: 0.493
iter 8 loss: 0.637
Actual params: [0.5836, 0.4929]
-Original Grad: 0.447, -lr * Pred Grad:  0.009, New P: 0.592
-Original Grad: -0.507, -lr * Pred Grad:  0.002, New P: 0.495
iter 9 loss: 0.631
Actual params: [0.5921, 0.4947]
-Original Grad: 0.421, -lr * Pred Grad:  0.000, New P: 0.592
-Original Grad: -0.545, -lr * Pred Grad:  -0.005, New P: 0.490
iter 10 loss: 0.628
Actual params: [0.5922, 0.4898]
-Original Grad: 0.425, -lr * Pred Grad:  0.004, New P: 0.596
-Original Grad: -0.515, -lr * Pred Grad:  -0.002, New P: 0.488
iter 11 loss: 0.624
Actual params: [0.5959, 0.4881]
-Original Grad: 0.415, -lr * Pred Grad:  0.001, New P: 0.597
-Original Grad: -0.524, -lr * Pred Grad:  -0.004, New P: 0.484
iter 12 loss: 0.621
Actual params: [0.5967, 0.4843]
-Original Grad: 0.420, -lr * Pred Grad:  0.006, New P: 0.602
-Original Grad: -0.490, -lr * Pred Grad:  0.000, New P: 0.485
iter 13 loss: 0.616
Actual params: [0.6023, 0.4846]
-Original Grad: 0.420, -lr * Pred Grad:  0.008, New P: 0.610
-Original Grad: -0.462, -lr * Pred Grad:  0.003, New P: 0.487
iter 14 loss: 0.608
Actual params: [0.6103, 0.4872]
-Original Grad: 0.418, -lr * Pred Grad:  0.001, New P: 0.611
-Original Grad: -0.513, -lr * Pred Grad:  -0.004, New P: 0.483
iter 15 loss: 0.604
Actual params: [0.6108, 0.4835]
-Original Grad: 0.395, -lr * Pred Grad:  -0.009, New P: 0.602
-Original Grad: -0.557, -lr * Pred Grad:  -0.012, New P: 0.472
iter 16 loss: 0.604
Actual params: [0.6018, 0.4718]
-Original Grad: 0.403, -lr * Pred Grad:  0.007, New P: 0.608
-Original Grad: -0.452, -lr * Pred Grad:  0.002, New P: 0.474
iter 17 loss: 0.598
Actual params: [0.6084, 0.4735]
-Original Grad: 0.397, -lr * Pred Grad:  0.012, New P: 0.620
-Original Grad: -0.386, -lr * Pred Grad:  0.007, New P: 0.481
iter 18 loss: 0.590
Actual params: [0.6204, 0.4805]
-Original Grad: 0.390, -lr * Pred Grad:  -0.003, New P: 0.617
-Original Grad: -0.493, -lr * Pred Grad:  -0.007, New P: 0.474
iter 19 loss: 0.588
Actual params: [0.6172, 0.4739]
-Original Grad: 0.406, -lr * Pred Grad:  0.008, New P: 0.626
-Original Grad: -0.424, -lr * Pred Grad:  0.004, New P: 0.478
iter 20 loss: 0.581
Actual params: [0.6256, 0.4777]
-Original Grad: 0.370, -lr * Pred Grad:  0.004, New P: 0.630
-Original Grad: -0.401, -lr * Pred Grad:  0.001, New P: 0.478
Target params: [1.1812, 0.2779]
iter 0 loss: 0.727
Actual params: [0.5941, 0.5941]
-Original Grad: 0.979, -lr * Pred Grad:  0.038, New P: 0.632
-Original Grad: 0.047, -lr * Pred Grad:  0.007, New P: 0.601
iter 1 loss: 0.720
Actual params: [0.6319, 0.6006]
-Original Grad: 0.970, -lr * Pred Grad:  0.019, New P: 0.651
-Original Grad: 0.045, -lr * Pred Grad:  0.005, New P: 0.606
iter 2 loss: 0.720
Actual params: [0.6509, 0.6057]
-Original Grad: 0.928, -lr * Pred Grad:  0.012, New P: 0.663
-Original Grad: 0.050, -lr * Pred Grad:  0.018, New P: 0.624
iter 3 loss: 0.714
Actual params: [0.6632, 0.6239]
-Original Grad: 0.843, -lr * Pred Grad:  0.009, New P: 0.672
-Original Grad: 0.048, -lr * Pred Grad:  0.019, New P: 0.643
iter 4 loss: 0.710
Actual params: [0.6723, 0.6432]
-Original Grad: 0.867, -lr * Pred Grad:  0.009, New P: 0.681
-Original Grad: 0.044, -lr * Pred Grad:  0.007, New P: 0.650
iter 5 loss: 0.706
Actual params: [0.6808, 0.6499]
-Original Grad: 0.767, -lr * Pred Grad:  0.008, New P: 0.689
-Original Grad: 0.029, -lr * Pred Grad:  -0.020, New P: 0.630
iter 6 loss: 0.704
Actual params: [0.689 , 0.6296]
-Original Grad: 0.742, -lr * Pred Grad:  0.005, New P: 0.694
-Original Grad: 0.045, -lr * Pred Grad:  0.030, New P: 0.660
iter 7 loss: 0.700
Actual params: [0.6941, 0.6596]
-Original Grad: 0.564, -lr * Pred Grad:  0.007, New P: 0.701
-Original Grad: 0.006, -lr * Pred Grad:  -0.050, New P: 0.609
iter 8 loss: 0.699
Actual params: [0.7014, 0.6091]
-Original Grad: 0.583, -lr * Pred Grad:  0.007, New P: 0.708
-Original Grad: 0.007, -lr * Pred Grad:  -0.041, New P: 0.568
iter 9 loss: 0.697
Actual params: [0.7084, 0.5681]
-Original Grad: 0.572, -lr * Pred Grad:  0.005, New P: 0.713
-Original Grad: 0.031, -lr * Pred Grad:  0.008, New P: 0.577
iter 10 loss: 0.695
Actual params: [0.713 , 0.5765]
-Original Grad: 0.713, -lr * Pred Grad:  0.006, New P: 0.719
-Original Grad: 0.029, -lr * Pred Grad:  -0.010, New P: 0.567
iter 11 loss: 0.692
Actual params: [0.7193, 0.5666]
-Original Grad: 0.472, -lr * Pred Grad:  0.005, New P: 0.725
-Original Grad: 0.006, -lr * Pred Grad:  -0.032, New P: 0.535
iter 12 loss: 0.689
Actual params: [0.7247, 0.5347]
-Original Grad: 0.510, -lr * Pred Grad:  0.001, New P: 0.726
-Original Grad: 0.087, -lr * Pred Grad:  0.054, New P: 0.589
iter 13 loss: 0.688
Actual params: [0.726 , 0.5891]
-Original Grad: 0.569, -lr * Pred Grad:  0.005, New P: 0.731
-Original Grad: 0.035, -lr * Pred Grad:  0.002, New P: 0.591
iter 14 loss: 0.686
Actual params: [0.7309, 0.5907]
-Original Grad: 0.639, -lr * Pred Grad:  0.006, New P: 0.737
-Original Grad: 0.032, -lr * Pred Grad:  -0.005, New P: 0.586
iter 15 loss: 0.689
Actual params: [0.7366, 0.5858]
-Original Grad: 0.462, -lr * Pred Grad:  0.005, New P: 0.742
-Original Grad: 0.005, -lr * Pred Grad:  -0.024, New P: 0.562
iter 16 loss: 0.687
Actual params: [0.7421, 0.5621]
-Original Grad: 0.658, -lr * Pred Grad:  0.003, New P: 0.745
-Original Grad: 0.071, -lr * Pred Grad:  0.036, New P: 0.598
iter 17 loss: 0.687
Actual params: [0.7455, 0.5984]
-Original Grad: 0.687, -lr * Pred Grad:  0.005, New P: 0.751
-Original Grad: 0.049, -lr * Pred Grad:  0.008, New P: 0.606
iter 18 loss: 0.686
Actual params: [0.7507, 0.606 ]
-Original Grad: 0.453, -lr * Pred Grad:  0.007, New P: 0.757
-Original Grad: -0.016, -lr * Pred Grad:  -0.049, New P: 0.557
iter 19 loss: 0.677
Actual params: [0.7574, 0.5568]
-Original Grad: 0.553, -lr * Pred Grad:  0.006, New P: 0.763
-Original Grad: 0.013, -lr * Pred Grad:  -0.020, New P: 0.536
iter 20 loss: 0.682
Actual params: [0.7633, 0.5364]
-Original Grad: 0.620, -lr * Pred Grad:  0.001, New P: 0.764
-Original Grad: 0.108, -lr * Pred Grad:  0.061, New P: 0.597
Target params: [1.1812, 0.2779]
iter 0 loss: 0.636
Actual params: [0.5941, 0.5941]
-Original Grad: 0.254, -lr * Pred Grad:  0.102, New P: 0.696
-Original Grad: -0.086, -lr * Pred Grad:  -0.033, New P: 0.561
iter 1 loss: 0.571
Actual params: [0.6958, 0.5614]
-Original Grad: 0.336, -lr * Pred Grad:  0.058, New P: 0.754
-Original Grad: -0.081, -lr * Pred Grad:  0.002, New P: 0.563
iter 2 loss: 0.510
Actual params: [0.7543, 0.5633]
-Original Grad: 0.259, -lr * Pred Grad:  0.003, New P: 0.757
-Original Grad: -0.164, -lr * Pred Grad:  -0.090, New P: 0.473
iter 3 loss: 0.570
Actual params: [0.7568, 0.4732]
-Original Grad: 0.511, -lr * Pred Grad:  0.029, New P: 0.786
-Original Grad: -0.072, -lr * Pred Grad:  0.015, New P: 0.488
iter 4 loss: 0.525
Actual params: [0.7857, 0.4879]
-Original Grad: 0.671, -lr * Pred Grad:  0.021, New P: 0.807
-Original Grad: -0.093, -lr * Pred Grad:  0.012, New P: 0.500
iter 5 loss: 0.492
Actual params: [0.807 , 0.4999]
-Original Grad: 0.676, -lr * Pred Grad:  0.015, New P: 0.822
-Original Grad: -0.078, -lr * Pred Grad:  0.015, New P: 0.515
iter 6 loss: 0.474
Actual params: [0.8223, 0.5153]
-Original Grad: 0.523, -lr * Pred Grad:  0.010, New P: 0.833
-Original Grad: -0.054, -lr * Pred Grad:  0.010, New P: 0.525
iter 7 loss: 0.460
Actual params: [0.8326, 0.525 ]
-Original Grad: 0.635, -lr * Pred Grad:  0.010, New P: 0.843
-Original Grad: -0.065, -lr * Pred Grad:  0.009, New P: 0.534
iter 8 loss: 0.446
Actual params: [0.8428, 0.5343]
-Original Grad: 0.658, -lr * Pred Grad:  0.010, New P: 0.853
-Original Grad: 0.002, -lr * Pred Grad:  0.020, New P: 0.554
iter 9 loss: 0.436
Actual params: [0.8526, 0.5544]
-Original Grad: 0.544, -lr * Pred Grad:  0.005, New P: 0.857
-Original Grad: -0.258, -lr * Pred Grad:  -0.016, New P: 0.538
iter 10 loss: 0.439
Actual params: [0.8573, 0.538 ]
-Original Grad: 0.097, -lr * Pred Grad:  0.003, New P: 0.860
-Original Grad: -0.625, -lr * Pred Grad:  -0.012, New P: 0.526
iter 11 loss: 0.441
Actual params: [0.8598, 0.5262]
-Original Grad: 0.255, -lr * Pred Grad:  0.004, New P: 0.863
-Original Grad: -0.457, -lr * Pred Grad:  -0.007, New P: 0.519
iter 12 loss: 0.441
Actual params: [0.8634, 0.5193]
-Original Grad: 0.698, -lr * Pred Grad:  0.005, New P: 0.869
-Original Grad: -0.033, -lr * Pred Grad:  -0.002, New P: 0.517
iter 13 loss: 0.437
Actual params: [0.8687, 0.517 ]
-Original Grad: 0.746, -lr * Pred Grad:  0.005, New P: 0.874
-Original Grad: 0.030, -lr * Pred Grad:  -0.001, New P: 0.516
iter 14 loss: 0.434
Actual params: [0.8736, 0.5158]
-Original Grad: 0.735, -lr * Pred Grad:  0.005, New P: 0.878
-Original Grad: -0.098, -lr * Pred Grad:  -0.003, New P: 0.513
iter 15 loss: 0.430
Actual params: [0.8781, 0.513 ]
-Original Grad: 0.613, -lr * Pred Grad:  0.004, New P: 0.882
-Original Grad: -0.217, -lr * Pred Grad:  -0.004, New P: 0.509
iter 16 loss: 0.429
Actual params: [0.8818, 0.5088]
-Original Grad: 0.655, -lr * Pred Grad:  0.004, New P: 0.885
-Original Grad: -0.023, -lr * Pred Grad:  -0.001, New P: 0.508
iter 17 loss: 0.426
Actual params: [0.8853, 0.5076]
-Original Grad: 0.733, -lr * Pred Grad:  0.004, New P: 0.889
-Original Grad: -0.080, -lr * Pred Grad:  -0.002, New P: 0.506
iter 18 loss: 0.423
Actual params: [0.889 , 0.5056]
-Original Grad: 0.572, -lr * Pred Grad:  0.003, New P: 0.892
-Original Grad: -0.099, -lr * Pred Grad:  -0.002, New P: 0.503
iter 19 loss: 0.421
Actual params: [0.8919, 0.5034]
-Original Grad: 0.643, -lr * Pred Grad:  0.003, New P: 0.895
-Original Grad: -0.017, -lr * Pred Grad:  -0.001, New P: 0.503
iter 20 loss: 0.417
Actual params: [0.8949, 0.5028]
-Original Grad: 0.603, -lr * Pred Grad:  0.003, New P: 0.898
-Original Grad: 0.073, -lr * Pred Grad:  0.001, New P: 0.504
Target params: [1.1812, 0.2779]
iter 0 loss: 0.473
Actual params: [0.5941, 0.5941]
-Original Grad: 0.148, -lr * Pred Grad:  0.027, New P: 0.621
-Original Grad: -0.565, -lr * Pred Grad:  -0.037, New P: 0.557
iter 1 loss: 0.455
Actual params: [0.6207, 0.5569]
-Original Grad: 0.150, -lr * Pred Grad:  0.032, New P: 0.653
-Original Grad: -0.461, -lr * Pred Grad:  -0.011, New P: 0.545
iter 2 loss: 0.435
Actual params: [0.6525, 0.5454]
-Original Grad: 0.251, -lr * Pred Grad:  0.052, New P: 0.705
-Original Grad: -0.572, -lr * Pred Grad:  0.002, New P: 0.547
iter 3 loss: 0.408
Actual params: [0.705, 0.547]
-Original Grad: 0.291, -lr * Pred Grad:  0.033, New P: 0.738
-Original Grad: -0.648, -lr * Pred Grad:  0.000, New P: 0.547
iter 4 loss: 0.388
Actual params: [0.7385, 0.5474]
-Original Grad: 0.291, -lr * Pred Grad:  0.035, New P: 0.773
-Original Grad: -0.584, -lr * Pred Grad:  0.005, New P: 0.552
iter 5 loss: 0.373
Actual params: [0.7731, 0.5523]
-Original Grad: 0.360, -lr * Pred Grad:  0.027, New P: 0.800
-Original Grad: -0.628, -lr * Pred Grad:  0.005, New P: 0.557
iter 6 loss: 0.362
Actual params: [0.8   , 0.5571]
-Original Grad: 0.376, -lr * Pred Grad:  0.020, New P: 0.820
-Original Grad: -0.485, -lr * Pred Grad:  0.006, New P: 0.563
iter 7 loss: 0.355
Actual params: [0.8198, 0.5626]
-Original Grad: 0.495, -lr * Pred Grad:  0.010, New P: 0.830
-Original Grad: -0.709, -lr * Pred Grad:  0.001, New P: 0.563
iter 8 loss: 0.353
Actual params: [0.8301, 0.5633]
-Original Grad: 0.411, -lr * Pred Grad:  0.004, New P: 0.834
-Original Grad: -0.651, -lr * Pred Grad:  -0.002, New P: 0.562
iter 9 loss: 0.352
Actual params: [0.8341, 0.5616]
-Original Grad: 0.335, -lr * Pred Grad:  0.003, New P: 0.837
-Original Grad: -0.529, -lr * Pred Grad:  -0.001, New P: 0.560
iter 10 loss: 0.351
Actual params: [0.837 , 0.5604]
-Original Grad: 0.449, -lr * Pred Grad:  -0.002, New P: 0.835
-Original Grad: -0.831, -lr * Pred Grad:  -0.005, New P: 0.556
iter 11 loss: 0.351
Actual params: [0.835 , 0.5555]
-Original Grad: 0.437, -lr * Pred Grad:  0.005, New P: 0.840
-Original Grad: -0.672, -lr * Pred Grad:  0.000, New P: 0.556
iter 12 loss: 0.349
Actual params: [0.8401, 0.5555]
-Original Grad: 0.324, -lr * Pred Grad:  0.007, New P: 0.848
-Original Grad: -0.391, -lr * Pred Grad:  0.003, New P: 0.558
iter 13 loss: 0.348
Actual params: [0.8476, 0.5582]
-Original Grad: 0.418, -lr * Pred Grad:  -0.001, New P: 0.847
-Original Grad: -0.732, -lr * Pred Grad:  -0.003, New P: 0.555
iter 14 loss: 0.348
Actual params: [0.8468, 0.5549]
-Original Grad: 0.406, -lr * Pred Grad:  -0.002, New P: 0.844
-Original Grad: -0.753, -lr * Pred Grad:  -0.004, New P: 0.551
iter 15 loss: 0.348
Actual params: [0.8443, 0.5508]
-Original Grad: 0.389, -lr * Pred Grad:  0.000, New P: 0.845
-Original Grad: -0.670, -lr * Pred Grad:  -0.002, New P: 0.549
iter 16 loss: 0.347
Actual params: [0.8447, 0.5488]
-Original Grad: 0.291, -lr * Pred Grad:  0.004, New P: 0.849
-Original Grad: -0.420, -lr * Pred Grad:  0.001, New P: 0.550
iter 17 loss: 0.346
Actual params: [0.849 , 0.5498]
-Original Grad: 0.297, -lr * Pred Grad:  0.005, New P: 0.854
-Original Grad: -0.400, -lr * Pred Grad:  0.002, New P: 0.551
iter 18 loss: 0.346
Actual params: [0.8538, 0.5513]
-Original Grad: 0.387, -lr * Pred Grad:  0.000, New P: 0.854
-Original Grad: -0.655, -lr * Pred Grad:  -0.002, New P: 0.549
iter 19 loss: 0.345
Actual params: [0.854 , 0.5492]
-Original Grad: 0.388, -lr * Pred Grad:  0.001, New P: 0.855
-Original Grad: -0.651, -lr * Pred Grad:  -0.002, New P: 0.547
iter 20 loss: 0.345
Actual params: [0.8545, 0.5474]
-Original Grad: 0.381, -lr * Pred Grad:  0.002, New P: 0.856
-Original Grad: -0.617, -lr * Pred Grad:  -0.001, New P: 0.546
Target params: [1.1812, 0.2779]
iter 0 loss: 0.459
Actual params: [0.5941, 0.5941]
-Original Grad: 0.221, -lr * Pred Grad:  0.020, New P: 0.614
-Original Grad: -0.667, -lr * Pred Grad:  -0.022, New P: 0.572
iter 1 loss: 0.415
Actual params: [0.6144, 0.572 ]
-Original Grad: 0.235, -lr * Pred Grad:  0.018, New P: 0.632
-Original Grad: -0.447, -lr * Pred Grad:  -0.008, New P: 0.564
iter 2 loss: 0.397
Actual params: [0.6324, 0.5638]
-Original Grad: 0.174, -lr * Pred Grad:  0.000, New P: 0.633
-Original Grad: -0.903, -lr * Pred Grad:  -0.013, New P: 0.551
iter 3 loss: 0.384
Actual params: [0.6328, 0.5511]
-Original Grad: 0.174, -lr * Pred Grad:  0.005, New P: 0.637
-Original Grad: -0.693, -lr * Pred Grad:  -0.006, New P: 0.545
iter 4 loss: 0.376
Actual params: [0.6374, 0.5447]
-Original Grad: 0.275, -lr * Pred Grad:  0.011, New P: 0.649
-Original Grad: -0.724, -lr * Pred Grad:  -0.004, New P: 0.541
iter 5 loss: 0.365
Actual params: [0.6488, 0.541 ]
-Original Grad: 0.346, -lr * Pred Grad:  0.014, New P: 0.663
-Original Grad: -0.625, -lr * Pred Grad:  -0.001, New P: 0.540
iter 6 loss: 0.356
Actual params: [0.6627, 0.5399]
-Original Grad: 0.332, -lr * Pred Grad:  0.009, New P: 0.671
-Original Grad: -0.684, -lr * Pred Grad:  -0.002, New P: 0.538
iter 7 loss: 0.349
Actual params: [0.6712, 0.538 ]
-Original Grad: 0.330, -lr * Pred Grad:  0.008, New P: 0.679
-Original Grad: -0.532, -lr * Pred Grad:  -0.001, New P: 0.537
iter 8 loss: 0.344
Actual params: [0.6792, 0.5372]
-Original Grad: 0.362, -lr * Pred Grad:  0.005, New P: 0.684
-Original Grad: -0.758, -lr * Pred Grad:  -0.002, New P: 0.535
iter 9 loss: 0.341
Actual params: [0.6842, 0.5348]
-Original Grad: 0.158, -lr * Pred Grad:  -0.001, New P: 0.684
-Original Grad: -0.512, -lr * Pred Grad:  -0.003, New P: 0.532
iter 10 loss: 0.339
Actual params: [0.6836, 0.5319]
-Original Grad: 0.452, -lr * Pred Grad:  0.008, New P: 0.692
-Original Grad: -0.659, -lr * Pred Grad:  -0.000, New P: 0.532
iter 11 loss: 0.339
Actual params: [0.6918, 0.5318]
-Original Grad: 0.326, -lr * Pred Grad:  0.002, New P: 0.694
-Original Grad: -0.726, -lr * Pred Grad:  -0.002, New P: 0.529
iter 12 loss: 0.336
Actual params: [0.694 , 0.5293]
-Original Grad: 0.356, -lr * Pred Grad:  0.004, New P: 0.698
-Original Grad: -0.607, -lr * Pred Grad:  -0.001, New P: 0.528
iter 13 loss: 0.333
Actual params: [0.6978, 0.5281]
-Original Grad: 0.198, -lr * Pred Grad:  -0.001, New P: 0.697
-Original Grad: -0.658, -lr * Pred Grad:  -0.003, New P: 0.525
iter 14 loss: 0.331
Actual params: [0.6966, 0.5251]
-Original Grad: 0.359, -lr * Pred Grad:  0.002, New P: 0.699
-Original Grad: -0.804, -lr * Pred Grad:  -0.002, New P: 0.523
iter 15 loss: 0.329
Actual params: [0.6987, 0.5228]
-Original Grad: 0.299, -lr * Pred Grad:  0.001, New P: 0.700
-Original Grad: -0.703, -lr * Pred Grad:  -0.002, New P: 0.521
iter 16 loss: 0.327
Actual params: [0.7   , 0.5207]
-Original Grad: 0.217, -lr * Pred Grad:  0.002, New P: 0.702
-Original Grad: -0.405, -lr * Pred Grad:  -0.001, New P: 0.520
iter 17 loss: 0.325
Actual params: [0.702, 0.52 ]
-Original Grad: 0.140, -lr * Pred Grad:  0.000, New P: 0.702
-Original Grad: -0.380, -lr * Pred Grad:  -0.001, New P: 0.519
iter 18 loss: 0.324
Actual params: [0.7021, 0.5185]
-Original Grad: 0.201, -lr * Pred Grad:  -0.002, New P: 0.700
-Original Grad: -0.737, -lr * Pred Grad:  -0.003, New P: 0.515
iter 19 loss: 0.323
Actual params: [0.7002, 0.5153]
-Original Grad: 0.315, -lr * Pred Grad:  0.002, New P: 0.702
-Original Grad: -0.746, -lr * Pred Grad:  -0.002, New P: 0.513
iter 20 loss: 0.320
Actual params: [0.7019, 0.5133]
-Original Grad: 0.151, -lr * Pred Grad:  0.000, New P: 0.702
-Original Grad: -0.410, -lr * Pred Grad:  -0.001, New P: 0.512
Target params: [1.1812, 0.2779]
iter 0 loss: 0.606
Actual params: [0.5941, 0.5941]
-Original Grad: 0.199, -lr * Pred Grad:  0.027, New P: 0.622
-Original Grad: -0.953, -lr * Pred Grad:  -0.033, New P: 0.561
iter 1 loss: 0.543
Actual params: [0.6215, 0.5606]
-Original Grad: 0.352, -lr * Pred Grad:  0.033, New P: 0.655
-Original Grad: -1.007, -lr * Pred Grad:  -0.012, New P: 0.549
iter 2 loss: 0.495
Actual params: [0.6545, 0.5491]
-Original Grad: 0.367, -lr * Pred Grad:  0.017, New P: 0.671
-Original Grad: -1.119, -lr * Pred Grad:  -0.010, New P: 0.539
iter 3 loss: 0.466
Actual params: [0.6711, 0.5394]
-Original Grad: 0.301, -lr * Pred Grad:  0.020, New P: 0.691
-Original Grad: -0.658, -lr * Pred Grad:  -0.002, New P: 0.538
iter 4 loss: 0.436
Actual params: [0.6913, 0.5379]
-Original Grad: 0.317, -lr * Pred Grad:  0.017, New P: 0.708
-Original Grad: -0.618, -lr * Pred Grad:  -0.001, New P: 0.536
iter 5 loss: 0.417
Actual params: [0.7078, 0.5365]
-Original Grad: 0.387, -lr * Pred Grad:  0.016, New P: 0.724
-Original Grad: -0.677, -lr * Pred Grad:  -0.001, New P: 0.536
iter 6 loss: 0.397
Actual params: [0.7242, 0.5356]
-Original Grad: 0.391, -lr * Pred Grad:  0.014, New P: 0.738
-Original Grad: -0.652, -lr * Pred Grad:  -0.001, New P: 0.535
iter 7 loss: 0.382
Actual params: [0.7382, 0.5349]
-Original Grad: 0.414, -lr * Pred Grad:  0.007, New P: 0.745
-Original Grad: -0.855, -lr * Pred Grad:  -0.004, New P: 0.531
iter 8 loss: 0.378
Actual params: [0.7454, 0.5306]
-Original Grad: 0.410, -lr * Pred Grad:  0.012, New P: 0.758
-Original Grad: -0.656, -lr * Pred Grad:  0.000, New P: 0.531
iter 9 loss: 0.371
Actual params: [0.7578, 0.5307]
-Original Grad: 0.372, -lr * Pred Grad:  0.008, New P: 0.765
-Original Grad: -0.658, -lr * Pred Grad:  -0.002, New P: 0.529
iter 10 loss: 0.366
Actual params: [0.7654, 0.5289]
-Original Grad: 0.449, -lr * Pred Grad:  0.012, New P: 0.777
-Original Grad: -0.669, -lr * Pred Grad:  0.000, New P: 0.529
iter 11 loss: 0.360
Actual params: [0.7772, 0.5294]
-Original Grad: 0.350, -lr * Pred Grad:  -0.002, New P: 0.775
-Original Grad: -0.802, -lr * Pred Grad:  -0.007, New P: 0.523
iter 12 loss: 0.361
Actual params: [0.775 , 0.5226]
-Original Grad: 0.457, -lr * Pred Grad:  0.014, New P: 0.789
-Original Grad: -0.506, -lr * Pred Grad:  0.003, New P: 0.526
iter 13 loss: 0.355
Actual params: [0.7892, 0.5259]
-Original Grad: 0.477, -lr * Pred Grad:  0.011, New P: 0.800
-Original Grad: -0.564, -lr * Pred Grad:  0.002, New P: 0.528
iter 14 loss: 0.351
Actual params: [0.7999, 0.5276]
-Original Grad: 0.514, -lr * Pred Grad:  0.007, New P: 0.807
-Original Grad: -0.697, -lr * Pred Grad:  -0.001, New P: 0.527
iter 15 loss: 0.348
Actual params: [0.8072, 0.5269]
-Original Grad: 0.522, -lr * Pred Grad:  0.010, New P: 0.817
-Original Grad: -0.561, -lr * Pred Grad:  0.002, New P: 0.529
iter 16 loss: 0.344
Actual params: [0.8174, 0.5292]
-Original Grad: 0.554, -lr * Pred Grad:  0.003, New P: 0.821
-Original Grad: -0.811, -lr * Pred Grad:  -0.003, New P: 0.526
iter 17 loss: 0.343
Actual params: [0.8208, 0.526 ]
-Original Grad: 0.532, -lr * Pred Grad:  0.004, New P: 0.825
-Original Grad: -0.736, -lr * Pred Grad:  -0.002, New P: 0.524
iter 18 loss: 0.342
Actual params: [0.8251, 0.5242]
-Original Grad: 0.483, -lr * Pred Grad:  0.005, New P: 0.830
-Original Grad: -0.625, -lr * Pred Grad:  -0.001, New P: 0.524
iter 19 loss: 0.341
Actual params: [0.8304, 0.5236]
-Original Grad: 0.546, -lr * Pred Grad:  0.006, New P: 0.836
-Original Grad: -0.701, -lr * Pred Grad:  -0.001, New P: 0.523
iter 20 loss: 0.339
Actual params: [0.8361, 0.5231]
-Original Grad: 0.502, -lr * Pred Grad:  0.003, New P: 0.839
-Original Grad: -0.703, -lr * Pred Grad:  -0.002, New P: 0.521
Target params: [1.1812, 0.2779]
iter 0 loss: 0.419
Actual params: [0.5941, 0.5941]
-Original Grad: 0.118, -lr * Pred Grad:  0.017, New P: 0.611
-Original Grad: -0.587, -lr * Pred Grad:  -0.017, New P: 0.577
iter 1 loss: 0.394
Actual params: [0.6113, 0.5772]
-Original Grad: 0.179, -lr * Pred Grad:  0.014, New P: 0.625
-Original Grad: -0.238, -lr * Pred Grad:  -0.005, New P: 0.572
iter 2 loss: 0.382
Actual params: [0.6254, 0.5723]
-Original Grad: 0.159, -lr * Pred Grad:  0.008, New P: 0.633
-Original Grad: -0.133, -lr * Pred Grad:  -0.002, New P: 0.570
iter 3 loss: 0.377
Actual params: [0.6332, 0.5698]
-Original Grad: 0.066, -lr * Pred Grad:  0.002, New P: 0.635
-Original Grad: -0.433, -lr * Pred Grad:  -0.008, New P: 0.562
iter 4 loss: 0.373
Actual params: [0.6347, 0.5622]
-Original Grad: 0.160, -lr * Pred Grad:  0.005, New P: 0.640
-Original Grad: -0.175, -lr * Pred Grad:  -0.002, New P: 0.560
iter 5 loss: 0.369
Actual params: [0.6401, 0.5598]
-Original Grad: 0.084, -lr * Pred Grad:  0.002, New P: 0.642
-Original Grad: -0.307, -lr * Pred Grad:  -0.004, New P: 0.555
iter 6 loss: 0.367
Actual params: [0.6422, 0.5553]
-Original Grad: 0.096, -lr * Pred Grad:  0.002, New P: 0.645
-Original Grad: -0.271, -lr * Pred Grad:  -0.004, New P: 0.552
iter 7 loss: 0.365
Actual params: [0.6447, 0.5516]
-Original Grad: 0.052, -lr * Pred Grad:  0.001, New P: 0.646
-Original Grad: -0.371, -lr * Pred Grad:  -0.004, New P: 0.548
iter 8 loss: 0.363
Actual params: [0.6457, 0.5477]
-Original Grad: 0.146, -lr * Pred Grad:  0.004, New P: 0.650
-Original Grad: -0.153, -lr * Pred Grad:  -0.001, New P: 0.546
iter 9 loss: 0.362
Actual params: [0.6495, 0.5462]
-Original Grad: 0.019, -lr * Pred Grad:  -0.001, New P: 0.649
-Original Grad: -0.683, -lr * Pred Grad:  -0.006, New P: 0.540
iter 10 loss: 0.361
Actual params: [0.649 , 0.5401]
-Original Grad: 0.119, -lr * Pred Grad:  0.003, New P: 0.652
-Original Grad: -0.324, -lr * Pred Grad:  -0.003, New P: 0.538
iter 11 loss: 0.359
Actual params: [0.6519, 0.5376]
-Original Grad: 0.153, -lr * Pred Grad:  0.003, New P: 0.655
-Original Grad: -0.262, -lr * Pred Grad:  -0.002, New P: 0.536
iter 12 loss: 0.357
Actual params: [0.6553, 0.5356]
-Original Grad: 0.138, -lr * Pred Grad:  0.003, New P: 0.658
-Original Grad: -0.155, -lr * Pred Grad:  -0.001, New P: 0.534
iter 13 loss: 0.356
Actual params: [0.6581, 0.5345]
-Original Grad: 0.120, -lr * Pred Grad:  0.002, New P: 0.660
-Original Grad: -0.310, -lr * Pred Grad:  -0.002, New P: 0.532
iter 14 loss: 0.355
Actual params: [0.6603, 0.532 ]
-Original Grad: 0.095, -lr * Pred Grad:  0.002, New P: 0.662
-Original Grad: -0.072, -lr * Pred Grad:  -0.001, New P: 0.531
iter 15 loss: 0.354
Actual params: [0.6622, 0.5314]
-Original Grad: 0.110, -lr * Pred Grad:  0.002, New P: 0.664
-Original Grad: -0.307, -lr * Pred Grad:  -0.002, New P: 0.529
iter 16 loss: 0.353
Actual params: [0.664 , 0.5291]
-Original Grad: 0.093, -lr * Pred Grad:  0.002, New P: 0.666
-Original Grad: -0.294, -lr * Pred Grad:  -0.002, New P: 0.527
iter 17 loss: 0.352
Actual params: [0.6656, 0.5269]
-Original Grad: 0.090, -lr * Pred Grad:  0.001, New P: 0.667
-Original Grad: -0.300, -lr * Pred Grad:  -0.002, New P: 0.525
iter 18 loss: 0.351
Actual params: [0.667 , 0.5247]
-Original Grad: 0.076, -lr * Pred Grad:  0.001, New P: 0.668
-Original Grad: -0.343, -lr * Pred Grad:  -0.002, New P: 0.522
iter 19 loss: 0.351
Actual params: [0.6682, 0.5224]
-Original Grad: 0.094, -lr * Pred Grad:  0.002, New P: 0.670
-Original Grad: -0.217, -lr * Pred Grad:  -0.001, New P: 0.521
iter 20 loss: 0.350
Actual params: [0.6698, 0.521 ]
-Original Grad: 0.069, -lr * Pred Grad:  0.001, New P: 0.671
-Original Grad: -0.117, -lr * Pred Grad:  -0.001, New P: 0.520
Target params: [1.1812, 0.2779]
iter 0 loss: 0.388
Actual params: [0.5941, 0.5941]
-Original Grad: -0.045, -lr * Pred Grad:  -0.024, New P: 0.570
-Original Grad: -0.150, -lr * Pred Grad:  -0.033, New P: 0.561
iter 1 loss: 0.383
Actual params: [0.5701, 0.5609]
-Original Grad: -0.007, -lr * Pred Grad:  0.014, New P: 0.584
-Original Grad: -0.069, -lr * Pred Grad:  -0.019, New P: 0.542
iter 2 loss: 0.375
Actual params: [0.5839, 0.542 ]
-Original Grad: -0.022, -lr * Pred Grad:  0.006, New P: 0.590
-Original Grad: -0.138, -lr * Pred Grad:  -0.019, New P: 0.523
iter 3 loss: 0.367
Actual params: [0.5902, 0.523 ]
-Original Grad: -0.013, -lr * Pred Grad:  0.014, New P: 0.604
-Original Grad: -0.117, -lr * Pred Grad:  -0.015, New P: 0.508
iter 4 loss: 0.355
Actual params: [0.6039, 0.5085]
-Original Grad: -0.024, -lr * Pred Grad:  0.009, New P: 0.612
-Original Grad: -0.154, -lr * Pred Grad:  -0.014, New P: 0.495
iter 5 loss: 0.346
Actual params: [0.6125, 0.4949]
-Original Grad: 0.006, -lr * Pred Grad:  0.033, New P: 0.646
-Original Grad: -0.107, -lr * Pred Grad:  -0.013, New P: 0.482
iter 6 loss: 0.333
Actual params: [0.6457, 0.4822]
-Original Grad: -0.018, -lr * Pred Grad:  0.007, New P: 0.653
-Original Grad: -0.140, -lr * Pred Grad:  -0.009, New P: 0.473
iter 7 loss: 0.326
Actual params: [0.6529, 0.4727]
-Original Grad: -0.002, -lr * Pred Grad:  0.025, New P: 0.678
-Original Grad: -0.128, -lr * Pred Grad:  -0.011, New P: 0.462
iter 8 loss: 0.315
Actual params: [0.6777, 0.4616]
-Original Grad: 0.002, -lr * Pred Grad:  0.024, New P: 0.701
-Original Grad: -0.126, -lr * Pred Grad:  -0.010, New P: 0.451
iter 9 loss: 0.302
Actual params: [0.7014, 0.4511]
-Original Grad: 0.003, -lr * Pred Grad:  0.020, New P: 0.721
-Original Grad: -0.134, -lr * Pred Grad:  -0.010, New P: 0.442
iter 10 loss: 0.290
Actual params: [0.7214, 0.4416]
-Original Grad: 0.027, -lr * Pred Grad:  0.023, New P: 0.745
-Original Grad: -0.088, -lr * Pred Grad:  -0.007, New P: 0.435
iter 11 loss: 0.278
Actual params: [0.7448, 0.4347]
-Original Grad: 0.011, -lr * Pred Grad:  0.015, New P: 0.760
-Original Grad: -0.175, -lr * Pred Grad:  -0.009, New P: 0.426
iter 12 loss: 0.267
Actual params: [0.7602, 0.4261]
-Original Grad: 0.011, -lr * Pred Grad:  0.014, New P: 0.774
-Original Grad: -0.240, -lr * Pred Grad:  -0.009, New P: 0.418
iter 13 loss: 0.257
Actual params: [0.7743, 0.4176]
-Original Grad: 0.027, -lr * Pred Grad:  0.016, New P: 0.790
-Original Grad: -0.210, -lr * Pred Grad:  -0.006, New P: 0.412
iter 14 loss: 0.248
Actual params: [0.7903, 0.4116]
-Original Grad: 0.037, -lr * Pred Grad:  0.016, New P: 0.806
-Original Grad: -0.198, -lr * Pred Grad:  -0.005, New P: 0.407
iter 15 loss: 0.240
Actual params: [0.8065, 0.4071]
-Original Grad: 0.035, -lr * Pred Grad:  0.012, New P: 0.819
-Original Grad: -0.180, -lr * Pred Grad:  -0.003, New P: 0.404
iter 16 loss: 0.234
Actual params: [0.8188, 0.4037]
-Original Grad: 0.051, -lr * Pred Grad:  0.015, New P: 0.834
-Original Grad: -0.184, -lr * Pred Grad:  -0.003, New P: 0.401
iter 17 loss: 0.229
Actual params: [0.8341, 0.401 ]
-Original Grad: 0.026, -lr * Pred Grad:  0.004, New P: 0.838
-Original Grad: -0.184, -lr * Pred Grad:  -0.003, New P: 0.398
iter 18 loss: 0.225
Actual params: [0.8383, 0.3979]
-Original Grad: -0.001, -lr * Pred Grad:  -0.006, New P: 0.832
-Original Grad: -0.256, -lr * Pred Grad:  -0.005, New P: 0.393
iter 19 loss: 0.223
Actual params: [0.832 , 0.3933]
-Original Grad: 0.007, -lr * Pred Grad:  -0.003, New P: 0.829
-Original Grad: -0.255, -lr * Pred Grad:  -0.004, New P: 0.389
iter 20 loss: 0.220
Actual params: [0.8295, 0.3894]
-Original Grad: 0.013, -lr * Pred Grad:  0.000, New P: 0.830
-Original Grad: -0.256, -lr * Pred Grad:  -0.003, New P: 0.386
Target params: [1.1812, 0.2779]
iter 0 loss: 1.328
Actual params: [0.5941, 0.5941]
-Original Grad: 0.387, -lr * Pred Grad:  0.082, New P: 0.676
-Original Grad: -0.041, -lr * Pred Grad:  -0.018, New P: 0.576
iter 1 loss: 1.276
Actual params: [0.6758, 0.5757]
-Original Grad: 0.428, -lr * Pred Grad:  0.047, New P: 0.723
-Original Grad: -0.035, -lr * Pred Grad:  -0.004, New P: 0.572
iter 2 loss: 1.239
Actual params: [0.7227, 0.5719]
-Original Grad: 0.549, -lr * Pred Grad:  0.037, New P: 0.760
-Original Grad: -0.025, -lr * Pred Grad:  0.020, New P: 0.592
iter 3 loss: 1.217
Actual params: [0.7597, 0.5923]
-Original Grad: 0.641, -lr * Pred Grad:  0.029, New P: 0.789
-Original Grad: -0.021, -lr * Pred Grad:  0.028, New P: 0.620
iter 4 loss: 1.191
Actual params: [0.7885, 0.6205]
-Original Grad: 0.836, -lr * Pred Grad:  0.025, New P: 0.813
-Original Grad: -0.011, -lr * Pred Grad:  0.052, New P: 0.673
iter 5 loss: 1.155
Actual params: [0.8134, 0.6727]
-Original Grad: 1.168, -lr * Pred Grad:  0.019, New P: 0.833
-Original Grad: -0.012, -lr * Pred Grad:  0.046, New P: 0.719
iter 6 loss: 1.111
Actual params: [0.8329, 0.7192]
-Original Grad: 1.343, -lr * Pred Grad:  0.014, New P: 0.847
-Original Grad: 0.019, -lr * Pred Grad:  0.083, New P: 0.802
iter 7 loss: 1.052
Actual params: [0.8474, 0.8021]
-Original Grad: 1.622, -lr * Pred Grad:  0.011, New P: 0.858
-Original Grad: 0.043, -lr * Pred Grad:  0.076, New P: 0.878
iter 8 loss: 0.942
Actual params: [0.8581, 0.8776]
-Original Grad: 2.318, -lr * Pred Grad:  0.008, New P: 0.866
-Original Grad: 0.078, -lr * Pred Grad:  0.079, New P: 0.956
iter 9 loss: 0.767
Actual params: [0.866 , 0.9562]
-Original Grad: 3.020, -lr * Pred Grad:  0.005, New P: 0.871
-Original Grad: 0.127, -lr * Pred Grad:  0.066, New P: 1.022
iter 10 loss: 0.908
Actual params: [0.8711, 1.0217]
-Original Grad: 3.560, -lr * Pred Grad:  0.003, New P: 0.874
-Original Grad: 0.177, -lr * Pred Grad:  0.062, New P: 1.084
iter 11 loss: 0.984
Actual params: [0.874 , 1.0841]
-Original Grad: 4.109, -lr * Pred Grad:  0.005, New P: 0.879
-Original Grad: -0.023, -lr * Pred Grad:  -0.068, New P: 1.016
iter 12 loss: 0.904
Actual params: [0.8795, 1.016 ]
-Original Grad: 4.410, -lr * Pred Grad:  0.002, New P: 0.882
-Original Grad: 0.178, -lr * Pred Grad:  0.042, New P: 1.058
iter 13 loss: 0.956
Actual params: [0.8817, 1.0578]
-Original Grad: 5.160, -lr * Pred Grad:  0.000, New P: 0.882
-Original Grad: 0.319, -lr * Pred Grad:  0.063, New P: 1.121
iter 14 loss: 1.008
Actual params: [0.8821, 1.1207]
-Original Grad: 4.343, -lr * Pred Grad:  0.001, New P: 0.883
-Original Grad: 0.267, -lr * Pred Grad:  0.030, New P: 1.151
iter 15 loss: 1.017
Actual params: [0.8831, 1.1508]
-Original Grad: 4.036, -lr * Pred Grad:  0.003, New P: 0.886
-Original Grad: -0.238, -lr * Pred Grad:  -0.029, New P: 1.122
iter 16 loss: 1.009
Actual params: [0.8858, 1.122 ]
-Original Grad: 5.225, -lr * Pred Grad:  0.002, New P: 0.888
-Original Grad: 0.148, -lr * Pred Grad:  -0.002, New P: 1.120
iter 17 loss: 1.009
Actual params: [0.8876, 1.1203]
-Original Grad: 4.065, -lr * Pred Grad:  0.001, New P: 0.889
-Original Grad: 0.209, -lr * Pred Grad:  0.005, New P: 1.126
iter 18 loss: 1.011
Actual params: [0.8887, 1.1255]
-Original Grad: 4.415, -lr * Pred Grad:  0.001, New P: 0.890
-Original Grad: 0.198, -lr * Pred Grad:  0.002, New P: 1.128
iter 19 loss: 1.012
Actual params: [0.89 , 1.128]
-Original Grad: 2.676, -lr * Pred Grad:  0.001, New P: 0.891
-Original Grad: -0.343, -lr * Pred Grad:  -0.017, New P: 1.111
iter 20 loss: 1.005
Actual params: [0.8914, 1.1107]
-Original Grad: 3.491, -lr * Pred Grad:  0.001, New P: 0.893
-Original Grad: -0.127, -lr * Pred Grad:  -0.008, New P: 1.102
Target params: [1.1812, 0.2779]
iter 0 loss: 0.321
Actual params: [0.5941, 0.5941]
-Original Grad: 0.039, -lr * Pred Grad:  -0.008, New P: 0.586
-Original Grad: -0.122, -lr * Pred Grad:  -0.019, New P: 0.575
iter 1 loss: 0.323
Actual params: [0.586 , 0.5752]
-Original Grad: 0.009, -lr * Pred Grad:  -0.011, New P: 0.575
-Original Grad: -0.076, -lr * Pred Grad:  -0.012, New P: 0.563
iter 2 loss: 0.323
Actual params: [0.5747, 0.5631]
-Original Grad: 0.031, -lr * Pred Grad:  -0.008, New P: 0.567
-Original Grad: -0.119, -lr * Pred Grad:  -0.010, New P: 0.554
iter 3 loss: 0.324
Actual params: [0.5671, 0.5536]
-Original Grad: 0.063, -lr * Pred Grad:  0.001, New P: 0.568
-Original Grad: -0.135, -lr * Pred Grad:  -0.004, New P: 0.549
iter 4 loss: 0.324
Actual params: [0.5679, 0.5492]
-Original Grad: 0.051, -lr * Pred Grad:  -0.004, New P: 0.564
-Original Grad: -0.127, -lr * Pred Grad:  -0.005, New P: 0.544
iter 5 loss: 0.324
Actual params: [0.564 , 0.5438]
-Original Grad: 0.053, -lr * Pred Grad:  -0.001, New P: 0.563
-Original Grad: -0.115, -lr * Pred Grad:  -0.003, New P: 0.540
iter 6 loss: 0.324
Actual params: [0.5627, 0.5404]
-Original Grad: 0.071, -lr * Pred Grad:  0.005, New P: 0.568
-Original Grad: -0.102, -lr * Pred Grad:  0.001, New P: 0.541
iter 7 loss: 0.324
Actual params: [0.5678, 0.541 ]
-Original Grad: 0.048, -lr * Pred Grad:  -0.002, New P: 0.566
-Original Grad: -0.100, -lr * Pred Grad:  -0.003, New P: 0.538
iter 8 loss: 0.324
Actual params: [0.5656, 0.5377]
-Original Grad: 0.035, -lr * Pred Grad:  -0.004, New P: 0.561
-Original Grad: -0.092, -lr * Pred Grad:  -0.004, New P: 0.534
iter 9 loss: 0.324
Actual params: [0.5614, 0.5336]
-Original Grad: 0.032, -lr * Pred Grad:  -0.001, New P: 0.560
-Original Grad: -0.066, -lr * Pred Grad:  -0.002, New P: 0.532
iter 10 loss: 0.324
Actual params: [0.5602, 0.5316]
-Original Grad: 0.035, -lr * Pred Grad:  0.001, New P: 0.561
-Original Grad: -0.055, -lr * Pred Grad:  -0.001, New P: 0.531
iter 11 loss: 0.324
Actual params: [0.5612, 0.531 ]
-Original Grad: 0.035, -lr * Pred Grad:  -0.001, New P: 0.560
-Original Grad: -0.066, -lr * Pred Grad:  -0.002, New P: 0.529
iter 12 loss: 0.325
Actual params: [0.5604, 0.5292]
-Original Grad: 0.044, -lr * Pred Grad:  0.000, New P: 0.561
-Original Grad: -0.072, -lr * Pred Grad:  -0.001, New P: 0.528
iter 13 loss: 0.325
Actual params: [0.5608, 0.5279]
-Original Grad: 0.065, -lr * Pred Grad:  0.002, New P: 0.563
-Original Grad: -0.093, -lr * Pred Grad:  -0.001, New P: 0.527
iter 14 loss: 0.325
Actual params: [0.5626, 0.5272]
-Original Grad: 0.047, -lr * Pred Grad:  0.000, New P: 0.563
-Original Grad: -0.072, -lr * Pred Grad:  -0.001, New P: 0.526
iter 15 loss: 0.325
Actual params: [0.563, 0.526]
-Original Grad: 0.042, -lr * Pred Grad:  -0.002, New P: 0.561
-Original Grad: -0.078, -lr * Pred Grad:  -0.003, New P: 0.523
iter 16 loss: 0.325
Actual params: [0.5611, 0.5233]
-Original Grad: 0.044, -lr * Pred Grad:  0.000, New P: 0.561
-Original Grad: -0.067, -lr * Pred Grad:  -0.001, New P: 0.522
iter 17 loss: 0.325
Actual params: [0.5615, 0.5223]
-Original Grad: 0.020, -lr * Pred Grad:  -0.000, New P: 0.561
-Original Grad: -0.032, -lr * Pred Grad:  -0.001, New P: 0.521
iter 18 loss: 0.325
Actual params: [0.5613, 0.5215]
-Original Grad: 0.037, -lr * Pred Grad:  -0.000, New P: 0.561
-Original Grad: -0.058, -lr * Pred Grad:  -0.001, New P: 0.520
iter 19 loss: 0.325
Actual params: [0.5613, 0.5201]
-Original Grad: 0.033, -lr * Pred Grad:  -0.002, New P: 0.560
-Original Grad: -0.062, -lr * Pred Grad:  -0.002, New P: 0.518
iter 20 loss: 0.325
Actual params: [0.5597, 0.5178]
-Original Grad: 0.026, -lr * Pred Grad:  -0.001, New P: 0.559
-Original Grad: -0.046, -lr * Pred Grad:  -0.001, New P: 0.516
