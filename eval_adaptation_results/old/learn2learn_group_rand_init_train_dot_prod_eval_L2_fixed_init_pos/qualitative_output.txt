Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.412, -lr * Pred Grad: 0.050, New P: 0.644
-Original Grad: 0.105, -lr * Pred Grad: 0.046, New P: 0.640
iter 0 loss: 0.351
Actual params: [0.644 , 0.6404]
-Original Grad: 0.447, -lr * Pred Grad: 0.063, New P: 0.707
-Original Grad: -0.057, -lr * Pred Grad: 0.062, New P: 0.703
iter 1 loss: 0.321
Actual params: [0.7069, 0.7026]
-Original Grad: 1.448, -lr * Pred Grad: 0.065, New P: 0.772
-Original Grad: -0.004, -lr * Pred Grad: 0.059, New P: 0.762
iter 2 loss: 0.277
Actual params: [0.7719, 0.762 ]
-Original Grad: 2.612, -lr * Pred Grad: 0.065, New P: 0.837
-Original Grad: -0.547, -lr * Pred Grad: 0.058, New P: 0.820
iter 3 loss: 0.188
Actual params: [0.837, 0.82 ]
-Original Grad: 1.112, -lr * Pred Grad: 0.065, New P: 0.902
-Original Grad: -0.436, -lr * Pred Grad: 0.050, New P: 0.870
iter 4 loss: 0.089
Actual params: [0.9023, 0.8704]
-Original Grad: -2.694, -lr * Pred Grad: 0.065, New P: 0.968
-Original Grad: 1.072, -lr * Pred Grad: 0.046, New P: 0.916
iter 5 loss: 0.154
Actual params: [0.9675, 0.9164]
-Original Grad: -1.259, -lr * Pred Grad: 0.065, New P: 1.033
-Original Grad: 0.474, -lr * Pred Grad: 0.023, New P: 0.940
iter 6 loss: 0.240
Actual params: [1.0327, 0.9398]
-Original Grad: -0.477, -lr * Pred Grad: 0.065, New P: 1.098
-Original Grad: 0.487, -lr * Pred Grad: -0.020, New P: 0.920
iter 7 loss: 0.283
Actual params: [1.098 , 0.9202]
-Original Grad: -0.343, -lr * Pred Grad: 0.065, New P: 1.163
-Original Grad: 0.047, -lr * Pred Grad: -0.050, New P: 0.870
iter 8 loss: 0.297
Actual params: [1.1632, 0.8705]
-Original Grad: 0.484, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: 0.183, -lr * Pred Grad: -0.060, New P: 0.811
iter 9 loss: 0.271
Actual params: [1.2284, 0.8109]
-Original Grad: 2.129, -lr * Pred Grad: 0.065, New P: 1.294
-Original Grad: 0.355, -lr * Pred Grad: -0.062, New P: 0.749
iter 10 loss: 0.210
Actual params: [1.2937, 0.7491]
-Original Grad: 0.776, -lr * Pred Grad: 0.065, New P: 1.359
-Original Grad: -0.114, -lr * Pred Grad: -0.062, New P: 0.687
iter 11 loss: 0.156
Actual params: [1.3589, 0.6869]
-Original Grad: 0.921, -lr * Pred Grad: 0.065, New P: 1.424
-Original Grad: -0.502, -lr * Pred Grad: -0.062, New P: 0.625
iter 12 loss: 0.091
Actual params: [1.4241, 0.6246]
-Original Grad: -0.305, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: 0.132, -lr * Pred Grad: -0.062, New P: 0.562
iter 13 loss: 0.058
Actual params: [1.4894, 0.5623]
-Original Grad: -0.556, -lr * Pred Grad: 0.065, New P: 1.555
-Original Grad: 0.685, -lr * Pred Grad: -0.062, New P: 0.500
iter 14 loss: 0.135
Actual params: [1.5546, 0.5   ]
-Original Grad: -0.522, -lr * Pred Grad: 0.065, New P: 1.620
-Original Grad: 0.802, -lr * Pred Grad: -0.062, New P: 0.438
iter 15 loss: 0.223
Actual params: [1.6198, 0.4377]
-Original Grad: -0.277, -lr * Pred Grad: 0.065, New P: 1.685
-Original Grad: 0.484, -lr * Pred Grad: -0.062, New P: 0.375
iter 16 loss: 0.287
Actual params: [1.6851, 0.3754]
-Original Grad: -0.280, -lr * Pred Grad: 0.065, New P: 1.750
-Original Grad: 0.922, -lr * Pred Grad: -0.062, New P: 0.313
iter 17 loss: 0.322
Actual params: [1.7503, 0.3132]
-Original Grad: -0.153, -lr * Pred Grad: 0.065, New P: 1.816
-Original Grad: 0.230, -lr * Pred Grad: -0.062, New P: 0.251
iter 18 loss: 0.338
Actual params: [1.8155, 0.2509]
-Original Grad: -0.164, -lr * Pred Grad: 0.065, New P: 1.881
-Original Grad: 0.121, -lr * Pred Grad: -0.062, New P: 0.189
iter 19 loss: 0.348
Actual params: [1.8808, 0.1886]
-Original Grad: -0.330, -lr * Pred Grad: 0.065, New P: 1.946
-Original Grad: -0.047, -lr * Pred Grad: -0.062, New P: 0.126
iter 20 loss: 0.357
Actual params: [1.946 , 0.1263]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.474, -lr * Pred Grad: 0.050, New P: 0.644
-Original Grad: -0.580, -lr * Pred Grad: -0.044, New P: 0.550
iter 0 loss: 0.187
Actual params: [0.644 , 0.5496]
-Original Grad: 0.564, -lr * Pred Grad: 0.063, New P: 0.707
-Original Grad: -0.526, -lr * Pred Grad: -0.060, New P: 0.490
iter 1 loss: 0.144
Actual params: [0.707 , 0.4901]
-Original Grad: 0.023, -lr * Pred Grad: 0.065, New P: 0.772
-Original Grad: -0.049, -lr * Pred Grad: -0.062, New P: 0.428
iter 2 loss: 0.120
Actual params: [0.7719, 0.4282]
-Original Grad: -0.238, -lr * Pred Grad: 0.065, New P: 0.837
-Original Grad: 0.191, -lr * Pred Grad: -0.062, New P: 0.366
iter 3 loss: 0.132
Actual params: [0.8371, 0.366 ]
-Original Grad: -0.290, -lr * Pred Grad: 0.065, New P: 0.902
-Original Grad: 0.172, -lr * Pred Grad: -0.062, New P: 0.304
iter 4 loss: 0.162
Actual params: [0.9023, 0.3037]
-Original Grad: -0.411, -lr * Pred Grad: 0.065, New P: 0.968
-Original Grad: 1.787, -lr * Pred Grad: -0.062, New P: 0.241
iter 5 loss: 0.183
Actual params: [0.9675, 0.2414]
-Original Grad: 0.028, -lr * Pred Grad: 0.065, New P: 1.033
-Original Grad: 0.059, -lr * Pred Grad: -0.062, New P: 0.179
iter 6 loss: 0.189
Actual params: [1.0328, 0.1791]
-Original Grad: 0.154, -lr * Pred Grad: 0.065, New P: 1.098
-Original Grad: 0.015, -lr * Pred Grad: -0.062, New P: 0.117
iter 7 loss: 0.184
Actual params: [1.098 , 0.1168]
-Original Grad: 0.199, -lr * Pred Grad: 0.065, New P: 1.163
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: 0.055
iter 8 loss: 0.172
Actual params: [1.1632, 0.0545]
-Original Grad: 0.230, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: -0.013, -lr * Pred Grad: -0.062, New P: -0.008
iter 9 loss: 0.158
Actual params: [ 1.2285, -0.0077]
-Original Grad: 0.246, -lr * Pred Grad: 0.065, New P: 1.294
-Original Grad: -0.010, -lr * Pred Grad: -0.062, New P: -0.070
iter 10 loss: 0.142
Actual params: [ 1.2937, -0.07  ]
-Original Grad: 0.160, -lr * Pred Grad: 0.065, New P: 1.359
-Original Grad: -0.041, -lr * Pred Grad: -0.062, New P: -0.132
iter 11 loss: 0.132
Actual params: [ 1.3589, -0.1323]
-Original Grad: 0.166, -lr * Pred Grad: 0.065, New P: 1.424
-Original Grad: -0.055, -lr * Pred Grad: -0.062, New P: -0.195
iter 12 loss: 0.118
Actual params: [ 1.4241, -0.1946]
-Original Grad: 0.162, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: -0.133, -lr * Pred Grad: -0.062, New P: -0.257
iter 13 loss: 0.102
Actual params: [ 1.4894, -0.2569]
-Original Grad: 0.121, -lr * Pred Grad: 0.065, New P: 1.555
-Original Grad: -0.097, -lr * Pred Grad: -0.062, New P: -0.319
iter 14 loss: 0.086
Actual params: [ 1.5546, -0.3192]
-Original Grad: 0.082, -lr * Pred Grad: 0.065, New P: 1.620
-Original Grad: -0.076, -lr * Pred Grad: -0.062, New P: -0.381
iter 15 loss: 0.073
Actual params: [ 1.6198, -0.3815]
-Original Grad: 0.032, -lr * Pred Grad: 0.065, New P: 1.685
-Original Grad: -0.031, -lr * Pred Grad: -0.062, New P: -0.444
iter 16 loss: 0.066
Actual params: [ 1.6851, -0.4438]
-Original Grad: 0.003, -lr * Pred Grad: 0.065, New P: 1.750
-Original Grad: 0.040, -lr * Pred Grad: -0.062, New P: -0.506
iter 17 loss: 0.065
Actual params: [ 1.7503, -0.506 ]
-Original Grad: -0.032, -lr * Pred Grad: 0.065, New P: 1.816
-Original Grad: 0.115, -lr * Pred Grad: -0.062, New P: -0.568
iter 18 loss: 0.070
Actual params: [ 1.8155, -0.5683]
-Original Grad: -0.025, -lr * Pred Grad: 0.065, New P: 1.881
-Original Grad: 0.129, -lr * Pred Grad: -0.062, New P: -0.631
iter 19 loss: 0.080
Actual params: [ 1.8808, -0.6306]
-Original Grad: -0.023, -lr * Pred Grad: 0.065, New P: 1.946
-Original Grad: 0.142, -lr * Pred Grad: -0.062, New P: -0.693
iter 20 loss: 0.090
Actual params: [ 1.946 , -0.6929]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: -0.634, -lr * Pred Grad: -0.044, New P: 0.550
-Original Grad: -1.458, -lr * Pred Grad: -0.041, New P: 0.553
iter 0 loss: 0.389
Actual params: [0.55  , 0.5532]
-Original Grad: -0.670, -lr * Pred Grad: -0.059, New P: 0.491
-Original Grad: -1.122, -lr * Pred Grad: -0.059, New P: 0.494
iter 1 loss: 0.310
Actual params: [0.4905, 0.4943]
-Original Grad: -0.424, -lr * Pred Grad: -0.062, New P: 0.429
-Original Grad: -0.357, -lr * Pred Grad: -0.062, New P: 0.433
iter 2 loss: 0.244
Actual params: [0.4287, 0.4325]
-Original Grad: -0.618, -lr * Pred Grad: -0.062, New P: 0.366
-Original Grad: -0.425, -lr * Pred Grad: -0.062, New P: 0.370
iter 3 loss: 0.199
Actual params: [0.3664, 0.3703]
-Original Grad: 0.271, -lr * Pred Grad: -0.062, New P: 0.304
-Original Grad: 0.038, -lr * Pred Grad: -0.062, New P: 0.308
iter 4 loss: 0.186
Actual params: [0.3041, 0.308 ]
-Original Grad: 0.200, -lr * Pred Grad: -0.062, New P: 0.242
-Original Grad: -0.018, -lr * Pred Grad: -0.062, New P: 0.246
iter 5 loss: 0.201
Actual params: [0.2419, 0.2457]
-Original Grad: 0.090, -lr * Pred Grad: -0.062, New P: 0.180
-Original Grad: -0.010, -lr * Pred Grad: -0.062, New P: 0.183
iter 6 loss: 0.210
Actual params: [0.1796, 0.1834]
-Original Grad: 0.018, -lr * Pred Grad: -0.062, New P: 0.117
-Original Grad: 0.004, -lr * Pred Grad: -0.062, New P: 0.121
iter 7 loss: 0.213
Actual params: [0.1173, 0.1211]
-Original Grad: -0.009, -lr * Pred Grad: -0.062, New P: 0.055
-Original Grad: -0.045, -lr * Pred Grad: -0.062, New P: 0.059
iter 8 loss: 0.212
Actual params: [0.055 , 0.0589]
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -0.007
-Original Grad: -0.074, -lr * Pred Grad: -0.062, New P: -0.003
iter 9 loss: 0.208
Actual params: [-0.0073, -0.0034]
-Original Grad: 0.041, -lr * Pred Grad: -0.062, New P: -0.070
-Original Grad: -0.056, -lr * Pred Grad: -0.062, New P: -0.066
iter 10 loss: 0.205
Actual params: [-0.0696, -0.0657]
-Original Grad: 0.009, -lr * Pred Grad: -0.062, New P: -0.132
-Original Grad: -0.061, -lr * Pred Grad: -0.062, New P: -0.128
iter 11 loss: 0.203
Actual params: [-0.1319, -0.128 ]
-Original Grad: 0.098, -lr * Pred Grad: -0.062, New P: -0.194
-Original Grad: 0.014, -lr * Pred Grad: -0.062, New P: -0.190
iter 12 loss: 0.204
Actual params: [-0.1942, -0.1903]
-Original Grad: 0.083, -lr * Pred Grad: -0.062, New P: -0.256
-Original Grad: 0.028, -lr * Pred Grad: -0.062, New P: -0.253
iter 13 loss: 0.210
Actual params: [-0.2565, -0.2526]
-Original Grad: 0.073, -lr * Pred Grad: -0.062, New P: -0.319
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -0.315
iter 14 loss: 0.220
Actual params: [-0.3187, -0.3149]
-Original Grad: 0.190, -lr * Pred Grad: -0.062, New P: -0.381
-Original Grad: 0.046, -lr * Pred Grad: -0.062, New P: -0.377
iter 15 loss: 0.235
Actual params: [-0.381 , -0.3772]
-Original Grad: 0.221, -lr * Pred Grad: -0.062, New P: -0.443
-Original Grad: 0.051, -lr * Pred Grad: -0.062, New P: -0.439
iter 16 loss: 0.250
Actual params: [-0.4433, -0.4395]
-Original Grad: 0.227, -lr * Pred Grad: -0.062, New P: -0.506
-Original Grad: 0.065, -lr * Pred Grad: -0.062, New P: -0.502
iter 17 loss: 0.270
Actual params: [-0.5056, -0.5017]
-Original Grad: 0.265, -lr * Pred Grad: -0.062, New P: -0.568
-Original Grad: 0.069, -lr * Pred Grad: -0.062, New P: -0.564
iter 18 loss: 0.290
Actual params: [-0.5679, -0.564 ]
-Original Grad: 0.453, -lr * Pred Grad: -0.062, New P: -0.630
-Original Grad: 0.079, -lr * Pred Grad: -0.062, New P: -0.626
iter 19 loss: 0.317
Actual params: [-0.6302, -0.6263]
-Original Grad: 1.083, -lr * Pred Grad: -0.062, New P: -0.692
-Original Grad: 0.201, -lr * Pred Grad: -0.062, New P: -0.689
iter 20 loss: 0.368
Actual params: [-0.6925, -0.6886]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.185, -lr * Pred Grad: 0.049, New P: 0.643
-Original Grad: -0.062, -lr * Pred Grad: -0.022, New P: 0.572
iter 0 loss: 0.726
Actual params: [0.6433, 0.5725]
-Original Grad: 0.241, -lr * Pred Grad: 0.063, New P: 0.706
-Original Grad: -0.084, -lr * Pred Grad: -0.055, New P: 0.518
iter 1 loss: 0.714
Actual params: [0.7061, 0.5179]
-Original Grad: 0.438, -lr * Pred Grad: 0.065, New P: 0.771
-Original Grad: -0.178, -lr * Pred Grad: -0.061, New P: 0.457
iter 2 loss: 0.686
Actual params: [0.771 , 0.4567]
-Original Grad: 1.375, -lr * Pred Grad: 0.065, New P: 0.836
-Original Grad: -0.537, -lr * Pred Grad: -0.062, New P: 0.395
iter 3 loss: 0.618
Actual params: [0.8362, 0.3946]
-Original Grad: 1.983, -lr * Pred Grad: 0.065, New P: 0.901
-Original Grad: -0.549, -lr * Pred Grad: -0.062, New P: 0.332
iter 4 loss: 0.461
Actual params: [0.9014, 0.3323]
-Original Grad: 1.433, -lr * Pred Grad: 0.065, New P: 0.967
-Original Grad: -0.457, -lr * Pred Grad: -0.062, New P: 0.270
iter 5 loss: 0.306
Actual params: [0.9666, 0.27  ]
-Original Grad: 0.781, -lr * Pred Grad: 0.065, New P: 1.032
-Original Grad: -0.222, -lr * Pred Grad: -0.062, New P: 0.208
iter 6 loss: 0.185
Actual params: [1.0319, 0.2077]
-Original Grad: 0.399, -lr * Pred Grad: 0.065, New P: 1.097
-Original Grad: -0.092, -lr * Pred Grad: -0.062, New P: 0.145
iter 7 loss: 0.135
Actual params: [1.0971, 0.1455]
-Original Grad: -0.291, -lr * Pred Grad: 0.065, New P: 1.162
-Original Grad: -0.313, -lr * Pred Grad: -0.062, New P: 0.083
iter 8 loss: 0.115
Actual params: [1.1623, 0.0832]
-Original Grad: -0.155, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: -0.000, -lr * Pred Grad: -0.062, New P: 0.021
iter 9 loss: 0.119
Actual params: [1.2276, 0.0209]
-Original Grad: -0.397, -lr * Pred Grad: 0.065, New P: 1.293
-Original Grad: 0.034, -lr * Pred Grad: -0.062, New P: -0.041
iter 10 loss: 0.138
Actual params: [ 1.2928, -0.0414]
-Original Grad: -0.386, -lr * Pred Grad: 0.065, New P: 1.358
-Original Grad: 0.061, -lr * Pred Grad: -0.062, New P: -0.104
iter 11 loss: 0.174
Actual params: [ 1.358 , -0.1037]
-Original Grad: -0.625, -lr * Pred Grad: 0.065, New P: 1.423
-Original Grad: 0.145, -lr * Pred Grad: -0.062, New P: -0.166
iter 12 loss: 0.213
Actual params: [ 1.4233, -0.166 ]
-Original Grad: -0.575, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: 0.150, -lr * Pred Grad: -0.062, New P: -0.228
iter 13 loss: 0.252
Actual params: [ 1.4885, -0.2283]
-Original Grad: -0.690, -lr * Pred Grad: 0.065, New P: 1.554
-Original Grad: 0.178, -lr * Pred Grad: -0.062, New P: -0.291
iter 14 loss: 0.298
Actual params: [ 1.5537, -0.2906]
-Original Grad: -0.552, -lr * Pred Grad: 0.065, New P: 1.619
-Original Grad: 0.193, -lr * Pred Grad: -0.062, New P: -0.353
iter 15 loss: 0.342
Actual params: [ 1.619 , -0.3528]
-Original Grad: -0.508, -lr * Pred Grad: 0.065, New P: 1.684
-Original Grad: 0.198, -lr * Pred Grad: -0.062, New P: -0.415
iter 16 loss: 0.389
Actual params: [ 1.6842, -0.4151]
-Original Grad: -0.399, -lr * Pred Grad: 0.065, New P: 1.749
-Original Grad: 0.190, -lr * Pred Grad: -0.062, New P: -0.477
iter 17 loss: 0.431
Actual params: [ 1.7494, -0.4774]
-Original Grad: -0.323, -lr * Pred Grad: 0.065, New P: 1.815
-Original Grad: 0.187, -lr * Pred Grad: -0.062, New P: -0.540
iter 18 loss: 0.466
Actual params: [ 1.8147, -0.5397]
-Original Grad: -0.332, -lr * Pred Grad: 0.065, New P: 1.880
-Original Grad: 0.198, -lr * Pred Grad: -0.062, New P: -0.602
iter 19 loss: 0.499
Actual params: [ 1.8799, -0.602 ]
-Original Grad: -0.332, -lr * Pred Grad: 0.065, New P: 1.945
-Original Grad: 0.231, -lr * Pred Grad: -0.062, New P: -0.664
iter 20 loss: 0.534
Actual params: [ 1.9451, -0.6643]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.291, -lr * Pred Grad: 0.050, New P: 0.644
-Original Grad: -0.145, -lr * Pred Grad: -0.042, New P: 0.552
iter 0 loss: 0.627
Actual params: [0.6439, 0.5523]
-Original Grad: 0.350, -lr * Pred Grad: 0.063, New P: 0.707
-Original Grad: -0.166, -lr * Pred Grad: -0.059, New P: 0.493
iter 1 loss: 0.605
Actual params: [0.7068, 0.4933]
-Original Grad: 0.519, -lr * Pred Grad: 0.065, New P: 0.772
-Original Grad: -0.203, -lr * Pred Grad: -0.062, New P: 0.431
iter 2 loss: 0.567
Actual params: [0.7717, 0.4314]
-Original Grad: 0.772, -lr * Pred Grad: 0.065, New P: 0.837
-Original Grad: -0.239, -lr * Pred Grad: -0.062, New P: 0.369
iter 3 loss: 0.511
Actual params: [0.8369, 0.3692]
-Original Grad: 2.426, -lr * Pred Grad: 0.065, New P: 0.902
-Original Grad: -0.442, -lr * Pred Grad: -0.062, New P: 0.307
iter 4 loss: 0.445
Actual params: [0.9021, 0.3069]
-Original Grad: 0.773, -lr * Pred Grad: 0.065, New P: 0.967
-Original Grad: -0.129, -lr * Pred Grad: -0.062, New P: 0.245
iter 5 loss: 0.386
Actual params: [0.9674, 0.2446]
-Original Grad: 0.607, -lr * Pred Grad: 0.065, New P: 1.033
-Original Grad: -0.190, -lr * Pred Grad: -0.062, New P: 0.182
iter 6 loss: 0.335
Actual params: [1.0326, 0.1823]
-Original Grad: 0.331, -lr * Pred Grad: 0.065, New P: 1.098
-Original Grad: -0.306, -lr * Pred Grad: -0.062, New P: 0.120
iter 7 loss: 0.296
Actual params: [1.0978, 0.1201]
-Original Grad: -0.181, -lr * Pred Grad: 0.065, New P: 1.163
-Original Grad: -0.099, -lr * Pred Grad: -0.062, New P: 0.058
iter 8 loss: 0.290
Actual params: [1.1631, 0.0578]
-Original Grad: -0.366, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: -0.076, -lr * Pred Grad: -0.062, New P: -0.005
iter 9 loss: 0.302
Actual params: [ 1.2283, -0.0045]
-Original Grad: -0.363, -lr * Pred Grad: 0.065, New P: 1.294
-Original Grad: -0.090, -lr * Pred Grad: -0.062, New P: -0.067
iter 10 loss: 0.322
Actual params: [ 1.2935, -0.0668]
-Original Grad: -0.422, -lr * Pred Grad: 0.065, New P: 1.359
-Original Grad: -0.048, -lr * Pred Grad: -0.062, New P: -0.129
iter 11 loss: 0.342
Actual params: [ 1.3588, -0.1291]
-Original Grad: -0.382, -lr * Pred Grad: 0.065, New P: 1.424
-Original Grad: -0.047, -lr * Pred Grad: -0.062, New P: -0.191
iter 12 loss: 0.361
Actual params: [ 1.424 , -0.1914]
-Original Grad: -0.256, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: -0.006, -lr * Pred Grad: -0.062, New P: -0.254
iter 13 loss: 0.379
Actual params: [ 1.4892, -0.2537]
-Original Grad: 2.258, -lr * Pred Grad: 0.065, New P: 1.554
-Original Grad: 0.032, -lr * Pred Grad: -0.062, New P: -0.316
iter 14 loss: 0.390
Actual params: [ 1.5545, -0.316 ]
-Original Grad: -0.152, -lr * Pred Grad: 0.065, New P: 1.620
-Original Grad: 0.027, -lr * Pred Grad: -0.062, New P: -0.378
iter 15 loss: 0.402
Actual params: [ 1.6197, -0.3782]
-Original Grad: -0.105, -lr * Pred Grad: 0.065, New P: 1.685
-Original Grad: 0.031, -lr * Pred Grad: -0.062, New P: -0.441
iter 16 loss: 0.412
Actual params: [ 1.6849, -0.4405]
-Original Grad: -0.081, -lr * Pred Grad: 0.065, New P: 1.750
-Original Grad: 0.029, -lr * Pred Grad: -0.062, New P: -0.503
iter 17 loss: 0.420
Actual params: [ 1.7502, -0.5028]
-Original Grad: -0.064, -lr * Pred Grad: 0.065, New P: 1.815
-Original Grad: 0.027, -lr * Pred Grad: -0.062, New P: -0.565
iter 18 loss: 0.426
Actual params: [ 1.8154, -0.5651]
-Original Grad: 0.009, -lr * Pred Grad: 0.065, New P: 1.881
-Original Grad: 0.044, -lr * Pred Grad: -0.062, New P: -0.627
iter 19 loss: 0.431
Actual params: [ 1.8806, -0.6274]
-Original Grad: 3.049, -lr * Pred Grad: 0.065, New P: 1.946
-Original Grad: 5.379, -lr * Pred Grad: -0.062, New P: -0.690
iter 20 loss: 0.432
Actual params: [ 1.9459, -0.6897]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.091, -lr * Pred Grad: 0.045, New P: 0.639
-Original Grad: -0.323, -lr * Pred Grad: -0.046, New P: 0.548
iter 0 loss: 0.503
Actual params: [0.6391, 0.5484]
-Original Grad: 0.291, -lr * Pred Grad: 0.062, New P: 0.701
-Original Grad: -0.080, -lr * Pred Grad: -0.060, New P: 0.489
iter 1 loss: 0.483
Actual params: [0.7012, 0.4886]
-Original Grad: 0.541, -lr * Pred Grad: 0.065, New P: 0.766
-Original Grad: -0.199, -lr * Pred Grad: -0.062, New P: 0.427
iter 2 loss: 0.450
Actual params: [0.766 , 0.4267]
-Original Grad: -30.366, -lr * Pred Grad: 0.064, New P: 0.830
-Original Grad: -7.243, -lr * Pred Grad: -0.062, New P: 0.364
iter 3 loss: 0.388
Actual params: [0.8296, 0.3645]
-Original Grad: 1.335, -lr * Pred Grad: 0.065, New P: 0.895
-Original Grad: -0.031, -lr * Pred Grad: -0.062, New P: 0.302
iter 4 loss: 0.308
Actual params: [0.8947, 0.3022]
-Original Grad: 0.497, -lr * Pred Grad: 0.065, New P: 0.960
-Original Grad: -0.103, -lr * Pred Grad: -0.062, New P: 0.240
iter 5 loss: 0.237
Actual params: [0.96  , 0.2399]
-Original Grad: 0.582, -lr * Pred Grad: 0.065, New P: 1.025
-Original Grad: -0.077, -lr * Pred Grad: -0.062, New P: 0.178
iter 6 loss: 0.186
Actual params: [1.0252, 0.1776]
-Original Grad: 0.263, -lr * Pred Grad: 0.065, New P: 1.090
-Original Grad: -0.073, -lr * Pred Grad: -0.062, New P: 0.115
iter 7 loss: 0.156
Actual params: [1.0904, 0.1153]
-Original Grad: 0.033, -lr * Pred Grad: 0.065, New P: 1.156
-Original Grad: -0.024, -lr * Pred Grad: -0.062, New P: 0.053
iter 8 loss: 0.142
Actual params: [1.1557, 0.053 ]
-Original Grad: -0.105, -lr * Pred Grad: 0.065, New P: 1.221
-Original Grad: -0.011, -lr * Pred Grad: -0.062, New P: -0.009
iter 9 loss: 0.144
Actual params: [ 1.2209, -0.0092]
-Original Grad: -0.057, -lr * Pred Grad: 0.065, New P: 1.286
-Original Grad: -0.004, -lr * Pred Grad: -0.062, New P: -0.072
iter 10 loss: 0.152
Actual params: [ 1.2861, -0.0715]
-Original Grad: -0.277, -lr * Pred Grad: 0.065, New P: 1.351
-Original Grad: 0.029, -lr * Pred Grad: -0.062, New P: -0.134
iter 11 loss: 0.162
Actual params: [ 1.3514, -0.1338]
-Original Grad: -0.350, -lr * Pred Grad: 0.065, New P: 1.417
-Original Grad: 0.015, -lr * Pred Grad: -0.062, New P: -0.196
iter 12 loss: 0.185
Actual params: [ 1.4166, -0.1961]
-Original Grad: -0.376, -lr * Pred Grad: 0.065, New P: 1.482
-Original Grad: -0.019, -lr * Pred Grad: -0.062, New P: -0.258
iter 13 loss: 0.204
Actual params: [ 1.4818, -0.2584]
-Original Grad: -0.237, -lr * Pred Grad: 0.065, New P: 1.547
-Original Grad: -0.030, -lr * Pred Grad: -0.062, New P: -0.321
iter 14 loss: 0.222
Actual params: [ 1.5471, -0.3207]
-Original Grad: -0.046, -lr * Pred Grad: 0.065, New P: 1.612
-Original Grad: 0.018, -lr * Pred Grad: -0.062, New P: -0.383
iter 15 loss: 0.234
Actual params: [ 1.6123, -0.383 ]
-Original Grad: 0.663, -lr * Pred Grad: 0.065, New P: 1.678
-Original Grad: 0.262, -lr * Pred Grad: -0.062, New P: -0.445
iter 16 loss: 0.241
Actual params: [ 1.6775, -0.4453]
-Original Grad: -0.162, -lr * Pred Grad: 0.065, New P: 1.743
-Original Grad: -0.035, -lr * Pred Grad: -0.062, New P: -0.508
iter 17 loss: 0.243
Actual params: [ 1.7428, -0.5075]
-Original Grad: -0.164, -lr * Pred Grad: 0.065, New P: 1.808
-Original Grad: -0.038, -lr * Pred Grad: -0.062, New P: -0.570
iter 18 loss: 0.251
Actual params: [ 1.808 , -0.5698]
-Original Grad: -0.117, -lr * Pred Grad: 0.065, New P: 1.873
-Original Grad: -0.013, -lr * Pred Grad: -0.062, New P: -0.632
iter 19 loss: 0.259
Actual params: [ 1.8732, -0.6321]
-Original Grad: -0.068, -lr * Pred Grad: 0.065, New P: 1.938
-Original Grad: 0.015, -lr * Pred Grad: -0.062, New P: -0.694
iter 20 loss: 0.265
Actual params: [ 1.9385, -0.6944]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.193, -lr * Pred Grad: 0.049, New P: 0.643
-Original Grad: -0.171, -lr * Pred Grad: -0.044, New P: 0.550
iter 0 loss: 0.286
Actual params: [0.6434, 0.5505]
-Original Grad: 0.213, -lr * Pred Grad: 0.063, New P: 0.706
-Original Grad: -0.151, -lr * Pred Grad: -0.059, New P: 0.491
iter 1 loss: 0.269
Actual params: [0.7062, 0.4911]
-Original Grad: 0.230, -lr * Pred Grad: 0.065, New P: 0.771
-Original Grad: -0.069, -lr * Pred Grad: -0.062, New P: 0.429
iter 2 loss: 0.248
Actual params: [0.7711, 0.4293]
-Original Grad: 0.195, -lr * Pred Grad: 0.065, New P: 0.836
-Original Grad: -0.006, -lr * Pred Grad: -0.062, New P: 0.367
iter 3 loss: 0.233
Actual params: [0.8363, 0.367 ]
-Original Grad: 0.133, -lr * Pred Grad: 0.065, New P: 0.902
-Original Grad: 0.011, -lr * Pred Grad: -0.062, New P: 0.305
iter 4 loss: 0.221
Actual params: [0.9015, 0.3048]
-Original Grad: 0.061, -lr * Pred Grad: 0.065, New P: 0.967
-Original Grad: 0.036, -lr * Pred Grad: -0.062, New P: 0.242
iter 5 loss: 0.217
Actual params: [0.9668, 0.2425]
-Original Grad: 0.001, -lr * Pred Grad: 0.065, New P: 1.032
-Original Grad: 0.084, -lr * Pred Grad: -0.062, New P: 0.180
iter 6 loss: 0.218
Actual params: [1.032 , 0.1802]
-Original Grad: 0.005, -lr * Pred Grad: 0.065, New P: 1.097
-Original Grad: 0.080, -lr * Pred Grad: -0.062, New P: 0.118
iter 7 loss: 0.224
Actual params: [1.0972, 0.1179]
-Original Grad: -0.000, -lr * Pred Grad: 0.065, New P: 1.162
-Original Grad: 0.089, -lr * Pred Grad: -0.062, New P: 0.056
iter 8 loss: 0.229
Actual params: [1.1625, 0.0556]
-Original Grad: -0.006, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: 0.093, -lr * Pred Grad: -0.062, New P: -0.007
iter 9 loss: 0.235
Actual params: [ 1.2277, -0.0067]
-Original Grad: -0.008, -lr * Pred Grad: 0.065, New P: 1.293
-Original Grad: 0.048, -lr * Pred Grad: -0.062, New P: -0.069
iter 10 loss: 0.240
Actual params: [ 1.2929, -0.069 ]
-Original Grad: -0.007, -lr * Pred Grad: 0.065, New P: 1.358
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -0.131
iter 11 loss: 0.242
Actual params: [ 1.3582, -0.1313]
-Original Grad: -0.005, -lr * Pred Grad: 0.065, New P: 1.423
-Original Grad: -0.010, -lr * Pred Grad: -0.062, New P: -0.194
iter 12 loss: 0.242
Actual params: [ 1.4234, -0.1936]
-Original Grad: -0.005, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: 0.016, -lr * Pred Grad: -0.062, New P: -0.256
iter 13 loss: 0.243
Actual params: [ 1.4886, -0.2558]
-Original Grad: -0.007, -lr * Pred Grad: 0.065, New P: 1.554
-Original Grad: 0.022, -lr * Pred Grad: -0.062, New P: -0.318
iter 14 loss: 0.245
Actual params: [ 1.5539, -0.3181]
-Original Grad: -0.013, -lr * Pred Grad: 0.065, New P: 1.619
-Original Grad: 0.018, -lr * Pred Grad: -0.062, New P: -0.380
iter 15 loss: 0.247
Actual params: [ 1.6191, -0.3804]
-Original Grad: -0.014, -lr * Pred Grad: 0.065, New P: 1.684
-Original Grad: 0.020, -lr * Pred Grad: -0.062, New P: -0.443
iter 16 loss: 0.249
Actual params: [ 1.6843, -0.4427]
-Original Grad: -0.005, -lr * Pred Grad: 0.065, New P: 1.750
-Original Grad: 0.039, -lr * Pred Grad: -0.062, New P: -0.505
iter 17 loss: 0.251
Actual params: [ 1.7496, -0.505 ]
-Original Grad: -0.001, -lr * Pred Grad: 0.065, New P: 1.815
-Original Grad: 0.058, -lr * Pred Grad: -0.062, New P: -0.567
iter 18 loss: 0.254
Actual params: [ 1.8148, -0.5673]
-Original Grad: -0.003, -lr * Pred Grad: 0.065, New P: 1.880
-Original Grad: 0.048, -lr * Pred Grad: -0.062, New P: -0.630
iter 19 loss: 0.258
Actual params: [ 1.88  , -0.6296]
-Original Grad: -0.000, -lr * Pred Grad: 0.065, New P: 1.945
-Original Grad: 0.044, -lr * Pred Grad: -0.062, New P: -0.692
iter 20 loss: 0.261
Actual params: [ 1.9453, -0.6919]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.249, -lr * Pred Grad: 0.050, New P: 0.644
-Original Grad: 0.156, -lr * Pred Grad: 0.049, New P: 0.643
iter 0 loss: 0.331
Actual params: [0.6438, 0.6428]
-Original Grad: 0.318, -lr * Pred Grad: 0.063, New P: 0.707
-Original Grad: 0.228, -lr * Pred Grad: 0.063, New P: 0.705
iter 1 loss: 0.310
Actual params: [0.7067, 0.7055]
-Original Grad: 0.211, -lr * Pred Grad: 0.065, New P: 0.772
-Original Grad: -0.185, -lr * Pred Grad: 0.065, New P: 0.770
iter 2 loss: 0.277
Actual params: [0.7716, 0.7703]
-Original Grad: -0.804, -lr * Pred Grad: 0.065, New P: 0.837
-Original Grad: -1.644, -lr * Pred Grad: 0.065, New P: 0.836
iter 3 loss: 0.324
Actual params: [0.8368, 0.8355]
-Original Grad: -0.791, -lr * Pred Grad: 0.065, New P: 0.902
-Original Grad: -2.169, -lr * Pred Grad: 0.065, New P: 0.901
iter 4 loss: 0.384
Actual params: [0.902 , 0.9007]
-Original Grad: -0.929, -lr * Pred Grad: 0.065, New P: 0.967
-Original Grad: -2.171, -lr * Pred Grad: 0.065, New P: 0.966
iter 5 loss: 0.576
Actual params: [0.9672, 0.9659]
-Original Grad: -0.040, -lr * Pred Grad: 0.065, New P: 1.032
-Original Grad: 0.264, -lr * Pred Grad: 0.065, New P: 1.031
iter 6 loss: 0.839
Actual params: [1.0325, 1.0311]
-Original Grad: -1.585, -lr * Pred Grad: 0.065, New P: 1.098
-Original Grad: -0.376, -lr * Pred Grad: 0.065, New P: 1.096
iter 7 loss: 0.938
Actual params: [1.0977, 1.0963]
-Original Grad: -2.119, -lr * Pred Grad: 0.065, New P: 1.163
-Original Grad: 0.968, -lr * Pred Grad: 0.065, New P: 1.161
iter 8 loss: 1.041
Actual params: [1.1629, 1.1614]
-Original Grad: 0.255, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: 1.591, -lr * Pred Grad: 0.065, New P: 1.226
iter 9 loss: 1.027
Actual params: [1.2282, 1.2264]
-Original Grad: 2.005, -lr * Pred Grad: 0.065, New P: 1.293
-Original Grad: 2.152, -lr * Pred Grad: 0.065, New P: 1.291
iter 10 loss: 0.813
Actual params: [1.2934, 1.2913]
-Original Grad: 2.147, -lr * Pred Grad: 0.065, New P: 1.359
-Original Grad: 2.124, -lr * Pred Grad: 0.065, New P: 1.356
iter 11 loss: 0.491
Actual params: [1.3586, 1.356 ]
-Original Grad: 1.378, -lr * Pred Grad: 0.065, New P: 1.424
-Original Grad: 0.766, -lr * Pred Grad: 0.065, New P: 1.421
iter 12 loss: 0.298
Actual params: [1.4239, 1.421 ]
-Original Grad: 0.863, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: 0.119, -lr * Pred Grad: 0.065, New P: 1.486
iter 13 loss: 0.204
Actual params: [1.4891, 1.4861]
-Original Grad: 0.555, -lr * Pred Grad: 0.065, New P: 1.554
-Original Grad: 0.102, -lr * Pred Grad: 0.065, New P: 1.551
iter 14 loss: 0.154
Actual params: [1.5543, 1.5512]
-Original Grad: 0.346, -lr * Pred Grad: 0.065, New P: 1.620
-Original Grad: 0.111, -lr * Pred Grad: 0.065, New P: 1.616
iter 15 loss: 0.121
Actual params: [1.6196, 1.6162]
-Original Grad: 0.249, -lr * Pred Grad: 0.065, New P: 1.685
-Original Grad: -0.015, -lr * Pred Grad: 0.065, New P: 1.681
iter 16 loss: 0.099
Actual params: [1.6848, 1.6811]
-Original Grad: 0.150, -lr * Pred Grad: 0.065, New P: 1.750
-Original Grad: -0.177, -lr * Pred Grad: 0.065, New P: 1.746
iter 17 loss: 0.092
Actual params: [1.75  , 1.7457]
-Original Grad: 0.064, -lr * Pred Grad: 0.065, New P: 1.815
-Original Grad: -0.301, -lr * Pred Grad: 0.064, New P: 1.810
iter 18 loss: 0.099
Actual params: [1.8153, 1.8098]
-Original Grad: 0.014, -lr * Pred Grad: 0.065, New P: 1.881
-Original Grad: -0.240, -lr * Pred Grad: 0.063, New P: 1.873
iter 19 loss: 0.114
Actual params: [1.8805, 1.8731]
-Original Grad: 0.270, -lr * Pred Grad: 0.065, New P: 1.946
-Original Grad: 0.192, -lr * Pred Grad: 0.062, New P: 1.935
iter 20 loss: 0.127
Actual params: [1.9457, 1.9354]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: -0.146, -lr * Pred Grad: -0.042, New P: 0.552
-Original Grad: 0.040, -lr * Pred Grad: 0.035, New P: 0.629
iter 0 loss: 0.328
Actual params: [0.5522, 0.6291]
-Original Grad: 0.097, -lr * Pred Grad: -0.059, New P: 0.493
-Original Grad: -0.032, -lr * Pred Grad: 0.038, New P: 0.667
iter 1 loss: 0.340
Actual params: [0.4931, 0.6672]
-Original Grad: 0.350, -lr * Pred Grad: -0.062, New P: 0.431
-Original Grad: -0.168, -lr * Pred Grad: 0.010, New P: 0.677
iter 2 loss: 0.360
Actual params: [0.4313, 0.6775]
-Original Grad: -0.035, -lr * Pred Grad: -0.062, New P: 0.369
-Original Grad: 0.010, -lr * Pred Grad: -0.040, New P: 0.637
iter 3 loss: 0.362
Actual params: [0.3691, 0.6371]
-Original Grad: 0.047, -lr * Pred Grad: -0.062, New P: 0.307
-Original Grad: -0.023, -lr * Pred Grad: -0.059, New P: 0.578
iter 4 loss: 0.362
Actual params: [0.3068, 0.5785]
-Original Grad: 0.076, -lr * Pred Grad: -0.062, New P: 0.245
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: 0.517
iter 5 loss: 0.373
Actual params: [0.2445, 0.5167]
-Original Grad: 0.037, -lr * Pred Grad: -0.062, New P: 0.182
-Original Grad: 0.018, -lr * Pred Grad: -0.062, New P: 0.454
iter 6 loss: 0.377
Actual params: [0.1822, 0.4545]
-Original Grad: 0.030, -lr * Pred Grad: -0.062, New P: 0.120
-Original Grad: 0.033, -lr * Pred Grad: -0.062, New P: 0.392
iter 7 loss: 0.381
Actual params: [0.1199, 0.3922]
-Original Grad: 0.037, -lr * Pred Grad: -0.062, New P: 0.058
-Original Grad: 0.049, -lr * Pred Grad: -0.062, New P: 0.330
iter 8 loss: 0.385
Actual params: [0.0577, 0.3299]
-Original Grad: 0.041, -lr * Pred Grad: -0.062, New P: -0.005
-Original Grad: 0.052, -lr * Pred Grad: -0.062, New P: 0.268
iter 9 loss: 0.391
Actual params: [-0.0046,  0.2676]
-Original Grad: 0.024, -lr * Pred Grad: -0.062, New P: -0.067
-Original Grad: 0.046, -lr * Pred Grad: -0.062, New P: 0.205
iter 10 loss: 0.396
Actual params: [-0.0669,  0.2053]
-Original Grad: 0.007, -lr * Pred Grad: -0.062, New P: -0.129
-Original Grad: 0.036, -lr * Pred Grad: -0.062, New P: 0.143
iter 11 loss: 0.399
Actual params: [-0.1292,  0.143 ]
-Original Grad: -0.007, -lr * Pred Grad: -0.062, New P: -0.192
-Original Grad: 0.033, -lr * Pred Grad: -0.062, New P: 0.081
iter 12 loss: 0.401
Actual params: [-0.1915,  0.0808]
-Original Grad: -0.016, -lr * Pred Grad: -0.062, New P: -0.254
-Original Grad: 0.033, -lr * Pred Grad: -0.062, New P: 0.018
iter 13 loss: 0.403
Actual params: [-0.2538,  0.0185]
-Original Grad: -0.017, -lr * Pred Grad: -0.062, New P: -0.316
-Original Grad: 0.034, -lr * Pred Grad: -0.062, New P: -0.044
iter 14 loss: 0.404
Actual params: [-0.3161, -0.0438]
-Original Grad: -0.012, -lr * Pred Grad: -0.062, New P: -0.378
-Original Grad: 0.025, -lr * Pred Grad: -0.062, New P: -0.106
iter 15 loss: 0.405
Actual params: [-0.3784, -0.1061]
-Original Grad: -0.010, -lr * Pred Grad: -0.062, New P: -0.441
-Original Grad: 0.012, -lr * Pred Grad: -0.062, New P: -0.168
iter 16 loss: 0.405
Actual params: [-0.4407, -0.1684]
-Original Grad: -0.012, -lr * Pred Grad: -0.062, New P: -0.503
-Original Grad: -0.023, -lr * Pred Grad: -0.062, New P: -0.231
iter 17 loss: 0.404
Actual params: [-0.5029, -0.2307]
-Original Grad: -0.009, -lr * Pred Grad: -0.062, New P: -0.565
-Original Grad: -0.009, -lr * Pred Grad: -0.062, New P: -0.293
iter 18 loss: 0.403
Actual params: [-0.5652, -0.293 ]
-Original Grad: -0.009, -lr * Pred Grad: -0.062, New P: -0.628
-Original Grad: -0.005, -lr * Pred Grad: -0.062, New P: -0.355
iter 19 loss: 0.402
Actual params: [-0.6275, -0.3553]
-Original Grad: -0.008, -lr * Pred Grad: -0.062, New P: -0.690
-Original Grad: -0.010, -lr * Pred Grad: -0.062, New P: -0.418
iter 20 loss: 0.401
Actual params: [-0.6898, -0.4175]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.507, -lr * Pred Grad: 0.050, New P: 0.644
-Original Grad: -2.357, -lr * Pred Grad: -0.041, New P: 0.553
iter 0 loss: 0.494
Actual params: [0.644 , 0.5534]
-Original Grad: 0.511, -lr * Pred Grad: 0.063, New P: 0.707
-Original Grad: -2.968, -lr * Pred Grad: -0.059, New P: 0.495
iter 1 loss: 0.344
Actual params: [0.707 , 0.4946]
-Original Grad: 0.274, -lr * Pred Grad: 0.065, New P: 0.772
-Original Grad: -1.111, -lr * Pred Grad: -0.062, New P: 0.433
iter 2 loss: 0.198
Actual params: [0.7719, 0.4329]
-Original Grad: 0.138, -lr * Pred Grad: 0.065, New P: 0.837
-Original Grad: -0.450, -lr * Pred Grad: -0.062, New P: 0.371
iter 3 loss: 0.146
Actual params: [0.8371, 0.3707]
-Original Grad: 0.092, -lr * Pred Grad: 0.065, New P: 0.902
-Original Grad: -0.039, -lr * Pred Grad: -0.062, New P: 0.308
iter 4 loss: 0.133
Actual params: [0.9023, 0.3084]
-Original Grad: 0.077, -lr * Pred Grad: 0.065, New P: 0.968
-Original Grad: 0.011, -lr * Pred Grad: -0.062, New P: 0.246
iter 5 loss: 0.126
Actual params: [0.9675, 0.2461]
-Original Grad: 0.139, -lr * Pred Grad: 0.065, New P: 1.033
-Original Grad: 0.026, -lr * Pred Grad: -0.062, New P: 0.184
iter 6 loss: 0.120
Actual params: [1.0328, 0.1838]
-Original Grad: 0.149, -lr * Pred Grad: 0.065, New P: 1.098
-Original Grad: 0.070, -lr * Pred Grad: -0.062, New P: 0.122
iter 7 loss: 0.114
Actual params: [1.098 , 0.1215]
-Original Grad: 0.136, -lr * Pred Grad: 0.065, New P: 1.163
-Original Grad: 0.129, -lr * Pred Grad: -0.062, New P: 0.059
iter 8 loss: 0.110
Actual params: [1.1632, 0.0593]
-Original Grad: 0.148, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: 0.162, -lr * Pred Grad: -0.062, New P: -0.003
iter 9 loss: 0.110
Actual params: [ 1.2285, -0.003 ]
-Original Grad: 0.204, -lr * Pred Grad: 0.065, New P: 1.294
-Original Grad: 0.278, -lr * Pred Grad: -0.062, New P: -0.065
iter 10 loss: 0.112
Actual params: [ 1.2937, -0.0653]
-Original Grad: 0.106, -lr * Pred Grad: 0.065, New P: 1.359
-Original Grad: 0.159, -lr * Pred Grad: -0.062, New P: -0.128
iter 11 loss: 0.115
Actual params: [ 1.3589, -0.1276]
-Original Grad: 0.090, -lr * Pred Grad: 0.065, New P: 1.424
-Original Grad: 0.190, -lr * Pred Grad: -0.062, New P: -0.190
iter 12 loss: 0.120
Actual params: [ 1.4242, -0.1899]
-Original Grad: 0.074, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: 0.241, -lr * Pred Grad: -0.062, New P: -0.252
iter 13 loss: 0.132
Actual params: [ 1.4894, -0.2522]
-Original Grad: 0.072, -lr * Pred Grad: 0.065, New P: 1.555
-Original Grad: 0.249, -lr * Pred Grad: -0.062, New P: -0.314
iter 14 loss: 0.142
Actual params: [ 1.5546, -0.3145]
-Original Grad: 0.068, -lr * Pred Grad: 0.065, New P: 1.620
-Original Grad: 0.155, -lr * Pred Grad: -0.062, New P: -0.377
iter 15 loss: 0.152
Actual params: [ 1.6199, -0.3768]
-Original Grad: 0.072, -lr * Pred Grad: 0.065, New P: 1.685
-Original Grad: 0.109, -lr * Pred Grad: -0.062, New P: -0.439
iter 16 loss: 0.156
Actual params: [ 1.6851, -0.4391]
-Original Grad: 0.079, -lr * Pred Grad: 0.065, New P: 1.750
-Original Grad: 0.128, -lr * Pred Grad: -0.062, New P: -0.501
iter 17 loss: 0.158
Actual params: [ 1.7503, -0.5013]
-Original Grad: 0.074, -lr * Pred Grad: 0.065, New P: 1.816
-Original Grad: 0.156, -lr * Pred Grad: -0.062, New P: -0.564
iter 18 loss: 0.161
Actual params: [ 1.8156, -0.5636]
-Original Grad: 0.062, -lr * Pred Grad: 0.065, New P: 1.881
-Original Grad: 0.198, -lr * Pred Grad: -0.062, New P: -0.626
iter 19 loss: 0.168
Actual params: [ 1.8808, -0.6259]
-Original Grad: 0.060, -lr * Pred Grad: 0.065, New P: 1.946
-Original Grad: 0.229, -lr * Pred Grad: -0.062, New P: -0.688
iter 20 loss: 0.177
Actual params: [ 1.946 , -0.6882]
