Target params: [1.1812, 0.2779]
Actual params: [0.4128, 0.9469]
-Original Grad: 0.270, -lr * Pred Grad:  0.810, New P: 1.223
-Original Grad: 0.062, -lr * Pred Grad:  0.208, New P: 1.155
iter 0 loss: 0.761
Actual params: [1.2232, 1.1552]
-Original Grad: -3.325, -lr * Pred Grad:  -0.094, New P: 1.129
-Original Grad: -3.098, -lr * Pred Grad:  0.079, New P: 1.234
iter 1 loss: 0.681
Actual params: [1.1294, 1.2344]
-Original Grad: -1.040, -lr * Pred Grad:  -0.012, New P: 1.117
-Original Grad: 0.919, -lr * Pred Grad:  0.006, New P: 1.241
iter 2 loss: 1.724
Actual params: [1.1175, 1.2406]
-Original Grad: -4.079, -lr * Pred Grad:  -0.027, New P: 1.091
-Original Grad: 5.938, -lr * Pred Grad:  0.014, New P: 1.254
iter 3 loss: 1.735
Actual params: [1.0905, 1.2542]
-Original Grad: -8.327, -lr * Pred Grad:  -0.046, New P: 1.044
-Original Grad: 6.939, -lr * Pred Grad:  0.016, New P: 1.270
iter 4 loss: 1.749
Actual params: [1.0445, 1.2699]
-Original Grad: 2.951, -lr * Pred Grad:  0.016, New P: 1.060
-Original Grad: -3.108, -lr * Pred Grad:  -0.007, New P: 1.263
iter 5 loss: 1.743
Actual params: [1.0604, 1.2628]
-Original Grad: -11.213, -lr * Pred Grad:  -0.045, New P: 1.016
-Original Grad: 6.802, -lr * Pred Grad:  0.011, New P: 1.273
iter 6 loss: 1.748
Actual params: [1.0159, 1.2734]
-Original Grad: 2.416, -lr * Pred Grad:  0.009, New P: 1.025
-Original Grad: -2.835, -lr * Pred Grad:  -0.006, New P: 1.268
iter 7 loss: 1.726
Actual params: [1.0251, 1.2676]
-Original Grad: 1.691, -lr * Pred Grad:  0.006, New P: 1.031
-Original Grad: -2.311, -lr * Pred Grad:  -0.005, New P: 1.262
iter 8 loss: 1.742
Actual params: [1.0312, 1.2623]
-Original Grad: -11.348, -lr * Pred Grad:  -0.040, New P: 0.991
-Original Grad: 4.887, -lr * Pred Grad:  0.004, New P: 1.267
iter 9 loss: 1.729
Actual params: [0.9908, 1.2667]
-Original Grad: 0.452, -lr * Pred Grad:  -0.000, New P: 0.991
-Original Grad: -1.832, -lr * Pred Grad:  -0.006, New P: 1.261
iter 10 loss: 1.718
Actual params: [0.9907, 1.261 ]
-Original Grad: 2.146, -lr * Pred Grad:  0.006, New P: 0.997
-Original Grad: -2.884, -lr * Pred Grad:  -0.008, New P: 1.253
iter 11 loss: 1.711
Actual params: [0.9969, 1.2528]
-Original Grad: 1.727, -lr * Pred Grad:  0.005, New P: 1.002
-Original Grad: -2.103, -lr * Pred Grad:  -0.006, New P: 1.247
iter 12 loss: 1.731
Actual params: [1.0022, 1.2465]
-Original Grad: 2.649, -lr * Pred Grad:  0.009, New P: 1.011
-Original Grad: -2.837, -lr * Pred Grad:  -0.009, New P: 1.238
iter 13 loss: 1.714
Actual params: [1.0108, 1.2378]
-Original Grad: 2.400, -lr * Pred Grad:  0.007, New P: 1.018
-Original Grad: -2.852, -lr * Pred Grad:  -0.010, New P: 1.228
iter 14 loss: 1.724
Actual params: [1.0179, 1.228 ]
-Original Grad: 3.445, -lr * Pred Grad:  0.010, New P: 1.028
-Original Grad: -3.844, -lr * Pred Grad:  -0.014, New P: 1.214
iter 15 loss: 1.728
Actual params: [1.0277, 1.2144]
-Original Grad: -17.398, -lr * Pred Grad:  -0.042, New P: 0.986
-Original Grad: 9.705, -lr * Pred Grad:  0.006, New P: 1.221
iter 16 loss: 1.731
Actual params: [0.9859, 1.2205]
-Original Grad: 1.036, -lr * Pred Grad:  -0.004, New P: 0.982
-Original Grad: -2.624, -lr * Pred Grad:  -0.014, New P: 1.207
iter 17 loss: 1.705
Actual params: [0.982, 1.207]
-Original Grad: 1.739, -lr * Pred Grad:  -0.002, New P: 0.980
-Original Grad: -2.934, -lr * Pred Grad:  -0.015, New P: 1.192
iter 18 loss: 1.690
Actual params: [0.9798, 1.1925]
-Original Grad: 1.326, -lr * Pred Grad:  -0.005, New P: 0.975
-Original Grad: -3.096, -lr * Pred Grad:  -0.019, New P: 1.174
iter 19 loss: 1.708
Actual params: [0.9746, 1.1739]
-Original Grad: 2.650, -lr * Pred Grad:  -0.003, New P: 0.972
-Original Grad: -4.053, -lr * Pred Grad:  -0.022, New P: 1.152
iter 20 loss: 1.684
Actual params: [0.9717, 1.1515]
Target params: [1.1812, 0.2779]
Actual params: [0.8379, 0.635 ]
-Original Grad: 0.071, -lr * Pred Grad:  0.221, New P: 1.059
-Original Grad: -0.028, -lr * Pred Grad:  0.071, New P: 0.706
iter 0 loss: 0.245
Actual params: [1.0588, 0.7058]
-Original Grad: -0.049, -lr * Pred Grad:  -0.102, New P: 0.956
-Original Grad: -0.005, -lr * Pred Grad:  -0.065, New P: 0.641
iter 1 loss: 0.238
Actual params: [0.9564, 0.6412]
-Original Grad: 0.007, -lr * Pred Grad:  0.003, New P: 0.960
-Original Grad: -0.026, -lr * Pred Grad:  -0.074, New P: 0.567
iter 2 loss: 0.244
Actual params: [0.9596, 0.5675]
-Original Grad: 0.010, -lr * Pred Grad:  0.022, New P: 0.981
-Original Grad: -0.009, -lr * Pred Grad:  -0.017, New P: 0.550
iter 3 loss: 0.255
Actual params: [0.9814, 0.5503]
-Original Grad: 0.006, -lr * Pred Grad:  0.015, New P: 0.996
-Original Grad: -0.004, -lr * Pred Grad:  -0.008, New P: 0.542
iter 4 loss: 0.257
Actual params: [0.996 , 0.5424]
-Original Grad: -0.008, -lr * Pred Grad:  -0.007, New P: 0.989
-Original Grad: -0.010, -lr * Pred Grad:  -0.006, New P: 0.537
iter 5 loss: 0.257
Actual params: [0.9891, 0.5366]
-Original Grad: 0.003, -lr * Pred Grad:  0.003, New P: 0.992
-Original Grad: 0.003, -lr * Pred Grad:  0.001, New P: 0.538
iter 6 loss: 0.259
Actual params: [0.9917, 0.5378]
-Original Grad: 0.003, -lr * Pred Grad:  0.003, New P: 0.995
-Original Grad: 0.002, -lr * Pred Grad:  -0.000, New P: 0.537
iter 7 loss: 0.259
Actual params: [0.9946, 0.5375]
-Original Grad: -0.002, -lr * Pred Grad:  0.001, New P: 0.996
-Original Grad: -0.005, -lr * Pred Grad:  -0.003, New P: 0.534
iter 8 loss: 0.258
Actual params: [0.996, 0.534]
-Original Grad: 0.003, -lr * Pred Grad:  0.002, New P: 0.998
-Original Grad: 0.003, -lr * Pred Grad:  0.000, New P: 0.534
iter 9 loss: 0.259
Actual params: [0.9978, 0.5343]
-Original Grad: -0.003, -lr * Pred Grad:  -0.000, New P: 0.998
-Original Grad: -0.004, -lr * Pred Grad:  -0.002, New P: 0.533
iter 10 loss: 0.259
Actual params: [0.9976, 0.5326]
-Original Grad: 0.003, -lr * Pred Grad:  0.006, New P: 1.004
-Original Grad: 0.001, -lr * Pred Grad:  -0.004, New P: 0.529
iter 11 loss: 0.259
Actual params: [1.0038, 0.5286]
-Original Grad: -0.003, -lr * Pred Grad:  0.007, New P: 1.011
-Original Grad: -0.007, -lr * Pred Grad:  -0.008, New P: 0.521
iter 12 loss: 0.259
Actual params: [1.0112, 0.5207]
-Original Grad: 0.003, -lr * Pred Grad:  0.006, New P: 1.017
-Original Grad: 0.002, -lr * Pred Grad:  -0.003, New P: 0.517
iter 13 loss: 0.260
Actual params: [1.0173, 0.5173]
-Original Grad: 0.003, -lr * Pred Grad:  0.008, New P: 1.025
-Original Grad: 0.001, -lr * Pred Grad:  -0.005, New P: 0.512
iter 14 loss: 0.260
Actual params: [1.025 , 0.5122]
-Original Grad: -0.002, -lr * Pred Grad:  0.011, New P: 1.036
-Original Grad: -0.006, -lr * Pred Grad:  -0.010, New P: 0.502
iter 15 loss: 0.260
Actual params: [1.036 , 0.5022]
-Original Grad: 0.004, -lr * Pred Grad:  0.022, New P: 1.058
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: 0.486
iter 16 loss: 0.261
Actual params: [1.0579, 0.4863]
-Original Grad: 0.010, -lr * Pred Grad:  0.037, New P: 1.095
-Original Grad: 0.004, -lr * Pred Grad:  -0.025, New P: 0.461
iter 17 loss: 0.262
Actual params: [1.0954, 0.4614]
-Original Grad: 0.010, -lr * Pred Grad:  0.041, New P: 1.136
-Original Grad: 0.004, -lr * Pred Grad:  -0.027, New P: 0.434
iter 18 loss: 0.264
Actual params: [1.1362, 0.4342]
-Original Grad: -0.001, -lr * Pred Grad:  0.019, New P: 1.155
-Original Grad: -0.005, -lr * Pred Grad:  -0.015, New P: 0.419
iter 19 loss: 0.264
Actual params: [1.1554, 0.4188]
-Original Grad: -0.002, -lr * Pred Grad:  0.018, New P: 1.174
-Original Grad: -0.007, -lr * Pred Grad:  -0.016, New P: 0.403
iter 20 loss: 0.263
Actual params: [1.1736, 0.4033]
Target params: [1.1812, 0.2779]
Actual params: [0.5397, 0.7849]
-Original Grad: -0.956, -lr * Pred Grad:  -0.577, New P: -0.038
-Original Grad: -1.750, -lr * Pred Grad:  -0.027, New P: 0.757
iter 0 loss: 1.107
Actual params: [-0.0378,  0.7574]
-Original Grad: 0.185, -lr * Pred Grad:  0.375, New P: 0.337
-Original Grad: -0.573, -lr * Pred Grad:  -0.183, New P: 0.575
iter 1 loss: 0.725
Actual params: [0.3373, 0.5746]
-Original Grad: 1.712, -lr * Pred Grad:  0.044, New P: 0.381
-Original Grad: -0.794, -lr * Pred Grad:  0.003, New P: 0.578
iter 2 loss: 0.403
Actual params: [0.381 , 0.5776]
-Original Grad: -0.329, -lr * Pred Grad:  -0.012, New P: 0.369
-Original Grad: -0.282, -lr * Pred Grad:  -0.013, New P: 0.565
iter 3 loss: 0.242
Actual params: [0.3694, 0.5649]
-Original Grad: 0.332, -lr * Pred Grad:  0.007, New P: 0.376
-Original Grad: 0.583, -lr * Pred Grad:  0.012, New P: 0.577
iter 4 loss: 0.249
Actual params: [0.3759, 0.5767]
-Original Grad: -0.323, -lr * Pred Grad:  -0.006, New P: 0.370
-Original Grad: -0.216, -lr * Pred Grad:  -0.002, New P: 0.574
iter 5 loss: 0.240
Actual params: [0.3698, 0.5744]
-Original Grad: -0.265, -lr * Pred Grad:  -0.005, New P: 0.365
-Original Grad: -0.131, -lr * Pred Grad:  0.000, New P: 0.575
iter 6 loss: 0.240
Actual params: [0.3652, 0.5747]
-Original Grad: -0.032, -lr * Pred Grad:  -0.003, New P: 0.362
-Original Grad: 0.242, -lr * Pred Grad:  0.003, New P: 0.578
iter 7 loss: 0.238
Actual params: [0.3624, 0.5781]
-Original Grad: 0.261, -lr * Pred Grad:  0.000, New P: 0.362
-Original Grad: 0.315, -lr * Pred Grad:  0.002, New P: 0.580
iter 8 loss: 0.236
Actual params: [0.3624, 0.5799]
-Original Grad: -0.120, -lr * Pred Grad:  -0.003, New P: 0.359
-Original Grad: 0.154, -lr * Pred Grad:  0.003, New P: 0.583
iter 9 loss: 0.233
Actual params: [0.3593, 0.5834]
-Original Grad: 0.705, -lr * Pred Grad:  0.001, New P: 0.360
-Original Grad: 0.379, -lr * Pred Grad:  0.000, New P: 0.584
iter 10 loss: 0.234
Actual params: [0.3598, 0.5837]
-Original Grad: 0.272, -lr * Pred Grad:  -0.001, New P: 0.359
-Original Grad: 0.312, -lr * Pred Grad:  0.002, New P: 0.586
iter 11 loss: 0.230
Actual params: [0.3588, 0.586 ]
-Original Grad: 0.719, -lr * Pred Grad:  -0.000, New P: 0.359
-Original Grad: 0.429, -lr * Pred Grad:  0.001, New P: 0.587
iter 12 loss: 0.228
Actual params: [0.3587, 0.5872]
-Original Grad: 0.274, -lr * Pred Grad:  -0.001, New P: 0.358
-Original Grad: 0.256, -lr * Pred Grad:  0.002, New P: 0.589
iter 13 loss: 0.225
Actual params: [0.358 , 0.5891]
-Original Grad: 0.307, -lr * Pred Grad:  -0.001, New P: 0.357
-Original Grad: 0.281, -lr * Pred Grad:  0.002, New P: 0.591
iter 14 loss: 0.224
Actual params: [0.357 , 0.5914]
-Original Grad: 0.556, -lr * Pred Grad:  -0.001, New P: 0.356
-Original Grad: 0.360, -lr * Pred Grad:  0.002, New P: 0.593
iter 15 loss: 0.223
Actual params: [0.3563, 0.5934]
-Original Grad: 1.078, -lr * Pred Grad:  0.000, New P: 0.356
-Original Grad: 0.451, -lr * Pred Grad:  0.000, New P: 0.594
iter 16 loss: 0.222
Actual params: [0.3564, 0.5938]
-Original Grad: 1.112, -lr * Pred Grad:  -0.000, New P: 0.356
-Original Grad: 0.452, -lr * Pred Grad:  0.001, New P: 0.594
iter 17 loss: 0.219
Actual params: [0.3563, 0.5944]
-Original Grad: 0.147, -lr * Pred Grad:  -0.000, New P: 0.356
-Original Grad: 0.141, -lr * Pred Grad:  0.001, New P: 0.596
iter 18 loss: 0.217
Actual params: [0.3559, 0.5957]
-Original Grad: 0.020, -lr * Pred Grad:  -0.001, New P: 0.355
-Original Grad: 0.133, -lr * Pred Grad:  0.002, New P: 0.598
iter 19 loss: 0.217
Actual params: [0.3552, 0.5977]
-Original Grad: 3.021, -lr * Pred Grad:  0.000, New P: 0.356
-Original Grad: 0.828, -lr * Pred Grad:  -0.001, New P: 0.597
iter 20 loss: 0.219
Actual params: [0.3556, 0.597 ]
Target params: [1.1812, 0.2779]
Actual params: [0.5515, 0.3482]
-Original Grad: 0.411, -lr * Pred Grad:  0.927, New P: 1.478
-Original Grad: -0.323, -lr * Pred Grad:  -0.575, New P: -0.227
iter 0 loss: 1.223
Actual params: [ 1.4784, -0.227 ]
-Original Grad: -0.689, -lr * Pred Grad:  -0.212, New P: 1.266
-Original Grad: 0.144, -lr * Pred Grad:  -0.114, New P: -0.341
iter 1 loss: 0.624
Actual params: [ 1.2662, -0.3412]
-Original Grad: -0.547, -lr * Pred Grad:  -0.070, New P: 1.196
-Original Grad: 0.164, -lr * Pred Grad:  0.106, New P: -0.235
iter 2 loss: 0.523
Actual params: [ 1.1963, -0.2354]
-Original Grad: -0.406, -lr * Pred Grad:  -0.066, New P: 1.130
-Original Grad: 0.107, -lr * Pred Grad:  -0.027, New P: -0.262
iter 3 loss: 0.453
Actual params: [ 1.1304, -0.2623]
-Original Grad: -0.244, -lr * Pred Grad:  0.041, New P: 1.172
-Original Grad: 0.117, -lr * Pred Grad:  0.246, New P: -0.016
iter 4 loss: 0.432
Actual params: [ 1.1718, -0.0159]
-Original Grad: -0.247, -lr * Pred Grad:  -0.129, New P: 1.043
-Original Grad: -0.021, -lr * Pred Grad:  -0.364, New P: -0.380
iter 5 loss: 0.418
Actual params: [ 1.0432, -0.3804]
-Original Grad: 0.112, -lr * Pred Grad:  0.095, New P: 1.138
-Original Grad: 0.119, -lr * Pred Grad:  0.310, New P: -0.070
iter 6 loss: 0.458
Actual params: [ 1.1384, -0.0703]
-Original Grad: -0.190, -lr * Pred Grad:  -0.054, New P: 1.084
-Original Grad: -0.010, -lr * Pred Grad:  -0.119, New P: -0.189
iter 7 loss: 0.412
Actual params: [ 1.0841, -0.1889]
-Original Grad: -0.074, -lr * Pred Grad:  -0.000, New P: 1.084
-Original Grad: 0.036, -lr * Pred Grad:  0.041, New P: -0.147
iter 8 loss: 0.410
Actual params: [ 1.0838, -0.1474]
-Original Grad: -0.059, -lr * Pred Grad:  -0.030, New P: 1.054
-Original Grad: -0.019, -lr * Pred Grad:  -0.085, New P: -0.232
iter 9 loss: 0.406
Actual params: [ 1.0541, -0.2324]
-Original Grad: 0.074, -lr * Pred Grad:  0.052, New P: 1.106
-Original Grad: 0.056, -lr * Pred Grad:  0.161, New P: -0.071
iter 10 loss: 0.421
Actual params: [ 1.1062, -0.071 ]
-Original Grad: -0.091, -lr * Pred Grad:  -0.037, New P: 1.069
-Original Grad: -0.018, -lr * Pred Grad:  -0.093, New P: -0.164
iter 11 loss: 0.405
Actual params: [ 1.0687, -0.1644]
-Original Grad: 0.020, -lr * Pred Grad:  -0.001, New P: 1.067
-Original Grad: -0.013, -lr * Pred Grad:  -0.022, New P: -0.186
iter 12 loss: 0.408
Actual params: [ 1.0672, -0.186 ]
-Original Grad: 0.034, -lr * Pred Grad:  0.034, New P: 1.101
-Original Grad: 0.034, -lr * Pred Grad:  0.108, New P: -0.078
iter 13 loss: 0.410
Actual params: [ 1.1008, -0.0779]
-Original Grad: -0.079, -lr * Pred Grad:  -0.042, New P: 1.059
-Original Grad: -0.021, -lr * Pred Grad:  -0.108, New P: -0.186
iter 14 loss: 0.405
Actual params: [ 1.0592, -0.1863]
-Original Grad: 0.064, -lr * Pred Grad:  0.047, New P: 1.107
-Original Grad: 0.039, -lr * Pred Grad:  0.138, New P: -0.049
iter 15 loss: 0.411
Actual params: [ 1.1066, -0.0486]
-Original Grad: -0.091, -lr * Pred Grad:  -0.044, New P: 1.063
-Original Grad: -0.017, -lr * Pred Grad:  -0.103, New P: -0.152
iter 16 loss: 0.405
Actual params: [ 1.0627, -0.1519]
-Original Grad: 0.041, -lr * Pred Grad:  -0.000, New P: 1.063
-Original Grad: -0.023, -lr * Pred Grad:  -0.040, New P: -0.192
iter 17 loss: 0.408
Actual params: [ 1.0627, -0.192 ]
-Original Grad: 0.053, -lr * Pred Grad:  0.056, New P: 1.119
-Original Grad: 0.054, -lr * Pred Grad:  0.173, New P: -0.019
iter 18 loss: 0.412
Actual params: [ 1.1188, -0.0188]
-Original Grad: -0.134, -lr * Pred Grad:  -0.066, New P: 1.052
-Original Grad: -0.033, -lr * Pred Grad:  -0.160, New P: -0.179
iter 19 loss: 0.406
Actual params: [ 1.0524, -0.1791]
-Original Grad: 0.092, -lr * Pred Grad:  0.051, New P: 1.103
-Original Grad: 0.032, -lr * Pred Grad:  0.126, New P: -0.053
iter 20 loss: 0.412
Actual params: [ 1.1032, -0.053 ]
Target params: [1.1812, 0.2779]
Actual params: [1.0909, 1.0332]
-Original Grad: -0.116, -lr * Pred Grad:  -0.128, New P: 0.963
-Original Grad: 0.296, -lr * Pred Grad:  0.241, New P: 1.275
iter 0 loss: 0.606
Actual params: [0.9626, 1.2746]
-Original Grad: 0.627, -lr * Pred Grad:  0.112, New P: 1.074
-Original Grad: 0.282, -lr * Pred Grad:  0.037, New P: 1.311
iter 1 loss: 0.597
Actual params: [1.0743, 1.3112]
-Original Grad: -0.062, -lr * Pred Grad:  -0.035, New P: 1.039
-Original Grad: 0.255, -lr * Pred Grad:  0.087, New P: 1.398
iter 2 loss: 0.551
Actual params: [1.0395, 1.3978]
-Original Grad: 0.023, -lr * Pred Grad:  -0.014, New P: 1.026
-Original Grad: 0.428, -lr * Pred Grad:  0.070, New P: 1.468
iter 3 loss: 0.532
Actual params: [1.026 , 1.4676]
-Original Grad: 0.122, -lr * Pred Grad:  0.008, New P: 1.034
-Original Grad: 0.364, -lr * Pred Grad:  0.031, New P: 1.499
iter 4 loss: 0.512
Actual params: [1.0341, 1.4988]
-Original Grad: 0.047, -lr * Pred Grad:  -0.010, New P: 1.024
-Original Grad: 0.366, -lr * Pred Grad:  0.027, New P: 1.526
iter 5 loss: 0.499
Actual params: [1.0242, 1.526 ]
-Original Grad: 0.117, -lr * Pred Grad:  0.010, New P: 1.034
-Original Grad: 0.329, -lr * Pred Grad:  0.016, New P: 1.542
iter 6 loss: 0.490
Actual params: [1.0339, 1.5423]
-Original Grad: 0.031, -lr * Pred Grad:  -0.014, New P: 1.020
-Original Grad: 0.294, -lr * Pred Grad:  0.018, New P: 1.560
iter 7 loss: 0.480
Actual params: [1.0202, 1.5604]
-Original Grad: 0.112, -lr * Pred Grad:  0.014, New P: 1.034
-Original Grad: 0.266, -lr * Pred Grad:  0.009, New P: 1.569
iter 8 loss: 0.478
Actual params: [1.0339, 1.5693]
-Original Grad: 0.012, -lr * Pred Grad:  -0.020, New P: 1.014
-Original Grad: 0.251, -lr * Pred Grad:  0.016, New P: 1.585
iter 9 loss: 0.471
Actual params: [1.0142, 1.5855]
-Original Grad: 0.142, -lr * Pred Grad:  0.023, New P: 1.037
-Original Grad: 0.235, -lr * Pred Grad:  0.003, New P: 1.588
iter 10 loss: 0.469
Actual params: [1.0373, 1.5881]
-Original Grad: 0.001, -lr * Pred Grad:  -0.021, New P: 1.016
-Original Grad: 0.213, -lr * Pred Grad:  0.015, New P: 1.603
iter 11 loss: 0.461
Actual params: [1.0162, 1.6029]
-Original Grad: 0.094, -lr * Pred Grad:  0.010, New P: 1.026
-Original Grad: 0.208, -lr * Pred Grad:  0.005, New P: 1.608
iter 12 loss: 0.462
Actual params: [1.0262, 1.608 ]
-Original Grad: 0.047, -lr * Pred Grad:  -0.006, New P: 1.020
-Original Grad: 0.193, -lr * Pred Grad:  0.010, New P: 1.618
iter 13 loss: 0.457
Actual params: [1.0201, 1.6175]
-Original Grad: 0.070, -lr * Pred Grad:  0.002, New P: 1.022
-Original Grad: 0.189, -lr * Pred Grad:  0.007, New P: 1.624
iter 14 loss: 0.455
Actual params: [1.0223, 1.624 ]
-Original Grad: 0.056, -lr * Pred Grad:  -0.002, New P: 1.020
-Original Grad: 0.172, -lr * Pred Grad:  0.007, New P: 1.631
iter 15 loss: 0.451
Actual params: [1.0199, 1.6315]
-Original Grad: 0.049, -lr * Pred Grad:  -0.005, New P: 1.015
-Original Grad: 0.158, -lr * Pred Grad:  0.008, New P: 1.639
iter 16 loss: 0.450
Actual params: [1.0152, 1.6392]
-Original Grad: 0.068, -lr * Pred Grad:  0.004, New P: 1.019
-Original Grad: 0.151, -lr * Pred Grad:  0.004, New P: 1.643
iter 17 loss: 0.450
Actual params: [1.0194, 1.6433]
-Original Grad: 0.051, -lr * Pred Grad:  -0.005, New P: 1.014
-Original Grad: 0.146, -lr * Pred Grad:  0.008, New P: 1.651
iter 18 loss: 0.446
Actual params: [1.0144, 1.6508]
-Original Grad: 0.065, -lr * Pred Grad:  0.001, New P: 1.015
-Original Grad: 0.145, -lr * Pred Grad:  0.005, New P: 1.656
iter 19 loss: 0.445
Actual params: [1.0151, 1.6559]
-Original Grad: 0.057, -lr * Pred Grad:  -0.002, New P: 1.013
-Original Grad: 0.134, -lr * Pred Grad:  0.006, New P: 1.662
iter 20 loss: 0.442
Actual params: [1.0126, 1.6621]
Target params: [1.1812, 0.2779]
Actual params: [1.1249, 0.5545]
-Original Grad: 0.673, -lr * Pred Grad:  1.965, New P: 3.090
-Original Grad: -0.218, -lr * Pred Grad:  -0.357, New P: 0.198
iter 0 loss: 1.819
Actual params: [3.0901, 0.1979]
-Original Grad: 0.045, -lr * Pred Grad:  0.086, New P: 3.177
-Original Grad: -0.013, -lr * Pred Grad:  -0.016, New P: 0.182
iter 1 loss: 0.435
Actual params: [3.1766, 0.182 ]
-Original Grad: 0.034, -lr * Pred Grad:  0.050, New P: 3.227
-Original Grad: -0.010, -lr * Pred Grad:  -0.009, New P: 0.172
iter 2 loss: 0.427
Actual params: [3.227 , 0.1725]
-Original Grad: 0.028, -lr * Pred Grad:  0.035, New P: 3.262
-Original Grad: -0.010, -lr * Pred Grad:  -0.007, New P: 0.166
iter 3 loss: 0.422
Actual params: [3.2618, 0.1657]
-Original Grad: 0.025, -lr * Pred Grad:  0.027, New P: 3.289
-Original Grad: -0.008, -lr * Pred Grad:  -0.005, New P: 0.160
iter 4 loss: 0.418
Actual params: [3.289 , 0.1603]
-Original Grad: 0.021, -lr * Pred Grad:  0.020, New P: 3.309
-Original Grad: -0.009, -lr * Pred Grad:  -0.004, New P: 0.156
iter 5 loss: 0.417
Actual params: [3.3094, 0.1561]
-Original Grad: 0.019, -lr * Pred Grad:  0.017, New P: 3.327
-Original Grad: -0.009, -lr * Pred Grad:  -0.004, New P: 0.152
iter 6 loss: 0.415
Actual params: [3.3265, 0.1523]
-Original Grad: 0.017, -lr * Pred Grad:  0.015, New P: 3.341
-Original Grad: -0.010, -lr * Pred Grad:  -0.003, New P: 0.149
iter 7 loss: 0.413
Actual params: [3.341 , 0.1489]
-Original Grad: 0.015, -lr * Pred Grad:  0.013, New P: 3.354
-Original Grad: -0.010, -lr * Pred Grad:  -0.003, New P: 0.146
iter 8 loss: 0.412
Actual params: [3.3538, 0.1458]
-Original Grad: 0.014, -lr * Pred Grad:  0.011, New P: 3.365
-Original Grad: -0.011, -lr * Pred Grad:  -0.003, New P: 0.143
iter 9 loss: 0.411
Actual params: [3.365 , 0.1428]
-Original Grad: 0.013, -lr * Pred Grad:  0.010, New P: 3.375
-Original Grad: -0.010, -lr * Pred Grad:  -0.003, New P: 0.140
iter 10 loss: 0.409
Actual params: [3.3749, 0.1399]
-Original Grad: 0.012, -lr * Pred Grad:  0.009, New P: 3.384
-Original Grad: -0.012, -lr * Pred Grad:  -0.003, New P: 0.137
iter 11 loss: 0.408
Actual params: [3.3839, 0.137 ]
-Original Grad: 0.011, -lr * Pred Grad:  0.008, New P: 3.392
-Original Grad: -0.012, -lr * Pred Grad:  -0.003, New P: 0.134
iter 12 loss: 0.407
Actual params: [3.3921, 0.1341]
-Original Grad: 0.010, -lr * Pred Grad:  0.008, New P: 3.400
-Original Grad: -0.013, -lr * Pred Grad:  -0.003, New P: 0.131
iter 13 loss: 0.407
Actual params: [3.3999, 0.131 ]
-Original Grad: 0.010, -lr * Pred Grad:  0.008, New P: 3.408
-Original Grad: -0.014, -lr * Pred Grad:  -0.003, New P: 0.128
iter 14 loss: 0.406
Actual params: [3.4075, 0.1276]
-Original Grad: 0.009, -lr * Pred Grad:  0.007, New P: 3.415
-Original Grad: -0.015, -lr * Pred Grad:  -0.004, New P: 0.124
iter 15 loss: 0.405
Actual params: [3.4147, 0.1239]
-Original Grad: 0.008, -lr * Pred Grad:  0.007, New P: 3.421
-Original Grad: -0.015, -lr * Pred Grad:  -0.004, New P: 0.120
iter 16 loss: 0.404
Actual params: [3.4212, 0.1202]
-Original Grad: 0.008, -lr * Pred Grad:  0.006, New P: 3.427
-Original Grad: -0.016, -lr * Pred Grad:  -0.004, New P: 0.116
iter 17 loss: 0.403
Actual params: [3.4273, 0.1161]
-Original Grad: 0.007, -lr * Pred Grad:  0.006, New P: 3.433
-Original Grad: -0.016, -lr * Pred Grad:  -0.004, New P: 0.112
iter 18 loss: 0.405
Actual params: [3.4329, 0.1118]
-Original Grad: 0.007, -lr * Pred Grad:  0.005, New P: 3.438
-Original Grad: -0.016, -lr * Pred Grad:  -0.005, New P: 0.107
iter 19 loss: 0.404
Actual params: [3.4384, 0.1073]
-Original Grad: 0.006, -lr * Pred Grad:  0.005, New P: 3.444
-Original Grad: -0.017, -lr * Pred Grad:  -0.005, New P: 0.102
iter 20 loss: 0.404
Actual params: [3.4436, 0.1022]
Target params: [1.1812, 0.2779]
Actual params: [0.6859, 0.3761]
-Original Grad: -0.007, -lr * Pred Grad:  -0.004, New P: 0.682
-Original Grad: -0.047, -lr * Pred Grad:  -0.228, New P: 0.149
iter 0 loss: 0.285
Actual params: [0.6822, 0.1485]
-Original Grad: 0.043, -lr * Pred Grad:  0.096, New P: 0.778
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: 0.135
iter 1 loss: 0.288
Actual params: [0.7781, 0.1352]
-Original Grad: 0.047, -lr * Pred Grad:  0.093, New P: 0.871
-Original Grad: 0.010, -lr * Pred Grad:  0.039, New P: 0.174
iter 2 loss: 0.281
Actual params: [0.8713, 0.1744]
-Original Grad: 0.027, -lr * Pred Grad:  0.055, New P: 0.926
-Original Grad: 0.012, -lr * Pred Grad:  0.061, New P: 0.235
iter 3 loss: 0.265
Actual params: [0.9262, 0.2351]
-Original Grad: 0.023, -lr * Pred Grad:  0.050, New P: 0.976
-Original Grad: 0.010, -lr * Pred Grad:  0.054, New P: 0.289
iter 4 loss: 0.257
Actual params: [0.9762, 0.2895]
-Original Grad: 0.013, -lr * Pred Grad:  0.034, New P: 1.010
-Original Grad: -0.004, -lr * Pred Grad:  -0.028, New P: 0.262
iter 5 loss: 0.253
Actual params: [1.0099, 0.2615]
-Original Grad: 0.010, -lr * Pred Grad:  0.026, New P: 1.036
-Original Grad: 0.008, -lr * Pred Grad:  0.053, New P: 0.315
iter 6 loss: 0.256
Actual params: [1.0362, 0.3145]
-Original Grad: 0.010, -lr * Pred Grad:  0.026, New P: 1.062
-Original Grad: -0.016, -lr * Pred Grad:  -0.094, New P: 0.220
iter 7 loss: 0.255
Actual params: [1.0622, 0.2203]
-Original Grad: 0.007, -lr * Pred Grad:  0.028, New P: 1.090
-Original Grad: 0.014, -lr * Pred Grad:  0.098, New P: 0.319
iter 8 loss: 0.260
Actual params: [1.0904, 0.3187]
-Original Grad: 0.007, -lr * Pred Grad:  0.015, New P: 1.106
-Original Grad: -0.016, -lr * Pred Grad:  -0.099, New P: 0.220
iter 9 loss: 0.256
Actual params: [1.1058, 0.2196]
-Original Grad: 0.004, -lr * Pred Grad:  0.024, New P: 1.129
-Original Grad: 0.016, -lr * Pred Grad:  0.111, New P: 0.330
iter 10 loss: 0.261
Actual params: [1.1294, 0.3304]
-Original Grad: 0.006, -lr * Pred Grad:  0.012, New P: 1.141
-Original Grad: -0.015, -lr * Pred Grad:  -0.097, New P: 0.234
iter 11 loss: 0.257
Actual params: [1.141 , 0.2337]
-Original Grad: 0.003, -lr * Pred Grad:  0.023, New P: 1.165
-Original Grad: 0.011, -lr * Pred Grad:  0.080, New P: 0.314
iter 12 loss: 0.262
Actual params: [1.1645, 0.3138]
-Original Grad: 0.005, -lr * Pred Grad:  0.011, New P: 1.175
-Original Grad: -0.012, -lr * Pred Grad:  -0.082, New P: 0.232
iter 13 loss: 0.259
Actual params: [1.1755, 0.2318]
-Original Grad: 0.003, -lr * Pred Grad:  0.030, New P: 1.205
-Original Grad: 0.011, -lr * Pred Grad:  0.088, New P: 0.320
iter 14 loss: 0.263
Actual params: [1.2053, 0.3201]
-Original Grad: 0.003, -lr * Pred Grad:  0.002, New P: 1.208
-Original Grad: -0.011, -lr * Pred Grad:  -0.081, New P: 0.239
iter 15 loss: 0.261
Actual params: [1.2077, 0.239 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.033, New P: 1.241
-Original Grad: 0.010, -lr * Pred Grad:  0.092, New P: 0.331
iter 16 loss: 0.264
Actual params: [1.2407, 0.3312]
-Original Grad: 0.002, -lr * Pred Grad:  -0.024, New P: 1.217
-Original Grad: -0.018, -lr * Pred Grad:  -0.141, New P: 0.190
iter 17 loss: 0.263
Actual params: [1.2172, 0.19  ]
-Original Grad: 0.002, -lr * Pred Grad:  0.058, New P: 1.275
-Original Grad: 0.018, -lr * Pred Grad:  0.161, New P: 0.351
iter 18 loss: 0.267
Actual params: [1.275 , 0.3511]
-Original Grad: 0.003, -lr * Pred Grad:  -0.028, New P: 1.247
-Original Grad: -0.022, -lr * Pred Grad:  -0.165, New P: 0.186
iter 19 loss: 0.266
Actual params: [1.2474, 0.1861]
-Original Grad: 0.003, -lr * Pred Grad:  0.077, New P: 1.324
-Original Grad: 0.018, -lr * Pred Grad:  0.155, New P: 0.341
iter 20 loss: 0.269
Actual params: [1.3239, 0.341 ]
Target params: [1.1812, 0.2779]
Actual params: [0.7976, 0.9218]
-Original Grad: -5.317, -lr * Pred Grad:  -0.515, New P: 0.283
-Original Grad: -2.236, -lr * Pred Grad:  0.848, New P: 1.770
iter 0 loss: 1.568
Actual params: [0.2826, 1.77  ]
-Original Grad: 0.323, -lr * Pred Grad:  0.159, New P: 0.442
-Original Grad: 0.017, -lr * Pred Grad:  -0.330, New P: 1.440
iter 1 loss: 0.521
Actual params: [0.442, 1.44 ]
-Original Grad: -0.062, -lr * Pred Grad:  -0.103, New P: 0.339
-Original Grad: 0.268, -lr * Pred Grad:  0.227, New P: 1.667
iter 2 loss: 0.569
Actual params: [0.3388, 1.6671]
-Original Grad: 0.237, -lr * Pred Grad:  0.050, New P: 0.389
-Original Grad: -0.038, -lr * Pred Grad:  -0.098, New P: 1.569
iter 3 loss: 0.525
Actual params: [0.3888, 1.5691]
-Original Grad: 0.292, -lr * Pred Grad:  0.043, New P: 0.432
-Original Grad: -0.019, -lr * Pred Grad:  -0.079, New P: 1.490
iter 4 loss: 0.539
Actual params: [0.4316, 1.49  ]
-Original Grad: -0.021, -lr * Pred Grad:  -0.046, New P: 0.386
-Original Grad: 0.279, -lr * Pred Grad:  0.110, New P: 1.600
iter 5 loss: 0.554
Actual params: [0.3855, 1.6001]
-Original Grad: 0.281, -lr * Pred Grad:  0.032, New P: 0.417
-Original Grad: -0.038, -lr * Pred Grad:  -0.054, New P: 1.546
iter 6 loss: 0.530
Actual params: [0.4173, 1.5464]
-Original Grad: 0.261, -lr * Pred Grad:  0.016, New P: 0.434
-Original Grad: 0.054, -lr * Pred Grad:  -0.015, New P: 1.531
iter 7 loss: 0.539
Actual params: [0.4336, 1.5311]
-Original Grad: 0.041, -lr * Pred Grad:  -0.017, New P: 0.417
-Original Grad: 0.196, -lr * Pred Grad:  0.051, New P: 1.582
iter 8 loss: 0.538
Actual params: [0.4168, 1.5822]
-Original Grad: -0.132, -lr * Pred Grad:  -0.003, New P: 0.414
-Original Grad: 0.156, -lr * Pred Grad:  0.015, New P: 1.597
iter 9 loss: 0.524
Actual params: [0.4143, 1.5968]
-Original Grad: 0.309, -lr * Pred Grad:  0.008, New P: 0.422
-Original Grad: -0.040, -lr * Pred Grad:  -0.003, New P: 1.594
iter 10 loss: 0.519
Actual params: [0.4223, 1.5941]
-Original Grad: -0.129, -lr * Pred Grad:  0.000, New P: 0.422
-Original Grad: 0.158, -lr * Pred Grad:  0.012, New P: 1.606
iter 11 loss: 0.517
Actual params: [0.4224, 1.6062]
-Original Grad: 0.337, -lr * Pred Grad:  0.007, New P: 0.430
-Original Grad: -0.033, -lr * Pred Grad:  0.004, New P: 1.610
iter 12 loss: 0.512
Actual params: [0.4298, 1.6097]
-Original Grad: -0.200, -lr * Pred Grad:  0.001, New P: 0.431
-Original Grad: 0.176, -lr * Pred Grad:  0.012, New P: 1.621
iter 13 loss: 0.508
Actual params: [0.4307, 1.6213]
-Original Grad: 0.213, -lr * Pred Grad:  0.005, New P: 0.435
-Original Grad: -0.001, -lr * Pred Grad:  0.006, New P: 1.628
iter 14 loss: 0.503
Actual params: [0.4354, 1.6278]
-Original Grad: 0.147, -lr * Pred Grad:  0.005, New P: 0.440
-Original Grad: 0.029, -lr * Pred Grad:  0.008, New P: 1.636
iter 15 loss: 0.498
Actual params: [0.44 , 1.636]
-Original Grad: 0.129, -lr * Pred Grad:  0.004, New P: 0.444
-Original Grad: 0.030, -lr * Pred Grad:  0.008, New P: 1.644
iter 16 loss: 0.492
Actual params: [0.4444, 1.6444]
-Original Grad: 0.133, -lr * Pred Grad:  0.004, New P: 0.449
-Original Grad: 0.019, -lr * Pred Grad:  0.008, New P: 1.652
iter 17 loss: 0.487
Actual params: [0.4488, 1.6521]
-Original Grad: 0.111, -lr * Pred Grad:  0.005, New P: 0.453
-Original Grad: 0.028, -lr * Pred Grad:  0.009, New P: 1.661
iter 18 loss: 0.482
Actual params: [0.4534, 1.6606]
-Original Grad: 0.013, -lr * Pred Grad:  0.003, New P: 0.456
-Original Grad: 0.051, -lr * Pred Grad:  0.008, New P: 1.668
iter 19 loss: 0.477
Actual params: [0.4563, 1.6683]
-Original Grad: 0.098, -lr * Pred Grad:  0.004, New P: 0.460
-Original Grad: 0.010, -lr * Pred Grad:  0.007, New P: 1.675
iter 20 loss: 0.472
Actual params: [0.4602, 1.675 ]
Target params: [1.1812, 0.2779]
Actual params: [0.7765, 0.9   ]
-Original Grad: 1.699, -lr * Pred Grad:  0.153, New P: 0.930
-Original Grad: -1.217, -lr * Pred Grad:  -0.081, New P: 0.820
iter 0 loss: 0.762
Actual params: [0.9295, 0.8195]
-Original Grad: -8.740, -lr * Pred Grad:  -0.062, New P: 0.868
-Original Grad: -3.424, -lr * Pred Grad:  -0.183, New P: 0.636
iter 1 loss: 1.099
Actual params: [0.868 , 0.6362]
-Original Grad: 1.392, -lr * Pred Grad:  0.013, New P: 0.881
-Original Grad: -0.165, -lr * Pred Grad:  -0.018, New P: 0.618
iter 2 loss: 0.567
Actual params: [0.881 , 0.6181]
-Original Grad: 1.317, -lr * Pred Grad:  0.011, New P: 0.892
-Original Grad: -0.149, -lr * Pred Grad:  -0.015, New P: 0.603
iter 3 loss: 0.552
Actual params: [0.8922, 0.603 ]
-Original Grad: 1.363, -lr * Pred Grad:  0.011, New P: 0.903
-Original Grad: -0.168, -lr * Pred Grad:  -0.015, New P: 0.588
iter 4 loss: 0.539
Actual params: [0.9029, 0.5876]
-Original Grad: 1.368, -lr * Pred Grad:  0.010, New P: 0.913
-Original Grad: -0.196, -lr * Pred Grad:  -0.017, New P: 0.571
iter 5 loss: 0.525
Actual params: [0.9128, 0.5709]
-Original Grad: 1.380, -lr * Pred Grad:  0.009, New P: 0.922
-Original Grad: -0.230, -lr * Pred Grad:  -0.019, New P: 0.552
iter 6 loss: 0.508
Actual params: [0.9218, 0.5521]
-Original Grad: 1.407, -lr * Pred Grad:  0.008, New P: 0.930
-Original Grad: -0.281, -lr * Pred Grad:  -0.022, New P: 0.530
iter 7 loss: 0.487
Actual params: [0.9299, 0.5299]
-Original Grad: 1.346, -lr * Pred Grad:  0.007, New P: 0.937
-Original Grad: -0.327, -lr * Pred Grad:  -0.026, New P: 0.504
iter 8 loss: 0.463
Actual params: [0.9365, 0.5043]
-Original Grad: 1.356, -lr * Pred Grad:  0.005, New P: 0.941
-Original Grad: -0.436, -lr * Pred Grad:  -0.033, New P: 0.471
iter 9 loss: 0.433
Actual params: [0.9413, 0.4713]
-Original Grad: 1.317, -lr * Pred Grad:  0.003, New P: 0.944
-Original Grad: -0.521, -lr * Pred Grad:  -0.035, New P: 0.436
iter 10 loss: 0.387
Actual params: [0.9438, 0.4361]
-Original Grad: 1.318, -lr * Pred Grad:  -0.000, New P: 0.943
-Original Grad: -0.712, -lr * Pred Grad:  -0.035, New P: 0.401
iter 11 loss: 0.329
Actual params: [0.9434, 0.4011]
-Original Grad: 0.654, -lr * Pred Grad:  0.002, New P: 0.946
-Original Grad: -0.216, -lr * Pred Grad:  -0.002, New P: 0.399
iter 12 loss: 0.258
Actual params: [0.9457, 0.3995]
-Original Grad: 0.279, -lr * Pred Grad:  0.003, New P: 0.949
-Original Grad: 0.046, -lr * Pred Grad:  0.005, New P: 0.405
iter 13 loss: 0.245
Actual params: [0.9486, 0.4047]
-Original Grad: 0.263, -lr * Pred Grad:  0.003, New P: 0.951
-Original Grad: 0.030, -lr * Pred Grad:  0.004, New P: 0.409
iter 14 loss: 0.246
Actual params: [0.9513, 0.409 ]
-Original Grad: 0.267, -lr * Pred Grad:  0.002, New P: 0.954
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: 0.412
iter 15 loss: 0.246
Actual params: [0.9538, 0.4125]
-Original Grad: 0.404, -lr * Pred Grad:  0.002, New P: 0.956
-Original Grad: -0.123, -lr * Pred Grad:  0.002, New P: 0.414
iter 16 loss: 0.246
Actual params: [0.9561, 0.4142]
-Original Grad: 0.222, -lr * Pred Grad:  0.002, New P: 0.959
-Original Grad: 0.014, -lr * Pred Grad:  0.003, New P: 0.418
iter 17 loss: 0.242
Actual params: [0.9585, 0.4176]
-Original Grad: 0.387, -lr * Pred Grad:  0.002, New P: 0.961
-Original Grad: -0.148, -lr * Pred Grad:  0.001, New P: 0.419
iter 18 loss: 0.242
Actual params: [0.9607, 0.4191]
-Original Grad: 0.209, -lr * Pred Grad:  0.003, New P: 0.963
-Original Grad: 0.029, -lr * Pred Grad:  0.004, New P: 0.423
iter 19 loss: 0.239
Actual params: [0.9635, 0.4226]
-Original Grad: 0.352, -lr * Pred Grad:  0.002, New P: 0.966
-Original Grad: -0.132, -lr * Pred Grad:  0.002, New P: 0.424
iter 20 loss: 0.240
Actual params: [0.9658, 0.4245]
Target params: [1.1812, 0.2779]
Actual params: [0.9202, 0.6989]
-Original Grad: -0.122, -lr * Pred Grad:  -0.348, New P: 0.572
-Original Grad: -1.114, -lr * Pred Grad:  -0.239, New P: 0.460
iter 0 loss: 0.902
Actual params: [0.5718, 0.4599]
-Original Grad: 0.015, -lr * Pred Grad:  0.044, New P: 0.616
-Original Grad: -0.255, -lr * Pred Grad:  -0.040, New P: 0.420
iter 1 loss: 0.366
Actual params: [0.6159, 0.4197]
-Original Grad: -0.003, -lr * Pred Grad:  -0.027, New P: 0.588
-Original Grad: -0.130, -lr * Pred Grad:  -0.017, New P: 0.403
iter 2 loss: 0.314
Actual params: [0.5884, 0.4028]
-Original Grad: -0.007, -lr * Pred Grad:  -0.053, New P: 0.536
-Original Grad: -0.168, -lr * Pred Grad:  -0.021, New P: 0.381
iter 3 loss: 0.302
Actual params: [0.5356, 0.3813]
-Original Grad: -0.002, -lr * Pred Grad:  -0.032, New P: 0.504
-Original Grad: -0.155, -lr * Pred Grad:  -0.019, New P: 0.362
iter 4 loss: 0.290
Actual params: [0.5037, 0.3622]
-Original Grad: 0.013, -lr * Pred Grad:  0.043, New P: 0.547
-Original Grad: -0.139, -lr * Pred Grad:  -0.015, New P: 0.347
iter 5 loss: 0.282
Actual params: [0.5466, 0.3472]
-Original Grad: 0.003, -lr * Pred Grad:  -0.011, New P: 0.536
-Original Grad: -0.124, -lr * Pred Grad:  -0.015, New P: 0.332
iter 6 loss: 0.272
Actual params: [0.5358, 0.3317]
-Original Grad: 0.009, -lr * Pred Grad:  0.024, New P: 0.560
-Original Grad: -0.100, -lr * Pred Grad:  -0.011, New P: 0.320
iter 7 loss: 0.265
Actual params: [0.56  , 0.3203]
-Original Grad: 0.002, -lr * Pred Grad:  -0.017, New P: 0.543
-Original Grad: -0.094, -lr * Pred Grad:  -0.013, New P: 0.307
iter 8 loss: 0.258
Actual params: [0.5427, 0.3069]
-Original Grad: 0.006, -lr * Pred Grad:  0.013, New P: 0.555
-Original Grad: -0.077, -lr * Pred Grad:  -0.010, New P: 0.297
iter 9 loss: 0.254
Actual params: [0.5552, 0.2966]
-Original Grad: 0.000, -lr * Pred Grad:  -0.030, New P: 0.525
-Original Grad: -0.077, -lr * Pred Grad:  -0.013, New P: 0.283
iter 10 loss: 0.250
Actual params: [0.5254, 0.2833]
-Original Grad: 0.011, -lr * Pred Grad:  0.051, New P: 0.577
-Original Grad: -0.070, -lr * Pred Grad:  -0.008, New P: 0.275
iter 11 loss: 0.248
Actual params: [0.5767, 0.2751]
-Original Grad: 0.000, -lr * Pred Grad:  -0.032, New P: 0.545
-Original Grad: -0.068, -lr * Pred Grad:  -0.014, New P: 0.261
iter 12 loss: 0.242
Actual params: [0.5448, 0.2612]
-Original Grad: 0.008, -lr * Pred Grad:  0.035, New P: 0.580
-Original Grad: -0.056, -lr * Pred Grad:  -0.008, New P: 0.253
iter 13 loss: 0.241
Actual params: [0.5796, 0.253 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.033, New P: 0.547
-Original Grad: -0.056, -lr * Pred Grad:  -0.014, New P: 0.239
iter 14 loss: 0.238
Actual params: [0.5466, 0.2394]
-Original Grad: 0.005, -lr * Pred Grad:  0.014, New P: 0.561
-Original Grad: -0.051, -lr * Pred Grad:  -0.010, New P: 0.229
iter 15 loss: 0.238
Actual params: [0.5608, 0.2294]
-Original Grad: 0.003, -lr * Pred Grad:  -0.009, New P: 0.552
-Original Grad: -0.048, -lr * Pred Grad:  -0.012, New P: 0.218
iter 16 loss: 0.236
Actual params: [0.5517, 0.2177]
-Original Grad: 0.003, -lr * Pred Grad:  -0.007, New P: 0.545
-Original Grad: -0.046, -lr * Pred Grad:  -0.012, New P: 0.206
iter 17 loss: 0.235
Actual params: [0.5449, 0.2058]
-Original Grad: 0.003, -lr * Pred Grad:  -0.005, New P: 0.540
-Original Grad: -0.042, -lr * Pred Grad:  -0.011, New P: 0.194
iter 18 loss: 0.234
Actual params: [0.54  , 0.1943]
-Original Grad: 0.008, -lr * Pred Grad:  0.033, New P: 0.573
-Original Grad: -0.042, -lr * Pred Grad:  -0.008, New P: 0.186
iter 19 loss: 0.233
Actual params: [0.5727, 0.1861]
-Original Grad: 0.002, -lr * Pred Grad:  -0.024, New P: 0.549
-Original Grad: -0.040, -lr * Pred Grad:  -0.014, New P: 0.172
iter 20 loss: 0.230
Actual params: [0.5492, 0.172 ]
