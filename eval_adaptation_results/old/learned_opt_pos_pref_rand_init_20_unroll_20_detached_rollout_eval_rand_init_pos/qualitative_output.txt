Target params: [1.1812, 0.2779]
Actual params: [1.084 , 1.4018]
-Original Grad: -0.370, -lr * Pred Grad: 0.001, New P: 1.085
-Original Grad: 0.147, -lr * Pred Grad: 0.006, New P: 1.408
iter 0 loss: 0.234
Actual params: [1.085 , 1.4083]
-Original Grad: -0.255, -lr * Pred Grad: -0.004, New P: 1.081
-Original Grad: 0.120, -lr * Pred Grad: 0.015, New P: 1.424
iter 1 loss: 0.230
Actual params: [1.0809, 1.4237]
-Original Grad: -0.175, -lr * Pred Grad: -0.012, New P: 1.068
-Original Grad: 0.096, -lr * Pred Grad: 0.018, New P: 1.442
iter 2 loss: 0.211
Actual params: [1.0685, 1.4422]
-Original Grad: -0.677, -lr * Pred Grad: -0.014, New P: 1.055
-Original Grad: 0.320, -lr * Pred Grad: 0.019, New P: 1.461
iter 3 loss: 0.176
Actual params: [1.0547, 1.4612]
-Original Grad: -0.458, -lr * Pred Grad: -0.015, New P: 1.040
-Original Grad: 0.068, -lr * Pred Grad: 0.019, New P: 1.480
iter 4 loss: 0.137
Actual params: [1.0401, 1.4804]
-Original Grad: -0.778, -lr * Pred Grad: -0.015, New P: 1.025
-Original Grad: 0.242, -lr * Pred Grad: 0.019, New P: 1.500
iter 5 loss: 0.109
Actual params: [1.0252, 1.4996]
-Original Grad: -0.957, -lr * Pred Grad: -0.015, New P: 1.010
-Original Grad: 0.356, -lr * Pred Grad: 0.019, New P: 1.519
iter 6 loss: 0.070
Actual params: [1.0104, 1.5187]
-Original Grad: -0.931, -lr * Pred Grad: -0.015, New P: 0.995
-Original Grad: 0.197, -lr * Pred Grad: 0.019, New P: 1.538
iter 7 loss: 0.060
Actual params: [0.9955, 1.5379]
-Original Grad: -0.976, -lr * Pred Grad: -0.015, New P: 0.981
-Original Grad: 0.308, -lr * Pred Grad: 0.019, New P: 1.557
iter 8 loss: 0.084
Actual params: [0.9806, 1.5571]
-Original Grad: -0.908, -lr * Pred Grad: -0.015, New P: 0.966
-Original Grad: 0.274, -lr * Pred Grad: 0.019, New P: 1.576
iter 9 loss: 0.123
Actual params: [0.9657, 1.5762]
-Original Grad: -0.810, -lr * Pred Grad: -0.015, New P: 0.951
-Original Grad: 0.245, -lr * Pred Grad: 0.019, New P: 1.595
iter 10 loss: 0.154
Actual params: [0.9508, 1.5954]
-Original Grad: -0.857, -lr * Pred Grad: -0.015, New P: 0.936
-Original Grad: 0.193, -lr * Pred Grad: 0.019, New P: 1.615
iter 11 loss: 0.208
Actual params: [0.9359, 1.6146]
-Original Grad: -0.737, -lr * Pred Grad: -0.015, New P: 0.921
-Original Grad: 0.205, -lr * Pred Grad: 0.019, New P: 1.634
iter 12 loss: 0.247
Actual params: [0.9209, 1.6337]
-Original Grad: -0.800, -lr * Pred Grad: -0.015, New P: 0.906
-Original Grad: 0.231, -lr * Pred Grad: 0.019, New P: 1.653
iter 13 loss: 0.287
Actual params: [0.906 , 1.6529]
-Original Grad: -0.683, -lr * Pred Grad: -0.015, New P: 0.891
-Original Grad: 0.152, -lr * Pred Grad: 0.019, New P: 1.672
iter 14 loss: 0.316
Actual params: [0.8911, 1.6721]
-Original Grad: -0.780, -lr * Pred Grad: -0.015, New P: 0.876
-Original Grad: 0.317, -lr * Pred Grad: 0.019, New P: 1.691
iter 15 loss: 0.356
Actual params: [0.8762, 1.6912]
-Original Grad: -0.487, -lr * Pred Grad: -0.015, New P: 0.861
-Original Grad: 0.157, -lr * Pred Grad: 0.019, New P: 1.710
iter 16 loss: 0.382
Actual params: [0.8613, 1.7104]
-Original Grad: -0.573, -lr * Pred Grad: -0.015, New P: 0.846
-Original Grad: 0.265, -lr * Pred Grad: 0.019, New P: 1.730
iter 17 loss: 0.415
Actual params: [0.8464, 1.7296]
-Original Grad: -0.611, -lr * Pred Grad: -0.015, New P: 0.831
-Original Grad: 0.176, -lr * Pred Grad: 0.019, New P: 1.749
iter 18 loss: 0.449
Actual params: [0.8315, 1.7487]
-Original Grad: -0.261, -lr * Pred Grad: -0.015, New P: 0.817
-Original Grad: 0.213, -lr * Pred Grad: 0.019, New P: 1.768
iter 19 loss: 0.477
Actual params: [0.8166, 1.7679]
-Original Grad: -0.380, -lr * Pred Grad: -0.015, New P: 0.802
-Original Grad: 0.155, -lr * Pred Grad: 0.019, New P: 1.787
iter 20 loss: 0.495
Actual params: [0.8017, 1.787 ]
Target params: [1.1812, 0.2779]
Actual params: [ 0.0029, -1.5044]
-Original Grad: -0.016, -lr * Pred Grad: 0.005, New P: 0.008
-Original Grad: -0.062, -lr * Pred Grad: 0.004, New P: -1.500
iter 0 loss: 0.132
Actual params: [ 0.0077, -1.5   ]
-Original Grad: -0.016, -lr * Pred Grad: 0.013, New P: 0.021
-Original Grad: -0.069, -lr * Pred Grad: 0.012, New P: -1.488
iter 1 loss: 0.133
Actual params: [ 0.0208, -1.4883]
-Original Grad: -0.011, -lr * Pred Grad: 0.018, New P: 0.039
-Original Grad: -0.058, -lr * Pred Grad: 0.017, New P: -1.472
iter 2 loss: 0.133
Actual params: [ 0.0385, -1.4716]
-Original Grad: -0.006, -lr * Pred Grad: 0.019, New P: 0.057
-Original Grad: -0.062, -lr * Pred Grad: 0.018, New P: -1.453
iter 3 loss: 0.134
Actual params: [ 0.0574, -1.4533]
-Original Grad: -0.011, -lr * Pred Grad: 0.019, New P: 0.077
-Original Grad: -0.079, -lr * Pred Grad: 0.019, New P: -1.434
iter 4 loss: 0.134
Actual params: [ 0.0765, -1.4345]
-Original Grad: -0.016, -lr * Pred Grad: 0.019, New P: 0.096
-Original Grad: -0.074, -lr * Pred Grad: 0.019, New P: -1.416
iter 5 loss: 0.135
Actual params: [ 0.0957, -1.4157]
-Original Grad: -0.016, -lr * Pred Grad: 0.019, New P: 0.115
-Original Grad: -0.099, -lr * Pred Grad: 0.017, New P: -1.399
iter 6 loss: 0.135
Actual params: [ 0.1148, -1.3989]
-Original Grad: -0.010, -lr * Pred Grad: 0.019, New P: 0.134
-Original Grad: -0.081, -lr * Pred Grad: 0.011, New P: -1.387
iter 7 loss: 0.135
Actual params: [ 0.134 , -1.3875]
-Original Grad: -0.017, -lr * Pred Grad: 0.019, New P: 0.153
-Original Grad: -0.107, -lr * Pred Grad: 0.003, New P: -1.384
iter 8 loss: 0.135
Actual params: [ 0.1532, -1.3842]
-Original Grad: -0.007, -lr * Pred Grad: 0.019, New P: 0.172
-Original Grad: -0.100, -lr * Pred Grad: -0.005, New P: -1.389
iter 9 loss: 0.134
Actual params: [ 0.1723, -1.3891]
-Original Grad: -0.001, -lr * Pred Grad: 0.019, New P: 0.191
-Original Grad: -0.094, -lr * Pred Grad: -0.010, New P: -1.399
iter 10 loss: 0.133
Actual params: [ 0.1915, -1.3994]
-Original Grad: -0.006, -lr * Pred Grad: 0.019, New P: 0.211
-Original Grad: -0.096, -lr * Pred Grad: -0.014, New P: -1.413
iter 11 loss: 0.132
Actual params: [ 0.2107, -1.413 ]
-Original Grad: -0.012, -lr * Pred Grad: 0.019, New P: 0.230
-Original Grad: -0.060, -lr * Pred Grad: -0.015, New P: -1.428
iter 12 loss: 0.131
Actual params: [ 0.2298, -1.4276]
-Original Grad: -0.006, -lr * Pred Grad: 0.019, New P: 0.249
-Original Grad: -0.075, -lr * Pred Grad: -0.015, New P: -1.442
iter 13 loss: 0.129
Actual params: [ 0.249 , -1.4424]
-Original Grad: -0.007, -lr * Pred Grad: 0.019, New P: 0.268
-Original Grad: -0.057, -lr * Pred Grad: -0.015, New P: -1.457
iter 14 loss: 0.128
Actual params: [ 0.2682, -1.4573]
-Original Grad: 0.021, -lr * Pred Grad: 0.019, New P: 0.287
-Original Grad: -0.081, -lr * Pred Grad: -0.015, New P: -1.472
iter 15 loss: 0.126
Actual params: [ 0.2873, -1.4722]
-Original Grad: 0.017, -lr * Pred Grad: 0.019, New P: 0.306
-Original Grad: -0.086, -lr * Pred Grad: -0.015, New P: -1.487
iter 16 loss: 0.125
Actual params: [ 0.3065, -1.4871]
-Original Grad: 0.045, -lr * Pred Grad: 0.019, New P: 0.326
-Original Grad: -0.069, -lr * Pred Grad: -0.015, New P: -1.502
iter 17 loss: 0.124
Actual params: [ 0.3257, -1.502 ]
-Original Grad: 0.017, -lr * Pred Grad: 0.019, New P: 0.345
-Original Grad: -0.055, -lr * Pred Grad: -0.015, New P: -1.517
iter 18 loss: 0.123
Actual params: [ 0.3448, -1.5169]
-Original Grad: 0.036, -lr * Pred Grad: 0.019, New P: 0.364
-Original Grad: -0.048, -lr * Pred Grad: -0.015, New P: -1.532
iter 19 loss: 0.121
Actual params: [ 0.364 , -1.5318]
-Original Grad: 0.019, -lr * Pred Grad: 0.019, New P: 0.383
-Original Grad: -0.039, -lr * Pred Grad: -0.015, New P: -1.547
iter 20 loss: 0.120
Actual params: [ 0.3832, -1.5467]
Target params: [1.1812, 0.2779]
Actual params: [-0.8962,  0.3381]
-Original Grad: -0.159, -lr * Pred Grad: 0.003, New P: -0.893
-Original Grad: 0.409, -lr * Pred Grad: 0.008, New P: 0.347
iter 0 loss: 0.360
Actual params: [-0.893 ,  0.3466]
-Original Grad: -0.142, -lr * Pred Grad: 0.007, New P: -0.886
-Original Grad: 0.379, -lr * Pred Grad: 0.017, New P: 0.363
iter 1 loss: 0.360
Actual params: [-0.8857,  0.3633]
-Original Grad: -0.130, -lr * Pred Grad: 0.005, New P: -0.881
-Original Grad: 0.353, -lr * Pred Grad: 0.019, New P: 0.382
iter 2 loss: 0.359
Actual params: [-0.8805,  0.3821]
-Original Grad: -0.138, -lr * Pred Grad: -0.007, New P: -0.887
-Original Grad: 0.432, -lr * Pred Grad: 0.019, New P: 0.401
iter 3 loss: 0.359
Actual params: [-0.8873,  0.4012]
-Original Grad: -0.124, -lr * Pred Grad: -0.013, New P: -0.900
-Original Grad: 0.338, -lr * Pred Grad: 0.019, New P: 0.420
iter 4 loss: 0.357
Actual params: [-0.9004,  0.4204]
-Original Grad: -0.122, -lr * Pred Grad: -0.014, New P: -0.915
-Original Grad: 0.391, -lr * Pred Grad: 0.019, New P: 0.440
iter 5 loss: 0.358
Actual params: [-0.9148,  0.4396]
-Original Grad: -0.107, -lr * Pred Grad: -0.015, New P: -0.929
-Original Grad: 0.466, -lr * Pred Grad: 0.019, New P: 0.459
iter 6 loss: 0.364
Actual params: [-0.9295,  0.4587]
-Original Grad: -0.122, -lr * Pred Grad: -0.015, New P: -0.944
-Original Grad: 0.313, -lr * Pred Grad: 0.019, New P: 0.478
iter 7 loss: 0.364
Actual params: [-0.9444,  0.4779]
-Original Grad: -0.126, -lr * Pred Grad: -0.015, New P: -0.959
-Original Grad: 0.260, -lr * Pred Grad: 0.019, New P: 0.497
iter 8 loss: 0.362
Actual params: [-0.9592,  0.4971]
-Original Grad: -0.105, -lr * Pred Grad: -0.015, New P: -0.974
-Original Grad: 0.279, -lr * Pred Grad: 0.019, New P: 0.516
iter 9 loss: 0.361
Actual params: [-0.9741,  0.5162]
-Original Grad: -0.136, -lr * Pred Grad: -0.015, New P: -0.989
-Original Grad: 0.254, -lr * Pred Grad: 0.019, New P: 0.535
iter 10 loss: 0.361
Actual params: [-0.989 ,  0.5354]
-Original Grad: -0.123, -lr * Pred Grad: -0.015, New P: -1.004
-Original Grad: 0.237, -lr * Pred Grad: 0.019, New P: 0.555
iter 11 loss: 0.361
Actual params: [-1.0039,  0.5546]
-Original Grad: -0.107, -lr * Pred Grad: -0.015, New P: -1.019
-Original Grad: 0.184, -lr * Pred Grad: 0.019, New P: 0.574
iter 12 loss: 0.361
Actual params: [-1.0189,  0.5737]
-Original Grad: -0.111, -lr * Pred Grad: -0.015, New P: -1.034
-Original Grad: 0.154, -lr * Pred Grad: 0.019, New P: 0.593
iter 13 loss: 0.362
Actual params: [-1.0338,  0.5929]
-Original Grad: -0.095, -lr * Pred Grad: -0.015, New P: -1.049
-Original Grad: 0.169, -lr * Pred Grad: 0.019, New P: 0.612
iter 14 loss: 0.362
Actual params: [-1.0487,  0.6121]
-Original Grad: -0.118, -lr * Pred Grad: -0.015, New P: -1.064
-Original Grad: 0.120, -lr * Pred Grad: 0.019, New P: 0.631
iter 15 loss: 0.363
Actual params: [-1.0636,  0.6312]
-Original Grad: -0.110, -lr * Pred Grad: -0.015, New P: -1.078
-Original Grad: 0.122, -lr * Pred Grad: 0.019, New P: 0.650
iter 16 loss: 0.363
Actual params: [-1.0785,  0.6504]
-Original Grad: -0.107, -lr * Pred Grad: -0.015, New P: -1.093
-Original Grad: 0.114, -lr * Pred Grad: 0.019, New P: 0.670
iter 17 loss: 0.363
Actual params: [-1.0934,  0.6696]
-Original Grad: -0.096, -lr * Pred Grad: -0.015, New P: -1.108
-Original Grad: 0.082, -lr * Pred Grad: 0.019, New P: 0.689
iter 18 loss: 0.363
Actual params: [-1.1083,  0.6887]
-Original Grad: -0.121, -lr * Pred Grad: -0.015, New P: -1.123
-Original Grad: 0.097, -lr * Pred Grad: 0.019, New P: 0.708
iter 19 loss: 0.363
Actual params: [-1.1232,  0.7079]
-Original Grad: -0.089, -lr * Pred Grad: -0.015, New P: -1.138
-Original Grad: 0.065, -lr * Pred Grad: 0.019, New P: 0.727
iter 20 loss: 0.363
Actual params: [-1.1381,  0.7271]
Target params: [1.1812, 0.2779]
Actual params: [ 0.3685, -2.1923]
-Original Grad: -0.026, -lr * Pred Grad: 0.005, New P: 0.373
-Original Grad: 0.544, -lr * Pred Grad: 0.009, New P: -2.183
iter 0 loss: 0.498
Actual params: [ 0.3732, -2.183 ]
-Original Grad: -0.030, -lr * Pred Grad: 0.013, New P: 0.386
-Original Grad: 0.508, -lr * Pred Grad: 0.017, New P: -2.166
iter 1 loss: 0.501
Actual params: [ 0.3861, -2.1659]
-Original Grad: -0.036, -lr * Pred Grad: 0.018, New P: 0.404
-Original Grad: 0.624, -lr * Pred Grad: 0.019, New P: -2.147
iter 2 loss: 0.489
Actual params: [ 0.4036, -2.147 ]
-Original Grad: -0.002, -lr * Pred Grad: 0.019, New P: 0.422
-Original Grad: 0.627, -lr * Pred Grad: 0.019, New P: -2.128
iter 3 loss: 0.475
Actual params: [ 0.4224, -2.1279]
-Original Grad: -0.036, -lr * Pred Grad: 0.019, New P: 0.442
-Original Grad: 0.508, -lr * Pred Grad: 0.019, New P: -2.109
iter 4 loss: 0.465
Actual params: [ 0.4415, -2.1088]
-Original Grad: -0.059, -lr * Pred Grad: 0.019, New P: 0.461
-Original Grad: 0.580, -lr * Pred Grad: 0.019, New P: -2.090
iter 5 loss: 0.445
Actual params: [ 0.4607, -2.0896]
-Original Grad: -0.133, -lr * Pred Grad: 0.019, New P: 0.480
-Original Grad: 0.765, -lr * Pred Grad: 0.019, New P: -2.070
iter 6 loss: 0.446
Actual params: [ 0.4798, -2.0704]
-Original Grad: -0.161, -lr * Pred Grad: 0.019, New P: 0.499
-Original Grad: 0.588, -lr * Pred Grad: 0.019, New P: -2.051
iter 7 loss: 0.431
Actual params: [ 0.499 , -2.0513]
-Original Grad: -0.289, -lr * Pred Grad: 0.019, New P: 0.518
-Original Grad: 0.559, -lr * Pred Grad: 0.019, New P: -2.032
iter 8 loss: 0.420
Actual params: [ 0.5182, -2.0321]
-Original Grad: -0.267, -lr * Pred Grad: 0.019, New P: 0.537
-Original Grad: 0.578, -lr * Pred Grad: 0.019, New P: -2.013
iter 9 loss: 0.415
Actual params: [ 0.5373, -2.0129]
-Original Grad: -0.134, -lr * Pred Grad: 0.019, New P: 0.557
-Original Grad: 0.717, -lr * Pred Grad: 0.019, New P: -1.994
iter 10 loss: 0.413
Actual params: [ 0.5565, -1.9938]
-Original Grad: -0.235, -lr * Pred Grad: 0.019, New P: 0.576
-Original Grad: 0.661, -lr * Pred Grad: 0.019, New P: -1.975
iter 11 loss: 0.411
Actual params: [ 0.5757, -1.9746]
-Original Grad: -0.307, -lr * Pred Grad: 0.019, New P: 0.595
-Original Grad: 0.697, -lr * Pred Grad: 0.019, New P: -1.955
iter 12 loss: 0.408
Actual params: [ 0.5948, -1.9554]
-Original Grad: -0.294, -lr * Pred Grad: 0.019, New P: 0.614
-Original Grad: 0.598, -lr * Pred Grad: 0.019, New P: -1.936
iter 13 loss: 0.407
Actual params: [ 0.614 , -1.9363]
-Original Grad: -0.269, -lr * Pred Grad: 0.019, New P: 0.633
-Original Grad: 0.646, -lr * Pred Grad: 0.019, New P: -1.917
iter 14 loss: 0.406
Actual params: [ 0.6332, -1.9171]
-Original Grad: -0.253, -lr * Pred Grad: 0.019, New P: 0.652
-Original Grad: 0.645, -lr * Pred Grad: 0.019, New P: -1.898
iter 15 loss: 0.399
Actual params: [ 0.6523, -1.8979]
-Original Grad: -0.228, -lr * Pred Grad: 0.019, New P: 0.671
-Original Grad: 0.618, -lr * Pred Grad: 0.019, New P: -1.879
iter 16 loss: 0.401
Actual params: [ 0.6715, -1.8788]
-Original Grad: -0.083, -lr * Pred Grad: 0.019, New P: 0.691
-Original Grad: 0.737, -lr * Pred Grad: 0.019, New P: -1.860
iter 17 loss: 0.405
Actual params: [ 0.6907, -1.8596]
-Original Grad: -0.092, -lr * Pred Grad: 0.019, New P: 0.710
-Original Grad: 0.806, -lr * Pred Grad: 0.019, New P: -1.840
iter 18 loss: 0.411
Actual params: [ 0.7098, -1.8404]
-Original Grad: -0.113, -lr * Pred Grad: 0.019, New P: 0.729
-Original Grad: 0.718, -lr * Pred Grad: 0.019, New P: -1.821
iter 19 loss: 0.420
Actual params: [ 0.729 , -1.8213]
-Original Grad: -0.065, -lr * Pred Grad: 0.019, New P: 0.748
-Original Grad: 0.780, -lr * Pred Grad: 0.019, New P: -1.802
iter 20 loss: 0.431
Actual params: [ 0.7482, -1.8021]
Target params: [1.1812, 0.2779]
Actual params: [-0.1438, -1.3168]
-Original Grad: 0.635, -lr * Pred Grad: 0.010, New P: -0.134
-Original Grad: 1.489, -lr * Pred Grad: 0.012, New P: -1.305
iter 0 loss: 0.310
Actual params: [-0.1341, -1.3048]
-Original Grad: 0.874, -lr * Pred Grad: 0.017, New P: -0.117
-Original Grad: 1.800, -lr * Pred Grad: 0.018, New P: -1.287
iter 1 loss: 0.302
Actual params: [-0.1168, -1.2868]
-Original Grad: 0.766, -lr * Pred Grad: 0.019, New P: -0.098
-Original Grad: 1.819, -lr * Pred Grad: 0.019, New P: -1.268
iter 2 loss: 0.306
Actual params: [-0.0979, -1.2677]
-Original Grad: 0.639, -lr * Pred Grad: 0.019, New P: -0.079
-Original Grad: 1.274, -lr * Pred Grad: 0.019, New P: -1.249
iter 3 loss: 0.311
Actual params: [-0.0787, -1.2486]
-Original Grad: 1.141, -lr * Pred Grad: 0.019, New P: -0.060
-Original Grad: 2.621, -lr * Pred Grad: 0.019, New P: -1.229
iter 4 loss: 0.305
Actual params: [-0.0596, -1.2294]
-Original Grad: 1.105, -lr * Pred Grad: 0.019, New P: -0.040
-Original Grad: 2.215, -lr * Pred Grad: 0.019, New P: -1.210
iter 5 loss: 0.298
Actual params: [-0.0404, -1.2103]
-Original Grad: 1.596, -lr * Pred Grad: 0.019, New P: -0.021
-Original Grad: 2.244, -lr * Pred Grad: 0.019, New P: -1.191
iter 6 loss: 0.291
Actual params: [-0.0212, -1.1911]
-Original Grad: 0.780, -lr * Pred Grad: 0.019, New P: -0.002
-Original Grad: 1.130, -lr * Pred Grad: 0.019, New P: -1.172
iter 7 loss: 0.284
Actual params: [-0.0021, -1.1719]
-Original Grad: 0.924, -lr * Pred Grad: 0.019, New P: 0.017
-Original Grad: 1.067, -lr * Pred Grad: 0.019, New P: -1.153
iter 8 loss: 0.278
Actual params: [ 0.0171, -1.1528]
-Original Grad: 1.215, -lr * Pred Grad: 0.019, New P: 0.036
-Original Grad: 2.039, -lr * Pred Grad: 0.019, New P: -1.134
iter 9 loss: 0.273
Actual params: [ 0.0363, -1.1336]
-Original Grad: 1.258, -lr * Pred Grad: 0.019, New P: 0.055
-Original Grad: 3.297, -lr * Pred Grad: 0.019, New P: -1.114
iter 10 loss: 0.272
Actual params: [ 0.0554, -1.1144]
-Original Grad: 0.786, -lr * Pred Grad: 0.019, New P: 0.075
-Original Grad: 1.543, -lr * Pred Grad: 0.019, New P: -1.095
iter 11 loss: 0.288
Actual params: [ 0.0746, -1.0953]
-Original Grad: 0.599, -lr * Pred Grad: 0.019, New P: 0.094
-Original Grad: 1.004, -lr * Pred Grad: 0.019, New P: -1.076
iter 12 loss: 0.270
Actual params: [ 0.0938, -1.0761]
-Original Grad: 0.543, -lr * Pred Grad: 0.019, New P: 0.113
-Original Grad: 0.187, -lr * Pred Grad: 0.019, New P: -1.057
iter 13 loss: 0.284
Actual params: [ 0.1129, -1.0569]
-Original Grad: 0.397, -lr * Pred Grad: 0.019, New P: 0.132
-Original Grad: 0.943, -lr * Pred Grad: 0.019, New P: -1.038
iter 14 loss: 0.283
Actual params: [ 0.1321, -1.0378]
-Original Grad: 0.465, -lr * Pred Grad: 0.019, New P: 0.151
-Original Grad: 0.477, -lr * Pred Grad: 0.019, New P: -1.019
iter 15 loss: 0.291
Actual params: [ 0.1513, -1.0186]
-Original Grad: 0.364, -lr * Pred Grad: 0.019, New P: 0.170
-Original Grad: 0.516, -lr * Pred Grad: 0.019, New P: -0.999
iter 16 loss: 0.287
Actual params: [ 0.1704, -0.9994]
-Original Grad: 0.407, -lr * Pred Grad: 0.019, New P: 0.190
-Original Grad: 1.160, -lr * Pred Grad: 0.019, New P: -0.980
iter 17 loss: 0.283
Actual params: [ 0.1896, -0.9803]
-Original Grad: 0.372, -lr * Pred Grad: 0.019, New P: 0.209
-Original Grad: 1.881, -lr * Pred Grad: 0.019, New P: -0.961
iter 18 loss: 0.280
Actual params: [ 0.2088, -0.9611]
-Original Grad: 0.212, -lr * Pred Grad: 0.019, New P: 0.228
-Original Grad: 0.080, -lr * Pred Grad: 0.019, New P: -0.942
iter 19 loss: 0.277
Actual params: [ 0.2279, -0.9419]
-Original Grad: 0.423, -lr * Pred Grad: 0.019, New P: 0.247
-Original Grad: 0.848, -lr * Pred Grad: 0.019, New P: -0.923
iter 20 loss: 0.274
Actual params: [ 0.2471, -0.9228]
Target params: [1.1812, 0.2779]
Actual params: [-1.23  , -0.0332]
-Original Grad: -0.006, -lr * Pred Grad: 0.005, New P: -1.225
-Original Grad: 0.025, -lr * Pred Grad: 0.005, New P: -0.028
iter 0 loss: 0.294
Actual params: [-1.225 , -0.0279]
-Original Grad: -0.004, -lr * Pred Grad: 0.013, New P: -1.212
-Original Grad: -0.012, -lr * Pred Grad: 0.014, New P: -0.014
iter 1 loss: 0.294
Actual params: [-1.2117, -0.0141]
-Original Grad: 0.000, -lr * Pred Grad: 0.018, New P: -1.194
-Original Grad: -0.024, -lr * Pred Grad: 0.018, New P: 0.004
iter 2 loss: 0.295
Actual params: [-1.1939,  0.0039]
-Original Grad: 0.001, -lr * Pred Grad: 0.019, New P: -1.175
-Original Grad: -0.014, -lr * Pred Grad: 0.019, New P: 0.023
iter 3 loss: 0.296
Actual params: [-1.175 ,  0.0228]
-Original Grad: 0.008, -lr * Pred Grad: 0.019, New P: -1.156
-Original Grad: -0.010, -lr * Pred Grad: 0.019, New P: 0.042
iter 4 loss: 0.298
Actual params: [-1.1558,  0.042 ]
-Original Grad: 0.011, -lr * Pred Grad: 0.019, New P: -1.137
-Original Grad: -0.022, -lr * Pred Grad: 0.019, New P: 0.061
iter 5 loss: 0.300
Actual params: [-1.1367,  0.0611]
-Original Grad: 0.006, -lr * Pred Grad: 0.019, New P: -1.118
-Original Grad: -0.039, -lr * Pred Grad: 0.019, New P: 0.080
iter 6 loss: 0.302
Actual params: [-1.1175,  0.0803]
-Original Grad: 0.027, -lr * Pred Grad: 0.019, New P: -1.098
-Original Grad: -0.053, -lr * Pred Grad: 0.019, New P: 0.099
iter 7 loss: 0.304
Actual params: [-1.0983,  0.0995]
-Original Grad: 0.021, -lr * Pred Grad: 0.019, New P: -1.079
-Original Grad: 0.007, -lr * Pred Grad: 0.019, New P: 0.119
iter 8 loss: 0.307
Actual params: [-1.0792,  0.1186]
-Original Grad: 0.024, -lr * Pred Grad: 0.019, New P: -1.060
-Original Grad: -0.027, -lr * Pred Grad: 0.019, New P: 0.138
iter 9 loss: 0.309
Actual params: [-1.06  ,  0.1378]
-Original Grad: 0.033, -lr * Pred Grad: 0.019, New P: -1.041
-Original Grad: -0.075, -lr * Pred Grad: 0.019, New P: 0.157
iter 10 loss: 0.312
Actual params: [-1.0408,  0.157 ]
-Original Grad: 0.025, -lr * Pred Grad: 0.019, New P: -1.022
-Original Grad: -0.058, -lr * Pred Grad: 0.019, New P: 0.176
iter 11 loss: 0.314
Actual params: [-1.0217,  0.1761]
-Original Grad: 0.027, -lr * Pred Grad: 0.019, New P: -1.003
-Original Grad: -0.021, -lr * Pred Grad: 0.019, New P: 0.195
iter 12 loss: 0.317
Actual params: [-1.0025,  0.1953]
-Original Grad: 0.020, -lr * Pred Grad: 0.019, New P: -0.983
-Original Grad: -0.123, -lr * Pred Grad: 0.019, New P: 0.214
iter 13 loss: 0.320
Actual params: [-0.9833,  0.2145]
-Original Grad: 0.019, -lr * Pred Grad: 0.019, New P: -0.964
-Original Grad: -0.191, -lr * Pred Grad: 0.019, New P: 0.234
iter 14 loss: 0.323
Actual params: [-0.9642,  0.2336]
-Original Grad: 0.036, -lr * Pred Grad: 0.019, New P: -0.945
-Original Grad: -0.076, -lr * Pred Grad: 0.019, New P: 0.253
iter 15 loss: 0.328
Actual params: [-0.945 ,  0.2528]
-Original Grad: 0.039, -lr * Pred Grad: 0.019, New P: -0.926
-Original Grad: -0.053, -lr * Pred Grad: 0.019, New P: 0.272
iter 16 loss: 0.333
Actual params: [-0.9259,  0.2719]
-Original Grad: 0.046, -lr * Pred Grad: 0.019, New P: -0.907
-Original Grad: -0.136, -lr * Pred Grad: 0.019, New P: 0.291
iter 17 loss: 0.339
Actual params: [-0.9067,  0.2911]
-Original Grad: 0.046, -lr * Pred Grad: 0.019, New P: -0.888
-Original Grad: -0.020, -lr * Pred Grad: 0.019, New P: 0.310
iter 18 loss: 0.345
Actual params: [-0.8875,  0.3103]
-Original Grad: 0.047, -lr * Pred Grad: 0.019, New P: -0.868
-Original Grad: -0.122, -lr * Pred Grad: 0.019, New P: 0.329
iter 19 loss: 0.352
Actual params: [-0.8684,  0.3294]
-Original Grad: 0.048, -lr * Pred Grad: 0.019, New P: -0.849
-Original Grad: -0.193, -lr * Pred Grad: 0.019, New P: 0.349
iter 20 loss: 0.361
Actual params: [-0.8492,  0.3486]
Target params: [1.1812, 0.2779]
Actual params: [ 1.0788, -1.6003]
-Original Grad: -0.399, -lr * Pred Grad: 0.001, New P: 1.079
-Original Grad: 1.591, -lr * Pred Grad: 0.012, New P: -1.588
iter 0 loss: 1.235
Actual params: [ 1.0795, -1.5882]
-Original Grad: -0.479, -lr * Pred Grad: -0.007, New P: 1.073
-Original Grad: 1.958, -lr * Pred Grad: 0.018, New P: -1.570
iter 1 loss: 1.183
Actual params: [ 1.0729, -1.57  ]
-Original Grad: -0.416, -lr * Pred Grad: -0.013, New P: 1.060
-Original Grad: 1.767, -lr * Pred Grad: 0.019, New P: -1.551
iter 2 loss: 1.204
Actual params: [ 1.0602, -1.551 ]
-Original Grad: -0.268, -lr * Pred Grad: -0.014, New P: 1.046
-Original Grad: 1.334, -lr * Pred Grad: 0.019, New P: -1.532
iter 3 loss: 1.247
Actual params: [ 1.0462, -1.5319]
-Original Grad: -0.348, -lr * Pred Grad: -0.015, New P: 1.032
-Original Grad: 1.594, -lr * Pred Grad: 0.019, New P: -1.513
iter 4 loss: 1.251
Actual params: [ 1.0315, -1.5127]
-Original Grad: -0.240, -lr * Pred Grad: -0.015, New P: 1.017
-Original Grad: 1.404, -lr * Pred Grad: 0.019, New P: -1.494
iter 5 loss: 1.235
Actual params: [ 1.0167, -1.4936]
-Original Grad: -0.171, -lr * Pred Grad: -0.015, New P: 1.002
-Original Grad: 1.238, -lr * Pred Grad: 0.019, New P: -1.474
iter 6 loss: 1.200
Actual params: [ 1.0018, -1.4744]
-Original Grad: 0.212, -lr * Pred Grad: -0.015, New P: 0.987
-Original Grad: 1.333, -lr * Pred Grad: 0.019, New P: -1.455
iter 7 loss: 1.170
Actual params: [ 0.9869, -1.4552]
-Original Grad: 0.207, -lr * Pred Grad: -0.015, New P: 0.972
-Original Grad: 1.366, -lr * Pred Grad: 0.019, New P: -1.436
iter 8 loss: 1.139
Actual params: [ 0.972 , -1.4361]
-Original Grad: 0.173, -lr * Pred Grad: -0.015, New P: 0.957
-Original Grad: 1.045, -lr * Pred Grad: 0.019, New P: -1.417
iter 9 loss: 1.105
Actual params: [ 0.9571, -1.4169]
-Original Grad: 0.312, -lr * Pred Grad: -0.015, New P: 0.942
-Original Grad: 1.379, -lr * Pred Grad: 0.019, New P: -1.398
iter 10 loss: 1.066
Actual params: [ 0.9422, -1.3977]
-Original Grad: 0.361, -lr * Pred Grad: -0.015, New P: 0.927
-Original Grad: 1.439, -lr * Pred Grad: 0.019, New P: -1.379
iter 11 loss: 1.042
Actual params: [ 0.9273, -1.3786]
-Original Grad: 0.454, -lr * Pred Grad: -0.015, New P: 0.912
-Original Grad: 1.229, -lr * Pred Grad: 0.019, New P: -1.359
iter 12 loss: 1.006
Actual params: [ 0.9124, -1.3594]
-Original Grad: 0.339, -lr * Pred Grad: -0.015, New P: 0.898
-Original Grad: 1.346, -lr * Pred Grad: 0.019, New P: -1.340
iter 13 loss: 0.980
Actual params: [ 0.8975, -1.3402]
-Original Grad: 0.465, -lr * Pred Grad: -0.015, New P: 0.883
-Original Grad: 1.055, -lr * Pred Grad: 0.019, New P: -1.321
iter 14 loss: 0.945
Actual params: [ 0.8826, -1.3211]
-Original Grad: 0.582, -lr * Pred Grad: -0.015, New P: 0.868
-Original Grad: 0.840, -lr * Pred Grad: 0.019, New P: -1.302
iter 15 loss: 0.885
Actual params: [ 0.8677, -1.3019]
-Original Grad: 0.479, -lr * Pred Grad: -0.015, New P: 0.853
-Original Grad: 1.110, -lr * Pred Grad: 0.019, New P: -1.283
iter 16 loss: 0.378
Actual params: [ 0.8528, -1.2827]
-Original Grad: 0.681, -lr * Pred Grad: -0.015, New P: 0.838
-Original Grad: 0.811, -lr * Pred Grad: 0.019, New P: -1.264
iter 17 loss: 0.347
Actual params: [ 0.8379, -1.2636]
-Original Grad: 0.771, -lr * Pred Grad: -0.015, New P: 0.823
-Original Grad: 0.846, -lr * Pred Grad: 0.019, New P: -1.244
iter 18 loss: 0.329
Actual params: [ 0.823 , -1.2444]
-Original Grad: 0.641, -lr * Pred Grad: -0.015, New P: 0.808
-Original Grad: 0.701, -lr * Pred Grad: 0.019, New P: -1.225
iter 19 loss: 0.318
Actual params: [ 0.8081, -1.2252]
-Original Grad: 0.640, -lr * Pred Grad: -0.015, New P: 0.793
-Original Grad: 0.947, -lr * Pred Grad: 0.019, New P: -1.206
iter 20 loss: 0.312
Actual params: [ 0.7932, -1.2061]
Target params: [1.1812, 0.2779]
Actual params: [-0.7653,  1.3313]
-Original Grad: 0.784, -lr * Pred Grad: 0.010, New P: -0.755
-Original Grad: -0.257, -lr * Pred Grad: 0.002, New P: 1.333
iter 0 loss: 0.562
Actual params: [-0.7551,  1.3335]
-Original Grad: 0.852, -lr * Pred Grad: 0.018, New P: -0.737
-Original Grad: -0.333, -lr * Pred Grad: -0.000, New P: 1.333
iter 1 loss: 0.558
Actual params: [-0.7375,  1.3331]
-Original Grad: 0.933, -lr * Pred Grad: 0.019, New P: -0.719
-Original Grad: -0.468, -lr * Pred Grad: -0.011, New P: 1.322
iter 2 loss: 0.562
Actual params: [-0.7185,  1.3219]
-Original Grad: 0.901, -lr * Pred Grad: 0.019, New P: -0.699
-Original Grad: -0.335, -lr * Pred Grad: -0.014, New P: 1.308
iter 3 loss: 0.551
Actual params: [-0.6994,  1.3084]
-Original Grad: 1.012, -lr * Pred Grad: 0.019, New P: -0.680
-Original Grad: -0.263, -lr * Pred Grad: -0.015, New P: 1.294
iter 4 loss: 0.553
Actual params: [-0.6803,  1.2938]
-Original Grad: 0.880, -lr * Pred Grad: 0.019, New P: -0.661
-Original Grad: -0.348, -lr * Pred Grad: -0.015, New P: 1.279
iter 5 loss: 0.554
Actual params: [-0.6611,  1.2791]
-Original Grad: 0.745, -lr * Pred Grad: 0.019, New P: -0.642
-Original Grad: -0.216, -lr * Pred Grad: -0.015, New P: 1.264
iter 6 loss: 0.556
Actual params: [-0.6419,  1.2642]
-Original Grad: 0.928, -lr * Pred Grad: 0.019, New P: -0.623
-Original Grad: -0.365, -lr * Pred Grad: -0.015, New P: 1.249
iter 7 loss: 0.557
Actual params: [-0.6228,  1.2493]
-Original Grad: 0.944, -lr * Pred Grad: 0.019, New P: -0.604
-Original Grad: -0.286, -lr * Pred Grad: -0.015, New P: 1.234
iter 8 loss: 0.558
Actual params: [-0.6036,  1.2344]
-Original Grad: 0.880, -lr * Pred Grad: 0.019, New P: -0.584
-Original Grad: -0.203, -lr * Pred Grad: -0.015, New P: 1.220
iter 9 loss: 0.560
Actual params: [-0.5844,  1.2195]
-Original Grad: 0.759, -lr * Pred Grad: 0.019, New P: -0.565
-Original Grad: -0.351, -lr * Pred Grad: -0.015, New P: 1.205
iter 10 loss: 0.561
Actual params: [-0.5653,  1.2046]
-Original Grad: 0.934, -lr * Pred Grad: 0.019, New P: -0.546
-Original Grad: -0.230, -lr * Pred Grad: -0.015, New P: 1.190
iter 11 loss: 0.562
Actual params: [-0.5461,  1.1897]
-Original Grad: 0.795, -lr * Pred Grad: 0.019, New P: -0.527
-Original Grad: -0.255, -lr * Pred Grad: -0.015, New P: 1.175
iter 12 loss: 0.563
Actual params: [-0.5269,  1.1748]
-Original Grad: 0.798, -lr * Pred Grad: 0.019, New P: -0.508
-Original Grad: -0.236, -lr * Pred Grad: -0.015, New P: 1.160
iter 13 loss: 0.563
Actual params: [-0.5078,  1.1599]
-Original Grad: 0.807, -lr * Pred Grad: 0.019, New P: -0.489
-Original Grad: -0.343, -lr * Pred Grad: -0.015, New P: 1.145
iter 14 loss: 0.559
Actual params: [-0.4886,  1.145 ]
-Original Grad: 0.861, -lr * Pred Grad: 0.019, New P: -0.469
-Original Grad: -0.363, -lr * Pred Grad: -0.015, New P: 1.130
iter 15 loss: 0.558
Actual params: [-0.4694,  1.1301]
-Original Grad: 0.552, -lr * Pred Grad: 0.019, New P: -0.450
-Original Grad: -0.155, -lr * Pred Grad: -0.015, New P: 1.115
iter 16 loss: 0.558
Actual params: [-0.4503,  1.1152]
-Original Grad: 0.682, -lr * Pred Grad: 0.019, New P: -0.431
-Original Grad: -0.078, -lr * Pred Grad: -0.015, New P: 1.100
iter 17 loss: 0.558
Actual params: [-0.4311,  1.1002]
-Original Grad: 0.903, -lr * Pred Grad: 0.019, New P: -0.412
-Original Grad: -0.150, -lr * Pred Grad: -0.015, New P: 1.085
iter 18 loss: 0.559
Actual params: [-0.4119,  1.0853]
-Original Grad: 0.716, -lr * Pred Grad: 0.019, New P: -0.393
-Original Grad: -0.032, -lr * Pred Grad: -0.015, New P: 1.070
iter 19 loss: 0.561
Actual params: [-0.3928,  1.0704]
-Original Grad: 0.749, -lr * Pred Grad: 0.019, New P: -0.374
-Original Grad: -0.122, -lr * Pred Grad: -0.015, New P: 1.056
iter 20 loss: 0.561
Actual params: [-0.3736,  1.0555]
Target params: [1.1812, 0.2779]
Actual params: [ 0.3094, -0.0048]
-Original Grad: 0.431, -lr * Pred Grad: 0.009, New P: 0.318
-Original Grad: 0.110, -lr * Pred Grad: 0.006, New P: 0.001
iter 0 loss: 0.496
Actual params: [0.3179, 0.0013]
-Original Grad: 0.436, -lr * Pred Grad: 0.017, New P: 0.335
-Original Grad: 0.089, -lr * Pred Grad: 0.015, New P: 0.016
iter 1 loss: 0.495
Actual params: [0.3348, 0.0164]
-Original Grad: 0.585, -lr * Pred Grad: 0.019, New P: 0.354
-Original Grad: -0.027, -lr * Pred Grad: 0.018, New P: 0.035
iter 2 loss: 0.493
Actual params: [0.3536, 0.0348]
-Original Grad: 0.559, -lr * Pred Grad: 0.019, New P: 0.373
-Original Grad: 0.129, -lr * Pred Grad: 0.019, New P: 0.054
iter 3 loss: 0.491
Actual params: [0.3727, 0.0538]
-Original Grad: 0.486, -lr * Pred Grad: 0.019, New P: 0.392
-Original Grad: 0.065, -lr * Pred Grad: 0.019, New P: 0.073
iter 4 loss: 0.489
Actual params: [0.3919, 0.073 ]
-Original Grad: 0.633, -lr * Pred Grad: 0.019, New P: 0.411
-Original Grad: -0.018, -lr * Pred Grad: 0.019, New P: 0.092
iter 5 loss: 0.483
Actual params: [0.411 , 0.0922]
-Original Grad: 0.527, -lr * Pred Grad: 0.019, New P: 0.430
-Original Grad: -0.028, -lr * Pred Grad: 0.019, New P: 0.111
iter 6 loss: 0.480
Actual params: [0.4302, 0.1113]
-Original Grad: 0.562, -lr * Pred Grad: 0.019, New P: 0.449
-Original Grad: -0.058, -lr * Pred Grad: 0.019, New P: 0.130
iter 7 loss: 0.477
Actual params: [0.4494, 0.1305]
-Original Grad: 0.559, -lr * Pred Grad: 0.019, New P: 0.469
-Original Grad: -0.054, -lr * Pred Grad: 0.019, New P: 0.150
iter 8 loss: 0.475
Actual params: [0.4685, 0.1497]
-Original Grad: 0.761, -lr * Pred Grad: 0.019, New P: 0.488
-Original Grad: -0.199, -lr * Pred Grad: 0.019, New P: 0.169
iter 9 loss: 0.472
Actual params: [0.4877, 0.1688]
-Original Grad: 0.662, -lr * Pred Grad: 0.019, New P: 0.507
-Original Grad: -0.063, -lr * Pred Grad: 0.019, New P: 0.188
iter 10 loss: 0.470
Actual params: [0.5069, 0.188 ]
-Original Grad: 0.617, -lr * Pred Grad: 0.019, New P: 0.526
-Original Grad: -0.243, -lr * Pred Grad: 0.019, New P: 0.207
iter 11 loss: 0.468
Actual params: [0.526 , 0.2071]
-Original Grad: 0.611, -lr * Pred Grad: 0.019, New P: 0.545
-Original Grad: -0.205, -lr * Pred Grad: 0.019, New P: 0.226
iter 12 loss: 0.466
Actual params: [0.5452, 0.2263]
-Original Grad: 0.450, -lr * Pred Grad: 0.019, New P: 0.564
-Original Grad: -0.151, -lr * Pred Grad: 0.019, New P: 0.245
iter 13 loss: 0.464
Actual params: [0.5644, 0.2455]
-Original Grad: 0.663, -lr * Pred Grad: 0.019, New P: 0.584
-Original Grad: -0.110, -lr * Pred Grad: 0.019, New P: 0.265
iter 14 loss: 0.462
Actual params: [0.5835, 0.2646]
-Original Grad: 0.467, -lr * Pred Grad: 0.019, New P: 0.603
-Original Grad: -0.170, -lr * Pred Grad: 0.019, New P: 0.284
iter 15 loss: 0.460
Actual params: [0.6027, 0.2838]
-Original Grad: 0.423, -lr * Pred Grad: 0.019, New P: 0.622
-Original Grad: -0.214, -lr * Pred Grad: 0.019, New P: 0.303
iter 16 loss: 0.457
Actual params: [0.6219, 0.303 ]
-Original Grad: 0.476, -lr * Pred Grad: 0.019, New P: 0.641
-Original Grad: -0.145, -lr * Pred Grad: 0.019, New P: 0.322
iter 17 loss: 0.454
Actual params: [0.641 , 0.3221]
-Original Grad: 0.444, -lr * Pred Grad: 0.019, New P: 0.660
-Original Grad: -0.227, -lr * Pred Grad: 0.019, New P: 0.341
iter 18 loss: 0.451
Actual params: [0.6602, 0.3413]
-Original Grad: 0.506, -lr * Pred Grad: 0.019, New P: 0.679
-Original Grad: -0.164, -lr * Pred Grad: 0.019, New P: 0.360
iter 19 loss: 0.446
Actual params: [0.6794, 0.3605]
-Original Grad: 0.578, -lr * Pred Grad: 0.019, New P: 0.699
-Original Grad: -0.267, -lr * Pred Grad: 0.019, New P: 0.380
iter 20 loss: 0.441
Actual params: [0.6985, 0.3796]
Target params: [1.1812, 0.2779]
Actual params: [1.7812, 1.0158]
-Original Grad: -0.101, -lr * Pred Grad: 0.004, New P: 1.785
-Original Grad: -0.283, -lr * Pred Grad: 0.002, New P: 1.018
iter 0 loss: 0.479
Actual params: [1.7852, 1.0177]
-Original Grad: -0.089, -lr * Pred Grad: 0.010, New P: 1.795
-Original Grad: -0.265, -lr * Pred Grad: -0.001, New P: 1.017
iter 1 loss: 0.479
Actual params: [1.7955, 1.0171]
-Original Grad: -0.073, -lr * Pred Grad: 0.014, New P: 1.810
-Original Grad: -0.165, -lr * Pred Grad: -0.011, New P: 1.006
iter 2 loss: 0.483
Actual params: [1.8099, 1.0061]
-Original Grad: -0.020, -lr * Pred Grad: 0.014, New P: 1.824
-Original Grad: -0.119, -lr * Pred Grad: -0.014, New P: 0.992
iter 3 loss: 0.484
Actual params: [1.8238, 0.9924]
-Original Grad: -0.070, -lr * Pred Grad: 0.007, New P: 1.830
-Original Grad: -0.276, -lr * Pred Grad: -0.015, New P: 0.978
iter 4 loss: 0.484
Actual params: [1.8304, 0.9779]
-Original Grad: -0.042, -lr * Pred Grad: -0.004, New P: 1.827
-Original Grad: -0.170, -lr * Pred Grad: -0.015, New P: 0.963
iter 5 loss: 0.484
Actual params: [1.8267, 0.9632]
-Original Grad: -0.093, -lr * Pred Grad: -0.011, New P: 1.816
-Original Grad: -0.479, -lr * Pred Grad: -0.015, New P: 0.948
iter 6 loss: 0.485
Actual params: [1.8159, 0.9483]
-Original Grad: -0.039, -lr * Pred Grad: -0.014, New P: 1.802
-Original Grad: -0.372, -lr * Pred Grad: -0.015, New P: 0.933
iter 7 loss: 0.485
Actual params: [1.8022, 0.9334]
-Original Grad: -0.120, -lr * Pred Grad: -0.015, New P: 1.788
-Original Grad: -0.836, -lr * Pred Grad: -0.015, New P: 0.919
iter 8 loss: 0.482
Actual params: [1.7876, 0.9185]
-Original Grad: -0.134, -lr * Pred Grad: -0.015, New P: 1.773
-Original Grad: -0.949, -lr * Pred Grad: -0.015, New P: 0.904
iter 9 loss: 0.486
Actual params: [1.7728, 0.9036]
-Original Grad: -0.078, -lr * Pred Grad: -0.015, New P: 1.758
-Original Grad: -0.645, -lr * Pred Grad: -0.015, New P: 0.889
iter 10 loss: 0.489
Actual params: [1.7579, 0.8887]
-Original Grad: -0.126, -lr * Pred Grad: -0.015, New P: 1.743
-Original Grad: -1.238, -lr * Pred Grad: -0.015, New P: 0.874
iter 11 loss: 0.489
Actual params: [1.743 , 0.8738]
-Original Grad: -0.149, -lr * Pred Grad: -0.015, New P: 1.728
-Original Grad: -1.382, -lr * Pred Grad: -0.015, New P: 0.859
iter 12 loss: 0.491
Actual params: [1.7281, 0.8589]
-Original Grad: -0.125, -lr * Pred Grad: -0.015, New P: 1.713
-Original Grad: -1.101, -lr * Pred Grad: -0.015, New P: 0.844
iter 13 loss: 0.495
Actual params: [1.7132, 0.844 ]
-Original Grad: -0.089, -lr * Pred Grad: -0.015, New P: 1.698
-Original Grad: -1.003, -lr * Pred Grad: -0.015, New P: 0.829
iter 14 loss: 0.498
Actual params: [1.6983, 0.8291]
-Original Grad: -0.057, -lr * Pred Grad: -0.015, New P: 1.683
-Original Grad: -0.772, -lr * Pred Grad: -0.015, New P: 0.814
iter 15 loss: 0.495
Actual params: [1.6834, 0.8141]
-Original Grad: -0.090, -lr * Pred Grad: -0.015, New P: 1.669
-Original Grad: -1.010, -lr * Pred Grad: -0.015, New P: 0.799
iter 16 loss: 0.495
Actual params: [1.6685, 0.7992]
-Original Grad: -0.078, -lr * Pred Grad: -0.015, New P: 1.654
-Original Grad: -0.852, -lr * Pred Grad: -0.015, New P: 0.784
iter 17 loss: 0.497
Actual params: [1.6536, 0.7843]
-Original Grad: -0.011, -lr * Pred Grad: -0.015, New P: 1.639
-Original Grad: -0.302, -lr * Pred Grad: -0.015, New P: 0.769
iter 18 loss: 0.498
Actual params: [1.6387, 0.7694]
-Original Grad: -0.043, -lr * Pred Grad: -0.015, New P: 1.624
-Original Grad: -0.556, -lr * Pred Grad: -0.015, New P: 0.755
iter 19 loss: 0.498
Actual params: [1.6238, 0.7545]
-Original Grad: -0.003, -lr * Pred Grad: -0.015, New P: 1.609
-Original Grad: -0.251, -lr * Pred Grad: -0.015, New P: 0.740
iter 20 loss: 0.497
Actual params: [1.6089, 0.7396]
