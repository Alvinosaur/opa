Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.496, -lr * Pred Grad:  0.391, New P: 0.985
-Original Grad: 0.279, -lr * Pred Grad:  0.313, New P: 0.907
iter 0 loss: 0.739
Actual params: [0.9854, 0.9069]
-Original Grad: -0.509, -lr * Pred Grad:  -0.102, New P: 0.884
-Original Grad: 0.205, -lr * Pred Grad:  0.157, New P: 1.064
iter 1 loss: 0.717
Actual params: [0.8838, 1.0642]
-Original Grad: -4.640, -lr * Pred Grad:  -0.152, New P: 0.731
-Original Grad: -2.870, -lr * Pred Grad:  -0.326, New P: 0.738
iter 2 loss: 1.589
Actual params: [0.7313, 0.7384]
-Original Grad: 0.614, -lr * Pred Grad:  0.011, New P: 0.742
-Original Grad: 0.354, -lr * Pred Grad:  0.035, New P: 0.774
iter 3 loss: 0.664
Actual params: [0.742 , 0.7739]
-Original Grad: 0.697, -lr * Pred Grad:  0.010, New P: 0.752
-Original Grad: 0.557, -lr * Pred Grad:  0.038, New P: 0.812
iter 4 loss: 0.646
Actual params: [0.7516, 0.8118]
-Original Grad: 0.369, -lr * Pred Grad:  0.001, New P: 0.753
-Original Grad: 1.298, -lr * Pred Grad:  0.046, New P: 0.858
iter 5 loss: 0.619
Actual params: [0.753 , 0.8578]
-Original Grad: 0.783, -lr * Pred Grad:  0.009, New P: 0.762
-Original Grad: 2.260, -lr * Pred Grad:  0.008, New P: 0.866
iter 6 loss: 0.561
Actual params: [0.7623, 0.8661]
-Original Grad: 0.381, -lr * Pred Grad:  0.013, New P: 0.775
-Original Grad: -1.954, -lr * Pred Grad:  -0.007, New P: 0.859
iter 7 loss: 0.537
Actual params: [0.7749, 0.8588]
-Original Grad: -0.410, -lr * Pred Grad:  -0.003, New P: 0.772
-Original Grad: 4.950, -lr * Pred Grad:  0.007, New P: 0.865
iter 8 loss: 0.540
Actual params: [0.772 , 0.8653]
-Original Grad: 0.563, -lr * Pred Grad:  0.005, New P: 0.777
-Original Grad: -4.596, -lr * Pred Grad:  -0.004, New P: 0.861
iter 9 loss: 0.523
Actual params: [0.777 , 0.8614]
-Original Grad: -0.447, -lr * Pred Grad:  0.002, New P: 0.779
-Original Grad: 4.528, -lr * Pred Grad:  0.002, New P: 0.864
iter 10 loss: 0.531
Actual params: [0.7785, 0.8635]
-Original Grad: 0.728, -lr * Pred Grad:  0.004, New P: 0.783
-Original Grad: -1.648, -lr * Pred Grad:  0.000, New P: 0.864
iter 11 loss: 0.518
Actual params: [0.783 , 0.8636]
-Original Grad: -0.347, -lr * Pred Grad:  0.002, New P: 0.785
-Original Grad: 3.274, -lr * Pred Grad:  0.001, New P: 0.865
iter 12 loss: 0.520
Actual params: [0.7851, 0.8645]
-Original Grad: 1.479, -lr * Pred Grad:  0.003, New P: 0.788
-Original Grad: -4.860, -lr * Pred Grad:  0.000, New P: 0.865
iter 13 loss: 0.512
Actual params: [0.7878, 0.8646]
-Original Grad: -2.097, -lr * Pred Grad:  -0.000, New P: 0.787
-Original Grad: 7.444, -lr * Pred Grad:  0.000, New P: 0.865
iter 14 loss: 0.521
Actual params: [0.7874, 0.8651]
-Original Grad: 2.655, -lr * Pred Grad:  0.002, New P: 0.789
-Original Grad: -7.949, -lr * Pred Grad:  0.000, New P: 0.865
iter 15 loss: 0.511
Actual params: [0.7892, 0.8651]
-Original Grad: -1.784, -lr * Pred Grad:  0.001, New P: 0.790
-Original Grad: 6.552, -lr * Pred Grad:  0.001, New P: 0.866
iter 16 loss: 0.520
Actual params: [0.79  , 0.8656]
-Original Grad: -1.783, -lr * Pred Grad:  0.001, New P: 0.791
-Original Grad: 6.966, -lr * Pred Grad:  0.001, New P: 0.866
iter 17 loss: 0.514
Actual params: [0.7914, 0.8662]
-Original Grad: 8.449, -lr * Pred Grad:  -0.000, New P: 0.791
-Original Grad: -28.680, -lr * Pred Grad:  -0.000, New P: 0.866
iter 18 loss: 0.507
Actual params: [0.7912, 0.866 ]
-Original Grad: -3.803, -lr * Pred Grad:  -0.000, New P: 0.791
-Original Grad: 11.222, -lr * Pred Grad:  -0.000, New P: 0.866
iter 19 loss: 0.518
Actual params: [0.7908, 0.8659]
-Original Grad: -2.028, -lr * Pred Grad:  0.000, New P: 0.791
-Original Grad: 7.215, -lr * Pred Grad:  0.000, New P: 0.866
iter 20 loss: 0.514
Actual params: [0.7912, 0.8661]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.053, -lr * Pred Grad:  0.081, New P: 0.675
-Original Grad: -0.040, -lr * Pred Grad:  -0.040, New P: 0.554
iter 0 loss: 0.312
Actual params: [0.675 , 0.5544]
-Original Grad: -0.093, -lr * Pred Grad:  0.010, New P: 0.685
-Original Grad: 0.074, -lr * Pred Grad:  0.031, New P: 0.586
iter 1 loss: 0.260
Actual params: [0.6847, 0.5857]
-Original Grad: 0.085, -lr * Pred Grad:  0.006, New P: 0.691
-Original Grad: -0.065, -lr * Pred Grad:  -0.007, New P: 0.579
iter 2 loss: 0.266
Actual params: [0.6907, 0.5785]
-Original Grad: 0.084, -lr * Pred Grad:  0.014, New P: 0.704
-Original Grad: -0.063, -lr * Pred Grad:  0.004, New P: 0.583
iter 3 loss: 0.261
Actual params: [0.7044, 0.5828]
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.722
-Original Grad: 0.004, -lr * Pred Grad:  0.023, New P: 0.606
iter 4 loss: 0.261
Actual params: [0.7221, 0.6055]
-Original Grad: -0.010, -lr * Pred Grad:  0.021, New P: 0.743
-Original Grad: 0.013, -lr * Pred Grad:  0.028, New P: 0.633
iter 5 loss: 0.262
Actual params: [0.7428, 0.6334]
-Original Grad: 0.032, -lr * Pred Grad:  0.044, New P: 0.786
-Original Grad: -0.011, -lr * Pred Grad:  0.055, New P: 0.688
iter 6 loss: 0.265
Actual params: [0.7864, 0.6879]
-Original Grad: 0.097, -lr * Pred Grad:  0.149, New P: 0.935
-Original Grad: 0.007, -lr * Pred Grad:  0.193, New P: 0.881
iter 7 loss: 0.269
Actual params: [0.9353, 0.8812]
-Original Grad: 0.097, -lr * Pred Grad:  0.042, New P: 0.978
-Original Grad: 0.201, -lr * Pred Grad:  0.051, New P: 0.933
iter 8 loss: 0.214
Actual params: [0.9775, 0.9326]
-Original Grad: -2.200, -lr * Pred Grad:  -0.099, New P: 0.878
-Original Grad: -8.909, -lr * Pred Grad:  -0.100, New P: 0.832
iter 9 loss: 1.978
Actual params: [0.8783, 0.8321]
-Original Grad: 0.038, -lr * Pred Grad:  0.002, New P: 0.880
-Original Grad: 0.134, -lr * Pred Grad:  0.002, New P: 0.834
iter 10 loss: 0.243
Actual params: [0.8801, 0.8336]
-Original Grad: 0.032, -lr * Pred Grad:  0.001, New P: 0.881
-Original Grad: 0.143, -lr * Pred Grad:  0.002, New P: 0.835
iter 11 loss: 0.243
Actual params: [0.8815, 0.8353]
-Original Grad: 0.031, -lr * Pred Grad:  0.001, New P: 0.883
-Original Grad: 0.146, -lr * Pred Grad:  0.002, New P: 0.837
iter 12 loss: 0.242
Actual params: [0.8828, 0.8369]
-Original Grad: 0.030, -lr * Pred Grad:  0.001, New P: 0.884
-Original Grad: 0.154, -lr * Pred Grad:  0.002, New P: 0.839
iter 13 loss: 0.241
Actual params: [0.884 , 0.8387]
-Original Grad: 0.033, -lr * Pred Grad:  0.001, New P: 0.885
-Original Grad: 0.146, -lr * Pred Grad:  0.002, New P: 0.840
iter 14 loss: 0.240
Actual params: [0.8854, 0.8404]
-Original Grad: 0.032, -lr * Pred Grad:  0.001, New P: 0.887
-Original Grad: 0.148, -lr * Pred Grad:  0.002, New P: 0.842
iter 15 loss: 0.239
Actual params: [0.8867, 0.842 ]
-Original Grad: 0.032, -lr * Pred Grad:  0.001, New P: 0.888
-Original Grad: 0.151, -lr * Pred Grad:  0.002, New P: 0.844
iter 16 loss: 0.238
Actual params: [0.888 , 0.8438]
-Original Grad: 0.032, -lr * Pred Grad:  0.001, New P: 0.889
-Original Grad: 0.155, -lr * Pred Grad:  0.002, New P: 0.846
iter 17 loss: 0.237
Actual params: [0.8893, 0.8455]
-Original Grad: 0.027, -lr * Pred Grad:  0.001, New P: 0.890
-Original Grad: 0.158, -lr * Pred Grad:  0.002, New P: 0.847
iter 18 loss: 0.236
Actual params: [0.8903, 0.8474]
-Original Grad: 0.028, -lr * Pred Grad:  0.001, New P: 0.891
-Original Grad: 0.162, -lr * Pred Grad:  0.002, New P: 0.849
iter 19 loss: 0.235
Actual params: [0.8913, 0.8492]
-Original Grad: 0.027, -lr * Pred Grad:  0.001, New P: 0.892
-Original Grad: 0.166, -lr * Pred Grad:  0.002, New P: 0.851
iter 20 loss: 0.235
Actual params: [0.8922, 0.8511]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: -0.498, -lr * Pred Grad:  -0.253, New P: 0.341
-Original Grad: -1.867, -lr * Pred Grad:  -0.001, New P: 0.593
iter 0 loss: 0.445
Actual params: [0.3409, 0.5927]
-Original Grad: 2.155, -lr * Pred Grad:  0.036, New P: 0.377
-Original Grad: -0.827, -lr * Pred Grad:  -0.004, New P: 0.589
iter 1 loss: 0.410
Actual params: [0.3773, 0.5888]
-Original Grad: -0.464, -lr * Pred Grad:  -0.009, New P: 0.368
-Original Grad: -0.502, -lr * Pred Grad:  -0.010, New P: 0.579
iter 2 loss: 0.235
Actual params: [0.3683, 0.5789]
-Original Grad: -0.321, -lr * Pred Grad:  -0.004, New P: 0.364
-Original Grad: -0.070, -lr * Pred Grad:  -0.000, New P: 0.578
iter 3 loss: 0.235
Actual params: [0.3642, 0.5785]
-Original Grad: -0.146, -lr * Pred Grad:  -0.002, New P: 0.362
-Original Grad: 0.088, -lr * Pred Grad:  0.001, New P: 0.580
iter 4 loss: 0.235
Actual params: [0.3623, 0.5797]
-Original Grad: -0.125, -lr * Pred Grad:  -0.002, New P: 0.361
-Original Grad: 0.144, -lr * Pred Grad:  0.002, New P: 0.581
iter 5 loss: 0.233
Actual params: [0.3606, 0.5814]
-Original Grad: 0.168, -lr * Pred Grad:  -0.000, New P: 0.360
-Original Grad: 0.202, -lr * Pred Grad:  0.001, New P: 0.582
iter 6 loss: 0.233
Actual params: [0.3604, 0.5823]
-Original Grad: 0.301, -lr * Pred Grad:  -0.000, New P: 0.360
-Original Grad: 0.289, -lr * Pred Grad:  0.001, New P: 0.583
iter 7 loss: 0.231
Actual params: [0.3602, 0.5834]
-Original Grad: 0.412, -lr * Pred Grad:  -0.000, New P: 0.360
-Original Grad: 0.364, -lr * Pred Grad:  0.001, New P: 0.585
iter 8 loss: 0.230
Actual params: [0.36  , 0.5846]
-Original Grad: 0.052, -lr * Pred Grad:  -0.001, New P: 0.359
-Original Grad: 0.215, -lr * Pred Grad:  0.002, New P: 0.586
iter 9 loss: 0.227
Actual params: [0.3592, 0.5861]
-Original Grad: 0.120, -lr * Pred Grad:  -0.001, New P: 0.359
-Original Grad: 0.224, -lr * Pred Grad:  0.001, New P: 0.587
iter 10 loss: 0.226
Actual params: [0.3586, 0.5874]
-Original Grad: 0.287, -lr * Pred Grad:  -0.000, New P: 0.358
-Original Grad: 0.260, -lr * Pred Grad:  0.001, New P: 0.588
iter 11 loss: 0.225
Actual params: [0.3583, 0.5883]
-Original Grad: 0.289, -lr * Pred Grad:  -0.000, New P: 0.358
-Original Grad: 0.268, -lr * Pred Grad:  0.001, New P: 0.589
iter 12 loss: 0.224
Actual params: [0.3579, 0.5893]
-Original Grad: 0.300, -lr * Pred Grad:  -0.000, New P: 0.358
-Original Grad: 0.281, -lr * Pred Grad:  0.001, New P: 0.590
iter 13 loss: 0.224
Actual params: [0.3575, 0.5903]
-Original Grad: 0.358, -lr * Pred Grad:  -0.000, New P: 0.357
-Original Grad: 0.306, -lr * Pred Grad:  0.001, New P: 0.591
iter 14 loss: 0.223
Actual params: [0.3571, 0.5914]
-Original Grad: 0.437, -lr * Pred Grad:  -0.000, New P: 0.357
-Original Grad: 0.341, -lr * Pred Grad:  0.001, New P: 0.592
iter 15 loss: 0.222
Actual params: [0.3567, 0.5924]
-Original Grad: 0.602, -lr * Pred Grad:  -0.000, New P: 0.356
-Original Grad: 0.383, -lr * Pred Grad:  0.001, New P: 0.593
iter 16 loss: 0.221
Actual params: [0.3565, 0.5932]
-Original Grad: 1.179, -lr * Pred Grad:  0.000, New P: 0.357
-Original Grad: 0.462, -lr * Pred Grad:  -0.000, New P: 0.593
iter 17 loss: 0.221
Actual params: [0.3567, 0.5931]
-Original Grad: 0.486, -lr * Pred Grad:  -0.000, New P: 0.356
-Original Grad: 0.353, -lr * Pred Grad:  0.001, New P: 0.594
iter 18 loss: 0.219
Actual params: [0.3564, 0.594 ]
-Original Grad: 1.044, -lr * Pred Grad:  0.000, New P: 0.356
-Original Grad: 0.433, -lr * Pred Grad:  0.000, New P: 0.594
iter 19 loss: 0.219
Actual params: [0.3564, 0.5942]
-Original Grad: 0.151, -lr * Pred Grad:  -0.000, New P: 0.356
-Original Grad: 0.142, -lr * Pred Grad:  0.000, New P: 0.595
iter 20 loss: 0.217
Actual params: [0.3562, 0.5946]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.541, -lr * Pred Grad:  1.153, New P: 1.747
-Original Grad: -0.053, -lr * Pred Grad:  0.058, New P: 0.653
iter 0 loss: 1.269
Actual params: [1.7471, 0.6525]
-Original Grad: -0.639, -lr * Pred Grad:  -0.340, New P: 1.407
-Original Grad: 0.050, -lr * Pred Grad:  0.193, New P: 0.846
iter 1 loss: 0.978
Actual params: [1.4068, 0.8456]
-Original Grad: -1.142, -lr * Pred Grad:  -0.158, New P: 1.249
-Original Grad: 0.206, -lr * Pred Grad:  0.428, New P: 1.273
iter 2 loss: 0.755
Actual params: [1.249 , 1.2734]
-Original Grad: -1.333, -lr * Pred Grad:  -0.063, New P: 1.186
-Original Grad: 0.411, -lr * Pred Grad:  0.127, New P: 1.401
iter 3 loss: 0.478
Actual params: [1.1864, 1.4006]
-Original Grad: 181.074, -lr * Pred Grad:  0.002, New P: 1.188
-Original Grad: -222.907, -lr * Pred Grad:  -0.000, New P: 1.401
iter 4 loss: 0.798
Actual params: [1.1881, 1.4006]
-Original Grad: -22.762, -lr * Pred Grad:  0.015, New P: 1.203
-Original Grad: 46.405, -lr * Pred Grad:  0.012, New P: 1.413
iter 5 loss: 0.795
Actual params: [1.2035, 1.413 ]
-Original Grad: 239.914, -lr * Pred Grad:  0.002, New P: 1.205
-Original Grad: -64.796, -lr * Pred Grad:  0.001, New P: 1.414
iter 6 loss: 0.792
Actual params: [1.2051, 1.4143]
-Original Grad: 50.175, -lr * Pred Grad:  0.000, New P: 1.205
-Original Grad: -13.234, -lr * Pred Grad:  -0.000, New P: 1.414
iter 7 loss: 0.791
Actual params: [1.2051, 1.4143]
-Original Grad: -366.043, -lr * Pred Grad:  -0.000, New P: 1.205
-Original Grad: 90.872, -lr * Pred Grad:  -0.000, New P: 1.414
iter 8 loss: 0.791
Actual params: [1.2051, 1.4143]
-Original Grad: 60.408, -lr * Pred Grad:  0.000, New P: 1.205
-Original Grad: -16.035, -lr * Pred Grad:  -0.000, New P: 1.414
iter 9 loss: 0.790
Actual params: [1.2051, 1.4143]
-Original Grad: 74.906, -lr * Pred Grad:  0.000, New P: 1.205
-Original Grad: -19.745, -lr * Pred Grad:  -0.000, New P: 1.414
iter 10 loss: 0.790
Actual params: [1.2051, 1.4142]
-Original Grad: -112.672, -lr * Pred Grad:  -0.000, New P: 1.205
-Original Grad: 27.606, -lr * Pred Grad:  -0.000, New P: 1.414
iter 11 loss: 0.790
Actual params: [1.2051, 1.4142]
-Original Grad: 35.914, -lr * Pred Grad:  -0.000, New P: 1.205
-Original Grad: -9.813, -lr * Pred Grad:  -0.000, New P: 1.414
iter 12 loss: 0.790
Actual params: [1.2051, 1.4142]
-Original Grad: -80.080, -lr * Pred Grad:  -0.000, New P: 1.205
-Original Grad: 19.371, -lr * Pred Grad:  -0.000, New P: 1.414
iter 13 loss: 0.790
Actual params: [1.205 , 1.4142]
-Original Grad: -80.108, -lr * Pred Grad:  -0.000, New P: 1.205
-Original Grad: 19.560, -lr * Pred Grad:  -0.000, New P: 1.414
iter 14 loss: 0.790
Actual params: [1.205 , 1.4142]
-Original Grad: -206.340, -lr * Pred Grad:  -0.000, New P: 1.205
-Original Grad: 51.482, -lr * Pred Grad:  -0.000, New P: 1.414
iter 15 loss: 0.790
Actual params: [1.205 , 1.4142]
-Original Grad: 596.104, -lr * Pred Grad:  0.000, New P: 1.205
-Original Grad: -151.440, -lr * Pred Grad:  0.000, New P: 1.414
iter 16 loss: 0.790
Actual params: [1.2051, 1.4142]
-Original Grad: -173.387, -lr * Pred Grad:  -0.000, New P: 1.205
-Original Grad: 42.864, -lr * Pred Grad:  -0.000, New P: 1.414
iter 17 loss: 0.790
Actual params: [1.205 , 1.4142]
-Original Grad: -80.894, -lr * Pred Grad:  -0.000, New P: 1.205
-Original Grad: 19.574, -lr * Pred Grad:  -0.000, New P: 1.414
iter 18 loss: 0.790
Actual params: [1.205 , 1.4142]
-Original Grad: 78.203, -lr * Pred Grad:  0.000, New P: 1.205
-Original Grad: -20.591, -lr * Pred Grad:  -0.000, New P: 1.414
iter 19 loss: 0.790
Actual params: [1.205 , 1.4142]
-Original Grad: 76.951, -lr * Pred Grad:  0.000, New P: 1.205
-Original Grad: -20.288, -lr * Pred Grad:  -0.000, New P: 1.414
iter 20 loss: 0.790
Actual params: [1.205 , 1.4142]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.297, -lr * Pred Grad:  0.751, New P: 1.345
-Original Grad: -0.151, -lr * Pred Grad:  -0.364, New P: 0.230
iter 0 loss: 0.863
Actual params: [1.3449, 0.2304]
-Original Grad: -0.441, -lr * Pred Grad:  -0.378, New P: 0.967
-Original Grad: -0.202, -lr * Pred Grad:  -0.477, New P: -0.246
iter 1 loss: 0.572
Actual params: [ 0.9668, -0.2462]
-Original Grad: 0.617, -lr * Pred Grad:  0.217, New P: 1.184
-Original Grad: -0.024, -lr * Pred Grad:  -0.116, New P: -0.363
iter 2 loss: 0.490
Actual params: [ 1.1838, -0.3626]
-Original Grad: -0.376, -lr * Pred Grad:  -0.110, New P: 1.074
-Original Grad: 0.030, -lr * Pred Grad:  0.104, New P: -0.259
iter 3 loss: 0.492
Actual params: [ 1.0738, -0.2587]
-Original Grad: -0.175, -lr * Pred Grad:  -0.049, New P: 1.025
-Original Grad: -0.001, -lr * Pred Grad:  0.004, New P: -0.255
iter 4 loss: 0.461
Actual params: [ 1.0247, -0.2551]
-Original Grad: 0.099, -lr * Pred Grad:  0.028, New P: 1.053
-Original Grad: -0.013, -lr * Pred Grad:  -0.042, New P: -0.297
iter 5 loss: 0.462
Actual params: [ 1.0528, -0.2973]
-Original Grad: -0.058, -lr * Pred Grad:  -0.017, New P: 1.036
-Original Grad: 0.006, -lr * Pred Grad:  0.020, New P: -0.278
iter 6 loss: 0.462
Actual params: [ 1.0362, -0.2778]
-Original Grad: 0.033, -lr * Pred Grad:  0.009, New P: 1.046
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: -0.278
iter 7 loss: 0.462
Actual params: [ 1.0456, -0.2776]
-Original Grad: -0.009, -lr * Pred Grad:  -0.003, New P: 1.043
-Original Grad: 0.002, -lr * Pred Grad:  0.007, New P: -0.271
iter 8 loss: 0.461
Actual params: [ 1.0429, -0.2708]
-Original Grad: -0.015, -lr * Pred Grad:  -0.004, New P: 1.039
-Original Grad: -0.005, -lr * Pred Grad:  -0.012, New P: -0.283
iter 9 loss: 0.461
Actual params: [ 1.0387, -0.2832]
-Original Grad: 0.011, -lr * Pred Grad:  0.003, New P: 1.042
-Original Grad: 0.001, -lr * Pred Grad:  0.003, New P: -0.281
iter 10 loss: 0.462
Actual params: [ 1.0418, -0.2807]
-Original Grad: -0.020, -lr * Pred Grad:  -0.006, New P: 1.036
-Original Grad: 0.001, -lr * Pred Grad:  0.005, New P: -0.276
iter 11 loss: 0.462
Actual params: [ 1.0358, -0.276 ]
-Original Grad: 0.034, -lr * Pred Grad:  0.010, New P: 1.046
-Original Grad: 0.001, -lr * Pred Grad:  -0.001, New P: -0.277
iter 12 loss: 0.462
Actual params: [ 1.0457, -0.2767]
-Original Grad: -0.017, -lr * Pred Grad:  -0.005, New P: 1.041
-Original Grad: 0.002, -lr * Pred Grad:  0.007, New P: -0.269
iter 13 loss: 0.461
Actual params: [ 1.0406, -0.2693]
-Original Grad: -0.003, -lr * Pred Grad:  -0.001, New P: 1.040
-Original Grad: -0.003, -lr * Pred Grad:  -0.006, New P: -0.276
iter 14 loss: 0.461
Actual params: [ 1.0398, -0.2757]
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: 1.039
-Original Grad: 0.003, -lr * Pred Grad:  0.008, New P: -0.268
iter 15 loss: 0.462
Actual params: [ 1.0389, -0.2677]
-Original Grad: 0.013, -lr * Pred Grad:  0.004, New P: 1.043
-Original Grad: -0.004, -lr * Pred Grad:  -0.011, New P: -0.279
iter 16 loss: 0.461
Actual params: [ 1.0433, -0.2789]
-Original Grad: -0.017, -lr * Pred Grad:  -0.005, New P: 1.038
-Original Grad: 0.001, -lr * Pred Grad:  0.005, New P: -0.274
iter 17 loss: 0.461
Actual params: [ 1.038 , -0.2743]
-Original Grad: 0.013, -lr * Pred Grad:  0.004, New P: 1.042
-Original Grad: 0.003, -lr * Pred Grad:  0.005, New P: -0.269
iter 18 loss: 0.462
Actual params: [ 1.0418, -0.2692]
-Original Grad: -0.019, -lr * Pred Grad:  -0.006, New P: 1.036
-Original Grad: -0.003, -lr * Pred Grad:  -0.004, New P: -0.274
iter 19 loss: 0.461
Actual params: [ 1.0361, -0.2735]
-Original Grad: 0.035, -lr * Pred Grad:  0.011, New P: 1.047
-Original Grad: 0.003, -lr * Pred Grad:  0.002, New P: -0.271
iter 20 loss: 0.462
Actual params: [ 1.0467, -0.2713]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.472, -lr * Pred Grad:  0.706, New P: 1.300
-Original Grad: 0.005, -lr * Pred Grad:  0.146, New P: 0.741
iter 0 loss: 0.885
Actual params: [1.3002, 0.7405]
-Original Grad: 1.073, -lr * Pred Grad:  0.734, New P: 2.034
-Original Grad: -0.358, -lr * Pred Grad:  0.448, New P: 1.188
iter 1 loss: 1.831
Actual params: [2.034 , 1.1884]
-Original Grad: 0.599, -lr * Pred Grad:  0.173, New P: 2.207
-Original Grad: 0.822, -lr * Pred Grad:  0.125, New P: 1.314
iter 2 loss: 0.578
Actual params: [2.2069, 1.3137]
-Original Grad: 0.269, -lr * Pred Grad:  0.063, New P: 2.269
-Original Grad: 0.407, -lr * Pred Grad:  0.046, New P: 1.360
iter 3 loss: 0.461
Actual params: [2.2695, 1.3601]
-Original Grad: 0.211, -lr * Pred Grad:  0.042, New P: 2.312
-Original Grad: 0.314, -lr * Pred Grad:  0.031, New P: 1.391
iter 4 loss: 0.441
Actual params: [2.3116, 1.3909]
-Original Grad: 0.174, -lr * Pred Grad:  0.032, New P: 2.344
-Original Grad: 0.295, -lr * Pred Grad:  0.025, New P: 1.416
iter 5 loss: 0.434
Actual params: [2.3438, 1.4158]
-Original Grad: 0.157, -lr * Pred Grad:  0.026, New P: 2.370
-Original Grad: 0.253, -lr * Pred Grad:  0.020, New P: 1.436
iter 6 loss: 0.430
Actual params: [2.3703, 1.4356]
-Original Grad: 0.138, -lr * Pred Grad:  0.022, New P: 2.392
-Original Grad: 0.207, -lr * Pred Grad:  0.015, New P: 1.451
iter 7 loss: 0.428
Actual params: [2.392 , 1.4511]
-Original Grad: 0.129, -lr * Pred Grad:  0.019, New P: 2.411
-Original Grad: 0.188, -lr * Pred Grad:  0.013, New P: 1.465
iter 8 loss: 0.426
Actual params: [2.4113, 1.4646]
-Original Grad: 0.119, -lr * Pred Grad:  0.017, New P: 2.428
-Original Grad: 0.169, -lr * Pred Grad:  0.012, New P: 1.476
iter 9 loss: 0.426
Actual params: [2.4283, 1.4763]
-Original Grad: 0.118, -lr * Pred Grad:  0.016, New P: 2.445
-Original Grad: 0.162, -lr * Pred Grad:  0.011, New P: 1.487
iter 10 loss: 0.425
Actual params: [2.4446, 1.4872]
-Original Grad: 0.110, -lr * Pred Grad:  0.015, New P: 2.459
-Original Grad: 0.155, -lr * Pred Grad:  0.010, New P: 1.497
iter 11 loss: 0.424
Actual params: [2.4595, 1.4973]
-Original Grad: 0.105, -lr * Pred Grad:  0.014, New P: 2.473
-Original Grad: 0.153, -lr * Pred Grad:  0.010, New P: 1.507
iter 12 loss: 0.424
Actual params: [2.4732, 1.5069]
-Original Grad: 0.104, -lr * Pred Grad:  0.013, New P: 2.487
-Original Grad: 0.155, -lr * Pred Grad:  0.009, New P: 1.516
iter 13 loss: 0.424
Actual params: [2.4867, 1.5164]
-Original Grad: 0.105, -lr * Pred Grad:  0.013, New P: 2.500
-Original Grad: 0.148, -lr * Pred Grad:  0.009, New P: 1.525
iter 14 loss: 0.424
Actual params: [2.4998, 1.5252]
-Original Grad: 0.100, -lr * Pred Grad:  0.012, New P: 2.512
-Original Grad: 0.141, -lr * Pred Grad:  0.008, New P: 1.533
iter 15 loss: 0.424
Actual params: [2.5119, 1.5335]
-Original Grad: 0.098, -lr * Pred Grad:  0.012, New P: 2.524
-Original Grad: 0.141, -lr * Pred Grad:  0.008, New P: 1.541
iter 16 loss: 0.424
Actual params: [2.5237, 1.5415]
-Original Grad: 0.087, -lr * Pred Grad:  0.010, New P: 2.534
-Original Grad: 0.128, -lr * Pred Grad:  0.007, New P: 1.549
iter 17 loss: 0.425
Actual params: [2.5339, 1.5487]
-Original Grad: 0.084, -lr * Pred Grad:  0.010, New P: 2.544
-Original Grad: 0.122, -lr * Pred Grad:  0.007, New P: 1.555
iter 18 loss: 0.425
Actual params: [2.5437, 1.5554]
-Original Grad: 0.080, -lr * Pred Grad:  0.009, New P: 2.553
-Original Grad: 0.121, -lr * Pred Grad:  0.007, New P: 1.562
iter 19 loss: 0.426
Actual params: [2.5529, 1.562 ]
-Original Grad: 0.064, -lr * Pred Grad:  0.007, New P: 2.560
-Original Grad: 0.098, -lr * Pred Grad:  0.005, New P: 1.567
iter 20 loss: 0.426
Actual params: [2.5601, 1.5673]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: -0.071, -lr * Pred Grad:  0.189, New P: 0.783
-Original Grad: -0.220, -lr * Pred Grad:  -0.264, New P: 0.330
iter 0 loss: 0.338
Actual params: [0.7832, 0.3303]
-Original Grad: 0.054, -lr * Pred Grad:  0.311, New P: 1.094
-Original Grad: -0.016, -lr * Pred Grad:  -0.147, New P: 0.183
iter 1 loss: 0.269
Actual params: [1.0938, 0.1833]
-Original Grad: 0.005, -lr * Pred Grad:  -0.009, New P: 1.085
-Original Grad: 0.016, -lr * Pred Grad:  0.016, New P: 0.200
iter 2 loss: 0.263
Actual params: [1.0851, 0.1996]
-Original Grad: 0.006, -lr * Pred Grad:  -0.003, New P: 1.082
-Original Grad: 0.016, -lr * Pred Grad:  0.014, New P: 0.214
iter 3 loss: 0.262
Actual params: [1.0823, 0.2138]
-Original Grad: 0.007, -lr * Pred Grad:  -0.004, New P: 1.079
-Original Grad: 0.017, -lr * Pred Grad:  0.015, New P: 0.229
iter 4 loss: 0.261
Actual params: [1.0787, 0.229 ]
-Original Grad: 0.007, -lr * Pred Grad:  0.009, New P: 1.088
-Original Grad: 0.011, -lr * Pred Grad:  0.005, New P: 0.234
iter 5 loss: 0.260
Actual params: [1.0879, 0.2343]
-Original Grad: 0.007, -lr * Pred Grad:  0.014, New P: 1.101
-Original Grad: 0.010, -lr * Pred Grad:  0.003, New P: 0.237
iter 6 loss: 0.260
Actual params: [1.1015, 0.237 ]
-Original Grad: 0.003, -lr * Pred Grad:  -0.004, New P: 1.097
-Original Grad: 0.010, -lr * Pred Grad:  0.010, New P: 0.247
iter 7 loss: 0.260
Actual params: [1.0974, 0.2473]
-Original Grad: 0.005, -lr * Pred Grad:  0.005, New P: 1.102
-Original Grad: 0.010, -lr * Pred Grad:  0.006, New P: 0.254
iter 8 loss: 0.259
Actual params: [1.1023, 0.2536]
-Original Grad: 0.004, -lr * Pred Grad:  0.002, New P: 1.105
-Original Grad: 0.009, -lr * Pred Grad:  0.007, New P: 0.260
iter 9 loss: 0.259
Actual params: [1.1046, 0.2604]
-Original Grad: 0.004, -lr * Pred Grad:  0.005, New P: 1.110
-Original Grad: 0.008, -lr * Pred Grad:  0.004, New P: 0.265
iter 10 loss: 0.259
Actual params: [1.1101, 0.2646]
-Original Grad: 0.006, -lr * Pred Grad:  0.017, New P: 1.127
-Original Grad: 0.005, -lr * Pred Grad:  -0.003, New P: 0.262
iter 11 loss: 0.259
Actual params: [1.1274, 0.2615]
-Original Grad: 0.003, -lr * Pred Grad:  -0.006, New P: 1.122
-Original Grad: 0.010, -lr * Pred Grad:  0.010, New P: 0.272
iter 12 loss: 0.259
Actual params: [1.1218, 0.2719]
-Original Grad: 0.004, -lr * Pred Grad:  0.008, New P: 1.130
-Original Grad: 0.006, -lr * Pred Grad:  0.002, New P: 0.274
iter 13 loss: 0.259
Actual params: [1.1295, 0.2741]
-Original Grad: 0.004, -lr * Pred Grad:  0.010, New P: 1.139
-Original Grad: 0.006, -lr * Pred Grad:  0.001, New P: 0.275
iter 14 loss: 0.259
Actual params: [1.1391, 0.2753]
-Original Grad: 0.003, -lr * Pred Grad:  0.001, New P: 1.140
-Original Grad: 0.007, -lr * Pred Grad:  0.005, New P: 0.280
iter 15 loss: 0.259
Actual params: [1.1404, 0.2804]
-Original Grad: 0.004, -lr * Pred Grad:  0.014, New P: 1.154
-Original Grad: 0.003, -lr * Pred Grad:  -0.003, New P: 0.278
iter 16 loss: 0.259
Actual params: [1.1542, 0.2778]
-Original Grad: 0.003, -lr * Pred Grad:  0.003, New P: 1.157
-Original Grad: 0.006, -lr * Pred Grad:  0.004, New P: 0.282
iter 17 loss: 0.260
Actual params: [1.1574, 0.2819]
-Original Grad: 0.003, -lr * Pred Grad:  0.006, New P: 1.163
-Original Grad: 0.005, -lr * Pred Grad:  0.002, New P: 0.284
iter 18 loss: 0.260
Actual params: [1.1631, 0.2843]
-Original Grad: 0.003, -lr * Pred Grad:  0.006, New P: 1.169
-Original Grad: 0.004, -lr * Pred Grad:  0.002, New P: 0.286
iter 19 loss: 0.260
Actual params: [1.1693, 0.286 ]
-Original Grad: 0.003, -lr * Pred Grad:  0.007, New P: 1.176
-Original Grad: 0.004, -lr * Pred Grad:  0.001, New P: 0.287
iter 20 loss: 0.260
Actual params: [1.1762, 0.2874]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.290, -lr * Pred Grad:  0.399, New P: 0.993
-Original Grad: 0.194, -lr * Pred Grad:  0.123, New P: 0.717
iter 0 loss: 0.605
Actual params: [0.9931, 0.7171]
-Original Grad: -1.596, -lr * Pred Grad:  -0.585, New P: 0.408
-Original Grad: -2.521, -lr * Pred Grad:  0.117, New P: 0.834
iter 1 loss: 1.554
Actual params: [0.4082, 0.8338]
-Original Grad: 0.041, -lr * Pred Grad:  0.105, New P: 0.514
-Original Grad: -0.130, -lr * Pred Grad:  -0.055, New P: 0.779
iter 2 loss: 0.722
Actual params: [0.5136, 0.7792]
-Original Grad: -0.069, -lr * Pred Grad:  0.133, New P: 0.646
-Original Grad: -0.467, -lr * Pred Grad:  -0.082, New P: 0.697
iter 3 loss: 0.708
Actual params: [0.6462, 0.6972]
-Original Grad: 0.022, -lr * Pred Grad:  0.185, New P: 0.831
-Original Grad: -0.880, -lr * Pred Grad:  -0.097, New P: 0.600
iter 4 loss: 0.633
Actual params: [0.8313, 0.6001]
-Original Grad: 0.085, -lr * Pred Grad:  0.099, New P: 0.930
-Original Grad: -0.645, -lr * Pred Grad:  -0.046, New P: 0.554
iter 5 loss: 0.450
Actual params: [0.9299, 0.5544]
-Original Grad: -0.648, -lr * Pred Grad:  0.006, New P: 0.936
-Original Grad: -0.886, -lr * Pred Grad:  -0.010, New P: 0.544
iter 6 loss: 0.318
Actual params: [0.9364, 0.544 ]
-Original Grad: 0.227, -lr * Pred Grad:  0.008, New P: 0.945
-Original Grad: -0.014, -lr * Pred Grad:  -0.008, New P: 0.536
iter 7 loss: 0.308
Actual params: [0.9446, 0.5364]
-Original Grad: -2.128, -lr * Pred Grad:  0.003, New P: 0.948
-Original Grad: -2.045, -lr * Pred Grad:  -0.006, New P: 0.531
iter 8 loss: 0.301
Actual params: [0.9476, 0.5307]
-Original Grad: 0.303, -lr * Pred Grad:  0.005, New P: 0.952
-Original Grad: 0.100, -lr * Pred Grad:  -0.005, New P: 0.526
iter 9 loss: 0.297
Actual params: [0.9523, 0.5257]
-Original Grad: 0.307, -lr * Pred Grad:  0.005, New P: 0.957
-Original Grad: 0.094, -lr * Pred Grad:  -0.005, New P: 0.521
iter 10 loss: 0.292
Actual params: [0.957 , 0.5206]
-Original Grad: -2.719, -lr * Pred Grad:  -0.002, New P: 0.955
-Original Grad: -2.133, -lr * Pred Grad:  0.001, New P: 0.522
iter 11 loss: 0.290
Actual params: [0.955 , 0.5218]
-Original Grad: 0.288, -lr * Pred Grad:  0.003, New P: 0.958
-Original Grad: 0.080, -lr * Pred Grad:  -0.003, New P: 0.519
iter 12 loss: 0.289
Actual params: [0.9577, 0.5186]
-Original Grad: 0.344, -lr * Pred Grad:  0.002, New P: 0.960
-Original Grad: 0.156, -lr * Pred Grad:  -0.002, New P: 0.516
iter 13 loss: 0.287
Actual params: [0.9598, 0.5161]
-Original Grad: 0.322, -lr * Pred Grad:  0.002, New P: 0.961
-Original Grad: 0.166, -lr * Pred Grad:  -0.002, New P: 0.514
iter 14 loss: 0.284
Actual params: [0.9615, 0.5142]
-Original Grad: 0.303, -lr * Pred Grad:  0.001, New P: 0.963
-Original Grad: 0.171, -lr * Pred Grad:  -0.001, New P: 0.513
iter 15 loss: 0.283
Actual params: [0.9628, 0.5128]
-Original Grad: 0.303, -lr * Pred Grad:  0.001, New P: 0.964
-Original Grad: 0.170, -lr * Pred Grad:  -0.001, New P: 0.511
iter 16 loss: 0.282
Actual params: [0.9641, 0.5113]
-Original Grad: 0.410, -lr * Pred Grad:  0.001, New P: 0.965
-Original Grad: 0.293, -lr * Pred Grad:  -0.001, New P: 0.511
iter 17 loss: 0.281
Actual params: [0.9648, 0.5106]
-Original Grad: 0.409, -lr * Pred Grad:  0.001, New P: 0.965
-Original Grad: 0.302, -lr * Pred Grad:  -0.001, New P: 0.510
iter 18 loss: 0.280
Actual params: [0.9654, 0.51  ]
-Original Grad: 0.331, -lr * Pred Grad:  0.001, New P: 0.966
-Original Grad: 0.221, -lr * Pred Grad:  -0.001, New P: 0.509
iter 19 loss: 0.280
Actual params: [0.9663, 0.5091]
-Original Grad: 0.253, -lr * Pred Grad:  0.001, New P: 0.967
-Original Grad: 0.154, -lr * Pred Grad:  -0.001, New P: 0.508
iter 20 loss: 0.279
Actual params: [0.9672, 0.5081]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.914, -lr * Pred Grad:  0.113, New P: 0.707
-Original Grad: -0.187, -lr * Pred Grad:  -0.044, New P: 0.550
iter 0 loss: 0.789
Actual params: [0.7071, 0.5501]
-Original Grad: 1.379, -lr * Pred Grad:  0.115, New P: 0.822
-Original Grad: -0.168, -lr * Pred Grad:  0.255, New P: 0.806
iter 1 loss: 0.716
Actual params: [0.8218, 0.8056]
-Original Grad: 1.934, -lr * Pred Grad:  0.018, New P: 0.840
-Original Grad: -0.664, -lr * Pred Grad:  -0.091, New P: 0.715
iter 2 loss: 0.672
Actual params: [0.8398, 0.7147]
-Original Grad: 1.841, -lr * Pred Grad:  0.057, New P: 0.897
-Original Grad: -0.387, -lr * Pred Grad:  0.111, New P: 0.826
iter 3 loss: 0.613
Actual params: [0.8967, 0.826 ]
-Original Grad: -25.882, -lr * Pred Grad:  0.003, New P: 0.900
-Original Grad: -27.744, -lr * Pred Grad:  -0.019, New P: 0.807
iter 4 loss: 0.980
Actual params: [0.8997, 0.8069]
-Original Grad: 2.816, -lr * Pred Grad:  0.015, New P: 0.915
-Original Grad: 0.079, -lr * Pred Grad:  -0.014, New P: 0.793
iter 5 loss: 0.538
Actual params: [0.9148, 0.7927]
-Original Grad: 2.350, -lr * Pred Grad:  0.010, New P: 0.924
-Original Grad: -0.288, -lr * Pred Grad:  -0.009, New P: 0.784
iter 6 loss: 0.502
Actual params: [0.9244, 0.7836]
-Original Grad: 2.066, -lr * Pred Grad:  0.006, New P: 0.931
-Original Grad: 0.044, -lr * Pred Grad:  -0.006, New P: 0.778
iter 7 loss: 0.484
Actual params: [0.9306, 0.7779]
-Original Grad: 2.126, -lr * Pred Grad:  0.005, New P: 0.936
-Original Grad: 0.143, -lr * Pred Grad:  -0.005, New P: 0.773
iter 8 loss: 0.473
Actual params: [0.9359, 0.7731]
-Original Grad: 1.947, -lr * Pred Grad:  0.004, New P: 0.940
-Original Grad: 0.146, -lr * Pred Grad:  -0.004, New P: 0.769
iter 9 loss: 0.466
Actual params: [0.9403, 0.7692]
-Original Grad: 1.823, -lr * Pred Grad:  0.004, New P: 0.944
-Original Grad: 0.205, -lr * Pred Grad:  -0.003, New P: 0.766
iter 10 loss: 0.458
Actual params: [0.9439, 0.766 ]
-Original Grad: 1.575, -lr * Pred Grad:  0.003, New P: 0.947
-Original Grad: 0.266, -lr * Pred Grad:  -0.002, New P: 0.764
iter 11 loss: 0.455
Actual params: [0.9467, 0.7636]
-Original Grad: 1.463, -lr * Pred Grad:  0.003, New P: 0.949
-Original Grad: 0.216, -lr * Pred Grad:  -0.002, New P: 0.761
iter 12 loss: 0.452
Actual params: [0.9492, 0.7614]
-Original Grad: 1.296, -lr * Pred Grad:  0.002, New P: 0.951
-Original Grad: 0.233, -lr * Pred Grad:  -0.002, New P: 0.760
iter 13 loss: 0.449
Actual params: [0.9514, 0.7595]
-Original Grad: 1.190, -lr * Pred Grad:  0.002, New P: 0.953
-Original Grad: 0.247, -lr * Pred Grad:  -0.002, New P: 0.758
iter 14 loss: 0.447
Actual params: [0.9532, 0.758 ]
-Original Grad: 1.164, -lr * Pred Grad:  0.002, New P: 0.955
-Original Grad: 0.231, -lr * Pred Grad:  -0.002, New P: 0.756
iter 15 loss: 0.445
Actual params: [0.955 , 0.7564]
-Original Grad: 1.173, -lr * Pred Grad:  0.002, New P: 0.957
-Original Grad: 0.203, -lr * Pred Grad:  -0.002, New P: 0.755
iter 16 loss: 0.444
Actual params: [0.9569, 0.7548]
-Original Grad: 1.095, -lr * Pred Grad:  0.002, New P: 0.959
-Original Grad: 0.201, -lr * Pred Grad:  -0.001, New P: 0.753
iter 17 loss: 0.445
Actual params: [0.9586, 0.7534]
-Original Grad: 1.012, -lr * Pred Grad:  0.002, New P: 0.960
-Original Grad: 0.195, -lr * Pred Grad:  -0.001, New P: 0.752
iter 18 loss: 0.444
Actual params: [0.9601, 0.7521]
-Original Grad: 0.992, -lr * Pred Grad:  0.001, New P: 0.962
-Original Grad: 0.252, -lr * Pred Grad:  -0.001, New P: 0.751
iter 19 loss: 0.443
Actual params: [0.9615, 0.751 ]
-Original Grad: 0.884, -lr * Pred Grad:  0.001, New P: 0.963
-Original Grad: 0.236, -lr * Pred Grad:  -0.001, New P: 0.750
iter 20 loss: 0.442
Actual params: [0.9627, 0.75  ]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.476, -lr * Pred Grad:  -0.441, New P: 0.153
-Original Grad: -2.818, -lr * Pred Grad:  -0.174, New P: 0.420
iter 0 loss: 0.743
Actual params: [0.1535, 0.4201]
-Original Grad: 0.036, -lr * Pred Grad:  -0.147, New P: 0.007
-Original Grad: -0.717, -lr * Pred Grad:  -0.038, New P: 0.382
iter 1 loss: 0.402
Actual params: [0.0066, 0.382 ]
-Original Grad: 0.084, -lr * Pred Grad:  0.113, New P: 0.120
-Original Grad: -0.120, -lr * Pred Grad:  0.014, New P: 0.397
iter 2 loss: 0.378
Actual params: [0.1197, 0.3965]
-Original Grad: 0.055, -lr * Pred Grad:  0.033, New P: 0.153
-Original Grad: -0.222, -lr * Pred Grad:  0.000, New P: 0.397
iter 3 loss: 0.377
Actual params: [0.1532, 0.3966]
-Original Grad: -0.004, -lr * Pred Grad:  -0.023, New P: 0.131
-Original Grad: -0.130, -lr * Pred Grad:  -0.006, New P: 0.390
iter 4 loss: 0.373
Actual params: [0.1305, 0.3904]
-Original Grad: 0.059, -lr * Pred Grad:  0.030, New P: 0.161
-Original Grad: -0.157, -lr * Pred Grad:  0.002, New P: 0.392
iter 5 loss: 0.370
Actual params: [0.1608, 0.3921]
-Original Grad: 0.042, -lr * Pred Grad:  0.018, New P: 0.179
-Original Grad: -0.122, -lr * Pred Grad:  0.000, New P: 0.393
iter 6 loss: 0.368
Actual params: [0.1785, 0.3926]
-Original Grad: 0.058, -lr * Pred Grad:  0.025, New P: 0.203
-Original Grad: -0.154, -lr * Pred Grad:  0.001, New P: 0.394
iter 7 loss: 0.365
Actual params: [0.2031, 0.3936]
-Original Grad: 0.055, -lr * Pred Grad:  0.016, New P: 0.219
-Original Grad: -0.206, -lr * Pred Grad:  -0.001, New P: 0.392
iter 8 loss: 0.361
Actual params: [0.2193, 0.3924]
-Original Grad: 0.058, -lr * Pred Grad:  0.023, New P: 0.242
-Original Grad: -0.157, -lr * Pred Grad:  0.001, New P: 0.393
iter 9 loss: 0.356
Actual params: [0.2424, 0.3932]
-Original Grad: 0.058, -lr * Pred Grad:  0.015, New P: 0.257
-Original Grad: -0.226, -lr * Pred Grad:  -0.002, New P: 0.392
iter 10 loss: 0.352
Actual params: [0.2572, 0.3916]
-Original Grad: 0.057, -lr * Pred Grad:  0.020, New P: 0.277
-Original Grad: -0.169, -lr * Pred Grad:  0.000, New P: 0.392
iter 11 loss: 0.347
Actual params: [0.2768, 0.3919]
-Original Grad: 0.057, -lr * Pred Grad:  0.019, New P: 0.296
-Original Grad: -0.170, -lr * Pred Grad:  0.000, New P: 0.392
iter 12 loss: 0.343
Actual params: [0.296 , 0.3921]
-Original Grad: 0.060, -lr * Pred Grad:  0.019, New P: 0.315
-Original Grad: -0.179, -lr * Pred Grad:  0.000, New P: 0.392
iter 13 loss: 0.340
Actual params: [0.3151, 0.3922]
-Original Grad: 0.059, -lr * Pred Grad:  0.017, New P: 0.333
-Original Grad: -0.185, -lr * Pred Grad:  -0.000, New P: 0.392
iter 14 loss: 0.336
Actual params: [0.3326, 0.392 ]
-Original Grad: 0.059, -lr * Pred Grad:  0.018, New P: 0.351
-Original Grad: -0.169, -lr * Pred Grad:  0.000, New P: 0.392
iter 15 loss: 0.333
Actual params: [0.3509, 0.3923]
-Original Grad: 0.058, -lr * Pred Grad:  0.022, New P: 0.373
-Original Grad: -0.112, -lr * Pred Grad:  0.002, New P: 0.394
iter 16 loss: 0.330
Actual params: [0.3729, 0.3942]
-Original Grad: 0.020, -lr * Pred Grad:  0.006, New P: 0.379
-Original Grad: -0.046, -lr * Pred Grad:  0.000, New P: 0.395
iter 17 loss: 0.327
Actual params: [0.3793, 0.3946]
-Original Grad: 0.021, -lr * Pred Grad:  0.006, New P: 0.385
-Original Grad: -0.047, -lr * Pred Grad:  0.000, New P: 0.395
iter 18 loss: 0.326
Actual params: [0.3849, 0.3949]
-Original Grad: 0.022, -lr * Pred Grad:  0.005, New P: 0.390
-Original Grad: -0.059, -lr * Pred Grad:  0.000, New P: 0.395
iter 19 loss: 0.326
Actual params: [0.3898, 0.3949]
-Original Grad: 0.028, -lr * Pred Grad:  0.006, New P: 0.396
-Original Grad: -0.073, -lr * Pred Grad:  0.000, New P: 0.395
iter 20 loss: 0.325
Actual params: [0.3958, 0.3949]
