Target params: [1.1812, 0.2779]
Actual params: [0.4128, 0.9469]
-Original Grad: 0.270, -lr * Pred Grad:  0.951, New P: 1.364
-Original Grad: 0.062, -lr * Pred Grad:  0.262, New P: 1.209
iter 0 loss: 0.761
Actual params: [1.3638, 1.2088]
-Original Grad: -53.291, -lr * Pred Grad:  -0.000, New P: 1.364
-Original Grad: -143.191, -lr * Pred Grad:  -0.000, New P: 1.209
iter 1 loss: 1.679
Actual params: [1.3638, 1.2087]
-Original Grad: 5.881, -lr * Pred Grad:  -0.037, New P: 1.327
-Original Grad: 20.299, -lr * Pred Grad:  0.014, New P: 1.222
iter 2 loss: 1.679
Actual params: [1.3269, 1.2225]
-Original Grad: 2.583, -lr * Pred Grad:  0.075, New P: 1.402
-Original Grad: 0.472, -lr * Pred Grad:  -0.028, New P: 1.195
iter 3 loss: 1.689
Actual params: [1.4017, 1.1946]
-Original Grad: 0.490, -lr * Pred Grad:  0.051, New P: 1.453
-Original Grad: -0.487, -lr * Pred Grad:  -0.019, New P: 1.176
iter 4 loss: 1.670
Actual params: [1.4526, 1.1756]
-Original Grad: 0.809, -lr * Pred Grad:  0.159, New P: 1.612
-Original Grad: -0.360, -lr * Pred Grad:  -0.059, New P: 1.116
iter 5 loss: 1.651
Actual params: [1.612 , 1.1162]
-Original Grad: 1.757, -lr * Pred Grad:  0.439, New P: 2.051
-Original Grad: 0.280, -lr * Pred Grad:  -0.163, New P: 0.953
iter 6 loss: 1.552
Actual params: [2.0506, 0.9527]
-Original Grad: -0.445, -lr * Pred Grad:  -0.227, New P: 1.824
-Original Grad: 0.299, -lr * Pred Grad:  0.085, New P: 1.037
iter 7 loss: 0.750
Actual params: [1.8236, 1.0373]
-Original Grad: 1.261, -lr * Pred Grad:  0.502, New P: 2.326
-Original Grad: -2.531, -lr * Pred Grad:  -0.187, New P: 0.850
iter 8 loss: 1.379
Actual params: [2.3257, 0.8501]
-Original Grad: -0.273, -lr * Pred Grad:  -0.132, New P: 2.193
-Original Grad: -0.044, -lr * Pred Grad:  0.049, New P: 0.899
iter 9 loss: 0.808
Actual params: [2.1935, 0.899 ]
-Original Grad: -0.286, -lr * Pred Grad:  -0.310, New P: 1.883
-Original Grad: 0.278, -lr * Pred Grad:  0.115, New P: 1.014
iter 10 loss: 0.782
Actual params: [1.8831, 1.0141]
-Original Grad: -3.001, -lr * Pred Grad:  -0.012, New P: 1.871
-Original Grad: 11.047, -lr * Pred Grad:  0.001, New P: 1.015
iter 11 loss: 0.677
Actual params: [1.871 , 1.0153]
-Original Grad: 11.641, -lr * Pred Grad:  0.006, New P: 1.877
-Original Grad: -41.132, -lr * Pred Grad:  -0.002, New P: 1.014
iter 12 loss: 0.709
Actual params: [1.8768, 1.0136]
-Original Grad: -3.598, -lr * Pred Grad:  -0.002, New P: 1.875
-Original Grad: 13.738, -lr * Pred Grad:  0.001, New P: 1.015
iter 13 loss: 0.678
Actual params: [1.8753, 1.0146]
-Original Grad: -3.835, -lr * Pred Grad:  -0.048, New P: 1.828
-Original Grad: 13.606, -lr * Pred Grad:  -0.011, New P: 1.003
iter 14 loss: 0.685
Actual params: [1.8277, 1.0034]
-Original Grad: -6.498, -lr * Pred Grad:  0.005, New P: 1.833
-Original Grad: 34.910, -lr * Pred Grad:  0.002, New P: 1.005
iter 15 loss: 0.665
Actual params: [1.8331, 1.0055]
-Original Grad: 63.945, -lr * Pred Grad:  0.005, New P: 1.838
-Original Grad: -264.772, -lr * Pred Grad:  0.001, New P: 1.006
iter 16 loss: 0.696
Actual params: [1.8382, 1.0062]
-Original Grad: -5.687, -lr * Pred Grad:  0.009, New P: 1.847
-Original Grad: 26.038, -lr * Pred Grad:  0.002, New P: 1.008
iter 17 loss: 0.675
Actual params: [1.8468, 1.0083]
-Original Grad: 0.025, -lr * Pred Grad:  -0.012, New P: 1.835
-Original Grad: -1.402, -lr * Pred Grad:  -0.003, New P: 1.006
iter 18 loss: 0.683
Actual params: [1.8349, 1.0055]
-Original Grad: -6.853, -lr * Pred Grad:  -0.005, New P: 1.830
-Original Grad: 30.529, -lr * Pred Grad:  -0.001, New P: 1.005
iter 19 loss: 0.676
Actual params: [1.8301, 1.0047]
-Original Grad: 64.020, -lr * Pred Grad:  0.012, New P: 1.842
-Original Grad: -255.708, -lr * Pred Grad:  0.003, New P: 1.007
iter 20 loss: 0.690
Actual params: [1.8424, 1.0073]
Target params: [1.1812, 0.2779]
Actual params: [0.8379, 0.635 ]
-Original Grad: 0.071, -lr * Pred Grad:  0.341, New P: 1.179
-Original Grad: -0.028, -lr * Pred Grad:  0.212, New P: 0.847
iter 0 loss: 0.245
Actual params: [1.1785, 0.8474]
-Original Grad: -3.400, -lr * Pred Grad:  -0.662, New P: 0.517
-Original Grad: 7.022, -lr * Pred Grad:  -0.121, New P: 0.726
iter 1 loss: 1.764
Actual params: [0.517 , 0.7265]
-Original Grad: 0.372, -lr * Pred Grad:  0.123, New P: 0.640
-Original Grad: -0.210, -lr * Pred Grad:  0.026, New P: 0.753
iter 2 loss: 0.434
Actual params: [0.6401, 0.7527]
-Original Grad: 0.198, -lr * Pred Grad:  0.091, New P: 0.731
-Original Grad: -0.037, -lr * Pred Grad:  0.021, New P: 0.774
iter 3 loss: 0.376
Actual params: [0.731 , 0.7736]
-Original Grad: 0.073, -lr * Pred Grad:  0.070, New P: 0.800
-Original Grad: 0.078, -lr * Pred Grad:  0.020, New P: 0.794
iter 4 loss: 0.323
Actual params: [0.8005, 0.7939]
-Original Grad: 0.118, -lr * Pred Grad:  0.097, New P: 0.897
-Original Grad: 0.094, -lr * Pred Grad:  0.031, New P: 0.825
iter 5 loss: 0.279
Actual params: [0.897 , 0.8245]
-Original Grad: 0.005, -lr * Pred Grad:  0.042, New P: 0.939
-Original Grad: 0.132, -lr * Pred Grad:  0.042, New P: 0.867
iter 6 loss: 0.240
Actual params: [0.9387, 0.867 ]
-Original Grad: -0.007, -lr * Pred Grad:  0.048, New P: 0.987
-Original Grad: 0.223, -lr * Pred Grad:  0.060, New P: 0.927
iter 7 loss: 0.219
Actual params: [0.9867, 0.927 ]
-Original Grad: -2.159, -lr * Pred Grad:  -0.821, New P: 0.166
-Original Grad: -8.573, -lr * Pred Grad:  0.039, New P: 0.966
iter 8 loss: 1.961
Actual params: [0.1656, 0.9658]
-Original Grad: 0.218, -lr * Pred Grad:  0.448, New P: 0.613
-Original Grad: -0.015, -lr * Pred Grad:  -0.090, New P: 0.876
iter 9 loss: 0.634
Actual params: [0.6131, 0.8757]
-Original Grad: 0.314, -lr * Pred Grad:  0.186, New P: 0.799
-Original Grad: -0.155, -lr * Pred Grad:  -0.036, New P: 0.839
iter 10 loss: 0.424
Actual params: [0.7991, 0.8393]
-Original Grad: -0.083, -lr * Pred Grad:  0.009, New P: 0.808
-Original Grad: -0.188, -lr * Pred Grad:  -0.020, New P: 0.820
iter 11 loss: 0.283
Actual params: [0.8085, 0.8196]
-Original Grad: 0.117, -lr * Pred Grad:  0.009, New P: 0.817
-Original Grad: 0.115, -lr * Pred Grad:  0.011, New P: 0.831
iter 12 loss: 0.274
Actual params: [0.8171, 0.8309]
-Original Grad: 0.103, -lr * Pred Grad:  0.003, New P: 0.820
-Original Grad: 0.117, -lr * Pred Grad:  0.034, New P: 0.865
iter 13 loss: 0.272
Actual params: [0.8204, 0.8648]
-Original Grad: 0.182, -lr * Pred Grad:  0.085, New P: 0.905
-Original Grad: -0.106, -lr * Pred Grad:  -0.046, New P: 0.819
iter 14 loss: 0.272
Actual params: [0.9049, 0.8188]
-Original Grad: 0.011, -lr * Pred Grad:  -0.005, New P: 0.900
-Original Grad: 0.114, -lr * Pred Grad:  0.069, New P: 0.888
iter 15 loss: 0.239
Actual params: [0.9001, 0.8877]
-Original Grad: 0.129, -lr * Pred Grad:  0.129, New P: 1.029
-Original Grad: -0.158, -lr * Pred Grad:  -0.018, New P: 0.869
iter 16 loss: 0.228
Actual params: [1.0288, 0.8694]
-Original Grad: -1.586, -lr * Pred Grad:  -0.428, New P: 0.601
-Original Grad: -6.007, -lr * Pred Grad:  -0.076, New P: 0.794
iter 17 loss: 1.738
Actual params: [0.6008, 0.7935]
-Original Grad: 0.317, -lr * Pred Grad:  0.141, New P: 0.741
-Original Grad: -0.111, -lr * Pred Grad:  -0.029, New P: 0.765
iter 18 loss: 0.408
Actual params: [0.7415, 0.7646]
-Original Grad: 0.085, -lr * Pred Grad:  0.047, New P: 0.788
-Original Grad: 0.065, -lr * Pred Grad:  0.003, New P: 0.767
iter 19 loss: 0.312
Actual params: [0.7885, 0.7675]
-Original Grad: 0.120, -lr * Pred Grad:  0.078, New P: 0.866
-Original Grad: 0.071, -lr * Pred Grad:  0.011, New P: 0.778
iter 20 loss: 0.285
Actual params: [0.8663, 0.7785]
Target params: [1.1812, 0.2779]
Actual params: [0.5397, 0.7849]
-Original Grad: -0.956, -lr * Pred Grad:  -0.657, New P: -0.117
-Original Grad: -1.750, -lr * Pred Grad:  0.005, New P: 0.790
iter 0 loss: 1.107
Actual params: [-0.1171,  0.7899]
-Original Grad: 0.216, -lr * Pred Grad:  0.495, New P: 0.378
-Original Grad: -0.527, -lr * Pred Grad:  -0.228, New P: 0.562
iter 1 loss: 0.779
Actual params: [0.3775, 0.562 ]
-Original Grad: 0.010, -lr * Pred Grad:  -0.093, New P: 0.284
-Original Grad: 0.273, -lr * Pred Grad:  0.063, New P: 0.625
iter 2 loss: 0.250
Actual params: [0.2843, 0.6254]
-Original Grad: 0.485, -lr * Pred Grad:  0.143, New P: 0.428
-Original Grad: -0.850, -lr * Pred Grad:  -0.070, New P: 0.555
iter 3 loss: 0.502
Actual params: [0.4277, 0.555 ]
-Original Grad: -0.370, -lr * Pred Grad:  -0.058, New P: 0.370
-Original Grad: -0.438, -lr * Pred Grad:  0.003, New P: 0.558
iter 4 loss: 0.268
Actual params: [0.3696, 0.5579]
-Original Grad: 0.781, -lr * Pred Grad:  0.023, New P: 0.393
-Original Grad: 0.706, -lr * Pred Grad:  -0.004, New P: 0.554
iter 5 loss: 0.261
Actual params: [0.3927, 0.5543]
-Original Grad: -0.080, -lr * Pred Grad:  -0.045, New P: 0.348
-Original Grad: 0.119, -lr * Pred Grad:  0.039, New P: 0.594
iter 6 loss: 0.258
Actual params: [0.3481, 0.5938]
-Original Grad: 3.697, -lr * Pred Grad:  0.019, New P: 0.367
-Original Grad: -0.615, -lr * Pred Grad:  -0.001, New P: 0.592
iter 7 loss: 0.379
Actual params: [0.3675, 0.5924]
-Original Grad: -0.681, -lr * Pred Grad:  -0.007, New P: 0.360
-Original Grad: -0.530, -lr * Pred Grad:  -0.018, New P: 0.575
iter 8 loss: 0.225
Actual params: [0.3601, 0.5749]
-Original Grad: 4.929, -lr * Pred Grad:  0.004, New P: 0.364
-Original Grad: 1.728, -lr * Pred Grad:  0.004, New P: 0.579
iter 9 loss: 0.255
Actual params: [0.3643, 0.5792]
-Original Grad: -0.239, -lr * Pred Grad:  -0.002, New P: 0.362
-Original Grad: 0.017, -lr * Pred Grad:  0.004, New P: 0.583
iter 10 loss: 0.234
Actual params: [0.362 , 0.5835]
-Original Grad: -0.453, -lr * Pred Grad:  -0.006, New P: 0.356
-Original Grad: -0.038, -lr * Pred Grad:  0.007, New P: 0.590
iter 11 loss: 0.228
Actual params: [0.3564, 0.59  ]
-Original Grad: 14.591, -lr * Pred Grad:  0.002, New P: 0.359
-Original Grad: 3.345, -lr * Pred Grad:  -0.002, New P: 0.588
iter 12 loss: 0.240
Actual params: [0.3588, 0.5878]
-Original Grad: 0.118, -lr * Pred Grad:  -0.002, New P: 0.357
-Original Grad: 0.226, -lr * Pred Grad:  0.007, New P: 0.595
iter 13 loss: 0.223
Actual params: [0.357 , 0.5945]
-Original Grad: -1.099, -lr * Pred Grad:  -0.001, New P: 0.356
-Original Grad: -0.305, -lr * Pred Grad:  0.001, New P: 0.596
iter 14 loss: 0.214
Actual params: [0.356, 0.596]
-Original Grad: -0.811, -lr * Pred Grad:  -0.003, New P: 0.353
-Original Grad: -0.137, -lr * Pred Grad:  0.008, New P: 0.604
iter 15 loss: 0.214
Actual params: [0.3531, 0.6042]
-Original Grad: 29.358, -lr * Pred Grad:  0.001, New P: 0.354
-Original Grad: 2.347, -lr * Pred Grad:  -0.001, New P: 0.603
iter 16 loss: 0.307
Actual params: [0.3544, 0.6027]
-Original Grad: -3.979, -lr * Pred Grad:  0.000, New P: 0.354
-Original Grad: -0.908, -lr * Pred Grad:  -0.001, New P: 0.602
iter 17 loss: 0.208
Actual params: [0.3544, 0.6016]
-Original Grad: 1.719, -lr * Pred Grad:  -0.000, New P: 0.354
-Original Grad: 0.483, -lr * Pred Grad:  0.001, New P: 0.603
iter 18 loss: 0.214
Actual params: [0.3542, 0.6028]
-Original Grad: -2.133, -lr * Pred Grad:  -0.001, New P: 0.354
-Original Grad: -0.395, -lr * Pred Grad:  0.002, New P: 0.605
iter 19 loss: 0.211
Actual params: [0.3536, 0.605 ]
-Original Grad: 6.168, -lr * Pred Grad:  0.000, New P: 0.354
-Original Grad: 1.068, -lr * Pred Grad:  0.000, New P: 0.605
iter 20 loss: 0.213
Actual params: [0.3537, 0.605 ]
Target params: [1.1812, 0.2779]
Actual params: [0.5515, 0.3482]
-Original Grad: 0.411, -lr * Pred Grad:  1.111, New P: 1.663
-Original Grad: -0.323, -lr * Pred Grad:  -0.549, New P: -0.200
iter 0 loss: 1.223
Actual params: [ 1.6628, -0.2005]
-Original Grad: -0.750, -lr * Pred Grad:  -0.612, New P: 1.051
-Original Grad: 0.223, -lr * Pred Grad:  -0.766, New P: -0.967
iter 1 loss: 0.725
Actual params: [ 1.0507, -0.9668]
-Original Grad: 0.039, -lr * Pred Grad:  0.234, New P: 1.285
-Original Grad: 0.448, -lr * Pred Grad:  0.526, New P: -0.441
iter 2 loss: 0.675
Actual params: [ 1.285 , -0.4411]
-Original Grad: -0.594, -lr * Pred Grad:  -0.240, New P: 1.045
-Original Grad: 0.180, -lr * Pred Grad:  -0.015, New P: -0.456
iter 3 loss: 0.560
Actual params: [ 1.0454, -0.4557]
-Original Grad: 0.119, -lr * Pred Grad:  0.183, New P: 1.229
-Original Grad: 0.167, -lr * Pred Grad:  0.374, New P: -0.081
iter 4 loss: 0.474
Actual params: [ 1.2286, -0.0813]
-Original Grad: -0.444, -lr * Pred Grad:  -0.183, New P: 1.045
-Original Grad: 0.011, -lr * Pred Grad:  -0.159, New P: -0.241
iter 5 loss: 0.451
Actual params: [ 1.0452, -0.2408]
-Original Grad: 0.105, -lr * Pred Grad:  0.120, New P: 1.165
-Original Grad: 0.071, -lr * Pred Grad:  0.335, New P: 0.094
iter 6 loss: 0.425
Actual params: [1.1651, 0.0941]
-Original Grad: -0.215, -lr * Pred Grad:  -0.120, New P: 1.045
-Original Grad: -0.062, -lr * Pred Grad:  -0.345, New P: -0.251
iter 7 loss: 0.418
Actual params: [ 1.0449, -0.251 ]
-Original Grad: 0.105, -lr * Pred Grad:  0.126, New P: 1.171
-Original Grad: 0.073, -lr * Pred Grad:  0.327, New P: 0.076
iter 8 loss: 0.427
Actual params: [1.1712, 0.0758]
-Original Grad: -0.234, -lr * Pred Grad:  -0.115, New P: 1.056
-Original Grad: -0.072, -lr * Pred Grad:  -0.365, New P: -0.289
iter 9 loss: 0.419
Actual params: [ 1.0565, -0.2895]
-Original Grad: 0.046, -lr * Pred Grad:  0.063, New P: 1.120
-Original Grad: 0.089, -lr * Pred Grad:  0.377, New P: 0.088
iter 10 loss: 0.433
Actual params: [1.12  , 0.0878]
-Original Grad: -0.111, -lr * Pred Grad:  -0.069, New P: 1.051
-Original Grad: -0.070, -lr * Pred Grad:  -0.424, New P: -0.336
iter 11 loss: 0.410
Actual params: [ 1.0508, -0.3364]
-Original Grad: 0.086, -lr * Pred Grad:  0.164, New P: 1.215
-Original Grad: 0.115, -lr * Pred Grad:  0.358, New P: 0.022
iter 12 loss: 0.444
Actual params: [1.215 , 0.0217]
-Original Grad: -0.396, -lr * Pred Grad:  -0.174, New P: 1.041
-Original Grad: -0.037, -lr * Pred Grad:  -0.180, New P: -0.158
iter 13 loss: 0.435
Actual params: [ 1.0407, -0.1578]
-Original Grad: 0.106, -lr * Pred Grad:  0.100, New P: 1.141
-Original Grad: -0.015, -lr * Pred Grad:  -0.181, New P: -0.339
iter 14 loss: 0.413
Actual params: [ 1.1405, -0.3387]
-Original Grad: -0.282, -lr * Pred Grad:  -0.035, New P: 1.106
-Original Grad: 0.146, -lr * Pred Grad:  0.401, New P: 0.062
iter 15 loss: 0.453
Actual params: [1.1057, 0.0619]
-Original Grad: -0.072, -lr * Pred Grad:  -0.151, New P: 0.955
-Original Grad: -0.063, -lr * Pred Grad:  -0.526, New P: -0.464
iter 16 loss: 0.406
Actual params: [ 0.9551, -0.4643]
-Original Grad: 0.655, -lr * Pred Grad:  0.189, New P: 1.144
-Original Grad: 0.163, -lr * Pred Grad:  0.083, New P: -0.381
iter 17 loss: 0.539
Actual params: [ 1.1438, -0.3812]
-Original Grad: -0.314, -lr * Pred Grad:  -0.098, New P: 1.046
-Original Grad: 0.129, -lr * Pred Grad:  0.313, New P: -0.068
iter 18 loss: 0.469
Actual params: [ 1.0462, -0.0684]
-Original Grad: 0.109, -lr * Pred Grad:  0.077, New P: 1.124
-Original Grad: -0.032, -lr * Pred Grad:  -0.140, New P: -0.209
iter 19 loss: 0.409
Actual params: [ 1.1237, -0.2087]
-Original Grad: -0.207, -lr * Pred Grad:  -0.108, New P: 1.016
-Original Grad: 0.069, -lr * Pred Grad:  0.134, New P: -0.074
iter 20 loss: 0.419
Actual params: [ 1.0156, -0.0745]
Target params: [1.1812, 0.2779]
Actual params: [1.0909, 1.0332]
-Original Grad: -0.116, -lr * Pred Grad:  -0.172, New P: 0.919
-Original Grad: 0.296, -lr * Pred Grad:  0.238, New P: 1.271
iter 0 loss: 0.606
Actual params: [0.9186, 1.2708]
-Original Grad: 1.238, -lr * Pred Grad:  0.103, New P: 1.022
-Original Grad: 0.314, -lr * Pred Grad:  0.030, New P: 1.301
iter 1 loss: 0.639
Actual params: [1.0217, 1.3005]
-Original Grad: 0.267, -lr * Pred Grad:  -0.048, New P: 0.974
-Original Grad: 0.357, -lr * Pred Grad:  0.250, New P: 1.551
iter 2 loss: 0.557
Actual params: [0.9741, 1.5505]
-Original Grad: 0.502, -lr * Pred Grad:  -0.013, New P: 0.961
-Original Grad: 0.365, -lr * Pred Grad:  0.085, New P: 1.636
iter 3 loss: 0.510
Actual params: [0.9615, 1.6358]
-Original Grad: 0.370, -lr * Pred Grad:  -0.030, New P: 0.932
-Original Grad: 0.257, -lr * Pred Grad:  0.090, New P: 1.725
iter 4 loss: 0.485
Actual params: [0.9319, 1.7254]
-Original Grad: 0.429, -lr * Pred Grad:  0.056, New P: 0.988
-Original Grad: 0.209, -lr * Pred Grad:  -0.067, New P: 1.659
iter 5 loss: 0.475
Actual params: [0.9882, 1.6588]
-Original Grad: 0.168, -lr * Pred Grad:  -0.098, New P: 0.890
-Original Grad: 0.170, -lr * Pred Grad:  0.180, New P: 1.839
iter 6 loss: 0.455
Actual params: [0.8904, 1.8389]
-Original Grad: 0.629, -lr * Pred Grad:  0.036, New P: 0.927
-Original Grad: 0.216, -lr * Pred Grad:  -0.049, New P: 1.789
iter 7 loss: 0.479
Actual params: [0.9266, 1.7894]
-Original Grad: 0.280, -lr * Pred Grad:  -0.024, New P: 0.903
-Original Grad: 0.139, -lr * Pred Grad:  0.079, New P: 1.868
iter 8 loss: 0.451
Actual params: [0.9031, 1.8679]
-Original Grad: 0.294, -lr * Pred Grad:  0.035, New P: 0.938
-Original Grad: 0.107, -lr * Pred Grad:  -0.064, New P: 1.804
iter 9 loss: 0.448
Actual params: [0.9378, 1.8037]
-Original Grad: 0.159, -lr * Pred Grad:  -0.054, New P: 0.883
-Original Grad: 0.095, -lr * Pred Grad:  0.136, New P: 1.940
iter 10 loss: 0.436
Actual params: [0.8834, 1.9398]
-Original Grad: 0.292, -lr * Pred Grad:  0.042, New P: 0.925
-Original Grad: 0.093, -lr * Pred Grad:  -0.089, New P: 1.851
iter 11 loss: 0.446
Actual params: [0.9249, 1.8509]
-Original Grad: 0.151, -lr * Pred Grad:  -0.018, New P: 0.907
-Original Grad: 0.067, -lr * Pred Grad:  0.060, New P: 1.911
iter 12 loss: 0.431
Actual params: [0.9066, 1.9111]
-Original Grad: 0.151, -lr * Pred Grad:  0.053, New P: 0.960
-Original Grad: 0.054, -lr * Pred Grad:  -0.120, New P: 1.791
iter 13 loss: 0.429
Actual params: [0.9597, 1.7913]
-Original Grad: 0.077, -lr * Pred Grad:  -0.050, New P: 0.910
-Original Grad: 0.072, -lr * Pred Grad:  0.111, New P: 1.903
iter 14 loss: 0.425
Actual params: [0.9099, 1.9027]
-Original Grad: 0.151, -lr * Pred Grad:  0.029, New P: 0.939
-Original Grad: 0.054, -lr * Pred Grad:  -0.053, New P: 1.849
iter 15 loss: 0.428
Actual params: [0.9393, 1.8492]
-Original Grad: 0.066, -lr * Pred Grad:  -0.036, New P: 0.903
-Original Grad: 0.042, -lr * Pred Grad:  0.085, New P: 1.934
iter 16 loss: 0.421
Actual params: [0.9031, 1.9342]
-Original Grad: 0.124, -lr * Pred Grad:  0.043, New P: 0.946
-Original Grad: 0.040, -lr * Pred Grad:  -0.094, New P: 1.840
iter 17 loss: 0.426
Actual params: [0.9463, 1.8399]
-Original Grad: 0.048, -lr * Pred Grad:  -0.045, New P: 0.902
-Original Grad: 0.041, -lr * Pred Grad:  0.100, New P: 1.940
iter 18 loss: 0.419
Actual params: [0.9017, 1.9401]
-Original Grad: 0.117, -lr * Pred Grad:  0.035, New P: 0.937
-Original Grad: 0.038, -lr * Pred Grad:  -0.074, New P: 1.866
iter 19 loss: 0.426
Actual params: [0.9366, 1.8662]
-Original Grad: 0.052, -lr * Pred Grad:  -0.031, New P: 0.905
-Original Grad: 0.033, -lr * Pred Grad:  0.075, New P: 1.941
iter 20 loss: 0.418
Actual params: [0.9053, 1.9411]
Target params: [1.1812, 0.2779]
Actual params: [1.1249, 0.5545]
-Original Grad: 0.673, -lr * Pred Grad:  2.292, New P: 3.417
-Original Grad: -0.218, -lr * Pred Grad:  -0.415, New P: 0.139
iter 0 loss: 1.819
Actual params: [3.4166, 0.1391]
-Original Grad: 0.007, -lr * Pred Grad:  0.029, New P: 3.445
-Original Grad: -0.015, -lr * Pred Grad:  -0.006, New P: 0.133
iter 1 loss: 0.404
Actual params: [3.4453, 0.1328]
-Original Grad: 0.004, -lr * Pred Grad:  0.019, New P: 3.464
-Original Grad: -0.016, -lr * Pred Grad:  -0.006, New P: 0.126
iter 2 loss: 0.404
Actual params: [3.4644, 0.1264]
-Original Grad: 0.002, -lr * Pred Grad:  0.014, New P: 3.479
-Original Grad: -0.017, -lr * Pred Grad:  -0.010, New P: 0.117
iter 3 loss: 0.403
Actual params: [3.4785, 0.1165]
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 3.491
-Original Grad: -0.017, -lr * Pred Grad:  -0.021, New P: 0.095
iter 4 loss: 0.401
Actual params: [3.4912, 0.0954]
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 3.505
-Original Grad: -0.021, -lr * Pred Grad:  -0.059, New P: 0.037
iter 5 loss: 0.399
Actual params: [3.5054, 0.0365]
-Original Grad: -0.002, -lr * Pred Grad:  0.015, New P: 3.520
-Original Grad: -0.018, -lr * Pred Grad:  -0.117, New P: -0.081
iter 6 loss: 0.396
Actual params: [ 3.52  , -0.0809]
-Original Grad: -0.004, -lr * Pred Grad:  -0.006, New P: 3.514
-Original Grad: 0.007, -lr * Pred Grad:  0.089, New P: 0.008
iter 7 loss: 0.392
Actual params: [3.5137, 0.0084]
-Original Grad: -0.003, -lr * Pred Grad:  0.024, New P: 3.538
-Original Grad: -0.018, -lr * Pred Grad:  -0.515, New P: -0.507
iter 8 loss: 0.394
Actual params: [ 3.5379, -0.5071]
-Original Grad: -0.008, -lr * Pred Grad:  0.049, New P: 3.586
-Original Grad: -0.010, -lr * Pred Grad:  -0.227, New P: -0.734
iter 9 loss: 0.392
Actual params: [ 3.5865, -0.7344]
-Original Grad: -0.002, -lr * Pred Grad:  0.004, New P: 3.590
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.754
iter 10 loss: 0.396
Actual params: [ 3.5904, -0.7537]
-Original Grad: -0.001, -lr * Pred Grad:  -0.099, New P: 3.491
-Original Grad: 0.001, -lr * Pred Grad:  0.169, New P: -0.585
iter 11 loss: 0.395
Actual params: [ 3.4913, -0.5851]
-Original Grad: 0.000, -lr * Pred Grad:  0.672, New P: 4.164
-Original Grad: -0.005, -lr * Pred Grad:  -1.218, New P: -1.803
iter 12 loss: 0.395
Actual params: [ 4.1637, -1.8034]
-Original Grad: 0.023, -lr * Pred Grad:  -1.086, New P: 3.078
-Original Grad: 0.037, -lr * Pred Grad:  1.682, New P: -0.121
iter 13 loss: 0.487
Actual params: [ 3.0779, -0.1214]
-Original Grad: 0.057, -lr * Pred Grad:  0.206, New P: 3.284
-Original Grad: 0.002, -lr * Pred Grad:  -0.196, New P: -0.317
iter 14 loss: 0.430
Actual params: [ 3.284 , -0.3169]
-Original Grad: 0.019, -lr * Pred Grad:  0.055, New P: 3.339
-Original Grad: -0.007, -lr * Pred Grad:  -0.314, New P: -0.631
iter 15 loss: 0.409
Actual params: [ 3.3385, -0.631 ]
-Original Grad: 0.016, -lr * Pred Grad:  0.086, New P: 3.424
-Original Grad: 0.001, -lr * Pred Grad:  -0.134, New P: -0.765
iter 16 loss: 0.407
Actual params: [ 3.4244, -0.7654]
-Original Grad: 0.014, -lr * Pred Grad:  -0.064, New P: 3.361
-Original Grad: 0.010, -lr * Pred Grad:  0.233, New P: -0.532
iter 17 loss: 0.406
Actual params: [ 3.3606, -0.5323]
-Original Grad: 0.012, -lr * Pred Grad:  0.465, New P: 3.825
-Original Grad: -0.002, -lr * Pred Grad:  -0.964, New P: -1.496
iter 18 loss: 0.405
Actual params: [ 3.8254, -1.4959]
-Original Grad: 0.027, -lr * Pred Grad:  -0.644, New P: 3.181
-Original Grad: 0.050, -lr * Pred Grad:  1.064, New P: -0.432
iter 19 loss: 0.469
Actual params: [ 3.1811, -0.4321]
-Original Grad: 0.028, -lr * Pred Grad:  0.285, New P: 3.466
-Original Grad: -0.006, -lr * Pred Grad:  -0.544, New P: -0.976
iter 20 loss: 0.416
Actual params: [ 3.466, -0.976]
Target params: [1.1812, 0.2779]
Actual params: [0.6859, 0.3761]
-Original Grad: -0.007, -lr * Pred Grad:  -0.000, New P: 0.686
-Original Grad: -0.047, -lr * Pred Grad:  -0.302, New P: 0.074
iter 0 loss: 0.285
Actual params: [0.6858, 0.0744]
-Original Grad: 0.038, -lr * Pred Grad:  0.199, New P: 0.885
-Original Grad: 0.002, -lr * Pred Grad:  -0.008, New P: 0.066
iter 1 loss: 0.291
Actual params: [0.8846, 0.0662]
-Original Grad: 0.021, -lr * Pred Grad:  0.190, New P: 1.075
-Original Grad: 0.019, -lr * Pred Grad:  0.558, New P: 0.624
iter 2 loss: 0.273
Actual params: [1.0748, 0.6241]
-Original Grad: 0.044, -lr * Pred Grad:  -0.001, New P: 1.074
-Original Grad: -0.237, -lr * Pred Grad:  -0.171, New P: 0.454
iter 3 loss: 0.279
Actual params: [1.0736, 0.4536]
-Original Grad: 0.007, -lr * Pred Grad:  -0.032, New P: 1.042
-Original Grad: -0.037, -lr * Pred Grad:  -0.060, New P: 0.393
iter 4 loss: 0.259
Actual params: [1.0417, 0.3935]
-Original Grad: 0.009, -lr * Pred Grad:  0.101, New P: 1.143
-Original Grad: -0.036, -lr * Pred Grad:  -0.077, New P: 0.317
iter 5 loss: 0.256
Actual params: [1.1428, 0.3167]
-Original Grad: 0.005, -lr * Pred Grad:  0.234, New P: 1.377
-Original Grad: -0.014, -lr * Pred Grad:  -0.026, New P: 0.291
iter 6 loss: 0.258
Actual params: [1.3768, 0.2907]
-Original Grad: 0.002, -lr * Pred Grad:  0.253, New P: 1.629
-Original Grad: -0.003, -lr * Pred Grad:  0.022, New P: 0.312
iter 7 loss: 0.273
Actual params: [1.6293, 0.3124]
-Original Grad: -0.001, -lr * Pred Grad:  -3.538, New P: -1.908
-Original Grad: -0.017, -lr * Pred Grad:  -1.430, New P: -1.117
iter 8 loss: 0.298
Actual params: [-1.9085, -1.1171]
-Original Grad: 0.315, -lr * Pred Grad:  0.189, New P: -1.720
-Original Grad: 0.257, -lr * Pred Grad:  0.583, New P: -0.534
iter 9 loss: 0.488
Actual params: [-1.7197, -0.5337]
-Original Grad: 0.154, -lr * Pred Grad:  0.588, New P: -1.131
-Original Grad: 0.089, -lr * Pred Grad:  -0.475, New P: -1.009
iter 10 loss: 0.363
Actual params: [-1.1315, -1.009 ]
-Original Grad: 0.028, -lr * Pred Grad:  -0.539, New P: -1.670
-Original Grad: 0.103, -lr * Pred Grad:  0.729, New P: -0.280
iter 11 loss: 0.358
Actual params: [-1.6701, -0.2799]
-Original Grad: 0.018, -lr * Pred Grad:  0.068, New P: -1.602
-Original Grad: 0.014, -lr * Pred Grad:  0.028, New P: -0.251
iter 12 loss: 0.341
Actual params: [-1.6023, -0.2515]
-Original Grad: 0.001, -lr * Pred Grad:  -0.112, New P: -1.714
-Original Grad: 0.005, -lr * Pred Grad:  0.188, New P: -0.064
iter 13 loss: 0.333
Actual params: [-1.714 , -0.0636]
-Original Grad: -0.006, -lr * Pred Grad:  -0.526, New P: -2.240
-Original Grad: 0.005, -lr * Pred Grad:  0.787, New P: 0.724
iter 14 loss: 0.331
Actual params: [-2.2395,  0.7237]
-Original Grad: 0.727, -lr * Pred Grad:  -0.047, New P: -2.287
-Original Grad: -1.765, -lr * Pred Grad:  -0.286, New P: 0.438
iter 15 loss: 0.740
Actual params: [-2.287 ,  0.4379]
-Original Grad: 0.294, -lr * Pred Grad:  0.121, New P: -2.166
-Original Grad: -0.624, -lr * Pred Grad:  -0.079, New P: 0.359
iter 16 loss: 0.476
Actual params: [-2.1663,  0.3587]
-Original Grad: 0.193, -lr * Pred Grad:  0.316, New P: -1.850
-Original Grad: -0.294, -lr * Pred Grad:  0.050, New P: 0.409
iter 17 loss: 0.412
Actual params: [-1.8502,  0.4087]
-Original Grad: 0.080, -lr * Pred Grad:  -0.445, New P: -2.295
-Original Grad: -0.219, -lr * Pred Grad:  -0.368, New P: 0.041
iter 18 loss: 0.358
Actual params: [-2.2952,  0.0407]
-Original Grad: 0.325, -lr * Pred Grad:  0.367, New P: -1.928
-Original Grad: 0.150, -lr * Pred Grad:  0.208, New P: 0.249
iter 19 loss: 0.431
Actual params: [-1.9281,  0.2491]
-Original Grad: 0.046, -lr * Pred Grad:  0.019, New P: -1.909
-Original Grad: -0.140, -lr * Pred Grad:  -0.246, New P: 0.003
iter 20 loss: 0.345
Actual params: [-1.9088,  0.0032]
Target params: [1.1812, 0.2779]
Actual params: [0.7976, 0.9218]
-Original Grad: -5.317, -lr * Pred Grad:  -0.628, New P: 0.169
-Original Grad: -2.237, -lr * Pred Grad:  1.089, New P: 2.011
iter 0 loss: 1.568
Actual params: [0.1694, 2.0112]
-Original Grad: 0.190, -lr * Pred Grad:  0.193, New P: 0.362
-Original Grad: 0.047, -lr * Pred Grad:  -0.393, New P: 1.618
iter 1 loss: 0.507
Actual params: [0.3621, 1.6179]
-Original Grad: 0.257, -lr * Pred Grad:  0.169, New P: 0.531
-Original Grad: -0.054, -lr * Pred Grad:  -0.331, New P: 1.287
iter 2 loss: 0.534
Actual params: [0.5313, 1.2868]
-Original Grad: -0.296, -lr * Pred Grad:  -0.111, New P: 0.420
-Original Grad: 0.357, -lr * Pred Grad:  0.285, New P: 1.572
iter 3 loss: 0.609
Actual params: [0.4201, 1.5722]
-Original Grad: 0.285, -lr * Pred Grad:  0.072, New P: 0.492
-Original Grad: -0.028, -lr * Pred Grad:  0.018, New P: 1.590
iter 4 loss: 0.527
Actual params: [0.4921, 1.59  ]
-Original Grad: -0.551, -lr * Pred Grad:  0.045, New P: 0.537
-Original Grad: 0.343, -lr * Pred Grad:  0.165, New P: 1.755
iter 5 loss: 0.489
Actual params: [0.5371, 1.7546]
-Original Grad: 0.382, -lr * Pred Grad:  0.040, New P: 0.577
-Original Grad: -0.086, -lr * Pred Grad:  0.076, New P: 1.830
iter 6 loss: 0.397
Actual params: [0.5772, 1.8303]
-Original Grad: 0.314, -lr * Pred Grad:  0.054, New P: 0.632
-Original Grad: -0.049, -lr * Pred Grad:  0.113, New P: 1.943
iter 7 loss: 0.352
Actual params: [0.6316, 1.9429]
-Original Grad: 0.413, -lr * Pred Grad:  0.060, New P: 0.692
-Original Grad: -0.030, -lr * Pred Grad:  0.124, New P: 2.067
iter 8 loss: 0.283
Actual params: [0.6917, 2.067 ]
-Original Grad: 0.531, -lr * Pred Grad:  0.022, New P: 0.714
-Original Grad: 0.046, -lr * Pred Grad:  0.038, New P: 2.105
iter 9 loss: 0.203
Actual params: [0.7136, 2.1045]
-Original Grad: 0.258, -lr * Pred Grad:  0.018, New P: 0.731
-Original Grad: 0.012, -lr * Pred Grad:  -0.075, New P: 2.030
iter 10 loss: 0.186
Actual params: [0.7314, 2.0297]
-Original Grad: 0.189, -lr * Pred Grad:  0.029, New P: 0.760
-Original Grad: -0.010, -lr * Pred Grad:  -0.207, New P: 1.823
iter 11 loss: 0.182
Actual params: [0.7605, 1.8231]
-Original Grad: -0.134, -lr * Pred Grad:  -0.011, New P: 0.750
-Original Grad: 0.291, -lr * Pred Grad:  0.100, New P: 1.923
iter 12 loss: 0.198
Actual params: [0.7499, 1.9228]
-Original Grad: -0.122, -lr * Pred Grad:  -0.002, New P: 0.748
-Original Grad: 0.088, -lr * Pred Grad:  0.047, New P: 1.970
iter 13 loss: 0.179
Actual params: [0.7477, 1.9695]
-Original Grad: 0.022, -lr * Pred Grad:  0.001, New P: 0.749
-Original Grad: -0.016, -lr * Pred Grad:  -0.016, New P: 1.953
iter 14 loss: 0.178
Actual params: [0.7488, 1.9534]
-Original Grad: 0.027, -lr * Pred Grad:  0.001, New P: 0.750
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 1.951
iter 15 loss: 0.178
Actual params: [0.7498, 1.9511]
-Original Grad: -0.022, -lr * Pred Grad:  -0.001, New P: 0.749
-Original Grad: 0.007, -lr * Pred Grad:  0.011, New P: 1.962
iter 16 loss: 0.178
Actual params: [0.7487, 1.962 ]
-Original Grad: -0.025, -lr * Pred Grad:  -0.000, New P: 0.748
-Original Grad: -0.007, -lr * Pred Grad:  -0.009, New P: 1.953
iter 17 loss: 0.178
Actual params: [0.7483, 1.9532]
-Original Grad: 0.048, -lr * Pred Grad:  0.002, New P: 0.750
-Original Grad: 0.004, -lr * Pred Grad:  0.001, New P: 1.954
iter 18 loss: 0.177
Actual params: [0.7503, 1.9545]
-Original Grad: -0.025, -lr * Pred Grad:  -0.001, New P: 0.749
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: 1.953
iter 19 loss: 0.178
Actual params: [0.7493, 1.9534]
-Original Grad: 0.025, -lr * Pred Grad:  0.001, New P: 0.751
-Original Grad: -0.001, -lr * Pred Grad:  -0.004, New P: 1.949
iter 20 loss: 0.178
Actual params: [0.7507, 1.9494]
Target params: [1.1812, 0.2779]
Actual params: [0.7765, 0.9   ]
-Original Grad: 1.699, -lr * Pred Grad:  0.154, New P: 0.931
-Original Grad: -1.217, -lr * Pred Grad:  -0.080, New P: 0.820
iter 0 loss: 0.762
Actual params: [0.9307, 0.8199]
-Original Grad: -8.667, -lr * Pred Grad:  -0.052, New P: 0.879
-Original Grad: -3.246, -lr * Pred Grad:  -0.233, New P: 0.587
iter 1 loss: 1.106
Actual params: [0.8788, 0.5866]
-Original Grad: 1.333, -lr * Pred Grad:  0.025, New P: 0.904
-Original Grad: -0.185, -lr * Pred Grad:  -0.058, New P: 0.528
iter 2 loss: 0.549
Actual params: [0.9041, 0.5283]
-Original Grad: 1.404, -lr * Pred Grad:  0.026, New P: 0.931
-Original Grad: -0.287, -lr * Pred Grad:  -0.098, New P: 0.431
iter 3 loss: 0.499
Actual params: [0.9306, 0.4307]
-Original Grad: 1.579, -lr * Pred Grad:  -0.001, New P: 0.929
-Original Grad: -0.706, -lr * Pred Grad:  -0.092, New P: 0.339
iter 4 loss: 0.357
Actual params: [0.9292, 0.3389]
-Original Grad: -14.849, -lr * Pred Grad:  0.028, New P: 0.957
-Original Grad: 5.161, -lr * Pred Grad:  0.211, New P: 0.550
iter 5 loss: 1.096
Actual params: [0.9567, 0.5502]
-Original Grad: 0.897, -lr * Pred Grad:  -0.021, New P: 0.936
-Original Grad: -0.341, -lr * Pred Grad:  -0.081, New P: 0.469
iter 6 loss: 0.442
Actual params: [0.9359, 0.4695]
-Original Grad: 1.405, -lr * Pred Grad:  -0.044, New P: 0.892
-Original Grad: -0.546, -lr * Pred Grad:  -0.181, New P: 0.289
iter 7 loss: 0.398
Actual params: [0.8919, 0.2885]
-Original Grad: -25.016, -lr * Pred Grad:  -0.034, New P: 0.858
-Original Grad: 12.602, -lr * Pred Grad:  -0.023, New P: 0.265
iter 8 loss: 1.019
Actual params: [0.8577, 0.2653]
-Original Grad: -0.280, -lr * Pred Grad:  0.034, New P: 0.892
-Original Grad: 0.894, -lr * Pred Grad:  0.070, New P: 0.335
iter 9 loss: 0.435
Actual params: [0.8919, 0.335 ]
-Original Grad: 2.255, -lr * Pred Grad:  0.010, New P: 0.902
-Original Grad: -1.028, -lr * Pred Grad:  0.006, New P: 0.341
iter 10 loss: 0.355
Actual params: [0.9023, 0.3407]
-Original Grad: 0.929, -lr * Pred Grad:  0.023, New P: 0.925
-Original Grad: -0.141, -lr * Pred Grad:  0.037, New P: 0.378
iter 11 loss: 0.325
Actual params: [0.9255, 0.378 ]
-Original Grad: 1.066, -lr * Pred Grad:  0.018, New P: 0.943
-Original Grad: -0.353, -lr * Pred Grad:  0.022, New P: 0.400
iter 12 loss: 0.286
Actual params: [0.9431, 0.4003]
-Original Grad: 0.641, -lr * Pred Grad:  0.016, New P: 0.959
-Original Grad: -0.206, -lr * Pred Grad:  0.019, New P: 0.419
iter 13 loss: 0.258
Actual params: [0.959 , 0.4194]
-Original Grad: 0.395, -lr * Pred Grad:  0.015, New P: 0.974
-Original Grad: -0.153, -lr * Pred Grad:  0.014, New P: 0.433
iter 14 loss: 0.245
Actual params: [0.9736, 0.4332]
-Original Grad: 0.317, -lr * Pred Grad:  0.017, New P: 0.991
-Original Grad: -0.169, -lr * Pred Grad:  0.013, New P: 0.446
iter 15 loss: 0.244
Actual params: [0.9906, 0.4458]
-Original Grad: 0.279, -lr * Pred Grad:  0.024, New P: 1.014
-Original Grad: -0.198, -lr * Pred Grad:  0.016, New P: 0.462
iter 16 loss: 0.238
Actual params: [1.0141, 0.4619]
-Original Grad: 0.223, -lr * Pred Grad:  0.015, New P: 1.029
-Original Grad: -0.237, -lr * Pred Grad:  0.010, New P: 0.472
iter 17 loss: 0.226
Actual params: [1.0288, 0.4716]
-Original Grad: 0.140, -lr * Pred Grad:  0.037, New P: 1.066
-Original Grad: -0.095, -lr * Pred Grad:  0.028, New P: 0.500
iter 18 loss: 0.211
Actual params: [1.0662, 0.4999]
-Original Grad: 0.304, -lr * Pred Grad:  -0.035, New P: 1.031
-Original Grad: -1.335, -lr * Pred Grad:  -0.028, New P: 0.472
iter 19 loss: 0.277
Actual params: [1.0307, 0.4723]
-Original Grad: -0.481, -lr * Pred Grad:  0.008, New P: 1.039
-Original Grad: 0.740, -lr * Pred Grad:  0.007, New P: 0.479
iter 20 loss: 0.200
Actual params: [1.0391, 0.4794]
Target params: [1.1812, 0.2779]
Actual params: [0.9202, 0.6989]
-Original Grad: -0.122, -lr * Pred Grad:  -0.703, New P: 0.217
-Original Grad: -1.114, -lr * Pred Grad:  -0.213, New P: 0.485
iter 0 loss: 0.902
Actual params: [0.217 , 0.4855]
-Original Grad: 0.234, -lr * Pred Grad:  0.393, New P: 0.610
-Original Grad: -2.863, -lr * Pred Grad:  -0.020, New P: 0.465
iter 1 loss: 0.557
Actual params: [0.6104, 0.4651]
-Original Grad: -0.008, -lr * Pred Grad:  -0.659, New P: -0.049
-Original Grad: -0.235, -lr * Pred Grad:  -0.055, New P: 0.410
iter 2 loss: 0.364
Actual params: [-0.0487,  0.4097]
-Original Grad: 0.006, -lr * Pred Grad:  0.003, New P: -0.045
-Original Grad: -0.013, -lr * Pred Grad:  -0.000, New P: 0.409
iter 3 loss: 0.421
Actual params: [-0.0453,  0.4095]
-Original Grad: 0.064, -lr * Pred Grad:  0.086, New P: 0.040
-Original Grad: -0.109, -lr * Pred Grad:  0.006, New P: 0.416
iter 4 loss: 0.420
Actual params: [0.0404, 0.4155]
-Original Grad: 0.049, -lr * Pred Grad:  0.038, New P: 0.079
-Original Grad: -0.406, -lr * Pred Grad:  -0.046, New P: 0.369
iter 5 loss: 0.412
Actual params: [0.0788, 0.3695]
-Original Grad: 0.071, -lr * Pred Grad:  0.207, New P: 0.286
-Original Grad: -0.129, -lr * Pred Grad:  -0.006, New P: 0.363
iter 6 loss: 0.356
Actual params: [0.2859, 0.3631]
-Original Grad: 0.063, -lr * Pred Grad:  0.146, New P: 0.432
-Original Grad: -0.157, -lr * Pred Grad:  -0.035, New P: 0.328
iter 7 loss: 0.315
Actual params: [0.4318, 0.3284]
-Original Grad: 0.037, -lr * Pred Grad:  0.078, New P: 0.510
-Original Grad: -0.092, -lr * Pred Grad:  -0.050, New P: 0.278
iter 8 loss: 0.274
Actual params: [0.5096, 0.2779]
-Original Grad: 0.020, -lr * Pred Grad:  -0.066, New P: 0.443
-Original Grad: -0.065, -lr * Pred Grad:  -0.122, New P: 0.156
iter 9 loss: 0.248
Actual params: [0.4433, 0.1559]
-Original Grad: 0.020, -lr * Pred Grad:  -0.103, New P: 0.340
-Original Grad: -0.050, -lr * Pred Grad:  -0.161, New P: -0.005
iter 10 loss: 0.230
Actual params: [ 0.3403, -0.0051]
-Original Grad: -0.007, -lr * Pred Grad:  -0.430, New P: -0.089
-Original Grad: -0.012, -lr * Pred Grad:  -0.294, New P: -0.299
iter 11 loss: 0.216
Actual params: [-0.0894, -0.2987]
-Original Grad: 0.003, -lr * Pred Grad:  0.097, New P: 0.007
-Original Grad: 0.001, -lr * Pred Grad:  0.046, New P: -0.252
iter 12 loss: 0.183
Actual params: [ 0.0075, -0.2524]
-Original Grad: -0.006, -lr * Pred Grad:  -0.072, New P: -0.065
-Original Grad: 0.003, -lr * Pred Grad:  -0.034, New P: -0.287
iter 13 loss: 0.194
Actual params: [-0.0648, -0.2865]
-Original Grad: 0.006, -lr * Pred Grad:  0.072, New P: 0.007
-Original Grad: -0.008, -lr * Pred Grad:  0.020, New P: -0.266
iter 14 loss: 0.186
Actual params: [ 0.0073, -0.2661]
-Original Grad: -0.018, -lr * Pred Grad:  -0.095, New P: -0.088
-Original Grad: 0.022, -lr * Pred Grad:  -0.023, New P: -0.289
iter 15 loss: 0.193
Actual params: [-0.0881, -0.2891]
-Original Grad: 0.009, -lr * Pred Grad:  0.079, New P: -0.009
-Original Grad: -0.014, -lr * Pred Grad:  0.014, New P: -0.275
iter 16 loss: 0.184
Actual params: [-0.0087, -0.2751]
-Original Grad: -0.017, -lr * Pred Grad:  -0.098, New P: -0.106
-Original Grad: 0.021, -lr * Pred Grad:  -0.022, New P: -0.297
iter 17 loss: 0.192
Actual params: [-0.1064, -0.2968]
-Original Grad: 0.008, -lr * Pred Grad:  0.062, New P: -0.044
-Original Grad: -0.016, -lr * Pred Grad:  0.004, New P: -0.292
iter 18 loss: 0.182
Actual params: [-0.0443, -0.2924]
-Original Grad: -0.007, -lr * Pred Grad:  -0.043, New P: -0.087
-Original Grad: 0.015, -lr * Pred Grad:  -0.000, New P: -0.293
iter 19 loss: 0.188
Actual params: [-0.0868, -0.2927]
-Original Grad: 0.009, -lr * Pred Grad:  0.145, New P: 0.058
-Original Grad: -0.016, -lr * Pred Grad:  0.029, New P: -0.264
iter 20 loss: 0.184
Actual params: [ 0.0584, -0.2638]
