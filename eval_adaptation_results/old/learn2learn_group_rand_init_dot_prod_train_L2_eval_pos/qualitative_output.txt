Target params: [1.1812, 0.2779]
Actual params: [1.084 , 1.4018]
-Original Grad: -0.176, -lr * Pred Grad: -0.044, New P: 1.040
-Original Grad: -0.113, -lr * Pred Grad: -0.038, New P: 1.364
iter 0 loss: 0.234
Actual params: [1.0402, 1.3642]
-Original Grad: 0.358, -lr * Pred Grad: -0.059, New P: 0.981
-Original Grad: -0.645, -lr * Pred Grad: -0.058, New P: 1.306
iter 1 loss: 0.193
Actual params: [0.9808, 1.306 ]
-Original Grad: -2.974, -lr * Pred Grad: -0.062, New P: 0.919
-Original Grad: 1.152, -lr * Pred Grad: -0.062, New P: 1.244
iter 2 loss: 0.121
Actual params: [0.9189, 1.2443]
-Original Grad: 0.035, -lr * Pred Grad: -0.062, New P: 0.857
-Original Grad: 0.054, -lr * Pred Grad: -0.062, New P: 1.182
iter 3 loss: 0.063
Actual params: [0.8567, 1.1821]
-Original Grad: 1.305, -lr * Pred Grad: -0.062, New P: 0.794
-Original Grad: 1.680, -lr * Pred Grad: -0.062, New P: 1.120
iter 4 loss: 0.146
Actual params: [0.7944, 1.1198]
-Original Grad: -3.208, -lr * Pred Grad: -0.062, New P: 0.732
-Original Grad: 1.292, -lr * Pred Grad: -0.062, New P: 1.058
iter 5 loss: 0.247
Actual params: [0.7321, 1.0575]
-Original Grad: 1.913, -lr * Pred Grad: -0.062, New P: 0.670
-Original Grad: 0.180, -lr * Pred Grad: -0.062, New P: 0.995
iter 6 loss: 0.391
Actual params: [0.6698, 0.9952]
-Original Grad: 5.048, -lr * Pred Grad: -0.062, New P: 0.608
-Original Grad: 1.207, -lr * Pred Grad: -0.062, New P: 0.933
iter 7 loss: 0.384
Actual params: [0.6075, 0.9329]
-Original Grad: -1.415, -lr * Pred Grad: -0.062, New P: 0.545
-Original Grad: -0.938, -lr * Pred Grad: -0.062, New P: 0.871
iter 8 loss: 0.391
Actual params: [0.5452, 0.8707]
-Original Grad: -0.029, -lr * Pred Grad: -0.062, New P: 0.483
-Original Grad: -0.324, -lr * Pred Grad: -0.062, New P: 0.808
iter 9 loss: 0.386
Actual params: [0.4829, 0.8084]
-Original Grad: 0.280, -lr * Pred Grad: -0.062, New P: 0.421
-Original Grad: -0.110, -lr * Pred Grad: -0.062, New P: 0.746
iter 10 loss: 0.386
Actual params: [0.4207, 0.7461]
-Original Grad: 0.223, -lr * Pred Grad: -0.062, New P: 0.358
-Original Grad: -0.131, -lr * Pred Grad: -0.062, New P: 0.684
iter 11 loss: 0.399
Actual params: [0.3584, 0.6838]
-Original Grad: 0.218, -lr * Pred Grad: -0.062, New P: 0.296
-Original Grad: -0.086, -lr * Pred Grad: -0.062, New P: 0.622
iter 12 loss: 0.406
Actual params: [0.2961, 0.6215]
-Original Grad: -0.437, -lr * Pred Grad: -0.062, New P: 0.234
-Original Grad: 1.813, -lr * Pred Grad: -0.062, New P: 0.559
iter 13 loss: 0.418
Actual params: [0.2338, 0.5592]
-Original Grad: 0.207, -lr * Pred Grad: -0.062, New P: 0.172
-Original Grad: -0.032, -lr * Pred Grad: -0.062, New P: 0.497
iter 14 loss: 0.441
Actual params: [0.1715, 0.4969]
-Original Grad: 0.095, -lr * Pred Grad: -0.062, New P: 0.109
-Original Grad: 0.034, -lr * Pred Grad: -0.062, New P: 0.435
iter 15 loss: 0.451
Actual params: [0.1092, 0.4346]
-Original Grad: 0.131, -lr * Pred Grad: -0.062, New P: 0.047
-Original Grad: 0.038, -lr * Pred Grad: -0.062, New P: 0.372
iter 16 loss: 0.464
Actual params: [0.0469, 0.3723]
-Original Grad: -0.631, -lr * Pred Grad: -0.062, New P: -0.015
-Original Grad: -0.757, -lr * Pred Grad: -0.062, New P: 0.310
iter 17 loss: 0.465
Actual params: [-0.0154,  0.3101]
-Original Grad: 0.107, -lr * Pred Grad: -0.062, New P: -0.078
-Original Grad: 0.041, -lr * Pred Grad: -0.062, New P: 0.248
iter 18 loss: 0.469
Actual params: [-0.0776,  0.2478]
-Original Grad: 0.104, -lr * Pred Grad: -0.062, New P: -0.140
-Original Grad: 0.058, -lr * Pred Grad: -0.062, New P: 0.185
iter 19 loss: 0.478
Actual params: [-0.1399,  0.1855]
-Original Grad: -0.026, -lr * Pred Grad: -0.062, New P: -0.202
-Original Grad: -0.107, -lr * Pred Grad: -0.062, New P: 0.123
iter 20 loss: 0.473
Actual params: [-0.2022,  0.1232]
Target params: [1.1812, 0.2779]
Actual params: [ 0.0029, -1.5044]
-Original Grad: 0.098, -lr * Pred Grad: 0.046, New P: 0.049
-Original Grad: 0.134, -lr * Pred Grad: 0.048, New P: -1.456
iter 0 loss: 0.455
Actual params: [ 0.0486, -1.4564]
-Original Grad: 0.524, -lr * Pred Grad: 0.062, New P: 0.111
-Original Grad: 0.375, -lr * Pred Grad: 0.063, New P: -1.394
iter 1 loss: 0.442
Actual params: [ 0.1108, -1.3938]
-Original Grad: 0.113, -lr * Pred Grad: 0.065, New P: 0.176
-Original Grad: 0.111, -lr * Pred Grad: 0.065, New P: -1.329
iter 2 loss: 0.425
Actual params: [ 0.1756, -1.3289]
-Original Grad: 0.186, -lr * Pred Grad: 0.065, New P: 0.241
-Original Grad: 0.132, -lr * Pred Grad: 0.065, New P: -1.264
iter 3 loss: 0.404
Actual params: [ 0.2408, -1.2638]
-Original Grad: 0.278, -lr * Pred Grad: 0.065, New P: 0.306
-Original Grad: 0.167, -lr * Pred Grad: 0.065, New P: -1.199
iter 4 loss: 0.384
Actual params: [ 0.306 , -1.1985]
-Original Grad: 0.297, -lr * Pred Grad: 0.065, New P: 0.371
-Original Grad: 0.175, -lr * Pred Grad: 0.065, New P: -1.133
iter 5 loss: 0.352
Actual params: [ 0.3713, -1.1333]
-Original Grad: 0.412, -lr * Pred Grad: 0.065, New P: 0.436
-Original Grad: 0.217, -lr * Pred Grad: 0.065, New P: -1.068
iter 6 loss: 0.308
Actual params: [ 0.4365, -1.0681]
-Original Grad: 0.502, -lr * Pred Grad: 0.065, New P: 0.502
-Original Grad: 0.275, -lr * Pred Grad: 0.065, New P: -1.003
iter 7 loss: 0.265
Actual params: [ 0.5017, -1.0028]
-Original Grad: 0.442, -lr * Pred Grad: 0.065, New P: 0.567
-Original Grad: 0.251, -lr * Pred Grad: 0.065, New P: -0.938
iter 8 loss: 0.209
Actual params: [ 0.567 , -0.9376]
-Original Grad: 0.303, -lr * Pred Grad: 0.065, New P: 0.632
-Original Grad: 0.184, -lr * Pred Grad: 0.065, New P: -0.872
iter 9 loss: 0.172
Actual params: [ 0.6322, -0.8724]
-Original Grad: 0.238, -lr * Pred Grad: 0.065, New P: 0.697
-Original Grad: 0.132, -lr * Pred Grad: 0.065, New P: -0.807
iter 10 loss: 0.145
Actual params: [ 0.6974, -0.8071]
-Original Grad: 0.227, -lr * Pred Grad: 0.065, New P: 0.763
-Original Grad: 0.109, -lr * Pred Grad: 0.065, New P: -0.742
iter 11 loss: 0.121
Actual params: [ 0.7627, -0.7419]
-Original Grad: -0.001, -lr * Pred Grad: 0.065, New P: 0.828
-Original Grad: -0.008, -lr * Pred Grad: 0.065, New P: -0.677
iter 12 loss: 0.116
Actual params: [ 0.8279, -0.6767]
-Original Grad: -0.154, -lr * Pred Grad: 0.065, New P: 0.893
-Original Grad: -0.092, -lr * Pred Grad: 0.065, New P: -0.611
iter 13 loss: 0.117
Actual params: [ 0.8931, -0.6114]
-Original Grad: -0.050, -lr * Pred Grad: 0.065, New P: 0.958
-Original Grad: -0.136, -lr * Pred Grad: 0.065, New P: -0.546
iter 14 loss: 0.135
Actual params: [ 0.9584, -0.5462]
-Original Grad: 0.090, -lr * Pred Grad: 0.065, New P: 1.024
-Original Grad: -0.114, -lr * Pred Grad: 0.065, New P: -0.481
iter 15 loss: 0.142
Actual params: [ 1.0236, -0.481 ]
-Original Grad: 0.203, -lr * Pred Grad: 0.065, New P: 1.089
-Original Grad: -0.114, -lr * Pred Grad: 0.065, New P: -0.416
iter 16 loss: 0.139
Actual params: [ 1.0888, -0.4157]
-Original Grad: 0.172, -lr * Pred Grad: 0.065, New P: 1.154
-Original Grad: -0.121, -lr * Pred Grad: 0.065, New P: -0.351
iter 17 loss: 0.136
Actual params: [ 1.1541, -0.3505]
-Original Grad: 0.201, -lr * Pred Grad: 0.065, New P: 1.219
-Original Grad: -0.118, -lr * Pred Grad: 0.065, New P: -0.285
iter 18 loss: 0.132
Actual params: [ 1.2193, -0.2853]
-Original Grad: 0.207, -lr * Pred Grad: 0.065, New P: 1.285
-Original Grad: -0.130, -lr * Pred Grad: 0.065, New P: -0.220
iter 19 loss: 0.126
Actual params: [ 1.2845, -0.22  ]
-Original Grad: 0.181, -lr * Pred Grad: 0.065, New P: 1.350
-Original Grad: -0.139, -lr * Pred Grad: 0.065, New P: -0.155
iter 20 loss: 0.122
Actual params: [ 1.3497, -0.1548]
Target params: [1.1812, 0.2779]
Actual params: [-0.8962,  0.3381]
-Original Grad: 0.164, -lr * Pred Grad: 0.049, New P: -0.847
-Original Grad: -0.126, -lr * Pred Grad: -0.040, New P: 0.299
iter 0 loss: 0.659
Actual params: [-0.8473,  0.2985]
-Original Grad: -0.381, -lr * Pred Grad: 0.060, New P: -0.787
-Original Grad: 0.090, -lr * Pred Grad: -0.059, New P: 0.240
iter 1 loss: 0.660
Actual params: [-0.7872,  0.2399]
-Original Grad: 1.626, -lr * Pred Grad: 0.065, New P: -0.723
-Original Grad: -0.195, -lr * Pred Grad: -0.062, New P: 0.178
iter 2 loss: 0.529
Actual params: [-0.7227,  0.1781]
-Original Grad: 1.719, -lr * Pred Grad: 0.065, New P: -0.658
-Original Grad: -0.055, -lr * Pred Grad: -0.062, New P: 0.116
iter 3 loss: 0.382
Actual params: [-0.6575,  0.1159]
-Original Grad: 0.634, -lr * Pred Grad: 0.065, New P: -0.592
-Original Grad: 0.048, -lr * Pred Grad: -0.062, New P: 0.054
iter 4 loss: 0.311
Actual params: [-0.5923,  0.0536]
-Original Grad: 0.275, -lr * Pred Grad: 0.065, New P: -0.527
-Original Grad: 0.027, -lr * Pred Grad: -0.062, New P: -0.009
iter 5 loss: 0.284
Actual params: [-0.5271, -0.0087]
-Original Grad: 0.186, -lr * Pred Grad: 0.065, New P: -0.462
-Original Grad: 0.015, -lr * Pred Grad: -0.062, New P: -0.071
iter 6 loss: 0.269
Actual params: [-0.4619, -0.071 ]
-Original Grad: 0.194, -lr * Pred Grad: 0.065, New P: -0.397
-Original Grad: 0.011, -lr * Pred Grad: -0.062, New P: -0.133
iter 7 loss: 0.257
Actual params: [-0.3966, -0.1332]
-Original Grad: 0.231, -lr * Pred Grad: 0.065, New P: -0.331
-Original Grad: 0.031, -lr * Pred Grad: -0.062, New P: -0.196
iter 8 loss: 0.244
Actual params: [-0.3314, -0.1955]
-Original Grad: 0.206, -lr * Pred Grad: 0.065, New P: -0.266
-Original Grad: 0.042, -lr * Pred Grad: -0.062, New P: -0.258
iter 9 loss: 0.232
Actual params: [-0.2662, -0.2578]
-Original Grad: 0.253, -lr * Pred Grad: 0.065, New P: -0.201
-Original Grad: 0.090, -lr * Pred Grad: -0.062, New P: -0.320
iter 10 loss: 0.222
Actual params: [-0.2009, -0.3201]
-Original Grad: 0.098, -lr * Pred Grad: 0.065, New P: -0.136
-Original Grad: 0.046, -lr * Pred Grad: -0.062, New P: -0.382
iter 11 loss: 0.215
Actual params: [-0.1357, -0.3824]
-Original Grad: 0.062, -lr * Pred Grad: 0.065, New P: -0.070
-Original Grad: 0.035, -lr * Pred Grad: -0.062, New P: -0.445
iter 12 loss: 0.213
Actual params: [-0.0705, -0.4447]
-Original Grad: 0.059, -lr * Pred Grad: 0.065, New P: -0.005
-Original Grad: 0.037, -lr * Pred Grad: -0.062, New P: -0.507
iter 13 loss: 0.211
Actual params: [-0.0052, -0.507 ]
-Original Grad: -0.118, -lr * Pred Grad: 0.065, New P: 0.060
-Original Grad: 0.000, -lr * Pred Grad: -0.062, New P: -0.569
iter 14 loss: 0.208
Actual params: [ 0.06  , -0.5693]
-Original Grad: 0.050, -lr * Pred Grad: 0.065, New P: 0.125
-Original Grad: 0.031, -lr * Pred Grad: -0.062, New P: -0.632
iter 15 loss: 0.204
Actual params: [ 0.1252, -0.6315]
-Original Grad: 0.062, -lr * Pred Grad: 0.065, New P: 0.190
-Original Grad: 0.051, -lr * Pred Grad: -0.062, New P: -0.694
iter 16 loss: 0.203
Actual params: [ 0.1905, -0.6938]
-Original Grad: 0.092, -lr * Pred Grad: 0.065, New P: 0.256
-Original Grad: 0.047, -lr * Pred Grad: -0.062, New P: -0.756
iter 17 loss: 0.201
Actual params: [ 0.2557, -0.7561]
-Original Grad: 0.132, -lr * Pred Grad: 0.065, New P: 0.321
-Original Grad: 0.053, -lr * Pred Grad: -0.062, New P: -0.818
iter 18 loss: 0.197
Actual params: [ 0.3209, -0.8184]
-Original Grad: 0.217, -lr * Pred Grad: 0.065, New P: 0.386
-Original Grad: 0.064, -lr * Pred Grad: -0.062, New P: -0.881
iter 19 loss: 0.189
Actual params: [ 0.3862, -0.8807]
-Original Grad: 0.228, -lr * Pred Grad: 0.065, New P: 0.451
-Original Grad: -0.055, -lr * Pred Grad: -0.062, New P: -0.943
iter 20 loss: 0.176
Actual params: [ 0.4514, -0.943 ]
Target params: [1.1812, 0.2779]
Actual params: [ 0.3685, -2.1923]
-Original Grad: -0.149, -lr * Pred Grad: -0.042, New P: 0.326
-Original Grad: 0.130, -lr * Pred Grad: 0.048, New P: -2.144
iter 0 loss: 0.738
Actual params: [ 0.3264, -2.1444]
-Original Grad: 0.091, -lr * Pred Grad: -0.059, New P: 0.267
-Original Grad: -0.157, -lr * Pred Grad: 0.062, New P: -2.082
iter 1 loss: 0.742
Actual params: [ 0.2673, -2.0821]
-Original Grad: 0.566, -lr * Pred Grad: -0.062, New P: 0.205
-Original Grad: -0.828, -lr * Pred Grad: 0.061, New P: -2.021
iter 2 loss: 0.745
Actual params: [ 0.2054, -2.0209]
-Original Grad: 0.007, -lr * Pred Grad: -0.062, New P: 0.143
-Original Grad: 0.062, -lr * Pred Grad: 0.059, New P: -1.961
iter 3 loss: 0.749
Actual params: [ 0.1432, -1.9615]
-Original Grad: 0.073, -lr * Pred Grad: -0.062, New P: 0.081
-Original Grad: -0.250, -lr * Pred Grad: 0.055, New P: -1.906
iter 4 loss: 0.751
Actual params: [ 0.0809, -1.9064]
-Original Grad: -0.026, -lr * Pred Grad: -0.062, New P: 0.019
-Original Grad: 0.233, -lr * Pred Grad: 0.048, New P: -1.858
iter 5 loss: 0.755
Actual params: [ 0.0186, -1.8582]
-Original Grad: -0.004, -lr * Pred Grad: -0.062, New P: -0.044
-Original Grad: 0.046, -lr * Pred Grad: 0.030, New P: -1.828
iter 6 loss: 0.757
Actual params: [-0.0437, -1.8283]
-Original Grad: 0.000, -lr * Pred Grad: -0.062, New P: -0.106
-Original Grad: 0.052, -lr * Pred Grad: -0.012, New P: -1.841
iter 7 loss: 0.755
Actual params: [-0.1059, -1.8405]
-Original Grad: -0.008, -lr * Pred Grad: -0.062, New P: -0.168
-Original Grad: 0.017, -lr * Pred Grad: -0.048, New P: -1.889
iter 8 loss: 0.755
Actual params: [-0.1682, -1.8889]
-Original Grad: -0.021, -lr * Pred Grad: -0.062, New P: -0.231
-Original Grad: 0.035, -lr * Pred Grad: -0.060, New P: -1.948
iter 9 loss: 0.755
Actual params: [-0.2305, -1.9484]
-Original Grad: -0.098, -lr * Pred Grad: -0.062, New P: -0.293
-Original Grad: -0.157, -lr * Pred Grad: -0.062, New P: -2.010
iter 10 loss: 0.759
Actual params: [-0.2928, -2.0103]
-Original Grad: -0.036, -lr * Pred Grad: -0.062, New P: -0.355
-Original Grad: -0.077, -lr * Pred Grad: -0.062, New P: -2.072
iter 11 loss: 0.750
Actual params: [-0.3551, -2.0725]
-Original Grad: -0.045, -lr * Pred Grad: -0.062, New P: -0.417
-Original Grad: -0.164, -lr * Pred Grad: -0.062, New P: -2.135
iter 12 loss: 0.754
Actual params: [-0.4174, -2.1348]
-Original Grad: -0.027, -lr * Pred Grad: -0.062, New P: -0.480
-Original Grad: -0.142, -lr * Pred Grad: -0.062, New P: -2.197
iter 13 loss: 0.745
Actual params: [-0.4797, -2.1971]
-Original Grad: -0.020, -lr * Pred Grad: -0.062, New P: -0.542
-Original Grad: 0.123, -lr * Pred Grad: -0.062, New P: -2.259
iter 14 loss: 0.741
Actual params: [-0.542 , -2.2593]
-Original Grad: -0.054, -lr * Pred Grad: -0.062, New P: -0.604
-Original Grad: -0.129, -lr * Pred Grad: -0.062, New P: -2.322
iter 15 loss: 0.736
Actual params: [-0.6042, -2.3216]
-Original Grad: -0.272, -lr * Pred Grad: -0.062, New P: -0.667
-Original Grad: -2.135, -lr * Pred Grad: -0.062, New P: -2.384
iter 16 loss: 0.730
Actual params: [-0.6665, -2.3839]
-Original Grad: -0.071, -lr * Pred Grad: -0.062, New P: -0.729
-Original Grad: -0.126, -lr * Pred Grad: -0.062, New P: -2.446
iter 17 loss: 0.717
Actual params: [-0.7288, -2.4462]
-Original Grad: -0.104, -lr * Pred Grad: -0.062, New P: -0.791
-Original Grad: -0.223, -lr * Pred Grad: -0.062, New P: -2.508
iter 18 loss: 0.708
Actual params: [-0.7911, -2.5085]
-Original Grad: -0.075, -lr * Pred Grad: -0.062, New P: -0.853
-Original Grad: -0.105, -lr * Pred Grad: -0.062, New P: -2.571
iter 19 loss: 0.695
Actual params: [-0.8534, -2.5708]
-Original Grad: -0.075, -lr * Pred Grad: -0.062, New P: -0.916
-Original Grad: -0.156, -lr * Pred Grad: -0.062, New P: -2.633
iter 20 loss: 0.683
Actual params: [-0.9157, -2.6331]
Target params: [1.1812, 0.2779]
Actual params: [-0.1438, -1.3168]
-Original Grad: 0.142, -lr * Pred Grad: 0.048, New P: -0.096
-Original Grad: 0.022, -lr * Pred Grad: 0.028, New P: -1.288
iter 0 loss: 0.709
Actual params: [-0.0955, -1.2885]
-Original Grad: 0.265, -lr * Pred Grad: 0.063, New P: -0.033
-Original Grad: 0.043, -lr * Pred Grad: 0.036, New P: -1.253
iter 1 loss: 0.698
Actual params: [-0.0329, -1.2526]
-Original Grad: 0.233, -lr * Pred Grad: 0.065, New P: 0.032
-Original Grad: 0.031, -lr * Pred Grad: -0.000, New P: -1.253
iter 2 loss: 0.679
Actual params: [ 0.032 , -1.2526]
-Original Grad: 0.193, -lr * Pred Grad: 0.065, New P: 0.097
-Original Grad: 0.033, -lr * Pred Grad: -0.046, New P: -1.299
iter 3 loss: 0.681
Actual params: [ 0.0971, -1.2986]
-Original Grad: 0.196, -lr * Pred Grad: 0.065, New P: 0.162
-Original Grad: 0.046, -lr * Pred Grad: -0.060, New P: -1.358
iter 4 loss: 0.669
Actual params: [ 0.1624, -1.3583]
-Original Grad: 0.181, -lr * Pred Grad: 0.065, New P: 0.228
-Original Grad: 0.057, -lr * Pred Grad: -0.062, New P: -1.420
iter 5 loss: 0.660
Actual params: [ 0.2276, -1.4202]
-Original Grad: 0.203, -lr * Pred Grad: 0.065, New P: 0.293
-Original Grad: 0.069, -lr * Pred Grad: -0.062, New P: -1.482
iter 6 loss: 0.652
Actual params: [ 0.2928, -1.4824]
-Original Grad: 0.195, -lr * Pred Grad: 0.065, New P: 0.358
-Original Grad: 0.065, -lr * Pred Grad: -0.062, New P: -1.545
iter 7 loss: 0.643
Actual params: [ 0.3581, -1.5447]
-Original Grad: 0.206, -lr * Pred Grad: 0.065, New P: 0.423
-Original Grad: 0.068, -lr * Pred Grad: -0.062, New P: -1.607
iter 8 loss: 0.635
Actual params: [ 0.4233, -1.607 ]
-Original Grad: 0.103, -lr * Pred Grad: 0.065, New P: 0.489
-Original Grad: 0.012, -lr * Pred Grad: -0.062, New P: -1.669
iter 9 loss: 0.626
Actual params: [ 0.4885, -1.6693]
-Original Grad: 0.184, -lr * Pred Grad: 0.065, New P: 0.554
-Original Grad: 0.062, -lr * Pred Grad: -0.062, New P: -1.732
iter 10 loss: 0.618
Actual params: [ 0.5538, -1.7316]
-Original Grad: 0.189, -lr * Pred Grad: 0.065, New P: 0.619
-Original Grad: 0.083, -lr * Pred Grad: -0.062, New P: -1.794
iter 11 loss: 0.611
Actual params: [ 0.619 , -1.7939]
-Original Grad: 0.242, -lr * Pred Grad: 0.065, New P: 0.684
-Original Grad: 0.089, -lr * Pred Grad: -0.062, New P: -1.856
iter 12 loss: 0.602
Actual params: [ 0.6842, -1.8561]
-Original Grad: 0.200, -lr * Pred Grad: 0.065, New P: 0.749
-Original Grad: 0.076, -lr * Pred Grad: -0.062, New P: -1.918
iter 13 loss: 0.593
Actual params: [ 0.7495, -1.9184]
-Original Grad: 0.279, -lr * Pred Grad: 0.065, New P: 0.815
-Original Grad: 0.085, -lr * Pred Grad: -0.062, New P: -1.981
iter 14 loss: 0.583
Actual params: [ 0.8147, -1.9807]
-Original Grad: 0.240, -lr * Pred Grad: 0.065, New P: 0.880
-Original Grad: 0.091, -lr * Pred Grad: -0.062, New P: -2.043
iter 15 loss: 0.572
Actual params: [ 0.8799, -2.043 ]
-Original Grad: 0.397, -lr * Pred Grad: 0.065, New P: 0.945
-Original Grad: 0.148, -lr * Pred Grad: -0.062, New P: -2.105
iter 16 loss: 0.561
Actual params: [ 0.9452, -2.1053]
-Original Grad: 0.163, -lr * Pred Grad: 0.065, New P: 1.010
-Original Grad: 0.095, -lr * Pred Grad: -0.062, New P: -2.168
iter 17 loss: 0.554
Actual params: [ 1.0104, -2.1676]
-Original Grad: 0.103, -lr * Pred Grad: 0.065, New P: 1.076
-Original Grad: 0.065, -lr * Pred Grad: -0.062, New P: -2.230
iter 18 loss: 0.549
Actual params: [ 1.0756, -2.2299]
-Original Grad: 0.041, -lr * Pred Grad: 0.065, New P: 1.141
-Original Grad: 0.059, -lr * Pred Grad: -0.062, New P: -2.292
iter 19 loss: 0.548
Actual params: [ 1.1409, -2.2922]
-Original Grad: 0.024, -lr * Pred Grad: 0.065, New P: 1.206
-Original Grad: 0.010, -lr * Pred Grad: -0.062, New P: -2.354
iter 20 loss: 0.547
Actual params: [ 1.2061, -2.3544]
Target params: [1.1812, 0.2779]
Actual params: [-1.23  , -0.0332]
-Original Grad: -0.029, -lr * Pred Grad: -0.003, New P: -1.233
-Original Grad: 0.006, -lr * Pred Grad: 0.020, New P: -0.013
iter 0 loss: 0.523
Actual params: [-1.2327, -0.013 ]
-Original Grad: -0.034, -lr * Pred Grad: -0.048, New P: -1.281
-Original Grad: 0.007, -lr * Pred Grad: -0.016, New P: -0.029
iter 1 loss: 0.523
Actual params: [-1.2812, -0.0287]
-Original Grad: -0.028, -lr * Pred Grad: -0.060, New P: -1.341
-Original Grad: 0.004, -lr * Pred Grad: -0.048, New P: -0.077
iter 2 loss: 0.521
Actual params: [-1.3414, -0.0772]
-Original Grad: 0.018, -lr * Pred Grad: -0.062, New P: -1.403
-Original Grad: -0.006, -lr * Pred Grad: -0.060, New P: -0.137
iter 3 loss: 0.521
Actual params: [-1.4034, -0.1369]
-Original Grad: 0.045, -lr * Pred Grad: -0.062, New P: -1.466
-Original Grad: -0.014, -lr * Pred Grad: -0.062, New P: -0.199
iter 4 loss: 0.523
Actual params: [-1.4657, -0.1988]
-Original Grad: 0.040, -lr * Pred Grad: -0.062, New P: -1.528
-Original Grad: -0.016, -lr * Pred Grad: -0.062, New P: -0.261
iter 5 loss: 0.524
Actual params: [-1.5279, -0.261 ]
-Original Grad: 0.031, -lr * Pred Grad: -0.062, New P: -1.590
-Original Grad: -0.008, -lr * Pred Grad: -0.062, New P: -0.323
iter 6 loss: 0.526
Actual params: [-1.5902, -0.3233]
-Original Grad: 0.043, -lr * Pred Grad: -0.062, New P: -1.653
-Original Grad: -0.006, -lr * Pred Grad: -0.062, New P: -0.386
iter 7 loss: 0.522
Actual params: [-1.6525, -0.3856]
-Original Grad: 0.048, -lr * Pred Grad: -0.062, New P: -1.715
-Original Grad: -0.006, -lr * Pred Grad: -0.062, New P: -0.448
iter 8 loss: 0.523
Actual params: [-1.7148, -0.4479]
-Original Grad: 0.036, -lr * Pred Grad: -0.062, New P: -1.777
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -0.510
iter 9 loss: 0.525
Actual params: [-1.7771, -0.5102]
-Original Grad: 0.015, -lr * Pred Grad: -0.062, New P: -1.839
-Original Grad: -0.000, -lr * Pred Grad: -0.062, New P: -0.572
iter 10 loss: 0.527
Actual params: [-1.8394, -0.5725]
-Original Grad: 0.000, -lr * Pred Grad: -0.062, New P: -1.902
-Original Grad: 0.001, -lr * Pred Grad: -0.062, New P: -0.635
iter 11 loss: 0.527
Actual params: [-1.9017, -0.6347]
-Original Grad: -0.023, -lr * Pred Grad: -0.062, New P: -1.964
-Original Grad: 0.003, -lr * Pred Grad: -0.062, New P: -0.697
iter 12 loss: 0.527
Actual params: [-1.964, -0.697]
-Original Grad: -0.001, -lr * Pred Grad: -0.062, New P: -2.026
-Original Grad: 0.003, -lr * Pred Grad: -0.062, New P: -0.759
iter 13 loss: 0.527
Actual params: [-2.0262, -0.7593]
-Original Grad: 0.106, -lr * Pred Grad: -0.062, New P: -2.089
-Original Grad: 0.019, -lr * Pred Grad: -0.062, New P: -0.822
iter 14 loss: 0.531
Actual params: [-2.0885, -0.8216]
-Original Grad: -0.193, -lr * Pred Grad: -0.062, New P: -2.151
-Original Grad: -0.053, -lr * Pred Grad: -0.062, New P: -0.884
iter 15 loss: 0.532
Actual params: [-2.1508, -0.8839]
-Original Grad: -0.685, -lr * Pred Grad: -0.062, New P: -2.213
-Original Grad: -0.135, -lr * Pred Grad: -0.062, New P: -0.946
iter 16 loss: 0.533
Actual params: [-2.2131, -0.9462]
-Original Grad: 0.028, -lr * Pred Grad: -0.062, New P: -2.275
-Original Grad: 0.004, -lr * Pred Grad: -0.062, New P: -1.008
iter 17 loss: 0.538
Actual params: [-2.2754, -1.0085]
-Original Grad: 0.010, -lr * Pred Grad: -0.062, New P: -2.338
-Original Grad: 0.006, -lr * Pred Grad: -0.062, New P: -1.071
iter 18 loss: 0.540
Actual params: [-2.3377, -1.0708]
-Original Grad: 0.036, -lr * Pred Grad: -0.062, New P: -2.400
-Original Grad: 0.007, -lr * Pred Grad: -0.062, New P: -1.133
iter 19 loss: 0.541
Actual params: [-2.4  , -1.133]
-Original Grad: 0.037, -lr * Pred Grad: -0.062, New P: -2.462
-Original Grad: 0.009, -lr * Pred Grad: -0.062, New P: -1.195
iter 20 loss: 0.545
Actual params: [-2.4623, -1.1953]
Target params: [1.1812, 0.2779]
Actual params: [ 1.0788, -1.6003]
-Original Grad: 0.073, -lr * Pred Grad: 0.043, New P: 1.122
-Original Grad: 0.236, -lr * Pred Grad: 0.050, New P: -1.551
iter 0 loss: 0.382
Actual params: [ 1.1215, -1.5506]
-Original Grad: 0.083, -lr * Pred Grad: 0.062, New P: 1.183
-Original Grad: 0.254, -lr * Pred Grad: 0.063, New P: -1.488
iter 1 loss: 0.369
Actual params: [ 1.183 , -1.4877]
-Original Grad: 0.055, -lr * Pred Grad: 0.064, New P: 1.247
-Original Grad: 0.161, -lr * Pred Grad: 0.065, New P: -1.423
iter 2 loss: 0.354
Actual params: [ 1.2472, -1.4228]
-Original Grad: 0.040, -lr * Pred Grad: 0.064, New P: 1.311
-Original Grad: 0.142, -lr * Pred Grad: 0.065, New P: -1.358
iter 3 loss: 0.340
Actual params: [ 1.3113, -1.3576]
-Original Grad: 0.019, -lr * Pred Grad: 0.064, New P: 1.375
-Original Grad: 0.086, -lr * Pred Grad: 0.065, New P: -1.292
iter 4 loss: 0.330
Actual params: [ 1.375 , -1.2924]
-Original Grad: 1.350, -lr * Pred Grad: 0.064, New P: 1.439
-Original Grad: 3.650, -lr * Pred Grad: 0.065, New P: -1.227
iter 5 loss: 0.318
Actual params: [ 1.4385, -1.2272]
-Original Grad: 0.013, -lr * Pred Grad: 0.063, New P: 1.501
-Original Grad: 0.070, -lr * Pred Grad: 0.065, New P: -1.162
iter 6 loss: 0.311
Actual params: [ 1.501 , -1.1619]
-Original Grad: 0.005, -lr * Pred Grad: 0.062, New P: 1.563
-Original Grad: 0.064, -lr * Pred Grad: 0.065, New P: -1.097
iter 7 loss: 0.306
Actual params: [ 1.5627, -1.0967]
-Original Grad: 0.008, -lr * Pred Grad: 0.060, New P: 1.622
-Original Grad: 0.070, -lr * Pred Grad: 0.065, New P: -1.031
iter 8 loss: 0.291
Actual params: [ 1.6223, -1.0315]
-Original Grad: 0.004, -lr * Pred Grad: 0.057, New P: 1.679
-Original Grad: 0.068, -lr * Pred Grad: 0.065, New P: -0.966
iter 9 loss: 0.286
Actual params: [ 1.6789, -0.9662]
-Original Grad: 0.008, -lr * Pred Grad: 0.050, New P: 1.729
-Original Grad: 0.068, -lr * Pred Grad: 0.065, New P: -0.901
iter 10 loss: 0.281
Actual params: [ 1.7293, -0.901 ]
-Original Grad: 0.010, -lr * Pred Grad: 0.038, New P: 1.768
-Original Grad: 0.063, -lr * Pred Grad: 0.065, New P: -0.836
iter 11 loss: 0.277
Actual params: [ 1.7677, -0.8358]
-Original Grad: 0.006, -lr * Pred Grad: 0.008, New P: 1.775
-Original Grad: 0.057, -lr * Pred Grad: 0.065, New P: -0.771
iter 12 loss: 0.272
Actual params: [ 1.7753, -0.7705]
-Original Grad: 0.008, -lr * Pred Grad: -0.033, New P: 1.742
-Original Grad: 0.066, -lr * Pred Grad: 0.065, New P: -0.705
iter 13 loss: 0.268
Actual params: [ 1.7419, -0.7053]
-Original Grad: 0.005, -lr * Pred Grad: -0.048, New P: 1.694
-Original Grad: 0.052, -lr * Pred Grad: 0.065, New P: -0.640
iter 14 loss: 0.265
Actual params: [ 1.6943, -0.6401]
-Original Grad: 0.002, -lr * Pred Grad: -0.050, New P: 1.644
-Original Grad: 0.051, -lr * Pred Grad: 0.065, New P: -0.575
iter 15 loss: 0.262
Actual params: [ 1.6441, -0.5748]
-Original Grad: -0.009, -lr * Pred Grad: -0.051, New P: 1.593
-Original Grad: 0.058, -lr * Pred Grad: 0.065, New P: -0.510
iter 16 loss: 0.258
Actual params: [ 1.5929, -0.5096]
-Original Grad: -0.008, -lr * Pred Grad: -0.053, New P: 1.540
-Original Grad: 0.061, -lr * Pred Grad: 0.065, New P: -0.444
iter 17 loss: 0.254
Actual params: [ 1.5397, -0.4444]
-Original Grad: -0.009, -lr * Pred Grad: -0.055, New P: 1.485
-Original Grad: 0.045, -lr * Pred Grad: 0.065, New P: -0.379
iter 18 loss: 0.250
Actual params: [ 1.4846, -0.3791]
-Original Grad: -0.004, -lr * Pred Grad: -0.057, New P: 1.427
-Original Grad: 0.024, -lr * Pred Grad: 0.065, New P: -0.314
iter 19 loss: 0.247
Actual params: [ 1.4274, -0.3139]
-Original Grad: -0.002, -lr * Pred Grad: -0.059, New P: 1.368
-Original Grad: 0.021, -lr * Pred Grad: 0.065, New P: -0.249
iter 20 loss: 0.245
Actual params: [ 1.3683, -0.2487]
Target params: [1.1812, 0.2779]
Actual params: [-0.7653,  1.3313]
-Original Grad: 0.008, -lr * Pred Grad: 0.021, New P: -0.744
-Original Grad: 0.023, -lr * Pred Grad: 0.028, New P: 1.360
iter 0 loss: 0.396
Actual params: [-0.7443,  1.3597]
-Original Grad: 0.010, -lr * Pred Grad: -0.013, New P: -0.757
-Original Grad: 0.029, -lr * Pred Grad: 0.029, New P: 1.389
iter 1 loss: 0.395
Actual params: [-0.7571,  1.3891]
-Original Grad: 0.010, -lr * Pred Grad: -0.047, New P: -0.804
-Original Grad: 0.035, -lr * Pred Grad: -0.013, New P: 1.376
iter 2 loss: 0.394
Actual params: [-0.804 ,  1.3759]
-Original Grad: 0.009, -lr * Pred Grad: -0.059, New P: -0.863
-Original Grad: 0.031, -lr * Pred Grad: -0.051, New P: 1.325
iter 3 loss: 0.395
Actual params: [-0.8632,  1.3253]
-Original Grad: -0.016, -lr * Pred Grad: -0.062, New P: -0.925
-Original Grad: 0.013, -lr * Pred Grad: -0.060, New P: 1.265
iter 4 loss: 0.396
Actual params: [-0.9251,  1.2649]
-Original Grad: -0.015, -lr * Pred Grad: -0.062, New P: -0.987
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: 1.203
iter 5 loss: 0.396
Actual params: [-0.9873,  1.2029]
-Original Grad: 0.008, -lr * Pred Grad: -0.062, New P: -1.050
-Original Grad: 0.003, -lr * Pred Grad: -0.062, New P: 1.141
iter 6 loss: 0.395
Actual params: [-1.0496,  1.1406]
-Original Grad: 0.017, -lr * Pred Grad: -0.062, New P: -1.112
-Original Grad: 0.009, -lr * Pred Grad: -0.062, New P: 1.078
iter 7 loss: 0.397
Actual params: [-1.1119,  1.0783]
-Original Grad: 0.012, -lr * Pred Grad: -0.062, New P: -1.174
-Original Grad: 0.007, -lr * Pred Grad: -0.062, New P: 1.016
iter 8 loss: 0.398
Actual params: [-1.1742,  1.016 ]
-Original Grad: 0.008, -lr * Pred Grad: -0.062, New P: -1.236
-Original Grad: 0.004, -lr * Pred Grad: -0.062, New P: 0.954
iter 9 loss: 0.399
Actual params: [-1.2364,  0.9537]
-Original Grad: 0.004, -lr * Pred Grad: -0.062, New P: -1.299
-Original Grad: 0.010, -lr * Pred Grad: -0.062, New P: 0.891
iter 10 loss: 0.400
Actual params: [-1.2987,  0.8915]
-Original Grad: 0.009, -lr * Pred Grad: -0.062, New P: -1.361
-Original Grad: 0.010, -lr * Pred Grad: -0.062, New P: 0.829
iter 11 loss: 0.402
Actual params: [-1.361 ,  0.8292]
-Original Grad: 0.003, -lr * Pred Grad: -0.062, New P: -1.423
-Original Grad: 0.019, -lr * Pred Grad: -0.062, New P: 0.767
iter 12 loss: 0.403
Actual params: [-1.4233,  0.7669]
-Original Grad: 0.004, -lr * Pred Grad: -0.062, New P: -1.486
-Original Grad: 0.020, -lr * Pred Grad: -0.062, New P: 0.705
iter 13 loss: 0.404
Actual params: [-1.4856,  0.7046]
-Original Grad: 0.004, -lr * Pred Grad: -0.062, New P: -1.548
-Original Grad: 0.024, -lr * Pred Grad: -0.062, New P: 0.642
iter 14 loss: 0.406
Actual params: [-1.5479,  0.6423]
-Original Grad: 0.009, -lr * Pred Grad: -0.062, New P: -1.610
-Original Grad: 0.026, -lr * Pred Grad: -0.062, New P: 0.580
iter 15 loss: 0.408
Actual params: [-1.6102,  0.58  ]
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: -1.672
-Original Grad: 0.021, -lr * Pred Grad: -0.062, New P: 0.518
iter 16 loss: 0.410
Actual params: [-1.6725,  0.5177]
-Original Grad: 0.007, -lr * Pred Grad: -0.062, New P: -1.735
-Original Grad: 0.012, -lr * Pred Grad: -0.062, New P: 0.455
iter 17 loss: 0.411
Actual params: [-1.7348,  0.4554]
-Original Grad: 0.012, -lr * Pred Grad: -0.062, New P: -1.797
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: 0.393
iter 18 loss: 0.412
Actual params: [-1.797 ,  0.3931]
-Original Grad: 0.019, -lr * Pred Grad: -0.062, New P: -1.859
-Original Grad: -0.001, -lr * Pred Grad: -0.062, New P: 0.331
iter 19 loss: 0.413
Actual params: [-1.8593,  0.3309]
-Original Grad: 0.018, -lr * Pred Grad: -0.062, New P: -1.922
-Original Grad: -0.006, -lr * Pred Grad: -0.062, New P: 0.269
iter 20 loss: 0.414
Actual params: [-1.9216,  0.2686]
Target params: [1.1812, 0.2779]
Actual params: [ 0.3094, -0.0048]
-Original Grad: 0.053, -lr * Pred Grad: 0.039, New P: 0.348
-Original Grad: 0.029, -lr * Pred Grad: 0.031, New P: 0.027
iter 0 loss: 0.386
Actual params: [0.348 , 0.0265]
-Original Grad: -0.060, -lr * Pred Grad: 0.048, New P: 0.396
-Original Grad: 0.010, -lr * Pred Grad: 0.035, New P: 0.062
iter 1 loss: 0.387
Actual params: [0.3956, 0.0617]
-Original Grad: -0.095, -lr * Pred Grad: 0.034, New P: 0.430
-Original Grad: 0.024, -lr * Pred Grad: -0.000, New P: 0.061
iter 2 loss: 0.390
Actual params: [0.4301, 0.0614]
-Original Grad: 0.133, -lr * Pred Grad: -0.008, New P: 0.422
-Original Grad: 0.059, -lr * Pred Grad: -0.046, New P: 0.016
iter 3 loss: 0.391
Actual params: [0.4216, 0.0155]
-Original Grad: -0.043, -lr * Pred Grad: -0.050, New P: 0.371
-Original Grad: 0.022, -lr * Pred Grad: -0.060, New P: -0.044
iter 4 loss: 0.393
Actual params: [ 0.3712, -0.0441]
-Original Grad: -0.088, -lr * Pred Grad: -0.061, New P: 0.311
-Original Grad: -0.011, -lr * Pred Grad: -0.062, New P: -0.106
iter 5 loss: 0.388
Actual params: [ 0.3106, -0.106 ]
-Original Grad: 0.099, -lr * Pred Grad: -0.062, New P: 0.249
-Original Grad: 0.065, -lr * Pred Grad: -0.062, New P: -0.168
iter 6 loss: 0.399
Actual params: [ 0.2486, -0.1683]
-Original Grad: -0.021, -lr * Pred Grad: -0.062, New P: 0.186
-Original Grad: -0.012, -lr * Pred Grad: -0.062, New P: -0.231
iter 7 loss: 0.400
Actual params: [ 0.1863, -0.2305]
-Original Grad: -0.012, -lr * Pred Grad: -0.062, New P: 0.124
-Original Grad: -0.015, -lr * Pred Grad: -0.062, New P: -0.293
iter 8 loss: 0.397
Actual params: [ 0.1241, -0.2928]
-Original Grad: 0.009, -lr * Pred Grad: -0.062, New P: 0.062
-Original Grad: -0.013, -lr * Pred Grad: -0.062, New P: -0.355
iter 9 loss: 0.396
Actual params: [ 0.0618, -0.3551]
-Original Grad: 0.030, -lr * Pred Grad: -0.062, New P: -0.001
-Original Grad: -0.015, -lr * Pred Grad: -0.062, New P: -0.417
iter 10 loss: 0.397
Actual params: [-0.0005, -0.4174]
-Original Grad: 0.028, -lr * Pred Grad: -0.062, New P: -0.063
-Original Grad: -0.010, -lr * Pred Grad: -0.062, New P: -0.480
iter 11 loss: 0.398
Actual params: [-0.0628, -0.4797]
-Original Grad: 0.018, -lr * Pred Grad: -0.062, New P: -0.125
-Original Grad: -0.004, -lr * Pred Grad: -0.062, New P: -0.542
iter 12 loss: 0.399
Actual params: [-0.1251, -0.542 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.062, New P: -0.187
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -0.604
iter 13 loss: 0.400
Actual params: [-0.1874, -0.6043]
-Original Grad: 0.006, -lr * Pred Grad: -0.062, New P: -0.250
-Original Grad: 0.003, -lr * Pred Grad: -0.062, New P: -0.667
iter 14 loss: 0.400
Actual params: [-0.2497, -0.6666]
-Original Grad: 0.001, -lr * Pred Grad: -0.062, New P: -0.312
-Original Grad: 0.006, -lr * Pred Grad: -0.062, New P: -0.729
iter 15 loss: 0.400
Actual params: [-0.312 , -0.7288]
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -0.374
-Original Grad: 0.007, -lr * Pred Grad: -0.062, New P: -0.791
iter 16 loss: 0.401
Actual params: [-0.3742, -0.7911]
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: -0.437
-Original Grad: 0.006, -lr * Pred Grad: -0.062, New P: -0.853
iter 17 loss: 0.401
Actual params: [-0.4365, -0.8534]
-Original Grad: -0.006, -lr * Pred Grad: -0.062, New P: -0.499
-Original Grad: 0.003, -lr * Pred Grad: -0.062, New P: -0.916
iter 18 loss: 0.401
Actual params: [-0.4988, -0.9157]
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -0.561
-Original Grad: -0.000, -lr * Pred Grad: -0.062, New P: -0.978
iter 19 loss: 0.401
Actual params: [-0.5611, -0.978 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.062, New P: -0.623
-Original Grad: -0.001, -lr * Pred Grad: -0.062, New P: -1.040
iter 20 loss: 0.401
Actual params: [-0.6234, -1.0403]
Target params: [1.1812, 0.2779]
Actual params: [1.7812, 1.0158]
-Original Grad: 0.004, -lr * Pred Grad: 0.019, New P: 1.800
-Original Grad: 0.164, -lr * Pred Grad: 0.049, New P: 1.065
iter 0 loss: 0.681
Actual params: [1.8001, 1.0647]
-Original Grad: 0.001, -lr * Pred Grad: -0.020, New P: 1.780
-Original Grad: 0.183, -lr * Pred Grad: 0.063, New P: 1.127
iter 1 loss: 0.672
Actual params: [1.7797, 1.1274]
-Original Grad: -0.004, -lr * Pred Grad: -0.051, New P: 1.729
-Original Grad: 0.277, -lr * Pred Grad: 0.065, New P: 1.192
iter 2 loss: 0.657
Actual params: [1.7288, 1.1923]
-Original Grad: -0.017, -lr * Pred Grad: -0.060, New P: 1.668
-Original Grad: 0.355, -lr * Pred Grad: 0.065, New P: 1.257
iter 3 loss: 0.636
Actual params: [1.6685, 1.2575]
-Original Grad: -0.017, -lr * Pred Grad: -0.062, New P: 1.606
-Original Grad: 0.390, -lr * Pred Grad: 0.065, New P: 1.323
iter 4 loss: 0.611
Actual params: [1.6065, 1.3227]
-Original Grad: -0.021, -lr * Pred Grad: -0.062, New P: 1.544
-Original Grad: 0.452, -lr * Pred Grad: 0.065, New P: 1.388
iter 5 loss: 0.577
Actual params: [1.5442, 1.388 ]
-Original Grad: -0.028, -lr * Pred Grad: -0.062, New P: 1.482
-Original Grad: 0.520, -lr * Pred Grad: 0.065, New P: 1.453
iter 6 loss: 0.538
Actual params: [1.482 , 1.4532]
-Original Grad: -0.037, -lr * Pred Grad: -0.062, New P: 1.420
-Original Grad: 0.579, -lr * Pred Grad: 0.065, New P: 1.518
iter 7 loss: 0.496
Actual params: [1.4197, 1.5184]
-Original Grad: -0.050, -lr * Pred Grad: -0.062, New P: 1.357
-Original Grad: 0.570, -lr * Pred Grad: 0.065, New P: 1.584
iter 8 loss: 0.452
Actual params: [1.3574, 1.5836]
-Original Grad: -0.067, -lr * Pred Grad: -0.062, New P: 1.295
-Original Grad: 0.530, -lr * Pred Grad: 0.065, New P: 1.649
iter 9 loss: 0.406
Actual params: [1.2951, 1.6489]
-Original Grad: -0.089, -lr * Pred Grad: -0.062, New P: 1.233
-Original Grad: 0.369, -lr * Pred Grad: 0.065, New P: 1.714
iter 10 loss: 0.361
Actual params: [1.2328, 1.7141]
-Original Grad: -0.104, -lr * Pred Grad: -0.062, New P: 1.171
-Original Grad: 0.178, -lr * Pred Grad: 0.065, New P: 1.779
iter 11 loss: 0.340
Actual params: [1.1705, 1.7793]
-Original Grad: -0.133, -lr * Pred Grad: -0.062, New P: 1.108
-Original Grad: 0.163, -lr * Pred Grad: 0.065, New P: 1.845
iter 12 loss: 0.323
Actual params: [1.1082, 1.8446]
-Original Grad: -0.114, -lr * Pred Grad: -0.062, New P: 1.046
-Original Grad: 0.136, -lr * Pred Grad: 0.065, New P: 1.910
iter 13 loss: 0.306
Actual params: [1.0459, 1.9098]
-Original Grad: -0.136, -lr * Pred Grad: -0.062, New P: 0.984
-Original Grad: 0.085, -lr * Pred Grad: 0.065, New P: 1.975
iter 14 loss: 0.292
Actual params: [0.9837, 1.975 ]
-Original Grad: -0.133, -lr * Pred Grad: -0.062, New P: 0.921
-Original Grad: 0.057, -lr * Pred Grad: 0.065, New P: 2.040
iter 15 loss: 0.279
Actual params: [0.9214, 2.0403]
-Original Grad: -0.361, -lr * Pred Grad: -0.062, New P: 0.859
-Original Grad: 0.051, -lr * Pred Grad: 0.065, New P: 2.106
iter 16 loss: 0.261
Actual params: [0.8591, 2.1055]
-Original Grad: -0.242, -lr * Pred Grad: -0.062, New P: 0.797
-Original Grad: 0.032, -lr * Pred Grad: 0.065, New P: 2.171
iter 17 loss: 0.234
Actual params: [0.7968, 2.1707]
-Original Grad: 0.260, -lr * Pred Grad: -0.062, New P: 0.735
-Original Grad: 0.037, -lr * Pred Grad: 0.065, New P: 2.236
iter 18 loss: 0.234
Actual params: [0.7345, 2.236 ]
-Original Grad: 0.374, -lr * Pred Grad: -0.062, New P: 0.672
-Original Grad: 0.054, -lr * Pred Grad: 0.065, New P: 2.301
iter 19 loss: 0.251
Actual params: [0.6722, 2.3012]
-Original Grad: 0.279, -lr * Pred Grad: -0.062, New P: 0.610
-Original Grad: 0.052, -lr * Pred Grad: 0.065, New P: 2.366
iter 20 loss: 0.269
Actual params: [0.6099, 2.3664]
