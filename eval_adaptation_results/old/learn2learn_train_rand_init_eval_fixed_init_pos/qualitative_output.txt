Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.412, -lr * Pred Grad: 0.000, New P: 0.595
-Original Grad: 0.105, -lr * Pred Grad: 0.000, New P: 0.595
iter 0 loss: 0.351
Actual params: [0.5945, 0.5945]
-Original Grad: 0.414, -lr * Pred Grad: 0.001, New P: 0.595
-Original Grad: 0.096, -lr * Pred Grad: 0.001, New P: 0.595
iter 1 loss: 0.351
Actual params: [0.5954, 0.5954]
-Original Grad: 0.447, -lr * Pred Grad: 0.001, New P: 0.596
-Original Grad: 0.137, -lr * Pred Grad: 0.001, New P: 0.596
iter 2 loss: 0.350
Actual params: [0.5964, 0.5964]
-Original Grad: 0.405, -lr * Pred Grad: 0.001, New P: 0.598
-Original Grad: 0.090, -lr * Pred Grad: 0.001, New P: 0.598
iter 3 loss: 0.350
Actual params: [0.5976, 0.5977]
-Original Grad: 0.388, -lr * Pred Grad: 0.001, New P: 0.599
-Original Grad: 0.042, -lr * Pred Grad: 0.001, New P: 0.599
iter 4 loss: 0.349
Actual params: [0.5989, 0.599 ]
-Original Grad: 0.420, -lr * Pred Grad: 0.001, New P: 0.600
-Original Grad: 0.079, -lr * Pred Grad: 0.001, New P: 0.600
iter 5 loss: 0.349
Actual params: [0.6002, 0.6003]
-Original Grad: 0.577, -lr * Pred Grad: 0.001, New P: 0.602
-Original Grad: 0.342, -lr * Pred Grad: 0.001, New P: 0.602
iter 6 loss: 0.348
Actual params: [0.6016, 0.6017]
-Original Grad: 0.489, -lr * Pred Grad: 0.001, New P: 0.603
-Original Grad: 0.179, -lr * Pred Grad: 0.001, New P: 0.603
iter 7 loss: 0.347
Actual params: [0.6029, 0.6032]
-Original Grad: 0.494, -lr * Pred Grad: 0.001, New P: 0.604
-Original Grad: 0.181, -lr * Pred Grad: 0.001, New P: 0.605
iter 8 loss: 0.346
Actual params: [0.6043, 0.6046]
-Original Grad: 0.485, -lr * Pred Grad: 0.001, New P: 0.606
-Original Grad: 0.163, -lr * Pred Grad: 0.001, New P: 0.606
iter 9 loss: 0.345
Actual params: [0.6057, 0.606 ]
-Original Grad: 0.510, -lr * Pred Grad: 0.001, New P: 0.607
-Original Grad: 0.204, -lr * Pred Grad: 0.001, New P: 0.607
iter 10 loss: 0.345
Actual params: [0.6071, 0.6074]
-Original Grad: 0.489, -lr * Pred Grad: 0.001, New P: 0.609
-Original Grad: 0.147, -lr * Pred Grad: 0.001, New P: 0.609
iter 11 loss: 0.344
Actual params: [0.6085, 0.6089]
-Original Grad: 0.522, -lr * Pred Grad: 0.001, New P: 0.610
-Original Grad: 0.208, -lr * Pred Grad: 0.001, New P: 0.610
iter 12 loss: 0.343
Actual params: [0.6099, 0.6103]
-Original Grad: 0.484, -lr * Pred Grad: 0.001, New P: 0.611
-Original Grad: 0.136, -lr * Pred Grad: 0.001, New P: 0.612
iter 13 loss: 0.342
Actual params: [0.6113, 0.6118]
-Original Grad: 4.102, -lr * Pred Grad: 0.001, New P: 0.613
-Original Grad: 6.738, -lr * Pred Grad: 0.001, New P: 0.613
iter 14 loss: 0.340
Actual params: [0.6127, 0.6131]
-Original Grad: 0.451, -lr * Pred Grad: 0.001, New P: 0.614
-Original Grad: 0.119, -lr * Pred Grad: 0.001, New P: 0.615
iter 15 loss: 0.335
Actual params: [0.6141, 0.6145]
-Original Grad: 0.434, -lr * Pred Grad: 0.001, New P: 0.615
-Original Grad: 0.074, -lr * Pred Grad: 0.001, New P: 0.616
iter 16 loss: 0.334
Actual params: [0.6154, 0.6159]
-Original Grad: 0.439, -lr * Pred Grad: 0.001, New P: 0.617
-Original Grad: 0.068, -lr * Pred Grad: 0.001, New P: 0.617
iter 17 loss: 0.334
Actual params: [0.6168, 0.6173]
-Original Grad: 0.426, -lr * Pred Grad: 0.001, New P: 0.618
-Original Grad: 0.053, -lr * Pred Grad: 0.001, New P: 0.619
iter 18 loss: 0.333
Actual params: [0.6182, 0.6187]
-Original Grad: 0.427, -lr * Pred Grad: 0.001, New P: 0.620
-Original Grad: 0.037, -lr * Pred Grad: 0.001, New P: 0.620
iter 19 loss: 0.332
Actual params: [0.6196, 0.6202]
-Original Grad: 0.420, -lr * Pred Grad: 0.001, New P: 0.621
-Original Grad: 0.027, -lr * Pred Grad: 0.001, New P: 0.622
iter 20 loss: 0.332
Actual params: [0.621 , 0.6216]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.474, -lr * Pred Grad: 0.000, New P: 0.595
-Original Grad: -0.580, -lr * Pred Grad: 0.001, New P: 0.595
iter 0 loss: 0.187
Actual params: [0.5945, 0.5946]
-Original Grad: 0.475, -lr * Pred Grad: 0.001, New P: 0.595
-Original Grad: -0.578, -lr * Pred Grad: 0.001, New P: 0.595
iter 1 loss: 0.187
Actual params: [0.5953, 0.5954]
-Original Grad: 0.474, -lr * Pred Grad: 0.001, New P: 0.596
-Original Grad: -0.577, -lr * Pred Grad: 0.001, New P: 0.597
iter 2 loss: 0.187
Actual params: [0.5964, 0.5966]
-Original Grad: 0.479, -lr * Pred Grad: 0.001, New P: 0.598
-Original Grad: -0.578, -lr * Pred Grad: 0.001, New P: 0.598
iter 3 loss: 0.187
Actual params: [0.5976, 0.5979]
-Original Grad: 0.483, -lr * Pred Grad: 0.001, New P: 0.599
-Original Grad: -0.587, -lr * Pred Grad: 0.001, New P: 0.599
iter 4 loss: 0.187
Actual params: [0.5988, 0.5992]
-Original Grad: 0.491, -lr * Pred Grad: 0.001, New P: 0.600
-Original Grad: -0.590, -lr * Pred Grad: 0.001, New P: 0.601
iter 5 loss: 0.187
Actual params: [0.6002, 0.6007]
-Original Grad: 0.491, -lr * Pred Grad: 0.001, New P: 0.602
-Original Grad: -0.585, -lr * Pred Grad: 0.001, New P: 0.602
iter 6 loss: 0.188
Actual params: [0.6015, 0.6022]
-Original Grad: 0.488, -lr * Pred Grad: 0.001, New P: 0.603
-Original Grad: -0.600, -lr * Pred Grad: 0.001, New P: 0.604
iter 7 loss: 0.188
Actual params: [0.6029, 0.6037]
-Original Grad: 0.490, -lr * Pred Grad: 0.001, New P: 0.604
-Original Grad: -0.605, -lr * Pred Grad: 0.002, New P: 0.605
iter 8 loss: 0.188
Actual params: [0.6043, 0.6052]
-Original Grad: 0.481, -lr * Pred Grad: 0.001, New P: 0.606
-Original Grad: -0.604, -lr * Pred Grad: 0.002, New P: 0.607
iter 9 loss: 0.188
Actual params: [0.6057, 0.6067]
-Original Grad: 0.479, -lr * Pred Grad: 0.001, New P: 0.607
-Original Grad: -0.576, -lr * Pred Grad: 0.002, New P: 0.608
iter 10 loss: 0.188
Actual params: [0.6071, 0.6082]
-Original Grad: 0.477, -lr * Pred Grad: 0.001, New P: 0.608
-Original Grad: -0.570, -lr * Pred Grad: 0.002, New P: 0.610
iter 11 loss: 0.189
Actual params: [0.6085, 0.6098]
-Original Grad: 0.482, -lr * Pred Grad: 0.001, New P: 0.610
-Original Grad: -0.582, -lr * Pred Grad: 0.002, New P: 0.611
iter 12 loss: 0.189
Actual params: [0.6099, 0.6113]
-Original Grad: 0.485, -lr * Pred Grad: 0.001, New P: 0.611
-Original Grad: -0.572, -lr * Pred Grad: 0.002, New P: 0.613
iter 13 loss: 0.189
Actual params: [0.6113, 0.6128]
-Original Grad: 0.492, -lr * Pred Grad: 0.001, New P: 0.613
-Original Grad: -0.574, -lr * Pred Grad: 0.002, New P: 0.614
iter 14 loss: 0.189
Actual params: [0.6127, 0.6144]
-Original Grad: 0.489, -lr * Pred Grad: 0.001, New P: 0.614
-Original Grad: -0.577, -lr * Pred Grad: 0.002, New P: 0.616
iter 15 loss: 0.189
Actual params: [0.6141, 0.6159]
-Original Grad: 0.490, -lr * Pred Grad: 0.001, New P: 0.616
-Original Grad: -0.576, -lr * Pred Grad: 0.002, New P: 0.617
iter 16 loss: 0.190
Actual params: [0.6155, 0.6174]
-Original Grad: 0.490, -lr * Pred Grad: 0.001, New P: 0.617
-Original Grad: -0.590, -lr * Pred Grad: 0.002, New P: 0.619
iter 17 loss: 0.190
Actual params: [0.617, 0.619]
-Original Grad: 0.488, -lr * Pred Grad: 0.001, New P: 0.618
-Original Grad: -0.576, -lr * Pred Grad: 0.002, New P: 0.620
iter 18 loss: 0.190
Actual params: [0.6184, 0.6205]
-Original Grad: 0.488, -lr * Pred Grad: 0.001, New P: 0.620
-Original Grad: -0.578, -lr * Pred Grad: 0.002, New P: 0.622
iter 19 loss: 0.190
Actual params: [0.6198, 0.622 ]
-Original Grad: 0.489, -lr * Pred Grad: 0.001, New P: 0.621
-Original Grad: -0.593, -lr * Pred Grad: 0.002, New P: 0.624
iter 20 loss: 0.190
Actual params: [0.6212, 0.6236]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: -0.634, -lr * Pred Grad: 0.001, New P: 0.595
-Original Grad: -1.458, -lr * Pred Grad: 0.001, New P: 0.595
iter 0 loss: 0.389
Actual params: [0.5946, 0.5946]
-Original Grad: -0.639, -lr * Pred Grad: 0.001, New P: 0.595
-Original Grad: -1.444, -lr * Pred Grad: 0.001, New P: 0.596
iter 1 loss: 0.390
Actual params: [0.5954, 0.5955]
-Original Grad: -0.652, -lr * Pred Grad: 0.001, New P: 0.597
-Original Grad: -1.534, -lr * Pred Grad: 0.001, New P: 0.597
iter 2 loss: 0.392
Actual params: [0.5966, 0.5967]
-Original Grad: -0.652, -lr * Pred Grad: 0.001, New P: 0.598
-Original Grad: -1.575, -lr * Pred Grad: 0.001, New P: 0.598
iter 3 loss: 0.395
Actual params: [0.5979, 0.5981]
-Original Grad: -0.654, -lr * Pred Grad: 0.001, New P: 0.599
-Original Grad: -1.665, -lr * Pred Grad: 0.001, New P: 0.600
iter 4 loss: 0.398
Actual params: [0.5993, 0.5996]
-Original Grad: -0.695, -lr * Pred Grad: 0.001, New P: 0.601
-Original Grad: -1.919, -lr * Pred Grad: 0.002, New P: 0.601
iter 5 loss: 0.402
Actual params: [0.6007, 0.6012]
-Original Grad: -0.612, -lr * Pred Grad: 0.001, New P: 0.602
-Original Grad: -1.548, -lr * Pred Grad: 0.002, New P: 0.603
iter 6 loss: 0.406
Actual params: [0.6022, 0.6027]
-Original Grad: -0.642, -lr * Pred Grad: 0.002, New P: 0.604
-Original Grad: -1.703, -lr * Pred Grad: 0.002, New P: 0.604
iter 7 loss: 0.409
Actual params: [0.6037, 0.6044]
-Original Grad: -0.668, -lr * Pred Grad: 0.002, New P: 0.605
-Original Grad: -1.819, -lr * Pred Grad: 0.002, New P: 0.606
iter 8 loss: 0.413
Actual params: [0.6052, 0.606 ]
-Original Grad: -0.670, -lr * Pred Grad: 0.002, New P: 0.607
-Original Grad: -1.921, -lr * Pred Grad: 0.002, New P: 0.608
iter 9 loss: 0.417
Actual params: [0.6068, 0.6076]
-Original Grad: -0.673, -lr * Pred Grad: 0.002, New P: 0.608
-Original Grad: -1.994, -lr * Pred Grad: 0.002, New P: 0.609
iter 10 loss: 0.422
Actual params: [0.6083, 0.6093]
-Original Grad: -0.678, -lr * Pred Grad: 0.002, New P: 0.610
-Original Grad: -1.938, -lr * Pred Grad: 0.002, New P: 0.611
iter 11 loss: 0.426
Actual params: [0.6098, 0.611 ]
-Original Grad: -0.512, -lr * Pred Grad: 0.002, New P: 0.611
-Original Grad: -1.206, -lr * Pred Grad: 0.002, New P: 0.613
iter 12 loss: 0.431
Actual params: [0.6114, 0.6126]
-Original Grad: -0.677, -lr * Pred Grad: 0.002, New P: 0.613
-Original Grad: -2.020, -lr * Pred Grad: 0.002, New P: 0.614
iter 13 loss: 0.435
Actual params: [0.6129, 0.6143]
-Original Grad: -0.674, -lr * Pred Grad: 0.002, New P: 0.614
-Original Grad: -2.089, -lr * Pred Grad: 0.002, New P: 0.616
iter 14 loss: 0.439
Actual params: [0.6145, 0.6159]
-Original Grad: -0.688, -lr * Pred Grad: 0.002, New P: 0.616
-Original Grad: -2.108, -lr * Pred Grad: 0.002, New P: 0.618
iter 15 loss: 0.444
Actual params: [0.616 , 0.6176]
-Original Grad: -0.690, -lr * Pred Grad: 0.002, New P: 0.618
-Original Grad: -2.138, -lr * Pred Grad: 0.002, New P: 0.619
iter 16 loss: 0.449
Actual params: [0.6175, 0.6193]
-Original Grad: -0.715, -lr * Pred Grad: 0.002, New P: 0.619
-Original Grad: -2.251, -lr * Pred Grad: 0.002, New P: 0.621
iter 17 loss: 0.453
Actual params: [0.6191, 0.621 ]
-Original Grad: -0.749, -lr * Pred Grad: 0.002, New P: 0.621
-Original Grad: -2.391, -lr * Pred Grad: 0.002, New P: 0.623
iter 18 loss: 0.458
Actual params: [0.6206, 0.6227]
-Original Grad: -0.753, -lr * Pred Grad: 0.002, New P: 0.622
-Original Grad: -2.372, -lr * Pred Grad: 0.002, New P: 0.624
iter 19 loss: 0.464
Actual params: [0.6222, 0.6244]
-Original Grad: -0.733, -lr * Pred Grad: 0.002, New P: 0.624
-Original Grad: -2.322, -lr * Pred Grad: 0.002, New P: 0.626
iter 20 loss: 0.469
Actual params: [0.6237, 0.6261]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.185, -lr * Pred Grad: 0.000, New P: 0.595
-Original Grad: -0.062, -lr * Pred Grad: 0.000, New P: 0.595
iter 0 loss: 0.726
Actual params: [0.5945, 0.5946]
-Original Grad: 0.185, -lr * Pred Grad: 0.001, New P: 0.595
-Original Grad: -0.062, -lr * Pred Grad: 0.001, New P: 0.595
iter 1 loss: 0.726
Actual params: [0.5954, 0.5954]
-Original Grad: 0.187, -lr * Pred Grad: 0.001, New P: 0.596
-Original Grad: -0.051, -lr * Pred Grad: 0.001, New P: 0.596
iter 2 loss: 0.726
Actual params: [0.5964, 0.5965]
-Original Grad: 0.190, -lr * Pred Grad: 0.001, New P: 0.598
-Original Grad: -0.042, -lr * Pred Grad: 0.001, New P: 0.598
iter 3 loss: 0.726
Actual params: [0.5976, 0.5977]
-Original Grad: 0.191, -lr * Pred Grad: 0.001, New P: 0.599
-Original Grad: -0.039, -lr * Pred Grad: 0.001, New P: 0.599
iter 4 loss: 0.726
Actual params: [0.5989, 0.599 ]
-Original Grad: 0.193, -lr * Pred Grad: 0.001, New P: 0.600
-Original Grad: -0.038, -lr * Pred Grad: 0.001, New P: 0.600
iter 5 loss: 0.726
Actual params: [0.6003, 0.6004]
-Original Grad: 0.193, -lr * Pred Grad: 0.001, New P: 0.602
-Original Grad: -0.037, -lr * Pred Grad: 0.001, New P: 0.602
iter 6 loss: 0.725
Actual params: [0.6017, 0.6018]
-Original Grad: 0.190, -lr * Pred Grad: 0.001, New P: 0.603
-Original Grad: -0.036, -lr * Pred Grad: 0.001, New P: 0.603
iter 7 loss: 0.725
Actual params: [0.6031, 0.6033]
-Original Grad: 0.190, -lr * Pred Grad: 0.001, New P: 0.605
-Original Grad: -0.031, -lr * Pred Grad: 0.001, New P: 0.605
iter 8 loss: 0.725
Actual params: [0.6045, 0.6047]
-Original Grad: 0.192, -lr * Pred Grad: 0.001, New P: 0.606
-Original Grad: -0.028, -lr * Pred Grad: 0.001, New P: 0.606
iter 9 loss: 0.725
Actual params: [0.606 , 0.6062]
-Original Grad: 0.191, -lr * Pred Grad: 0.001, New P: 0.607
-Original Grad: -0.026, -lr * Pred Grad: 0.001, New P: 0.608
iter 10 loss: 0.724
Actual params: [0.6074, 0.6076]
-Original Grad: 0.195, -lr * Pred Grad: 0.001, New P: 0.609
-Original Grad: -0.021, -lr * Pred Grad: 0.001, New P: 0.609
iter 11 loss: 0.724
Actual params: [0.6088, 0.6091]
-Original Grad: 0.201, -lr * Pred Grad: 0.001, New P: 0.610
-Original Grad: -0.018, -lr * Pred Grad: 0.001, New P: 0.611
iter 12 loss: 0.724
Actual params: [0.6103, 0.6106]
-Original Grad: 0.206, -lr * Pred Grad: 0.001, New P: 0.612
-Original Grad: -0.022, -lr * Pred Grad: 0.001, New P: 0.612
iter 13 loss: 0.724
Actual params: [0.6117, 0.612 ]
-Original Grad: 0.209, -lr * Pred Grad: 0.001, New P: 0.613
-Original Grad: -0.015, -lr * Pred Grad: 0.001, New P: 0.614
iter 14 loss: 0.723
Actual params: [0.6132, 0.6135]
-Original Grad: 0.217, -lr * Pred Grad: 0.001, New P: 0.615
-Original Grad: -0.010, -lr * Pred Grad: 0.001, New P: 0.615
iter 15 loss: 0.723
Actual params: [0.6146, 0.615 ]
-Original Grad: 0.216, -lr * Pred Grad: 0.001, New P: 0.616
-Original Grad: -0.010, -lr * Pred Grad: 0.001, New P: 0.616
iter 16 loss: 0.723
Actual params: [0.616 , 0.6164]
-Original Grad: 0.222, -lr * Pred Grad: 0.001, New P: 0.617
-Original Grad: -0.004, -lr * Pred Grad: 0.001, New P: 0.618
iter 17 loss: 0.722
Actual params: [0.6175, 0.6179]
-Original Grad: 0.221, -lr * Pred Grad: 0.001, New P: 0.619
-Original Grad: -0.002, -lr * Pred Grad: 0.001, New P: 0.619
iter 18 loss: 0.722
Actual params: [0.6189, 0.6194]
-Original Grad: 0.215, -lr * Pred Grad: 0.001, New P: 0.620
-Original Grad: 0.006, -lr * Pred Grad: 0.001, New P: 0.621
iter 19 loss: 0.722
Actual params: [0.6204, 0.6209]
-Original Grad: 0.217, -lr * Pred Grad: 0.001, New P: 0.622
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.622
iter 20 loss: 0.722
Actual params: [0.6218, 0.6223]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.291, -lr * Pred Grad: 0.000, New P: 0.595
-Original Grad: -0.145, -lr * Pred Grad: 0.001, New P: 0.595
iter 0 loss: 0.627
Actual params: [0.5945, 0.5946]
-Original Grad: 0.292, -lr * Pred Grad: 0.001, New P: 0.595
-Original Grad: -0.143, -lr * Pred Grad: 0.001, New P: 0.595
iter 1 loss: 0.627
Actual params: [0.5954, 0.5954]
-Original Grad: 0.293, -lr * Pred Grad: 0.001, New P: 0.596
-Original Grad: -0.140, -lr * Pred Grad: 0.001, New P: 0.596
iter 2 loss: 0.627
Actual params: [0.5964, 0.5965]
-Original Grad: 0.299, -lr * Pred Grad: 0.001, New P: 0.598
-Original Grad: -0.140, -lr * Pred Grad: 0.001, New P: 0.598
iter 3 loss: 0.627
Actual params: [0.5976, 0.5977]
-Original Grad: 0.299, -lr * Pred Grad: 0.001, New P: 0.599
-Original Grad: -0.138, -lr * Pred Grad: 0.001, New P: 0.599
iter 4 loss: 0.627
Actual params: [0.5989, 0.5991]
-Original Grad: 0.302, -lr * Pred Grad: 0.001, New P: 0.600
-Original Grad: -0.135, -lr * Pred Grad: 0.001, New P: 0.600
iter 5 loss: 0.627
Actual params: [0.6003, 0.6005]
-Original Grad: 0.304, -lr * Pred Grad: 0.001, New P: 0.602
-Original Grad: -0.135, -lr * Pred Grad: 0.001, New P: 0.602
iter 6 loss: 0.626
Actual params: [0.6016, 0.6019]
-Original Grad: 0.307, -lr * Pred Grad: 0.001, New P: 0.603
-Original Grad: -0.135, -lr * Pred Grad: 0.001, New P: 0.603
iter 7 loss: 0.626
Actual params: [0.603 , 0.6033]
-Original Grad: 0.306, -lr * Pred Grad: 0.001, New P: 0.604
-Original Grad: -0.134, -lr * Pred Grad: 0.001, New P: 0.605
iter 8 loss: 0.626
Actual params: [0.6044, 0.6048]
-Original Grad: 0.308, -lr * Pred Grad: 0.001, New P: 0.606
-Original Grad: -0.136, -lr * Pred Grad: 0.001, New P: 0.606
iter 9 loss: 0.626
Actual params: [0.6059, 0.6063]
-Original Grad: 0.306, -lr * Pred Grad: 0.001, New P: 0.607
-Original Grad: -0.137, -lr * Pred Grad: 0.001, New P: 0.608
iter 10 loss: 0.625
Actual params: [0.6073, 0.6077]
-Original Grad: 0.306, -lr * Pred Grad: 0.001, New P: 0.609
-Original Grad: -0.136, -lr * Pred Grad: 0.001, New P: 0.609
iter 11 loss: 0.625
Actual params: [0.6087, 0.6092]
-Original Grad: 0.306, -lr * Pred Grad: 0.001, New P: 0.610
-Original Grad: -0.134, -lr * Pred Grad: 0.001, New P: 0.611
iter 12 loss: 0.625
Actual params: [0.6101, 0.6107]
-Original Grad: 0.307, -lr * Pred Grad: 0.001, New P: 0.612
-Original Grad: -0.133, -lr * Pred Grad: 0.001, New P: 0.612
iter 13 loss: 0.625
Actual params: [0.6116, 0.6122]
-Original Grad: 0.312, -lr * Pred Grad: 0.001, New P: 0.613
-Original Grad: -0.133, -lr * Pred Grad: 0.001, New P: 0.614
iter 14 loss: 0.625
Actual params: [0.613 , 0.6137]
-Original Grad: 0.314, -lr * Pred Grad: 0.001, New P: 0.614
-Original Grad: -0.130, -lr * Pred Grad: 0.001, New P: 0.615
iter 15 loss: 0.624
Actual params: [0.6144, 0.6151]
-Original Grad: 0.314, -lr * Pred Grad: 0.001, New P: 0.616
-Original Grad: -0.131, -lr * Pred Grad: 0.001, New P: 0.617
iter 16 loss: 0.624
Actual params: [0.6159, 0.6166]
-Original Grad: 0.316, -lr * Pred Grad: 0.001, New P: 0.617
-Original Grad: -0.130, -lr * Pred Grad: 0.001, New P: 0.618
iter 17 loss: 0.624
Actual params: [0.6173, 0.6181]
-Original Grad: 0.313, -lr * Pred Grad: 0.001, New P: 0.619
-Original Grad: -0.127, -lr * Pred Grad: 0.001, New P: 0.620
iter 18 loss: 0.623
Actual params: [0.6187, 0.6196]
-Original Grad: 0.314, -lr * Pred Grad: 0.001, New P: 0.620
-Original Grad: -0.132, -lr * Pred Grad: 0.001, New P: 0.621
iter 19 loss: 0.623
Actual params: [0.6201, 0.6211]
-Original Grad: 0.314, -lr * Pred Grad: 0.001, New P: 0.622
-Original Grad: -0.131, -lr * Pred Grad: 0.001, New P: 0.623
iter 20 loss: 0.623
Actual params: [0.6216, 0.6226]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.091, -lr * Pred Grad: 0.000, New P: 0.595
-Original Grad: -0.323, -lr * Pred Grad: 0.001, New P: 0.595
iter 0 loss: 0.503
Actual params: [0.5945, 0.5946]
-Original Grad: 0.059, -lr * Pred Grad: 0.001, New P: 0.595
-Original Grad: -0.456, -lr * Pred Grad: 0.001, New P: 0.595
iter 1 loss: 0.503
Actual params: [0.5954, 0.5954]
-Original Grad: 0.122, -lr * Pred Grad: 0.001, New P: 0.596
-Original Grad: -0.192, -lr * Pred Grad: 0.001, New P: 0.597
iter 2 loss: 0.503
Actual params: [0.5964, 0.5965]
-Original Grad: 0.142, -lr * Pred Grad: 0.001, New P: 0.598
-Original Grad: -0.310, -lr * Pred Grad: 0.001, New P: 0.598
iter 3 loss: 0.504
Actual params: [0.5977, 0.5978]
-Original Grad: 0.154, -lr * Pred Grad: 0.001, New P: 0.599
-Original Grad: -0.105, -lr * Pred Grad: 0.001, New P: 0.599
iter 4 loss: 0.503
Actual params: [0.599 , 0.5991]
-Original Grad: 0.172, -lr * Pred Grad: 0.001, New P: 0.600
-Original Grad: -0.043, -lr * Pred Grad: 0.001, New P: 0.601
iter 5 loss: 0.504
Actual params: [0.6003, 0.6005]
-Original Grad: 0.137, -lr * Pred Grad: 0.001, New P: 0.602
-Original Grad: -0.133, -lr * Pred Grad: 0.001, New P: 0.602
iter 6 loss: 0.504
Actual params: [0.6017, 0.602 ]
-Original Grad: 0.118, -lr * Pred Grad: 0.001, New P: 0.603
-Original Grad: -0.210, -lr * Pred Grad: 0.001, New P: 0.603
iter 7 loss: 0.504
Actual params: [0.6032, 0.6034]
-Original Grad: 0.106, -lr * Pred Grad: 0.001, New P: 0.605
-Original Grad: -0.283, -lr * Pred Grad: 0.001, New P: 0.605
iter 8 loss: 0.504
Actual params: [0.6046, 0.6049]
-Original Grad: 0.093, -lr * Pred Grad: 0.001, New P: 0.606
-Original Grad: -0.311, -lr * Pred Grad: 0.001, New P: 0.606
iter 9 loss: 0.504
Actual params: [0.606 , 0.6064]
-Original Grad: 0.113, -lr * Pred Grad: 0.001, New P: 0.607
-Original Grad: -0.228, -lr * Pred Grad: 0.001, New P: 0.608
iter 10 loss: 0.504
Actual params: [0.6075, 0.6079]
-Original Grad: 0.126, -lr * Pred Grad: 0.001, New P: 0.609
-Original Grad: -0.179, -lr * Pred Grad: 0.001, New P: 0.609
iter 11 loss: 0.505
Actual params: [0.6089, 0.6093]
-Original Grad: 0.133, -lr * Pred Grad: 0.001, New P: 0.610
-Original Grad: -0.163, -lr * Pred Grad: 0.001, New P: 0.611
iter 12 loss: 0.505
Actual params: [0.6104, 0.6108]
-Original Grad: 0.135, -lr * Pred Grad: 0.001, New P: 0.612
-Original Grad: -0.149, -lr * Pred Grad: 0.001, New P: 0.612
iter 13 loss: 0.505
Actual params: [0.6118, 0.6123]
-Original Grad: 0.138, -lr * Pred Grad: 0.001, New P: 0.613
-Original Grad: -0.135, -lr * Pred Grad: 0.001, New P: 0.614
iter 14 loss: 0.505
Actual params: [0.6133, 0.6138]
-Original Grad: 0.132, -lr * Pred Grad: 0.001, New P: 0.615
-Original Grad: -0.157, -lr * Pred Grad: 0.001, New P: 0.615
iter 15 loss: 0.505
Actual params: [0.6147, 0.6153]
-Original Grad: 0.124, -lr * Pred Grad: 0.001, New P: 0.616
-Original Grad: -0.163, -lr * Pred Grad: 0.001, New P: 0.617
iter 16 loss: 0.505
Actual params: [0.6162, 0.6168]
-Original Grad: 0.110, -lr * Pred Grad: 0.001, New P: 0.618
-Original Grad: -0.196, -lr * Pred Grad: 0.001, New P: 0.618
iter 17 loss: 0.505
Actual params: [0.6176, 0.6183]
-Original Grad: 0.085, -lr * Pred Grad: 0.001, New P: 0.619
-Original Grad: -0.248, -lr * Pred Grad: 0.001, New P: 0.620
iter 18 loss: 0.505
Actual params: [0.6191, 0.6198]
-Original Grad: 0.138, -lr * Pred Grad: 0.001, New P: 0.621
-Original Grad: -0.118, -lr * Pred Grad: 0.001, New P: 0.621
iter 19 loss: 0.505
Actual params: [0.6205, 0.6212]
-Original Grad: 0.160, -lr * Pred Grad: 0.001, New P: 0.622
-Original Grad: -0.059, -lr * Pred Grad: 0.001, New P: 0.623
iter 20 loss: 0.505
Actual params: [0.622 , 0.6227]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.193, -lr * Pred Grad: 0.000, New P: 0.595
-Original Grad: -0.171, -lr * Pred Grad: 0.001, New P: 0.595
iter 0 loss: 0.286
Actual params: [0.5945, 0.5946]
-Original Grad: 0.195, -lr * Pred Grad: 0.001, New P: 0.595
-Original Grad: -0.170, -lr * Pred Grad: 0.001, New P: 0.595
iter 1 loss: 0.286
Actual params: [0.5954, 0.5954]
-Original Grad: 0.200, -lr * Pred Grad: 0.001, New P: 0.596
-Original Grad: -0.166, -lr * Pred Grad: 0.001, New P: 0.596
iter 2 loss: 0.286
Actual params: [0.5964, 0.5965]
-Original Grad: 0.202, -lr * Pred Grad: 0.001, New P: 0.598
-Original Grad: -0.165, -lr * Pred Grad: 0.001, New P: 0.598
iter 3 loss: 0.286
Actual params: [0.5976, 0.5977]
-Original Grad: 0.208, -lr * Pred Grad: 0.001, New P: 0.599
-Original Grad: -0.159, -lr * Pred Grad: 0.001, New P: 0.599
iter 4 loss: 0.286
Actual params: [0.5989, 0.5991]
-Original Grad: 0.215, -lr * Pred Grad: 0.001, New P: 0.600
-Original Grad: -0.151, -lr * Pred Grad: 0.001, New P: 0.600
iter 5 loss: 0.286
Actual params: [0.6003, 0.6005]
-Original Grad: 0.221, -lr * Pred Grad: 0.001, New P: 0.602
-Original Grad: -0.149, -lr * Pred Grad: 0.001, New P: 0.602
iter 6 loss: 0.286
Actual params: [0.6017, 0.6019]
-Original Grad: 0.224, -lr * Pred Grad: 0.001, New P: 0.603
-Original Grad: -0.146, -lr * Pred Grad: 0.001, New P: 0.603
iter 7 loss: 0.286
Actual params: [0.6031, 0.6034]
-Original Grad: 0.246, -lr * Pred Grad: 0.001, New P: 0.605
-Original Grad: -0.116, -lr * Pred Grad: 0.001, New P: 0.605
iter 8 loss: 0.286
Actual params: [0.6045, 0.6048]
-Original Grad: 0.249, -lr * Pred Grad: 0.001, New P: 0.606
-Original Grad: -0.116, -lr * Pred Grad: 0.001, New P: 0.606
iter 9 loss: 0.285
Actual params: [0.6059, 0.6063]
-Original Grad: 0.251, -lr * Pred Grad: 0.001, New P: 0.607
-Original Grad: -0.116, -lr * Pred Grad: 0.001, New P: 0.608
iter 10 loss: 0.285
Actual params: [0.6074, 0.6078]
-Original Grad: 0.256, -lr * Pred Grad: 0.001, New P: 0.609
-Original Grad: -0.114, -lr * Pred Grad: 0.001, New P: 0.609
iter 11 loss: 0.285
Actual params: [0.6088, 0.6092]
-Original Grad: 0.235, -lr * Pred Grad: 0.001, New P: 0.610
-Original Grad: -0.118, -lr * Pred Grad: 0.001, New P: 0.611
iter 12 loss: 0.285
Actual params: [0.6102, 0.6107]
-Original Grad: 0.235, -lr * Pred Grad: 0.001, New P: 0.612
-Original Grad: -0.124, -lr * Pred Grad: 0.001, New P: 0.612
iter 13 loss: 0.285
Actual params: [0.6117, 0.6122]
-Original Grad: 0.232, -lr * Pred Grad: 0.001, New P: 0.613
-Original Grad: -0.131, -lr * Pred Grad: 0.001, New P: 0.614
iter 14 loss: 0.284
Actual params: [0.6131, 0.6137]
-Original Grad: 0.232, -lr * Pred Grad: 0.001, New P: 0.615
-Original Grad: -0.142, -lr * Pred Grad: 0.001, New P: 0.615
iter 15 loss: 0.284
Actual params: [0.6145, 0.6152]
-Original Grad: 0.227, -lr * Pred Grad: 0.001, New P: 0.616
-Original Grad: -0.151, -lr * Pred Grad: 0.001, New P: 0.617
iter 16 loss: 0.284
Actual params: [0.616 , 0.6166]
-Original Grad: 0.219, -lr * Pred Grad: 0.001, New P: 0.617
-Original Grad: -0.160, -lr * Pred Grad: 0.001, New P: 0.618
iter 17 loss: 0.284
Actual params: [0.6174, 0.6181]
-Original Grad: 0.204, -lr * Pred Grad: 0.001, New P: 0.619
-Original Grad: -0.198, -lr * Pred Grad: 0.001, New P: 0.620
iter 18 loss: 0.284
Actual params: [0.6189, 0.6196]
-Original Grad: 0.187, -lr * Pred Grad: 0.001, New P: 0.620
-Original Grad: -0.245, -lr * Pred Grad: 0.001, New P: 0.621
iter 19 loss: 0.284
Actual params: [0.6203, 0.6211]
-Original Grad: 0.189, -lr * Pred Grad: 0.001, New P: 0.622
-Original Grad: -0.257, -lr * Pred Grad: 0.001, New P: 0.623
iter 20 loss: 0.284
Actual params: [0.6218, 0.6226]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.249, -lr * Pred Grad: 0.000, New P: 0.595
-Original Grad: 0.156, -lr * Pred Grad: 0.000, New P: 0.595
iter 0 loss: 0.331
Actual params: [0.5945, 0.5945]
-Original Grad: 0.251, -lr * Pred Grad: 0.001, New P: 0.595
-Original Grad: 0.157, -lr * Pred Grad: 0.001, New P: 0.595
iter 1 loss: 0.331
Actual params: [0.5954, 0.5954]
-Original Grad: 0.248, -lr * Pred Grad: 0.001, New P: 0.596
-Original Grad: 0.138, -lr * Pred Grad: 0.001, New P: 0.596
iter 2 loss: 0.330
Actual params: [0.5964, 0.5964]
-Original Grad: 0.250, -lr * Pred Grad: 0.001, New P: 0.598
-Original Grad: 0.138, -lr * Pred Grad: 0.001, New P: 0.598
iter 3 loss: 0.330
Actual params: [0.5976, 0.5977]
-Original Grad: 0.253, -lr * Pred Grad: 0.001, New P: 0.599
-Original Grad: 0.139, -lr * Pred Grad: 0.001, New P: 0.599
iter 4 loss: 0.329
Actual params: [0.5989, 0.599 ]
-Original Grad: 0.252, -lr * Pred Grad: 0.001, New P: 0.600
-Original Grad: 0.135, -lr * Pred Grad: 0.001, New P: 0.600
iter 5 loss: 0.329
Actual params: [0.6003, 0.6003]
-Original Grad: 0.251, -lr * Pred Grad: 0.001, New P: 0.602
-Original Grad: 0.114, -lr * Pred Grad: 0.001, New P: 0.602
iter 6 loss: 0.328
Actual params: [0.6017, 0.6017]
-Original Grad: 0.252, -lr * Pred Grad: 0.001, New P: 0.603
-Original Grad: 0.110, -lr * Pred Grad: 0.001, New P: 0.603
iter 7 loss: 0.328
Actual params: [0.6031, 0.6031]
-Original Grad: 0.253, -lr * Pred Grad: 0.001, New P: 0.604
-Original Grad: 0.113, -lr * Pred Grad: 0.001, New P: 0.605
iter 8 loss: 0.327
Actual params: [0.6045, 0.6046]
-Original Grad: 0.250, -lr * Pred Grad: 0.001, New P: 0.606
-Original Grad: 0.091, -lr * Pred Grad: 0.001, New P: 0.606
iter 9 loss: 0.327
Actual params: [0.6059, 0.606 ]
-Original Grad: 0.252, -lr * Pred Grad: 0.001, New P: 0.607
-Original Grad: 0.095, -lr * Pred Grad: 0.001, New P: 0.607
iter 10 loss: 0.326
Actual params: [0.6073, 0.6074]
-Original Grad: 0.250, -lr * Pred Grad: 0.001, New P: 0.609
-Original Grad: 0.104, -lr * Pred Grad: 0.001, New P: 0.609
iter 11 loss: 0.326
Actual params: [0.6088, 0.6089]
-Original Grad: 0.251, -lr * Pred Grad: 0.001, New P: 0.610
-Original Grad: 0.114, -lr * Pred Grad: 0.001, New P: 0.610
iter 12 loss: 0.325
Actual params: [0.6102, 0.6103]
-Original Grad: 0.254, -lr * Pred Grad: 0.001, New P: 0.612
-Original Grad: 0.124, -lr * Pred Grad: 0.001, New P: 0.612
iter 13 loss: 0.325
Actual params: [0.6116, 0.6118]
-Original Grad: 0.254, -lr * Pred Grad: 0.001, New P: 0.613
-Original Grad: 0.119, -lr * Pred Grad: 0.001, New P: 0.613
iter 14 loss: 0.324
Actual params: [0.6131, 0.6133]
-Original Grad: 0.260, -lr * Pred Grad: 0.001, New P: 0.615
-Original Grad: 0.129, -lr * Pred Grad: 0.001, New P: 0.615
iter 15 loss: 0.324
Actual params: [0.6145, 0.6147]
-Original Grad: 0.265, -lr * Pred Grad: 0.001, New P: 0.616
-Original Grad: 0.138, -lr * Pred Grad: 0.001, New P: 0.616
iter 16 loss: 0.323
Actual params: [0.6159, 0.6162]
-Original Grad: 0.274, -lr * Pred Grad: 0.001, New P: 0.617
-Original Grad: 0.148, -lr * Pred Grad: 0.001, New P: 0.618
iter 17 loss: 0.323
Actual params: [0.6174, 0.6176]
-Original Grad: 0.276, -lr * Pred Grad: 0.001, New P: 0.619
-Original Grad: 0.157, -lr * Pred Grad: 0.001, New P: 0.619
iter 18 loss: 0.322
Actual params: [0.6188, 0.6191]
-Original Grad: 0.283, -lr * Pred Grad: 0.001, New P: 0.620
-Original Grad: 0.178, -lr * Pred Grad: 0.001, New P: 0.621
iter 19 loss: 0.321
Actual params: [0.6203, 0.6205]
-Original Grad: 0.282, -lr * Pred Grad: 0.001, New P: 0.622
-Original Grad: 0.179, -lr * Pred Grad: 0.001, New P: 0.622
iter 20 loss: 0.321
Actual params: [0.6217, 0.622 ]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: -0.146, -lr * Pred Grad: 0.001, New P: 0.595
-Original Grad: 0.040, -lr * Pred Grad: 0.000, New P: 0.595
iter 0 loss: 0.328
Actual params: [0.5946, 0.5946]
-Original Grad: -0.168, -lr * Pred Grad: 0.001, New P: 0.595
-Original Grad: 0.044, -lr * Pred Grad: 0.001, New P: 0.595
iter 1 loss: 0.328
Actual params: [0.5954, 0.5954]
-Original Grad: -0.049, -lr * Pred Grad: 0.001, New P: 0.596
-Original Grad: 0.022, -lr * Pred Grad: 0.001, New P: 0.596
iter 2 loss: 0.328
Actual params: [0.5965, 0.5965]
-Original Grad: 0.141, -lr * Pred Grad: 0.001, New P: 0.598
-Original Grad: -0.016, -lr * Pred Grad: 0.001, New P: 0.598
iter 3 loss: 0.328
Actual params: [0.5977, 0.5977]
-Original Grad: 0.141, -lr * Pred Grad: 0.001, New P: 0.599
-Original Grad: -0.017, -lr * Pred Grad: 0.001, New P: 0.599
iter 4 loss: 0.328
Actual params: [0.599, 0.599]
-Original Grad: 0.231, -lr * Pred Grad: 0.001, New P: 0.600
-Original Grad: -0.036, -lr * Pred Grad: 0.001, New P: 0.600
iter 5 loss: 0.327
Actual params: [0.6004, 0.6004]
-Original Grad: 0.080, -lr * Pred Grad: 0.001, New P: 0.602
-Original Grad: -0.012, -lr * Pred Grad: 0.001, New P: 0.602
iter 6 loss: 0.327
Actual params: [0.6018, 0.6018]
-Original Grad: -0.001, -lr * Pred Grad: 0.001, New P: 0.603
-Original Grad: 0.004, -lr * Pred Grad: 0.001, New P: 0.603
iter 7 loss: 0.327
Actual params: [0.6032, 0.6032]
-Original Grad: -0.128, -lr * Pred Grad: 0.001, New P: 0.605
-Original Grad: 0.031, -lr * Pred Grad: 0.001, New P: 0.605
iter 8 loss: 0.327
Actual params: [0.6047, 0.6047]
-Original Grad: 0.100, -lr * Pred Grad: 0.001, New P: 0.606
-Original Grad: -0.015, -lr * Pred Grad: 0.001, New P: 0.606
iter 9 loss: 0.327
Actual params: [0.6061, 0.6061]
-Original Grad: 0.257, -lr * Pred Grad: 0.001, New P: 0.608
-Original Grad: -0.046, -lr * Pred Grad: 0.001, New P: 0.608
iter 10 loss: 0.327
Actual params: [0.6076, 0.6076]
-Original Grad: 0.343, -lr * Pred Grad: 0.001, New P: 0.609
-Original Grad: -0.066, -lr * Pred Grad: 0.001, New P: 0.609
iter 11 loss: 0.327
Actual params: [0.609 , 0.6091]
-Original Grad: 0.329, -lr * Pred Grad: 0.001, New P: 0.610
-Original Grad: -0.063, -lr * Pred Grad: 0.001, New P: 0.611
iter 12 loss: 0.326
Actual params: [0.6105, 0.6105]
-Original Grad: 0.374, -lr * Pred Grad: 0.001, New P: 0.612
-Original Grad: -0.072, -lr * Pred Grad: 0.001, New P: 0.612
iter 13 loss: 0.326
Actual params: [0.6119, 0.612 ]
-Original Grad: 0.415, -lr * Pred Grad: 0.001, New P: 0.613
-Original Grad: -0.085, -lr * Pred Grad: 0.001, New P: 0.613
iter 14 loss: 0.325
Actual params: [0.6133, 0.6135]
-Original Grad: 0.453, -lr * Pred Grad: 0.001, New P: 0.615
-Original Grad: -0.102, -lr * Pred Grad: 0.001, New P: 0.615
iter 15 loss: 0.325
Actual params: [0.6148, 0.6149]
-Original Grad: 0.455, -lr * Pred Grad: 0.001, New P: 0.616
-Original Grad: -0.119, -lr * Pred Grad: 0.001, New P: 0.616
iter 16 loss: 0.325
Actual params: [0.6162, 0.6164]
-Original Grad: 0.520, -lr * Pred Grad: 0.001, New P: 0.618
-Original Grad: -0.132, -lr * Pred Grad: 0.001, New P: 0.618
iter 17 loss: 0.324
Actual params: [0.6176, 0.6179]
-Original Grad: 0.621, -lr * Pred Grad: 0.001, New P: 0.619
-Original Grad: -0.152, -lr * Pred Grad: 0.001, New P: 0.619
iter 18 loss: 0.323
Actual params: [0.619 , 0.6194]
-Original Grad: 0.673, -lr * Pred Grad: 0.001, New P: 0.620
-Original Grad: -0.170, -lr * Pred Grad: 0.001, New P: 0.621
iter 19 loss: 0.323
Actual params: [0.6204, 0.6209]
-Original Grad: 0.698, -lr * Pred Grad: 0.001, New P: 0.622
-Original Grad: -0.179, -lr * Pred Grad: 0.001, New P: 0.622
iter 20 loss: 0.322
Actual params: [0.6218, 0.6223]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.507, -lr * Pred Grad: 0.000, New P: 0.595
-Original Grad: -2.357, -lr * Pred Grad: 0.001, New P: 0.595
iter 0 loss: 0.494
Actual params: [0.5945, 0.5946]
-Original Grad: 0.511, -lr * Pred Grad: 0.001, New P: 0.595
-Original Grad: -2.371, -lr * Pred Grad: 0.001, New P: 0.596
iter 1 loss: 0.495
Actual params: [0.5953, 0.5956]
-Original Grad: 0.523, -lr * Pred Grad: 0.001, New P: 0.596
-Original Grad: -2.410, -lr * Pred Grad: 0.001, New P: 0.597
iter 2 loss: 0.497
Actual params: [0.5964, 0.5969]
-Original Grad: 0.515, -lr * Pred Grad: 0.001, New P: 0.598
-Original Grad: -2.370, -lr * Pred Grad: 0.001, New P: 0.598
iter 3 loss: 0.500
Actual params: [0.5976, 0.5983]
-Original Grad: 0.520, -lr * Pred Grad: 0.001, New P: 0.599
-Original Grad: -2.354, -lr * Pred Grad: 0.002, New P: 0.600
iter 4 loss: 0.502
Actual params: [0.5988, 0.5999]
-Original Grad: 0.490, -lr * Pred Grad: 0.001, New P: 0.600
-Original Grad: -2.213, -lr * Pred Grad: 0.002, New P: 0.601
iter 5 loss: 0.505
Actual params: [0.6001, 0.6015]
-Original Grad: 0.544, -lr * Pred Grad: 0.001, New P: 0.601
-Original Grad: -2.449, -lr * Pred Grad: 0.002, New P: 0.603
iter 6 loss: 0.508
Actual params: [0.6015, 0.6031]
-Original Grad: 0.518, -lr * Pred Grad: 0.001, New P: 0.603
-Original Grad: -2.344, -lr * Pred Grad: 0.002, New P: 0.605
iter 7 loss: 0.512
Actual params: [0.6029, 0.6048]
-Original Grad: 0.527, -lr * Pred Grad: 0.001, New P: 0.604
-Original Grad: -2.373, -lr * Pred Grad: 0.002, New P: 0.607
iter 8 loss: 0.515
Actual params: [0.6043, 0.6065]
-Original Grad: 1.230, -lr * Pred Grad: 0.001, New P: 0.606
-Original Grad: -5.160, -lr * Pred Grad: 0.002, New P: 0.608
iter 9 loss: 0.518
Actual params: [0.6056, 0.6083]
-Original Grad: 0.596, -lr * Pred Grad: 0.001, New P: 0.607
-Original Grad: -2.585, -lr * Pred Grad: 0.002, New P: 0.610
iter 10 loss: 0.523
Actual params: [0.607, 0.61 ]
-Original Grad: 0.553, -lr * Pred Grad: 0.001, New P: 0.608
-Original Grad: -2.430, -lr * Pred Grad: 0.002, New P: 0.612
iter 11 loss: 0.527
Actual params: [0.6084, 0.6117]
-Original Grad: 0.465, -lr * Pred Grad: 0.001, New P: 0.610
-Original Grad: -2.064, -lr * Pred Grad: 0.002, New P: 0.613
iter 12 loss: 0.530
Actual params: [0.6098, 0.6134]
-Original Grad: 0.450, -lr * Pred Grad: 0.001, New P: 0.611
-Original Grad: -1.967, -lr * Pred Grad: 0.002, New P: 0.615
iter 13 loss: 0.533
Actual params: [0.6112, 0.6151]
-Original Grad: 0.444, -lr * Pred Grad: 0.001, New P: 0.613
-Original Grad: -1.911, -lr * Pred Grad: 0.002, New P: 0.617
iter 14 loss: 0.535
Actual params: [0.6126, 0.6168]
-Original Grad: 0.423, -lr * Pred Grad: 0.001, New P: 0.614
-Original Grad: -1.786, -lr * Pred Grad: 0.002, New P: 0.619
iter 15 loss: 0.538
Actual params: [0.614 , 0.6185]
-Original Grad: 0.412, -lr * Pred Grad: 0.001, New P: 0.615
-Original Grad: -1.766, -lr * Pred Grad: 0.002, New P: 0.620
iter 16 loss: 0.540
Actual params: [0.6154, 0.6202]
-Original Grad: 0.410, -lr * Pred Grad: 0.001, New P: 0.617
-Original Grad: -1.823, -lr * Pred Grad: 0.002, New P: 0.622
iter 17 loss: 0.543
Actual params: [0.6168, 0.6219]
-Original Grad: 0.435, -lr * Pred Grad: 0.001, New P: 0.618
-Original Grad: -1.966, -lr * Pred Grad: 0.002, New P: 0.624
iter 18 loss: 0.545
Actual params: [0.6183, 0.6236]
-Original Grad: 0.509, -lr * Pred Grad: 0.001, New P: 0.620
-Original Grad: -1.973, -lr * Pred Grad: 0.002, New P: 0.625
iter 19 loss: 0.548
Actual params: [0.6197, 0.6253]
-Original Grad: 0.518, -lr * Pred Grad: 0.001, New P: 0.621
-Original Grad: -1.986, -lr * Pred Grad: 0.002, New P: 0.627
iter 20 loss: 0.551
Actual params: [0.6211, 0.6269]
