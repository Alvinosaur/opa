Target params: [-1.0746]
Actual params: [1.084 , 0.5507]
-Original Grad: 0.177, -lr * Pred Grad:  0.045, New P: 1.129
-Original Grad: 0.445, -lr * Pred Grad:  0.255, New P: 0.805
iter 0 loss: 0.911
Actual params: [1.1285, 0.8053]
-Original Grad: 0.304, -lr * Pred Grad:  0.435, New P: 1.564
-Original Grad: 0.411, -lr * Pred Grad:  0.017, New P: 0.822
iter 1 loss: 0.858
Actual params: [1.5638, 0.822 ]
-Original Grad: 0.191, -lr * Pred Grad:  -0.371, New P: 1.193
-Original Grad: 0.602, -lr * Pred Grad:  0.423, New P: 1.245
iter 2 loss: 0.787
Actual params: [1.1928, 1.2452]
-Original Grad: 0.237, -lr * Pred Grad:  0.590, New P: 1.783
-Original Grad: 0.405, -lr * Pred Grad:  -0.010, New P: 1.235
iter 3 loss: 0.747
Actual params: [1.7826, 1.2353]
-Original Grad: 0.327, -lr * Pred Grad:  0.666, New P: 2.449
-Original Grad: 0.387, -lr * Pred Grad:  -0.137, New P: 1.098
iter 4 loss: 0.602
Actual params: [2.4488, 1.0981]
-Original Grad: 0.147, -lr * Pred Grad:  -0.072, New P: 2.376
-Original Grad: 0.234, -lr * Pred Grad:  0.322, New P: 1.420
iter 5 loss: 0.504
Actual params: [2.3763, 1.4199]
-Original Grad: 0.086, -lr * Pred Grad:  -0.830, New P: 1.546
-Original Grad: 0.198, -lr * Pred Grad:  0.761, New P: 2.181
iter 6 loss: 0.447
Actual params: [1.546, 2.181]
-Original Grad: 0.002, -lr * Pred Grad:  -0.580, New P: 0.966
-Original Grad: 0.187, -lr * Pred Grad:  0.374, New P: 2.555
iter 7 loss: 0.357
Actual params: [0.9658, 2.5554]
-Original Grad: 0.014, -lr * Pred Grad:  -0.155, New P: 0.811
-Original Grad: 0.145, -lr * Pred Grad:  0.194, New P: 2.749
iter 8 loss: 0.221
Actual params: [0.8107, 2.7493]
-Original Grad: 0.001, -lr * Pred Grad:  -0.082, New P: 0.729
-Original Grad: 0.038, -lr * Pred Grad:  0.069, New P: 2.819
iter 9 loss: 0.144
Actual params: [0.7287, 2.8187]
-Original Grad: 0.002, -lr * Pred Grad:  0.271, New P: 1.000
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: 2.800
iter 10 loss: 0.117
Actual params: [1.    , 2.8004]
-Original Grad: 0.011, -lr * Pred Grad:  0.823, New P: 1.823
-Original Grad: 0.011, -lr * Pred Grad:  0.025, New P: 2.826
iter 11 loss: 0.143
Actual params: [1.8232, 2.8256]
-Original Grad: 0.005, -lr * Pred Grad:  0.403, New P: 2.226
-Original Grad: 0.021, -lr * Pred Grad:  0.085, New P: 2.911
iter 12 loss: 0.110
Actual params: [2.2259, 2.9108]
-Original Grad: 0.002, -lr * Pred Grad:  0.322, New P: 2.548
-Original Grad: 0.011, -lr * Pred Grad:  0.076, New P: 2.987
iter 13 loss: 0.068
Actual params: [2.548 , 2.9865]
-Original Grad: 0.000, -lr * Pred Grad:  -0.035, New P: 2.513
-Original Grad: 0.008, -lr * Pred Grad:  0.095, New P: 3.082
iter 14 loss: 0.044
Actual params: [2.5128, 3.0815]
-Original Grad: -0.000, -lr * Pred Grad:  -0.194, New P: 2.319
-Original Grad: 0.003, -lr * Pred Grad:  0.052, New P: 3.134
iter 15 loss: 0.046
Actual params: [2.3186, 3.1337]
-Original Grad: 0.001, -lr * Pred Grad:  1.683, New P: 4.002
-Original Grad: -0.000, -lr * Pred Grad:  -0.076, New P: 3.058
iter 16 loss: 0.058
Actual params: [4.0015, 3.0577]
-Original Grad: -0.003, -lr * Pred Grad:  -2.733, New P: 1.269
-Original Grad: -0.001, -lr * Pred Grad:  0.197, New P: 3.255
iter 17 loss: 0.042
Actual params: [1.2688, 3.2545]
-Original Grad: 0.043, -lr * Pred Grad:  0.265, New P: 1.534
-Original Grad: -0.030, -lr * Pred Grad:  -0.095, New P: 3.160
iter 18 loss: 0.134
Actual params: [1.534 , 3.1597]
-Original Grad: 0.011, -lr * Pred Grad:  0.140, New P: 1.674
-Original Grad: -0.008, -lr * Pred Grad:  -0.031, New P: 3.128
iter 19 loss: 0.084
Actual params: [1.6736, 3.1282]
-Original Grad: 0.008, -lr * Pred Grad:  0.219, New P: 1.893
-Original Grad: -0.001, -lr * Pred Grad:  0.021, New P: 3.149
iter 20 loss: 0.069
Actual params: [1.8927, 3.1493]
-Original Grad: 0.005, -lr * Pred Grad:  0.289, New P: 2.182
-Original Grad: -0.001, -lr * Pred Grad:  0.027, New P: 3.176
iter 21 loss: 0.070
Actual params: [2.1819, 3.1763]
-Original Grad: 0.002, -lr * Pred Grad:  0.277, New P: 2.459
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: 3.166
iter 22 loss: 0.070
Actual params: [2.4589, 3.1656]
-Original Grad: 0.000, -lr * Pred Grad:  0.059, New P: 2.518
-Original Grad: -0.001, -lr * Pred Grad:  -0.032, New P: 3.134
iter 23 loss: 0.064
Actual params: [2.5179, 3.1336]
-Original Grad: -0.000, -lr * Pred Grad:  -0.078, New P: 2.440
-Original Grad: 0.002, -lr * Pred Grad:  0.035, New P: 3.169
iter 24 loss: 0.056
Actual params: [2.4403, 3.1689]
-Original Grad: 0.000, -lr * Pred Grad:  0.413, New P: 2.854
-Original Grad: -0.001, -lr * Pred Grad:  -0.043, New P: 3.126
iter 25 loss: 0.065
Actual params: [2.8536, 3.1264]
-Original Grad: -0.001, -lr * Pred Grad:  -2.217, New P: 0.636
-Original Grad: -0.001, -lr * Pred Grad:  0.112, New P: 3.238
iter 26 loss: 0.053
Actual params: [0.6362, 3.2382]
-Original Grad: 0.000, -lr * Pred Grad:  1.950, New P: 2.586
-Original Grad: -0.000, -lr * Pred Grad:  -0.129, New P: 3.109
iter 27 loss: 0.110
Actual params: [2.5861, 3.1093]
-Original Grad: -0.000, -lr * Pred Grad:  -1.803, New P: 0.783
-Original Grad: 0.002, -lr * Pred Grad:  0.156, New P: 3.265
iter 28 loss: 0.051
Actual params: [0.7833, 3.265 ]
-Original Grad: 0.001, -lr * Pred Grad:  5.233, New P: 6.017
-Original Grad: -0.011, -lr * Pred Grad:  -0.155, New P: 3.110
iter 29 loss: 0.129
Actual params: [6.0168, 3.11  ]
-Original Grad: -0.006, -lr * Pred Grad:  -2.591, New P: 3.426
-Original Grad: -0.002, -lr * Pred Grad:  0.040, New P: 3.150
iter 30 loss: 0.055
Actual params: [3.4259, 3.15  ]
Target params: [-1.0746]
Actual params: [-1.0585,  1.4018]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -1.058
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 1.402
iter 0 loss: 0.105
Actual params: [-1.0582,  1.4018]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -1.058
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 1.402
iter 1 loss: 0.105
Actual params: [-1.0576,  1.4016]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -1.056
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 1.401
iter 2 loss: 0.105
Actual params: [-1.056 ,  1.4012]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -1.052
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 1.400
iter 3 loss: 0.105
Actual params: [-1.0518,  1.4003]
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: -1.041
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 1.398
iter 4 loss: 0.105
Actual params: [-1.0412,  1.3978]
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: -1.013
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: 1.391
iter 5 loss: 0.105
Actual params: [-1.0126,  1.3912]
-Original Grad: 0.000, -lr * Pred Grad:  0.088, New P: -0.925
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: 1.371
iter 6 loss: 0.105
Actual params: [-0.9245,  1.3708]
-Original Grad: 0.000, -lr * Pred Grad:  0.414, New P: -0.510
-Original Grad: -0.000, -lr * Pred Grad:  -0.096, New P: 1.275
iter 7 loss: 0.105
Actual params: [-0.5105,  1.2749]
-Original Grad: 0.001, -lr * Pred Grad:  15.325, New P: 14.814
-Original Grad: -0.000, -lr * Pred Grad:  -1.344, New P: -0.069
iter 8 loss: 0.106
Actual params: [14.8144, -0.0694]
-Original Grad: 0.000, -lr * Pred Grad:  0.392, New P: 15.206
-Original Grad: 0.000, -lr * Pred Grad:  0.555, New P: 0.486
iter 9 loss: 0.105
Actual params: [15.2064,  0.4859]
-Original Grad: 0.000, -lr * Pred Grad:  0.268, New P: 15.475
-Original Grad: 0.000, -lr * Pred Grad:  1.187, New P: 1.673
iter 10 loss: 0.105
Actual params: [15.4745,  1.6726]
-Original Grad: -0.000, -lr * Pred Grad:  -0.719, New P: 14.755
-Original Grad: 0.000, -lr * Pred Grad:  0.565, New P: 2.238
iter 11 loss: 0.105
Actual params: [14.7552,  2.2378]
-Original Grad: -0.000, -lr * Pred Grad:  -4.257, New P: 10.498
-Original Grad: 0.000, -lr * Pred Grad:  1.434, New P: 3.672
iter 12 loss: 0.105
Actual params: [10.4978,  3.6716]
-Original Grad: -0.000, -lr * Pred Grad:  -21.827, New P: -11.330
-Original Grad: -0.000, -lr * Pred Grad:  -176.969, New P: -173.297
iter 13 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 14 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 15 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 16 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 17 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 18 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 19 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 20 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 21 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 22 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 23 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 24 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 25 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 26 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 27 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 28 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 29 loss: 0.105
Actual params: [ -11.3295, -173.2974]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -11.330
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -173.297
iter 30 loss: 0.105
Actual params: [ -11.3295, -173.2974]
Target params: [-1.0746]
Actual params: [1.5477, 0.5327]
-Original Grad: 0.200, -lr * Pred Grad:  0.151, New P: 1.698
-Original Grad: -0.450, -lr * Pred Grad:  -0.418, New P: 0.114
iter 0 loss: 0.537
Actual params: [1.6985, 0.1144]
-Original Grad: 0.073, -lr * Pred Grad:  -0.737, New P: 0.961
-Original Grad: -0.299, -lr * Pred Grad:  -0.533, New P: -0.419
iter 1 loss: 0.376
Actual params: [ 0.9613, -0.4187]
-Original Grad: 0.010, -lr * Pred Grad:  -0.652, New P: 0.309
-Original Grad: -0.245, -lr * Pred Grad:  -0.275, New P: -0.693
iter 2 loss: 0.201
Actual params: [ 0.3089, -0.6932]
-Original Grad: 0.009, -lr * Pred Grad:  0.136, New P: 0.445
-Original Grad: -0.044, -lr * Pred Grad:  -0.053, New P: -0.746
iter 3 loss: 0.089
Actual params: [ 0.4445, -0.7463]
-Original Grad: 0.001, -lr * Pred Grad:  0.131, New P: 0.576
-Original Grad: -0.052, -lr * Pred Grad:  -0.077, New P: -0.823
iter 4 loss: 0.086
Actual params: [ 0.5758, -0.8235]
-Original Grad: -0.014, -lr * Pred Grad:  -0.070, New P: 0.506
-Original Grad: 0.013, -lr * Pred Grad:  0.016, New P: -0.807
iter 5 loss: 0.080
Actual params: [ 0.5062, -0.8072]
-Original Grad: 0.006, -lr * Pred Grad:  0.036, New P: 0.542
-Original Grad: -0.019, -lr * Pred Grad:  -0.039, New P: -0.846
iter 6 loss: 0.079
Actual params: [ 0.542 , -0.8459]
-Original Grad: -0.007, -lr * Pred Grad:  0.000, New P: 0.542
-Original Grad: 0.020, -lr * Pred Grad:  0.075, New P: -0.771
iter 7 loss: 0.076
Actual params: [ 0.5424, -0.7707]
-Original Grad: 0.004, -lr * Pred Grad:  0.048, New P: 0.590
-Original Grad: -0.050, -lr * Pred Grad:  -0.087, New P: -0.858
iter 8 loss: 0.087
Actual params: [ 0.59  , -0.8579]
-Original Grad: -0.025, -lr * Pred Grad:  -0.058, New P: 0.532
-Original Grad: 0.030, -lr * Pred Grad:  0.059, New P: -0.799
iter 9 loss: 0.076
Actual params: [ 0.5321, -0.7986]
-Original Grad: 0.002, -lr * Pred Grad:  -0.007, New P: 0.525
-Original Grad: -0.030, -lr * Pred Grad:  -0.056, New P: -0.855
iter 10 loss: 0.082
Actual params: [ 0.5253, -0.8547]
-Original Grad: -0.000, -lr * Pred Grad:  0.027, New P: 0.553
-Original Grad: 0.017, -lr * Pred Grad:  0.070, New P: -0.785
iter 11 loss: 0.074
Actual params: [ 0.5528, -0.7848]
-Original Grad: 0.002, -lr * Pred Grad:  0.000, New P: 0.553
-Original Grad: -0.033, -lr * Pred Grad:  -0.065, New P: -0.850
iter 12 loss: 0.085
Actual params: [ 0.5532, -0.85  ]
-Original Grad: -0.008, -lr * Pred Grad:  -0.005, New P: 0.548
-Original Grad: 0.021, -lr * Pred Grad:  0.073, New P: -0.777
iter 13 loss: 0.076
Actual params: [ 0.5481, -0.777 ]
-Original Grad: 0.004, -lr * Pred Grad:  0.048, New P: 0.596
-Original Grad: -0.049, -lr * Pred Grad:  -0.084, New P: -0.861
iter 14 loss: 0.086
Actual params: [ 0.5965, -0.8609]
-Original Grad: -0.025, -lr * Pred Grad:  -0.059, New P: 0.537
-Original Grad: 0.031, -lr * Pred Grad:  0.060, New P: -0.801
iter 15 loss: 0.076
Actual params: [ 0.5373, -0.8008]
-Original Grad: 0.002, -lr * Pred Grad:  -0.007, New P: 0.530
-Original Grad: -0.029, -lr * Pred Grad:  -0.055, New P: -0.856
iter 16 loss: 0.082
Actual params: [ 0.5303, -0.8557]
-Original Grad: -0.007, -lr * Pred Grad:  -0.003, New P: 0.527
-Original Grad: 0.019, -lr * Pred Grad:  0.067, New P: -0.788
iter 17 loss: 0.074
Actual params: [ 0.5273, -0.7883]
-Original Grad: 0.004, -lr * Pred Grad:  0.047, New P: 0.574
-Original Grad: -0.045, -lr * Pred Grad:  -0.079, New P: -0.867
iter 18 loss: 0.083
Actual params: [ 0.5739, -0.8673]
-Original Grad: -0.020, -lr * Pred Grad:  -0.052, New P: 0.522
-Original Grad: 0.029, -lr * Pred Grad:  0.064, New P: -0.803
iter 19 loss: 0.074
Actual params: [ 0.5217, -0.8033]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: 0.511
-Original Grad: -0.029, -lr * Pred Grad:  -0.056, New P: -0.859
iter 20 loss: 0.080
Actual params: [ 0.5105, -0.8594]
-Original Grad: 0.001, -lr * Pred Grad:  0.034, New P: 0.545
-Original Grad: 0.018, -lr * Pred Grad:  0.073, New P: -0.787
iter 21 loss: 0.072
Actual params: [ 0.5446, -0.7866]
-Original Grad: 0.002, -lr * Pred Grad:  0.006, New P: 0.551
-Original Grad: -0.032, -lr * Pred Grad:  -0.064, New P: -0.851
iter 22 loss: 0.084
Actual params: [ 0.551 , -0.8507]
-Original Grad: -0.008, -lr * Pred Grad:  -0.008, New P: 0.543
-Original Grad: 0.021, -lr * Pred Grad:  0.072, New P: -0.779
iter 23 loss: 0.075
Actual params: [ 0.5431, -0.7787]
-Original Grad: 0.004, -lr * Pred Grad:  0.049, New P: 0.592
-Original Grad: -0.048, -lr * Pred Grad:  -0.083, New P: -0.862
iter 24 loss: 0.086
Actual params: [ 0.5925, -0.862 ]
-Original Grad: -0.025, -lr * Pred Grad:  -0.060, New P: 0.533
-Original Grad: 0.030, -lr * Pred Grad:  0.059, New P: -0.803
iter 25 loss: 0.076
Actual params: [ 0.5328, -0.8028]
-Original Grad: 0.002, -lr * Pred Grad:  -0.007, New P: 0.526
-Original Grad: -0.029, -lr * Pred Grad:  -0.054, New P: -0.857
iter 26 loss: 0.081
Actual params: [ 0.5262, -0.8572]
-Original Grad: -0.006, -lr * Pred Grad:  0.003, New P: 0.530
-Original Grad: 0.020, -lr * Pred Grad:  0.070, New P: -0.787
iter 27 loss: 0.073
Actual params: [ 0.5297, -0.7869]
-Original Grad: 0.004, -lr * Pred Grad:  0.051, New P: 0.581
-Original Grad: -0.046, -lr * Pred Grad:  -0.080, New P: -0.866
iter 28 loss: 0.083
Actual params: [ 0.5805, -0.8665]
-Original Grad: -0.021, -lr * Pred Grad:  -0.053, New P: 0.528
-Original Grad: 0.030, -lr * Pred Grad:  0.064, New P: -0.803
iter 29 loss: 0.074
Actual params: [ 0.5278, -0.8026]
-Original Grad: 0.001, -lr * Pred Grad:  -0.005, New P: 0.523
-Original Grad: -0.029, -lr * Pred Grad:  -0.054, New P: -0.857
iter 30 loss: 0.081
Actual params: [ 0.523 , -0.8569]
Target params: [-1.0746]
Actual params: [0.0029, 0.9353]
-Original Grad: -0.019, -lr * Pred Grad:  -0.110, New P: -0.107
-Original Grad: -0.012, -lr * Pred Grad:  -0.072, New P: 0.863
iter 0 loss: 0.077
Actual params: [-0.1068,  0.863 ]
-Original Grad: -0.005, -lr * Pred Grad:  -0.044, New P: -0.151
-Original Grad: -0.003, -lr * Pred Grad:  -0.038, New P: 0.825
iter 1 loss: 0.062
Actual params: [-0.1511,  0.8246]
-Original Grad: -0.003, -lr * Pred Grad:  -0.042, New P: -0.193
-Original Grad: -0.002, -lr * Pred Grad:  -0.037, New P: 0.787
iter 2 loss: 0.057
Actual params: [-0.1929,  0.7872]
-Original Grad: -0.003, -lr * Pred Grad:  -0.064, New P: -0.257
-Original Grad: -0.002, -lr * Pred Grad:  -0.011, New P: 0.776
iter 3 loss: 0.052
Actual params: [-0.2573,  0.7759]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.254
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: 0.773
iter 4 loss: 0.047
Actual params: [-0.2535,  0.7729]
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: -0.244
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: 0.766
iter 5 loss: 0.047
Actual params: [-0.2441,  0.7656]
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: -0.221
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: 0.749
iter 6 loss: 0.047
Actual params: [-0.2213,  0.749 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.051, New P: -0.272
-Original Grad: -0.001, -lr * Pred Grad:  -0.068, New P: 0.681
iter 7 loss: 0.048
Actual params: [-0.272,  0.681]
-Original Grad: 0.000, -lr * Pred Grad:  -0.024, New P: -0.296
-Original Grad: 0.000, -lr * Pred Grad:  0.047, New P: 0.728
iter 8 loss: 0.043
Actual params: [-0.2958,  0.7281]
-Original Grad: 0.000, -lr * Pred Grad:  0.102, New P: -0.194
-Original Grad: 0.000, -lr * Pred Grad:  -0.136, New P: 0.592
iter 9 loss: 0.043
Actual params: [-0.1938,  0.5924]
-Original Grad: 0.000, -lr * Pred Grad:  -0.894, New P: -1.088
-Original Grad: 0.000, -lr * Pred Grad:  1.465, New P: 2.058
iter 10 loss: 0.044
Actual params: [-1.0877,  2.0577]
-Original Grad: 0.000, -lr * Pred Grad:  0.601, New P: -0.487
-Original Grad: -0.000, -lr * Pred Grad:  -0.932, New P: 1.125
iter 11 loss: 0.031
Actual params: [-0.487 ,  1.1252]
-Original Grad: 0.000, -lr * Pred Grad:  8.482, New P: 7.995
-Original Grad: 0.000, -lr * Pred Grad:  -13.002, New P: -11.876
iter 12 loss: 0.040
Actual params: [  7.9953, -11.8764]
-Original Grad: -0.000, -lr * Pred Grad:  -4.793, New P: 3.202
-Original Grad: 0.001, -lr * Pred Grad:  7.508, New P: -4.369
iter 13 loss: 0.033
Actual params: [ 3.2025, -4.3686]
-Original Grad: 0.054, -lr * Pred Grad:  0.917, New P: 4.119
-Original Grad: 0.005, -lr * Pred Grad:  -0.196, New P: -4.565
iter 14 loss: 0.211
Actual params: [ 4.1191, -4.5646]
-Original Grad: 0.021, -lr * Pred Grad:  0.343, New P: 4.462
-Original Grad: -0.013, -lr * Pred Grad:  -0.273, New P: -4.838
iter 15 loss: 0.106
Actual params: [ 4.4624, -4.8376]
-Original Grad: 0.010, -lr * Pred Grad:  0.172, New P: 4.635
-Original Grad: -0.009, -lr * Pred Grad:  -0.252, New P: -5.089
iter 16 loss: 0.080
Actual params: [ 4.6345, -5.0894]
-Original Grad: 0.006, -lr * Pred Grad:  0.085, New P: 4.719
-Original Grad: -0.007, -lr * Pred Grad:  -0.253, New P: -5.343
iter 17 loss: 0.066
Actual params: [ 4.7194, -5.3426]
-Original Grad: 0.001, -lr * Pred Grad:  0.043, New P: 4.762
-Original Grad: -0.001, -lr * Pred Grad:  -0.032, New P: -5.375
iter 18 loss: 0.056
Actual params: [ 4.7621, -5.3745]
-Original Grad: 0.000, -lr * Pred Grad:  0.111, New P: 4.873
-Original Grad: -0.000, -lr * Pred Grad:  0.060, New P: -5.314
iter 19 loss: 0.054
Actual params: [ 4.873 , -5.3142]
-Original Grad: 0.000, -lr * Pred Grad:  0.346, New P: 5.219
-Original Grad: 0.000, -lr * Pred Grad:  0.208, New P: -5.107
iter 20 loss: 0.054
Actual params: [ 5.2192, -5.1066]
-Original Grad: 0.000, -lr * Pred Grad:  0.538, New P: 5.757
-Original Grad: 0.000, -lr * Pred Grad:  0.410, New P: -4.696
iter 21 loss: 0.052
Actual params: [ 5.7568, -4.6963]
-Original Grad: -0.001, -lr * Pred Grad:  -0.556, New P: 5.201
-Original Grad: 0.000, -lr * Pred Grad:  -0.439, New P: -5.135
iter 22 loss: 0.046
Actual params: [ 5.2012, -5.1349]
-Original Grad: 0.000, -lr * Pred Grad:  0.774, New P: 5.975
-Original Grad: 0.000, -lr * Pred Grad:  0.716, New P: -4.419
iter 23 loss: 0.052
Actual params: [ 5.9751, -4.419 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.252, New P: 5.723
-Original Grad: 0.001, -lr * Pred Grad:  0.048, New P: -4.371
iter 24 loss: 0.041
Actual params: [ 5.7234, -4.3706]
-Original Grad: -0.001, -lr * Pred Grad:  -0.178, New P: 5.546
-Original Grad: 0.001, -lr * Pred Grad:  0.412, New P: -3.959
iter 25 loss: 0.042
Actual params: [ 5.5457, -3.9589]
-Original Grad: -0.001, -lr * Pred Grad:  -0.156, New P: 5.390
-Original Grad: -0.001, -lr * Pred Grad:  -0.255, New P: -4.214
iter 26 loss: 0.038
Actual params: [ 5.3899, -4.2141]
-Original Grad: 0.001, -lr * Pred Grad:  -0.461, New P: 4.929
-Original Grad: 0.001, -lr * Pred Grad:  0.992, New P: -3.222
iter 27 loss: 0.042
Actual params: [ 4.929 , -3.2224]
-Original Grad: -0.002, -lr * Pred Grad:  0.105, New P: 5.034
-Original Grad: -0.004, -lr * Pred Grad:  -0.363, New P: -3.585
iter 28 loss: 0.033
Actual params: [ 5.0342, -3.585 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.398, New P: 4.637
-Original Grad: -0.001, -lr * Pred Grad:  0.192, New P: -3.393
iter 29 loss: 0.038
Actual params: [ 4.6366, -3.3932]
-Original Grad: 0.000, -lr * Pred Grad:  -0.619, New P: 4.018
-Original Grad: 0.001, -lr * Pred Grad:  0.393, New P: -3.000
iter 30 loss: 0.038
Actual params: [ 4.018 , -3.0002]
Target params: [-1.0746]
Actual params: [-0.6756, -1.5044]
-Original Grad: 0.003, -lr * Pred Grad:  0.054, New P: -0.622
-Original Grad: 0.003, -lr * Pred Grad:  0.054, New P: -1.450
iter 0 loss: 0.041
Actual params: [-0.6219, -1.45  ]
-Original Grad: 0.003, -lr * Pred Grad:  0.035, New P: -0.587
-Original Grad: 0.003, -lr * Pred Grad:  0.093, New P: -1.357
iter 1 loss: 0.039
Actual params: [-0.5865, -1.3569]
-Original Grad: 0.004, -lr * Pred Grad:  -0.073, New P: -0.660
-Original Grad: 0.004, -lr * Pred Grad:  0.238, New P: -1.119
iter 2 loss: 0.037
Actual params: [-0.6599, -1.1194]
-Original Grad: 0.007, -lr * Pred Grad:  -0.242, New P: -0.902
-Original Grad: 0.007, -lr * Pred Grad:  0.554, New P: -0.566
iter 3 loss: 0.035
Actual params: [-0.9019, -0.5656]
-Original Grad: 0.009, -lr * Pred Grad:  0.261, New P: -0.641
-Original Grad: 0.000, -lr * Pred Grad:  -0.331, New P: -0.897
iter 4 loss: 0.038
Actual params: [-0.6408, -0.8968]
-Original Grad: 0.006, -lr * Pred Grad:  0.007, New P: -0.634
-Original Grad: 0.005, -lr * Pred Grad:  0.349, New P: -0.548
iter 5 loss: 0.033
Actual params: [-0.6341, -0.5477]
-Original Grad: 0.006, -lr * Pred Grad:  0.137, New P: -0.497
-Original Grad: -0.001, -lr * Pred Grad:  -0.049, New P: -0.596
iter 6 loss: 0.034
Actual params: [-0.4974, -0.5964]
-Original Grad: 0.006, -lr * Pred Grad:  0.175, New P: -0.323
-Original Grad: 0.001, -lr * Pred Grad:  0.068, New P: -0.528
iter 7 loss: 0.034
Actual params: [-0.3226, -0.5282]
-Original Grad: 0.003, -lr * Pred Grad:  0.068, New P: -0.255
-Original Grad: -0.001, -lr * Pred Grad:  0.002, New P: -0.527
iter 8 loss: 0.035
Actual params: [-0.2549, -0.5267]
-Original Grad: -0.000, -lr * Pred Grad:  0.007, New P: -0.248
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: -0.512
iter 9 loss: 0.036
Actual params: [-0.2483, -0.5116]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.243
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: -0.503
iter 10 loss: 0.036
Actual params: [-0.243 , -0.5032]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.238
-Original Grad: 0.001, -lr * Pred Grad:  0.005, New P: -0.498
iter 11 loss: 0.036
Actual params: [-0.2384, -0.4979]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -0.256
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.521
iter 12 loss: 0.037
Actual params: [-0.256 , -0.5206]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.249
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: -0.510
iter 13 loss: 0.036
Actual params: [-0.2494, -0.5101]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.244
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: -0.503
iter 14 loss: 0.036
Actual params: [-0.244 , -0.5027]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.239
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.498
iter 15 loss: 0.036
Actual params: [-0.2393, -0.4978]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.257
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.520
iter 16 loss: 0.037
Actual params: [-0.2566, -0.5203]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.250
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.510
iter 17 loss: 0.036
Actual params: [-0.2499, -0.5099]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.244
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: -0.503
iter 18 loss: 0.036
Actual params: [-0.2442, -0.503 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.239
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.498
iter 19 loss: 0.036
Actual params: [-0.2395, -0.498 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.257
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: -0.520
iter 20 loss: 0.037
Actual params: [-0.2567, -0.5204]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.250
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.510
iter 21 loss: 0.036
Actual params: [-0.25, -0.51]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.244
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: -0.503
iter 22 loss: 0.036
Actual params: [-0.2443, -0.503 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.240
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.498
iter 23 loss: 0.036
Actual params: [-0.2395, -0.498 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.257
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: -0.520
iter 24 loss: 0.037
Actual params: [-0.2568, -0.5205]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.250
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.510
iter 25 loss: 0.036
Actual params: [-0.25, -0.51]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.244
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: -0.503
iter 26 loss: 0.036
Actual params: [-0.2443, -0.503 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.240
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.498
iter 27 loss: 0.036
Actual params: [-0.2396, -0.498 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.257
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: -0.520
iter 28 loss: 0.037
Actual params: [-0.2568, -0.5205]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.250
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.510
iter 29 loss: 0.036
Actual params: [-0.25, -0.51]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.244
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: -0.503
iter 30 loss: 0.036
Actual params: [-0.2444, -0.5031]
Target params: [-1.0746]
Actual params: [-0.6634, -0.2295]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.658
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.227
iter 0 loss: 0.080
Actual params: [-0.6579, -0.2265]
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: -0.644
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.219
iter 1 loss: 0.080
Actual params: [-0.6442, -0.2191]
-Original Grad: 0.000, -lr * Pred Grad:  0.034, New P: -0.610
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: -0.201
iter 2 loss: 0.081
Actual params: [-0.6098, -0.2011]
-Original Grad: 0.000, -lr * Pred Grad:  0.093, New P: -0.517
-Original Grad: 0.000, -lr * Pred Grad:  0.043, New P: -0.158
iter 3 loss: 0.083
Actual params: [-0.5168, -0.1582]
-Original Grad: 0.000, -lr * Pred Grad:  0.277, New P: -0.240
-Original Grad: 0.000, -lr * Pred Grad:  0.088, New P: -0.070
iter 4 loss: 0.088
Actual params: [-0.2401, -0.0701]
-Original Grad: 0.001, -lr * Pred Grad:  1.180, New P: 0.940
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: -0.046
iter 5 loss: 0.115
Actual params: [ 0.9397, -0.0459]
-Original Grad: 0.030, -lr * Pred Grad:  0.050, New P: 0.990
-Original Grad: 0.268, -lr * Pred Grad:  0.232, New P: 0.186
iter 6 loss: 0.857
Actual params: [0.9895, 0.1862]
-Original Grad: 0.046, -lr * Pred Grad:  0.651, New P: 1.640
-Original Grad: 0.372, -lr * Pred Grad:  0.159, New P: 0.346
iter 7 loss: 0.840
Actual params: [1.6402, 0.3456]
-Original Grad: 0.476, -lr * Pred Grad:  0.472, New P: 2.112
-Original Grad: 0.528, -lr * Pred Grad:  0.014, New P: 0.359
iter 8 loss: 0.640
Actual params: [2.1123, 0.3594]
-Original Grad: 0.453, -lr * Pred Grad:  0.379, New P: 2.492
-Original Grad: 0.351, -lr * Pred Grad:  -0.094, New P: 0.265
iter 9 loss: 0.428
Actual params: [2.4916, 0.2653]
-Original Grad: 0.191, -lr * Pred Grad:  0.484, New P: 2.976
-Original Grad: 0.079, -lr * Pred Grad:  -0.372, New P: -0.107
iter 10 loss: 0.346
Actual params: [ 2.9756, -0.1068]
-Original Grad: 0.121, -lr * Pred Grad:  0.203, New P: 3.179
-Original Grad: -0.193, -lr * Pred Grad:  -0.265, New P: -0.372
iter 11 loss: 0.210
Actual params: [ 3.1791, -0.3718]
-Original Grad: 0.023, -lr * Pred Grad:  0.008, New P: 3.187
-Original Grad: -0.130, -lr * Pred Grad:  -0.075, New P: -0.447
iter 12 loss: 0.091
Actual params: [ 3.1874, -0.4465]
-Original Grad: 0.000, -lr * Pred Grad:  -0.029, New P: 3.158
-Original Grad: -0.031, -lr * Pred Grad:  -0.022, New P: -0.469
iter 13 loss: 0.066
Actual params: [ 3.1579, -0.4688]
-Original Grad: -0.002, -lr * Pred Grad:  -0.048, New P: 3.110
-Original Grad: 0.003, -lr * Pred Grad:  -0.001, New P: -0.470
iter 14 loss: 0.059
Actual params: [ 3.1104, -0.4699]
-Original Grad: -0.002, -lr * Pred Grad:  -0.118, New P: 2.992
-Original Grad: 0.003, -lr * Pred Grad:  -0.005, New P: -0.475
iter 15 loss: 0.059
Actual params: [ 2.9923, -0.4753]
-Original Grad: -0.002, -lr * Pred Grad:  -0.265, New P: 2.727
-Original Grad: 0.002, -lr * Pred Grad:  -0.015, New P: -0.491
iter 16 loss: 0.060
Actual params: [ 2.727 , -0.4907]
-Original Grad: -0.002, -lr * Pred Grad:  -0.598, New P: 2.129
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -0.526
iter 17 loss: 0.057
Actual params: [ 2.1291, -0.5258]
-Original Grad: 2.523, -lr * Pred Grad:  0.010, New P: 2.139
-Original Grad: -1.334, -lr * Pred Grad:  -0.002, New P: -0.527
iter 18 loss: 0.275
Actual params: [ 2.1393, -0.5275]
-Original Grad: 4.987, -lr * Pred Grad:  0.006, New P: 2.145
-Original Grad: -4.606, -lr * Pred Grad:  -0.001, New P: -0.528
iter 19 loss: 0.266
Actual params: [ 2.1454, -0.5281]
-Original Grad: 7.245, -lr * Pred Grad:  0.004, New P: 2.150
-Original Grad: -4.616, -lr * Pred Grad:  -0.000, New P: -0.528
iter 20 loss: 0.258
Actual params: [ 2.1497, -0.5284]
-Original Grad: 5.120, -lr * Pred Grad:  0.004, New P: 2.154
-Original Grad: -2.396, -lr * Pred Grad:  -0.000, New P: -0.528
iter 21 loss: 0.251
Actual params: [ 2.154 , -0.5285]
-Original Grad: 4.282, -lr * Pred Grad:  0.005, New P: 2.159
-Original Grad: -1.936, -lr * Pred Grad:  -0.001, New P: -0.529
iter 22 loss: 0.245
Actual params: [ 2.1587, -0.5291]
-Original Grad: 4.363, -lr * Pred Grad:  0.006, New P: 2.165
-Original Grad: -2.401, -lr * Pred Grad:  -0.001, New P: -0.530
iter 23 loss: 0.240
Actual params: [ 2.1646, -0.5297]
-Original Grad: 4.878, -lr * Pred Grad:  0.007, New P: 2.171
-Original Grad: -3.237, -lr * Pred Grad:  -0.000, New P: -0.530
iter 24 loss: 0.233
Actual params: [ 2.1711, -0.5299]
-Original Grad: 6.220, -lr * Pred Grad:  0.006, New P: 2.177
-Original Grad: -3.783, -lr * Pred Grad:  0.001, New P: -0.529
iter 25 loss: 0.224
Actual params: [ 2.1775, -0.5293]
-Original Grad: 5.835, -lr * Pred Grad:  0.005, New P: 2.183
-Original Grad: -2.797, -lr * Pred Grad:  0.000, New P: -0.529
iter 26 loss: 0.213
Actual params: [ 2.1829, -0.5289]
-Original Grad: 4.247, -lr * Pred Grad:  0.005, New P: 2.188
-Original Grad: -2.113, -lr * Pred Grad:  -0.000, New P: -0.529
iter 27 loss: 0.205
Actual params: [ 2.1883, -0.5289]
-Original Grad: 4.332, -lr * Pred Grad:  0.006, New P: 2.195
-Original Grad: -2.864, -lr * Pred Grad:  -0.000, New P: -0.529
iter 28 loss: 0.198
Actual params: [ 2.1946, -0.5291]
-Original Grad: 5.648, -lr * Pred Grad:  0.007, New P: 2.201
-Original Grad: -4.182, -lr * Pred Grad:  0.000, New P: -0.529
iter 29 loss: 0.190
Actual params: [ 2.2014, -0.5288]
-Original Grad: 6.824, -lr * Pred Grad:  0.007, New P: 2.208
-Original Grad: -4.202, -lr * Pred Grad:  0.001, New P: -0.528
iter 30 loss: 0.179
Actual params: [ 2.208 , -0.5279]
Target params: [-1.0746]
Actual params: [-0.8962,  0.1733]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.896
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.173
iter 0 loss: 0.072
Actual params: [-0.8965,  0.173 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.897
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.172
iter 1 loss: 0.072
Actual params: [-0.897 ,  0.1721]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.898
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 0.170
iter 2 loss: 0.072
Actual params: [-0.8985,  0.17  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.902
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: 0.165
iter 3 loss: 0.072
Actual params: [-0.902 ,  0.1648]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.911
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: 0.152
iter 4 loss: 0.072
Actual params: [-0.9106,  0.1518]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -0.931
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: 0.121
iter 5 loss: 0.071
Actual params: [-0.931 ,  0.1207]
-Original Grad: -0.000, -lr * Pred Grad:  -0.046, New P: -0.977
-Original Grad: -0.000, -lr * Pred Grad:  -0.071, New P: 0.050
iter 6 loss: 0.072
Actual params: [-0.9768,  0.0498]
-Original Grad: -0.000, -lr * Pred Grad:  -0.083, New P: -1.059
-Original Grad: -0.000, -lr * Pred Grad:  -0.140, New P: -0.091
iter 7 loss: 0.073
Actual params: [-1.0594, -0.0906]
-Original Grad: -0.000, -lr * Pred Grad:  -0.116, New P: -1.175
-Original Grad: -0.000, -lr * Pred Grad:  -0.232, New P: -0.323
iter 8 loss: 0.073
Actual params: [-1.1751, -0.3228]
-Original Grad: -0.000, -lr * Pred Grad:  -0.216, New P: -1.391
-Original Grad: -0.000, -lr * Pred Grad:  -0.426, New P: -0.749
iter 9 loss: 0.074
Actual params: [-1.3909, -0.7488]
-Original Grad: 0.000, -lr * Pred Grad:  0.042, New P: -1.349
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.746
iter 10 loss: 0.074
Actual params: [-1.3487, -0.7459]
-Original Grad: 0.000, -lr * Pred Grad:  0.136, New P: -1.213
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: -0.738
iter 11 loss: 0.074
Actual params: [-1.2129, -0.7376]
-Original Grad: 0.000, -lr * Pred Grad:  0.684, New P: -0.529
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -0.762
iter 12 loss: 0.074
Actual params: [-0.5286, -0.7615]
-Original Grad: 0.000, -lr * Pred Grad:  4.710, New P: 4.181
-Original Grad: -0.000, -lr * Pred Grad:  -10.884, New P: -11.645
iter 13 loss: 0.074
Actual params: [  4.1809, -11.6454]
-Original Grad: 0.011, -lr * Pred Grad:  5.690, New P: 9.871
-Original Grad: 0.008, -lr * Pred Grad:  -1.207, New P: -12.853
iter 14 loss: 0.188
Actual params: [  9.8713, -12.8527]
-Original Grad: 0.000, -lr * Pred Grad:  0.532, New P: 10.404
-Original Grad: -0.000, -lr * Pred Grad:  -0.466, New P: -13.319
iter 15 loss: 0.073
Actual params: [ 10.4038, -13.3186]
-Original Grad: -0.000, -lr * Pred Grad:  0.234, New P: 10.638
-Original Grad: -0.000, -lr * Pred Grad:  -0.328, New P: -13.647
iter 16 loss: 0.074
Actual params: [ 10.6381, -13.6469]
-Original Grad: -0.000, -lr * Pred Grad:  -0.132, New P: 10.506
-Original Grad: -0.000, -lr * Pred Grad:  -0.279, New P: -13.926
iter 17 loss: 0.074
Actual params: [ 10.5059, -13.926 ]
-Original Grad: -0.000, -lr * Pred Grad:  -1.701, New P: 8.805
-Original Grad: -0.000, -lr * Pred Grad:  0.157, New P: -13.769
iter 18 loss: 0.074
Actual params: [  8.8046, -13.7685]
-Original Grad: -0.000, -lr * Pred Grad:  -7.631, New P: 1.174
-Original Grad: -0.000, -lr * Pred Grad:  -3.215, New P: -16.984
iter 19 loss: 0.073
Actual params: [  1.174 , -16.9837]
-Original Grad: -0.041, -lr * Pred Grad:  -0.616, New P: 0.558
-Original Grad: 0.027, -lr * Pred Grad:  0.367, New P: -16.617
iter 20 loss: 0.158
Actual params: [  0.5581, -16.6166]
-Original Grad: -0.001, -lr * Pred Grad:  -0.048, New P: 0.510
-Original Grad: 0.000, -lr * Pred Grad:  -0.020, New P: -16.636
iter 21 loss: 0.119
Actual params: [  0.5103, -16.6363]
-Original Grad: -0.001, -lr * Pred Grad:  -0.090, New P: 0.421
-Original Grad: 0.000, -lr * Pred Grad:  -0.036, New P: -16.673
iter 22 loss: 0.111
Actual params: [  0.4205, -16.6727]
-Original Grad: -0.000, -lr * Pred Grad:  -0.127, New P: 0.294
-Original Grad: 0.000, -lr * Pred Grad:  -0.043, New P: -16.716
iter 23 loss: 0.097
Actual params: [  0.2936, -16.7161]
-Original Grad: -0.000, -lr * Pred Grad:  -0.151, New P: 0.142
-Original Grad: 0.000, -lr * Pred Grad:  -0.042, New P: -16.758
iter 24 loss: 0.084
Actual params: [  0.1424, -16.7577]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: 0.116
-Original Grad: 0.000, -lr * Pred Grad:  0.193, New P: -16.565
iter 25 loss: 0.077
Actual params: [  0.116 , -16.5648]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: 0.094
-Original Grad: 0.000, -lr * Pred Grad:  0.254, New P: -16.311
iter 26 loss: 0.091
Actual params: [  0.094 , -16.3112]
-Original Grad: -0.000, -lr * Pred Grad:  0.682, New P: 0.776
-Original Grad: 0.000, -lr * Pred Grad:  1.229, New P: -15.082
iter 27 loss: 0.105
Actual params: [  0.7762, -15.0824]
-Original Grad: -0.387, -lr * Pred Grad:  -0.039, New P: 0.738
-Original Grad: -0.480, -lr * Pred Grad:  -0.473, New P: -15.556
iter 28 loss: 0.540
Actual params: [  0.7375, -15.5559]
-Original Grad: -0.377, -lr * Pred Grad:  -0.024, New P: 0.713
-Original Grad: -0.427, -lr * Pred Grad:  -0.299, New P: -15.855
iter 29 loss: 0.399
Actual params: [  0.7132, -15.8552]
-Original Grad: -0.108, -lr * Pred Grad:  0.103, New P: 0.816
-Original Grad: -0.263, -lr * Pred Grad:  -0.294, New P: -16.149
iter 30 loss: 0.323
Actual params: [  0.8159, -16.1487]
Target params: [-1.0746]
Actual params: [1.5544, 0.3381]
-Original Grad: 0.010, -lr * Pred Grad:  0.021, New P: 1.575
-Original Grad: -0.076, -lr * Pred Grad:  -0.151, New P: 0.187
iter 0 loss: 0.110
Actual params: [1.5753, 0.1871]
-Original Grad: 0.006, -lr * Pred Grad:  0.150, New P: 1.725
-Original Grad: -0.023, -lr * Pred Grad:  -0.035, New P: 0.152
iter 1 loss: 0.066
Actual params: [1.725 , 0.1524]
-Original Grad: 0.003, -lr * Pred Grad:  0.143, New P: 1.868
-Original Grad: -0.010, -lr * Pred Grad:  -0.021, New P: 0.131
iter 2 loss: 0.051
Actual params: [1.8685, 0.1314]
-Original Grad: 0.002, -lr * Pred Grad:  0.159, New P: 2.028
-Original Grad: -0.002, -lr * Pred Grad:  -0.011, New P: 0.120
iter 3 loss: 0.043
Actual params: [2.0278, 0.1199]
-Original Grad: 0.001, -lr * Pred Grad:  0.200, New P: 2.228
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: 0.109
iter 4 loss: 0.038
Actual params: [2.2278, 0.1091]
-Original Grad: 0.001, -lr * Pred Grad:  0.318, New P: 2.545
-Original Grad: 0.001, -lr * Pred Grad:  -0.011, New P: 0.099
iter 5 loss: 0.035
Actual params: [2.5453, 0.0985]
-Original Grad: 0.001, -lr * Pred Grad:  0.504, New P: 3.049
-Original Grad: 0.001, -lr * Pred Grad:  -0.021, New P: 0.078
iter 6 loss: 0.031
Actual params: [3.0491, 0.0779]
-Original Grad: 0.001, -lr * Pred Grad:  0.406, New P: 3.455
-Original Grad: -0.001, -lr * Pred Grad:  -0.035, New P: 0.043
iter 7 loss: 0.026
Actual params: [3.4547, 0.0428]
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 3.555
-Original Grad: -0.003, -lr * Pred Grad:  -0.030, New P: 0.013
iter 8 loss: 0.019
Actual params: [3.5551, 0.0132]
-Original Grad: -0.001, -lr * Pred Grad:  -0.078, New P: 3.477
-Original Grad: -0.003, -lr * Pred Grad:  -0.018, New P: -0.005
iter 9 loss: 0.018
Actual params: [ 3.4767, -0.0048]
-Original Grad: 0.000, -lr * Pred Grad:  0.120, New P: 3.596
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -0.037
iter 10 loss: 0.019
Actual params: [ 3.5962, -0.0367]
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 3.607
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.043
iter 11 loss: 0.020
Actual params: [ 3.6073, -0.043 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.044, New P: 3.563
-Original Grad: -0.001, -lr * Pred Grad:  -0.004, New P: -0.047
iter 12 loss: 0.021
Actual params: [ 3.5634, -0.0473]
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 3.583
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -0.051
iter 13 loss: 0.021
Actual params: [ 3.5833, -0.0511]
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 3.599
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -0.055
iter 14 loss: 0.021
Actual params: [ 3.5995, -0.0548]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 3.603
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.054
iter 15 loss: 0.022
Actual params: [ 3.6029, -0.0543]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 3.605
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.054
iter 16 loss: 0.022
Actual params: [ 3.6053, -0.0539]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 3.607
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.054
iter 17 loss: 0.022
Actual params: [ 3.6071, -0.0536]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 3.608
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 18 loss: 0.022
Actual params: [ 3.6083, -0.0534]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 3.609
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 19 loss: 0.021
Actual params: [ 3.6092, -0.0533]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 3.610
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 20 loss: 0.021
Actual params: [ 3.6098, -0.0532]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 3.610
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 21 loss: 0.021
Actual params: [ 3.6102, -0.0532]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 3.611
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 22 loss: 0.021
Actual params: [ 3.6105, -0.0531]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 3.611
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 23 loss: 0.021
Actual params: [ 3.6107, -0.0531]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 3.611
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 24 loss: 0.021
Actual params: [ 3.6109, -0.0531]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 3.611
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 25 loss: 0.021
Actual params: [ 3.611 , -0.0531]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 3.611
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 26 loss: 0.021
Actual params: [ 3.6111, -0.0531]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 3.611
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 27 loss: 0.021
Actual params: [ 3.6111, -0.053 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 3.611
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 28 loss: 0.021
Actual params: [ 3.6112, -0.053 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 3.611
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 29 loss: 0.021
Actual params: [ 3.6112, -0.053 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 3.611
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.053
iter 30 loss: 0.021
Actual params: [ 3.6112, -0.053 ]
Target params: [-1.0746]
Actual params: [-0.7899, -0.493 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.789
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.492
iter 0 loss: 0.044
Actual params: [-0.7891, -0.4919]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.787
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.489
iter 1 loss: 0.044
Actual params: [-0.7871, -0.4891]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.782
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.482
iter 2 loss: 0.044
Actual params: [-0.7821, -0.4821]
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: -0.769
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: -0.464
iter 3 loss: 0.044
Actual params: [-0.7691, -0.4644]
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: -0.734
-Original Grad: 0.000, -lr * Pred Grad:  0.045, New P: -0.420
iter 4 loss: 0.044
Actual params: [-0.7341, -0.4199]
-Original Grad: 0.000, -lr * Pred Grad:  0.104, New P: -0.631
-Original Grad: 0.000, -lr * Pred Grad:  0.113, New P: -0.307
iter 5 loss: 0.044
Actual params: [-0.6306, -0.3072]
-Original Grad: 0.000, -lr * Pred Grad:  0.386, New P: -0.244
-Original Grad: 0.000, -lr * Pred Grad:  0.268, New P: -0.039
iter 6 loss: 0.043
Actual params: [-0.2443, -0.0395]
-Original Grad: 0.000, -lr * Pred Grad:  2.003, New P: 1.759
-Original Grad: -0.000, -lr * Pred Grad:  -0.064, New P: -0.103
iter 7 loss: 0.045
Actual params: [ 1.7591, -0.103 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.348, New P: 1.412
-Original Grad: -0.020, -lr * Pred Grad:  -0.159, New P: -0.262
iter 8 loss: 0.035
Actual params: [ 1.4116, -0.2624]
-Original Grad: 0.001, -lr * Pred Grad:  0.641, New P: 2.053
-Original Grad: 0.024, -lr * Pred Grad:  0.016, New P: -0.247
iter 9 loss: 0.061
Actual params: [ 2.0527, -0.2467]
-Original Grad: 0.000, -lr * Pred Grad:  0.969, New P: 3.022
-Original Grad: -0.005, -lr * Pred Grad:  -0.037, New P: -0.284
iter 10 loss: 0.045
Actual params: [ 3.0221, -0.2838]
-Original Grad: 0.000, -lr * Pred Grad:  0.480, New P: 3.502
-Original Grad: 0.007, -lr * Pred Grad:  0.016, New P: -0.268
iter 11 loss: 0.037
Actual params: [ 3.5025, -0.2683]
-Original Grad: 0.000, -lr * Pred Grad:  0.162, New P: 3.664
-Original Grad: 0.002, -lr * Pred Grad:  0.007, New P: -0.262
iter 12 loss: 0.029
Actual params: [ 3.6643, -0.2618]
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: 3.630
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.259
iter 13 loss: 0.027
Actual params: [ 3.6303, -0.2586]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: 3.606
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.256
iter 14 loss: 0.027
Actual params: [ 3.6059, -0.2558]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 3.602
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.254
iter 15 loss: 0.027
Actual params: [ 3.6024, -0.2537]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 3.599
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.252
iter 16 loss: 0.026
Actual params: [ 3.5994, -0.2523]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 3.596
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.251
iter 17 loss: 0.026
Actual params: [ 3.5963, -0.2512]
-Original Grad: -0.000, -lr * Pred Grad:  -0.059, New P: 3.537
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.251
iter 18 loss: 0.026
Actual params: [ 3.5369, -0.251 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: 3.513
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.251
iter 19 loss: 0.027
Actual params: [ 3.5127, -0.2507]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: 3.490
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.251
iter 20 loss: 0.027
Actual params: [ 3.4903, -0.2507]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: 3.465
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: -0.251
iter 21 loss: 0.027
Actual params: [ 3.4649, -0.2507]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: 3.429
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: -0.251
iter 22 loss: 0.027
Actual params: [ 3.4293, -0.251 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: 3.406
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: -0.251
iter 23 loss: 0.028
Actual params: [ 3.4059, -0.2512]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: 3.388
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: -0.251
iter 24 loss: 0.028
Actual params: [ 3.3876, -0.2513]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: 3.372
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: -0.251
iter 25 loss: 0.028
Actual params: [ 3.3723, -0.2515]
-Original Grad: 0.000, -lr * Pred Grad:  0.310, New P: 3.682
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: -0.244
iter 26 loss: 0.028
Actual params: [ 3.682 , -0.2435]
-Original Grad: -0.000, -lr * Pred Grad:  -0.055, New P: 3.627
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.245
iter 27 loss: 0.025
Actual params: [ 3.6269, -0.2453]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: 3.622
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.246
iter 28 loss: 0.025
Actual params: [ 3.6216, -0.2464]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 3.619
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.247
iter 29 loss: 0.025
Actual params: [ 3.6189, -0.2472]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 3.616
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.248
iter 30 loss: 0.026
Actual params: [ 3.6161, -0.2477]
Target params: [-1.0746]
Actual params: [0.3685, 0.155 ]
-Original Grad: -0.682, -lr * Pred Grad:  -0.204, New P: 0.165
-Original Grad: -0.234, -lr * Pred Grad:  -0.244, New P: -0.089
iter 0 loss: 0.337
Actual params: [ 0.1647, -0.0893]
-Original Grad: -0.422, -lr * Pred Grad:  -0.059, New P: 0.106
-Original Grad: -0.155, -lr * Pred Grad:  -0.299, New P: -0.389
iter 1 loss: 0.232
Actual params: [ 0.1059, -0.3888]
-Original Grad: -0.188, -lr * Pred Grad:  0.138, New P: 0.244
-Original Grad: -0.143, -lr * Pred Grad:  -0.517, New P: -0.906
iter 2 loss: 0.163
Actual params: [ 0.2441, -0.9056]
-Original Grad: -0.046, -lr * Pred Grad:  0.103, New P: 0.347
-Original Grad: -0.072, -lr * Pred Grad:  -0.252, New P: -1.158
iter 3 loss: 0.086
Actual params: [ 0.3466, -1.1578]
-Original Grad: -0.024, -lr * Pred Grad:  0.034, New P: 0.380
-Original Grad: -0.038, -lr * Pred Grad:  -0.117, New P: -1.275
iter 4 loss: 0.053
Actual params: [ 0.3801, -1.2749]
-Original Grad: -0.019, -lr * Pred Grad:  -0.040, New P: 0.340
-Original Grad: -0.024, -lr * Pred Grad:  -0.045, New P: -1.320
iter 5 loss: 0.037
Actual params: [ 0.3398, -1.3199]
-Original Grad: -0.013, -lr * Pred Grad:  -0.059, New P: 0.281
-Original Grad: -0.015, -lr * Pred Grad:  -0.038, New P: -1.357
iter 6 loss: 0.028
Actual params: [ 0.2809, -1.3575]
-Original Grad: -0.010, -lr * Pred Grad:  -0.072, New P: 0.209
-Original Grad: -0.012, -lr * Pred Grad:  -0.052, New P: -1.410
iter 7 loss: 0.021
Actual params: [ 0.2093, -1.4097]
-Original Grad: -0.005, -lr * Pred Grad:  -0.059, New P: 0.150
-Original Grad: -0.007, -lr * Pred Grad:  -0.059, New P: -1.468
iter 8 loss: 0.019
Actual params: [ 0.1501, -1.4683]
-Original Grad: -0.002, -lr * Pred Grad:  -0.053, New P: 0.097
-Original Grad: -0.004, -lr * Pred Grad:  -0.055, New P: -1.523
iter 9 loss: 0.020
Actual params: [ 0.0968, -1.5228]
-Original Grad: -0.001, -lr * Pred Grad:  -0.042, New P: 0.055
-Original Grad: -0.002, -lr * Pred Grad:  -0.061, New P: -1.584
iter 10 loss: 0.021
Actual params: [ 0.0547, -1.5842]
-Original Grad: -0.001, -lr * Pred Grad:  -0.056, New P: -0.002
-Original Grad: -0.001, -lr * Pred Grad:  -0.049, New P: -1.633
iter 11 loss: 0.022
Actual params: [-0.0018, -1.6328]
-Original Grad: -0.000, -lr * Pred Grad:  -0.045, New P: -0.047
-Original Grad: -0.000, -lr * Pred Grad:  -0.063, New P: -1.696
iter 12 loss: 0.020
Actual params: [-0.0466, -1.6961]
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: -0.038
-Original Grad: -0.000, -lr * Pred Grad:  -0.063, New P: -1.759
iter 13 loss: 0.019
Actual params: [-0.0378, -1.7595]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.050
-Original Grad: -0.000, -lr * Pred Grad:  -0.056, New P: -1.816
iter 14 loss: 0.019
Actual params: [-0.0505, -1.8158]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.048
-Original Grad: -0.000, -lr * Pred Grad:  -0.093, New P: -1.909
iter 15 loss: 0.019
Actual params: [-0.0476, -1.9085]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.062
-Original Grad: -0.000, -lr * Pred Grad:  -0.197, New P: -2.106
iter 16 loss: 0.019
Actual params: [-0.062 , -2.1057]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -0.067
-Original Grad: -0.000, -lr * Pred Grad:  -0.542, New P: -2.648
iter 17 loss: 0.018
Actual params: [-0.0665, -2.6475]
-Original Grad: -0.000, -lr * Pred Grad:  -0.118, New P: -0.184
-Original Grad: -0.000, -lr * Pred Grad:  -0.782, New P: -3.429
iter 18 loss: 0.020
Actual params: [-0.1842, -3.4292]
-Original Grad: -0.014, -lr * Pred Grad:  -0.177, New P: -0.361
-Original Grad: 0.004, -lr * Pred Grad:  -0.407, New P: -3.836
iter 19 loss: 0.027
Actual params: [-0.3612, -3.8358]
-Original Grad: -0.004, -lr * Pred Grad:  0.006, New P: -0.355
-Original Grad: 0.002, -lr * Pred Grad:  0.089, New P: -3.747
iter 20 loss: 0.022
Actual params: [-0.3554, -3.747 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.029, New P: -0.384
-Original Grad: -0.000, -lr * Pred Grad:  -0.086, New P: -3.833
iter 21 loss: 0.022
Actual params: [-0.3841, -3.8332]
-Original Grad: -0.001, -lr * Pred Grad:  0.007, New P: -0.377
-Original Grad: 0.001, -lr * Pred Grad:  0.096, New P: -3.737
iter 22 loss: 0.021
Actual params: [-0.3766, -3.7374]
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: -0.375
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -3.752
iter 23 loss: 0.020
Actual params: [-0.3754, -3.7519]
-Original Grad: -0.001, -lr * Pred Grad:  -0.025, New P: -0.401
-Original Grad: -0.000, -lr * Pred Grad:  -0.101, New P: -3.853
iter 24 loss: 0.021
Actual params: [-0.4008, -3.8531]
-Original Grad: 0.000, -lr * Pred Grad:  0.089, New P: -0.311
-Original Grad: 0.001, -lr * Pred Grad:  0.454, New P: -3.399
iter 25 loss: 0.020
Actual params: [-0.3114, -3.399 ]
-Original Grad: 0.002, -lr * Pred Grad:  -0.067, New P: -0.379
-Original Grad: -0.001, -lr * Pred Grad:  -0.390, New P: -3.789
iter 26 loss: 0.018
Actual params: [-0.3788, -3.7886]
-Original Grad: -0.000, -lr * Pred Grad:  0.127, New P: -0.252
-Original Grad: 0.000, -lr * Pred Grad:  0.518, New P: -3.270
iter 27 loss: 0.021
Actual params: [-0.2519, -3.2705]
-Original Grad: 0.002, -lr * Pred Grad:  -0.185, New P: -0.437
-Original Grad: -0.001, -lr * Pred Grad:  -0.735, New P: -4.006
iter 28 loss: 0.019
Actual params: [-0.4366, -4.0057]
-Original Grad: 0.000, -lr * Pred Grad:  0.062, New P: -0.375
-Original Grad: 0.001, -lr * Pred Grad:  0.393, New P: -3.613
iter 29 loss: 0.019
Actual params: [-0.3749, -3.613 ]
-Original Grad: 0.003, -lr * Pred Grad:  -0.008, New P: -0.383
-Original Grad: -0.001, -lr * Pred Grad:  -0.221, New P: -3.834
iter 30 loss: 0.019
Actual params: [-0.3826, -3.8338]
