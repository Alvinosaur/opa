Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.550, -lr * Pred Grad: 0.053, New P: 0.647
-Original Grad: -1.035, -lr * Pred Grad: -0.046, New P: 0.548
iter 0 loss: 0.351
Actual params: [0.6468, 0.548 ]
-Original Grad: 0.727, -lr * Pred Grad: 0.066, New P: 0.713
-Original Grad: -0.975, -lr * Pred Grad: -0.063, New P: 0.485
iter 1 loss: 0.331
Actual params: [0.7131, 0.4851]
-Original Grad: 0.548, -lr * Pred Grad: 0.068, New P: 0.782
-Original Grad: -0.694, -lr * Pred Grad: -0.066, New P: 0.420
iter 2 loss: 0.301
Actual params: [0.7816, 0.4196]
-Original Grad: 0.707, -lr * Pred Grad: 0.069, New P: 0.850
-Original Grad: -0.639, -lr * Pred Grad: -0.066, New P: 0.354
iter 3 loss: 0.275
Actual params: [0.8504, 0.3536]
-Original Grad: 0.549, -lr * Pred Grad: 0.069, New P: 0.919
-Original Grad: 0.067, -lr * Pred Grad: -0.066, New P: 0.287
iter 4 loss: 0.227
Actual params: [0.9192, 0.2875]
-Original Grad: 0.673, -lr * Pred Grad: 0.069, New P: 0.988
-Original Grad: 0.210, -lr * Pred Grad: -0.066, New P: 0.221
iter 5 loss: 0.149
Actual params: [0.988 , 0.2213]
-Original Grad: 0.395, -lr * Pred Grad: 0.069, New P: 1.057
-Original Grad: 0.338, -lr * Pred Grad: -0.066, New P: 0.155
iter 6 loss: 0.072
Actual params: [1.0567, 0.1552]
-Original Grad: -0.152, -lr * Pred Grad: 0.068, New P: 1.125
-Original Grad: 0.263, -lr * Pred Grad: -0.066, New P: 0.089
iter 7 loss: 0.073
Actual params: [1.1251, 0.089 ]
-Original Grad: 0.390, -lr * Pred Grad: 0.068, New P: 1.193
-Original Grad: 0.192, -lr * Pred Grad: -0.066, New P: 0.023
iter 8 loss: 0.101
Actual params: [1.1933, 0.0229]
-Original Grad: -0.532, -lr * Pred Grad: 0.067, New P: 1.261
-Original Grad: 0.834, -lr * Pred Grad: -0.066, New P: -0.043
iter 9 loss: 0.154
Actual params: [ 1.2605, -0.0433]
-Original Grad: -0.615, -lr * Pred Grad: 0.065, New P: 1.325
-Original Grad: 2.771, -lr * Pred Grad: -0.066, New P: -0.109
iter 10 loss: 0.201
Actual params: [ 1.3254, -0.1094]
-Original Grad: 0.160, -lr * Pred Grad: 0.064, New P: 1.389
-Original Grad: 3.699, -lr * Pred Grad: -0.066, New P: -0.176
iter 11 loss: 0.247
Actual params: [ 1.3889, -0.1756]
-Original Grad: 0.633, -lr * Pred Grad: 0.064, New P: 1.452
-Original Grad: 2.429, -lr * Pred Grad: -0.066, New P: -0.242
iter 12 loss: 0.285
Actual params: [ 1.4524, -0.2417]
-Original Grad: 0.298, -lr * Pred Grad: 0.061, New P: 1.514
-Original Grad: 1.843, -lr * Pred Grad: -0.066, New P: -0.308
iter 13 loss: 0.313
Actual params: [ 1.5135, -0.3077]
-Original Grad: 0.139, -lr * Pred Grad: 0.059, New P: 1.572
-Original Grad: 1.350, -lr * Pred Grad: -0.065, New P: -0.373
iter 14 loss: 0.333
Actual params: [ 1.5723, -0.3725]
-Original Grad: 0.393, -lr * Pred Grad: 0.059, New P: 1.631
-Original Grad: 1.652, -lr * Pred Grad: -0.059, New P: -0.432
iter 15 loss: 0.347
Actual params: [ 1.6313, -0.4315]
-Original Grad: 0.190, -lr * Pred Grad: 0.055, New P: 1.686
-Original Grad: 1.698, -lr * Pred Grad: -0.043, New P: -0.474
iter 16 loss: 0.093
Actual params: [ 1.6861, -0.4742]
-Original Grad: 0.192, -lr * Pred Grad: 0.054, New P: 1.740
-Original Grad: 1.568, -lr * Pred Grad: -0.013, New P: -0.487
iter 17 loss: 0.089
Actual params: [ 1.7399, -0.4872]
-Original Grad: 0.223, -lr * Pred Grad: 0.051, New P: 1.791
-Original Grad: 1.948, -lr * Pred Grad: 0.029, New P: -0.458
iter 18 loss: 0.090
Actual params: [ 1.7906, -0.4583]
-Original Grad: 0.073, -lr * Pred Grad: 0.048, New P: 1.839
-Original Grad: 1.675, -lr * Pred Grad: 0.059, New P: -0.400
iter 19 loss: 0.382
Actual params: [ 1.8385, -0.3998]
-Original Grad: 0.053, -lr * Pred Grad: 0.046, New P: 1.884
-Original Grad: 1.519, -lr * Pred Grad: 0.067, New P: -0.333
iter 20 loss: 0.391
Actual params: [ 1.8841, -0.3326]
-Original Grad: 0.029, -lr * Pred Grad: 0.044, New P: 1.928
-Original Grad: 1.321, -lr * Pred Grad: 0.069, New P: -0.264
iter 21 loss: 0.395
Actual params: [ 1.9283, -0.2639]
-Original Grad: 0.043, -lr * Pred Grad: 0.044, New P: 1.972
-Original Grad: 1.202, -lr * Pred Grad: 0.069, New P: -0.195
iter 22 loss: 0.397
Actual params: [ 1.9722, -0.1951]
-Original Grad: -0.018, -lr * Pred Grad: 0.042, New P: 2.014
-Original Grad: 0.879, -lr * Pred Grad: 0.069, New P: -0.126
iter 23 loss: 0.393
Actual params: [ 2.0144, -0.1262]
-Original Grad: -0.031, -lr * Pred Grad: 0.041, New P: 2.056
-Original Grad: 0.935, -lr * Pred Grad: 0.069, New P: -0.057
iter 24 loss: 0.395
Actual params: [ 2.0557, -0.0574]
-Original Grad: -0.097, -lr * Pred Grad: 0.039, New P: 2.095
-Original Grad: 1.040, -lr * Pred Grad: 0.069, New P: 0.012
iter 25 loss: 0.393
Actual params: [2.0947, 0.0115]
-Original Grad: -0.092, -lr * Pred Grad: 0.038, New P: 2.133
-Original Grad: 0.857, -lr * Pred Grad: 0.069, New P: 0.080
iter 26 loss: 0.395
Actual params: [2.133 , 0.0804]
-Original Grad: -0.151, -lr * Pred Grad: 0.037, New P: 2.170
-Original Grad: 1.081, -lr * Pred Grad: 0.069, New P: 0.149
iter 27 loss: 0.394
Actual params: [2.1697, 0.1493]
-Original Grad: -0.134, -lr * Pred Grad: 0.038, New P: 2.208
-Original Grad: 0.727, -lr * Pred Grad: 0.069, New P: 0.218
iter 28 loss: 0.398
Actual params: [2.2076, 0.2181]
-Original Grad: -0.136, -lr * Pred Grad: 0.040, New P: 2.248
-Original Grad: 0.491, -lr * Pred Grad: 0.069, New P: 0.287
iter 29 loss: 0.405
Actual params: [2.2477, 0.287 ]
-Original Grad: -0.165, -lr * Pred Grad: 0.043, New P: 2.291
-Original Grad: 0.563, -lr * Pred Grad: 0.069, New P: 0.356
iter 30 loss: 0.408
Actual params: [2.2906, 0.3559]
-Original Grad: -0.165, -lr * Pred Grad: 0.046, New P: 2.337
-Original Grad: 0.349, -lr * Pred Grad: 0.069, New P: 0.425
iter 31 loss: 0.406
Actual params: [2.3367, 0.4248]
-Original Grad: -0.124, -lr * Pred Grad: 0.050, New P: 2.387
-Original Grad: -0.007, -lr * Pred Grad: 0.069, New P: 0.494
iter 32 loss: 0.404
Actual params: [2.387 , 0.4936]
-Original Grad: -0.097, -lr * Pred Grad: 0.053, New P: 2.440
-Original Grad: -0.793, -lr * Pred Grad: 0.069, New P: 0.563
iter 33 loss: 0.398
Actual params: [2.4402, 0.5625]
-Original Grad: -0.067, -lr * Pred Grad: 0.056, New P: 2.496
-Original Grad: -0.953, -lr * Pred Grad: 0.069, New P: 0.631
iter 34 loss: 0.394
Actual params: [2.496 , 0.6314]
-Original Grad: -0.069, -lr * Pred Grad: 0.056, New P: 2.552
-Original Grad: -0.745, -lr * Pred Grad: 0.069, New P: 0.700
iter 35 loss: 0.392
Actual params: [2.552 , 0.7003]
-Original Grad: -0.050, -lr * Pred Grad: 0.057, New P: 2.609
-Original Grad: -0.682, -lr * Pred Grad: 0.069, New P: 0.769
iter 36 loss: 0.402
Actual params: [2.609 , 0.7691]
-Original Grad: -0.072, -lr * Pred Grad: 0.056, New P: 2.665
-Original Grad: -0.514, -lr * Pred Grad: 0.069, New P: 0.838
iter 37 loss: 0.425
Actual params: [2.6647, 0.838 ]
-Original Grad: -0.072, -lr * Pred Grad: 0.056, New P: 2.721
-Original Grad: -0.423, -lr * Pred Grad: 0.069, New P: 0.907
iter 38 loss: 0.453
Actual params: [2.7206, 0.9067]
-Original Grad: -0.068, -lr * Pred Grad: 0.055, New P: 2.775
-Original Grad: -0.385, -lr * Pred Grad: 0.069, New P: 0.975
iter 39 loss: 0.472
Actual params: [2.7755, 0.9753]
-Original Grad: -0.006, -lr * Pred Grad: 0.057, New P: 2.833
-Original Grad: -0.318, -lr * Pred Grad: 0.068, New P: 1.044
iter 40 loss: 0.481
Actual params: [2.8329, 1.0438]
-Original Grad: -0.008, -lr * Pred Grad: 0.057, New P: 2.890
-Original Grad: -0.175, -lr * Pred Grad: 0.068, New P: 1.112
iter 41 loss: 0.489
Actual params: [2.8899, 1.1123]
-Original Grad: -0.040, -lr * Pred Grad: 0.057, New P: 2.947
-Original Grad: -0.274, -lr * Pred Grad: 0.068, New P: 1.181
iter 42 loss: 0.494
Actual params: [2.9473, 1.1807]
-Original Grad: -0.047, -lr * Pred Grad: 0.056, New P: 3.003
-Original Grad: -0.201, -lr * Pred Grad: 0.068, New P: 1.249
iter 43 loss: 0.497
Actual params: [3.0034, 1.249 ]
-Original Grad: -0.005, -lr * Pred Grad: 0.058, New P: 3.061
-Original Grad: -0.155, -lr * Pred Grad: 0.068, New P: 1.317
iter 44 loss: 0.499
Actual params: [3.061 , 1.3169]
-Original Grad: -0.003, -lr * Pred Grad: 0.057, New P: 3.118
-Original Grad: -0.066, -lr * Pred Grad: 0.067, New P: 1.384
iter 45 loss: 0.496
Actual params: [3.118 , 1.3841]
-Original Grad: 0.019, -lr * Pred Grad: 0.059, New P: 3.177
-Original Grad: 0.013, -lr * Pred Grad: 0.067, New P: 1.451
iter 46 loss: 0.436
Actual params: [3.1766, 1.4507]
-Original Grad: 0.022, -lr * Pred Grad: 0.058, New P: 3.235
-Original Grad: -0.002, -lr * Pred Grad: 0.066, New P: 1.517
iter 47 loss: 0.428
Actual params: [3.2348, 1.517 ]
-Original Grad: -0.008, -lr * Pred Grad: 0.058, New P: 3.293
-Original Grad: -0.007, -lr * Pred Grad: 0.066, New P: 1.583
iter 48 loss: 0.426
Actual params: [3.293 , 1.5827]
-Original Grad: -0.001, -lr * Pred Grad: 0.058, New P: 3.351
-Original Grad: 0.055, -lr * Pred Grad: 0.065, New P: 1.648
iter 49 loss: 0.426
Actual params: [3.3509, 1.6479]
-Original Grad: -0.019, -lr * Pred Grad: 0.057, New P: 3.408
-Original Grad: 0.047, -lr * Pred Grad: 0.065, New P: 1.713
iter 50 loss: 0.427
Actual params: [3.4083, 1.7126]
-Original Grad: -0.010, -lr * Pred Grad: 0.057, New P: 3.466
-Original Grad: 0.048, -lr * Pred Grad: 0.064, New P: 1.777
iter 51 loss: 0.430
Actual params: [3.4656, 1.7768]
-Original Grad: -0.016, -lr * Pred Grad: 0.057, New P: 3.522
-Original Grad: 0.078, -lr * Pred Grad: 0.064, New P: 1.841
iter 52 loss: 0.433
Actual params: [3.5224, 1.8406]
-Original Grad: -0.032, -lr * Pred Grad: 0.056, New P: 3.579
-Original Grad: 0.040, -lr * Pred Grad: 0.063, New P: 1.904
iter 53 loss: 0.437
Actual params: [3.5788, 1.9038]
-Original Grad: -0.022, -lr * Pred Grad: 0.056, New P: 3.635
-Original Grad: 0.041, -lr * Pred Grad: 0.062, New P: 1.966
iter 54 loss: 0.441
Actual params: [3.6351, 1.966 ]
-Original Grad: -0.018, -lr * Pred Grad: 0.056, New P: 3.691
-Original Grad: 0.048, -lr * Pred Grad: 0.061, New P: 2.027
iter 55 loss: 0.446
Actual params: [3.6913, 2.0267]
-Original Grad: -0.009, -lr * Pred Grad: 0.057, New P: 3.748
-Original Grad: 0.063, -lr * Pred Grad: 0.059, New P: 2.085
iter 56 loss: 0.450
Actual params: [3.7478, 2.0855]
-Original Grad: -0.030, -lr * Pred Grad: 0.056, New P: 3.804
-Original Grad: 0.010, -lr * Pred Grad: 0.056, New P: 2.141
iter 57 loss: 0.453
Actual params: [3.8038, 2.1411]
-Original Grad: -0.027, -lr * Pred Grad: 0.056, New P: 3.860
-Original Grad: 0.007, -lr * Pred Grad: 0.052, New P: 2.193
iter 58 loss: 0.456
Actual params: [3.8599, 2.1932]
-Original Grad: -0.014, -lr * Pred Grad: 0.056, New P: 3.916
-Original Grad: 0.026, -lr * Pred Grad: 0.049, New P: 2.242
iter 59 loss: 0.456
Actual params: [3.916 , 2.2423]
-Original Grad: -0.014, -lr * Pred Grad: 0.056, New P: 3.972
-Original Grad: 0.020, -lr * Pred Grad: 0.047, New P: 2.290
iter 60 loss: 0.457
Actual params: [3.9725, 2.2896]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.564, -lr * Pred Grad: 0.053, New P: 0.647
-Original Grad: -0.070, -lr * Pred Grad: -0.023, New P: 0.571
iter 0 loss: 0.384
Actual params: [0.6468, 0.5712]
-Original Grad: 0.759, -lr * Pred Grad: 0.066, New P: 0.713
-Original Grad: 0.052, -lr * Pred Grad: -0.058, New P: 0.514
iter 1 loss: 0.364
Actual params: [0.7132, 0.5136]
-Original Grad: 1.001, -lr * Pred Grad: 0.068, New P: 0.782
-Original Grad: 0.263, -lr * Pred Grad: -0.065, New P: 0.449
iter 2 loss: 0.327
Actual params: [0.7816, 0.4488]
-Original Grad: 0.873, -lr * Pred Grad: 0.069, New P: 0.850
-Original Grad: 0.260, -lr * Pred Grad: -0.066, New P: 0.383
iter 3 loss: 0.294
Actual params: [0.8504, 0.3829]
-Original Grad: 0.513, -lr * Pred Grad: 0.069, New P: 0.919
-Original Grad: 0.397, -lr * Pred Grad: -0.065, New P: 0.318
iter 4 loss: 0.261
Actual params: [0.9193, 0.3176]
-Original Grad: 0.113, -lr * Pred Grad: 0.069, New P: 0.988
-Original Grad: 0.118, -lr * Pred Grad: -0.061, New P: 0.257
iter 5 loss: 0.253
Actual params: [0.9881, 0.2568]
-Original Grad: 0.182, -lr * Pred Grad: 0.069, New P: 1.057
-Original Grad: 0.315, -lr * Pred Grad: -0.040, New P: 0.217
iter 6 loss: 0.252
Actual params: [1.0568, 0.2165]
-Original Grad: 0.045, -lr * Pred Grad: 0.069, New P: 1.125
-Original Grad: 0.201, -lr * Pred Grad: 0.000, New P: 0.217
iter 7 loss: 0.249
Actual params: [1.1253, 0.2166]
-Original Grad: -0.075, -lr * Pred Grad: 0.068, New P: 1.193
-Original Grad: 0.130, -lr * Pred Grad: 0.030, New P: 0.246
iter 8 loss: 0.244
Actual params: [1.1933, 0.2462]
-Original Grad: -0.151, -lr * Pred Grad: 0.067, New P: 1.260
-Original Grad: 0.247, -lr * Pred Grad: 0.050, New P: 0.296
iter 9 loss: 0.248
Actual params: [1.2603, 0.2964]
-Original Grad: -0.177, -lr * Pred Grad: 0.065, New P: 1.326
-Original Grad: 0.319, -lr * Pred Grad: 0.062, New P: 0.358
iter 10 loss: 0.240
Actual params: [1.3258, 0.3583]
-Original Grad: -0.176, -lr * Pred Grad: 0.063, New P: 1.388
-Original Grad: 0.368, -lr * Pred Grad: 0.063, New P: 0.421
iter 11 loss: 0.222
Actual params: [1.3883, 0.4212]
-Original Grad: -0.095, -lr * Pred Grad: 0.058, New P: 1.446
-Original Grad: 0.177, -lr * Pred Grad: 0.059, New P: 0.480
iter 12 loss: 0.187
Actual params: [1.4458, 0.4804]
-Original Grad: -0.075, -lr * Pred Grad: 0.052, New P: 1.498
-Original Grad: 0.156, -lr * Pred Grad: 0.053, New P: 0.533
iter 13 loss: 0.160
Actual params: [1.4977, 0.5334]
-Original Grad: -0.057, -lr * Pred Grad: 0.047, New P: 1.544
-Original Grad: 0.141, -lr * Pred Grad: 0.048, New P: 0.582
iter 14 loss: 0.141
Actual params: [1.5443, 0.5819]
-Original Grad: -0.028, -lr * Pred Grad: 0.044, New P: 1.588
-Original Grad: -0.011, -lr * Pred Grad: 0.036, New P: 0.618
iter 15 loss: 0.118
Actual params: [1.5879, 0.618 ]
-Original Grad: -0.001, -lr * Pred Grad: 0.042, New P: 1.630
-Original Grad: -0.072, -lr * Pred Grad: 0.007, New P: 0.625
iter 16 loss: 0.116
Actual params: [1.6301, 0.625 ]
-Original Grad: 0.001, -lr * Pred Grad: 0.041, New P: 1.672
-Original Grad: -0.195, -lr * Pred Grad: -0.008, New P: 0.617
iter 17 loss: 0.123
Actual params: [1.6716, 0.6172]
-Original Grad: -0.028, -lr * Pred Grad: 0.040, New P: 1.712
-Original Grad: 0.070, -lr * Pred Grad: -0.020, New P: 0.597
iter 18 loss: 0.135
Actual params: [1.712 , 0.5972]
-Original Grad: -0.036, -lr * Pred Grad: 0.040, New P: 1.752
-Original Grad: -0.023, -lr * Pred Grad: -0.037, New P: 0.560
iter 19 loss: 0.671
Actual params: [1.752 , 0.5601]
-Original Grad: -0.090, -lr * Pred Grad: 0.039, New P: 1.791
-Original Grad: 0.292, -lr * Pred Grad: -0.017, New P: 0.544
iter 20 loss: 0.677
Actual params: [1.7906, 0.5436]
-Original Grad: -0.096, -lr * Pred Grad: 0.038, New P: 1.829
-Original Grad: 0.327, -lr * Pred Grad: 0.030, New P: 0.573
iter 21 loss: 0.690
Actual params: [1.8291, 0.5734]
-Original Grad: -0.058, -lr * Pred Grad: 0.040, New P: 1.869
-Original Grad: 0.189, -lr * Pred Grad: 0.060, New P: 0.633
iter 22 loss: 0.745
Actual params: [1.8694, 0.633 ]
-Original Grad: -0.063, -lr * Pred Grad: 0.043, New P: 1.913
-Original Grad: -0.304, -lr * Pred Grad: 0.040, New P: 0.673
iter 23 loss: 0.736
Actual params: [1.9127, 0.6731]
-Original Grad: -0.056, -lr * Pred Grad: 0.047, New P: 1.960
-Original Grad: -1.139, -lr * Pred Grad: -0.022, New P: 0.651
iter 24 loss: 0.204
Actual params: [1.9602, 0.6514]
-Original Grad: -0.021, -lr * Pred Grad: 0.052, New P: 2.013
-Original Grad: -0.584, -lr * Pred Grad: -0.041, New P: 0.610
iter 25 loss: 0.160
Actual params: [2.0126, 0.6104]
-Original Grad: -0.012, -lr * Pred Grad: 0.056, New P: 2.069
-Original Grad: 0.247, -lr * Pred Grad: -0.047, New P: 0.563
iter 26 loss: 0.817
Actual params: [2.0686, 0.5634]
-Original Grad: -0.012, -lr * Pred Grad: 0.058, New P: 2.126
-Original Grad: 0.449, -lr * Pred Grad: -0.048, New P: 0.515
iter 27 loss: 0.846
Actual params: [2.1265, 0.5151]
-Original Grad: -0.023, -lr * Pred Grad: 0.058, New P: 2.184
-Original Grad: 0.528, -lr * Pred Grad: -0.042, New P: 0.473
iter 28 loss: 0.816
Actual params: [2.1842, 0.4734]
-Original Grad: -0.015, -lr * Pred Grad: 0.057, New P: 2.242
-Original Grad: 0.586, -lr * Pred Grad: -0.015, New P: 0.459
iter 29 loss: 0.795
Actual params: [2.2417, 0.4587]
-Original Grad: -0.040, -lr * Pred Grad: 0.056, New P: 2.298
-Original Grad: 0.511, -lr * Pred Grad: 0.032, New P: 0.490
iter 30 loss: 0.793
Actual params: [2.2976, 0.4904]
-Original Grad: -0.044, -lr * Pred Grad: 0.055, New P: 2.353
-Original Grad: 0.443, -lr * Pred Grad: 0.061, New P: 0.551
iter 31 loss: 0.839
Actual params: [2.3529, 0.5509]
-Original Grad: -0.044, -lr * Pred Grad: 0.054, New P: 2.407
-Original Grad: 0.387, -lr * Pred Grad: 0.067, New P: 0.618
iter 32 loss: 0.896
Actual params: [2.4073, 0.6179]
-Original Grad: -0.031, -lr * Pred Grad: 0.055, New P: 2.462
-Original Grad: -0.201, -lr * Pred Grad: 0.060, New P: 0.678
iter 33 loss: 0.849
Actual params: [2.4623, 0.6783]
-Original Grad: -0.282, -lr * Pred Grad: 0.046, New P: 2.508
-Original Grad: -2.724, -lr * Pred Grad: -0.018, New P: 0.660
iter 34 loss: 0.202
Actual params: [2.508 , 0.6598]
-Original Grad: -0.071, -lr * Pred Grad: 0.053, New P: 2.561
-Original Grad: -0.450, -lr * Pred Grad: -0.038, New P: 0.622
iter 35 loss: 0.182
Actual params: [2.5608, 0.6221]
-Original Grad: -0.065, -lr * Pred Grad: 0.049, New P: 2.609
-Original Grad: -0.575, -lr * Pred Grad: -0.045, New P: 0.577
iter 36 loss: 0.843
Actual params: [2.6095, 0.5769]
-Original Grad: -0.037, -lr * Pred Grad: 0.056, New P: 2.666
-Original Grad: 0.500, -lr * Pred Grad: -0.048, New P: 0.529
iter 37 loss: 0.893
Actual params: [2.6659, 0.5285]
-Original Grad: -0.056, -lr * Pred Grad: 0.051, New P: 2.717
-Original Grad: 0.492, -lr * Pred Grad: -0.049, New P: 0.479
iter 38 loss: 0.933
Actual params: [2.7168, 0.4794]
-Original Grad: -0.065, -lr * Pred Grad: 0.056, New P: 2.773
-Original Grad: 0.597, -lr * Pred Grad: -0.043, New P: 0.436
iter 39 loss: 0.965
Actual params: [2.773 , 0.4361]
-Original Grad: -0.052, -lr * Pred Grad: 0.051, New P: 2.824
-Original Grad: 0.697, -lr * Pred Grad: -0.021, New P: 0.415
iter 40 loss: 0.982
Actual params: [2.824 , 0.4147]
-Original Grad: -0.079, -lr * Pred Grad: 0.055, New P: 2.879
-Original Grad: 0.684, -lr * Pred Grad: 0.022, New P: 0.437
iter 41 loss: 0.982
Actual params: [2.879 , 0.4367]
-Original Grad: -0.073, -lr * Pred Grad: 0.050, New P: 2.929
-Original Grad: 0.614, -lr * Pred Grad: 0.056, New P: 0.493
iter 42 loss: 0.991
Actual params: [2.9293, 0.4926]
-Original Grad: -0.075, -lr * Pred Grad: 0.054, New P: 2.984
-Original Grad: 0.720, -lr * Pred Grad: 0.067, New P: 0.559
iter 43 loss: 0.974
Actual params: [2.9837, 0.5592]
-Original Grad: -0.105, -lr * Pred Grad: 0.049, New P: 3.033
-Original Grad: 0.705, -lr * Pred Grad: 0.068, New P: 0.627
iter 44 loss: 0.915
Actual params: [3.0326, 0.6274]
-Original Grad: -0.773, -lr * Pred Grad: 0.001, New P: 3.033
-Original Grad: -3.886, -lr * Pred Grad: 0.034, New P: 0.661
iter 45 loss: 0.768
Actual params: [3.0332, 0.661 ]
-Original Grad: -0.591, -lr * Pred Grad: -0.006, New P: 3.028
-Original Grad: -4.960, -lr * Pred Grad: -0.020, New P: 0.641
iter 46 loss: 0.173
Actual params: [3.0276, 0.6413]
-Original Grad: -0.649, -lr * Pred Grad: -0.034, New P: 2.993
-Original Grad: -1.723, -lr * Pred Grad: -0.035, New P: 0.606
iter 47 loss: 0.131
Actual params: [2.9935, 0.6063]
-Original Grad: -0.564, -lr * Pred Grad: -0.044, New P: 2.949
-Original Grad: -2.557, -lr * Pred Grad: -0.045, New P: 0.561
iter 48 loss: 0.881
Actual params: [2.9491, 0.5614]
-Original Grad: -0.110, -lr * Pred Grad: -0.047, New P: 2.902
-Original Grad: 0.576, -lr * Pred Grad: -0.049, New P: 0.513
iter 49 loss: 0.914
Actual params: [2.9022, 0.5129]
-Original Grad: -0.080, -lr * Pred Grad: -0.048, New P: 2.854
-Original Grad: 0.483, -lr * Pred Grad: -0.051, New P: 0.462
iter 50 loss: 0.958
Actual params: [2.8545, 0.462 ]
-Original Grad: -0.082, -lr * Pred Grad: -0.048, New P: 2.807
-Original Grad: 0.471, -lr * Pred Grad: -0.049, New P: 0.413
iter 51 loss: 0.984
Actual params: [2.8065, 0.4135]
-Original Grad: -0.036, -lr * Pred Grad: -0.048, New P: 2.758
-Original Grad: 0.841, -lr * Pred Grad: -0.041, New P: 0.372
iter 52 loss: 0.975
Actual params: [2.7585, 0.3721]
-Original Grad: -0.091, -lr * Pred Grad: -0.048, New P: 2.710
-Original Grad: 0.543, -lr * Pred Grad: -0.018, New P: 0.354
iter 53 loss: 0.434
Actual params: [2.7105, 0.3537]
-Original Grad: -0.086, -lr * Pred Grad: -0.048, New P: 2.662
-Original Grad: 0.607, -lr * Pred Grad: 0.024, New P: 0.378
iter 54 loss: 0.328
Actual params: [2.6624, 0.3777]
-Original Grad: -0.057, -lr * Pred Grad: -0.048, New P: 2.614
-Original Grad: 0.730, -lr * Pred Grad: 0.057, New P: 0.435
iter 55 loss: 0.341
Actual params: [2.6144, 0.4346]
-Original Grad: -0.058, -lr * Pred Grad: -0.049, New P: 2.566
-Original Grad: 0.603, -lr * Pred Grad: 0.067, New P: 0.501
iter 56 loss: 0.870
Actual params: [2.5658, 0.5013]
-Original Grad: -0.042, -lr * Pred Grad: -0.050, New P: 2.515
-Original Grad: 0.569, -lr * Pred Grad: 0.068, New P: 0.569
iter 57 loss: 0.934
Actual params: [2.5155, 0.5694]
-Original Grad: -0.043, -lr * Pred Grad: -0.052, New P: 2.463
-Original Grad: 0.558, -lr * Pred Grad: 0.068, New P: 0.637
iter 58 loss: 0.897
Actual params: [2.4634, 0.6374]
-Original Grad: -0.112, -lr * Pred Grad: -0.053, New P: 2.411
-Original Grad: -0.919, -lr * Pred Grad: 0.062, New P: 0.700
iter 59 loss: 0.791
Actual params: [2.4109, 0.6997]
-Original Grad: -0.664, -lr * Pred Grad: -0.053, New P: 2.358
-Original Grad: -5.545, -lr * Pred Grad: -0.017, New P: 0.682
iter 60 loss: 0.233
Actual params: [2.3583, 0.6825]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.001, -lr * Pred Grad: 0.015, New P: 0.609
-Original Grad: -0.081, -lr * Pred Grad: -0.027, New P: 0.567
iter 0 loss: 0.244
Actual params: [0.6095, 0.5669]
-Original Grad: -0.015, -lr * Pred Grad: -0.041, New P: 0.568
-Original Grad: -0.075, -lr * Pred Grad: -0.059, New P: 0.508
iter 1 loss: 0.233
Actual params: [0.5681, 0.5082]
-Original Grad: -0.011, -lr * Pred Grad: -0.062, New P: 0.506
-Original Grad: -0.042, -lr * Pred Grad: -0.065, New P: 0.443
iter 2 loss: 0.187
Actual params: [0.5062, 0.4433]
-Original Grad: 0.245, -lr * Pred Grad: -0.065, New P: 0.441
-Original Grad: -0.044, -lr * Pred Grad: -0.066, New P: 0.377
iter 3 loss: 0.151
Actual params: [0.4408, 0.3774]
-Original Grad: 0.486, -lr * Pred Grad: -0.066, New P: 0.375
-Original Grad: -0.015, -lr * Pred Grad: -0.066, New P: 0.311
iter 4 loss: 0.163
Actual params: [0.3748, 0.3113]
-Original Grad: 0.579, -lr * Pred Grad: -0.065, New P: 0.310
-Original Grad: -0.002, -lr * Pred Grad: -0.066, New P: 0.245
iter 5 loss: 0.216
Actual params: [0.3097, 0.2451]
-Original Grad: 0.585, -lr * Pred Grad: -0.059, New P: 0.251
-Original Grad: 0.006, -lr * Pred Grad: -0.066, New P: 0.179
iter 6 loss: 0.255
Actual params: [0.2511, 0.179 ]
-Original Grad: 0.501, -lr * Pred Grad: -0.030, New P: 0.222
-Original Grad: 0.015, -lr * Pred Grad: -0.066, New P: 0.113
iter 7 loss: 0.289
Actual params: [0.2216, 0.1129]
-Original Grad: 0.516, -lr * Pred Grad: 0.027, New P: 0.249
-Original Grad: 0.034, -lr * Pred Grad: -0.066, New P: 0.047
iter 8 loss: 0.304
Actual params: [0.2486, 0.0467]
-Original Grad: 0.474, -lr * Pred Grad: 0.060, New P: 0.309
-Original Grad: 0.072, -lr * Pred Grad: -0.066, New P: -0.019
iter 9 loss: 0.295
Actual params: [ 0.3087, -0.0194]
-Original Grad: 0.716, -lr * Pred Grad: 0.067, New P: 0.376
-Original Grad: 0.062, -lr * Pred Grad: -0.066, New P: -0.086
iter 10 loss: 0.269
Actual params: [ 0.376 , -0.0856]
-Original Grad: 0.627, -lr * Pred Grad: 0.068, New P: 0.444
-Original Grad: 0.055, -lr * Pred Grad: -0.066, New P: -0.152
iter 11 loss: 0.232
Actual params: [ 0.444 , -0.1517]
-Original Grad: 0.369, -lr * Pred Grad: 0.067, New P: 0.511
-Original Grad: 0.055, -lr * Pred Grad: -0.066, New P: -0.218
iter 12 loss: 0.195
Actual params: [ 0.5113, -0.2179]
-Original Grad: 0.521, -lr * Pred Grad: 0.067, New P: 0.578
-Original Grad: 0.051, -lr * Pred Grad: -0.066, New P: -0.284
iter 13 loss: 0.150
Actual params: [ 0.5783, -0.284 ]
-Original Grad: 0.163, -lr * Pred Grad: 0.065, New P: 0.643
-Original Grad: 0.050, -lr * Pred Grad: -0.066, New P: -0.350
iter 14 loss: 0.132
Actual params: [ 0.6431, -0.3502]
-Original Grad: -0.015, -lr * Pred Grad: 0.059, New P: 0.702
-Original Grad: 0.043, -lr * Pred Grad: -0.066, New P: -0.416
iter 15 loss: 0.129
Actual params: [ 0.7018, -0.4163]
-Original Grad: -0.033, -lr * Pred Grad: 0.053, New P: 0.755
-Original Grad: 0.017, -lr * Pred Grad: -0.066, New P: -0.482
iter 16 loss: 0.130
Actual params: [ 0.7548, -0.4825]
-Original Grad: -0.031, -lr * Pred Grad: 0.048, New P: 0.803
-Original Grad: 0.035, -lr * Pred Grad: -0.066, New P: -0.549
iter 17 loss: 0.132
Actual params: [ 0.8028, -0.5486]
-Original Grad: -0.051, -lr * Pred Grad: 0.044, New P: 0.847
-Original Grad: 0.073, -lr * Pred Grad: -0.066, New P: -0.615
iter 18 loss: 0.127
Actual params: [ 0.8468, -0.6148]
-Original Grad: 0.024, -lr * Pred Grad: 0.043, New P: 0.890
-Original Grad: 0.149, -lr * Pred Grad: -0.066, New P: -0.681
iter 19 loss: 0.119
Actual params: [ 0.8897, -0.6809]
-Original Grad: 0.090, -lr * Pred Grad: 0.043, New P: 0.933
-Original Grad: 0.107, -lr * Pred Grad: -0.066, New P: -0.747
iter 20 loss: 0.106
Actual params: [ 0.933 , -0.7471]
-Original Grad: 0.074, -lr * Pred Grad: 0.044, New P: 0.977
-Original Grad: 0.219, -lr * Pred Grad: -0.066, New P: -0.813
iter 21 loss: 0.087
Actual params: [ 0.9769, -0.8132]
-Original Grad: 0.056, -lr * Pred Grad: 0.045, New P: 1.022
-Original Grad: 0.281, -lr * Pred Grad: -0.066, New P: -0.879
iter 22 loss: 0.064
Actual params: [ 1.0217, -0.8794]
-Original Grad: 0.047, -lr * Pred Grad: 0.046, New P: 1.067
-Original Grad: 0.395, -lr * Pred Grad: -0.066, New P: -0.946
iter 23 loss: 0.053
Actual params: [ 1.0674, -0.9456]
-Original Grad: -0.014, -lr * Pred Grad: 0.046, New P: 1.113
-Original Grad: 0.406, -lr * Pred Grad: -0.066, New P: -1.012
iter 24 loss: 0.051
Actual params: [ 1.1133, -1.0117]
-Original Grad: -0.072, -lr * Pred Grad: 0.046, New P: 1.159
-Original Grad: 0.457, -lr * Pred Grad: -0.066, New P: -1.078
iter 25 loss: 0.215
Actual params: [ 1.1588, -1.0779]
-Original Grad: -0.079, -lr * Pred Grad: 0.046, New P: 1.205
-Original Grad: 0.415, -lr * Pred Grad: -0.066, New P: -1.144
iter 26 loss: 0.251
Actual params: [ 1.2051, -1.144 ]
-Original Grad: -0.042, -lr * Pred Grad: 0.050, New P: 1.255
-Original Grad: 0.874, -lr * Pred Grad: -0.066, New P: -1.210
iter 27 loss: 0.291
Actual params: [ 1.2554, -1.2102]
-Original Grad: -0.033, -lr * Pred Grad: 0.055, New P: 1.310
-Original Grad: 1.000, -lr * Pred Grad: -0.066, New P: -1.276
iter 28 loss: 0.330
Actual params: [ 1.3099, -1.2763]
-Original Grad: 0.020, -lr * Pred Grad: 0.060, New P: 1.370
-Original Grad: 0.855, -lr * Pred Grad: -0.066, New P: -1.342
iter 29 loss: 0.382
Actual params: [ 1.3697, -1.3425]
-Original Grad: 0.103, -lr * Pred Grad: 0.064, New P: 1.433
-Original Grad: 0.471, -lr * Pred Grad: -0.066, New P: -1.409
iter 30 loss: 0.435
Actual params: [ 1.4333, -1.4086]
-Original Grad: -0.034, -lr * Pred Grad: 0.063, New P: 1.496
-Original Grad: 1.422, -lr * Pred Grad: -0.066, New P: -1.475
iter 31 loss: 0.601
Actual params: [ 1.4962, -1.4746]
-Original Grad: -0.019, -lr * Pred Grad: 0.062, New P: 1.558
-Original Grad: 0.841, -lr * Pred Grad: -0.065, New P: -1.540
iter 32 loss: 0.682
Actual params: [ 1.5579, -1.5397]
-Original Grad: -0.008, -lr * Pred Grad: 0.060, New P: 1.618
-Original Grad: 0.922, -lr * Pred Grad: -0.062, New P: -1.602
iter 33 loss: 0.756
Actual params: [ 1.6176, -1.6019]
-Original Grad: -0.025, -lr * Pred Grad: 0.058, New P: 1.675
-Original Grad: 0.832, -lr * Pred Grad: -0.057, New P: -1.659
iter 34 loss: 0.819
Actual params: [ 1.6754, -1.6591]
-Original Grad: -0.007, -lr * Pred Grad: 0.057, New P: 1.732
-Original Grad: 0.717, -lr * Pred Grad: -0.050, New P: -1.709
iter 35 loss: 0.877
Actual params: [ 1.7322, -1.709 ]
-Original Grad: -0.015, -lr * Pred Grad: 0.056, New P: 1.788
-Original Grad: 0.761, -lr * Pred Grad: -0.042, New P: -1.751
iter 36 loss: 0.917
Actual params: [ 1.788 , -1.7508]
-Original Grad: -0.020, -lr * Pred Grad: 0.056, New P: 1.844
-Original Grad: 0.813, -lr * Pred Grad: -0.035, New P: -1.786
iter 37 loss: 0.941
Actual params: [ 1.8437, -1.7859]
-Original Grad: 0.024, -lr * Pred Grad: 0.057, New P: 1.901
-Original Grad: 0.837, -lr * Pred Grad: -0.030, New P: -1.816
iter 38 loss: 0.950
Actual params: [ 1.9006, -1.8164]
-Original Grad: 0.042, -lr * Pred Grad: 0.058, New P: 1.958
-Original Grad: 0.749, -lr * Pred Grad: -0.029, New P: -1.845
iter 39 loss: 0.956
Actual params: [ 1.9584, -1.8452]
-Original Grad: 0.072, -lr * Pred Grad: 0.059, New P: 2.018
-Original Grad: 0.797, -lr * Pred Grad: -0.029, New P: -1.874
iter 40 loss: 0.953
Actual params: [ 2.0178, -1.8738]
-Original Grad: 0.097, -lr * Pred Grad: 0.060, New P: 2.078
-Original Grad: 0.744, -lr * Pred Grad: -0.028, New P: -1.902
iter 41 loss: 0.953
Actual params: [ 2.0782, -1.9021]
-Original Grad: 0.111, -lr * Pred Grad: 0.061, New P: 2.139
-Original Grad: 0.835, -lr * Pred Grad: -0.028, New P: -1.930
iter 42 loss: 0.956
Actual params: [ 2.1394, -1.9298]
-Original Grad: 0.109, -lr * Pred Grad: 0.061, New P: 2.201
-Original Grad: 0.722, -lr * Pred Grad: -0.025, New P: -1.955
iter 43 loss: 0.960
Actual params: [ 2.2008, -1.9552]
-Original Grad: 0.124, -lr * Pred Grad: 0.062, New P: 2.263
-Original Grad: 0.752, -lr * Pred Grad: -0.021, New P: -1.977
iter 44 loss: 0.958
Actual params: [ 2.2626, -1.9766]
-Original Grad: 0.108, -lr * Pred Grad: 0.061, New P: 2.324
-Original Grad: 0.676, -lr * Pred Grad: -0.013, New P: -1.990
iter 45 loss: 0.957
Actual params: [ 2.3239, -1.99  ]
-Original Grad: 0.102, -lr * Pred Grad: 0.061, New P: 2.385
-Original Grad: 0.709, -lr * Pred Grad: 0.007, New P: -1.983
iter 46 loss: 0.947
Actual params: [ 2.3849, -1.983 ]
-Original Grad: 0.095, -lr * Pred Grad: 0.060, New P: 2.445
-Original Grad: 0.734, -lr * Pred Grad: 0.035, New P: -1.948
iter 47 loss: 0.925
Actual params: [ 2.445 , -1.9477]
-Original Grad: 0.092, -lr * Pred Grad: 0.059, New P: 2.504
-Original Grad: 0.866, -lr * Pred Grad: 0.057, New P: -1.891
iter 48 loss: 0.883
Actual params: [ 2.5044, -1.8912]
-Original Grad: 0.076, -lr * Pred Grad: 0.058, New P: 2.563
-Original Grad: 0.692, -lr * Pred Grad: 0.065, New P: -1.826
iter 49 loss: 0.832
Actual params: [ 2.5626, -1.8258]
-Original Grad: 0.063, -lr * Pred Grad: 0.057, New P: 2.619
-Original Grad: 0.691, -lr * Pred Grad: 0.067, New P: -1.758
iter 50 loss: 0.779
Actual params: [ 2.6194, -1.7584]
-Original Grad: 0.073, -lr * Pred Grad: 0.056, New P: 2.675
-Original Grad: 0.709, -lr * Pred Grad: 0.068, New P: -1.691
iter 51 loss: 0.710
Actual params: [ 2.6755, -1.6909]
-Original Grad: 0.055, -lr * Pred Grad: 0.055, New P: 2.730
-Original Grad: 0.713, -lr * Pred Grad: 0.067, New P: -1.623
iter 52 loss: 0.587
Actual params: [ 2.7303, -1.6234]
-Original Grad: 0.053, -lr * Pred Grad: 0.054, New P: 2.785
-Original Grad: 0.691, -lr * Pred Grad: 0.067, New P: -1.556
iter 53 loss: 0.539
Actual params: [ 2.7846, -1.556 ]
-Original Grad: 0.051, -lr * Pred Grad: 0.054, New P: 2.838
-Original Grad: 0.703, -lr * Pred Grad: 0.067, New P: -1.489
iter 54 loss: 0.491
Actual params: [ 2.8383, -1.4886]
-Original Grad: 0.042, -lr * Pred Grad: 0.053, New P: 2.892
-Original Grad: 0.713, -lr * Pred Grad: 0.067, New P: -1.421
iter 55 loss: 0.445
Actual params: [ 2.8918, -1.4214]
-Original Grad: 0.037, -lr * Pred Grad: 0.053, New P: 2.945
-Original Grad: 0.835, -lr * Pred Grad: 0.067, New P: -1.354
iter 56 loss: 0.404
Actual params: [ 2.9449, -1.354 ]
-Original Grad: 0.043, -lr * Pred Grad: 0.054, New P: 2.998
-Original Grad: 0.784, -lr * Pred Grad: 0.067, New P: -1.287
iter 57 loss: 0.371
Actual params: [ 2.9985, -1.2866]
-Original Grad: 0.051, -lr * Pred Grad: 0.054, New P: 3.052
-Original Grad: 1.015, -lr * Pred Grad: 0.068, New P: -1.219
iter 58 loss: 0.346
Actual params: [ 3.0522, -1.2189]
-Original Grad: 0.028, -lr * Pred Grad: 0.053, New P: 3.106
-Original Grad: 0.751, -lr * Pred Grad: 0.068, New P: -1.151
iter 59 loss: 0.327
Actual params: [ 3.1057, -1.1513]
-Original Grad: 0.021, -lr * Pred Grad: 0.053, New P: 3.159
-Original Grad: 0.714, -lr * Pred Grad: 0.068, New P: -1.084
iter 60 loss: 0.315
Actual params: [ 3.159 , -1.0838]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 1.025, -lr * Pred Grad: 0.053, New P: 0.647
-Original Grad: -0.205, -lr * Pred Grad: -0.046, New P: 0.548
iter 0 loss: 0.635
Actual params: [0.6469, 0.5476]
-Original Grad: 1.530, -lr * Pred Grad: 0.066, New P: 0.713
-Original Grad: -0.202, -lr * Pred Grad: -0.063, New P: 0.485
iter 1 loss: 0.590
Actual params: [0.7133, 0.4848]
-Original Grad: 1.597, -lr * Pred Grad: 0.069, New P: 0.782
-Original Grad: -0.032, -lr * Pred Grad: -0.066, New P: 0.419
iter 2 loss: 0.529
Actual params: [0.7818, 0.4193]
-Original Grad: 1.121, -lr * Pred Grad: 0.069, New P: 0.851
-Original Grad: -0.060, -lr * Pred Grad: -0.066, New P: 0.353
iter 3 loss: 0.465
Actual params: [0.8506, 0.3532]
-Original Grad: 0.570, -lr * Pred Grad: 0.069, New P: 0.919
-Original Grad: -0.115, -lr * Pred Grad: -0.066, New P: 0.287
iter 4 loss: 0.396
Actual params: [0.9195, 0.2871]
-Original Grad: 0.204, -lr * Pred Grad: 0.069, New P: 0.988
-Original Grad: -0.101, -lr * Pred Grad: -0.066, New P: 0.221
iter 5 loss: 0.371
Actual params: [0.9883, 0.221 ]
-Original Grad: 0.126, -lr * Pred Grad: 0.069, New P: 1.057
-Original Grad: -0.078, -lr * Pred Grad: -0.066, New P: 0.155
iter 6 loss: 0.368
Actual params: [1.0572, 0.1549]
-Original Grad: 0.071, -lr * Pred Grad: 0.069, New P: 1.126
-Original Grad: -0.082, -lr * Pred Grad: -0.066, New P: 0.089
iter 7 loss: 0.369
Actual params: [1.126 , 0.0887]
-Original Grad: 0.023, -lr * Pred Grad: 0.069, New P: 1.195
-Original Grad: -0.032, -lr * Pred Grad: -0.066, New P: 0.023
iter 8 loss: 0.378
Actual params: [1.1946, 0.0226]
-Original Grad: -0.026, -lr * Pred Grad: 0.068, New P: 1.263
-Original Grad: 0.006, -lr * Pred Grad: -0.066, New P: -0.044
iter 9 loss: 0.391
Actual params: [ 1.2631, -0.0436]
-Original Grad: -0.013, -lr * Pred Grad: 0.068, New P: 1.331
-Original Grad: -0.030, -lr * Pred Grad: -0.066, New P: -0.110
iter 10 loss: 0.412
Actual params: [ 1.3313, -0.1097]
-Original Grad: -0.043, -lr * Pred Grad: 0.067, New P: 1.399
-Original Grad: -0.015, -lr * Pred Grad: -0.066, New P: -0.176
iter 11 loss: 0.431
Actual params: [ 1.3987, -0.1759]
-Original Grad: -0.035, -lr * Pred Grad: 0.067, New P: 1.465
-Original Grad: -0.012, -lr * Pred Grad: -0.066, New P: -0.242
iter 12 loss: 0.456
Actual params: [ 1.4654, -0.242 ]
-Original Grad: -0.108, -lr * Pred Grad: 0.066, New P: 1.531
-Original Grad: 0.004, -lr * Pred Grad: -0.066, New P: -0.308
iter 13 loss: 0.480
Actual params: [ 1.5315, -0.3082]
-Original Grad: -0.072, -lr * Pred Grad: 0.065, New P: 1.597
-Original Grad: -0.013, -lr * Pred Grad: -0.066, New P: -0.374
iter 14 loss: 0.500
Actual params: [ 1.5966, -0.3743]
-Original Grad: -0.161, -lr * Pred Grad: 0.062, New P: 1.659
-Original Grad: 0.021, -lr * Pred Grad: -0.066, New P: -0.440
iter 15 loss: 0.523
Actual params: [ 1.6588, -0.4405]
-Original Grad: -0.128, -lr * Pred Grad: 0.058, New P: 1.716
-Original Grad: 0.002, -lr * Pred Grad: -0.066, New P: -0.507
iter 16 loss: 0.545
Actual params: [ 1.7163, -0.5066]
-Original Grad: -0.139, -lr * Pred Grad: 0.052, New P: 1.769
-Original Grad: 0.010, -lr * Pred Grad: -0.066, New P: -0.573
iter 17 loss: 0.566
Actual params: [ 1.7686, -0.5728]
-Original Grad: -0.102, -lr * Pred Grad: 0.048, New P: 1.816
-Original Grad: 0.016, -lr * Pred Grad: -0.066, New P: -0.639
iter 18 loss: 0.583
Actual params: [ 1.8161, -0.6389]
-Original Grad: -0.187, -lr * Pred Grad: 0.043, New P: 1.859
-Original Grad: 0.032, -lr * Pred Grad: -0.066, New P: -0.705
iter 19 loss: 0.598
Actual params: [ 1.8588, -0.7051]
-Original Grad: -0.215, -lr * Pred Grad: 0.039, New P: 1.898
-Original Grad: 0.046, -lr * Pred Grad: -0.066, New P: -0.771
iter 20 loss: 0.612
Actual params: [ 1.8981, -0.7712]
-Original Grad: -0.139, -lr * Pred Grad: 0.037, New P: 1.935
-Original Grad: 0.048, -lr * Pred Grad: -0.066, New P: -0.837
iter 21 loss: 0.623
Actual params: [ 1.9353, -0.8374]
-Original Grad: -0.117, -lr * Pred Grad: 0.036, New P: 1.971
-Original Grad: 0.043, -lr * Pred Grad: -0.066, New P: -0.904
iter 22 loss: 0.634
Actual params: [ 1.9714, -0.9035]
-Original Grad: -0.108, -lr * Pred Grad: 0.036, New P: 2.007
-Original Grad: 0.060, -lr * Pred Grad: -0.066, New P: -0.970
iter 23 loss: 0.643
Actual params: [ 2.0073, -0.9697]
-Original Grad: -0.146, -lr * Pred Grad: 0.035, New P: 2.043
-Original Grad: 0.089, -lr * Pred Grad: -0.066, New P: -1.036
iter 24 loss: 0.651
Actual params: [ 2.0425, -1.0358]
-Original Grad: -0.205, -lr * Pred Grad: 0.034, New P: 2.076
-Original Grad: 0.075, -lr * Pred Grad: -0.066, New P: -1.102
iter 25 loss: 0.657
Actual params: [ 2.0762, -1.102 ]
-Original Grad: -0.136, -lr * Pred Grad: 0.037, New P: 2.113
-Original Grad: 0.083, -lr * Pred Grad: -0.066, New P: -1.168
iter 26 loss: 0.661
Actual params: [ 2.1133, -1.1681]
-Original Grad: -0.102, -lr * Pred Grad: 0.042, New P: 2.156
-Original Grad: 0.058, -lr * Pred Grad: -0.066, New P: -1.234
iter 27 loss: 0.665
Actual params: [ 2.1558, -1.2343]
-Original Grad: -0.103, -lr * Pred Grad: 0.048, New P: 2.204
-Original Grad: 0.060, -lr * Pred Grad: -0.066, New P: -1.300
iter 28 loss: 0.672
Actual params: [ 2.2039, -1.3004]
-Original Grad: -0.168, -lr * Pred Grad: 0.049, New P: 2.253
-Original Grad: 0.044, -lr * Pred Grad: -0.066, New P: -1.367
iter 29 loss: 0.677
Actual params: [ 2.2529, -1.3666]
-Original Grad: -0.188, -lr * Pred Grad: 0.049, New P: 2.302
-Original Grad: 0.034, -lr * Pred Grad: -0.066, New P: -1.433
iter 30 loss: 0.682
Actual params: [ 2.3022, -1.4327]
-Original Grad: -0.173, -lr * Pred Grad: 0.049, New P: 2.351
-Original Grad: 0.032, -lr * Pred Grad: -0.066, New P: -1.499
iter 31 loss: 0.685
Actual params: [ 2.3509, -1.4989]
-Original Grad: -0.184, -lr * Pred Grad: 0.049, New P: 2.400
-Original Grad: 0.033, -lr * Pred Grad: -0.066, New P: -1.565
iter 32 loss: 0.685
Actual params: [ 2.3996, -1.565 ]
-Original Grad: -0.179, -lr * Pred Grad: 0.048, New P: 2.448
-Original Grad: 0.035, -lr * Pred Grad: -0.066, New P: -1.631
iter 33 loss: 0.689
Actual params: [ 2.4478, -1.6312]
-Original Grad: -0.137, -lr * Pred Grad: 0.050, New P: 2.498
-Original Grad: 0.025, -lr * Pred Grad: -0.066, New P: -1.697
iter 34 loss: 0.699
Actual params: [ 2.4979, -1.6973]
-Original Grad: -0.123, -lr * Pred Grad: 0.050, New P: 2.548
-Original Grad: 0.028, -lr * Pred Grad: -0.066, New P: -1.763
iter 35 loss: 0.705
Actual params: [ 2.5479, -1.7635]
-Original Grad: -0.126, -lr * Pred Grad: 0.050, New P: 2.598
-Original Grad: 0.015, -lr * Pred Grad: -0.066, New P: -1.830
iter 36 loss: 0.710
Actual params: [ 2.5984, -1.8296]
-Original Grad: -0.140, -lr * Pred Grad: 0.048, New P: 2.647
-Original Grad: -0.000, -lr * Pred Grad: -0.066, New P: -1.896
iter 37 loss: 0.714
Actual params: [ 2.6466, -1.8958]
-Original Grad: -0.090, -lr * Pred Grad: 0.050, New P: 2.696
-Original Grad: 0.012, -lr * Pred Grad: -0.066, New P: -1.962
iter 38 loss: 0.717
Actual params: [ 2.6961, -1.9619]
-Original Grad: -0.112, -lr * Pred Grad: 0.044, New P: 2.740
-Original Grad: -0.004, -lr * Pred Grad: -0.066, New P: -2.028
iter 39 loss: 0.720
Actual params: [ 2.7404, -2.0281]
-Original Grad: -0.077, -lr * Pred Grad: 0.038, New P: 2.779
-Original Grad: 0.019, -lr * Pred Grad: -0.066, New P: -2.094
iter 40 loss: 0.722
Actual params: [ 2.7786, -2.0942]
-Original Grad: -0.109, -lr * Pred Grad: 0.009, New P: 2.788
-Original Grad: 0.010, -lr * Pred Grad: -0.066, New P: -2.160
iter 41 loss: 0.726
Actual params: [ 2.7879, -2.1604]
-Original Grad: -0.134, -lr * Pred Grad: -0.007, New P: 2.781
-Original Grad: -0.012, -lr * Pred Grad: -0.066, New P: -2.227
iter 42 loss: 0.721
Actual params: [ 2.7809, -2.2265]
-Original Grad: -0.136, -lr * Pred Grad: -0.034, New P: 2.747
-Original Grad: -0.019, -lr * Pred Grad: -0.066, New P: -2.293
iter 43 loss: 0.725
Actual params: [ 2.7468, -2.2927]
-Original Grad: -0.130, -lr * Pred Grad: -0.045, New P: 2.702
-Original Grad: -0.002, -lr * Pred Grad: -0.066, New P: -2.359
iter 44 loss: 0.725
Actual params: [ 2.7022, -2.3588]
-Original Grad: -0.113, -lr * Pred Grad: -0.047, New P: 2.655
-Original Grad: 0.003, -lr * Pred Grad: -0.066, New P: -2.425
iter 45 loss: 0.719
Actual params: [ 2.6551, -2.425 ]
-Original Grad: -0.126, -lr * Pred Grad: -0.048, New P: 2.607
-Original Grad: 0.003, -lr * Pred Grad: -0.066, New P: -2.491
iter 46 loss: 0.712
Actual params: [ 2.6072, -2.4911]
-Original Grad: -0.108, -lr * Pred Grad: -0.048, New P: 2.559
-Original Grad: 0.009, -lr * Pred Grad: -0.066, New P: -2.557
iter 47 loss: 0.703
Actual params: [ 2.5592, -2.5573]
-Original Grad: -0.130, -lr * Pred Grad: -0.048, New P: 2.511
-Original Grad: 0.007, -lr * Pred Grad: -0.066, New P: -2.623
iter 48 loss: 0.706
Actual params: [ 2.5111, -2.6234]
-Original Grad: -0.056, -lr * Pred Grad: -0.048, New P: 2.463
-Original Grad: 0.042, -lr * Pred Grad: -0.066, New P: -2.690
iter 49 loss: 0.708
Actual params: [ 2.4629, -2.6896]
-Original Grad: -0.042, -lr * Pred Grad: -0.048, New P: 2.415
-Original Grad: 0.031, -lr * Pred Grad: -0.066, New P: -2.756
iter 50 loss: 0.705
Actual params: [ 2.4148, -2.7557]
-Original Grad: -0.085, -lr * Pred Grad: -0.048, New P: 2.366
-Original Grad: 0.031, -lr * Pred Grad: -0.066, New P: -2.822
iter 51 loss: 0.703
Actual params: [ 2.3664, -2.8219]
-Original Grad: -0.134, -lr * Pred Grad: -0.050, New P: 2.317
-Original Grad: 0.016, -lr * Pred Grad: -0.066, New P: -2.888
iter 52 loss: 0.703
Actual params: [ 2.3168, -2.888 ]
-Original Grad: -0.037, -lr * Pred Grad: -0.052, New P: 2.265
-Original Grad: 0.028, -lr * Pred Grad: -0.066, New P: -2.954
iter 53 loss: 0.702
Actual params: [ 2.2652, -2.9542]
-Original Grad: -0.100, -lr * Pred Grad: -0.052, New P: 2.213
-Original Grad: 0.025, -lr * Pred Grad: -0.066, New P: -3.020
iter 54 loss: 0.705
Actual params: [ 2.2127, -3.0203]
-Original Grad: -0.094, -lr * Pred Grad: -0.053, New P: 2.160
-Original Grad: 0.025, -lr * Pred Grad: -0.066, New P: -3.086
iter 55 loss: 0.711
Actual params: [ 2.16  , -3.0865]
-Original Grad: -0.039, -lr * Pred Grad: -0.053, New P: 2.107
-Original Grad: 0.038, -lr * Pred Grad: -0.066, New P: -3.153
iter 56 loss: 0.726
Actual params: [ 2.107 , -3.1526]
-Original Grad: -0.029, -lr * Pred Grad: -0.054, New P: 2.053
-Original Grad: 0.038, -lr * Pred Grad: -0.066, New P: -3.219
iter 57 loss: 0.943
Actual params: [ 2.0526, -3.2188]
-Original Grad: -0.053, -lr * Pred Grad: -0.058, New P: 1.995
-Original Grad: 0.036, -lr * Pred Grad: -0.066, New P: -3.285
iter 58 loss: 0.994
Actual params: [ 1.9946, -3.2849]
-Original Grad: -0.020, -lr * Pred Grad: -0.061, New P: 1.934
-Original Grad: 0.040, -lr * Pred Grad: -0.066, New P: -3.351
iter 59 loss: 1.031
Actual params: [ 1.9336, -3.3511]
-Original Grad: -0.009, -lr * Pred Grad: -0.062, New P: 1.872
-Original Grad: 0.044, -lr * Pred Grad: -0.066, New P: -3.417
iter 60 loss: 1.036
Actual params: [ 1.8718, -3.4172]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: -0.186, -lr * Pred Grad: -0.046, New P: 0.549
-Original Grad: -0.072, -lr * Pred Grad: -0.024, New P: 0.571
iter 0 loss: 0.187
Actual params: [0.5486, 0.5705]
-Original Grad: -0.160, -lr * Pred Grad: -0.063, New P: 0.486
-Original Grad: -0.082, -lr * Pred Grad: -0.058, New P: 0.513
iter 1 loss: 0.131
Actual params: [0.4859, 0.5127]
-Original Grad: -0.006, -lr * Pred Grad: -0.066, New P: 0.420
-Original Grad: 0.004, -lr * Pred Grad: -0.065, New P: 0.448
iter 2 loss: 0.094
Actual params: [0.4204, 0.4479]
-Original Grad: 0.465, -lr * Pred Grad: -0.066, New P: 0.354
-Original Grad: 0.323, -lr * Pred Grad: -0.066, New P: 0.382
iter 3 loss: 0.201
Actual params: [0.3543, 0.382 ]
-Original Grad: 0.771, -lr * Pred Grad: -0.066, New P: 0.288
-Original Grad: 0.553, -lr * Pred Grad: -0.066, New P: 0.316
iter 4 loss: 0.261
Actual params: [0.2882, 0.3159]
-Original Grad: 0.738, -lr * Pred Grad: -0.066, New P: 0.222
-Original Grad: 0.468, -lr * Pred Grad: -0.066, New P: 0.250
iter 5 loss: 0.296
Actual params: [0.2224, 0.2498]
-Original Grad: 0.524, -lr * Pred Grad: -0.064, New P: 0.158
-Original Grad: 0.532, -lr * Pred Grad: -0.065, New P: 0.185
iter 6 loss: 0.313
Actual params: [0.1584, 0.1846]
-Original Grad: 0.497, -lr * Pred Grad: -0.053, New P: 0.105
-Original Grad: 0.531, -lr * Pred Grad: -0.060, New P: 0.124
iter 7 loss: 0.332
Actual params: [0.1055, 0.1244]
-Original Grad: 0.311, -lr * Pred Grad: -0.013, New P: 0.092
-Original Grad: 0.497, -lr * Pred Grad: -0.037, New P: 0.087
iter 8 loss: 0.347
Actual params: [0.0924, 0.0874]
-Original Grad: 0.292, -lr * Pred Grad: 0.042, New P: 0.134
-Original Grad: 0.530, -lr * Pred Grad: 0.008, New P: 0.096
iter 9 loss: 0.350
Actual params: [0.134 , 0.0959]
-Original Grad: 0.352, -lr * Pred Grad: 0.064, New P: 0.198
-Original Grad: 0.491, -lr * Pred Grad: 0.046, New P: 0.142
iter 10 loss: 0.346
Actual params: [0.1979, 0.1423]
-Original Grad: 0.550, -lr * Pred Grad: 0.066, New P: 0.264
-Original Grad: 0.505, -lr * Pred Grad: 0.064, New P: 0.207
iter 11 loss: 0.332
Actual params: [0.2642, 0.2065]
-Original Grad: 0.797, -lr * Pred Grad: 0.067, New P: 0.331
-Original Grad: 0.542, -lr * Pred Grad: 0.067, New P: 0.274
iter 12 loss: 0.312
Actual params: [0.3311, 0.2738]
-Original Grad: 0.855, -lr * Pred Grad: 0.067, New P: 0.399
-Original Grad: 0.454, -lr * Pred Grad: 0.067, New P: 0.341
iter 13 loss: 0.291
Actual params: [0.3985, 0.3406]
-Original Grad: 0.597, -lr * Pred Grad: 0.067, New P: 0.466
-Original Grad: 0.404, -lr * Pred Grad: 0.066, New P: 0.407
iter 14 loss: 0.257
Actual params: [0.4657, 0.4065]
-Original Grad: 0.290, -lr * Pred Grad: 0.066, New P: 0.532
-Original Grad: 0.186, -lr * Pred Grad: 0.063, New P: 0.469
iter 15 loss: 0.194
Actual params: [0.5315, 0.4694]
-Original Grad: -0.004, -lr * Pred Grad: 0.061, New P: 0.592
-Original Grad: 0.028, -lr * Pred Grad: 0.057, New P: 0.526
iter 16 loss: 0.093
Actual params: [0.5923, 0.5262]
-Original Grad: -0.147, -lr * Pred Grad: 0.053, New P: 0.645
-Original Grad: -0.149, -lr * Pred Grad: 0.049, New P: 0.575
iter 17 loss: 0.141
Actual params: [0.645 , 0.5753]
-Original Grad: -0.255, -lr * Pred Grad: 0.045, New P: 0.690
-Original Grad: -0.122, -lr * Pred Grad: 0.044, New P: 0.620
iter 18 loss: 0.224
Actual params: [0.6905, 0.6197]
-Original Grad: -0.293, -lr * Pred Grad: 0.039, New P: 0.729
-Original Grad: -0.072, -lr * Pred Grad: 0.041, New P: 0.660
iter 19 loss: 0.298
Actual params: [0.7292, 0.6603]
-Original Grad: -0.341, -lr * Pred Grad: 0.031, New P: 0.760
-Original Grad: -0.070, -lr * Pred Grad: 0.039, New P: 0.699
iter 20 loss: 0.361
Actual params: [0.7602, 0.6991]
-Original Grad: -0.373, -lr * Pred Grad: 0.016, New P: 0.777
-Original Grad: -0.078, -lr * Pred Grad: 0.038, New P: 0.737
iter 21 loss: 0.409
Actual params: [0.7765, 0.737 ]
-Original Grad: -0.301, -lr * Pred Grad: 0.019, New P: 0.796
-Original Grad: -0.051, -lr * Pred Grad: 0.039, New P: 0.776
iter 22 loss: 0.436
Actual params: [0.7955, 0.7763]
-Original Grad: -0.307, -lr * Pred Grad: 0.012, New P: 0.807
-Original Grad: -0.023, -lr * Pred Grad: 0.043, New P: 0.819
iter 23 loss: 0.460
Actual params: [0.8071, 0.8191]
-Original Grad: -0.257, -lr * Pred Grad: 0.003, New P: 0.810
-Original Grad: -0.011, -lr * Pred Grad: 0.048, New P: 0.867
iter 24 loss: 0.474
Actual params: [0.8104, 0.867 ]
-Original Grad: -0.344, -lr * Pred Grad: -0.032, New P: 0.778
-Original Grad: 0.008, -lr * Pred Grad: 0.054, New P: 0.921
iter 25 loss: 0.477
Actual params: [0.7784, 0.9208]
-Original Grad: -0.442, -lr * Pred Grad: -0.044, New P: 0.734
-Original Grad: 0.012, -lr * Pred Grad: 0.059, New P: 0.979
iter 26 loss: 0.447
Actual params: [0.7345, 0.9793]
-Original Grad: -0.394, -lr * Pred Grad: -0.048, New P: 0.687
-Original Grad: 0.007, -lr * Pred Grad: 0.061, New P: 1.041
iter 27 loss: 0.399
Actual params: [0.6865, 1.0408]
-Original Grad: -0.327, -lr * Pred Grad: -0.051, New P: 0.635
-Original Grad: -0.004, -lr * Pred Grad: 0.062, New P: 1.103
iter 28 loss: 0.341
Actual params: [0.6354, 1.1031]
-Original Grad: -0.270, -lr * Pred Grad: -0.054, New P: 0.582
-Original Grad: -0.002, -lr * Pred Grad: 0.062, New P: 1.165
iter 29 loss: 0.273
Actual params: [0.5819, 1.1653]
-Original Grad: -0.248, -lr * Pred Grad: -0.056, New P: 0.526
-Original Grad: 0.001, -lr * Pred Grad: 0.061, New P: 1.227
iter 30 loss: 0.210
Actual params: [0.5255, 1.2267]
-Original Grad: -0.223, -lr * Pred Grad: -0.059, New P: 0.467
-Original Grad: -0.000, -lr * Pred Grad: 0.060, New P: 1.287
iter 31 loss: 0.151
Actual params: [0.4666, 1.287 ]
-Original Grad: -0.110, -lr * Pred Grad: -0.062, New P: 0.404
-Original Grad: -0.005, -lr * Pred Grad: 0.059, New P: 1.346
iter 32 loss: 0.095
Actual params: [0.4041, 1.3461]
-Original Grad: 0.055, -lr * Pred Grad: -0.065, New P: 0.339
-Original Grad: 0.002, -lr * Pred Grad: 0.058, New P: 1.404
iter 33 loss: 0.103
Actual params: [0.339 , 1.4044]
-Original Grad: 0.221, -lr * Pred Grad: -0.066, New P: 0.273
-Original Grad: 0.003, -lr * Pred Grad: 0.058, New P: 1.462
iter 34 loss: 0.168
Actual params: [0.273 , 1.4623]
-Original Grad: 0.334, -lr * Pred Grad: -0.066, New P: 0.207
-Original Grad: 0.011, -lr * Pred Grad: 0.058, New P: 1.520
iter 35 loss: 0.212
Actual params: [0.2069, 1.5203]
-Original Grad: 0.235, -lr * Pred Grad: -0.066, New P: 0.141
-Original Grad: 0.015, -lr * Pred Grad: 0.058, New P: 1.578
iter 36 loss: 0.239
Actual params: [0.1407, 1.5784]
-Original Grad: 0.199, -lr * Pred Grad: -0.066, New P: 0.075
-Original Grad: 0.023, -lr * Pred Grad: 0.059, New P: 1.637
iter 37 loss: 0.249
Actual params: [0.0746, 1.637 ]
-Original Grad: 0.210, -lr * Pred Grad: -0.066, New P: 0.008
-Original Grad: 0.025, -lr * Pred Grad: 0.059, New P: 1.696
iter 38 loss: 0.254
Actual params: [0.0084, 1.6956]
-Original Grad: 0.180, -lr * Pred Grad: -0.066, New P: -0.058
-Original Grad: 0.030, -lr * Pred Grad: 0.059, New P: 1.754
iter 39 loss: 0.257
Actual params: [-0.0578,  1.7544]
-Original Grad: 0.151, -lr * Pred Grad: -0.066, New P: -0.124
-Original Grad: 0.030, -lr * Pred Grad: 0.059, New P: 1.813
iter 40 loss: 0.261
Actual params: [-0.1239,  1.8129]
-Original Grad: 0.148, -lr * Pred Grad: -0.066, New P: -0.190
-Original Grad: 0.030, -lr * Pred Grad: 0.058, New P: 1.871
iter 41 loss: 0.264
Actual params: [-0.1901,  1.8713]
-Original Grad: 0.130, -lr * Pred Grad: -0.066, New P: -0.256
-Original Grad: 0.028, -lr * Pred Grad: 0.058, New P: 1.929
iter 42 loss: 0.270
Actual params: [-0.2562,  1.9291]
-Original Grad: 0.139, -lr * Pred Grad: -0.066, New P: -0.322
-Original Grad: 0.037, -lr * Pred Grad: 0.058, New P: 1.987
iter 43 loss: 0.275
Actual params: [-0.3224,  1.9869]
-Original Grad: 0.216, -lr * Pred Grad: -0.066, New P: -0.389
-Original Grad: 0.043, -lr * Pred Grad: 0.058, New P: 2.045
iter 44 loss: 0.300
Actual params: [-0.3885,  2.0446]
-Original Grad: 0.199, -lr * Pred Grad: -0.066, New P: -0.455
-Original Grad: 0.059, -lr * Pred Grad: 0.058, New P: 2.103
iter 45 loss: 0.318
Actual params: [-0.4547,  2.1027]
-Original Grad: 0.245, -lr * Pred Grad: -0.066, New P: -0.521
-Original Grad: 0.077, -lr * Pred Grad: 0.059, New P: 2.161
iter 46 loss: 0.745
Actual params: [-0.5208,  2.1613]
-Original Grad: 0.311, -lr * Pred Grad: -0.066, New P: -0.587
-Original Grad: 0.093, -lr * Pred Grad: 0.059, New P: 2.220
iter 47 loss: 0.744
Actual params: [-0.587 ,  2.2205]
-Original Grad: 0.310, -lr * Pred Grad: -0.066, New P: -0.653
-Original Grad: 0.086, -lr * Pred Grad: 0.059, New P: 2.280
iter 48 loss: 0.747
Actual params: [-0.6531,  2.2797]
-Original Grad: 0.385, -lr * Pred Grad: -0.066, New P: -0.719
-Original Grad: 0.092, -lr * Pred Grad: 0.059, New P: 2.339
iter 49 loss: 0.748
Actual params: [-0.7193,  2.3392]
-Original Grad: 0.400, -lr * Pred Grad: -0.066, New P: -0.785
-Original Grad: 0.094, -lr * Pred Grad: 0.059, New P: 2.399
iter 50 loss: 0.751
Actual params: [-0.7855,  2.3985]
-Original Grad: 0.452, -lr * Pred Grad: -0.066, New P: -0.852
-Original Grad: 0.091, -lr * Pred Grad: 0.059, New P: 2.458
iter 51 loss: 0.753
Actual params: [-0.8516,  2.4578]
-Original Grad: 0.497, -lr * Pred Grad: -0.066, New P: -0.918
-Original Grad: 0.087, -lr * Pred Grad: 0.059, New P: 2.517
iter 52 loss: 0.756
Actual params: [-0.9178,  2.5167]
-Original Grad: 0.574, -lr * Pred Grad: -0.066, New P: -0.984
-Original Grad: 0.095, -lr * Pred Grad: 0.059, New P: 2.576
iter 53 loss: 0.758
Actual params: [-0.9839,  2.5757]
-Original Grad: 0.512, -lr * Pred Grad: -0.066, New P: -1.050
-Original Grad: 0.093, -lr * Pred Grad: 0.059, New P: 2.634
iter 54 loss: 0.761
Actual params: [-1.0501,  2.6343]
-Original Grad: 0.617, -lr * Pred Grad: -0.066, New P: -1.116
-Original Grad: 0.099, -lr * Pred Grad: 0.059, New P: 2.693
iter 55 loss: 0.764
Actual params: [-1.1162,  2.693 ]
-Original Grad: 0.696, -lr * Pred Grad: -0.066, New P: -1.182
-Original Grad: 0.103, -lr * Pred Grad: 0.059, New P: 2.752
iter 56 loss: 0.766
Actual params: [-1.1824,  2.7516]
-Original Grad: 0.749, -lr * Pred Grad: -0.066, New P: -1.248
-Original Grad: 0.099, -lr * Pred Grad: 0.058, New P: 2.810
iter 57 loss: 0.768
Actual params: [-1.2483,  2.81  ]
-Original Grad: 0.836, -lr * Pred Grad: -0.065, New P: -1.313
-Original Grad: 0.100, -lr * Pred Grad: 0.058, New P: 2.868
iter 58 loss: 0.768
Actual params: [-1.3131,  2.8682]
-Original Grad: 0.894, -lr * Pred Grad: -0.062, New P: -1.375
-Original Grad: 0.086, -lr * Pred Grad: 0.058, New P: 2.926
iter 59 loss: 0.768
Actual params: [-1.375 ,  2.9258]
-Original Grad: 0.991, -lr * Pred Grad: -0.058, New P: -1.433
-Original Grad: 0.063, -lr * Pred Grad: 0.057, New P: 2.982
iter 60 loss: 0.769
Actual params: [-1.4329,  2.9823]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.029, -lr * Pred Grad: 0.029, New P: 0.623
-Original Grad: -0.417, -lr * Pred Grad: -0.048, New P: 0.546
iter 0 loss: 0.389
Actual params: [0.6226, 0.5456]
-Original Grad: 0.046, -lr * Pred Grad: 0.001, New P: 0.624
-Original Grad: -0.610, -lr * Pred Grad: -0.063, New P: 0.483
iter 1 loss: 0.375
Actual params: [0.624 , 0.4827]
-Original Grad: 0.046, -lr * Pred Grad: -0.049, New P: 0.575
-Original Grad: -0.384, -lr * Pred Grad: -0.066, New P: 0.417
iter 2 loss: 0.360
Actual params: [0.5752, 0.4172]
-Original Grad: 0.045, -lr * Pred Grad: -0.063, New P: 0.512
-Original Grad: -0.362, -lr * Pred Grad: -0.066, New P: 0.351
iter 3 loss: 0.338
Actual params: [0.512 , 0.3512]
-Original Grad: 0.035, -lr * Pred Grad: -0.066, New P: 0.446
-Original Grad: -0.466, -lr * Pred Grad: -0.066, New P: 0.285
iter 4 loss: 0.304
Actual params: [0.4463, 0.2851]
-Original Grad: -0.009, -lr * Pred Grad: -0.066, New P: 0.380
-Original Grad: -0.062, -lr * Pred Grad: -0.066, New P: 0.219
iter 5 loss: 0.270
Actual params: [0.3803, 0.219 ]
-Original Grad: -0.037, -lr * Pred Grad: -0.066, New P: 0.314
-Original Grad: 0.115, -lr * Pred Grad: -0.066, New P: 0.153
iter 6 loss: 0.241
Actual params: [0.3141, 0.1529]
-Original Grad: -0.041, -lr * Pred Grad: -0.066, New P: 0.248
-Original Grad: 0.092, -lr * Pred Grad: -0.066, New P: 0.087
iter 7 loss: 0.237
Actual params: [0.248 , 0.0867]
-Original Grad: -0.017, -lr * Pred Grad: -0.066, New P: 0.182
-Original Grad: 0.097, -lr * Pred Grad: -0.066, New P: 0.021
iter 8 loss: 0.237
Actual params: [0.1818, 0.0206]
-Original Grad: 0.001, -lr * Pred Grad: -0.066, New P: 0.116
-Original Grad: 0.104, -lr * Pred Grad: -0.066, New P: -0.046
iter 9 loss: 0.239
Actual params: [ 0.1157, -0.0456]
-Original Grad: 0.002, -lr * Pred Grad: -0.066, New P: 0.050
-Original Grad: 0.101, -lr * Pred Grad: -0.066, New P: -0.112
iter 10 loss: 0.240
Actual params: [ 0.0495, -0.1117]
-Original Grad: -0.005, -lr * Pred Grad: -0.066, New P: -0.017
-Original Grad: 0.083, -lr * Pred Grad: -0.066, New P: -0.178
iter 11 loss: 0.237
Actual params: [-0.0166, -0.1779]
-Original Grad: 0.026, -lr * Pred Grad: -0.066, New P: -0.083
-Original Grad: 0.071, -lr * Pred Grad: -0.066, New P: -0.244
iter 12 loss: 0.231
Actual params: [-0.0828, -0.244 ]
-Original Grad: 0.014, -lr * Pred Grad: -0.066, New P: -0.149
-Original Grad: 0.175, -lr * Pred Grad: -0.066, New P: -0.310
iter 13 loss: 0.219
Actual params: [-0.1489, -0.3102]
-Original Grad: 0.028, -lr * Pred Grad: -0.066, New P: -0.215
-Original Grad: 0.191, -lr * Pred Grad: -0.066, New P: -0.376
iter 14 loss: 0.200
Actual params: [-0.2151, -0.3764]
-Original Grad: 0.039, -lr * Pred Grad: -0.066, New P: -0.281
-Original Grad: 0.221, -lr * Pred Grad: -0.066, New P: -0.443
iter 15 loss: 0.182
Actual params: [-0.2812, -0.4425]
-Original Grad: 0.095, -lr * Pred Grad: -0.066, New P: -0.347
-Original Grad: 0.529, -lr * Pred Grad: -0.066, New P: -0.509
iter 16 loss: 0.171
Actual params: [-0.3474, -0.5087]
-Original Grad: 0.045, -lr * Pred Grad: -0.066, New P: -0.414
-Original Grad: 0.902, -lr * Pred Grad: -0.066, New P: -0.575
iter 17 loss: 0.172
Actual params: [-0.4135, -0.5748]
-Original Grad: 0.211, -lr * Pred Grad: -0.066, New P: -0.480
-Original Grad: 1.543, -lr * Pred Grad: -0.066, New P: -0.641
iter 18 loss: 0.187
Actual params: [-0.4797, -0.641 ]
-Original Grad: 0.342, -lr * Pred Grad: -0.066, New P: -0.546
-Original Grad: 2.176, -lr * Pred Grad: -0.066, New P: -0.707
iter 19 loss: 0.204
Actual params: [-0.5458, -0.7071]
-Original Grad: 0.699, -lr * Pred Grad: -0.066, New P: -0.612
-Original Grad: 3.233, -lr * Pred Grad: -0.066, New P: -0.773
iter 20 loss: 0.445
Actual params: [-0.612 , -0.7733]
-Original Grad: 0.944, -lr * Pred Grad: -0.066, New P: -0.678
-Original Grad: 4.354, -lr * Pred Grad: -0.066, New P: -0.839
iter 21 loss: 0.453
Actual params: [-0.6781, -0.8394]
-Original Grad: 0.943, -lr * Pred Grad: -0.066, New P: -0.744
-Original Grad: 3.222, -lr * Pred Grad: -0.066, New P: -0.905
iter 22 loss: 0.457
Actual params: [-0.7443, -0.9052]
-Original Grad: 0.536, -lr * Pred Grad: -0.066, New P: -0.810
-Original Grad: 2.504, -lr * Pred Grad: -0.064, New P: -0.970
iter 23 loss: 0.462
Actual params: [-0.8104, -0.9695]
-Original Grad: 1.140, -lr * Pred Grad: -0.066, New P: -0.877
-Original Grad: 2.591, -lr * Pred Grad: -0.060, New P: -1.029
iter 24 loss: 0.467
Actual params: [-0.8766, -1.0294]
-Original Grad: 0.682, -lr * Pred Grad: -0.066, New P: -0.943
-Original Grad: 2.060, -lr * Pred Grad: -0.052, New P: -1.082
iter 25 loss: 0.473
Actual params: [-0.9426, -1.0817]
-Original Grad: 1.652, -lr * Pred Grad: -0.066, New P: -1.008
-Original Grad: 2.084, -lr * Pred Grad: -0.043, New P: -1.125
iter 26 loss: 0.481
Actual params: [-1.0081, -1.1245]
-Original Grad: 1.380, -lr * Pred Grad: -0.062, New P: -1.071
-Original Grad: 1.755, -lr * Pred Grad: -0.034, New P: -1.159
iter 27 loss: 0.475
Actual params: [-1.0705, -1.1586]
-Original Grad: 0.570, -lr * Pred Grad: -0.053, New P: -1.124
-Original Grad: 1.044, -lr * Pred Grad: -0.030, New P: -1.188
iter 28 loss: 0.475
Actual params: [-1.1239, -1.1881]
-Original Grad: 0.728, -lr * Pred Grad: -0.042, New P: -1.166
-Original Grad: 1.218, -lr * Pred Grad: -0.028, New P: -1.216
iter 29 loss: 0.480
Actual params: [-1.1659, -1.2157]
-Original Grad: 3.901, -lr * Pred Grad: -0.034, New P: -1.200
-Original Grad: 3.691, -lr * Pred Grad: -0.023, New P: -1.239
iter 30 loss: 0.486
Actual params: [-1.1997, -1.2392]
-Original Grad: 0.489, -lr * Pred Grad: -0.030, New P: -1.229
-Original Grad: 0.825, -lr * Pred Grad: -0.010, New P: -1.249
iter 31 loss: 0.485
Actual params: [-1.2294, -1.2493]
-Original Grad: 3.023, -lr * Pred Grad: -0.029, New P: -1.258
-Original Grad: 2.784, -lr * Pred Grad: 0.022, New P: -1.227
iter 32 loss: 0.491
Actual params: [-1.258 , -1.2268]
-Original Grad: 1.127, -lr * Pred Grad: -0.028, New P: -1.286
-Original Grad: 1.433, -lr * Pred Grad: 0.055, New P: -1.172
iter 33 loss: 0.485
Actual params: [-1.2863, -1.1719]
-Original Grad: 0.572, -lr * Pred Grad: -0.027, New P: -1.314
-Original Grad: 0.831, -lr * Pred Grad: 0.066, New P: -1.105
iter 34 loss: 0.486
Actual params: [-1.3136, -1.1054]
-Original Grad: 2.334, -lr * Pred Grad: -0.024, New P: -1.338
-Original Grad: 2.277, -lr * Pred Grad: 0.069, New P: -1.037
iter 35 loss: 0.477
Actual params: [-1.3377, -1.0369]
-Original Grad: 0.701, -lr * Pred Grad: -0.015, New P: -1.353
-Original Grad: 0.865, -lr * Pred Grad: 0.069, New P: -0.968
iter 36 loss: 0.482
Actual params: [-1.3529, -0.9681]
-Original Grad: 1.648, -lr * Pred Grad: 0.010, New P: -1.343
-Original Grad: 0.973, -lr * Pred Grad: 0.069, New P: -0.899
iter 37 loss: 0.475
Actual params: [-1.3431, -0.8992]
-Original Grad: 2.320, -lr * Pred Grad: 0.044, New P: -1.299
-Original Grad: 1.155, -lr * Pred Grad: 0.069, New P: -0.830
iter 38 loss: 0.467
Actual params: [-1.2988, -0.8303]
-Original Grad: 2.102, -lr * Pred Grad: 0.064, New P: -1.235
-Original Grad: 1.374, -lr * Pred Grad: 0.069, New P: -0.761
iter 39 loss: 0.463
Actual params: [-1.2352, -0.7614]
-Original Grad: 2.877, -lr * Pred Grad: 0.068, New P: -1.167
-Original Grad: 1.999, -lr * Pred Grad: 0.069, New P: -0.693
iter 40 loss: 0.456
Actual params: [-1.1671, -0.6926]
-Original Grad: 3.387, -lr * Pred Grad: 0.069, New P: -1.098
-Original Grad: 3.965, -lr * Pred Grad: 0.069, New P: -0.624
iter 41 loss: 0.445
Actual params: [-1.0983, -0.6237]
-Original Grad: 2.575, -lr * Pred Grad: 0.069, New P: -1.029
-Original Grad: 3.255, -lr * Pred Grad: 0.069, New P: -0.555
iter 42 loss: 0.867
Actual params: [-1.0295, -0.5548]
-Original Grad: 1.845, -lr * Pred Grad: 0.069, New P: -0.961
-Original Grad: 2.198, -lr * Pred Grad: 0.069, New P: -0.486
iter 43 loss: 0.904
Actual params: [-0.9606, -0.486 ]
-Original Grad: 0.999, -lr * Pred Grad: 0.069, New P: -0.892
-Original Grad: 1.195, -lr * Pred Grad: 0.069, New P: -0.417
iter 44 loss: 0.237
Actual params: [-0.8917, -0.4171]
-Original Grad: 0.463, -lr * Pred Grad: 0.069, New P: -0.823
-Original Grad: 1.106, -lr * Pred Grad: 0.069, New P: -0.348
iter 45 loss: 0.198
Actual params: [-0.8229, -0.3482]
-Original Grad: 0.277, -lr * Pred Grad: 0.069, New P: -0.754
-Original Grad: 0.618, -lr * Pred Grad: 0.069, New P: -0.279
iter 46 loss: 0.205
Actual params: [-0.754 , -0.2794]
-Original Grad: 0.167, -lr * Pred Grad: 0.069, New P: -0.685
-Original Grad: 0.509, -lr * Pred Grad: 0.069, New P: -0.210
iter 47 loss: 0.222
Actual params: [-0.6851, -0.2105]
-Original Grad: 0.113, -lr * Pred Grad: 0.069, New P: -0.616
-Original Grad: 0.302, -lr * Pred Grad: 0.069, New P: -0.142
iter 48 loss: 0.239
Actual params: [-0.6162, -0.1416]
-Original Grad: 0.050, -lr * Pred Grad: 0.069, New P: -0.547
-Original Grad: 0.174, -lr * Pred Grad: 0.069, New P: -0.073
iter 49 loss: 0.243
Actual params: [-0.5474, -0.0727]
-Original Grad: -0.005, -lr * Pred Grad: 0.069, New P: -0.478
-Original Grad: 0.122, -lr * Pred Grad: 0.069, New P: -0.004
iter 50 loss: 0.251
Actual params: [-0.4785, -0.0038]
-Original Grad: 0.005, -lr * Pred Grad: 0.069, New P: -0.410
-Original Grad: 0.119, -lr * Pred Grad: 0.069, New P: 0.065
iter 51 loss: 0.250
Actual params: [-0.4096,  0.065 ]
-Original Grad: 0.016, -lr * Pred Grad: 0.069, New P: -0.341
-Original Grad: 0.083, -lr * Pred Grad: 0.069, New P: 0.134
iter 52 loss: 0.252
Actual params: [-0.3407,  0.1339]
-Original Grad: 0.003, -lr * Pred Grad: 0.069, New P: -0.272
-Original Grad: 0.060, -lr * Pred Grad: 0.069, New P: 0.203
iter 53 loss: 0.262
Actual params: [-0.2719,  0.2028]
-Original Grad: 0.011, -lr * Pred Grad: 0.069, New P: -0.203
-Original Grad: -0.008, -lr * Pred Grad: 0.069, New P: 0.272
iter 54 loss: 0.282
Actual params: [-0.203 ,  0.2717]
-Original Grad: 0.008, -lr * Pred Grad: 0.069, New P: -0.134
-Original Grad: -0.155, -lr * Pred Grad: 0.069, New P: 0.341
iter 55 loss: 0.325
Actual params: [-0.1341,  0.3405]
-Original Grad: 0.025, -lr * Pred Grad: 0.069, New P: -0.065
-Original Grad: -0.519, -lr * Pred Grad: 0.069, New P: 0.409
iter 56 loss: 0.370
Actual params: [-0.0653,  0.4094]
-Original Grad: 0.032, -lr * Pred Grad: 0.069, New P: 0.003
-Original Grad: -0.372, -lr * Pred Grad: 0.069, New P: 0.478
iter 57 loss: 0.414
Actual params: [0.0034, 0.4783]
-Original Grad: 0.053, -lr * Pred Grad: 0.069, New P: 0.072
-Original Grad: -0.474, -lr * Pred Grad: 0.069, New P: 0.547
iter 58 loss: 0.447
Actual params: [0.072 , 0.5472]
-Original Grad: 0.066, -lr * Pred Grad: 0.068, New P: 0.140
-Original Grad: -0.361, -lr * Pred Grad: 0.069, New P: 0.616
iter 59 loss: 0.458
Actual params: [0.1405, 0.616 ]
-Original Grad: 0.111, -lr * Pred Grad: 0.068, New P: 0.209
-Original Grad: -0.620, -lr * Pred Grad: 0.069, New P: 0.685
iter 60 loss: 0.462
Actual params: [0.2088, 0.6849]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 1.954, -lr * Pred Grad: 0.053, New P: 0.647
-Original Grad: -1.293, -lr * Pred Grad: -0.045, New P: 0.549
iter 0 loss: 0.398
Actual params: [0.6469, 0.5486]
-Original Grad: 2.021, -lr * Pred Grad: 0.066, New P: 0.713
-Original Grad: -0.981, -lr * Pred Grad: -0.063, New P: 0.486
iter 1 loss: 0.276
Actual params: [0.7133, 0.4856]
-Original Grad: 0.796, -lr * Pred Grad: 0.069, New P: 0.782
-Original Grad: -0.159, -lr * Pred Grad: -0.066, New P: 0.420
iter 2 loss: 0.132
Actual params: [0.7819, 0.4201]
-Original Grad: 0.035, -lr * Pred Grad: 0.069, New P: 0.851
-Original Grad: 0.003, -lr * Pred Grad: -0.066, New P: 0.354
iter 3 loss: 0.077
Actual params: [0.8507, 0.3541]
-Original Grad: 0.042, -lr * Pred Grad: 0.069, New P: 0.919
-Original Grad: 0.006, -lr * Pred Grad: -0.066, New P: 0.288
iter 4 loss: 0.063
Actual params: [0.9195, 0.288 ]
-Original Grad: 0.007, -lr * Pred Grad: 0.069, New P: 0.988
-Original Grad: -0.010, -lr * Pred Grad: -0.066, New P: 0.222
iter 5 loss: 0.051
Actual params: [0.9882, 0.2218]
-Original Grad: 0.001, -lr * Pred Grad: 0.068, New P: 1.057
-Original Grad: 0.004, -lr * Pred Grad: -0.066, New P: 0.156
iter 6 loss: 0.039
Actual params: [1.0566, 0.1557]
-Original Grad: -0.027, -lr * Pred Grad: 0.068, New P: 1.124
-Original Grad: 0.012, -lr * Pred Grad: -0.066, New P: 0.090
iter 7 loss: 0.045
Actual params: [1.1243, 0.0896]
-Original Grad: -0.048, -lr * Pred Grad: 0.067, New P: 1.191
-Original Grad: 0.020, -lr * Pred Grad: -0.066, New P: 0.023
iter 8 loss: 0.061
Actual params: [1.1909, 0.0234]
-Original Grad: -0.053, -lr * Pred Grad: 0.065, New P: 1.256
-Original Grad: 0.022, -lr * Pred Grad: -0.066, New P: -0.043
iter 9 loss: 0.077
Actual params: [ 1.256 , -0.0427]
-Original Grad: -0.088, -lr * Pred Grad: 0.062, New P: 1.318
-Original Grad: 0.034, -lr * Pred Grad: -0.066, New P: -0.109
iter 10 loss: 0.091
Actual params: [ 1.3175, -0.1089]
-Original Grad: -0.054, -lr * Pred Grad: 0.056, New P: 1.373
-Original Grad: 0.062, -lr * Pred Grad: -0.066, New P: -0.175
iter 11 loss: 0.106
Actual params: [ 1.3734, -0.175 ]
-Original Grad: -0.102, -lr * Pred Grad: 0.050, New P: 1.423
-Original Grad: 0.074, -lr * Pred Grad: -0.066, New P: -0.241
iter 12 loss: 0.123
Actual params: [ 1.4232, -0.2412]
-Original Grad: -0.141, -lr * Pred Grad: 0.044, New P: 1.467
-Original Grad: 0.074, -lr * Pred Grad: -0.066, New P: -0.307
iter 13 loss: 0.141
Actual params: [ 1.4675, -0.3073]
-Original Grad: -0.160, -lr * Pred Grad: 0.041, New P: 1.508
-Original Grad: 0.053, -lr * Pred Grad: -0.066, New P: -0.373
iter 14 loss: 0.164
Actual params: [ 1.508 , -0.3735]
-Original Grad: -0.231, -lr * Pred Grad: 0.036, New P: 1.544
-Original Grad: 0.116, -lr * Pred Grad: -0.066, New P: -0.440
iter 15 loss: 0.183
Actual params: [ 1.5441, -0.4396]
-Original Grad: -0.231, -lr * Pred Grad: 0.032, New P: 1.576
-Original Grad: 0.109, -lr * Pred Grad: -0.066, New P: -0.506
iter 16 loss: 0.213
Actual params: [ 1.576 , -0.5058]
-Original Grad: -0.217, -lr * Pred Grad: 0.030, New P: 1.606
-Original Grad: 0.086, -lr * Pred Grad: -0.066, New P: -0.572
iter 17 loss: 0.232
Actual params: [ 1.6062, -0.5719]
-Original Grad: -0.198, -lr * Pred Grad: 0.032, New P: 1.638
-Original Grad: 0.085, -lr * Pred Grad: -0.066, New P: -0.638
iter 18 loss: 0.248
Actual params: [ 1.6382, -0.6381]
-Original Grad: -0.197, -lr * Pred Grad: 0.035, New P: 1.673
-Original Grad: 0.099, -lr * Pred Grad: -0.066, New P: -0.704
iter 19 loss: 0.274
Actual params: [ 1.673 , -0.7043]
-Original Grad: -0.147, -lr * Pred Grad: 0.042, New P: 1.715
-Original Grad: 0.152, -lr * Pred Grad: -0.066, New P: -0.770
iter 20 loss: 0.289
Actual params: [ 1.7149, -0.7704]
-Original Grad: -0.098, -lr * Pred Grad: 0.048, New P: 1.763
-Original Grad: 0.240, -lr * Pred Grad: -0.066, New P: -0.837
iter 21 loss: 0.313
Actual params: [ 1.7632, -0.8366]
-Original Grad: -0.102, -lr * Pred Grad: 0.053, New P: 1.816
-Original Grad: 0.349, -lr * Pred Grad: -0.066, New P: -0.903
iter 22 loss: 0.327
Actual params: [ 1.8159, -0.9027]
-Original Grad: -0.074, -lr * Pred Grad: 0.056, New P: 1.871
-Original Grad: 0.468, -lr * Pred Grad: -0.066, New P: -0.969
iter 23 loss: 0.351
Actual params: [ 1.8715, -0.9689]
-Original Grad: 0.026, -lr * Pred Grad: 0.060, New P: 1.932
-Original Grad: 0.482, -lr * Pred Grad: -0.066, New P: -1.035
iter 24 loss: 0.381
Actual params: [ 1.9317, -1.035 ]
-Original Grad: 0.022, -lr * Pred Grad: 0.062, New P: 1.993
-Original Grad: 0.530, -lr * Pred Grad: -0.066, New P: -1.101
iter 25 loss: 0.410
Actual params: [ 1.9934, -1.1012]
-Original Grad: 0.055, -lr * Pred Grad: 0.064, New P: 2.057
-Original Grad: 0.578, -lr * Pred Grad: -0.066, New P: -1.167
iter 26 loss: 0.433
Actual params: [ 2.057 , -1.1673]
-Original Grad: 0.084, -lr * Pred Grad: 0.065, New P: 2.122
-Original Grad: 0.652, -lr * Pred Grad: -0.066, New P: -1.233
iter 27 loss: 0.455
Actual params: [ 2.1216, -1.2335]
-Original Grad: 0.094, -lr * Pred Grad: 0.065, New P: 2.187
-Original Grad: 0.662, -lr * Pred Grad: -0.066, New P: -1.300
iter 28 loss: 0.474
Actual params: [ 2.1869, -1.2996]
-Original Grad: 0.119, -lr * Pred Grad: 0.066, New P: 2.253
-Original Grad: 0.849, -lr * Pred Grad: -0.066, New P: -1.366
iter 29 loss: 0.498
Actual params: [ 2.2528, -1.3658]
-Original Grad: 0.129, -lr * Pred Grad: 0.066, New P: 2.319
-Original Grad: 0.936, -lr * Pred Grad: -0.066, New P: -1.432
iter 30 loss: 0.523
Actual params: [ 2.319 , -1.4318]
-Original Grad: 0.145, -lr * Pred Grad: 0.067, New P: 2.386
-Original Grad: 0.912, -lr * Pred Grad: -0.065, New P: -1.497
iter 31 loss: 0.548
Actual params: [ 2.3856, -1.4972]
-Original Grad: 0.144, -lr * Pred Grad: 0.067, New P: 2.452
-Original Grad: 0.859, -lr * Pred Grad: -0.063, New P: -1.560
iter 32 loss: 0.548
Actual params: [ 2.4523, -1.5598]
-Original Grad: 0.134, -lr * Pred Grad: 0.067, New P: 2.519
-Original Grad: 0.770, -lr * Pred Grad: -0.057, New P: -1.617
iter 33 loss: 0.512
Actual params: [ 2.519 , -1.6167]
-Original Grad: 0.164, -lr * Pred Grad: 0.067, New P: 2.586
-Original Grad: 0.905, -lr * Pred Grad: -0.049, New P: -1.665
iter 34 loss: 0.491
Actual params: [ 2.5858, -1.6654]
-Original Grad: 0.164, -lr * Pred Grad: 0.067, New P: 2.652
-Original Grad: 0.860, -lr * Pred Grad: -0.041, New P: -1.706
iter 35 loss: 0.479
Actual params: [ 2.6524, -1.7059]
-Original Grad: 0.147, -lr * Pred Grad: 0.066, New P: 2.719
-Original Grad: 0.785, -lr * Pred Grad: -0.034, New P: -1.740
iter 36 loss: 0.471
Actual params: [ 2.7187, -1.7401]
-Original Grad: 0.163, -lr * Pred Grad: 0.066, New P: 2.784
-Original Grad: 0.684, -lr * Pred Grad: -0.030, New P: -1.770
iter 37 loss: 0.465
Actual params: [ 2.7844, -1.7701]
-Original Grad: 0.165, -lr * Pred Grad: 0.065, New P: 2.849
-Original Grad: 0.787, -lr * Pred Grad: -0.029, New P: -1.799
iter 38 loss: 0.462
Actual params: [ 2.8492, -1.7988]
-Original Grad: 0.156, -lr * Pred Grad: 0.063, New P: 2.913
-Original Grad: 0.775, -lr * Pred Grad: -0.029, New P: -1.827
iter 39 loss: 0.459
Actual params: [ 2.9126, -1.8273]
-Original Grad: 0.153, -lr * Pred Grad: 0.062, New P: 2.974
-Original Grad: 0.657, -lr * Pred Grad: -0.028, New P: -1.856
iter 40 loss: 0.457
Actual params: [ 2.9742, -1.8557]
-Original Grad: 0.139, -lr * Pred Grad: 0.059, New P: 3.033
-Original Grad: 0.618, -lr * Pred Grad: -0.028, New P: -1.884
iter 41 loss: 0.454
Actual params: [ 3.0333, -1.8835]
-Original Grad: 0.142, -lr * Pred Grad: 0.056, New P: 3.090
-Original Grad: 0.588, -lr * Pred Grad: -0.026, New P: -1.909
iter 42 loss: 0.452
Actual params: [ 3.0896, -1.9091]
-Original Grad: 0.132, -lr * Pred Grad: 0.053, New P: 3.143
-Original Grad: 0.608, -lr * Pred Grad: -0.021, New P: -1.931
iter 43 loss: 0.450
Actual params: [ 3.1428, -1.9306]
-Original Grad: 0.130, -lr * Pred Grad: 0.051, New P: 3.194
-Original Grad: 0.498, -lr * Pred Grad: -0.013, New P: -1.944
iter 44 loss: 0.450
Actual params: [ 3.1939, -1.9438]
-Original Grad: 0.141, -lr * Pred Grad: 0.050, New P: 3.244
-Original Grad: 0.610, -lr * Pred Grad: 0.007, New P: -1.937
iter 45 loss: 0.451
Actual params: [ 3.2438, -1.9366]
-Original Grad: 0.122, -lr * Pred Grad: 0.049, New P: 3.293
-Original Grad: 0.537, -lr * Pred Grad: 0.036, New P: -1.901
iter 46 loss: 0.462
Actual params: [ 3.2932, -1.9007]
-Original Grad: 0.116, -lr * Pred Grad: 0.049, New P: 3.343
-Original Grad: 0.637, -lr * Pred Grad: 0.058, New P: -1.843
iter 47 loss: 0.484
Actual params: [ 3.3426, -1.8431]
-Original Grad: 0.128, -lr * Pred Grad: 0.050, New P: 3.393
-Original Grad: 0.700, -lr * Pred Grad: 0.065, New P: -1.778
iter 48 loss: 0.516
Actual params: [ 3.3928, -1.7779]
-Original Grad: 0.110, -lr * Pred Grad: 0.050, New P: 3.443
-Original Grad: 0.726, -lr * Pred Grad: 0.066, New P: -1.712
iter 49 loss: 0.545
Actual params: [ 3.4432, -1.7115]
-Original Grad: 0.110, -lr * Pred Grad: 0.051, New P: 3.494
-Original Grad: 0.748, -lr * Pred Grad: 0.067, New P: -1.645
iter 50 loss: 0.562
Actual params: [ 3.4941, -1.6449]
-Original Grad: 0.089, -lr * Pred Grad: 0.050, New P: 3.545
-Original Grad: 0.619, -lr * Pred Grad: 0.066, New P: -1.578
iter 51 loss: 0.548
Actual params: [ 3.5446, -1.5784]
-Original Grad: 0.085, -lr * Pred Grad: 0.051, New P: 3.595
-Original Grad: 0.696, -lr * Pred Grad: 0.067, New P: -1.512
iter 52 loss: 0.530
Actual params: [ 3.5953, -1.5119]
-Original Grad: 0.078, -lr * Pred Grad: 0.050, New P: 3.646
-Original Grad: 0.762, -lr * Pred Grad: 0.067, New P: -1.445
iter 53 loss: 0.519
Actual params: [ 3.6457, -1.4452]
-Original Grad: 0.079, -lr * Pred Grad: 0.051, New P: 3.697
-Original Grad: 0.619, -lr * Pred Grad: 0.066, New P: -1.379
iter 54 loss: 0.500
Actual params: [ 3.6968, -1.3788]
-Original Grad: 0.063, -lr * Pred Grad: 0.051, New P: 3.747
-Original Grad: 0.584, -lr * Pred Grad: 0.066, New P: -1.313
iter 55 loss: 0.486
Actual params: [ 3.7474, -1.3127]
-Original Grad: 0.050, -lr * Pred Grad: 0.051, New P: 3.798
-Original Grad: 0.504, -lr * Pred Grad: 0.065, New P: -1.247
iter 56 loss: 0.477
Actual params: [ 3.7983, -1.2474]
-Original Grad: 0.043, -lr * Pred Grad: 0.051, New P: 3.849
-Original Grad: 0.460, -lr * Pred Grad: 0.064, New P: -1.183
iter 57 loss: 0.468
Actual params: [ 3.849 , -1.1832]
-Original Grad: 0.038, -lr * Pred Grad: 0.051, New P: 3.900
-Original Grad: 0.411, -lr * Pred Grad: 0.063, New P: -1.120
iter 58 loss: 0.446
Actual params: [ 3.9001, -1.1204]
-Original Grad: 0.024, -lr * Pred Grad: 0.051, New P: 3.951
-Original Grad: 0.327, -lr * Pred Grad: 0.061, New P: -1.060
iter 59 loss: 0.436
Actual params: [ 3.9509, -1.0597]
-Original Grad: 0.026, -lr * Pred Grad: 0.052, New P: 4.003
-Original Grad: 0.266, -lr * Pred Grad: 0.059, New P: -1.001
iter 60 loss: 0.428
Actual params: [ 4.0025, -1.0012]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.459, -lr * Pred Grad: 0.053, New P: 0.647
-Original Grad: 0.003, -lr * Pred Grad: 0.016, New P: 0.610
iter 0 loss: 0.373
Actual params: [0.6468, 0.6103]
-Original Grad: 0.489, -lr * Pred Grad: 0.066, New P: 0.713
-Original Grad: 0.008, -lr * Pred Grad: -0.039, New P: 0.571
iter 1 loss: 0.323
Actual params: [0.713 , 0.5714]
-Original Grad: 0.363, -lr * Pred Grad: 0.068, New P: 0.781
-Original Grad: -0.016, -lr * Pred Grad: -0.061, New P: 0.510
iter 2 loss: 0.243
Actual params: [0.7813, 0.51  ]
-Original Grad: 0.483, -lr * Pred Grad: 0.069, New P: 0.850
-Original Grad: 0.107, -lr * Pred Grad: -0.065, New P: 0.445
iter 3 loss: 0.175
Actual params: [0.85  , 0.4447]
-Original Grad: 0.302, -lr * Pred Grad: 0.069, New P: 0.919
-Original Grad: 0.071, -lr * Pred Grad: -0.066, New P: 0.379
iter 4 loss: 0.118
Actual params: [0.9185, 0.3786]
-Original Grad: 0.079, -lr * Pred Grad: 0.068, New P: 0.986
-Original Grad: 0.053, -lr * Pred Grad: -0.066, New P: 0.313
iter 5 loss: 0.075
Actual params: [0.9865, 0.3125]
-Original Grad: 0.111, -lr * Pred Grad: 0.066, New P: 1.053
-Original Grad: 0.116, -lr * Pred Grad: -0.066, New P: 0.246
iter 6 loss: 0.054
Actual params: [1.0525, 0.2464]
-Original Grad: 0.083, -lr * Pred Grad: 0.062, New P: 1.114
-Original Grad: 0.073, -lr * Pred Grad: -0.066, New P: 0.180
iter 7 loss: 0.049
Actual params: [1.1143, 0.1802]
-Original Grad: 0.053, -lr * Pred Grad: 0.056, New P: 1.171
-Original Grad: 0.058, -lr * Pred Grad: -0.066, New P: 0.114
iter 8 loss: 0.042
Actual params: [1.1707, 0.1141]
-Original Grad: -0.134, -lr * Pred Grad: 0.048, New P: 1.218
-Original Grad: 0.062, -lr * Pred Grad: -0.066, New P: 0.048
iter 9 loss: 0.037
Actual params: [1.2184, 0.0479]
-Original Grad: -0.187, -lr * Pred Grad: 0.041, New P: 1.260
-Original Grad: 0.109, -lr * Pred Grad: -0.066, New P: -0.018
iter 10 loss: 0.053
Actual params: [ 1.2598, -0.0182]
-Original Grad: -0.128, -lr * Pred Grad: 0.038, New P: 1.297
-Original Grad: 0.116, -lr * Pred Grad: -0.066, New P: -0.084
iter 11 loss: 0.067
Actual params: [ 1.2974, -0.0844]
-Original Grad: -0.055, -lr * Pred Grad: 0.038, New P: 1.335
-Original Grad: 0.128, -lr * Pred Grad: -0.066, New P: -0.151
iter 12 loss: 0.079
Actual params: [ 1.335 , -0.1506]
-Original Grad: -0.001, -lr * Pred Grad: 0.040, New P: 1.375
-Original Grad: 0.239, -lr * Pred Grad: -0.066, New P: -0.217
iter 13 loss: 0.089
Actual params: [ 1.3754, -0.2167]
-Original Grad: 0.034, -lr * Pred Grad: 0.046, New P: 1.421
-Original Grad: 0.457, -lr * Pred Grad: -0.066, New P: -0.283
iter 14 loss: 0.100
Actual params: [ 1.4209, -0.2829]
-Original Grad: 0.156, -lr * Pred Grad: 0.055, New P: 1.476
-Original Grad: 0.369, -lr * Pred Grad: -0.066, New P: -0.349
iter 15 loss: 0.115
Actual params: [ 1.4763, -0.349 ]
-Original Grad: 0.314, -lr * Pred Grad: 0.064, New P: 1.540
-Original Grad: 0.606, -lr * Pred Grad: -0.066, New P: -0.415
iter 16 loss: 0.187
Actual params: [ 1.54  , -0.4152]
-Original Grad: 0.230, -lr * Pred Grad: 0.066, New P: 1.606
-Original Grad: 0.397, -lr * Pred Grad: -0.066, New P: -0.481
iter 17 loss: 0.191
Actual params: [ 1.6062, -0.4813]
-Original Grad: 0.322, -lr * Pred Grad: 0.067, New P: 1.674
-Original Grad: 0.590, -lr * Pred Grad: -0.066, New P: -0.547
iter 18 loss: 0.225
Actual params: [ 1.6735, -0.5475]
-Original Grad: 0.203, -lr * Pred Grad: 0.067, New P: 1.740
-Original Grad: 0.505, -lr * Pred Grad: -0.066, New P: -0.614
iter 19 loss: 0.209
Actual params: [ 1.7403, -0.6136]
-Original Grad: 0.063, -lr * Pred Grad: 0.064, New P: 1.804
-Original Grad: 0.287, -lr * Pred Grad: -0.066, New P: -0.680
iter 20 loss: 0.186
Actual params: [ 1.8039, -0.6798]
-Original Grad: 0.156, -lr * Pred Grad: 0.060, New P: 1.864
-Original Grad: 0.340, -lr * Pred Grad: -0.066, New P: -0.746
iter 21 loss: 0.192
Actual params: [ 1.8635, -0.7458]
-Original Grad: 0.200, -lr * Pred Grad: 0.057, New P: 1.920
-Original Grad: 0.577, -lr * Pred Grad: -0.065, New P: -0.811
iter 22 loss: 0.189
Actual params: [ 1.9201, -0.8112]
-Original Grad: 0.274, -lr * Pred Grad: 0.055, New P: 1.976
-Original Grad: 0.811, -lr * Pred Grad: -0.063, New P: -0.874
iter 23 loss: 0.297
Actual params: [ 1.9755, -0.874 ]
-Original Grad: 0.305, -lr * Pred Grad: 0.055, New P: 2.031
-Original Grad: 0.683, -lr * Pred Grad: -0.058, New P: -0.932
iter 24 loss: 0.321
Actual params: [ 2.0308, -0.9319]
-Original Grad: 0.262, -lr * Pred Grad: 0.054, New P: 2.085
-Original Grad: 0.525, -lr * Pred Grad: -0.050, New P: -0.982
iter 25 loss: 0.327
Actual params: [ 2.0851, -0.9824]
-Original Grad: 0.419, -lr * Pred Grad: 0.058, New P: 2.143
-Original Grad: 0.780, -lr * Pred Grad: -0.040, New P: -1.022
iter 26 loss: 0.330
Actual params: [ 2.1426, -1.0225]
-Original Grad: 0.441, -lr * Pred Grad: 0.058, New P: 2.201
-Original Grad: 0.654, -lr * Pred Grad: -0.030, New P: -1.053
iter 27 loss: 0.332
Actual params: [ 2.2007, -1.0529]
-Original Grad: 0.394, -lr * Pred Grad: 0.059, New P: 2.260
-Original Grad: 0.600, -lr * Pred Grad: -0.019, New P: -1.072
iter 28 loss: 0.331
Actual params: [ 2.2596, -1.0715]
-Original Grad: 0.289, -lr * Pred Grad: 0.056, New P: 2.316
-Original Grad: 0.393, -lr * Pred Grad: 0.011, New P: -1.061
iter 29 loss: 0.332
Actual params: [ 2.3156, -1.061 ]
-Original Grad: 0.349, -lr * Pred Grad: 0.057, New P: 2.372
-Original Grad: 0.548, -lr * Pred Grad: 0.046, New P: -1.015
iter 30 loss: 0.328
Actual params: [ 2.3723, -1.015 ]
-Original Grad: 0.310, -lr * Pred Grad: 0.054, New P: 2.427
-Original Grad: 0.464, -lr * Pred Grad: 0.063, New P: -0.952
iter 31 loss: 0.330
Actual params: [ 2.4266, -0.952 ]
-Original Grad: 0.180, -lr * Pred Grad: 0.053, New P: 2.479
-Original Grad: 0.279, -lr * Pred Grad: 0.062, New P: -0.890
iter 32 loss: 0.318
Actual params: [ 2.4792, -0.89  ]
-Original Grad: 0.130, -lr * Pred Grad: 0.050, New P: 2.529
-Original Grad: 0.228, -lr * Pred Grad: 0.058, New P: -0.832
iter 33 loss: 0.311
Actual params: [ 2.5289, -0.8317]
-Original Grad: 0.152, -lr * Pred Grad: 0.050, New P: 2.579
-Original Grad: 0.281, -lr * Pred Grad: 0.058, New P: -0.774
iter 34 loss: 0.287
Actual params: [ 2.5786, -0.7741]
-Original Grad: 0.119, -lr * Pred Grad: 0.048, New P: 2.626
-Original Grad: 0.261, -lr * Pred Grad: 0.055, New P: -0.719
iter 35 loss: 0.268
Actual params: [ 2.6264, -0.7186]
-Original Grad: 0.100, -lr * Pred Grad: 0.048, New P: 2.674
-Original Grad: 0.192, -lr * Pred Grad: 0.054, New P: -0.664
iter 36 loss: 0.254
Actual params: [ 2.6741, -0.6644]
-Original Grad: 0.093, -lr * Pred Grad: 0.046, New P: 2.720
-Original Grad: 0.094, -lr * Pred Grad: 0.051, New P: -0.613
iter 37 loss: 0.235
Actual params: [ 2.7204, -0.6132]
-Original Grad: 0.059, -lr * Pred Grad: 0.046, New P: 2.766
-Original Grad: 0.071, -lr * Pred Grad: 0.050, New P: -0.563
iter 38 loss: 0.221
Actual params: [ 2.7662, -0.5634]
-Original Grad: 0.042, -lr * Pred Grad: 0.045, New P: 2.811
-Original Grad: 0.100, -lr * Pred Grad: 0.048, New P: -0.516
iter 39 loss: 0.210
Actual params: [ 2.8108, -0.5156]
-Original Grad: 0.072, -lr * Pred Grad: 0.046, New P: 2.856
-Original Grad: 0.200, -lr * Pred Grad: 0.049, New P: -0.466
iter 40 loss: 0.197
Actual params: [ 2.8564, -0.4663]
-Original Grad: 0.025, -lr * Pred Grad: 0.045, New P: 2.901
-Original Grad: 0.124, -lr * Pred Grad: 0.047, New P: -0.419
iter 41 loss: 0.185
Actual params: [ 2.901 , -0.4192]
-Original Grad: 0.043, -lr * Pred Grad: 0.046, New P: 2.947
-Original Grad: 0.218, -lr * Pred Grad: 0.050, New P: -0.369
iter 42 loss: 0.174
Actual params: [ 2.9473, -0.3691]
-Original Grad: -0.007, -lr * Pred Grad: 0.045, New P: 2.992
-Original Grad: 0.088, -lr * Pred Grad: 0.046, New P: -0.323
iter 43 loss: 0.167
Actual params: [ 2.9925, -0.3228]
-Original Grad: -0.010, -lr * Pred Grad: 0.047, New P: 3.039
-Original Grad: 0.087, -lr * Pred Grad: 0.048, New P: -0.275
iter 44 loss: 0.162
Actual params: [ 3.0394, -0.2752]
-Original Grad: -0.007, -lr * Pred Grad: 0.047, New P: 3.087
-Original Grad: 0.110, -lr * Pred Grad: 0.045, New P: -0.230
iter 45 loss: 0.157
Actual params: [ 3.0866, -0.2298]
-Original Grad: -0.025, -lr * Pred Grad: 0.049, New P: 3.136
-Original Grad: 0.037, -lr * Pred Grad: 0.046, New P: -0.184
iter 46 loss: 0.153
Actual params: [ 3.1355, -0.1841]
-Original Grad: -0.026, -lr * Pred Grad: 0.049, New P: 3.185
-Original Grad: 0.032, -lr * Pred Grad: 0.044, New P: -0.140
iter 47 loss: 0.152
Actual params: [ 3.1849, -0.1401]
-Original Grad: -0.021, -lr * Pred Grad: 0.051, New P: 3.236
-Original Grad: 0.024, -lr * Pred Grad: 0.045, New P: -0.095
iter 48 loss: 0.150
Actual params: [ 3.2361, -0.0955]
-Original Grad: -0.036, -lr * Pred Grad: 0.051, New P: 3.287
-Original Grad: 0.004, -lr * Pred Grad: 0.044, New P: -0.051
iter 49 loss: 0.149
Actual params: [ 3.287 , -0.0514]
-Original Grad: -0.046, -lr * Pred Grad: 0.052, New P: 3.339
-Original Grad: -0.002, -lr * Pred Grad: 0.046, New P: -0.006
iter 50 loss: 0.151
Actual params: [ 3.3387, -0.0058]
-Original Grad: -0.032, -lr * Pred Grad: 0.052, New P: 3.390
-Original Grad: -0.007, -lr * Pred Grad: 0.047, New P: 0.041
iter 51 loss: 0.151
Actual params: [3.3905, 0.0408]
-Original Grad: -0.072, -lr * Pred Grad: 0.051, New P: 3.442
-Original Grad: -0.065, -lr * Pred Grad: 0.047, New P: 0.088
iter 52 loss: 0.152
Actual params: [3.4417, 0.0883]
-Original Grad: -0.065, -lr * Pred Grad: 0.051, New P: 3.493
-Original Grad: -0.048, -lr * Pred Grad: 0.049, New P: 0.137
iter 53 loss: 0.155
Actual params: [3.493 , 0.1374]
-Original Grad: -0.047, -lr * Pred Grad: 0.052, New P: 3.545
-Original Grad: -0.020, -lr * Pred Grad: 0.052, New P: 0.190
iter 54 loss: 0.158
Actual params: [3.5449, 0.1895]
-Original Grad: -0.048, -lr * Pred Grad: 0.052, New P: 3.597
-Original Grad: -0.009, -lr * Pred Grad: 0.055, New P: 0.244
iter 55 loss: 0.163
Actual params: [3.5971, 0.2443]
-Original Grad: -0.052, -lr * Pred Grad: 0.052, New P: 3.650
-Original Grad: -0.009, -lr * Pred Grad: 0.057, New P: 0.301
iter 56 loss: 0.167
Actual params: [3.6495, 0.3008]
-Original Grad: -0.077, -lr * Pred Grad: 0.052, New P: 3.701
-Original Grad: -0.018, -lr * Pred Grad: 0.057, New P: 0.358
iter 57 loss: 0.172
Actual params: [3.7011, 0.3577]
-Original Grad: -0.071, -lr * Pred Grad: 0.052, New P: 3.753
-Original Grad: -0.013, -lr * Pred Grad: 0.057, New P: 0.415
iter 58 loss: 0.180
Actual params: [3.753 , 0.4146]
-Original Grad: -0.078, -lr * Pred Grad: 0.051, New P: 3.804
-Original Grad: -0.081, -lr * Pred Grad: 0.054, New P: 0.469
iter 59 loss: 0.191
Actual params: [3.8042, 0.4686]
-Original Grad: -0.082, -lr * Pred Grad: 0.052, New P: 3.856
-Original Grad: -0.124, -lr * Pred Grad: 0.052, New P: 0.520
iter 60 loss: 0.207
Actual params: [3.8557, 0.5201]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: -0.070, -lr * Pred Grad: -0.023, New P: 0.571
-Original Grad: -0.507, -lr * Pred Grad: -0.048, New P: 0.546
iter 0 loss: 0.094
Actual params: [0.5713, 0.5459]
-Original Grad: -0.036, -lr * Pred Grad: -0.058, New P: 0.514
-Original Grad: -0.195, -lr * Pred Grad: -0.063, New P: 0.483
iter 1 loss: 0.076
Actual params: [0.5137, 0.4827]
-Original Grad: 0.051, -lr * Pred Grad: -0.065, New P: 0.449
-Original Grad: -0.261, -lr * Pred Grad: -0.066, New P: 0.417
iter 2 loss: 0.066
Actual params: [0.4489, 0.4172]
-Original Grad: 0.347, -lr * Pred Grad: -0.066, New P: 0.383
-Original Grad: -0.069, -lr * Pred Grad: -0.066, New P: 0.351
iter 3 loss: 0.116
Actual params: [0.383 , 0.3512]
-Original Grad: 0.634, -lr * Pred Grad: -0.066, New P: 0.317
-Original Grad: -0.134, -lr * Pred Grad: -0.066, New P: 0.285
iter 4 loss: 0.176
Actual params: [0.317 , 0.2851]
-Original Grad: 0.510, -lr * Pred Grad: -0.065, New P: 0.252
-Original Grad: -0.136, -lr * Pred Grad: -0.066, New P: 0.219
iter 5 loss: 0.223
Actual params: [0.2518, 0.2189]
-Original Grad: 0.622, -lr * Pred Grad: -0.060, New P: 0.192
-Original Grad: -0.078, -lr * Pred Grad: -0.066, New P: 0.153
iter 6 loss: 0.261
Actual params: [0.1918, 0.1528]
-Original Grad: 0.483, -lr * Pred Grad: -0.037, New P: 0.155
-Original Grad: -0.061, -lr * Pred Grad: -0.066, New P: 0.087
iter 7 loss: 0.285
Actual params: [0.1548, 0.0866]
-Original Grad: 0.346, -lr * Pred Grad: 0.012, New P: 0.167
-Original Grad: -0.014, -lr * Pred Grad: -0.066, New P: 0.020
iter 8 loss: 0.303
Actual params: [0.1671, 0.0205]
-Original Grad: 0.414, -lr * Pred Grad: 0.053, New P: 0.220
-Original Grad: 0.024, -lr * Pred Grad: -0.066, New P: -0.046
iter 9 loss: 0.298
Actual params: [ 0.2204, -0.0457]
-Original Grad: 0.422, -lr * Pred Grad: 0.066, New P: 0.286
-Original Grad: 0.052, -lr * Pred Grad: -0.066, New P: -0.112
iter 10 loss: 0.274
Actual params: [ 0.2864, -0.1118]
-Original Grad: 0.515, -lr * Pred Grad: 0.067, New P: 0.353
-Original Grad: 0.060, -lr * Pred Grad: -0.066, New P: -0.178
iter 11 loss: 0.245
Actual params: [ 0.3532, -0.178 ]
-Original Grad: 0.641, -lr * Pred Grad: 0.067, New P: 0.420
-Original Grad: 0.049, -lr * Pred Grad: -0.066, New P: -0.244
iter 12 loss: 0.214
Actual params: [ 0.42  , -0.2441]
-Original Grad: 0.408, -lr * Pred Grad: 0.066, New P: 0.486
-Original Grad: 0.050, -lr * Pred Grad: -0.066, New P: -0.310
iter 13 loss: 0.171
Actual params: [ 0.4859, -0.3103]
-Original Grad: 0.225, -lr * Pred Grad: 0.063, New P: 0.549
-Original Grad: 0.059, -lr * Pred Grad: -0.066, New P: -0.376
iter 14 loss: 0.133
Actual params: [ 0.5491, -0.3764]
-Original Grad: 0.106, -lr * Pred Grad: 0.058, New P: 0.607
-Original Grad: 0.042, -lr * Pred Grad: -0.066, New P: -0.443
iter 15 loss: 0.108
Actual params: [ 0.6072, -0.4426]
-Original Grad: 0.079, -lr * Pred Grad: 0.054, New P: 0.661
-Original Grad: 0.051, -lr * Pred Grad: -0.066, New P: -0.509
iter 16 loss: 0.097
Actual params: [ 0.661 , -0.5087]
-Original Grad: 0.028, -lr * Pred Grad: 0.049, New P: 0.710
-Original Grad: 0.055, -lr * Pred Grad: -0.066, New P: -0.575
iter 17 loss: 0.094
Actual params: [ 0.7099, -0.5749]
-Original Grad: 0.006, -lr * Pred Grad: 0.046, New P: 0.756
-Original Grad: 0.063, -lr * Pred Grad: -0.066, New P: -0.641
iter 18 loss: 0.094
Actual params: [ 0.7557, -0.641 ]
-Original Grad: 0.001, -lr * Pred Grad: 0.043, New P: 0.799
-Original Grad: 0.097, -lr * Pred Grad: -0.066, New P: -0.707
iter 19 loss: 0.100
Actual params: [ 0.7988, -0.7072]
-Original Grad: -0.011, -lr * Pred Grad: 0.042, New P: 0.840
-Original Grad: 0.102, -lr * Pred Grad: -0.066, New P: -0.773
iter 20 loss: 0.108
Actual params: [ 0.8404, -0.7733]
-Original Grad: -0.021, -lr * Pred Grad: 0.041, New P: 0.881
-Original Grad: 0.128, -lr * Pred Grad: -0.066, New P: -0.839
iter 21 loss: 0.120
Actual params: [ 0.8812, -0.8395]
-Original Grad: -0.010, -lr * Pred Grad: 0.042, New P: 0.923
-Original Grad: 0.139, -lr * Pred Grad: -0.066, New P: -0.906
iter 22 loss: 0.136
Actual params: [ 0.9228, -0.9056]
-Original Grad: -0.012, -lr * Pred Grad: 0.043, New P: 0.966
-Original Grad: 0.200, -lr * Pred Grad: -0.066, New P: -0.972
iter 23 loss: 0.156
Actual params: [ 0.966 , -0.9718]
-Original Grad: -0.026, -lr * Pred Grad: 0.046, New P: 1.012
-Original Grad: 0.203, -lr * Pred Grad: -0.066, New P: -1.038
iter 24 loss: 0.174
Actual params: [ 1.012 , -1.0379]
-Original Grad: -0.016, -lr * Pred Grad: 0.050, New P: 1.062
-Original Grad: 0.240, -lr * Pred Grad: -0.066, New P: -1.104
iter 25 loss: 0.196
Actual params: [ 1.062 , -1.1041]
-Original Grad: 0.018, -lr * Pred Grad: 0.055, New P: 1.117
-Original Grad: 0.298, -lr * Pred Grad: -0.066, New P: -1.170
iter 26 loss: 0.223
Actual params: [ 1.1174, -1.1702]
-Original Grad: 0.016, -lr * Pred Grad: 0.059, New P: 1.177
-Original Grad: 0.339, -lr * Pred Grad: -0.066, New P: -1.236
iter 27 loss: 0.255
Actual params: [ 1.1767, -1.2364]
-Original Grad: 0.009, -lr * Pred Grad: 0.062, New P: 1.238
-Original Grad: 0.375, -lr * Pred Grad: -0.066, New P: -1.303
iter 28 loss: 0.281
Actual params: [ 1.2383, -1.3025]
-Original Grad: -0.038, -lr * Pred Grad: 0.061, New P: 1.299
-Original Grad: 0.457, -lr * Pred Grad: -0.066, New P: -1.369
iter 29 loss: 0.307
Actual params: [ 1.299 , -1.3687]
-Original Grad: -0.031, -lr * Pred Grad: 0.059, New P: 1.358
-Original Grad: 0.589, -lr * Pred Grad: -0.066, New P: -1.435
iter 30 loss: 0.334
Actual params: [ 1.3583, -1.4348]
-Original Grad: -0.015, -lr * Pred Grad: 0.058, New P: 1.416
-Original Grad: 0.498, -lr * Pred Grad: -0.066, New P: -1.501
iter 31 loss: 0.363
Actual params: [ 1.4162, -1.501 ]
-Original Grad: 0.001, -lr * Pred Grad: 0.057, New P: 1.474
-Original Grad: 0.653, -lr * Pred Grad: -0.066, New P: -1.567
iter 32 loss: 0.391
Actual params: [ 1.4737, -1.5672]
-Original Grad: -0.001, -lr * Pred Grad: 0.057, New P: 1.531
-Original Grad: 0.631, -lr * Pred Grad: -0.066, New P: -1.633
iter 33 loss: 0.417
Actual params: [ 1.5307, -1.6333]
-Original Grad: -0.011, -lr * Pred Grad: 0.057, New P: 1.588
-Original Grad: 0.572, -lr * Pred Grad: -0.066, New P: -1.699
iter 34 loss: 0.445
Actual params: [ 1.5875, -1.6995]
-Original Grad: 0.008, -lr * Pred Grad: 0.057, New P: 1.645
-Original Grad: 0.523, -lr * Pred Grad: -0.066, New P: -1.766
iter 35 loss: 0.471
Actual params: [ 1.6447, -1.7656]
-Original Grad: 0.045, -lr * Pred Grad: 0.058, New P: 1.703
-Original Grad: 0.518, -lr * Pred Grad: -0.066, New P: -1.831
iter 36 loss: 0.496
Actual params: [ 1.703 , -1.8314]
-Original Grad: 0.072, -lr * Pred Grad: 0.060, New P: 1.763
-Original Grad: 0.554, -lr * Pred Grad: -0.064, New P: -1.896
iter 37 loss: 0.521
Actual params: [ 1.7626, -1.8957]
-Original Grad: 0.101, -lr * Pred Grad: 0.061, New P: 1.823
-Original Grad: 0.450, -lr * Pred Grad: -0.061, New P: -1.956
iter 38 loss: 0.544
Actual params: [ 1.8234, -1.9565]
-Original Grad: 0.091, -lr * Pred Grad: 0.061, New P: 1.884
-Original Grad: 0.515, -lr * Pred Grad: -0.055, New P: -2.012
iter 39 loss: 0.563
Actual params: [ 1.8844, -2.0117]
-Original Grad: 0.100, -lr * Pred Grad: 0.061, New P: 1.946
-Original Grad: 0.461, -lr * Pred Grad: -0.048, New P: -2.060
iter 40 loss: 0.579
Actual params: [ 1.9456, -2.0597]
-Original Grad: 0.084, -lr * Pred Grad: 0.060, New P: 2.006
-Original Grad: 0.471, -lr * Pred Grad: -0.041, New P: -2.100
iter 41 loss: 0.590
Actual params: [ 2.0061, -2.1004]
-Original Grad: 0.073, -lr * Pred Grad: 0.060, New P: 2.066
-Original Grad: 0.490, -lr * Pred Grad: -0.035, New P: -2.135
iter 42 loss: 0.599
Actual params: [ 2.0657, -2.1354]
-Original Grad: 0.077, -lr * Pred Grad: 0.059, New P: 2.124
-Original Grad: 0.472, -lr * Pred Grad: -0.031, New P: -2.166
iter 43 loss: 0.603
Actual params: [ 2.1245, -2.166 ]
-Original Grad: 0.080, -lr * Pred Grad: 0.058, New P: 2.183
-Original Grad: 0.534, -lr * Pred Grad: -0.029, New P: -2.195
iter 44 loss: 0.608
Actual params: [ 2.1826, -2.1949]
-Original Grad: 0.084, -lr * Pred Grad: 0.058, New P: 2.240
-Original Grad: 0.499, -lr * Pred Grad: -0.029, New P: -2.223
iter 45 loss: 0.610
Actual params: [ 2.2401, -2.2234]
-Original Grad: 0.090, -lr * Pred Grad: 0.057, New P: 2.297
-Original Grad: 0.464, -lr * Pred Grad: -0.028, New P: -2.252
iter 46 loss: 0.612
Actual params: [ 2.2974, -2.2519]
-Original Grad: 0.086, -lr * Pred Grad: 0.057, New P: 2.354
-Original Grad: 0.484, -lr * Pred Grad: -0.028, New P: -2.280
iter 47 loss: 0.614
Actual params: [ 2.3542, -2.2802]
-Original Grad: 0.079, -lr * Pred Grad: 0.056, New P: 2.411
-Original Grad: 0.510, -lr * Pred Grad: -0.028, New P: -2.308
iter 48 loss: 0.616
Actual params: [ 2.4106, -2.3078]
-Original Grad: 0.083, -lr * Pred Grad: 0.056, New P: 2.467
-Original Grad: 0.493, -lr * Pred Grad: -0.026, New P: -2.333
iter 49 loss: 0.617
Actual params: [ 2.4666, -2.3334]
-Original Grad: 0.083, -lr * Pred Grad: 0.056, New P: 2.522
-Original Grad: 0.467, -lr * Pred Grad: -0.023, New P: -2.357
iter 50 loss: 0.618
Actual params: [ 2.5223, -2.3568]
-Original Grad: 0.085, -lr * Pred Grad: 0.056, New P: 2.578
-Original Grad: 0.475, -lr * Pred Grad: -0.021, New P: -2.377
iter 51 loss: 0.617
Actual params: [ 2.5778, -2.3775]
-Original Grad: 0.088, -lr * Pred Grad: 0.055, New P: 2.633
-Original Grad: 0.499, -lr * Pred Grad: -0.016, New P: -2.394
iter 52 loss: 0.616
Actual params: [ 2.6333, -2.3936]
-Original Grad: 0.095, -lr * Pred Grad: 0.056, New P: 2.689
-Original Grad: 0.418, -lr * Pred Grad: -0.001, New P: -2.395
iter 53 loss: 0.613
Actual params: [ 2.6889, -2.3949]
-Original Grad: 0.087, -lr * Pred Grad: 0.055, New P: 2.744
-Original Grad: 0.472, -lr * Pred Grad: 0.023, New P: -2.372
iter 54 loss: 0.604
Actual params: [ 2.7443, -2.3716]
-Original Grad: 0.081, -lr * Pred Grad: 0.055, New P: 2.799
-Original Grad: 0.466, -lr * Pred Grad: 0.049, New P: -2.323
iter 55 loss: 0.590
Actual params: [ 2.7994, -2.3231]
-Original Grad: 0.096, -lr * Pred Grad: 0.055, New P: 2.855
-Original Grad: 0.472, -lr * Pred Grad: 0.061, New P: -2.262
iter 56 loss: 0.563
Actual params: [ 2.8548, -2.2622]
-Original Grad: 0.086, -lr * Pred Grad: 0.055, New P: 2.910
-Original Grad: 0.481, -lr * Pred Grad: 0.060, New P: -2.202
iter 57 loss: 0.536
Actual params: [ 2.9098, -2.202 ]
-Original Grad: 0.083, -lr * Pred Grad: 0.055, New P: 2.965
-Original Grad: 0.514, -lr * Pred Grad: 0.062, New P: -2.140
iter 58 loss: 0.506
Actual params: [ 2.9649, -2.1396]
-Original Grad: 0.078, -lr * Pred Grad: 0.055, New P: 3.020
-Original Grad: 0.448, -lr * Pred Grad: 0.061, New P: -2.078
iter 59 loss: 0.471
Actual params: [ 3.0196, -2.0783]
-Original Grad: 0.081, -lr * Pred Grad: 0.055, New P: 3.074
-Original Grad: 0.474, -lr * Pred Grad: 0.062, New P: -2.016
iter 60 loss: 0.441
Actual params: [ 3.0744, -2.0162]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.082, -lr * Pred Grad: 0.043, New P: 0.637
-Original Grad: -0.304, -lr * Pred Grad: -0.048, New P: 0.546
iter 0 loss: 0.154
Actual params: [0.6374, 0.5457]
-Original Grad: -0.001, -lr * Pred Grad: 0.042, New P: 0.680
-Original Grad: -0.187, -lr * Pred Grad: -0.063, New P: 0.483
iter 1 loss: 0.117
Actual params: [0.6797, 0.4826]
-Original Grad: -0.031, -lr * Pred Grad: -0.027, New P: 0.653
-Original Grad: -0.064, -lr * Pred Grad: -0.066, New P: 0.417
iter 2 loss: 0.084
Actual params: [0.6527, 0.417 ]
-Original Grad: -0.036, -lr * Pred Grad: -0.057, New P: 0.595
-Original Grad: -0.016, -lr * Pred Grad: -0.066, New P: 0.351
iter 3 loss: 0.072
Actual params: [0.5953, 0.351 ]
-Original Grad: -0.042, -lr * Pred Grad: -0.065, New P: 0.531
-Original Grad: 0.004, -lr * Pred Grad: -0.066, New P: 0.285
iter 4 loss: 0.064
Actual params: [0.5307, 0.2848]
-Original Grad: -0.024, -lr * Pred Grad: -0.066, New P: 0.465
-Original Grad: 0.009, -lr * Pred Grad: -0.066, New P: 0.219
iter 5 loss: 0.057
Actual params: [0.4649, 0.2187]
-Original Grad: 0.020, -lr * Pred Grad: -0.066, New P: 0.399
-Original Grad: 0.016, -lr * Pred Grad: -0.066, New P: 0.153
iter 6 loss: 0.051
Actual params: [0.3988, 0.1526]
-Original Grad: 0.024, -lr * Pred Grad: -0.066, New P: 0.333
-Original Grad: 0.035, -lr * Pred Grad: -0.066, New P: 0.086
iter 7 loss: 0.049
Actual params: [0.3327, 0.0864]
-Original Grad: 0.037, -lr * Pred Grad: -0.066, New P: 0.267
-Original Grad: 0.078, -lr * Pred Grad: -0.066, New P: 0.020
iter 8 loss: 0.046
Actual params: [0.2665, 0.0203]
-Original Grad: 0.044, -lr * Pred Grad: -0.066, New P: 0.200
-Original Grad: 0.052, -lr * Pred Grad: -0.066, New P: -0.046
iter 9 loss: 0.046
Actual params: [ 0.2004, -0.0459]
-Original Grad: 0.018, -lr * Pred Grad: -0.066, New P: 0.134
-Original Grad: 0.079, -lr * Pred Grad: -0.066, New P: -0.112
iter 10 loss: 0.050
Actual params: [ 0.1342, -0.112 ]
-Original Grad: 0.048, -lr * Pred Grad: -0.066, New P: 0.068
-Original Grad: 0.098, -lr * Pred Grad: -0.066, New P: -0.178
iter 11 loss: 0.052
Actual params: [ 0.0681, -0.1782]
-Original Grad: 0.043, -lr * Pred Grad: -0.066, New P: 0.002
-Original Grad: 0.153, -lr * Pred Grad: -0.066, New P: -0.244
iter 12 loss: 0.055
Actual params: [ 0.0019, -0.2443]
-Original Grad: 0.060, -lr * Pred Grad: -0.066, New P: -0.064
-Original Grad: 0.207, -lr * Pred Grad: -0.066, New P: -0.310
iter 13 loss: 0.060
Actual params: [-0.0642, -0.3105]
-Original Grad: 0.049, -lr * Pred Grad: -0.066, New P: -0.130
-Original Grad: 0.277, -lr * Pred Grad: -0.066, New P: -0.377
iter 14 loss: 0.069
Actual params: [-0.1304, -0.3766]
-Original Grad: 0.112, -lr * Pred Grad: -0.066, New P: -0.197
-Original Grad: 0.525, -lr * Pred Grad: -0.066, New P: -0.443
iter 15 loss: 0.079
Actual params: [-0.1965, -0.4428]
-Original Grad: 0.158, -lr * Pred Grad: -0.066, New P: -0.263
-Original Grad: 0.402, -lr * Pred Grad: -0.066, New P: -0.509
iter 16 loss: 0.086
Actual params: [-0.2627, -0.509 ]
-Original Grad: 0.171, -lr * Pred Grad: -0.066, New P: -0.329
-Original Grad: 0.530, -lr * Pred Grad: -0.066, New P: -0.575
iter 17 loss: 0.095
Actual params: [-0.3288, -0.5751]
-Original Grad: 0.341, -lr * Pred Grad: -0.066, New P: -0.395
-Original Grad: 0.812, -lr * Pred Grad: -0.066, New P: -0.641
iter 18 loss: 0.107
Actual params: [-0.395 , -0.6413]
-Original Grad: 0.380, -lr * Pred Grad: -0.066, New P: -0.461
-Original Grad: 1.838, -lr * Pred Grad: -0.066, New P: -0.707
iter 19 loss: 0.144
Actual params: [-0.4612, -0.7074]
-Original Grad: 0.391, -lr * Pred Grad: -0.066, New P: -0.527
-Original Grad: 2.144, -lr * Pred Grad: -0.066, New P: -0.774
iter 20 loss: 0.120
Actual params: [-0.5273, -0.7736]
-Original Grad: 0.374, -lr * Pred Grad: -0.066, New P: -0.593
-Original Grad: 2.016, -lr * Pred Grad: -0.066, New P: -0.840
iter 21 loss: 0.219
Actual params: [-0.5935, -0.8397]
-Original Grad: 0.359, -lr * Pred Grad: -0.066, New P: -0.660
-Original Grad: 1.617, -lr * Pred Grad: -0.066, New P: -0.906
iter 22 loss: 0.225
Actual params: [-0.6596, -0.9057]
-Original Grad: 0.339, -lr * Pred Grad: -0.066, New P: -0.726
-Original Grad: 1.717, -lr * Pred Grad: -0.065, New P: -0.971
iter 23 loss: 0.241
Actual params: [-0.7258, -0.9708]
-Original Grad: 0.330, -lr * Pred Grad: -0.066, New P: -0.792
-Original Grad: 1.303, -lr * Pred Grad: -0.061, New P: -1.032
iter 24 loss: 0.247
Actual params: [-0.7919, -1.0322]
-Original Grad: 0.520, -lr * Pred Grad: -0.066, New P: -0.858
-Original Grad: 1.245, -lr * Pred Grad: -0.054, New P: -1.086
iter 25 loss: 0.252
Actual params: [-0.8581, -1.0857]
-Original Grad: 0.860, -lr * Pred Grad: -0.066, New P: -0.924
-Original Grad: 1.166, -lr * Pred Grad: -0.043, New P: -1.129
iter 26 loss: 0.257
Actual params: [-0.9242, -1.1289]
-Original Grad: 0.926, -lr * Pred Grad: -0.066, New P: -0.990
-Original Grad: 1.113, -lr * Pred Grad: -0.034, New P: -1.163
iter 27 loss: 0.262
Actual params: [-0.9902, -1.1629]
-Original Grad: 0.896, -lr * Pred Grad: -0.065, New P: -1.056
-Original Grad: 1.087, -lr * Pred Grad: -0.029, New P: -1.192
iter 28 loss: 0.267
Actual params: [-1.0555, -1.1922]
-Original Grad: 0.990, -lr * Pred Grad: -0.063, New P: -1.118
-Original Grad: 0.977, -lr * Pred Grad: -0.027, New P: -1.219
iter 29 loss: 0.277
Actual params: [-1.1181, -1.2191]
-Original Grad: 0.921, -lr * Pred Grad: -0.058, New P: -1.176
-Original Grad: 1.095, -lr * Pred Grad: -0.019, New P: -1.238
iter 30 loss: 0.284
Actual params: [-1.1758, -1.2377]
-Original Grad: 0.801, -lr * Pred Grad: -0.050, New P: -1.226
-Original Grad: 1.013, -lr * Pred Grad: 0.004, New P: -1.234
iter 31 loss: 0.290
Actual params: [-1.2262, -1.2335]
-Original Grad: 0.903, -lr * Pred Grad: -0.041, New P: -1.267
-Original Grad: 1.032, -lr * Pred Grad: 0.040, New P: -1.194
iter 32 loss: 0.294
Actual params: [-1.2674, -1.1939]
-Original Grad: 0.792, -lr * Pred Grad: -0.033, New P: -1.301
-Original Grad: 0.923, -lr * Pred Grad: 0.062, New P: -1.132
iter 33 loss: 0.289
Actual params: [-1.3006, -1.1319]
-Original Grad: 0.929, -lr * Pred Grad: -0.029, New P: -1.330
-Original Grad: 0.947, -lr * Pred Grad: 0.068, New P: -1.064
iter 34 loss: 0.282
Actual params: [-1.33  , -1.0641]
-Original Grad: 0.750, -lr * Pred Grad: -0.028, New P: -1.358
-Original Grad: 0.873, -lr * Pred Grad: 0.069, New P: -0.995
iter 35 loss: 0.275
Actual params: [-1.3577, -0.9953]
-Original Grad: 0.915, -lr * Pred Grad: -0.023, New P: -1.381
-Original Grad: 0.835, -lr * Pred Grad: 0.069, New P: -0.926
iter 36 loss: 0.266
Actual params: [-1.3809, -0.9265]
-Original Grad: 1.033, -lr * Pred Grad: -0.010, New P: -1.391
-Original Grad: 0.903, -lr * Pred Grad: 0.069, New P: -0.858
iter 37 loss: 0.259
Actual params: [-1.391 , -0.8576]
-Original Grad: 0.931, -lr * Pred Grad: 0.016, New P: -1.375
-Original Grad: 0.983, -lr * Pred Grad: 0.069, New P: -0.789
iter 38 loss: 0.255
Actual params: [-1.3753, -0.7887]
-Original Grad: 0.929, -lr * Pred Grad: 0.048, New P: -1.327
-Original Grad: 0.877, -lr * Pred Grad: 0.069, New P: -0.720
iter 39 loss: 0.249
Actual params: [-1.327 , -0.7199]
-Original Grad: 1.346, -lr * Pred Grad: 0.065, New P: -1.262
-Original Grad: 1.604, -lr * Pred Grad: 0.069, New P: -0.651
iter 40 loss: 0.231
Actual params: [-1.2623, -0.651 ]
-Original Grad: 1.555, -lr * Pred Grad: 0.068, New P: -1.195
-Original Grad: 2.031, -lr * Pred Grad: 0.069, New P: -0.582
iter 41 loss: 0.221
Actual params: [-1.1946, -0.5821]
-Original Grad: 1.533, -lr * Pred Grad: 0.068, New P: -1.127
-Original Grad: 2.227, -lr * Pred Grad: 0.069, New P: -0.513
iter 42 loss: 0.262
Actual params: [-1.1266, -0.5132]
-Original Grad: 1.266, -lr * Pred Grad: 0.068, New P: -1.058
-Original Grad: 0.995, -lr * Pred Grad: 0.069, New P: -0.444
iter 43 loss: 0.147
Actual params: [-1.0585, -0.4444]
-Original Grad: 1.071, -lr * Pred Grad: 0.068, New P: -0.990
-Original Grad: 0.914, -lr * Pred Grad: 0.069, New P: -0.375
iter 44 loss: 0.130
Actual params: [-0.9904, -0.3755]
-Original Grad: 0.775, -lr * Pred Grad: 0.068, New P: -0.922
-Original Grad: 0.940, -lr * Pred Grad: 0.069, New P: -0.307
iter 45 loss: 0.113
Actual params: [-0.9223, -0.3066]
-Original Grad: 0.677, -lr * Pred Grad: 0.068, New P: -0.854
-Original Grad: 0.859, -lr * Pred Grad: 0.069, New P: -0.238
iter 46 loss: 0.098
Actual params: [-0.8542, -0.2377]
-Original Grad: 0.439, -lr * Pred Grad: 0.068, New P: -0.786
-Original Grad: 0.590, -lr * Pred Grad: 0.069, New P: -0.169
iter 47 loss: 0.088
Actual params: [-0.7862, -0.1689]
-Original Grad: 0.217, -lr * Pred Grad: 0.068, New P: -0.718
-Original Grad: 0.300, -lr * Pred Grad: 0.069, New P: -0.100
iter 48 loss: 0.084
Actual params: [-0.7184, -0.1   ]
-Original Grad: 0.177, -lr * Pred Grad: 0.067, New P: -0.651
-Original Grad: 0.178, -lr * Pred Grad: 0.069, New P: -0.031
iter 49 loss: 0.078
Actual params: [-0.651 , -0.0311]
-Original Grad: 0.179, -lr * Pred Grad: 0.067, New P: -0.584
-Original Grad: 0.093, -lr * Pred Grad: 0.069, New P: 0.038
iter 50 loss: 0.078
Actual params: [-0.5844,  0.0378]
-Original Grad: 0.139, -lr * Pred Grad: 0.065, New P: -0.520
-Original Grad: 0.078, -lr * Pred Grad: 0.069, New P: 0.107
iter 51 loss: 0.086
Actual params: [-0.5197,  0.1066]
-Original Grad: 0.153, -lr * Pred Grad: 0.062, New P: -0.457
-Original Grad: 0.081, -lr * Pred Grad: 0.069, New P: 0.176
iter 52 loss: 0.094
Actual params: [-0.4574,  0.1755]
-Original Grad: 0.133, -lr * Pred Grad: 0.060, New P: -0.398
-Original Grad: -0.022, -lr * Pred Grad: 0.069, New P: 0.244
iter 53 loss: 0.107
Actual params: [-0.3977,  0.2444]
-Original Grad: 0.131, -lr * Pred Grad: 0.057, New P: -0.340
-Original Grad: -0.064, -lr * Pred Grad: 0.069, New P: 0.313
iter 54 loss: 0.124
Actual params: [-0.3404,  0.3132]
-Original Grad: 0.112, -lr * Pred Grad: 0.055, New P: -0.285
-Original Grad: -0.147, -lr * Pred Grad: 0.069, New P: 0.382
iter 55 loss: 0.142
Actual params: [-0.2854,  0.3821]
-Original Grad: 0.108, -lr * Pred Grad: 0.053, New P: -0.232
-Original Grad: -0.150, -lr * Pred Grad: 0.069, New P: 0.451
iter 56 loss: 0.164
Actual params: [-0.232 ,  0.4508]
-Original Grad: 0.098, -lr * Pred Grad: 0.052, New P: -0.180
-Original Grad: -0.303, -lr * Pred Grad: 0.069, New P: 0.519
iter 57 loss: 0.189
Actual params: [-0.18  ,  0.5194]
-Original Grad: 0.078, -lr * Pred Grad: 0.051, New P: -0.129
-Original Grad: -0.427, -lr * Pred Grad: 0.068, New P: 0.588
iter 58 loss: 0.221
Actual params: [-0.1294,  0.5879]
-Original Grad: 0.076, -lr * Pred Grad: 0.049, New P: -0.080
-Original Grad: -0.544, -lr * Pred Grad: 0.068, New P: 0.656
iter 59 loss: 0.261
Actual params: [-0.0799,  0.6564]
-Original Grad: 0.071, -lr * Pred Grad: 0.048, New P: -0.032
-Original Grad: -0.684, -lr * Pred Grad: 0.068, New P: 0.725
iter 60 loss: 0.316
Actual params: [-0.0316,  0.7248]
