Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.496, -lr * Pred Grad:  0.394, New P: 0.988
-Original Grad: 0.279, -lr * Pred Grad:  0.317, New P: 0.911
iter 0 loss: 0.739
Actual params: [0.9877, 0.911 ]
-Original Grad: -0.490, -lr * Pred Grad:  -0.101, New P: 0.887
-Original Grad: 0.206, -lr * Pred Grad:  0.170, New P: 1.081
iter 1 loss: 0.717
Actual params: [0.8868, 1.0809]
-Original Grad: -4.353, -lr * Pred Grad:  -0.273, New P: 0.614
-Original Grad: 1.597, -lr * Pred Grad:  0.165, New P: 1.246
iter 2 loss: 1.607
Actual params: [0.6142, 1.2456]
-Original Grad: 0.186, -lr * Pred Grad:  -0.004, New P: 0.610
-Original Grad: -0.222, -lr * Pred Grad:  -0.091, New P: 1.154
iter 3 loss: 0.677
Actual params: [0.6098, 1.1544]
-Original Grad: 0.738, -lr * Pred Grad:  0.022, New P: 0.632
-Original Grad: -0.124, -lr * Pred Grad:  -0.038, New P: 1.116
iter 4 loss: 0.668
Actual params: [0.6318, 1.1159]
-Original Grad: 0.200, -lr * Pred Grad:  0.008, New P: 0.640
-Original Grad: -0.234, -lr * Pred Grad:  -0.071, New P: 1.045
iter 5 loss: 0.650
Actual params: [0.6401, 1.045 ]
-Original Grad: 1.001, -lr * Pred Grad:  0.022, New P: 0.662
-Original Grad: -0.047, -lr * Pred Grad:  -0.042, New P: 1.003
iter 6 loss: 0.643
Actual params: [0.6619, 1.0028]
-Original Grad: -0.377, -lr * Pred Grad:  0.011, New P: 0.673
-Original Grad: -0.507, -lr * Pred Grad:  -0.079, New P: 0.923
iter 7 loss: 0.624
Actual params: [0.6728, 0.9234]
-Original Grad: 1.992, -lr * Pred Grad:  0.015, New P: 0.688
-Original Grad: 0.667, -lr * Pred Grad:  0.020, New P: 0.943
iter 8 loss: 0.623
Actual params: [0.688, 0.943]
-Original Grad: -0.954, -lr * Pred Grad:  0.014, New P: 0.702
-Original Grad: -0.848, -lr * Pred Grad:  -0.066, New P: 0.877
iter 9 loss: 0.599
Actual params: [0.702 , 0.8767]
-Original Grad: 2.105, -lr * Pred Grad:  -0.002, New P: 0.700
-Original Grad: 1.365, -lr * Pred Grad:  0.044, New P: 0.921
iter 10 loss: 0.606
Actual params: [0.7001, 0.9209]
-Original Grad: -0.865, -lr * Pred Grad:  0.015, New P: 0.715
-Original Grad: -0.981, -lr * Pred Grad:  -0.046, New P: 0.875
iter 11 loss: 0.592
Actual params: [0.7147, 0.8753]
-Original Grad: 1.678, -lr * Pred Grad:  0.006, New P: 0.721
-Original Grad: 0.990, -lr * Pred Grad:  0.008, New P: 0.884
iter 12 loss: 0.591
Actual params: [0.7211, 0.8838]
-Original Grad: 0.758, -lr * Pred Grad:  0.015, New P: 0.736
-Original Grad: -0.227, -lr * Pred Grad:  -0.018, New P: 0.866
iter 13 loss: 0.578
Actual params: [0.7359, 0.8658]
-Original Grad: 1.129, -lr * Pred Grad:  0.006, New P: 0.742
-Original Grad: 0.806, -lr * Pred Grad:  0.000, New P: 0.866
iter 14 loss: 0.572
Actual params: [0.742 , 0.8661]
-Original Grad: 1.072, -lr * Pred Grad:  0.007, New P: 0.749
-Original Grad: 0.666, -lr * Pred Grad:  -0.002, New P: 0.865
iter 15 loss: 0.561
Actual params: [0.7494, 0.8645]
-Original Grad: 0.883, -lr * Pred Grad:  0.009, New P: 0.759
-Original Grad: 0.185, -lr * Pred Grad:  -0.005, New P: 0.859
iter 16 loss: 0.554
Actual params: [0.7588, 0.8592]
-Original Grad: 0.869, -lr * Pred Grad:  -0.001, New P: 0.757
-Original Grad: 3.117, -lr * Pred Grad:  0.008, New P: 0.868
iter 17 loss: 0.550
Actual params: [0.7573, 0.8676]
-Original Grad: 0.402, -lr * Pred Grad:  0.008, New P: 0.766
-Original Grad: -1.866, -lr * Pred Grad:  -0.006, New P: 0.861
iter 18 loss: 0.541
Actual params: [0.7658, 0.8613]
-Original Grad: 0.794, -lr * Pred Grad:  0.005, New P: 0.770
-Original Grad: 1.491, -lr * Pred Grad:  0.001, New P: 0.863
iter 19 loss: 0.537
Actual params: [0.7703, 0.8626]
-Original Grad: 0.849, -lr * Pred Grad:  0.007, New P: 0.778
-Original Grad: -0.100, -lr * Pred Grad:  -0.001, New P: 0.861
iter 20 loss: 0.531
Actual params: [0.7777, 0.8615]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.053, -lr * Pred Grad:  0.083, New P: 0.677
-Original Grad: -0.040, -lr * Pred Grad:  -0.039, New P: 0.555
iter 0 loss: 0.312
Actual params: [0.6771, 0.5552]
-Original Grad: -0.070, -lr * Pred Grad:  0.015, New P: 0.692
-Original Grad: 0.057, -lr * Pred Grad:  0.039, New P: 0.594
iter 1 loss: 0.260
Actual params: [0.6922, 0.5938]
-Original Grad: 0.086, -lr * Pred Grad:  0.048, New P: 0.740
-Original Grad: -0.059, -lr * Pred Grad:  0.042, New P: 0.636
iter 2 loss: 0.266
Actual params: [0.7397, 0.6355]
-Original Grad: -0.012, -lr * Pred Grad:  0.021, New P: 0.761
-Original Grad: 0.014, -lr * Pred Grad:  0.034, New P: 0.669
iter 3 loss: 0.266
Actual params: [0.761 , 0.6692]
-Original Grad: 0.052, -lr * Pred Grad:  0.063, New P: 0.824
-Original Grad: -0.003, -lr * Pred Grad:  0.091, New P: 0.760
iter 4 loss: 0.272
Actual params: [0.8243, 0.76  ]
-Original Grad: 0.098, -lr * Pred Grad:  0.101, New P: 0.925
-Original Grad: 0.076, -lr * Pred Grad:  0.158, New P: 0.918
iter 5 loss: 0.265
Actual params: [0.925 , 0.9183]
-Original Grad: -1.287, -lr * Pred Grad:  -0.131, New P: 0.794
-Original Grad: -8.124, -lr * Pred Grad:  -0.107, New P: 0.812
iter 6 loss: 1.852
Actual params: [0.794 , 0.8116]
-Original Grad: 0.136, -lr * Pred Grad:  0.017, New P: 0.811
-Original Grad: 0.097, -lr * Pred Grad:  0.001, New P: 0.812
iter 7 loss: 0.285
Actual params: [0.8105, 0.8124]
-Original Grad: 0.115, -lr * Pred Grad:  0.014, New P: 0.824
-Original Grad: 0.111, -lr * Pred Grad:  0.001, New P: 0.814
iter 8 loss: 0.272
Actual params: [0.8243, 0.8137]
-Original Grad: 0.052, -lr * Pred Grad:  0.005, New P: 0.830
-Original Grad: 0.060, -lr * Pred Grad:  0.001, New P: 0.814
iter 9 loss: 0.267
Actual params: [0.8297, 0.8144]
-Original Grad: 0.085, -lr * Pred Grad:  0.009, New P: 0.839
-Original Grad: 0.094, -lr * Pred Grad:  0.001, New P: 0.816
iter 10 loss: 0.264
Actual params: [0.8385, 0.8157]
-Original Grad: 0.085, -lr * Pred Grad:  0.009, New P: 0.848
-Original Grad: 0.102, -lr * Pred Grad:  0.002, New P: 0.817
iter 11 loss: 0.260
Actual params: [0.8477, 0.8172]
-Original Grad: 0.077, -lr * Pred Grad:  0.008, New P: 0.856
-Original Grad: 0.110, -lr * Pred Grad:  0.002, New P: 0.819
iter 12 loss: 0.256
Actual params: [0.8562, 0.8191]
-Original Grad: 0.067, -lr * Pred Grad:  0.008, New P: 0.864
-Original Grad: 0.115, -lr * Pred Grad:  0.002, New P: 0.821
iter 13 loss: 0.253
Actual params: [0.8638, 0.8214]
-Original Grad: 0.053, -lr * Pred Grad:  0.006, New P: 0.870
-Original Grad: 0.125, -lr * Pred Grad:  0.003, New P: 0.824
iter 14 loss: 0.250
Actual params: [0.8698, 0.8245]
-Original Grad: 0.046, -lr * Pred Grad:  0.005, New P: 0.875
-Original Grad: 0.130, -lr * Pred Grad:  0.004, New P: 0.828
iter 15 loss: 0.247
Actual params: [0.8751, 0.8281]
-Original Grad: 0.042, -lr * Pred Grad:  0.005, New P: 0.880
-Original Grad: 0.135, -lr * Pred Grad:  0.004, New P: 0.832
iter 16 loss: 0.245
Actual params: [0.8797, 0.8323]
-Original Grad: 0.033, -lr * Pred Grad:  0.003, New P: 0.883
-Original Grad: 0.136, -lr * Pred Grad:  0.005, New P: 0.837
iter 17 loss: 0.243
Actual params: [0.8831, 0.837 ]
-Original Grad: 0.030, -lr * Pred Grad:  0.002, New P: 0.885
-Original Grad: 0.154, -lr * Pred Grad:  0.006, New P: 0.843
iter 18 loss: 0.241
Actual params: [0.8854, 0.843 ]
-Original Grad: 0.032, -lr * Pred Grad:  0.003, New P: 0.888
-Original Grad: 0.154, -lr * Pred Grad:  0.006, New P: 0.849
iter 19 loss: 0.238
Actual params: [0.8884, 0.8494]
-Original Grad: 0.027, -lr * Pred Grad:  0.001, New P: 0.890
-Original Grad: 0.168, -lr * Pred Grad:  0.008, New P: 0.857
iter 20 loss: 0.235
Actual params: [0.8898, 0.857 ]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: -0.498, -lr * Pred Grad:  -0.265, New P: 0.329
-Original Grad: -1.867, -lr * Pred Grad:  0.002, New P: 0.596
iter 0 loss: 0.445
Actual params: [0.3291, 0.5956]
-Original Grad: 1.191, -lr * Pred Grad:  0.075, New P: 0.404
-Original Grad: -0.800, -lr * Pred Grad:  -0.014, New P: 0.582
iter 1 loss: 0.437
Actual params: [0.4041, 0.5819]
-Original Grad: -0.576, -lr * Pred Grad:  -0.020, New P: 0.384
-Original Grad: -0.669, -lr * Pred Grad:  -0.008, New P: 0.574
iter 2 loss: 0.255
Actual params: [0.3837, 0.5741]
-Original Grad: -0.309, -lr * Pred Grad:  -0.011, New P: 0.372
-Original Grad: -0.251, -lr * Pred Grad:  -0.001, New P: 0.573
iter 3 loss: 0.246
Actual params: [0.3725, 0.5732]
-Original Grad: -0.254, -lr * Pred Grad:  -0.010, New P: 0.362
-Original Grad: -0.038, -lr * Pred Grad:  0.003, New P: 0.576
iter 4 loss: 0.241
Actual params: [0.3621, 0.5763]
-Original Grad: 0.838, -lr * Pred Grad:  0.005, New P: 0.367
-Original Grad: 0.715, -lr * Pred Grad:  0.002, New P: 0.578
iter 5 loss: 0.239
Actual params: [0.3675, 0.578 ]
-Original Grad: -0.333, -lr * Pred Grad:  -0.005, New P: 0.362
-Original Grad: -0.085, -lr * Pred Grad:  0.003, New P: 0.581
iter 6 loss: 0.235
Actual params: [0.3625, 0.5811]
-Original Grad: -0.318, -lr * Pred Grad:  -0.007, New P: 0.356
-Original Grad: 0.064, -lr * Pred Grad:  0.005, New P: 0.586
iter 7 loss: 0.231
Actual params: [0.3558, 0.5864]
-Original Grad: 4.872, -lr * Pred Grad:  0.004, New P: 0.360
-Original Grad: 0.876, -lr * Pred Grad:  -0.001, New P: 0.585
iter 8 loss: 0.284
Actual params: [0.3601, 0.585 ]
-Original Grad: -0.202, -lr * Pred Grad:  -0.001, New P: 0.360
-Original Grad: 0.121, -lr * Pred Grad:  0.001, New P: 0.586
iter 9 loss: 0.226
Actual params: [0.3596, 0.5864]
-Original Grad: -0.054, -lr * Pred Grad:  -0.000, New P: 0.359
-Original Grad: 0.175, -lr * Pred Grad:  0.001, New P: 0.588
iter 10 loss: 0.224
Actual params: [0.3591, 0.5879]
-Original Grad: -0.345, -lr * Pred Grad:  -0.001, New P: 0.358
-Original Grad: 0.041, -lr * Pred Grad:  0.001, New P: 0.589
iter 11 loss: 0.222
Actual params: [0.3584, 0.5891]
-Original Grad: -0.208, -lr * Pred Grad:  -0.001, New P: 0.358
-Original Grad: 0.075, -lr * Pred Grad:  0.001, New P: 0.590
iter 12 loss: 0.221
Actual params: [0.3578, 0.5903]
-Original Grad: 0.221, -lr * Pred Grad:  -0.000, New P: 0.357
-Original Grad: 0.263, -lr * Pred Grad:  0.002, New P: 0.592
iter 13 loss: 0.221
Actual params: [0.3574, 0.5918]
-Original Grad: -0.231, -lr * Pred Grad:  -0.001, New P: 0.357
-Original Grad: 0.094, -lr * Pred Grad:  0.002, New P: 0.594
iter 14 loss: 0.219
Actual params: [0.3566, 0.5936]
-Original Grad: 0.450, -lr * Pred Grad:  -0.001, New P: 0.356
-Original Grad: 0.347, -lr * Pred Grad:  0.002, New P: 0.595
iter 15 loss: 0.218
Actual params: [0.356 , 0.5953]
-Original Grad: 0.915, -lr * Pred Grad:  -0.000, New P: 0.356
-Original Grad: 0.421, -lr * Pred Grad:  0.001, New P: 0.596
iter 16 loss: 0.217
Actual params: [0.3559, 0.5961]
-Original Grad: -0.010, -lr * Pred Grad:  -0.001, New P: 0.355
-Original Grad: 0.135, -lr * Pred Grad:  0.002, New P: 0.598
iter 17 loss: 0.215
Actual params: [0.3553, 0.5977]
-Original Grad: 0.568, -lr * Pred Grad:  -0.000, New P: 0.355
-Original Grad: 0.230, -lr * Pred Grad:  0.001, New P: 0.598
iter 18 loss: 0.216
Actual params: [0.3552, 0.5982]
-Original Grad: 0.852, -lr * Pred Grad:  -0.000, New P: 0.355
-Original Grad: 0.302, -lr * Pred Grad:  0.001, New P: 0.599
iter 19 loss: 0.215
Actual params: [0.3551, 0.5987]
-Original Grad: 0.756, -lr * Pred Grad:  -0.000, New P: 0.355
-Original Grad: 0.279, -lr * Pred Grad:  0.001, New P: 0.599
iter 20 loss: 0.214
Actual params: [0.355 , 0.5993]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.541, -lr * Pred Grad:  1.177, New P: 1.771
-Original Grad: -0.053, -lr * Pred Grad:  0.074, New P: 0.668
iter 0 loss: 1.269
Actual params: [1.7713, 0.6683]
-Original Grad: -0.568, -lr * Pred Grad:  -0.298, New P: 1.473
-Original Grad: -0.339, -lr * Pred Grad:  -0.693, New P: -0.025
iter 1 loss: 0.988
Actual params: [ 1.4734, -0.0246]
-Original Grad: -0.583, -lr * Pred Grad:  -0.146, New P: 1.327
-Original Grad: -0.095, -lr * Pred Grad:  -0.129, New P: -0.153
iter 2 loss: 0.606
Actual params: [ 1.3273, -0.1534]
-Original Grad: -0.523, -lr * Pred Grad:  -0.095, New P: 1.233
-Original Grad: 0.080, -lr * Pred Grad:  0.149, New P: -0.004
iter 3 loss: 0.532
Actual params: [ 1.2328, -0.0044]
-Original Grad: -0.443, -lr * Pred Grad:  -0.064, New P: 1.168
-Original Grad: -0.010, -lr * Pred Grad:  -0.052, New P: -0.056
iter 4 loss: 0.449
Actual params: [ 1.1684, -0.0562]
-Original Grad: -0.257, -lr * Pred Grad:  -0.036, New P: 1.132
-Original Grad: -0.022, -lr * Pred Grad:  -0.073, New P: -0.129
iter 5 loss: 0.419
Actual params: [ 1.1322, -0.1294]
-Original Grad: -0.190, -lr * Pred Grad:  -0.026, New P: 1.106
-Original Grad: -0.005, -lr * Pred Grad:  -0.033, New P: -0.163
iter 6 loss: 0.414
Actual params: [ 1.1061, -0.1626]
-Original Grad: -0.124, -lr * Pred Grad:  -0.017, New P: 1.089
-Original Grad: 0.002, -lr * Pred Grad:  -0.013, New P: -0.176
iter 7 loss: 0.410
Actual params: [ 1.0888, -0.1757]
-Original Grad: -0.072, -lr * Pred Grad:  -0.008, New P: 1.081
-Original Grad: 0.018, -lr * Pred Grad:  0.037, New P: -0.139
iter 8 loss: 0.409
Actual params: [ 1.0812, -0.1387]
-Original Grad: -0.047, -lr * Pred Grad:  -0.012, New P: 1.069
-Original Grad: -0.018, -lr * Pred Grad:  -0.066, New P: -0.205
iter 9 loss: 0.406
Actual params: [ 1.0694, -0.2046]
-Original Grad: 0.025, -lr * Pred Grad:  0.016, New P: 1.086
-Original Grad: 0.053, -lr * Pred Grad:  0.141, New P: -0.064
iter 10 loss: 0.413
Actual params: [ 1.0857, -0.0637]
-Original Grad: -0.039, -lr * Pred Grad:  -0.013, New P: 1.073
-Original Grad: -0.020, -lr * Pred Grad:  -0.065, New P: -0.129
iter 11 loss: 0.403
Actual params: [ 1.0731, -0.1288]
-Original Grad: -0.006, -lr * Pred Grad:  -0.009, New P: 1.065
-Original Grad: -0.025, -lr * Pred Grad:  -0.076, New P: -0.205
iter 12 loss: 0.405
Actual params: [ 1.0646, -0.2047]
-Original Grad: 0.039, -lr * Pred Grad:  0.024, New P: 1.088
-Original Grad: 0.052, -lr * Pred Grad:  0.142, New P: -0.062
iter 13 loss: 0.414
Actual params: [ 1.0884, -0.0624]
-Original Grad: -0.040, -lr * Pred Grad:  -0.021, New P: 1.068
-Original Grad: -0.036, -lr * Pred Grad:  -0.110, New P: -0.172
iter 14 loss: 0.403
Actual params: [ 1.0678, -0.1723]
-Original Grad: 0.023, -lr * Pred Grad:  0.004, New P: 1.072
-Original Grad: -0.004, -lr * Pred Grad:  -0.005, New P: -0.177
iter 15 loss: 0.408
Actual params: [ 1.0722, -0.1769]
-Original Grad: -0.002, -lr * Pred Grad:  0.003, New P: 1.075
-Original Grad: 0.009, -lr * Pred Grad:  0.026, New P: -0.151
iter 16 loss: 0.408
Actual params: [ 1.0755, -0.1508]
-Original Grad: -0.035, -lr * Pred Grad:  -0.022, New P: 1.054
-Original Grad: -0.024, -lr * Pred Grad:  -0.094, New P: -0.245
iter 17 loss: 0.406
Actual params: [ 1.0536, -0.2452]
-Original Grad: 0.091, -lr * Pred Grad:  0.053, New P: 1.106
-Original Grad: 0.077, -lr * Pred Grad:  0.204, New P: -0.041
iter 18 loss: 0.424
Actual params: [ 1.1063, -0.0413]
-Original Grad: -0.090, -lr * Pred Grad:  -0.037, New P: 1.069
-Original Grad: -0.030, -lr * Pred Grad:  -0.103, New P: -0.144
iter 19 loss: 0.404
Actual params: [ 1.0694, -0.1441]
-Original Grad: 0.016, -lr * Pred Grad:  -0.003, New P: 1.066
-Original Grad: -0.022, -lr * Pred Grad:  -0.051, New P: -0.195
iter 20 loss: 0.406
Actual params: [ 1.0663, -0.1951]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.297, -lr * Pred Grad:  0.769, New P: 1.363
-Original Grad: -0.151, -lr * Pred Grad:  -0.371, New P: 0.223
iter 0 loss: 0.863
Actual params: [1.3629, 0.2234]
-Original Grad: -0.422, -lr * Pred Grad:  -0.383, New P: 0.980
-Original Grad: -0.211, -lr * Pred Grad:  -0.509, New P: -0.285
iter 1 loss: 0.580
Actual params: [ 0.9798, -0.2853]
-Original Grad: 0.559, -lr * Pred Grad:  0.263, New P: 1.243
-Original Grad: -0.007, -lr * Pred Grad:  -0.140, New P: -0.426
iter 2 loss: 0.483
Actual params: [ 1.2427, -0.4256]
-Original Grad: -0.270, -lr * Pred Grad:  -0.095, New P: 1.148
-Original Grad: 0.005, -lr * Pred Grad:  0.065, New P: -0.360
iter 3 loss: 0.524
Actual params: [ 1.1475, -0.3602]
-Original Grad: -0.335, -lr * Pred Grad:  -0.106, New P: 1.041
-Original Grad: 0.028, -lr * Pred Grad:  0.156, New P: -0.204
iter 4 loss: 0.491
Actual params: [ 1.0412, -0.2038]
-Original Grad: 0.011, -lr * Pred Grad:  0.001, New P: 1.042
-Original Grad: 0.022, -lr * Pred Grad:  0.107, New P: -0.097
iter 5 loss: 0.457
Actual params: [ 1.0424, -0.0969]
-Original Grad: -0.026, -lr * Pred Grad:  -0.008, New P: 1.035
-Original Grad: 0.010, -lr * Pred Grad:  0.049, New P: -0.048
iter 6 loss: 0.458
Actual params: [ 1.0346, -0.0477]
-Original Grad: 0.099, -lr * Pred Grad:  0.024, New P: 1.059
-Original Grad: -0.081, -lr * Pred Grad:  -0.292, New P: -0.340
iter 7 loss: 0.451
Actual params: [ 1.0588, -0.3398]
-Original Grad: -0.094, -lr * Pred Grad:  -0.033, New P: 1.026
-Original Grad: 0.016, -lr * Pred Grad:  0.052, New P: -0.288
iter 8 loss: 0.466
Actual params: [ 1.0261, -0.2879]
-Original Grad: 0.100, -lr * Pred Grad:  0.039, New P: 1.065
-Original Grad: -0.005, -lr * Pred Grad:  -0.008, New P: -0.296
iter 9 loss: 0.464
Actual params: [ 1.0655, -0.2956]
-Original Grad: -0.150, -lr * Pred Grad:  -0.063, New P: 1.002
-Original Grad: -0.002, -lr * Pred Grad:  -0.021, New P: -0.317
iter 10 loss: 0.463
Actual params: [ 1.0024, -0.3165]
-Original Grad: 0.299, -lr * Pred Grad:  0.118, New P: 1.120
-Original Grad: 0.011, -lr * Pred Grad:  0.074, New P: -0.243
iter 11 loss: 0.473
Actual params: [ 1.1202, -0.243 ]
-Original Grad: -0.323, -lr * Pred Grad:  -0.115, New P: 1.005
-Original Grad: -0.013, -lr * Pred Grad:  -0.060, New P: -0.303
iter 12 loss: 0.472
Actual params: [ 1.0048, -0.3027]
-Original Grad: 0.276, -lr * Pred Grad:  0.097, New P: 1.101
-Original Grad: 0.004, -lr * Pred Grad:  0.020, New P: -0.283
iter 13 loss: 0.471
Actual params: [ 1.1014, -0.2832]
-Original Grad: -0.268, -lr * Pred Grad:  -0.092, New P: 1.010
-Original Grad: 0.010, -lr * Pred Grad:  0.059, New P: -0.225
iter 14 loss: 0.469
Actual params: [ 1.0097, -0.2246]
-Original Grad: 0.216, -lr * Pred Grad:  0.076, New P: 1.086
-Original Grad: 0.006, -lr * Pred Grad:  0.032, New P: -0.193
iter 15 loss: 0.465
Actual params: [ 1.0857, -0.1929]
-Original Grad: -0.237, -lr * Pred Grad:  -0.082, New P: 1.004
-Original Grad: 0.038, -lr * Pred Grad:  0.223, New P: 0.030
iter 16 loss: 0.459
Actual params: [1.0037, 0.0304]
-Original Grad: 0.347, -lr * Pred Grad:  0.084, New P: 1.088
-Original Grad: -0.110, -lr * Pred Grad:  -0.451, New P: -0.421
iter 17 loss: 0.471
Actual params: [ 1.0882, -0.4206]
-Original Grad: -0.224, -lr * Pred Grad:  -0.068, New P: 1.021
-Original Grad: 0.031, -lr * Pred Grad:  0.096, New P: -0.325
iter 18 loss: 0.479
Actual params: [ 1.0206, -0.3247]
-Original Grad: 0.120, -lr * Pred Grad:  0.049, New P: 1.069
-Original Grad: 0.010, -lr * Pred Grad:  0.099, New P: -0.226
iter 19 loss: 0.468
Actual params: [ 1.0692, -0.2255]
-Original Grad: -0.157, -lr * Pred Grad:  -0.055, New P: 1.014
-Original Grad: 0.019, -lr * Pred Grad:  0.062, New P: -0.164
iter 20 loss: 0.459
Actual params: [ 1.0138, -0.1636]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.472, -lr * Pred Grad:  0.716, New P: 1.310
-Original Grad: 0.005, -lr * Pred Grad:  0.152, New P: 0.746
iter 0 loss: 0.885
Actual params: [1.3102, 0.7461]
-Original Grad: 1.058, -lr * Pred Grad:  0.788, New P: 2.098
-Original Grad: -0.351, -lr * Pred Grad:  0.475, New P: 1.221
iter 1 loss: 1.827
Actual params: [2.0982, 1.2213]
-Original Grad: 0.375, -lr * Pred Grad:  0.191, New P: 2.290
-Original Grad: 0.639, -lr * Pred Grad:  0.137, New P: 1.358
iter 2 loss: 0.520
Actual params: [2.2897, 1.3579]
-Original Grad: 0.188, -lr * Pred Grad:  0.078, New P: 2.367
-Original Grad: 0.302, -lr * Pred Grad:  0.054, New P: 1.412
iter 3 loss: 0.439
Actual params: [2.3673, 1.4122]
-Original Grad: 0.156, -lr * Pred Grad:  0.058, New P: 2.425
-Original Grad: 0.249, -lr * Pred Grad:  0.040, New P: 1.452
iter 4 loss: 0.430
Actual params: [2.4252, 1.4524]
-Original Grad: 0.120, -lr * Pred Grad:  0.043, New P: 2.468
-Original Grad: 0.176, -lr * Pred Grad:  0.029, New P: 1.481
iter 5 loss: 0.426
Actual params: [2.4678, 1.481 ]
-Original Grad: 0.106, -lr * Pred Grad:  0.037, New P: 2.505
-Original Grad: 0.149, -lr * Pred Grad:  0.025, New P: 1.506
iter 6 loss: 0.424
Actual params: [2.5053, 1.5056]
-Original Grad: 0.103, -lr * Pred Grad:  0.037, New P: 2.542
-Original Grad: 0.147, -lr * Pred Grad:  0.024, New P: 1.530
iter 7 loss: 0.424
Actual params: [2.542 , 1.5299]
-Original Grad: 0.085, -lr * Pred Grad:  0.031, New P: 2.573
-Original Grad: 0.124, -lr * Pred Grad:  0.021, New P: 1.551
iter 8 loss: 0.425
Actual params: [2.5734, 1.5508]
-Original Grad: 0.067, -lr * Pred Grad:  0.025, New P: 2.599
-Original Grad: 0.097, -lr * Pred Grad:  0.017, New P: 1.567
iter 9 loss: 0.427
Actual params: [2.5986, 1.5675]
-Original Grad: 0.025, -lr * Pred Grad:  0.009, New P: 2.608
-Original Grad: 0.038, -lr * Pred Grad:  0.006, New P: 1.574
iter 10 loss: 0.429
Actual params: [2.608 , 1.5739]
-Original Grad: 0.009, -lr * Pred Grad:  0.004, New P: 2.612
-Original Grad: 0.017, -lr * Pred Grad:  0.003, New P: 1.577
iter 11 loss: 0.430
Actual params: [2.6117, 1.5768]
-Original Grad: 0.009, -lr * Pred Grad:  0.003, New P: 2.615
-Original Grad: 0.016, -lr * Pred Grad:  0.003, New P: 1.580
iter 12 loss: 0.430
Actual params: [2.6151, 1.5795]
-Original Grad: -0.012, -lr * Pred Grad:  -0.004, New P: 2.611
-Original Grad: -0.013, -lr * Pred Grad:  -0.002, New P: 1.577
iter 13 loss: 0.431
Actual params: [2.6107, 1.5771]
-Original Grad: 0.009, -lr * Pred Grad:  0.003, New P: 2.614
-Original Grad: 0.016, -lr * Pred Grad:  0.003, New P: 1.580
iter 14 loss: 0.430
Actual params: [2.6141, 1.5799]
-Original Grad: 0.007, -lr * Pred Grad:  0.003, New P: 2.617
-Original Grad: 0.014, -lr * Pred Grad:  0.002, New P: 1.582
iter 15 loss: 0.430
Actual params: [2.6168, 1.5824]
-Original Grad: -0.012, -lr * Pred Grad:  -0.005, New P: 2.612
-Original Grad: -0.014, -lr * Pred Grad:  -0.002, New P: 1.580
iter 16 loss: 0.431
Actual params: [2.6119, 1.5799]
-Original Grad: 0.008, -lr * Pred Grad:  0.003, New P: 2.615
-Original Grad: 0.015, -lr * Pred Grad:  0.003, New P: 1.583
iter 17 loss: 0.430
Actual params: [2.6149, 1.5826]
-Original Grad: -0.012, -lr * Pred Grad:  -0.005, New P: 2.610
-Original Grad: -0.014, -lr * Pred Grad:  -0.002, New P: 1.580
iter 18 loss: 0.431
Actual params: [2.6099, 1.5803]
-Original Grad: 0.008, -lr * Pred Grad:  0.003, New P: 2.613
-Original Grad: 0.015, -lr * Pred Grad:  0.003, New P: 1.583
iter 19 loss: 0.430
Actual params: [2.6128, 1.5831]
-Original Grad: -0.012, -lr * Pred Grad:  -0.005, New P: 2.608
-Original Grad: -0.014, -lr * Pred Grad:  -0.002, New P: 1.581
iter 20 loss: 0.431
Actual params: [2.608 , 1.5809]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: -0.071, -lr * Pred Grad:  0.210, New P: 0.804
-Original Grad: -0.220, -lr * Pred Grad:  -0.275, New P: 0.319
iter 0 loss: 0.338
Actual params: [0.8041, 0.3189]
-Original Grad: 0.051, -lr * Pred Grad:  0.339, New P: 1.144
-Original Grad: -0.015, -lr * Pred Grad:  -0.159, New P: 0.160
iter 1 loss: 0.265
Actual params: [1.1436, 0.1598]
-Original Grad: 0.003, -lr * Pred Grad:  -0.027, New P: 1.117
-Original Grad: 0.017, -lr * Pred Grad:  0.028, New P: 0.188
iter 2 loss: 0.266
Actual params: [1.1168, 0.1879]
-Original Grad: 0.004, -lr * Pred Grad:  -0.031, New P: 1.086
-Original Grad: 0.019, -lr * Pred Grad:  0.033, New P: 0.221
iter 3 loss: 0.263
Actual params: [1.0856, 0.2212]
-Original Grad: 0.006, -lr * Pred Grad:  -0.000, New P: 1.085
-Original Grad: 0.015, -lr * Pred Grad:  0.018, New P: 0.239
iter 4 loss: 0.260
Actual params: [1.0852, 0.2391]
-Original Grad: 0.007, -lr * Pred Grad:  0.019, New P: 1.104
-Original Grad: 0.011, -lr * Pred Grad:  0.007, New P: 0.246
iter 5 loss: 0.259
Actual params: [1.104 , 0.2456]
-Original Grad: 0.003, -lr * Pred Grad:  -0.012, New P: 1.092
-Original Grad: 0.011, -lr * Pred Grad:  0.021, New P: 0.267
iter 6 loss: 0.260
Actual params: [1.092 , 0.2668]
-Original Grad: 0.006, -lr * Pred Grad:  0.033, New P: 1.125
-Original Grad: 0.005, -lr * Pred Grad:  -0.006, New P: 0.261
iter 7 loss: 0.258
Actual params: [1.1251, 0.2611]
-Original Grad: 0.002, -lr * Pred Grad:  -0.012, New P: 1.113
-Original Grad: 0.010, -lr * Pred Grad:  0.022, New P: 0.283
iter 8 loss: 0.259
Actual params: [1.1128, 0.2828]
-Original Grad: 0.006, -lr * Pred Grad:  0.049, New P: 1.162
-Original Grad: 0.001, -lr * Pred Grad:  -0.016, New P: 0.267
iter 9 loss: 0.258
Actual params: [1.1622, 0.2666]
-Original Grad: 0.003, -lr * Pred Grad:  0.003, New P: 1.165
-Original Grad: 0.006, -lr * Pred Grad:  0.012, New P: 0.279
iter 10 loss: 0.261
Actual params: [1.1651, 0.2789]
-Original Grad: 0.003, -lr * Pred Grad:  0.007, New P: 1.172
-Original Grad: 0.006, -lr * Pred Grad:  0.012, New P: 0.291
iter 11 loss: 0.260
Actual params: [1.1718, 0.2908]
-Original Grad: 0.003, -lr * Pred Grad:  0.020, New P: 1.192
-Original Grad: 0.003, -lr * Pred Grad:  0.001, New P: 0.292
iter 12 loss: 0.260
Actual params: [1.1915, 0.2921]
-Original Grad: 0.002, -lr * Pred Grad:  0.004, New P: 1.195
-Original Grad: 0.004, -lr * Pred Grad:  0.011, New P: 0.303
iter 13 loss: 0.261
Actual params: [1.1951, 0.3029]
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 1.211
-Original Grad: -0.001, -lr * Pred Grad:  -0.007, New P: 0.296
iter 14 loss: 0.261
Actual params: [1.2111, 0.2957]
-Original Grad: 0.002, -lr * Pred Grad:  0.014, New P: 1.225
-Original Grad: 0.003, -lr * Pred Grad:  0.006, New P: 0.302
iter 15 loss: 0.262
Actual params: [1.2249, 0.3017]
-Original Grad: 0.001, -lr * Pred Grad:  0.020, New P: 1.244
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: 0.295
iter 16 loss: 0.263
Actual params: [1.2445, 0.2955]
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: 1.247
-Original Grad: 0.003, -lr * Pred Grad:  0.011, New P: 0.306
iter 17 loss: 0.264
Actual params: [1.2469, 0.3062]
-Original Grad: 0.001, -lr * Pred Grad:  0.024, New P: 1.270
-Original Grad: -0.004, -lr * Pred Grad:  -0.024, New P: 0.282
iter 18 loss: 0.264
Actual params: [1.2704, 0.2821]
-Original Grad: 0.003, -lr * Pred Grad:  0.029, New P: 1.299
-Original Grad: 0.006, -lr * Pred Grad:  0.022, New P: 0.304
iter 19 loss: 0.266
Actual params: [1.2993, 0.304 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 1.315
-Original Grad: -0.004, -lr * Pred Grad:  -0.025, New P: 0.279
iter 20 loss: 0.267
Actual params: [1.3152, 0.2792]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.290, -lr * Pred Grad:  0.405, New P: 0.999
-Original Grad: 0.194, -lr * Pred Grad:  0.122, New P: 0.716
iter 0 loss: 0.605
Actual params: [0.999 , 0.7164]
-Original Grad: -1.132, -lr * Pred Grad:  -0.665, New P: 0.334
-Original Grad: -1.753, -lr * Pred Grad:  0.144, New P: 0.861
iter 1 loss: 1.560
Actual params: [0.3342, 0.8609]
-Original Grad: 0.016, -lr * Pred Grad:  0.013, New P: 0.347
-Original Grad: 0.016, -lr * Pred Grad:  -0.004, New P: 0.857
iter 2 loss: 0.723
Actual params: [0.3468, 0.8571]
-Original Grad: 0.018, -lr * Pred Grad:  0.018, New P: 0.365
-Original Grad: 0.009, -lr * Pred Grad:  -0.007, New P: 0.850
iter 3 loss: 0.724
Actual params: [0.3653, 0.8504]
-Original Grad: 0.011, -lr * Pred Grad:  0.032, New P: 0.397
-Original Grad: -0.038, -lr * Pred Grad:  -0.016, New P: 0.834
iter 4 loss: 0.724
Actual params: [0.397 , 0.8344]
-Original Grad: 0.062, -lr * Pred Grad:  0.125, New P: 0.522
-Original Grad: -0.114, -lr * Pred Grad:  -0.063, New P: 0.772
iter 5 loss: 0.722
Actual params: [0.5224, 0.7715]
-Original Grad: -0.074, -lr * Pred Grad:  0.159, New P: 0.681
-Original Grad: -0.533, -lr * Pred Grad:  -0.105, New P: 0.667
iter 6 loss: 0.704
Actual params: [0.6814, 0.6669]
-Original Grad: 0.083, -lr * Pred Grad:  0.186, New P: 0.868
-Original Grad: -0.781, -lr * Pred Grad:  -0.090, New P: 0.577
iter 7 loss: 0.600
Actual params: [0.8677, 0.5768]
-Original Grad: 0.280, -lr * Pred Grad:  0.105, New P: 0.973
-Original Grad: -0.206, -lr * Pred Grad:  -0.036, New P: 0.541
iter 8 loss: 0.394
Actual params: [0.9729, 0.5405]
-Original Grad: 0.142, -lr * Pred Grad:  0.046, New P: 1.019
-Original Grad: -0.105, -lr * Pred Grad:  -0.017, New P: 0.524
iter 9 loss: 0.271
Actual params: [1.0193, 0.5237]
-Original Grad: 0.131, -lr * Pred Grad:  0.038, New P: 1.058
-Original Grad: -0.081, -lr * Pred Grad:  -0.012, New P: 0.511
iter 10 loss: 0.244
Actual params: [1.0576, 0.5112]
-Original Grad: 0.065, -lr * Pred Grad:  0.017, New P: 1.075
-Original Grad: -0.007, -lr * Pred Grad:  -0.004, New P: 0.507
iter 11 loss: 0.225
Actual params: [1.0751, 0.507 ]
-Original Grad: 0.013, -lr * Pred Grad:  0.004, New P: 1.079
-Original Grad: -0.009, -lr * Pred Grad:  -0.001, New P: 0.506
iter 12 loss: 0.222
Actual params: [1.0792, 0.5059]
-Original Grad: 0.013, -lr * Pred Grad:  0.004, New P: 1.083
-Original Grad: -0.007, -lr * Pred Grad:  -0.001, New P: 0.505
iter 13 loss: 0.222
Actual params: [1.0833, 0.505 ]
-Original Grad: 0.004, -lr * Pred Grad:  0.001, New P: 1.084
-Original Grad: 0.003, -lr * Pred Grad:  -0.000, New P: 0.505
iter 14 loss: 0.221
Actual params: [1.0843, 0.5049]
-Original Grad: 0.002, -lr * Pred Grad:  0.001, New P: 1.085
-Original Grad: 0.001, -lr * Pred Grad:  -0.000, New P: 0.505
iter 15 loss: 0.221
Actual params: [1.0848, 0.5049]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 1.085
-Original Grad: 0.002, -lr * Pred Grad:  0.000, New P: 0.505
iter 16 loss: 0.221
Actual params: [1.085 , 0.5049]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 1.085
-Original Grad: 0.002, -lr * Pred Grad:  0.000, New P: 0.505
iter 17 loss: 0.221
Actual params: [1.0853, 0.5049]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 1.086
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 0.505
iter 18 loss: 0.221
Actual params: [1.0856, 0.5049]
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: 1.085
-Original Grad: 0.002, -lr * Pred Grad:  0.000, New P: 0.505
iter 19 loss: 0.221
Actual params: [1.085, 0.505]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 1.085
-Original Grad: 0.002, -lr * Pred Grad:  0.000, New P: 0.505
iter 20 loss: 0.221
Actual params: [1.0854, 0.505 ]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.914, -lr * Pred Grad:  0.113, New P: 0.707
-Original Grad: -0.187, -lr * Pred Grad:  -0.046, New P: 0.548
iter 0 loss: 0.789
Actual params: [0.7068, 0.5482]
-Original Grad: 1.384, -lr * Pred Grad:  0.120, New P: 0.827
-Original Grad: -0.171, -lr * Pred Grad:  0.271, New P: 0.819
iter 1 loss: 0.716
Actual params: [0.8272, 0.8191]
-Original Grad: 1.922, -lr * Pred Grad:  0.061, New P: 0.889
-Original Grad: -0.390, -lr * Pred Grad:  0.049, New P: 0.868
iter 2 loss: 0.672
Actual params: [0.8887, 0.8683]
-Original Grad: -9.844, -lr * Pred Grad:  -0.011, New P: 0.878
-Original Grad: -4.154, -lr * Pred Grad:  -0.139, New P: 0.729
iter 3 loss: 1.042
Actual params: [0.8778, 0.7292]
-Original Grad: 1.695, -lr * Pred Grad:  0.016, New P: 0.894
-Original Grad: -0.260, -lr * Pred Grad:  -0.039, New P: 0.690
iter 4 loss: 0.563
Actual params: [0.8939, 0.6899]
-Original Grad: 1.471, -lr * Pred Grad:  0.011, New P: 0.905
-Original Grad: -0.171, -lr * Pred Grad:  -0.025, New P: 0.665
iter 5 loss: 0.542
Actual params: [0.905 , 0.6652]
-Original Grad: 1.470, -lr * Pred Grad:  0.010, New P: 0.915
-Original Grad: -0.157, -lr * Pred Grad:  -0.020, New P: 0.645
iter 6 loss: 0.528
Actual params: [0.9147, 0.6449]
-Original Grad: 1.379, -lr * Pred Grad:  0.008, New P: 0.923
-Original Grad: -0.118, -lr * Pred Grad:  -0.016, New P: 0.629
iter 7 loss: 0.518
Actual params: [0.9229, 0.629 ]
-Original Grad: 1.331, -lr * Pred Grad:  0.008, New P: 0.931
-Original Grad: -0.112, -lr * Pred Grad:  -0.014, New P: 0.615
iter 8 loss: 0.509
Actual params: [0.9306, 0.6146]
-Original Grad: 1.254, -lr * Pred Grad:  0.007, New P: 0.938
-Original Grad: -0.142, -lr * Pred Grad:  -0.015, New P: 0.599
iter 9 loss: 0.499
Actual params: [0.9377, 0.5993]
-Original Grad: 1.176, -lr * Pred Grad:  0.007, New P: 0.944
-Original Grad: -0.179, -lr * Pred Grad:  -0.017, New P: 0.582
iter 10 loss: 0.488
Actual params: [0.9445, 0.5823]
-Original Grad: 1.126, -lr * Pred Grad:  0.006, New P: 0.951
-Original Grad: -0.239, -lr * Pred Grad:  -0.020, New P: 0.562
iter 11 loss: 0.475
Actual params: [0.9509, 0.5622]
-Original Grad: 1.024, -lr * Pred Grad:  0.006, New P: 0.957
-Original Grad: -0.312, -lr * Pred Grad:  -0.024, New P: 0.538
iter 12 loss: 0.456
Actual params: [0.9566, 0.5384]
-Original Grad: 0.902, -lr * Pred Grad:  0.005, New P: 0.961
-Original Grad: -0.357, -lr * Pred Grad:  -0.025, New P: 0.513
iter 13 loss: 0.434
Actual params: [0.9613, 0.5132]
-Original Grad: 0.814, -lr * Pred Grad:  0.004, New P: 0.965
-Original Grad: -0.459, -lr * Pred Grad:  -0.029, New P: 0.485
iter 14 loss: 0.405
Actual params: [0.9649, 0.4846]
-Original Grad: 0.807, -lr * Pred Grad:  0.003, New P: 0.967
-Original Grad: -0.559, -lr * Pred Grad:  -0.028, New P: 0.457
iter 15 loss: 0.366
Actual params: [0.9675, 0.4567]
-Original Grad: 0.847, -lr * Pred Grad:  0.001, New P: 0.969
-Original Grad: -0.721, -lr * Pred Grad:  -0.024, New P: 0.433
iter 16 loss: 0.317
Actual params: [0.9688, 0.4326]
-Original Grad: 0.565, -lr * Pred Grad:  0.001, New P: 0.970
-Original Grad: -0.434, -lr * Pred Grad:  -0.006, New P: 0.426
iter 17 loss: 0.256
Actual params: [0.9701, 0.4262]
-Original Grad: 0.130, -lr * Pred Grad:  0.002, New P: 0.972
-Original Grad: 0.082, -lr * Pred Grad:  0.002, New P: 0.428
iter 18 loss: 0.228
Actual params: [0.9718, 0.4284]
-Original Grad: 0.129, -lr * Pred Grad:  0.002, New P: 0.973
-Original Grad: 0.068, -lr * Pred Grad:  0.002, New P: 0.430
iter 19 loss: 0.231
Actual params: [0.9735, 0.4302]
-Original Grad: 0.130, -lr * Pred Grad:  0.002, New P: 0.975
-Original Grad: 0.064, -lr * Pred Grad:  0.002, New P: 0.432
iter 20 loss: 0.232
Actual params: [0.9753, 0.4319]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.476, -lr * Pred Grad:  -0.466, New P: 0.128
-Original Grad: -2.818, -lr * Pred Grad:  -0.179, New P: 0.415
iter 0 loss: 0.743
Actual params: [0.1284, 0.4151]
-Original Grad: 0.029, -lr * Pred Grad:  -0.127, New P: 0.001
-Original Grad: -0.576, -lr * Pred Grad:  -0.034, New P: 0.381
iter 1 loss: 0.398
Actual params: [0.0012, 0.381 ]
-Original Grad: 0.085, -lr * Pred Grad:  0.128, New P: 0.130
-Original Grad: -0.122, -lr * Pred Grad:  0.016, New P: 0.397
iter 2 loss: 0.378
Actual params: [0.1297, 0.3973]
-Original Grad: 0.054, -lr * Pred Grad:  0.038, New P: 0.167
-Original Grad: -0.221, -lr * Pred Grad:  -0.001, New P: 0.396
iter 3 loss: 0.377
Actual params: [0.1675, 0.3964]
-Original Grad: 0.039, -lr * Pred Grad:  0.016, New P: 0.183
-Original Grad: -0.193, -lr * Pred Grad:  -0.003, New P: 0.393
iter 4 loss: 0.372
Actual params: [0.1833, 0.393 ]
-Original Grad: 0.059, -lr * Pred Grad:  0.054, New P: 0.238
-Original Grad: -0.154, -lr * Pred Grad:  0.003, New P: 0.396
iter 5 loss: 0.365
Actual params: [0.2377, 0.3962]
-Original Grad: 0.056, -lr * Pred Grad:  0.035, New P: 0.273
-Original Grad: -0.227, -lr * Pred Grad:  -0.002, New P: 0.394
iter 6 loss: 0.357
Actual params: [0.2728, 0.3941]
-Original Grad: 0.058, -lr * Pred Grad:  0.039, New P: 0.312
-Original Grad: -0.225, -lr * Pred Grad:  -0.002, New P: 0.392
iter 7 loss: 0.346
Actual params: [0.3117, 0.3924]
-Original Grad: 0.061, -lr * Pred Grad:  0.052, New P: 0.363
-Original Grad: -0.185, -lr * Pred Grad:  0.001, New P: 0.394
iter 8 loss: 0.337
Actual params: [0.3632, 0.3938]
-Original Grad: 0.062, -lr * Pred Grad:  0.057, New P: 0.421
-Original Grad: -0.154, -lr * Pred Grad:  0.003, New P: 0.397
iter 9 loss: 0.329
Actual params: [0.4206, 0.3972]
-Original Grad: 0.041, -lr * Pred Grad:  0.036, New P: 0.456
-Original Grad: -0.117, -lr * Pred Grad:  0.001, New P: 0.398
iter 10 loss: 0.321
Actual params: [0.4564, 0.3985]
-Original Grad: 0.023, -lr * Pred Grad:  -0.000, New P: 0.456
-Original Grad: -0.140, -lr * Pred Grad:  -0.006, New P: 0.393
iter 11 loss: 0.315
Actual params: [0.4562, 0.3925]
-Original Grad: 0.027, -lr * Pred Grad:  0.002, New P: 0.459
-Original Grad: -0.152, -lr * Pred Grad:  -0.006, New P: 0.386
iter 12 loss: 0.310
Actual params: [0.4586, 0.3861]
-Original Grad: 0.024, -lr * Pred Grad:  0.002, New P: 0.461
-Original Grad: -0.131, -lr * Pred Grad:  -0.006, New P: 0.380
iter 13 loss: 0.304
Actual params: [0.4609, 0.3801]
-Original Grad: 0.025, -lr * Pred Grad:  0.005, New P: 0.466
-Original Grad: -0.129, -lr * Pred Grad:  -0.006, New P: 0.374
iter 14 loss: 0.298
Actual params: [0.4662, 0.3744]
-Original Grad: 0.025, -lr * Pred Grad:  0.001, New P: 0.467
-Original Grad: -0.139, -lr * Pred Grad:  -0.008, New P: 0.367
iter 15 loss: 0.293
Actual params: [0.4668, 0.3668]
-Original Grad: 0.027, -lr * Pred Grad:  0.009, New P: 0.476
-Original Grad: -0.129, -lr * Pred Grad:  -0.006, New P: 0.361
iter 16 loss: 0.288
Actual params: [0.4756, 0.3608]
-Original Grad: 0.023, -lr * Pred Grad:  -0.004, New P: 0.472
-Original Grad: -0.128, -lr * Pred Grad:  -0.009, New P: 0.352
iter 17 loss: 0.283
Actual params: [0.4717, 0.3521]
-Original Grad: 0.031, -lr * Pred Grad:  0.023, New P: 0.495
-Original Grad: -0.119, -lr * Pred Grad:  -0.004, New P: 0.349
iter 18 loss: 0.280
Actual params: [0.4946, 0.3486]
-Original Grad: 0.018, -lr * Pred Grad:  -0.011, New P: 0.484
-Original Grad: -0.110, -lr * Pred Grad:  -0.010, New P: 0.339
iter 19 loss: 0.276
Actual params: [0.4837, 0.3386]
-Original Grad: 0.024, -lr * Pred Grad:  0.015, New P: 0.499
-Original Grad: -0.097, -lr * Pred Grad:  -0.004, New P: 0.334
iter 20 loss: 0.273
Actual params: [0.4989, 0.3342]
