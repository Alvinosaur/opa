Target params: [1.1812, 0.2779]
Actual params: [1.084 , 1.4018]
-Original Grad: -0.176, -lr * Pred Grad: -0.013, New P: 1.071
-Original Grad: -0.113, -lr * Pred Grad: -0.010, New P: 1.392
iter 0 loss: 0.234
Actual params: [1.0708, 1.3922]
-Original Grad: -3.911, -lr * Pred Grad: -0.020, New P: 1.051
-Original Grad: 1.896, -lr * Pred Grad: 0.010, New P: 1.402
iter 1 loss: 0.206
Actual params: [1.0507, 1.4023]
-Original Grad: -1.117, -lr * Pred Grad: -0.021, New P: 1.029
-Original Grad: 0.285, -lr * Pred Grad: 0.020, New P: 1.422
iter 2 loss: 0.176
Actual params: [1.0294, 1.4219]
-Original Grad: -0.721, -lr * Pred Grad: -0.022, New P: 1.008
-Original Grad: 0.445, -lr * Pred Grad: 0.021, New P: 1.443
iter 3 loss: 0.130
Actual params: [1.0079, 1.4433]
-Original Grad: -1.107, -lr * Pred Grad: -0.022, New P: 0.986
-Original Grad: 0.552, -lr * Pred Grad: 0.022, New P: 1.465
iter 4 loss: 0.098
Actual params: [0.9864, 1.465 ]
-Original Grad: -0.278, -lr * Pred Grad: -0.022, New P: 0.965
-Original Grad: 0.078, -lr * Pred Grad: 0.022, New P: 1.487
iter 5 loss: 0.064
Actual params: [0.9648, 1.4867]
-Original Grad: 0.347, -lr * Pred Grad: -0.022, New P: 0.943
-Original Grad: 0.111, -lr * Pred Grad: 0.022, New P: 1.508
iter 6 loss: 0.061
Actual params: [0.9433, 1.5084]
-Original Grad: 1.637, -lr * Pred Grad: -0.022, New P: 0.922
-Original Grad: -1.938, -lr * Pred Grad: 0.022, New P: 1.530
iter 7 loss: 0.124
Actual params: [0.9218, 1.5302]
-Original Grad: 1.537, -lr * Pred Grad: -0.022, New P: 0.900
-Original Grad: -1.335, -lr * Pred Grad: 0.022, New P: 1.552
iter 8 loss: 0.153
Actual params: [0.9002, 1.5519]
-Original Grad: 1.076, -lr * Pred Grad: -0.022, New P: 0.879
-Original Grad: -0.396, -lr * Pred Grad: 0.022, New P: 1.574
iter 9 loss: 0.223
Actual params: [0.8787, 1.5736]
-Original Grad: 1.388, -lr * Pred Grad: -0.022, New P: 0.857
-Original Grad: -0.954, -lr * Pred Grad: 0.022, New P: 1.595
iter 10 loss: 0.285
Actual params: [0.8571, 1.5953]
-Original Grad: 1.383, -lr * Pred Grad: -0.022, New P: 0.836
-Original Grad: -1.095, -lr * Pred Grad: 0.022, New P: 1.617
iter 11 loss: 0.334
Actual params: [0.8356, 1.617 ]
-Original Grad: 0.934, -lr * Pred Grad: -0.022, New P: 0.814
-Original Grad: -0.267, -lr * Pred Grad: 0.022, New P: 1.639
iter 12 loss: 0.376
Actual params: [0.8141, 1.6387]
-Original Grad: 1.377, -lr * Pred Grad: -0.022, New P: 0.793
-Original Grad: -0.745, -lr * Pred Grad: 0.022, New P: 1.660
iter 13 loss: 0.422
Actual params: [0.7925, 1.6604]
-Original Grad: 1.709, -lr * Pred Grad: -0.022, New P: 0.771
-Original Grad: -0.804, -lr * Pred Grad: 0.022, New P: 1.682
iter 14 loss: 0.472
Actual params: [0.771 , 1.6822]
-Original Grad: 2.031, -lr * Pred Grad: -0.022, New P: 0.749
-Original Grad: -1.066, -lr * Pred Grad: 0.022, New P: 1.704
iter 15 loss: 0.507
Actual params: [0.7495, 1.7039]
-Original Grad: -0.137, -lr * Pred Grad: -0.022, New P: 0.728
-Original Grad: 0.341, -lr * Pred Grad: 0.022, New P: 1.726
iter 16 loss: 0.538
Actual params: [0.7279, 1.7256]
-Original Grad: -0.188, -lr * Pred Grad: -0.022, New P: 0.706
-Original Grad: 0.077, -lr * Pred Grad: 0.022, New P: 1.747
iter 17 loss: 0.490
Actual params: [0.7064, 1.7473]
-Original Grad: 1.433, -lr * Pred Grad: -0.022, New P: 0.685
-Original Grad: -0.586, -lr * Pred Grad: 0.022, New P: 1.769
iter 18 loss: 0.494
Actual params: [0.6849, 1.769 ]
-Original Grad: -0.351, -lr * Pred Grad: -0.022, New P: 0.663
-Original Grad: -0.115, -lr * Pred Grad: 0.022, New P: 1.791
iter 19 loss: 0.499
Actual params: [0.6633, 1.7907]
-Original Grad: -0.244, -lr * Pred Grad: -0.022, New P: 0.642
-Original Grad: -0.115, -lr * Pred Grad: 0.022, New P: 1.812
iter 20 loss: 0.499
Actual params: [0.6418, 1.8124]
Target params: [1.1812, 0.2779]
Actual params: [ 0.0029, -1.5044]
-Original Grad: 0.098, -lr * Pred Grad: 0.013, New P: 0.016
-Original Grad: 0.134, -lr * Pred Grad: 0.014, New P: -1.490
iter 0 loss: 0.455
Actual params: [ 0.0158, -1.4903]
-Original Grad: 0.105, -lr * Pred Grad: 0.020, New P: 0.036
-Original Grad: 0.130, -lr * Pred Grad: 0.021, New P: -1.470
iter 1 loss: 0.452
Actual params: [ 0.0361, -1.4697]
-Original Grad: 0.126, -lr * Pred Grad: 0.022, New P: 0.058
-Original Grad: 0.139, -lr * Pred Grad: 0.022, New P: -1.448
iter 2 loss: 0.447
Actual params: [ 0.0576, -1.4481]
-Original Grad: 0.164, -lr * Pred Grad: 0.022, New P: 0.079
-Original Grad: 0.155, -lr * Pred Grad: 0.022, New P: -1.426
iter 3 loss: 0.439
Actual params: [ 0.0793, -1.4265]
-Original Grad: 0.121, -lr * Pred Grad: 0.022, New P: 0.101
-Original Grad: 0.121, -lr * Pred Grad: 0.022, New P: -1.405
iter 4 loss: 0.433
Actual params: [ 0.101 , -1.4047]
-Original Grad: 0.108, -lr * Pred Grad: 0.022, New P: 0.123
-Original Grad: 0.112, -lr * Pred Grad: 0.022, New P: -1.383
iter 5 loss: 0.427
Actual params: [ 0.1227, -1.383 ]
-Original Grad: 0.124, -lr * Pred Grad: 0.022, New P: 0.144
-Original Grad: 0.114, -lr * Pred Grad: 0.022, New P: -1.361
iter 6 loss: 0.422
Actual params: [ 0.1444, -1.3613]
-Original Grad: 0.166, -lr * Pred Grad: 0.022, New P: 0.166
-Original Grad: 0.129, -lr * Pred Grad: 0.022, New P: -1.340
iter 7 loss: 0.417
Actual params: [ 0.1661, -1.3396]
-Original Grad: 0.252, -lr * Pred Grad: 0.022, New P: 0.188
-Original Grad: 0.157, -lr * Pred Grad: 0.022, New P: -1.318
iter 8 loss: 0.407
Actual params: [ 0.1878, -1.3179]
-Original Grad: 0.167, -lr * Pred Grad: 0.022, New P: 0.210
-Original Grad: 0.118, -lr * Pred Grad: 0.022, New P: -1.296
iter 9 loss: 0.400
Actual params: [ 0.2096, -1.2962]
-Original Grad: 0.166, -lr * Pred Grad: 0.022, New P: 0.231
-Original Grad: 0.119, -lr * Pred Grad: 0.022, New P: -1.274
iter 10 loss: 0.394
Actual params: [ 0.2313, -1.2745]
-Original Grad: 0.193, -lr * Pred Grad: 0.022, New P: 0.253
-Original Grad: 0.134, -lr * Pred Grad: 0.022, New P: -1.253
iter 11 loss: 0.388
Actual params: [ 0.253 , -1.2528]
-Original Grad: 0.392, -lr * Pred Grad: 0.022, New P: 0.275
-Original Grad: 0.213, -lr * Pred Grad: 0.022, New P: -1.231
iter 12 loss: 0.375
Actual params: [ 0.2747, -1.2311]
-Original Grad: 0.226, -lr * Pred Grad: 0.022, New P: 0.296
-Original Grad: 0.153, -lr * Pred Grad: 0.022, New P: -1.209
iter 13 loss: 0.365
Actual params: [ 0.2964, -1.2094]
-Original Grad: 0.256, -lr * Pred Grad: 0.022, New P: 0.318
-Original Grad: 0.161, -lr * Pred Grad: 0.022, New P: -1.188
iter 14 loss: 0.357
Actual params: [ 0.3181, -1.1877]
-Original Grad: 0.568, -lr * Pred Grad: 0.022, New P: 0.340
-Original Grad: 0.271, -lr * Pred Grad: 0.022, New P: -1.166
iter 15 loss: 0.346
Actual params: [ 0.3398, -1.1659]
-Original Grad: 0.374, -lr * Pred Grad: 0.022, New P: 0.362
-Original Grad: 0.198, -lr * Pred Grad: 0.022, New P: -1.144
iter 16 loss: 0.327
Actual params: [ 0.3615, -1.1442]
-Original Grad: 0.383, -lr * Pred Grad: 0.022, New P: 0.383
-Original Grad: 0.204, -lr * Pred Grad: 0.022, New P: -1.123
iter 17 loss: 0.315
Actual params: [ 0.3832, -1.1225]
-Original Grad: 0.418, -lr * Pred Grad: 0.022, New P: 0.405
-Original Grad: 0.217, -lr * Pred Grad: 0.022, New P: -1.101
iter 18 loss: 0.301
Actual params: [ 0.4049, -1.1008]
-Original Grad: 0.422, -lr * Pred Grad: 0.022, New P: 0.427
-Original Grad: 0.223, -lr * Pred Grad: 0.022, New P: -1.079
iter 19 loss: 0.288
Actual params: [ 0.4266, -1.0791]
-Original Grad: 0.482, -lr * Pred Grad: 0.022, New P: 0.448
-Original Grad: 0.251, -lr * Pred Grad: 0.022, New P: -1.057
iter 20 loss: 0.272
Actual params: [ 0.4484, -1.0574]
Target params: [1.1812, 0.2779]
Actual params: [-0.8962,  0.3381]
-Original Grad: 0.164, -lr * Pred Grad: 0.015, New P: -0.881
-Original Grad: -0.126, -lr * Pred Grad: -0.011, New P: 0.328
iter 0 loss: 0.659
Actual params: [-0.8815,  0.3275]
-Original Grad: 0.099, -lr * Pred Grad: 0.021, New P: -0.861
-Original Grad: -0.103, -lr * Pred Grad: -0.020, New P: 0.308
iter 1 loss: 0.656
Actual params: [-0.8608,  0.3078]
-Original Grad: -0.765, -lr * Pred Grad: 0.022, New P: -0.839
-Original Grad: 0.109, -lr * Pred Grad: -0.021, New P: 0.287
iter 2 loss: 0.656
Actual params: [-0.8392,  0.2866]
-Original Grad: 1.802, -lr * Pred Grad: 0.022, New P: -0.818
-Original Grad: -0.422, -lr * Pred Grad: -0.021, New P: 0.265
iter 3 loss: 0.628
Actual params: [-0.8175,  0.2651]
-Original Grad: 1.976, -lr * Pred Grad: 0.022, New P: -0.796
-Original Grad: -0.280, -lr * Pred Grad: -0.022, New P: 0.244
iter 4 loss: 0.591
Actual params: [-0.7958,  0.2435]
-Original Grad: 1.811, -lr * Pred Grad: 0.022, New P: -0.774
-Original Grad: -0.209, -lr * Pred Grad: -0.022, New P: 0.222
iter 5 loss: 0.544
Actual params: [-0.7741,  0.222 ]
-Original Grad: 1.887, -lr * Pred Grad: 0.022, New P: -0.752
-Original Grad: -0.173, -lr * Pred Grad: -0.022, New P: 0.200
iter 6 loss: 0.501
Actual params: [-0.7524,  0.2005]
-Original Grad: 2.828, -lr * Pred Grad: 0.022, New P: -0.731
-Original Grad: -0.111, -lr * Pred Grad: -0.022, New P: 0.179
iter 7 loss: 0.451
Actual params: [-0.7308,  0.1789]
-Original Grad: 2.022, -lr * Pred Grad: 0.022, New P: -0.709
-Original Grad: -0.082, -lr * Pred Grad: -0.022, New P: 0.157
iter 8 loss: 0.398
Actual params: [-0.7091,  0.1574]
-Original Grad: 0.913, -lr * Pred Grad: 0.022, New P: -0.687
-Original Grad: 0.067, -lr * Pred Grad: -0.022, New P: 0.136
iter 9 loss: 0.359
Actual params: [-0.6874,  0.1359]
-Original Grad: 1.049, -lr * Pred Grad: 0.022, New P: -0.666
-Original Grad: 0.032, -lr * Pred Grad: -0.022, New P: 0.114
iter 10 loss: 0.335
Actual params: [-0.6657,  0.1143]
-Original Grad: 0.676, -lr * Pred Grad: 0.022, New P: -0.644
-Original Grad: 0.049, -lr * Pred Grad: -0.022, New P: 0.093
iter 11 loss: 0.317
Actual params: [-0.6439,  0.0928]
-Original Grad: 0.791, -lr * Pred Grad: 0.022, New P: -0.622
-Original Grad: 0.033, -lr * Pred Grad: -0.022, New P: 0.071
iter 12 loss: 0.304
Actual params: [-0.6222,  0.0712]
-Original Grad: 0.400, -lr * Pred Grad: 0.022, New P: -0.601
-Original Grad: 0.035, -lr * Pred Grad: -0.022, New P: 0.050
iter 13 loss: 0.294
Actual params: [-0.6005,  0.0497]
-Original Grad: 0.299, -lr * Pred Grad: 0.022, New P: -0.579
-Original Grad: 0.034, -lr * Pred Grad: -0.022, New P: 0.028
iter 14 loss: 0.287
Actual params: [-0.5788,  0.0282]
-Original Grad: 0.258, -lr * Pred Grad: 0.022, New P: -0.557
-Original Grad: 0.031, -lr * Pred Grad: -0.022, New P: 0.007
iter 15 loss: 0.281
Actual params: [-0.5571,  0.0066]
-Original Grad: 0.325, -lr * Pred Grad: 0.022, New P: -0.535
-Original Grad: 0.041, -lr * Pred Grad: -0.022, New P: -0.015
iter 16 loss: 0.276
Actual params: [-0.5354, -0.0149]
-Original Grad: 0.222, -lr * Pred Grad: 0.022, New P: -0.514
-Original Grad: 0.020, -lr * Pred Grad: -0.022, New P: -0.036
iter 17 loss: 0.270
Actual params: [-0.5137, -0.0364]
-Original Grad: 0.193, -lr * Pred Grad: 0.022, New P: -0.492
-Original Grad: 0.016, -lr * Pred Grad: -0.022, New P: -0.058
iter 18 loss: 0.267
Actual params: [-0.492, -0.058]
-Original Grad: 0.184, -lr * Pred Grad: 0.022, New P: -0.470
-Original Grad: 0.012, -lr * Pred Grad: -0.022, New P: -0.079
iter 19 loss: 0.263
Actual params: [-0.4703, -0.0795]
-Original Grad: 0.209, -lr * Pred Grad: 0.022, New P: -0.449
-Original Grad: 0.025, -lr * Pred Grad: -0.022, New P: -0.101
iter 20 loss: 0.259
Actual params: [-0.4486, -0.101 ]
Target params: [1.1812, 0.2779]
Actual params: [ 0.3685, -2.1923]
-Original Grad: -0.149, -lr * Pred Grad: -0.012, New P: 0.356
-Original Grad: 0.130, -lr * Pred Grad: 0.014, New P: -2.178
iter 0 loss: 0.738
Actual params: [ 0.3565, -2.1783]
-Original Grad: 0.348, -lr * Pred Grad: -0.019, New P: 0.337
-Original Grad: -0.482, -lr * Pred Grad: 0.021, New P: -2.158
iter 1 loss: 0.735
Actual params: [ 0.3373, -2.1577]
-Original Grad: 0.091, -lr * Pred Grad: -0.020, New P: 0.317
-Original Grad: -0.137, -lr * Pred Grad: 0.022, New P: -2.136
iter 2 loss: 0.739
Actual params: [ 0.3173, -2.1361]
-Original Grad: 0.083, -lr * Pred Grad: -0.017, New P: 0.300
-Original Grad: -0.147, -lr * Pred Grad: 0.022, New P: -2.114
iter 3 loss: 0.744
Actual params: [ 0.3002, -2.1144]
-Original Grad: -0.286, -lr * Pred Grad: -0.009, New P: 0.291
-Original Grad: 0.536, -lr * Pred Grad: 0.022, New P: -2.093
iter 4 loss: 0.740
Actual params: [ 0.2915, -2.0926]
-Original Grad: 0.033, -lr * Pred Grad: 0.007, New P: 0.299
-Original Grad: -0.069, -lr * Pred Grad: 0.022, New P: -2.071
iter 5 loss: 0.746
Actual params: [ 0.2986, -2.0709]
-Original Grad: -0.113, -lr * Pred Grad: 0.018, New P: 0.316
-Original Grad: 0.161, -lr * Pred Grad: 0.022, New P: -2.049
iter 6 loss: 0.745
Actual params: [ 0.3165, -2.0491]
-Original Grad: -0.062, -lr * Pred Grad: 0.021, New P: 0.337
-Original Grad: 0.115, -lr * Pred Grad: 0.022, New P: -2.027
iter 7 loss: 0.746
Actual params: [ 0.3375, -2.0274]
-Original Grad: 0.009, -lr * Pred Grad: 0.022, New P: 0.359
-Original Grad: -0.045, -lr * Pred Grad: 0.022, New P: -2.006
iter 8 loss: 0.748
Actual params: [ 0.3591, -2.0057]
-Original Grad: 0.069, -lr * Pred Grad: 0.022, New P: 0.381
-Original Grad: -0.139, -lr * Pred Grad: 0.022, New P: -1.984
iter 9 loss: 0.750
Actual params: [ 0.3808, -1.9839]
-Original Grad: 0.033, -lr * Pred Grad: 0.022, New P: 0.403
-Original Grad: -0.080, -lr * Pred Grad: 0.022, New P: -1.962
iter 10 loss: 0.751
Actual params: [ 0.4025, -1.9622]
-Original Grad: 0.095, -lr * Pred Grad: 0.022, New P: 0.424
-Original Grad: -0.191, -lr * Pred Grad: 0.022, New P: -1.941
iter 11 loss: 0.753
Actual params: [ 0.4242, -1.9405]
-Original Grad: 0.073, -lr * Pred Grad: 0.022, New P: 0.446
-Original Grad: -0.177, -lr * Pred Grad: 0.022, New P: -1.919
iter 12 loss: 0.754
Actual params: [ 0.4459, -1.9188]
-Original Grad: -0.460, -lr * Pred Grad: 0.022, New P: 0.468
-Original Grad: 1.080, -lr * Pred Grad: 0.022, New P: -1.897
iter 13 loss: 0.749
Actual params: [ 0.4676, -1.8971]
-Original Grad: -0.063, -lr * Pred Grad: 0.022, New P: 0.489
-Original Grad: 0.059, -lr * Pred Grad: 0.022, New P: -1.875
iter 14 loss: 0.753
Actual params: [ 0.4893, -1.8754]
-Original Grad: -0.038, -lr * Pred Grad: 0.022, New P: 0.511
-Original Grad: -0.054, -lr * Pred Grad: 0.022, New P: -1.854
iter 15 loss: 0.754
Actual params: [ 0.5111, -1.8537]
-Original Grad: -0.036, -lr * Pred Grad: 0.022, New P: 0.533
-Original Grad: 0.055, -lr * Pred Grad: 0.022, New P: -1.832
iter 16 loss: 0.754
Actual params: [ 0.5328, -1.832 ]
-Original Grad: 0.020, -lr * Pred Grad: 0.022, New P: 0.554
-Original Grad: 0.205, -lr * Pred Grad: 0.022, New P: -1.810
iter 17 loss: 0.752
Actual params: [ 0.5545, -1.8103]
-Original Grad: 0.045, -lr * Pred Grad: 0.022, New P: 0.576
-Original Grad: 0.151, -lr * Pred Grad: 0.022, New P: -1.789
iter 18 loss: 0.759
Actual params: [ 0.5762, -1.7886]
-Original Grad: -0.445, -lr * Pred Grad: 0.022, New P: 0.598
-Original Grad: -0.462, -lr * Pred Grad: 0.022, New P: -1.767
iter 19 loss: 0.757
Actual params: [ 0.5979, -1.7668]
-Original Grad: 0.098, -lr * Pred Grad: 0.022, New P: 0.620
-Original Grad: 0.204, -lr * Pred Grad: 0.022, New P: -1.745
iter 20 loss: 0.752
Actual params: [ 0.6196, -1.7451]
Target params: [1.1812, 0.2779]
Actual params: [-0.1438, -1.3168]
-Original Grad: 0.142, -lr * Pred Grad: 0.014, New P: -0.130
-Original Grad: 0.022, -lr * Pred Grad: 0.007, New P: -1.310
iter 0 loss: 0.709
Actual params: [-0.1295, -1.3095]
-Original Grad: 0.196, -lr * Pred Grad: 0.021, New P: -0.109
-Original Grad: 0.035, -lr * Pred Grad: 0.019, New P: -1.291
iter 1 loss: 0.707
Actual params: [-0.1089, -1.2905]
-Original Grad: 0.241, -lr * Pred Grad: 0.022, New P: -0.087
-Original Grad: 0.045, -lr * Pred Grad: 0.021, New P: -1.269
iter 2 loss: 0.701
Actual params: [-0.0874, -1.2692]
-Original Grad: 0.270, -lr * Pred Grad: 0.022, New P: -0.066
-Original Grad: 0.040, -lr * Pred Grad: 0.022, New P: -1.247
iter 3 loss: 0.695
Actual params: [-0.0657, -1.2475]
-Original Grad: 0.277, -lr * Pred Grad: 0.022, New P: -0.044
-Original Grad: 0.033, -lr * Pred Grad: 0.022, New P: -1.226
iter 4 loss: 0.688
Actual params: [-0.044 , -1.2258]
-Original Grad: 0.255, -lr * Pred Grad: 0.022, New P: -0.022
-Original Grad: 0.029, -lr * Pred Grad: 0.022, New P: -1.204
iter 5 loss: 0.681
Actual params: [-0.0223, -1.2041]
-Original Grad: 0.101, -lr * Pred Grad: 0.022, New P: -0.001
-Original Grad: 0.019, -lr * Pred Grad: 0.022, New P: -1.182
iter 6 loss: 0.675
Actual params: [-5.5498e-04, -1.1824e+00]
-Original Grad: 0.149, -lr * Pred Grad: 0.022, New P: 0.021
-Original Grad: 0.025, -lr * Pred Grad: 0.022, New P: -1.161
iter 7 loss: 0.685
Actual params: [ 0.0212, -1.1607]
-Original Grad: 0.220, -lr * Pred Grad: 0.022, New P: 0.043
-Original Grad: 0.035, -lr * Pred Grad: 0.022, New P: -1.139
iter 8 loss: 0.680
Actual params: [ 0.0429, -1.1389]
-Original Grad: 0.218, -lr * Pred Grad: 0.022, New P: 0.065
-Original Grad: 0.040, -lr * Pred Grad: 0.022, New P: -1.117
iter 9 loss: 0.674
Actual params: [ 0.0646, -1.1172]
-Original Grad: 0.221, -lr * Pred Grad: 0.022, New P: 0.086
-Original Grad: 0.042, -lr * Pred Grad: 0.022, New P: -1.096
iter 10 loss: 0.669
Actual params: [ 0.0863, -1.0955]
-Original Grad: 0.207, -lr * Pred Grad: 0.022, New P: 0.108
-Original Grad: 0.038, -lr * Pred Grad: 0.022, New P: -1.074
iter 11 loss: 0.663
Actual params: [ 0.108 , -1.0738]
-Original Grad: 0.196, -lr * Pred Grad: 0.022, New P: 0.130
-Original Grad: 0.039, -lr * Pred Grad: 0.022, New P: -1.052
iter 12 loss: 0.658
Actual params: [ 0.1297, -1.0521]
-Original Grad: 0.198, -lr * Pred Grad: 0.022, New P: 0.151
-Original Grad: 0.037, -lr * Pred Grad: 0.022, New P: -1.030
iter 13 loss: 0.653
Actual params: [ 0.1514, -1.0304]
-Original Grad: 0.201, -lr * Pred Grad: 0.022, New P: 0.173
-Original Grad: 0.041, -lr * Pred Grad: 0.022, New P: -1.009
iter 14 loss: 0.648
Actual params: [ 0.1731, -1.0087]
-Original Grad: 0.206, -lr * Pred Grad: 0.022, New P: 0.195
-Original Grad: 0.040, -lr * Pred Grad: 0.022, New P: -0.987
iter 15 loss: 0.642
Actual params: [ 0.1948, -0.987 ]
-Original Grad: 0.137, -lr * Pred Grad: 0.022, New P: 0.217
-Original Grad: 0.030, -lr * Pred Grad: 0.022, New P: -0.965
iter 16 loss: 0.638
Actual params: [ 0.2165, -0.9653]
-Original Grad: 0.255, -lr * Pred Grad: 0.022, New P: 0.238
-Original Grad: 0.046, -lr * Pred Grad: 0.022, New P: -0.944
iter 17 loss: 0.634
Actual params: [ 0.2382, -0.9436]
-Original Grad: 0.197, -lr * Pred Grad: 0.022, New P: 0.260
-Original Grad: 0.039, -lr * Pred Grad: 0.022, New P: -0.922
iter 18 loss: 0.643
Actual params: [ 0.26  , -0.9219]
-Original Grad: 0.223, -lr * Pred Grad: 0.022, New P: 0.282
-Original Grad: 0.043, -lr * Pred Grad: 0.022, New P: -0.900
iter 19 loss: 0.638
Actual params: [ 0.2817, -0.9001]
-Original Grad: 0.235, -lr * Pred Grad: 0.022, New P: 0.303
-Original Grad: 0.044, -lr * Pred Grad: 0.022, New P: -0.878
iter 20 loss: 0.632
Actual params: [ 0.3034, -0.8784]
Target params: [1.1812, 0.2779]
Actual params: [-1.23  , -0.0332]
-Original Grad: -0.029, -lr * Pred Grad: 0.000, New P: -1.229
-Original Grad: 0.006, -lr * Pred Grad: 0.005, New P: -0.028
iter 0 loss: 0.523
Actual params: [-1.2295, -0.0278]
-Original Grad: -0.028, -lr * Pred Grad: 0.012, New P: -1.217
-Original Grad: 0.006, -lr * Pred Grad: 0.018, New P: -0.009
iter 1 loss: 0.523
Actual params: [-1.2174, -0.0094]
-Original Grad: -0.024, -lr * Pred Grad: 0.020, New P: -1.197
-Original Grad: 0.005, -lr * Pred Grad: 0.021, New P: 0.012
iter 2 loss: 0.523
Actual params: [-1.1972,  0.0119]
-Original Grad: -0.024, -lr * Pred Grad: 0.022, New P: -1.176
-Original Grad: 0.004, -lr * Pred Grad: 0.022, New P: 0.034
iter 3 loss: 0.524
Actual params: [-1.1757,  0.0335]
-Original Grad: -0.026, -lr * Pred Grad: 0.022, New P: -1.154
-Original Grad: 0.002, -lr * Pred Grad: 0.022, New P: 0.055
iter 4 loss: 0.524
Actual params: [-1.154 ,  0.0552]
-Original Grad: -0.010, -lr * Pred Grad: 0.022, New P: -1.132
-Original Grad: -0.004, -lr * Pred Grad: 0.022, New P: 0.077
iter 5 loss: 0.524
Actual params: [-1.1323,  0.0769]
-Original Grad: -0.005, -lr * Pred Grad: 0.022, New P: -1.111
-Original Grad: -0.005, -lr * Pred Grad: 0.022, New P: 0.099
iter 6 loss: 0.525
Actual params: [-1.1106,  0.0986]
-Original Grad: -0.002, -lr * Pred Grad: 0.022, New P: -1.089
-Original Grad: -0.006, -lr * Pred Grad: 0.022, New P: 0.120
iter 7 loss: 0.525
Actual params: [-1.0889,  0.1204]
-Original Grad: -0.006, -lr * Pred Grad: 0.022, New P: -1.067
-Original Grad: -0.004, -lr * Pred Grad: 0.022, New P: 0.142
iter 8 loss: 0.525
Actual params: [-1.0671,  0.1421]
-Original Grad: 0.001, -lr * Pred Grad: 0.022, New P: -1.045
-Original Grad: -0.009, -lr * Pred Grad: 0.022, New P: 0.164
iter 9 loss: 0.525
Actual params: [-1.0454,  0.1638]
-Original Grad: 0.005, -lr * Pred Grad: 0.022, New P: -1.024
-Original Grad: -0.010, -lr * Pred Grad: 0.022, New P: 0.185
iter 10 loss: 0.525
Actual params: [-1.0237,  0.1855]
-Original Grad: 0.017, -lr * Pred Grad: 0.022, New P: -1.002
-Original Grad: 0.004, -lr * Pred Grad: 0.022, New P: 0.207
iter 11 loss: 0.525
Actual params: [-1.002 ,  0.2072]
-Original Grad: 0.005, -lr * Pred Grad: 0.022, New P: -0.980
-Original Grad: -0.014, -lr * Pred Grad: 0.022, New P: 0.229
iter 12 loss: 0.525
Actual params: [-0.9803,  0.2289]
-Original Grad: 0.007, -lr * Pred Grad: 0.022, New P: -0.959
-Original Grad: -0.004, -lr * Pred Grad: 0.022, New P: 0.251
iter 13 loss: 0.525
Actual params: [-0.9586,  0.2506]
-Original Grad: 0.007, -lr * Pred Grad: 0.022, New P: -0.937
-Original Grad: -0.006, -lr * Pred Grad: 0.022, New P: 0.272
iter 14 loss: 0.525
Actual params: [-0.9369,  0.2723]
-Original Grad: 0.005, -lr * Pred Grad: 0.022, New P: -0.915
-Original Grad: -0.008, -lr * Pred Grad: 0.022, New P: 0.294
iter 15 loss: 0.525
Actual params: [-0.9152,  0.294 ]
-Original Grad: 0.005, -lr * Pred Grad: 0.022, New P: -0.893
-Original Grad: -0.010, -lr * Pred Grad: 0.022, New P: 0.316
iter 16 loss: 0.525
Actual params: [-0.8935,  0.3157]
-Original Grad: 0.005, -lr * Pred Grad: 0.022, New P: -0.872
-Original Grad: -0.011, -lr * Pred Grad: 0.022, New P: 0.337
iter 17 loss: 0.525
Actual params: [-0.8718,  0.3375]
-Original Grad: 0.005, -lr * Pred Grad: 0.022, New P: -0.850
-Original Grad: -0.009, -lr * Pred Grad: 0.022, New P: 0.359
iter 18 loss: 0.525
Actual params: [-0.8501,  0.3592]
-Original Grad: 0.006, -lr * Pred Grad: 0.022, New P: -0.828
-Original Grad: -0.006, -lr * Pred Grad: 0.022, New P: 0.381
iter 19 loss: 0.525
Actual params: [-0.8283,  0.3809]
-Original Grad: 0.007, -lr * Pred Grad: 0.022, New P: -0.807
-Original Grad: -0.004, -lr * Pred Grad: 0.022, New P: 0.403
iter 20 loss: 0.525
Actual params: [-0.8066,  0.4026]
Target params: [1.1812, 0.2779]
Actual params: [ 1.0788, -1.6003]
-Original Grad: 0.073, -lr * Pred Grad: 0.012, New P: 1.090
-Original Grad: 0.236, -lr * Pred Grad: 0.016, New P: -1.585
iter 0 loss: 0.382
Actual params: [ 1.0904, -1.5848]
-Original Grad: 0.072, -lr * Pred Grad: 0.020, New P: 1.110
-Original Grad: 0.220, -lr * Pred Grad: 0.021, New P: -1.564
iter 1 loss: 0.376
Actual params: [ 1.1105, -1.564 ]
-Original Grad: -0.055, -lr * Pred Grad: 0.021, New P: 1.132
-Original Grad: -0.170, -lr * Pred Grad: 0.022, New P: -1.542
iter 2 loss: 0.371
Actual params: [ 1.132 , -1.5424]
-Original Grad: 0.059, -lr * Pred Grad: 0.022, New P: 1.154
-Original Grad: 0.184, -lr * Pred Grad: 0.022, New P: -1.521
iter 3 loss: 0.366
Actual params: [ 1.1536, -1.5207]
-Original Grad: 0.067, -lr * Pred Grad: 0.022, New P: 1.175
-Original Grad: 0.194, -lr * Pred Grad: 0.022, New P: -1.499
iter 4 loss: 0.360
Actual params: [ 1.1753, -1.499 ]
-Original Grad: -0.035, -lr * Pred Grad: 0.022, New P: 1.197
-Original Grad: -0.092, -lr * Pred Grad: 0.022, New P: -1.477
iter 5 loss: 0.355
Actual params: [ 1.1971, -1.4773]
-Original Grad: 0.058, -lr * Pred Grad: 0.022, New P: 1.219
-Original Grad: 0.160, -lr * Pred Grad: 0.022, New P: -1.456
iter 6 loss: 0.351
Actual params: [ 1.2188, -1.4556]
-Original Grad: 0.054, -lr * Pred Grad: 0.022, New P: 1.240
-Original Grad: 0.166, -lr * Pred Grad: 0.022, New P: -1.434
iter 7 loss: 0.346
Actual params: [ 1.2405, -1.4339]
-Original Grad: 0.041, -lr * Pred Grad: 0.022, New P: 1.262
-Original Grad: 0.143, -lr * Pred Grad: 0.022, New P: -1.412
iter 8 loss: 0.341
Actual params: [ 1.2622, -1.4121]
-Original Grad: 0.008, -lr * Pred Grad: 0.022, New P: 1.284
-Original Grad: 0.049, -lr * Pred Grad: 0.022, New P: -1.390
iter 9 loss: 0.337
Actual params: [ 1.2839, -1.3904]
-Original Grad: 0.043, -lr * Pred Grad: 0.022, New P: 1.306
-Original Grad: 0.159, -lr * Pred Grad: 0.022, New P: -1.369
iter 10 loss: 0.335
Actual params: [ 1.3056, -1.3687]
-Original Grad: 0.014, -lr * Pred Grad: 0.022, New P: 1.327
-Original Grad: 0.074, -lr * Pred Grad: 0.022, New P: -1.347
iter 11 loss: 0.331
Actual params: [ 1.3273, -1.347 ]
-Original Grad: 0.019, -lr * Pred Grad: 0.022, New P: 1.349
-Original Grad: 0.083, -lr * Pred Grad: 0.022, New P: -1.325
iter 12 loss: 0.329
Actual params: [ 1.349 , -1.3253]
-Original Grad: 0.038, -lr * Pred Grad: 0.022, New P: 1.371
-Original Grad: 0.137, -lr * Pred Grad: 0.022, New P: -1.304
iter 13 loss: 0.325
Actual params: [ 1.3707, -1.3036]
-Original Grad: 0.016, -lr * Pred Grad: 0.022, New P: 1.392
-Original Grad: 0.069, -lr * Pred Grad: 0.022, New P: -1.282
iter 14 loss: 0.322
Actual params: [ 1.3924, -1.2819]
-Original Grad: -0.000, -lr * Pred Grad: 0.022, New P: 1.414
-Original Grad: 0.037, -lr * Pred Grad: 0.022, New P: -1.260
iter 15 loss: 0.315
Actual params: [ 1.4141, -1.2602]
-Original Grad: 0.010, -lr * Pred Grad: 0.022, New P: 1.436
-Original Grad: 0.064, -lr * Pred Grad: 0.022, New P: -1.238
iter 16 loss: 0.314
Actual params: [ 1.4359, -1.2385]
-Original Grad: 0.012, -lr * Pred Grad: 0.022, New P: 1.458
-Original Grad: 0.067, -lr * Pred Grad: 0.022, New P: -1.217
iter 17 loss: 0.312
Actual params: [ 1.4576, -1.2168]
-Original Grad: 0.012, -lr * Pred Grad: 0.022, New P: 1.479
-Original Grad: 0.071, -lr * Pred Grad: 0.022, New P: -1.195
iter 18 loss: 0.310
Actual params: [ 1.4793, -1.195 ]
-Original Grad: 0.011, -lr * Pred Grad: 0.022, New P: 1.501
-Original Grad: 0.071, -lr * Pred Grad: 0.022, New P: -1.173
iter 19 loss: 0.308
Actual params: [ 1.501 , -1.1733]
-Original Grad: 0.011, -lr * Pred Grad: 0.022, New P: 1.523
-Original Grad: 0.075, -lr * Pred Grad: 0.022, New P: -1.152
iter 20 loss: 0.307
Actual params: [ 1.5227, -1.1516]
Target params: [1.1812, 0.2779]
Actual params: [-0.7653,  1.3313]
-Original Grad: 0.008, -lr * Pred Grad: 0.006, New P: -0.760
-Original Grad: 0.023, -lr * Pred Grad: 0.007, New P: 1.339
iter 0 loss: 0.396
Actual params: [-0.7598,  1.3386]
-Original Grad: 0.008, -lr * Pred Grad: 0.018, New P: -0.741
-Original Grad: 0.026, -lr * Pred Grad: 0.019, New P: 1.358
iter 1 loss: 0.396
Actual params: [-0.7413,  1.3576]
-Original Grad: 0.010, -lr * Pred Grad: 0.021, New P: -0.720
-Original Grad: 0.030, -lr * Pred Grad: 0.021, New P: 1.379
iter 2 loss: 0.395
Actual params: [-0.72 ,  1.379]
-Original Grad: 0.007, -lr * Pred Grad: 0.022, New P: -0.698
-Original Grad: 0.034, -lr * Pred Grad: 0.022, New P: 1.401
iter 3 loss: 0.394
Actual params: [-0.6984,  1.4006]
-Original Grad: 0.008, -lr * Pred Grad: 0.022, New P: -0.677
-Original Grad: 0.037, -lr * Pred Grad: 0.022, New P: 1.422
iter 4 loss: 0.394
Actual params: [-0.6767,  1.4223]
-Original Grad: 0.011, -lr * Pred Grad: 0.022, New P: -0.655
-Original Grad: 0.037, -lr * Pred Grad: 0.022, New P: 1.444
iter 5 loss: 0.392
Actual params: [-0.655,  1.444]
-Original Grad: 0.015, -lr * Pred Grad: 0.022, New P: -0.633
-Original Grad: 0.034, -lr * Pred Grad: 0.022, New P: 1.466
iter 6 loss: 0.391
Actual params: [-0.6333,  1.4657]
-Original Grad: 0.018, -lr * Pred Grad: 0.022, New P: -0.612
-Original Grad: 0.037, -lr * Pred Grad: 0.022, New P: 1.487
iter 7 loss: 0.390
Actual params: [-0.6115,  1.4875]
-Original Grad: 0.019, -lr * Pred Grad: 0.022, New P: -0.590
-Original Grad: 0.044, -lr * Pred Grad: 0.022, New P: 1.509
iter 8 loss: 0.389
Actual params: [-0.5898,  1.5092]
-Original Grad: 0.020, -lr * Pred Grad: 0.022, New P: -0.568
-Original Grad: 0.043, -lr * Pred Grad: 0.022, New P: 1.531
iter 9 loss: 0.388
Actual params: [-0.5681,  1.5309]
-Original Grad: 0.021, -lr * Pred Grad: 0.022, New P: -0.546
-Original Grad: 0.045, -lr * Pred Grad: 0.022, New P: 1.553
iter 10 loss: 0.386
Actual params: [-0.5464,  1.5526]
-Original Grad: 0.022, -lr * Pred Grad: 0.022, New P: -0.525
-Original Grad: 0.048, -lr * Pred Grad: 0.022, New P: 1.574
iter 11 loss: 0.385
Actual params: [-0.5247,  1.5743]
-Original Grad: 0.013, -lr * Pred Grad: 0.022, New P: -0.503
-Original Grad: 0.064, -lr * Pred Grad: 0.022, New P: 1.596
iter 12 loss: 0.383
Actual params: [-0.503,  1.596]
-Original Grad: -0.037, -lr * Pred Grad: 0.022, New P: -0.481
-Original Grad: 0.340, -lr * Pred Grad: 0.022, New P: 1.618
iter 13 loss: 0.381
Actual params: [-0.4813,  1.6177]
-Original Grad: 0.018, -lr * Pred Grad: 0.022, New P: -0.460
-Original Grad: 0.010, -lr * Pred Grad: 0.022, New P: 1.639
iter 14 loss: 0.380
Actual params: [-0.4596,  1.6394]
-Original Grad: 0.013, -lr * Pred Grad: 0.022, New P: -0.438
-Original Grad: 0.070, -lr * Pred Grad: 0.022, New P: 1.661
iter 15 loss: 0.379
Actual params: [-0.4379,  1.6611]
-Original Grad: 0.008, -lr * Pred Grad: 0.022, New P: -0.416
-Original Grad: 0.078, -lr * Pred Grad: 0.022, New P: 1.683
iter 16 loss: 0.377
Actual params: [-0.4162,  1.6828]
-Original Grad: 0.021, -lr * Pred Grad: 0.022, New P: -0.394
-Original Grad: 0.090, -lr * Pred Grad: 0.022, New P: 1.705
iter 17 loss: 0.375
Actual params: [-0.3944,  1.7045]
-Original Grad: 0.038, -lr * Pred Grad: 0.022, New P: -0.373
-Original Grad: 0.096, -lr * Pred Grad: 0.022, New P: 1.726
iter 18 loss: 0.372
Actual params: [-0.3727,  1.7263]
-Original Grad: 0.044, -lr * Pred Grad: 0.022, New P: -0.351
-Original Grad: 0.095, -lr * Pred Grad: 0.022, New P: 1.748
iter 19 loss: 0.369
Actual params: [-0.351,  1.748]
-Original Grad: 0.042, -lr * Pred Grad: 0.022, New P: -0.329
-Original Grad: 0.084, -lr * Pred Grad: 0.022, New P: 1.770
iter 20 loss: 0.366
Actual params: [-0.3293,  1.7697]
Target params: [1.1812, 0.2779]
Actual params: [ 0.3094, -0.0048]
-Original Grad: 0.053, -lr * Pred Grad: 0.010, New P: 0.320
-Original Grad: 0.029, -lr * Pred Grad: 0.008, New P: 0.003
iter 0 loss: 0.386
Actual params: [0.3195, 0.0033]
-Original Grad: 0.003, -lr * Pred Grad: 0.020, New P: 0.339
-Original Grad: 0.015, -lr * Pred Grad: 0.019, New P: 0.022
iter 1 loss: 0.386
Actual params: [0.3393, 0.0225]
-Original Grad: -0.044, -lr * Pred Grad: 0.021, New P: 0.361
-Original Grad: 0.011, -lr * Pred Grad: 0.021, New P: 0.044
iter 2 loss: 0.386
Actual params: [0.3607, 0.0439]
-Original Grad: -0.074, -lr * Pred Grad: 0.022, New P: 0.382
-Original Grad: 0.011, -lr * Pred Grad: 0.022, New P: 0.066
iter 3 loss: 0.387
Actual params: [0.3824, 0.0655]
-Original Grad: -0.104, -lr * Pred Grad: 0.022, New P: 0.404
-Original Grad: 0.019, -lr * Pred Grad: 0.022, New P: 0.087
iter 4 loss: 0.389
Actual params: [0.4041, 0.0872]
-Original Grad: -0.046, -lr * Pred Grad: 0.022, New P: 0.426
-Original Grad: 0.036, -lr * Pred Grad: 0.022, New P: 0.109
iter 5 loss: 0.390
Actual params: [0.4258, 0.109 ]
-Original Grad: 0.130, -lr * Pred Grad: 0.022, New P: 0.448
-Original Grad: 0.053, -lr * Pred Grad: 0.022, New P: 0.131
iter 6 loss: 0.389
Actual params: [0.4475, 0.1307]
-Original Grad: -0.058, -lr * Pred Grad: 0.022, New P: 0.469
-Original Grad: 0.033, -lr * Pred Grad: 0.022, New P: 0.152
iter 7 loss: 0.377
Actual params: [0.4692, 0.1524]
-Original Grad: -0.074, -lr * Pred Grad: 0.022, New P: 0.491
-Original Grad: 0.032, -lr * Pred Grad: 0.022, New P: 0.174
iter 8 loss: 0.378
Actual params: [0.491 , 0.1741]
-Original Grad: -0.029, -lr * Pred Grad: 0.022, New P: 0.513
-Original Grad: 0.059, -lr * Pred Grad: 0.022, New P: 0.196
iter 9 loss: 0.380
Actual params: [0.5127, 0.1958]
-Original Grad: 0.138, -lr * Pred Grad: 0.022, New P: 0.534
-Original Grad: 0.052, -lr * Pred Grad: 0.022, New P: 0.218
iter 10 loss: 0.368
Actual params: [0.5344, 0.2175]
-Original Grad: 0.038, -lr * Pred Grad: 0.022, New P: 0.556
-Original Grad: 0.068, -lr * Pred Grad: 0.022, New P: 0.239
iter 11 loss: 0.371
Actual params: [0.5561, 0.2392]
-Original Grad: -0.085, -lr * Pred Grad: 0.022, New P: 0.578
-Original Grad: 0.087, -lr * Pred Grad: 0.022, New P: 0.261
iter 12 loss: 0.360
Actual params: [0.5778, 0.2609]
-Original Grad: 0.343, -lr * Pred Grad: 0.022, New P: 0.600
-Original Grad: 0.102, -lr * Pred Grad: 0.022, New P: 0.283
iter 13 loss: 0.360
Actual params: [0.5995, 0.2826]
-Original Grad: -0.297, -lr * Pred Grad: 0.022, New P: 0.621
-Original Grad: 0.153, -lr * Pred Grad: 0.022, New P: 0.304
iter 14 loss: 0.349
Actual params: [0.6212, 0.3043]
-Original Grad: -0.034, -lr * Pred Grad: 0.022, New P: 0.643
-Original Grad: 0.152, -lr * Pred Grad: 0.022, New P: 0.326
iter 15 loss: 0.337
Actual params: [0.6429, 0.3261]
-Original Grad: 0.427, -lr * Pred Grad: 0.022, New P: 0.665
-Original Grad: 0.144, -lr * Pred Grad: 0.022, New P: 0.348
iter 16 loss: 0.334
Actual params: [0.6646, 0.3478]
-Original Grad: -0.025, -lr * Pred Grad: 0.022, New P: 0.686
-Original Grad: 0.179, -lr * Pred Grad: 0.022, New P: 0.369
iter 17 loss: 0.322
Actual params: [0.6863, 0.3695]
-Original Grad: 0.104, -lr * Pred Grad: 0.022, New P: 0.708
-Original Grad: 0.159, -lr * Pred Grad: 0.022, New P: 0.391
iter 18 loss: 0.309
Actual params: [0.708 , 0.3912]
-Original Grad: 0.214, -lr * Pred Grad: 0.022, New P: 0.730
-Original Grad: 0.155, -lr * Pred Grad: 0.022, New P: 0.413
iter 19 loss: 0.296
Actual params: [0.7298, 0.4129]
-Original Grad: -0.286, -lr * Pred Grad: 0.022, New P: 0.751
-Original Grad: 0.248, -lr * Pred Grad: 0.022, New P: 0.435
iter 20 loss: 0.282
Actual params: [0.7515, 0.4346]
Target params: [1.1812, 0.2779]
Actual params: [1.7812, 1.0158]
-Original Grad: 0.004, -lr * Pred Grad: 0.005, New P: 1.786
-Original Grad: 0.164, -lr * Pred Grad: 0.015, New P: 1.031
iter 0 loss: 0.681
Actual params: [1.7863, 1.0305]
-Original Grad: 0.003, -lr * Pred Grad: 0.018, New P: 1.805
-Original Grad: 0.167, -lr * Pred Grad: 0.021, New P: 1.051
iter 1 loss: 0.678
Actual params: [1.8046, 1.0512]
-Original Grad: 0.001, -lr * Pred Grad: 0.021, New P: 1.826
-Original Grad: 0.182, -lr * Pred Grad: 0.022, New P: 1.073
iter 2 loss: 0.675
Actual params: [1.8258, 1.0728]
-Original Grad: 0.006, -lr * Pred Grad: 0.022, New P: 1.848
-Original Grad: 0.192, -lr * Pred Grad: 0.022, New P: 1.094
iter 3 loss: 0.671
Actual params: [1.8475, 1.0945]
-Original Grad: 0.004, -lr * Pred Grad: 0.022, New P: 1.869
-Original Grad: 0.227, -lr * Pred Grad: 0.022, New P: 1.116
iter 4 loss: 0.666
Actual params: [1.8692, 1.1162]
-Original Grad: 0.010, -lr * Pred Grad: 0.022, New P: 1.891
-Original Grad: 0.278, -lr * Pred Grad: 0.022, New P: 1.138
iter 5 loss: 0.660
Actual params: [1.8909, 1.1379]
-Original Grad: 0.018, -lr * Pred Grad: 0.022, New P: 1.913
-Original Grad: 0.309, -lr * Pred Grad: 0.022, New P: 1.160
iter 6 loss: 0.654
Actual params: [1.9126, 1.1596]
-Original Grad: 0.021, -lr * Pred Grad: 0.022, New P: 1.934
-Original Grad: 0.318, -lr * Pred Grad: 0.022, New P: 1.181
iter 7 loss: 0.647
Actual params: [1.9343, 1.1813]
-Original Grad: 0.022, -lr * Pred Grad: 0.022, New P: 1.956
-Original Grad: 0.318, -lr * Pred Grad: 0.022, New P: 1.203
iter 8 loss: 0.640
Actual params: [1.956, 1.203]
-Original Grad: 0.030, -lr * Pred Grad: 0.022, New P: 1.978
-Original Grad: 0.317, -lr * Pred Grad: 0.022, New P: 1.225
iter 9 loss: 0.632
Actual params: [1.9778, 1.2247]
-Original Grad: 0.044, -lr * Pred Grad: 0.022, New P: 1.999
-Original Grad: 0.458, -lr * Pred Grad: 0.022, New P: 1.246
iter 10 loss: 0.623
Actual params: [1.9995, 1.2464]
-Original Grad: 0.030, -lr * Pred Grad: 0.022, New P: 2.021
-Original Grad: 0.245, -lr * Pred Grad: 0.022, New P: 1.268
iter 11 loss: 0.614
Actual params: [2.0212, 1.2681]
-Original Grad: 0.039, -lr * Pred Grad: 0.022, New P: 2.043
-Original Grad: 0.392, -lr * Pred Grad: 0.022, New P: 1.290
iter 12 loss: 0.606
Actual params: [2.0429, 1.2899]
-Original Grad: 0.037, -lr * Pred Grad: 0.022, New P: 2.065
-Original Grad: 0.438, -lr * Pred Grad: 0.022, New P: 1.312
iter 13 loss: 0.596
Actual params: [2.0646, 1.3116]
-Original Grad: 0.039, -lr * Pred Grad: 0.022, New P: 2.086
-Original Grad: 0.518, -lr * Pred Grad: 0.022, New P: 1.333
iter 14 loss: 0.583
Actual params: [2.0863, 1.3333]
-Original Grad: 0.032, -lr * Pred Grad: 0.022, New P: 2.108
-Original Grad: 0.390, -lr * Pred Grad: 0.022, New P: 1.355
iter 15 loss: 0.572
Actual params: [2.108, 1.355]
-Original Grad: 0.038, -lr * Pred Grad: 0.022, New P: 2.130
-Original Grad: 0.517, -lr * Pred Grad: 0.022, New P: 1.377
iter 16 loss: 0.561
Actual params: [2.1297, 1.3767]
-Original Grad: 0.041, -lr * Pred Grad: 0.022, New P: 2.151
-Original Grad: 0.659, -lr * Pred Grad: 0.022, New P: 1.398
iter 17 loss: 0.548
Actual params: [2.1514, 1.3984]
-Original Grad: 0.028, -lr * Pred Grad: 0.022, New P: 2.173
-Original Grad: 0.437, -lr * Pred Grad: 0.022, New P: 1.420
iter 18 loss: 0.535
Actual params: [2.1731, 1.4201]
-Original Grad: 0.027, -lr * Pred Grad: 0.022, New P: 2.195
-Original Grad: 0.485, -lr * Pred Grad: 0.022, New P: 1.442
iter 19 loss: 0.525
Actual params: [2.1949, 1.4418]
-Original Grad: 0.024, -lr * Pred Grad: 0.022, New P: 2.217
-Original Grad: 0.575, -lr * Pred Grad: 0.022, New P: 1.464
iter 20 loss: 0.512
Actual params: [2.2166, 1.4635]
