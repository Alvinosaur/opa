Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.412, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.105, -lr * Pred Grad:  0.100, New P: 0.694
iter 0 loss: 0.351
Actual params: [0.6941, 0.6941]
-Original Grad: 0.827, -lr * Pred Grad:  0.096, New P: 0.791
-Original Grad: -0.125, -lr * Pred Grad:  -0.014, New P: 0.680
iter 1 loss: 0.289
Actual params: [0.7905, 0.6805]
-Original Grad: -2.027, -lr * Pred Grad:  -0.027, New P: 0.763
-Original Grad: -0.698, -lr * Pred Grad:  -0.065, New P: 0.616
iter 2 loss: 0.140
Actual params: [0.7633, 0.6159]
-Original Grad: 1.526, -lr * Pred Grad:  0.014, New P: 0.778
-Original Grad: 0.651, -lr * Pred Grad:  -0.000, New P: 0.616
iter 3 loss: 0.229
Actual params: [0.7778, 0.6158]
-Original Grad: 1.506, -lr * Pred Grad:  0.037, New P: 0.815
-Original Grad: 0.672, -lr * Pred Grad:  0.031, New P: 0.647
iter 4 loss: 0.210
Actual params: [0.815 , 0.6469]
-Original Grad: 9.407, -lr * Pred Grad:  0.060, New P: 0.875
-Original Grad: 3.423, -lr * Pred Grad:  0.058, New P: 0.705
iter 5 loss: 0.106
Actual params: [0.8746, 0.705 ]
-Original Grad: -2.293, -lr * Pred Grad:  0.039, New P: 0.914
-Original Grad: -0.767, -lr * Pred Grad:  0.039, New P: 0.744
iter 6 loss: 0.133
Actual params: [0.9139, 0.7441]
-Original Grad: -0.512, -lr * Pred Grad:  0.032, New P: 0.946
-Original Grad: -0.041, -lr * Pred Grad:  0.034, New P: 0.778
iter 7 loss: 0.214
Actual params: [0.946, 0.778]
-Original Grad: -1.346, -lr * Pred Grad:  0.022, New P: 0.968
-Original Grad: 0.070, -lr * Pred Grad:  0.031, New P: 0.809
iter 8 loss: 0.262
Actual params: [0.9679, 0.8091]
-Original Grad: -1.018, -lr * Pred Grad:  0.015, New P: 0.982
-Original Grad: 0.212, -lr * Pred Grad:  0.030, New P: 0.839
iter 9 loss: 0.283
Actual params: [0.9824, 0.8395]
-Original Grad: -1.450, -lr * Pred Grad:  0.006, New P: 0.989
-Original Grad: 0.504, -lr * Pred Grad:  0.034, New P: 0.873
iter 10 loss: 0.290
Actual params: [0.9887, 0.873 ]
-Original Grad: 0.511, -lr * Pred Grad:  0.008, New P: 0.997
-Original Grad: -0.369, -lr * Pred Grad:  0.025, New P: 0.898
iter 11 loss: 0.282
Actual params: [0.9966, 0.8983]
-Original Grad: -0.965, -lr * Pred Grad:  0.003, New P: 0.999
-Original Grad: 0.511, -lr * Pred Grad:  0.029, New P: 0.927
iter 12 loss: 0.278
Actual params: [0.9993, 0.9274]
-Original Grad: -0.876, -lr * Pred Grad:  -0.002, New P: 0.998
-Original Grad: 0.494, -lr * Pred Grad:  0.032, New P: 0.960
iter 13 loss: 0.267
Actual params: [0.9977, 0.9597]
-Original Grad: -1.306, -lr * Pred Grad:  -0.007, New P: 0.990
-Original Grad: 0.830, -lr * Pred Grad:  0.039, New P: 0.999
iter 14 loss: 0.250
Actual params: [0.9902, 0.9986]
-Original Grad: -1.747, -lr * Pred Grad:  -0.015, New P: 0.976
-Original Grad: 0.962, -lr * Pred Grad:  0.046, New P: 1.045
iter 15 loss: 0.217
Actual params: [0.9755, 1.0446]
-Original Grad: -5.212, -lr * Pred Grad:  -0.034, New P: 0.942
-Original Grad: 3.210, -lr * Pred Grad:  0.063, New P: 1.108
iter 16 loss: 0.110
Actual params: [0.9419, 1.1079]
-Original Grad: -0.256, -lr * Pred Grad:  -0.032, New P: 0.910
-Original Grad: 0.044, -lr * Pred Grad:  0.058, New P: 1.166
iter 17 loss: 0.077
Actual params: [0.9103, 1.1659]
-Original Grad: 1.424, -lr * Pred Grad:  -0.023, New P: 0.888
-Original Grad: -0.425, -lr * Pred Grad:  0.048, New P: 1.214
iter 18 loss: 0.076
Actual params: [0.8878, 1.2142]
-Original Grad: 0.940, -lr * Pred Grad:  -0.016, New P: 0.871
-Original Grad: 0.277, -lr * Pred Grad:  0.047, New P: 1.261
iter 19 loss: 0.089
Actual params: [0.8714, 1.2609]
-Original Grad: 0.854, -lr * Pred Grad:  -0.011, New P: 0.860
-Original Grad: 0.824, -lr * Pred Grad:  0.050, New P: 1.311
iter 20 loss: 0.109
Actual params: [0.8601, 1.3109]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.474, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.580, -lr * Pred Grad:  -0.100, New P: 0.494
iter 0 loss: 0.187
Actual params: [0.6941, 0.4941]
-Original Grad: 0.039, -lr * Pred Grad:  0.073, New P: 0.767
-Original Grad: -0.071, -lr * Pred Grad:  -0.076, New P: 0.419
iter 1 loss: 0.121
Actual params: [0.767 , 0.4185]
-Original Grad: -0.199, -lr * Pred Grad:  0.027, New P: 0.794
-Original Grad: 0.142, -lr * Pred Grad:  -0.042, New P: 0.377
iter 2 loss: 0.133
Actual params: [0.7943, 0.3769]
-Original Grad: -0.368, -lr * Pred Grad:  -0.016, New P: 0.779
-Original Grad: 0.209, -lr * Pred Grad:  -0.013, New P: 0.364
iter 3 loss: 0.145
Actual params: [0.7786, 0.3638]
-Original Grad: -0.363, -lr * Pred Grad:  -0.039, New P: 0.740
-Original Grad: 0.201, -lr * Pred Grad:  0.006, New P: 0.370
iter 4 loss: 0.142
Actual params: [0.74  , 0.3697]
-Original Grad: -0.142, -lr * Pred Grad:  -0.043, New P: 0.697
-Original Grad: 0.042, -lr * Pred Grad:  0.008, New P: 0.378
iter 5 loss: 0.132
Actual params: [0.6973, 0.378 ]
-Original Grad: -0.225, -lr * Pred Grad:  -0.050, New P: 0.647
-Original Grad: 0.081, -lr * Pred Grad:  0.013, New P: 0.391
iter 6 loss: 0.122
Actual params: [0.647 , 0.3914]
-Original Grad: 0.496, -lr * Pred Grad:  -0.011, New P: 0.636
-Original Grad: -0.305, -lr * Pred Grad:  -0.010, New P: 0.382
iter 7 loss: 0.114
Actual params: [0.6363, 0.3817]
-Original Grad: 0.088, -lr * Pred Grad:  -0.005, New P: 0.631
-Original Grad: -0.096, -lr * Pred Grad:  -0.015, New P: 0.367
iter 8 loss: 0.116
Actual params: [0.6315, 0.3668]
-Original Grad: 1.747, -lr * Pred Grad:  0.041, New P: 0.672
-Original Grad: -0.922, -lr * Pred Grad:  -0.046, New P: 0.321
iter 9 loss: 0.115
Actual params: [0.6723, 0.3208]
-Original Grad: -0.230, -lr * Pred Grad:  0.031, New P: 0.703
-Original Grad: 0.056, -lr * Pred Grad:  -0.039, New P: 0.282
iter 10 loss: 0.120
Actual params: [0.7031, 0.2819]
-Original Grad: -0.128, -lr * Pred Grad:  0.025, New P: 0.728
-Original Grad: 0.018, -lr * Pred Grad:  -0.034, New P: 0.248
iter 11 loss: 0.130
Actual params: [0.7276, 0.2476]
-Original Grad: -0.428, -lr * Pred Grad:  0.011, New P: 0.739
-Original Grad: 0.064, -lr * Pred Grad:  -0.028, New P: 0.219
iter 12 loss: 0.140
Actual params: [0.7391, 0.2193]
-Original Grad: -0.411, -lr * Pred Grad:  0.001, New P: 0.740
-Original Grad: 0.047, -lr * Pred Grad:  -0.024, New P: 0.196
iter 13 loss: 0.146
Actual params: [0.7397, 0.1957]
-Original Grad: -0.411, -lr * Pred Grad:  -0.009, New P: 0.731
-Original Grad: 0.027, -lr * Pred Grad:  -0.020, New P: 0.175
iter 14 loss: 0.147
Actual params: [0.7308, 0.1755]
-Original Grad: -0.424, -lr * Pred Grad:  -0.018, New P: 0.713
-Original Grad: 0.022, -lr * Pred Grad:  -0.017, New P: 0.158
iter 15 loss: 0.144
Actual params: [0.7133, 0.158 ]
-Original Grad: -0.422, -lr * Pred Grad:  -0.025, New P: 0.688
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: 0.142
iter 16 loss: 0.137
Actual params: [0.6882, 0.1422]
-Original Grad: -0.240, -lr * Pred Grad:  -0.028, New P: 0.660
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: 0.128
iter 17 loss: 0.129
Actual params: [0.6602, 0.1278]
-Original Grad: -0.218, -lr * Pred Grad:  -0.030, New P: 0.630
-Original Grad: -0.004, -lr * Pred Grad:  -0.013, New P: 0.114
iter 18 loss: 0.122
Actual params: [0.6299, 0.1145]
-Original Grad: -0.179, -lr * Pred Grad:  -0.032, New P: 0.598
-Original Grad: 0.002, -lr * Pred Grad:  -0.012, New P: 0.102
iter 19 loss: 0.116
Actual params: [0.5983, 0.1025]
-Original Grad: 1.605, -lr * Pred Grad:  0.007, New P: 0.605
-Original Grad: 0.165, -lr * Pred Grad:  -0.004, New P: 0.099
iter 20 loss: 0.111
Actual params: [0.6051, 0.0987]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: -0.634, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -1.458, -lr * Pred Grad:  -0.100, New P: 0.494
iter 0 loss: 0.389
Actual params: [0.4941, 0.4941]
-Original Grad: -0.562, -lr * Pred Grad:  -0.100, New P: 0.395
-Original Grad: -0.766, -lr * Pred Grad:  -0.094, New P: 0.400
iter 1 loss: 0.245
Actual params: [0.3946, 0.4001]
-Original Grad: 0.217, -lr * Pred Grad:  -0.059, New P: 0.336
-Original Grad: 0.112, -lr * Pred Grad:  -0.068, New P: 0.332
iter 2 loss: 0.178
Actual params: [0.3359, 0.332 ]
-Original Grad: 0.217, -lr * Pred Grad:  -0.033, New P: 0.303
-Original Grad: 0.013, -lr * Pred Grad:  -0.055, New P: 0.277
iter 3 loss: 0.194
Actual params: [0.3033, 0.2766]
-Original Grad: 0.179, -lr * Pred Grad:  -0.016, New P: 0.287
-Original Grad: -0.015, -lr * Pred Grad:  -0.047, New P: 0.229
iter 4 loss: 0.201
Actual params: [0.2869, 0.2294]
-Original Grad: 0.173, -lr * Pred Grad:  -0.004, New P: 0.283
-Original Grad: -0.033, -lr * Pred Grad:  -0.042, New P: 0.188
iter 5 loss: 0.202
Actual params: [0.2827, 0.1876]
-Original Grad: 0.123, -lr * Pred Grad:  0.003, New P: 0.286
-Original Grad: -0.022, -lr * Pred Grad:  -0.037, New P: 0.150
iter 6 loss: 0.202
Actual params: [0.2857, 0.1505]
-Original Grad: 0.128, -lr * Pred Grad:  0.009, New P: 0.295
-Original Grad: -0.028, -lr * Pred Grad:  -0.034, New P: 0.117
iter 7 loss: 0.201
Actual params: [0.2951, 0.1169]
-Original Grad: 0.087, -lr * Pred Grad:  0.013, New P: 0.308
-Original Grad: -0.028, -lr * Pred Grad:  -0.031, New P: 0.086
iter 8 loss: 0.198
Actual params: [0.3077, 0.0862]
-Original Grad: 0.071, -lr * Pred Grad:  0.015, New P: 0.323
-Original Grad: -0.029, -lr * Pred Grad:  -0.028, New P: 0.058
iter 9 loss: 0.197
Actual params: [0.3227, 0.058 ]
-Original Grad: 0.024, -lr * Pred Grad:  0.015, New P: 0.337
-Original Grad: -0.007, -lr * Pred Grad:  -0.025, New P: 0.033
iter 10 loss: 0.195
Actual params: [0.3373, 0.0326]
-Original Grad: 0.010, -lr * Pred Grad:  0.014, New P: 0.351
-Original Grad: 0.008, -lr * Pred Grad:  -0.023, New P: 0.010
iter 11 loss: 0.195
Actual params: [0.3509, 0.0099]
-Original Grad: -0.035, -lr * Pred Grad:  0.010, New P: 0.361
-Original Grad: 0.017, -lr * Pred Grad:  -0.020, New P: -0.010
iter 12 loss: 0.196
Actual params: [ 0.3614, -0.0101]
-Original Grad: -0.082, -lr * Pred Grad:  0.005, New P: 0.367
-Original Grad: 0.013, -lr * Pred Grad:  -0.018, New P: -0.028
iter 13 loss: 0.197
Actual params: [ 0.3667, -0.0277]
-Original Grad: -0.058, -lr * Pred Grad:  0.002, New P: 0.369
-Original Grad: -0.027, -lr * Pred Grad:  -0.017, New P: -0.044
iter 14 loss: 0.197
Actual params: [ 0.3685, -0.0444]
-Original Grad: -0.043, -lr * Pred Grad:  -0.001, New P: 0.368
-Original Grad: -0.043, -lr * Pred Grad:  -0.016, New P: -0.061
iter 15 loss: 0.197
Actual params: [ 0.368 , -0.0609]
-Original Grad: -0.036, -lr * Pred Grad:  -0.002, New P: 0.366
-Original Grad: -0.054, -lr * Pred Grad:  -0.017, New P: -0.077
iter 16 loss: 0.196
Actual params: [ 0.3657, -0.0774]
-Original Grad: -0.027, -lr * Pred Grad:  -0.003, New P: 0.362
-Original Grad: -0.054, -lr * Pred Grad:  -0.017, New P: -0.094
iter 17 loss: 0.195
Actual params: [ 0.3622, -0.094 ]
-Original Grad: -0.017, -lr * Pred Grad:  -0.004, New P: 0.358
-Original Grad: -0.052, -lr * Pred Grad:  -0.017, New P: -0.111
iter 18 loss: 0.194
Actual params: [ 0.3582, -0.1107]
-Original Grad: -0.008, -lr * Pred Grad:  -0.004, New P: 0.354
-Original Grad: -0.045, -lr * Pred Grad:  -0.017, New P: -0.127
iter 19 loss: 0.193
Actual params: [ 0.3541, -0.1273]
-Original Grad: 0.015, -lr * Pred Grad:  -0.003, New P: 0.351
-Original Grad: -0.052, -lr * Pred Grad:  -0.017, New P: -0.144
iter 20 loss: 0.192
Actual params: [ 0.3511, -0.1439]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.185, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.062, -lr * Pred Grad:  -0.100, New P: 0.494
iter 0 loss: 0.726
Actual params: [0.6941, 0.4941]
-Original Grad: 0.350, -lr * Pred Grad:  0.097, New P: 0.791
-Original Grad: -0.222, -lr * Pred Grad:  -0.090, New P: 0.404
iter 1 loss: 0.687
Actual params: [0.7912, 0.4044]
-Original Grad: 1.000, -lr * Pred Grad:  0.087, New P: 0.878
-Original Grad: -0.525, -lr * Pred Grad:  -0.086, New P: 0.318
iter 2 loss: 0.554
Actual params: [0.8782, 0.3181]
-Original Grad: 0.526, -lr * Pred Grad:  0.090, New P: 0.968
-Original Grad: -0.157, -lr * Pred Grad:  -0.084, New P: 0.235
iter 3 loss: 0.339
Actual params: [0.9677, 0.2346]
-Original Grad: 0.034, -lr * Pred Grad:  0.077, New P: 1.045
-Original Grad: 0.114, -lr * Pred Grad:  -0.059, New P: 0.176
iter 4 loss: 0.173
Actual params: [1.0449, 0.1755]
-Original Grad: 0.329, -lr * Pred Grad:  0.078, New P: 1.123
-Original Grad: -0.109, -lr * Pred Grad:  -0.059, New P: 0.116
iter 5 loss: 0.127
Actual params: [1.1229, 0.1162]
-Original Grad: -24.237, -lr * Pred Grad:  -0.047, New P: 1.076
-Original Grad: -10.412, -lr * Pred Grad:  -0.054, New P: 0.063
iter 6 loss: 0.113
Actual params: [1.0758, 0.0626]
-Original Grad: 0.135, -lr * Pred Grad:  -0.041, New P: 1.035
-Original Grad: 0.012, -lr * Pred Grad:  -0.047, New P: 0.015
iter 7 loss: 0.116
Actual params: [1.0346, 0.0154]
-Original Grad: 0.290, -lr * Pred Grad:  -0.036, New P: 0.999
-Original Grad: 0.015, -lr * Pred Grad:  -0.042, New P: -0.026
iter 8 loss: 0.126
Actual params: [ 0.9986, -0.0265]
-Original Grad: 0.419, -lr * Pred Grad:  -0.031, New P: 0.967
-Original Grad: 0.011, -lr * Pred Grad:  -0.037, New P: -0.064
iter 9 loss: 0.139
Actual params: [ 0.9673, -0.0637]
-Original Grad: 0.638, -lr * Pred Grad:  -0.027, New P: 0.940
-Original Grad: -0.020, -lr * Pred Grad:  -0.033, New P: -0.097
iter 10 loss: 0.154
Actual params: [ 0.9405, -0.0972]
-Original Grad: 0.494, -lr * Pred Grad:  -0.023, New P: 0.917
-Original Grad: 0.044, -lr * Pred Grad:  -0.030, New P: -0.127
iter 11 loss: 0.170
Actual params: [ 0.9174, -0.1271]
-Original Grad: 0.790, -lr * Pred Grad:  -0.019, New P: 0.898
-Original Grad: 0.089, -lr * Pred Grad:  -0.027, New P: -0.154
iter 12 loss: 0.189
Actual params: [ 0.8982, -0.1537]
-Original Grad: 1.063, -lr * Pred Grad:  -0.015, New P: 0.883
-Original Grad: 0.127, -lr * Pred Grad:  -0.023, New P: -0.177
iter 13 loss: 0.210
Actual params: [ 0.8829, -0.1771]
-Original Grad: 1.344, -lr * Pred Grad:  -0.011, New P: 0.872
-Original Grad: 0.135, -lr * Pred Grad:  -0.021, New P: -0.198
iter 14 loss: 0.231
Actual params: [ 0.8719, -0.1976]
-Original Grad: 3.966, -lr * Pred Grad:  -0.002, New P: 0.870
-Original Grad: 0.294, -lr * Pred Grad:  -0.017, New P: -0.215
iter 15 loss: 0.247
Actual params: [ 0.8699, -0.2149]
-Original Grad: 1.551, -lr * Pred Grad:  0.001, New P: 0.871
-Original Grad: 0.139, -lr * Pred Grad:  -0.015, New P: -0.230
iter 16 loss: 0.255
Actual params: [ 0.8711, -0.2298]
-Original Grad: 1.569, -lr * Pred Grad:  0.004, New P: 0.875
-Original Grad: 0.140, -lr * Pred Grad:  -0.013, New P: -0.243
iter 17 loss: 0.255
Actual params: [ 0.8755, -0.2428]
-Original Grad: 45.571, -lr * Pred Grad:  0.046, New P: 0.921
-Original Grad: 3.116, -lr * Pred Grad:  0.003, New P: -0.240
iter 18 loss: 0.246
Actual params: [ 0.9215, -0.2396]
-Original Grad: 0.617, -lr * Pred Grad:  0.042, New P: 0.964
-Original Grad: 0.098, -lr * Pred Grad:  0.003, New P: -0.236
iter 19 loss: 0.198
Actual params: [ 0.9639, -0.2362]
-Original Grad: 0.473, -lr * Pred Grad:  0.039, New P: 1.003
-Original Grad: 0.100, -lr * Pred Grad:  0.004, New P: -0.233
iter 20 loss: 0.169
Actual params: [ 1.003 , -0.2327]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.291, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.145, -lr * Pred Grad:  -0.100, New P: 0.494
iter 0 loss: 0.627
Actual params: [0.6941, 0.4941]
-Original Grad: 0.484, -lr * Pred Grad:  0.098, New P: 0.792
-Original Grad: -0.205, -lr * Pred Grad:  -0.099, New P: 0.395
iter 1 loss: 0.574
Actual params: [0.7923, 0.3946]
-Original Grad: 0.653, -lr * Pred Grad:  0.098, New P: 0.890
-Original Grad: -0.140, -lr * Pred Grad:  -0.098, New P: 0.296
iter 2 loss: 0.485
Actual params: [0.8904, 0.2963]
-Original Grad: 0.775, -lr * Pred Grad:  0.099, New P: 0.989
-Original Grad: -0.117, -lr * Pred Grad:  -0.096, New P: 0.200
iter 3 loss: 0.394
Actual params: [0.9889, 0.1999]
-Original Grad: 0.567, -lr * Pred Grad:  0.099, New P: 1.088
-Original Grad: -0.195, -lr * Pred Grad:  -0.098, New P: 0.102
iter 4 loss: 0.317
Actual params: [1.0877, 0.1018]
-Original Grad: -0.171, -lr * Pred Grad:  0.078, New P: 1.165
-Original Grad: -0.109, -lr * Pred Grad:  -0.096, New P: 0.006
iter 5 loss: 0.287
Actual params: [1.1653, 0.0059]
-Original Grad: -0.392, -lr * Pred Grad:  0.050, New P: 1.215
-Original Grad: -0.107, -lr * Pred Grad:  -0.094, New P: -0.088
iter 6 loss: 0.298
Actual params: [ 1.2155, -0.0883]
-Original Grad: -0.406, -lr * Pred Grad:  0.028, New P: 1.244
-Original Grad: -0.011, -lr * Pred Grad:  -0.084, New P: -0.173
iter 7 loss: 0.312
Actual params: [ 1.2437, -0.1728]
-Original Grad: -0.314, -lr * Pred Grad:  0.014, New P: 1.258
-Original Grad: 0.012, -lr * Pred Grad:  -0.074, New P: -0.246
iter 8 loss: 0.322
Actual params: [ 1.2576, -0.2463]
-Original Grad: 2.017, -lr * Pred Grad:  0.047, New P: 1.304
-Original Grad: 0.057, -lr * Pred Grad:  -0.058, New P: -0.304
iter 9 loss: 0.326
Actual params: [ 1.3041, -0.3044]
-Original Grad: -0.376, -lr * Pred Grad:  0.034, New P: 1.338
-Original Grad: 0.038, -lr * Pred Grad:  -0.047, New P: -0.352
iter 10 loss: 0.347
Actual params: [ 1.3382, -0.3516]
-Original Grad: -0.193, -lr * Pred Grad:  0.027, New P: 1.365
-Original Grad: 0.048, -lr * Pred Grad:  -0.037, New P: -0.388
iter 11 loss: 0.360
Actual params: [ 1.365 , -0.3881]
-Original Grad: -0.313, -lr * Pred Grad:  0.018, New P: 1.383
-Original Grad: 0.041, -lr * Pred Grad:  -0.028, New P: -0.416
iter 12 loss: 0.367
Actual params: [ 1.383, -0.416]
-Original Grad: -0.298, -lr * Pred Grad:  0.011, New P: 1.394
-Original Grad: 0.044, -lr * Pred Grad:  -0.020, New P: -0.436
iter 13 loss: 0.373
Actual params: [ 1.3936, -0.4359]
-Original Grad: -0.292, -lr * Pred Grad:  0.004, New P: 1.398
-Original Grad: 0.041, -lr * Pred Grad:  -0.013, New P: -0.449
iter 14 loss: 0.377
Actual params: [ 1.3976, -0.4489]
-Original Grad: -0.283, -lr * Pred Grad:  -0.002, New P: 1.396
-Original Grad: 0.038, -lr * Pred Grad:  -0.007, New P: -0.456
iter 15 loss: 0.379
Actual params: [ 1.3958, -0.4562]
-Original Grad: -0.287, -lr * Pred Grad:  -0.007, New P: 1.389
-Original Grad: 0.037, -lr * Pred Grad:  -0.002, New P: -0.458
iter 16 loss: 0.379
Actual params: [ 1.3887, -0.4583]
-Original Grad: -0.299, -lr * Pred Grad:  -0.012, New P: 1.377
-Original Grad: 0.036, -lr * Pred Grad:  0.002, New P: -0.456
iter 17 loss: 0.377
Actual params: [ 1.3766, -0.4559]
-Original Grad: -0.312, -lr * Pred Grad:  -0.017, New P: 1.360
-Original Grad: 0.037, -lr * Pred Grad:  0.007, New P: -0.449
iter 18 loss: 0.373
Actual params: [ 1.3598, -0.4492]
-Original Grad: -0.513, -lr * Pred Grad:  -0.025, New P: 1.335
-Original Grad: 0.023, -lr * Pred Grad:  0.009, New P: -0.440
iter 19 loss: 0.367
Actual params: [ 1.335 , -0.4404]
-Original Grad: -0.263, -lr * Pred Grad:  -0.027, New P: 1.308
-Original Grad: 0.046, -lr * Pred Grad:  0.014, New P: -0.427
iter 20 loss: 0.363
Actual params: [ 1.3076, -0.4268]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.091, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.323, -lr * Pred Grad:  -0.100, New P: 0.494
iter 0 loss: 0.503
Actual params: [0.6941, 0.4941]
-Original Grad: 0.510, -lr * Pred Grad:  0.085, New P: 0.779
-Original Grad: -0.182, -lr * Pred Grad:  -0.095, New P: 0.399
iter 1 loss: 0.455
Actual params: [0.7791, 0.3992]
-Original Grad: 0.872, -lr * Pred Grad:  0.088, New P: 0.868
-Original Grad: -0.199, -lr * Pred Grad:  -0.095, New P: 0.304
iter 2 loss: 0.376
Actual params: [0.8676, 0.3043]
-Original Grad: 1.095, -lr * Pred Grad:  0.092, New P: 0.959
-Original Grad: -0.106, -lr * Pred Grad:  -0.090, New P: 0.215
iter 3 loss: 0.266
Actual params: [0.9595, 0.2148]
-Original Grad: 0.585, -lr * Pred Grad:  0.092, New P: 1.052
-Original Grad: -0.056, -lr * Pred Grad:  -0.082, New P: 0.133
iter 4 loss: 0.185
Actual params: [1.0517, 0.1328]
-Original Grad: 0.199, -lr * Pred Grad:  0.085, New P: 1.137
-Original Grad: -0.045, -lr * Pred Grad:  -0.076, New P: 0.057
iter 5 loss: 0.147
Actual params: [1.137 , 0.0572]
-Original Grad: -0.071, -lr * Pred Grad:  0.072, New P: 1.209
-Original Grad: -0.009, -lr * Pred Grad:  -0.067, New P: -0.010
iter 6 loss: 0.143
Actual params: [ 1.2092, -0.0098]
-Original Grad: -0.113, -lr * Pred Grad:  0.060, New P: 1.269
-Original Grad: -0.008, -lr * Pred Grad:  -0.060, New P: -0.070
iter 7 loss: 0.151
Actual params: [ 1.2692, -0.0699]
-Original Grad: -0.272, -lr * Pred Grad:  0.044, New P: 1.314
-Original Grad: -0.002, -lr * Pred Grad:  -0.054, New P: -0.123
iter 8 loss: 0.157
Actual params: [ 1.3137, -0.1234]
-Original Grad: -0.373, -lr * Pred Grad:  0.028, New P: 1.342
-Original Grad: -0.008, -lr * Pred Grad:  -0.049, New P: -0.172
iter 9 loss: 0.171
Actual params: [ 1.3416, -0.172 ]
-Original Grad: -0.411, -lr * Pred Grad:  0.013, New P: 1.354
-Original Grad: 0.008, -lr * Pred Grad:  -0.043, New P: -0.215
iter 10 loss: 0.181
Actual params: [ 1.3545, -0.2147]
-Original Grad: -0.443, -lr * Pred Grad:  -0.001, New P: 1.354
-Original Grad: 0.028, -lr * Pred Grad:  -0.035, New P: -0.250
iter 11 loss: 0.187
Actual params: [ 1.3537, -0.2499]
-Original Grad: -0.409, -lr * Pred Grad:  -0.011, New P: 1.342
-Original Grad: 0.022, -lr * Pred Grad:  -0.029, New P: -0.279
iter 12 loss: 0.187
Actual params: [ 1.3423, -0.2793]
-Original Grad: -0.359, -lr * Pred Grad:  -0.019, New P: 1.323
-Original Grad: 0.016, -lr * Pred Grad:  -0.025, New P: -0.304
iter 13 loss: 0.183
Actual params: [ 1.3228, -0.304 ]
-Original Grad: -0.317, -lr * Pred Grad:  -0.026, New P: 1.297
-Original Grad: 0.027, -lr * Pred Grad:  -0.019, New P: -0.323
iter 14 loss: 0.177
Actual params: [ 1.2973, -0.3234]
-Original Grad: -0.244, -lr * Pred Grad:  -0.029, New P: 1.268
-Original Grad: 0.051, -lr * Pred Grad:  -0.012, New P: -0.335
iter 15 loss: 0.171
Actual params: [ 1.2681, -0.3352]
-Original Grad: -0.149, -lr * Pred Grad:  -0.030, New P: 1.238
-Original Grad: 0.035, -lr * Pred Grad:  -0.007, New P: -0.342
iter 16 loss: 0.165
Actual params: [ 1.2379, -0.342 ]
-Original Grad: -0.086, -lr * Pred Grad:  -0.030, New P: 1.208
-Original Grad: 0.035, -lr * Pred Grad:  -0.002, New P: -0.344
iter 17 loss: 0.159
Actual params: [ 1.2082, -0.3442]
-Original Grad: -0.023, -lr * Pred Grad:  -0.028, New P: 1.181
-Original Grad: 0.033, -lr * Pred Grad:  0.002, New P: -0.342
iter 18 loss: 0.157
Actual params: [ 1.1806, -0.3425]
-Original Grad: -0.017, -lr * Pred Grad:  -0.026, New P: 1.155
-Original Grad: 0.032, -lr * Pred Grad:  0.005, New P: -0.337
iter 19 loss: 0.156
Actual params: [ 1.155 , -0.3373]
-Original Grad: -0.114, -lr * Pred Grad:  -0.026, New P: 1.129
-Original Grad: 0.069, -lr * Pred Grad:  0.013, New P: -0.325
iter 20 loss: 0.154
Actual params: [ 1.1288, -0.3248]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.193, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.171, -lr * Pred Grad:  -0.100, New P: 0.494
iter 0 loss: 0.286
Actual params: [0.6941, 0.4941]
-Original Grad: 0.213, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.092, -lr * Pred Grad:  -0.094, New P: 0.400
iter 1 loss: 0.251
Actual params: [0.7942, 0.3998]
-Original Grad: 0.195, -lr * Pred Grad:  0.100, New P: 0.894
-Original Grad: -0.010, -lr * Pred Grad:  -0.076, New P: 0.324
iter 2 loss: 0.228
Actual params: [0.8941, 0.3237]
-Original Grad: 0.058, -lr * Pred Grad:  0.090, New P: 0.984
-Original Grad: 0.029, -lr * Pred Grad:  -0.053, New P: 0.271
iter 3 loss: 0.216
Actual params: [0.9843, 0.2707]
-Original Grad: -0.009, -lr * Pred Grad:  0.075, New P: 1.059
-Original Grad: 0.061, -lr * Pred Grad:  -0.026, New P: 0.244
iter 4 loss: 0.216
Actual params: [1.0592, 0.2443]
-Original Grad: 0.000, -lr * Pred Grad:  0.065, New P: 1.124
-Original Grad: 0.077, -lr * Pred Grad:  -0.003, New P: 0.241
iter 5 loss: 0.219
Actual params: [1.1238, 0.2411]
-Original Grad: -0.012, -lr * Pred Grad:  0.055, New P: 1.178
-Original Grad: 0.075, -lr * Pred Grad:  0.014, New P: 0.255
iter 6 loss: 0.219
Actual params: [1.1784, 0.2549]
-Original Grad: -0.020, -lr * Pred Grad:  0.045, New P: 1.224
-Original Grad: 0.069, -lr * Pred Grad:  0.026, New P: 0.281
iter 7 loss: 0.219
Actual params: [1.2236, 0.2806]
-Original Grad: -0.028, -lr * Pred Grad:  0.036, New P: 1.260
-Original Grad: 0.053, -lr * Pred Grad:  0.033, New P: 0.313
iter 8 loss: 0.218
Actual params: [1.2597, 0.3134]
-Original Grad: -0.028, -lr * Pred Grad:  0.028, New P: 1.288
-Original Grad: 0.029, -lr * Pred Grad:  0.035, New P: 0.348
iter 9 loss: 0.218
Actual params: [1.288 , 0.3481]
-Original Grad: -0.028, -lr * Pred Grad:  0.022, New P: 1.310
-Original Grad: 0.007, -lr * Pred Grad:  0.032, New P: 0.381
iter 10 loss: 0.218
Actual params: [1.3096, 0.3806]
-Original Grad: -0.045, -lr * Pred Grad:  0.013, New P: 1.323
-Original Grad: -0.013, -lr * Pred Grad:  0.027, New P: 0.407
iter 11 loss: 0.219
Actual params: [1.3227, 0.4071]
-Original Grad: -0.045, -lr * Pred Grad:  0.006, New P: 1.328
-Original Grad: -0.033, -lr * Pred Grad:  0.017, New P: 0.424
iter 12 loss: 0.220
Actual params: [1.3285, 0.4244]
-Original Grad: -0.043, -lr * Pred Grad:  -0.001, New P: 1.328
-Original Grad: -0.006, -lr * Pred Grad:  0.015, New P: 0.439
iter 13 loss: 0.221
Actual params: [1.3279, 0.439 ]
-Original Grad: -0.043, -lr * Pred Grad:  -0.006, New P: 1.322
-Original Grad: -0.003, -lr * Pred Grad:  0.012, New P: 0.451
iter 14 loss: 0.221
Actual params: [1.3217, 0.4515]
-Original Grad: -0.042, -lr * Pred Grad:  -0.011, New P: 1.310
-Original Grad: 0.005, -lr * Pred Grad:  0.012, New P: 0.464
iter 15 loss: 0.221
Actual params: [1.3104, 0.4637]
-Original Grad: -0.045, -lr * Pred Grad:  -0.016, New P: 1.294
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.475
iter 16 loss: 0.220
Actual params: [1.2943, 0.475 ]
-Original Grad: -0.035, -lr * Pred Grad:  -0.019, New P: 1.275
-Original Grad: -0.013, -lr * Pred Grad:  0.008, New P: 0.483
iter 17 loss: 0.220
Actual params: [1.2751, 0.4827]
-Original Grad: -0.035, -lr * Pred Grad:  -0.022, New P: 1.253
-Original Grad: -0.019, -lr * Pred Grad:  0.003, New P: 0.486
iter 18 loss: 0.219
Actual params: [1.2529, 0.486 ]
-Original Grad: -0.029, -lr * Pred Grad:  -0.024, New P: 1.229
-Original Grad: -0.016, -lr * Pred Grad:  -0.000, New P: 0.486
iter 19 loss: 0.218
Actual params: [1.2289, 0.4857]
-Original Grad: -0.034, -lr * Pred Grad:  -0.026, New P: 1.202
-Original Grad: -0.015, -lr * Pred Grad:  -0.003, New P: 0.482
iter 20 loss: 0.218
Actual params: [1.2024, 0.4822]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.249, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.156, -lr * Pred Grad:  0.100, New P: 0.694
iter 0 loss: 0.331
Actual params: [0.6941, 0.6941]
-Original Grad: 0.321, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: 0.038, -lr * Pred Grad:  0.083, New P: 0.777
iter 1 loss: 0.281
Actual params: [0.7939, 0.7769]
-Original Grad: -1.646, -lr * Pred Grad:  -0.044, New P: 0.750
-Original Grad: -2.962, -lr * Pred Grad:  -0.060, New P: 0.717
iter 2 loss: 0.366
Actual params: [0.7504, 0.7166]
-Original Grad: 0.157, -lr * Pred Grad:  -0.030, New P: 0.720
-Original Grad: -0.375, -lr * Pred Grad:  -0.056, New P: 0.660
iter 3 loss: 0.270
Actual params: [0.7203, 0.6603]
-Original Grad: 0.458, -lr * Pred Grad:  -0.010, New P: 0.710
-Original Grad: 0.256, -lr * Pred Grad:  -0.043, New P: 0.618
iter 4 loss: 0.278
Actual params: [0.7099, 0.6175]
-Original Grad: 0.493, -lr * Pred Grad:  0.005, New P: 0.715
-Original Grad: 0.489, -lr * Pred Grad:  -0.028, New P: 0.590
iter 5 loss: 0.291
Actual params: [0.7154, 0.5895]
-Original Grad: 0.441, -lr * Pred Grad:  0.017, New P: 0.732
-Original Grad: 0.128, -lr * Pred Grad:  -0.022, New P: 0.567
iter 6 loss: 0.292
Actual params: [0.7319, 0.5673]
-Original Grad: 0.589, -lr * Pred Grad:  0.029, New P: 0.761
-Original Grad: 0.209, -lr * Pred Grad:  -0.016, New P: 0.551
iter 7 loss: 0.285
Actual params: [0.7606, 0.5511]
-Original Grad: 0.692, -lr * Pred Grad:  0.040, New P: 0.801
-Original Grad: 0.183, -lr * Pred Grad:  -0.011, New P: 0.540
iter 8 loss: 0.270
Actual params: [0.8008, 0.5397]
-Original Grad: 0.793, -lr * Pred Grad:  0.051, New P: 0.852
-Original Grad: 0.233, -lr * Pred Grad:  -0.006, New P: 0.533
iter 9 loss: 0.247
Actual params: [0.8517, 0.5333]
-Original Grad: 1.335, -lr * Pred Grad:  0.064, New P: 0.915
-Original Grad: 0.503, -lr * Pred Grad:  0.002, New P: 0.535
iter 10 loss: 0.209
Actual params: [0.9155, 0.5354]
-Original Grad: 1.494, -lr * Pred Grad:  0.074, New P: 0.989
-Original Grad: 0.642, -lr * Pred Grad:  0.012, New P: 0.547
iter 11 loss: 0.154
Actual params: [0.9892, 0.5471]
-Original Grad: 0.565, -lr * Pred Grad:  0.074, New P: 1.064
-Original Grad: 0.079, -lr * Pred Grad:  0.012, New P: 0.559
iter 12 loss: 0.095
Actual params: [1.0635, 0.5588]
-Original Grad: 0.094, -lr * Pred Grad:  0.069, New P: 1.132
-Original Grad: 0.365, -lr * Pred Grad:  0.016, New P: 0.575
iter 13 loss: 0.069
Actual params: [1.1321, 0.5748]
-Original Grad: -0.042, -lr * Pred Grad:  0.061, New P: 1.193
-Original Grad: 0.683, -lr * Pred Grad:  0.024, New P: 0.599
iter 14 loss: 0.062
Actual params: [1.1934, 0.5993]
-Original Grad: -0.173, -lr * Pred Grad:  0.053, New P: 1.246
-Original Grad: 0.406, -lr * Pred Grad:  0.028, New P: 0.627
iter 15 loss: 0.058
Actual params: [1.2462, 0.6273]
-Original Grad: -0.295, -lr * Pred Grad:  0.043, New P: 1.289
-Original Grad: 0.715, -lr * Pred Grad:  0.035, New P: 0.663
iter 16 loss: 0.055
Actual params: [1.289 , 0.6627]
-Original Grad: -0.209, -lr * Pred Grad:  0.035, New P: 1.325
-Original Grad: 0.315, -lr * Pred Grad:  0.037, New P: 0.699
iter 17 loss: 0.050
Actual params: [1.3245, 0.6993]
-Original Grad: -0.087, -lr * Pred Grad:  0.031, New P: 1.355
-Original Grad: -0.230, -lr * Pred Grad:  0.030, New P: 0.729
iter 18 loss: 0.054
Actual params: [1.3553, 0.7292]
-Original Grad: 0.040, -lr * Pred Grad:  0.029, New P: 1.384
-Original Grad: -0.685, -lr * Pred Grad:  0.016, New P: 0.746
iter 19 loss: 0.068
Actual params: [1.384 , 0.7456]
-Original Grad: 0.011, -lr * Pred Grad:  0.026, New P: 1.410
-Original Grad: -0.890, -lr * Pred Grad:  0.002, New P: 0.747
iter 20 loss: 0.080
Actual params: [1.4103, 0.7473]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: -0.146, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: 0.040, -lr * Pred Grad:  0.100, New P: 0.694
iter 0 loss: 0.328
Actual params: [0.4941, 0.6941]
-Original Grad: 0.065, -lr * Pred Grad:  -0.031, New P: 0.463
-Original Grad: -0.065, -lr * Pred Grad:  -0.028, New P: 0.666
iter 1 loss: 0.363
Actual params: [0.4632, 0.6658]
-Original Grad: -0.007, -lr * Pred Grad:  -0.027, New P: 0.436
-Original Grad: -0.012, -lr * Pred Grad:  -0.032, New P: 0.634
iter 2 loss: 0.362
Actual params: [0.4364, 0.634 ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.023, New P: 0.414
-Original Grad: 0.005, -lr * Pred Grad:  -0.022, New P: 0.612
iter 3 loss: 0.362
Actual params: [0.4135, 0.6116]
-Original Grad: -0.007, -lr * Pred Grad:  -0.022, New P: 0.392
-Original Grad: 0.004, -lr * Pred Grad:  -0.016, New P: 0.595
iter 4 loss: 0.362
Actual params: [0.3917, 0.5952]
-Original Grad: -0.012, -lr * Pred Grad:  -0.023, New P: 0.369
-Original Grad: 0.008, -lr * Pred Grad:  -0.008, New P: 0.587
iter 5 loss: 0.362
Actual params: [0.3692, 0.5868]
-Original Grad: 0.013, -lr * Pred Grad:  -0.016, New P: 0.354
-Original Grad: 0.002, -lr * Pred Grad:  -0.006, New P: 0.581
iter 6 loss: 0.362
Actual params: [0.3536, 0.5808]
-Original Grad: 0.055, -lr * Pred Grad:  0.003, New P: 0.357
-Original Grad: 0.004, -lr * Pred Grad:  -0.003, New P: 0.578
iter 7 loss: 0.362
Actual params: [0.3567, 0.5781]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.359
-Original Grad: 0.013, -lr * Pred Grad:  0.006, New P: 0.584
iter 8 loss: 0.362
Actual params: [0.3594, 0.5837]
-Original Grad: 0.027, -lr * Pred Grad:  0.010, New P: 0.370
-Original Grad: 0.005, -lr * Pred Grad:  0.008, New P: 0.592
iter 9 loss: 0.362
Actual params: [0.3696, 0.5919]
-Original Grad: 0.004, -lr * Pred Grad:  0.010, New P: 0.380
-Original Grad: 0.002, -lr * Pred Grad:  0.008, New P: 0.600
iter 10 loss: 0.362
Actual params: [0.3799, 0.6004]
-Original Grad: 0.021, -lr * Pred Grad:  0.015, New P: 0.395
-Original Grad: -0.006, -lr * Pred Grad:  0.004, New P: 0.604
iter 11 loss: 0.362
Actual params: [0.3951, 0.6041]
-Original Grad: -0.013, -lr * Pred Grad:  0.010, New P: 0.405
-Original Grad: 0.007, -lr * Pred Grad:  0.007, New P: 0.611
iter 12 loss: 0.362
Actual params: [0.405 , 0.6115]
-Original Grad: -0.023, -lr * Pred Grad:  0.003, New P: 0.408
-Original Grad: 0.010, -lr * Pred Grad:  0.013, New P: 0.624
iter 13 loss: 0.362
Actual params: [0.4077, 0.6243]
-Original Grad: -0.022, -lr * Pred Grad:  -0.004, New P: 0.404
-Original Grad: 0.006, -lr * Pred Grad:  0.015, New P: 0.640
iter 14 loss: 0.362
Actual params: [0.404 , 0.6395]
-Original Grad: -0.005, -lr * Pred Grad:  -0.005, New P: 0.399
-Original Grad: 0.002, -lr * Pred Grad:  0.015, New P: 0.654
iter 15 loss: 0.361
Actual params: [0.3993, 0.6543]
-Original Grad: 0.036, -lr * Pred Grad:  0.006, New P: 0.405
-Original Grad: -0.018, -lr * Pred Grad:  0.002, New P: 0.656
iter 16 loss: 0.362
Actual params: [0.4051, 0.6564]
-Original Grad: 0.022, -lr * Pred Grad:  0.011, New P: 0.416
-Original Grad: -0.013, -lr * Pred Grad:  -0.006, New P: 0.650
iter 17 loss: 0.362
Actual params: [0.4164, 0.6505]
-Original Grad: -0.039, -lr * Pred Grad:  -0.000, New P: 0.416
-Original Grad: 0.014, -lr * Pred Grad:  0.003, New P: 0.654
iter 18 loss: 0.362
Actual params: [0.416 , 0.6536]
-Original Grad: -0.044, -lr * Pred Grad:  -0.012, New P: 0.404
-Original Grad: 0.017, -lr * Pred Grad:  0.013, New P: 0.666
iter 19 loss: 0.362
Actual params: [0.404 , 0.6663]
-Original Grad: 0.033, -lr * Pred Grad:  -0.002, New P: 0.402
-Original Grad: -0.020, -lr * Pred Grad:  -0.000, New P: 0.666
iter 20 loss: 0.362
Actual params: [0.4018, 0.666 ]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.507, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -2.357, -lr * Pred Grad:  -0.100, New P: 0.494
iter 0 loss: 0.494
Actual params: [0.6941, 0.4941]
-Original Grad: 0.294, -lr * Pred Grad:  0.095, New P: 0.789
-Original Grad: -1.127, -lr * Pred Grad:  -0.093, New P: 0.401
iter 1 loss: 0.201
Actual params: [0.7894, 0.4015]
-Original Grad: 0.154, -lr * Pred Grad:  0.087, New P: 0.877
-Original Grad: -0.053, -lr * Pred Grad:  -0.073, New P: 0.329
iter 2 loss: 0.140
Actual params: [0.8769, 0.3287]
-Original Grad: 0.080, -lr * Pred Grad:  0.079, New P: 0.956
-Original Grad: 0.000, -lr * Pred Grad:  -0.060, New P: 0.269
iter 3 loss: 0.128
Actual params: [0.9555, 0.269 ]
-Original Grad: 0.137, -lr * Pred Grad:  0.077, New P: 1.032
-Original Grad: 0.025, -lr * Pred Grad:  -0.050, New P: 0.219
iter 4 loss: 0.121
Actual params: [1.0323, 0.2191]
-Original Grad: 0.154, -lr * Pred Grad:  0.077, New P: 1.109
-Original Grad: 0.053, -lr * Pred Grad:  -0.042, New P: 0.177
iter 5 loss: 0.112
Actual params: [1.109 , 0.1772]
-Original Grad: 0.130, -lr * Pred Grad:  0.076, New P: 1.185
-Original Grad: 0.080, -lr * Pred Grad:  -0.035, New P: 0.142
iter 6 loss: 0.103
Actual params: [1.1847, 0.1422]
-Original Grad: 0.133, -lr * Pred Grad:  0.075, New P: 1.260
-Original Grad: 0.105, -lr * Pred Grad:  -0.029, New P: 0.113
iter 7 loss: 0.097
Actual params: [1.26  , 0.1133]
-Original Grad: 0.099, -lr * Pred Grad:  0.073, New P: 1.333
-Original Grad: 0.108, -lr * Pred Grad:  -0.024, New P: 0.090
iter 8 loss: 0.091
Actual params: [1.3333, 0.0897]
-Original Grad: 0.074, -lr * Pred Grad:  0.070, New P: 1.403
-Original Grad: 0.114, -lr * Pred Grad:  -0.019, New P: 0.071
iter 9 loss: 0.087
Actual params: [1.4035, 0.0708]
-Original Grad: 0.060, -lr * Pred Grad:  0.067, New P: 1.470
-Original Grad: 0.129, -lr * Pred Grad:  -0.015, New P: 0.056
iter 10 loss: 0.085
Actual params: [1.4704, 0.0562]
-Original Grad: 0.099, -lr * Pred Grad:  0.066, New P: 1.537
-Original Grad: 0.240, -lr * Pred Grad:  -0.009, New P: 0.048
iter 11 loss: 0.083
Actual params: [1.5369, 0.0476]
-Original Grad: -0.125, -lr * Pred Grad:  0.050, New P: 1.587
-Original Grad: -0.461, -lr * Pred Grad:  -0.016, New P: 0.032
iter 12 loss: 0.076
Actual params: [1.5873, 0.0316]
-Original Grad: 0.102, -lr * Pred Grad:  0.052, New P: 1.639
-Original Grad: 0.216, -lr * Pred Grad:  -0.010, New P: 0.021
iter 13 loss: 0.074
Actual params: [1.6393, 0.0211]
-Original Grad: 0.054, -lr * Pred Grad:  0.051, New P: 1.690
-Original Grad: 0.078, -lr * Pred Grad:  -0.008, New P: 0.013
iter 14 loss: 0.072
Actual params: [1.69 , 0.013]
-Original Grad: 0.048, -lr * Pred Grad:  0.049, New P: 1.739
-Original Grad: 0.083, -lr * Pred Grad:  -0.006, New P: 0.007
iter 15 loss: 0.070
Actual params: [1.7391, 0.0072]
-Original Grad: 0.040, -lr * Pred Grad:  0.047, New P: 1.786
-Original Grad: 0.071, -lr * Pred Grad:  -0.004, New P: 0.003
iter 16 loss: 0.068
Actual params: [1.7864, 0.0033]
-Original Grad: 0.039, -lr * Pred Grad:  0.046, New P: 1.832
-Original Grad: 0.072, -lr * Pred Grad:  -0.002, New P: 0.001
iter 17 loss: 0.067
Actual params: [1.8320e+00, 1.0652e-03]
-Original Grad: 0.036, -lr * Pred Grad:  0.044, New P: 1.876
-Original Grad: 0.078, -lr * Pred Grad:  -0.001, New P: 0.000
iter 18 loss: 0.065
Actual params: [1.8759e+00, 4.9519e-04]
-Original Grad: 0.028, -lr * Pred Grad:  0.042, New P: 1.918
-Original Grad: 0.066, -lr * Pred Grad:  0.001, New P: 0.001
iter 19 loss: 0.064
Actual params: [1.9178e+00, 1.2306e-03]
-Original Grad: 0.011, -lr * Pred Grad:  0.039, New P: 1.957
-Original Grad: 0.035, -lr * Pred Grad:  0.001, New P: 0.003
iter 20 loss: 0.063
Actual params: [1.9567, 0.0026]
