Target params: [1.1812, 0.2779]
Actual params: [1.084 , 1.4018]
-Original Grad: -0.185, -lr * Pred Grad: -0.044, New P: 1.040
-Original Grad: 0.073, -lr * Pred Grad: 0.043, New P: 1.445
iter 0 loss: 0.506
Actual params: [1.0398, 1.4446]
-Original Grad: -0.315, -lr * Pred Grad: -0.059, New P: 0.980
-Original Grad: 0.087, -lr * Pred Grad: 0.062, New P: 1.506
iter 1 loss: 0.442
Actual params: [0.9804, 1.5061]
-Original Grad: -0.351, -lr * Pred Grad: -0.062, New P: 0.918
-Original Grad: 0.081, -lr * Pred Grad: 0.064, New P: 1.571
iter 2 loss: 0.354
Actual params: [0.9185, 1.5706]
-Original Grad: -0.402, -lr * Pred Grad: -0.062, New P: 0.856
-Original Grad: 0.138, -lr * Pred Grad: 0.064, New P: 1.635
iter 3 loss: 0.312
Actual params: [0.8562, 1.6349]
-Original Grad: -0.266, -lr * Pred Grad: -0.062, New P: 0.794
-Original Grad: 0.053, -lr * Pred Grad: 0.064, New P: 1.699
iter 4 loss: 0.310
Actual params: [0.794 , 1.6991]
-Original Grad: -0.209, -lr * Pred Grad: -0.062, New P: 0.732
-Original Grad: 0.109, -lr * Pred Grad: 0.064, New P: 1.763
iter 5 loss: 0.363
Actual params: [0.7317, 1.7628]
-Original Grad: -0.022, -lr * Pred Grad: -0.062, New P: 0.669
-Original Grad: 0.090, -lr * Pred Grad: 0.063, New P: 1.826
iter 6 loss: 0.259
Actual params: [0.6694, 1.826 ]
-Original Grad: 0.281, -lr * Pred Grad: -0.062, New P: 0.607
-Original Grad: 0.048, -lr * Pred Grad: 0.062, New P: 1.888
iter 7 loss: 0.292
Actual params: [0.6071, 1.8883]
-Original Grad: 0.214, -lr * Pred Grad: -0.062, New P: 0.545
-Original Grad: 0.065, -lr * Pred Grad: 0.061, New P: 1.949
iter 8 loss: 0.320
Actual params: [0.5448, 1.9491]
-Original Grad: 0.280, -lr * Pred Grad: -0.062, New P: 0.483
-Original Grad: 0.057, -lr * Pred Grad: 0.058, New P: 2.008
iter 9 loss: 0.348
Actual params: [0.4825, 2.0076]
-Original Grad: 0.192, -lr * Pred Grad: -0.062, New P: 0.420
-Original Grad: 0.022, -lr * Pred Grad: 0.054, New P: 2.062
iter 10 loss: 0.340
Actual params: [0.4202, 2.0619]
-Original Grad: 0.302, -lr * Pred Grad: -0.062, New P: 0.358
-Original Grad: 0.011, -lr * Pred Grad: 0.047, New P: 2.109
iter 11 loss: 0.379
Actual params: [0.3579, 2.109 ]
-Original Grad: 0.244, -lr * Pred Grad: -0.062, New P: 0.296
-Original Grad: 0.006, -lr * Pred Grad: 0.032, New P: 2.141
iter 12 loss: 0.341
Actual params: [0.2956, 2.1406]
-Original Grad: 0.220, -lr * Pred Grad: -0.062, New P: 0.233
-Original Grad: 0.020, -lr * Pred Grad: -0.006, New P: 2.135
iter 13 loss: 0.370
Actual params: [0.2334, 2.1348]
-Original Grad: 0.182, -lr * Pred Grad: -0.062, New P: 0.171
-Original Grad: 0.021, -lr * Pred Grad: -0.040, New P: 2.095
iter 14 loss: 0.380
Actual params: [0.1711, 2.0949]
-Original Grad: 0.152, -lr * Pred Grad: -0.062, New P: 0.109
-Original Grad: 0.030, -lr * Pred Grad: -0.049, New P: 2.046
iter 15 loss: 0.419
Actual params: [0.1088, 2.0461]
-Original Grad: 0.135, -lr * Pred Grad: -0.062, New P: 0.046
-Original Grad: 0.022, -lr * Pred Grad: -0.050, New P: 1.996
iter 16 loss: 0.408
Actual params: [0.0465, 1.996 ]
-Original Grad: 0.130, -lr * Pred Grad: -0.062, New P: -0.016
-Original Grad: 0.051, -lr * Pred Grad: -0.050, New P: 1.946
iter 17 loss: 0.401
Actual params: [-0.0158,  1.9455]
-Original Grad: 0.123, -lr * Pred Grad: -0.062, New P: -0.078
-Original Grad: 0.042, -lr * Pred Grad: -0.051, New P: 1.895
iter 18 loss: 0.450
Actual params: [-0.0781,  1.8946]
-Original Grad: 0.125, -lr * Pred Grad: -0.062, New P: -0.140
-Original Grad: 0.053, -lr * Pred Grad: -0.053, New P: 1.842
iter 19 loss: 0.444
Actual params: [-0.1404,  1.8419]
-Original Grad: 0.134, -lr * Pred Grad: -0.062, New P: -0.203
-Original Grad: 0.053, -lr * Pred Grad: -0.055, New P: 1.787
iter 20 loss: 0.457
Actual params: [-0.2027,  1.7872]
Target params: [1.1812, 0.2779]
Actual params: [ 0.0029, -1.5044]
-Original Grad: -0.008, -lr * Pred Grad: 0.011, New P: 0.014
-Original Grad: -0.031, -lr * Pred Grad: -0.004, New P: -1.508
iter 0 loss: 0.028
Actual params: [ 0.0143, -1.5081]
-Original Grad: -0.008, -lr * Pred Grad: -0.037, New P: -0.023
-Original Grad: -0.034, -lr * Pred Grad: -0.049, New P: -1.557
iter 1 loss: 0.034
Actual params: [-0.0227, -1.5571]
-Original Grad: -0.006, -lr * Pred Grad: -0.058, New P: -0.081
-Original Grad: -0.023, -lr * Pred Grad: -0.060, New P: -1.617
iter 2 loss: 0.022
Actual params: [-0.0806, -1.6173]
-Original Grad: -0.004, -lr * Pred Grad: -0.062, New P: -0.142
-Original Grad: -0.023, -lr * Pred Grad: -0.062, New P: -1.679
iter 3 loss: 0.024
Actual params: [-0.1423, -1.6793]
-Original Grad: -0.005, -lr * Pred Grad: -0.062, New P: -0.204
-Original Grad: -0.022, -lr * Pred Grad: -0.062, New P: -1.742
iter 4 loss: 0.022
Actual params: [-0.2045, -1.7416]
-Original Grad: -0.008, -lr * Pred Grad: -0.062, New P: -0.267
-Original Grad: -0.019, -lr * Pred Grad: -0.062, New P: -1.804
iter 5 loss: 0.024
Actual params: [-0.2668, -1.8039]
-Original Grad: -0.010, -lr * Pred Grad: -0.062, New P: -0.329
-Original Grad: -0.019, -lr * Pred Grad: -0.062, New P: -1.866
iter 6 loss: 0.026
Actual params: [-0.329 , -1.8662]
-Original Grad: -0.004, -lr * Pred Grad: -0.062, New P: -0.391
-Original Grad: -0.012, -lr * Pred Grad: -0.062, New P: -1.928
iter 7 loss: 0.020
Actual params: [-0.3913, -1.9284]
-Original Grad: -0.007, -lr * Pred Grad: -0.062, New P: -0.454
-Original Grad: -0.017, -lr * Pred Grad: -0.062, New P: -1.991
iter 8 loss: 0.024
Actual params: [-0.4536, -1.9907]
-Original Grad: -0.005, -lr * Pred Grad: -0.062, New P: -0.516
-Original Grad: -0.014, -lr * Pred Grad: -0.062, New P: -2.053
iter 9 loss: 0.020
Actual params: [-0.5159, -2.053 ]
-Original Grad: -0.004, -lr * Pred Grad: -0.062, New P: -0.578
-Original Grad: -0.014, -lr * Pred Grad: -0.062, New P: -2.115
iter 10 loss: 0.021
Actual params: [-0.5782, -2.1153]
-Original Grad: -0.006, -lr * Pred Grad: -0.062, New P: -0.640
-Original Grad: -0.011, -lr * Pred Grad: -0.062, New P: -2.178
iter 11 loss: 0.018
Actual params: [-0.6405, -2.1776]
-Original Grad: -0.006, -lr * Pred Grad: -0.062, New P: -0.703
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -2.240
iter 12 loss: 0.015
Actual params: [-0.7028, -2.2399]
-Original Grad: -0.005, -lr * Pred Grad: -0.062, New P: -0.765
-Original Grad: -0.007, -lr * Pred Grad: -0.062, New P: -2.302
iter 13 loss: 0.015
Actual params: [-0.7651, -2.3022]
-Original Grad: -0.004, -lr * Pred Grad: -0.062, New P: -0.827
-Original Grad: -0.005, -lr * Pred Grad: -0.062, New P: -2.364
iter 14 loss: 0.014
Actual params: [-0.8274, -2.3645]
-Original Grad: -0.004, -lr * Pred Grad: -0.062, New P: -0.890
-Original Grad: -0.010, -lr * Pred Grad: -0.062, New P: -2.427
iter 15 loss: 0.017
Actual params: [-0.8896, -2.4268]
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -0.952
-Original Grad: -0.008, -lr * Pred Grad: -0.062, New P: -2.489
iter 16 loss: 0.016
Actual params: [-0.9519, -2.489 ]
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -1.014
-Original Grad: -0.007, -lr * Pred Grad: -0.062, New P: -2.551
iter 17 loss: 0.013
Actual params: [-1.0142, -2.5513]
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -1.077
-Original Grad: -0.007, -lr * Pred Grad: -0.062, New P: -2.614
iter 18 loss: 0.015
Actual params: [-1.0765, -2.6136]
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -1.139
-Original Grad: -0.005, -lr * Pred Grad: -0.062, New P: -2.676
iter 19 loss: 0.010
Actual params: [-1.1388, -2.6759]
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -1.201
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -2.738
iter 20 loss: 0.012
Actual params: [-1.2011, -2.7382]
Target params: [1.1812, 0.2779]
Actual params: [-0.8962,  0.3381]
-Original Grad: -0.080, -lr * Pred Grad: -0.029, New P: -0.925
-Original Grad: 0.204, -lr * Pred Grad: 0.049, New P: 0.388
iter 0 loss: 0.325
Actual params: [-0.9252,  0.3876]
-Original Grad: -0.068, -lr * Pred Grad: -0.056, New P: -0.982
-Original Grad: 0.193, -lr * Pred Grad: 0.063, New P: 0.450
iter 1 loss: 0.280
Actual params: [-0.9816,  0.4504]
-Original Grad: -0.052, -lr * Pred Grad: -0.061, New P: -1.043
-Original Grad: 0.164, -lr * Pred Grad: 0.065, New P: 0.515
iter 2 loss: 0.247
Actual params: [-1.0431,  0.5153]
-Original Grad: -0.061, -lr * Pred Grad: -0.062, New P: -1.105
-Original Grad: 0.134, -lr * Pred Grad: 0.065, New P: 0.581
iter 3 loss: 0.231
Actual params: [-1.1053,  0.5805]
-Original Grad: -0.054, -lr * Pred Grad: -0.062, New P: -1.168
-Original Grad: 0.067, -lr * Pred Grad: 0.065, New P: 0.646
iter 4 loss: 0.219
Actual params: [-1.1675,  0.6458]
-Original Grad: -0.054, -lr * Pred Grad: -0.062, New P: -1.230
-Original Grad: 0.053, -lr * Pred Grad: 0.065, New P: 0.711
iter 5 loss: 0.258
Actual params: [-1.2298,  0.711 ]
-Original Grad: -0.037, -lr * Pred Grad: -0.062, New P: -1.292
-Original Grad: 0.033, -lr * Pred Grad: 0.065, New P: 0.776
iter 6 loss: 0.240
Actual params: [-1.2921,  0.7762]
-Original Grad: -0.036, -lr * Pred Grad: -0.062, New P: -1.354
-Original Grad: 0.028, -lr * Pred Grad: 0.065, New P: 0.841
iter 7 loss: 0.220
Actual params: [-1.3544,  0.8415]
-Original Grad: -0.042, -lr * Pred Grad: -0.062, New P: -1.417
-Original Grad: 0.019, -lr * Pred Grad: 0.065, New P: 0.907
iter 8 loss: 0.227
Actual params: [-1.4167,  0.9067]
-Original Grad: -0.025, -lr * Pred Grad: -0.062, New P: -1.479
-Original Grad: 0.015, -lr * Pred Grad: 0.065, New P: 0.972
iter 9 loss: 0.216
Actual params: [-1.479 ,  0.9719]
-Original Grad: -0.049, -lr * Pred Grad: -0.062, New P: -1.541
-Original Grad: 0.010, -lr * Pred Grad: 0.065, New P: 1.037
iter 10 loss: 0.272
Actual params: [-1.5413,  1.0371]
-Original Grad: -0.034, -lr * Pred Grad: -0.062, New P: -1.604
-Original Grad: -0.001, -lr * Pred Grad: 0.065, New P: 1.102
iter 11 loss: 0.217
Actual params: [-1.6035,  1.1024]
-Original Grad: -0.028, -lr * Pred Grad: -0.062, New P: -1.666
-Original Grad: -0.006, -lr * Pred Grad: 0.065, New P: 1.168
iter 12 loss: 0.192
Actual params: [-1.6658,  1.1676]
-Original Grad: -0.029, -lr * Pred Grad: -0.062, New P: -1.728
-Original Grad: -0.014, -lr * Pred Grad: 0.065, New P: 1.233
iter 13 loss: 0.230
Actual params: [-1.7281,  1.2328]
-Original Grad: -0.025, -lr * Pred Grad: -0.062, New P: -1.790
-Original Grad: -0.013, -lr * Pred Grad: 0.065, New P: 1.298
iter 14 loss: 0.200
Actual params: [-1.7904,  1.2981]
-Original Grad: -0.030, -lr * Pred Grad: -0.062, New P: -1.853
-Original Grad: -0.011, -lr * Pred Grad: 0.065, New P: 1.363
iter 15 loss: 0.196
Actual params: [-1.8527,  1.3633]
-Original Grad: -0.016, -lr * Pred Grad: -0.062, New P: -1.915
-Original Grad: 0.001, -lr * Pred Grad: 0.065, New P: 1.429
iter 16 loss: 0.207
Actual params: [-1.915 ,  1.4285]
-Original Grad: -0.033, -lr * Pred Grad: -0.062, New P: -1.977
-Original Grad: -0.003, -lr * Pred Grad: 0.065, New P: 1.494
iter 17 loss: 0.200
Actual params: [-1.9773,  1.4938]
-Original Grad: -0.018, -lr * Pred Grad: -0.062, New P: -2.040
-Original Grad: 0.008, -lr * Pred Grad: 0.065, New P: 1.559
iter 18 loss: 0.193
Actual params: [-2.0396,  1.559 ]
-Original Grad: -0.023, -lr * Pred Grad: -0.062, New P: -2.102
-Original Grad: 0.009, -lr * Pred Grad: 0.065, New P: 1.624
iter 19 loss: 0.200
Actual params: [-2.1018,  1.6242]
-Original Grad: -0.017, -lr * Pred Grad: -0.062, New P: -2.164
-Original Grad: 0.012, -lr * Pred Grad: 0.065, New P: 1.689
iter 20 loss: 0.176
Actual params: [-2.1641,  1.6895]
Target params: [1.1812, 0.2779]
Actual params: [ 0.3685, -2.1923]
-Original Grad: -0.013, -lr * Pred Grad: 0.008, New P: 0.377
-Original Grad: 0.272, -lr * Pred Grad: 0.050, New P: -2.142
iter 0 loss: 0.573
Actual params: [ 0.3768, -2.1424]
-Original Grad: -0.015, -lr * Pred Grad: -0.041, New P: 0.335
-Original Grad: 0.262, -lr * Pred Grad: 0.063, New P: -2.080
iter 1 loss: 0.605
Actual params: [ 0.3354, -2.0795]
-Original Grad: -0.007, -lr * Pred Grad: -0.059, New P: 0.276
-Original Grad: 0.305, -lr * Pred Grad: 0.065, New P: -2.015
iter 2 loss: 0.586
Actual params: [ 0.2765, -2.0146]
-Original Grad: 0.018, -lr * Pred Grad: -0.062, New P: 0.215
-Original Grad: 0.314, -lr * Pred Grad: 0.065, New P: -1.949
iter 3 loss: 0.595
Actual params: [ 0.2147, -1.9494]
-Original Grad: 0.006, -lr * Pred Grad: -0.062, New P: 0.152
-Original Grad: 0.290, -lr * Pred Grad: 0.065, New P: -1.884
iter 4 loss: 0.589
Actual params: [ 0.1524, -1.8842]
-Original Grad: 0.027, -lr * Pred Grad: -0.062, New P: 0.090
-Original Grad: 0.315, -lr * Pred Grad: 0.065, New P: -1.819
iter 5 loss: 0.549
Actual params: [ 0.0902, -1.819 ]
-Original Grad: 0.008, -lr * Pred Grad: -0.062, New P: 0.028
-Original Grad: 0.395, -lr * Pred Grad: 0.065, New P: -1.754
iter 6 loss: 0.541
Actual params: [ 0.0279, -1.7537]
-Original Grad: 0.046, -lr * Pred Grad: -0.062, New P: -0.034
-Original Grad: 0.349, -lr * Pred Grad: 0.065, New P: -1.689
iter 7 loss: 0.530
Actual params: [-0.0344, -1.6885]
-Original Grad: 0.054, -lr * Pred Grad: -0.062, New P: -0.097
-Original Grad: 0.344, -lr * Pred Grad: 0.065, New P: -1.623
iter 8 loss: 0.443
Actual params: [-0.0967, -1.6233]
-Original Grad: 0.047, -lr * Pred Grad: -0.062, New P: -0.159
-Original Grad: 0.350, -lr * Pred Grad: 0.065, New P: -1.558
iter 9 loss: 0.494
Actual params: [-0.159 , -1.5581]
-Original Grad: 0.037, -lr * Pred Grad: -0.062, New P: -0.221
-Original Grad: 0.370, -lr * Pred Grad: 0.065, New P: -1.493
iter 10 loss: 0.433
Actual params: [-0.2213, -1.4928]
-Original Grad: 0.044, -lr * Pred Grad: -0.062, New P: -0.284
-Original Grad: 0.377, -lr * Pred Grad: 0.065, New P: -1.428
iter 11 loss: 0.480
Actual params: [-0.2836, -1.4276]
-Original Grad: 0.083, -lr * Pred Grad: -0.062, New P: -0.346
-Original Grad: 0.337, -lr * Pred Grad: 0.065, New P: -1.362
iter 12 loss: 0.320
Actual params: [-0.3458, -1.3624]
-Original Grad: 0.064, -lr * Pred Grad: -0.062, New P: -0.408
-Original Grad: 0.341, -lr * Pred Grad: 0.065, New P: -1.297
iter 13 loss: 0.357
Actual params: [-0.4081, -1.2971]
-Original Grad: 0.062, -lr * Pred Grad: -0.062, New P: -0.470
-Original Grad: 0.366, -lr * Pred Grad: 0.065, New P: -1.232
iter 14 loss: 0.297
Actual params: [-0.4704, -1.2319]
-Original Grad: 0.056, -lr * Pred Grad: -0.062, New P: -0.533
-Original Grad: 0.397, -lr * Pred Grad: 0.065, New P: -1.167
iter 15 loss: 0.317
Actual params: [-0.5327, -1.1667]
-Original Grad: 0.059, -lr * Pred Grad: -0.062, New P: -0.595
-Original Grad: 0.413, -lr * Pred Grad: 0.065, New P: -1.101
iter 16 loss: 0.239
Actual params: [-0.595 , -1.1014]
-Original Grad: 0.051, -lr * Pred Grad: -0.062, New P: -0.657
-Original Grad: 0.486, -lr * Pred Grad: 0.065, New P: -1.036
iter 17 loss: 0.308
Actual params: [-0.6573, -1.0362]
-Original Grad: 0.036, -lr * Pred Grad: -0.062, New P: -0.720
-Original Grad: 0.331, -lr * Pred Grad: 0.065, New P: -0.971
iter 18 loss: 0.187
Actual params: [-0.7196, -0.971 ]
-Original Grad: 0.050, -lr * Pred Grad: -0.062, New P: -0.782
-Original Grad: 0.460, -lr * Pred Grad: 0.065, New P: -0.906
iter 19 loss: 0.225
Actual params: [-0.7819, -0.9057]
-Original Grad: 0.050, -lr * Pred Grad: -0.062, New P: -0.844
-Original Grad: 0.625, -lr * Pred Grad: 0.065, New P: -0.840
iter 20 loss: 0.252
Actual params: [-0.8442, -0.8405]
Target params: [1.1812, 0.2779]
Actual params: [-0.1438, -1.3168]
-Original Grad: 0.318, -lr * Pred Grad: 0.050, New P: -0.094
-Original Grad: 0.745, -lr * Pred Grad: 0.050, New P: -1.267
iter 0 loss: 0.775
Actual params: [-0.0939, -1.2668]
-Original Grad: 0.440, -lr * Pred Grad: 0.063, New P: -0.031
-Original Grad: 0.924, -lr * Pred Grad: 0.063, New P: -1.204
iter 1 loss: 0.725
Actual params: [-0.031 , -1.2039]
-Original Grad: 0.770, -lr * Pred Grad: 0.065, New P: 0.034
-Original Grad: 0.832, -lr * Pred Grad: 0.065, New P: -1.139
iter 2 loss: 0.688
Actual params: [ 0.0339, -1.1389]
-Original Grad: 0.862, -lr * Pred Grad: 0.065, New P: 0.099
-Original Grad: 1.910, -lr * Pred Grad: 0.065, New P: -1.074
iter 3 loss: 0.410
Actual params: [ 0.0991, -1.0738]
-Original Grad: 0.321, -lr * Pred Grad: 0.065, New P: 0.164
-Original Grad: 0.628, -lr * Pred Grad: 0.065, New P: -1.009
iter 4 loss: 0.306
Actual params: [ 0.1643, -1.0085]
-Original Grad: 0.192, -lr * Pred Grad: 0.065, New P: 0.230
-Original Grad: 0.539, -lr * Pred Grad: 0.065, New P: -0.943
iter 5 loss: 0.227
Actual params: [ 0.2295, -0.9433]
-Original Grad: 0.187, -lr * Pred Grad: 0.065, New P: 0.295
-Original Grad: 1.228, -lr * Pred Grad: 0.065, New P: -0.878
iter 6 loss: 0.221
Actual params: [ 0.2948, -0.8781]
-Original Grad: 0.096, -lr * Pred Grad: 0.065, New P: 0.360
-Original Grad: 0.644, -lr * Pred Grad: 0.065, New P: -0.813
iter 7 loss: 0.205
Actual params: [ 0.36  , -0.8128]
-Original Grad: 0.097, -lr * Pred Grad: 0.065, New P: 0.425
-Original Grad: 0.182, -lr * Pred Grad: 0.065, New P: -0.748
iter 8 loss: 0.110
Actual params: [ 0.4252, -0.7476]
-Original Grad: 0.073, -lr * Pred Grad: 0.065, New P: 0.490
-Original Grad: 0.068, -lr * Pred Grad: 0.065, New P: -0.682
iter 9 loss: 0.091
Actual params: [ 0.4905, -0.6824]
-Original Grad: -0.013, -lr * Pred Grad: 0.065, New P: 0.556
-Original Grad: 0.186, -lr * Pred Grad: 0.065, New P: -0.617
iter 10 loss: 0.076
Actual params: [ 0.5557, -0.6171]
-Original Grad: -0.034, -lr * Pred Grad: 0.065, New P: 0.621
-Original Grad: 0.142, -lr * Pred Grad: 0.065, New P: -0.552
iter 11 loss: 0.071
Actual params: [ 0.6209, -0.5519]
-Original Grad: 0.052, -lr * Pred Grad: 0.065, New P: 0.686
-Original Grad: 0.054, -lr * Pred Grad: 0.065, New P: -0.487
iter 12 loss: 0.044
Actual params: [ 0.6862, -0.4867]
-Original Grad: -0.112, -lr * Pred Grad: 0.065, New P: 0.751
-Original Grad: 0.137, -lr * Pred Grad: 0.065, New P: -0.421
iter 13 loss: 0.056
Actual params: [ 0.7514, -0.4214]
-Original Grad: -0.352, -lr * Pred Grad: 0.065, New P: 0.817
-Original Grad: 0.108, -lr * Pred Grad: 0.065, New P: -0.356
iter 14 loss: 0.040
Actual params: [ 0.8166, -0.3562]
-Original Grad: -0.483, -lr * Pred Grad: 0.065, New P: 0.882
-Original Grad: 0.175, -lr * Pred Grad: 0.065, New P: -0.291
iter 15 loss: 0.079
Actual params: [ 0.8819, -0.291 ]
-Original Grad: 0.004, -lr * Pred Grad: 0.065, New P: 0.947
-Original Grad: 0.227, -lr * Pred Grad: 0.065, New P: -0.226
iter 16 loss: 0.083
Actual params: [ 0.9471, -0.2257]
-Original Grad: 0.159, -lr * Pred Grad: 0.065, New P: 1.012
-Original Grad: 0.104, -lr * Pred Grad: 0.065, New P: -0.161
iter 17 loss: 0.054
Actual params: [ 1.0123, -0.1605]
-Original Grad: 0.130, -lr * Pred Grad: 0.065, New P: 1.078
-Original Grad: 0.079, -lr * Pred Grad: 0.065, New P: -0.095
iter 18 loss: 0.037
Actual params: [ 1.0776, -0.0953]
-Original Grad: 0.092, -lr * Pred Grad: 0.065, New P: 1.143
-Original Grad: 0.078, -lr * Pred Grad: 0.065, New P: -0.030
iter 19 loss: 0.033
Actual params: [ 1.1428, -0.03  ]
-Original Grad: 0.044, -lr * Pred Grad: 0.065, New P: 1.208
-Original Grad: 0.038, -lr * Pred Grad: 0.065, New P: 0.035
iter 20 loss: 0.020
Actual params: [1.208 , 0.0352]
Target params: [1.1812, 0.2779]
Actual params: [-1.23  , -0.0332]
-Original Grad: -0.003, -lr * Pred Grad: 0.015, New P: -1.215
-Original Grad: 0.012, -lr * Pred Grad: 0.024, New P: -0.010
iter 0 loss: 0.086
Actual params: [-1.2154, -0.0097]
-Original Grad: -0.001, -lr * Pred Grad: -0.031, New P: -1.246
-Original Grad: -0.005, -lr * Pred Grad: -0.009, New P: -0.018
iter 1 loss: 0.073
Actual params: [-1.246 , -0.0185]
-Original Grad: 0.001, -lr * Pred Grad: -0.056, New P: -1.302
-Original Grad: -0.014, -lr * Pred Grad: -0.045, New P: -0.063
iter 2 loss: 0.076
Actual params: [-1.3018, -0.0631]
-Original Grad: -0.001, -lr * Pred Grad: -0.061, New P: -1.363
-Original Grad: -0.009, -lr * Pred Grad: -0.059, New P: -0.122
iter 3 loss: 0.084
Actual params: [-1.3631, -0.1216]
-Original Grad: -0.000, -lr * Pred Grad: -0.062, New P: -1.425
-Original Grad: 0.001, -lr * Pred Grad: -0.062, New P: -0.183
iter 4 loss: 0.083
Actual params: [-1.4253, -0.1833]
-Original Grad: -0.005, -lr * Pred Grad: -0.062, New P: -1.488
-Original Grad: -0.008, -lr * Pred Grad: -0.062, New P: -0.246
iter 5 loss: 0.079
Actual params: [-1.4875, -0.2455]
-Original Grad: -0.000, -lr * Pred Grad: -0.062, New P: -1.550
-Original Grad: -0.015, -lr * Pred Grad: -0.062, New P: -0.308
iter 6 loss: 0.081
Actual params: [-1.5498, -0.3078]
-Original Grad: -0.001, -lr * Pred Grad: -0.062, New P: -1.612
-Original Grad: -0.017, -lr * Pred Grad: -0.062, New P: -0.370
iter 7 loss: 0.069
Actual params: [-1.6121, -0.3701]
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -1.674
-Original Grad: -0.035, -lr * Pred Grad: -0.062, New P: -0.432
iter 8 loss: 0.085
Actual params: [-1.6744, -0.4324]
-Original Grad: -0.001, -lr * Pred Grad: -0.062, New P: -1.737
-Original Grad: -0.023, -lr * Pred Grad: -0.062, New P: -0.495
iter 9 loss: 0.077
Actual params: [-1.7367, -0.4947]
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -1.799
-Original Grad: -0.017, -lr * Pred Grad: -0.062, New P: -0.557
iter 10 loss: 0.076
Actual params: [-1.799, -0.557]
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -1.861
-Original Grad: -0.017, -lr * Pred Grad: -0.062, New P: -0.619
iter 11 loss: 0.079
Actual params: [-1.8613, -0.6193]
-Original Grad: -0.004, -lr * Pred Grad: -0.062, New P: -1.924
-Original Grad: -0.011, -lr * Pred Grad: -0.062, New P: -0.682
iter 12 loss: 0.078
Actual params: [-1.9235, -0.6815]
-Original Grad: -0.004, -lr * Pred Grad: -0.062, New P: -1.986
-Original Grad: -0.001, -lr * Pred Grad: -0.062, New P: -0.744
iter 13 loss: 0.065
Actual params: [-1.9858, -0.7438]
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -2.048
-Original Grad: -0.001, -lr * Pred Grad: -0.062, New P: -0.806
iter 14 loss: 0.061
Actual params: [-2.0481, -0.8061]
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -2.110
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -0.868
iter 15 loss: 0.073
Actual params: [-2.1104, -0.8684]
-Original Grad: -0.004, -lr * Pred Grad: -0.062, New P: -2.173
-Original Grad: -0.001, -lr * Pred Grad: -0.062, New P: -0.931
iter 16 loss: 0.072
Actual params: [-2.1727, -0.9307]
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -2.235
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -0.993
iter 17 loss: 0.064
Actual params: [-2.235, -0.993]
-Original Grad: -0.007, -lr * Pred Grad: -0.062, New P: -2.297
-Original Grad: -0.011, -lr * Pred Grad: -0.062, New P: -1.055
iter 18 loss: 0.070
Actual params: [-2.2973, -1.0553]
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: -2.360
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: -1.118
iter 19 loss: 0.076
Actual params: [-2.3596, -1.1176]
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -2.422
-Original Grad: 0.004, -lr * Pred Grad: -0.062, New P: -1.180
iter 20 loss: 0.060
Actual params: [-2.4218, -1.1798]
Target params: [1.1812, 0.2779]
Actual params: [ 1.0788, -1.6003]
-Original Grad: -0.200, -lr * Pred Grad: -0.045, New P: 1.034
-Original Grad: 0.796, -lr * Pred Grad: 0.050, New P: -1.550
iter 0 loss: 0.396
Actual params: [ 1.0342, -1.5503]
-Original Grad: -0.064, -lr * Pred Grad: -0.060, New P: 0.975
-Original Grad: 0.832, -lr * Pred Grad: 0.063, New P: -1.487
iter 1 loss: 0.437
Actual params: [ 0.9746, -1.4873]
-Original Grad: 0.124, -lr * Pred Grad: -0.062, New P: 0.913
-Original Grad: 0.749, -lr * Pred Grad: 0.065, New P: -1.422
iter 2 loss: 0.410
Actual params: [ 0.9127, -1.4224]
-Original Grad: 0.256, -lr * Pred Grad: -0.062, New P: 0.850
-Original Grad: 0.547, -lr * Pred Grad: 0.065, New P: -1.357
iter 3 loss: 0.341
Actual params: [ 0.8505, -1.3572]
-Original Grad: 0.239, -lr * Pred Grad: -0.062, New P: 0.788
-Original Grad: 0.714, -lr * Pred Grad: 0.065, New P: -1.292
iter 4 loss: 0.320
Actual params: [ 0.7882, -1.292 ]
-Original Grad: 0.176, -lr * Pred Grad: -0.062, New P: 0.726
-Original Grad: 0.567, -lr * Pred Grad: 0.065, New P: -1.227
iter 5 loss: 0.257
Actual params: [ 0.7259, -1.2268]
-Original Grad: 0.304, -lr * Pred Grad: -0.062, New P: 0.664
-Original Grad: 0.445, -lr * Pred Grad: 0.065, New P: -1.162
iter 6 loss: 0.232
Actual params: [ 0.6636, -1.1615]
-Original Grad: 0.543, -lr * Pred Grad: -0.062, New P: 0.601
-Original Grad: 0.378, -lr * Pred Grad: 0.065, New P: -1.096
iter 7 loss: 0.297
Actual params: [ 0.6013, -1.0963]
-Original Grad: 0.495, -lr * Pred Grad: -0.062, New P: 0.539
-Original Grad: 0.336, -lr * Pred Grad: 0.065, New P: -1.031
iter 8 loss: 0.276
Actual params: [ 0.539 , -1.0311]
-Original Grad: 0.434, -lr * Pred Grad: -0.062, New P: 0.477
-Original Grad: 0.309, -lr * Pred Grad: 0.065, New P: -0.966
iter 9 loss: 0.309
Actual params: [ 0.4768, -0.9658]
-Original Grad: 0.486, -lr * Pred Grad: -0.062, New P: 0.414
-Original Grad: 0.389, -lr * Pred Grad: 0.065, New P: -0.901
iter 10 loss: 0.359
Actual params: [ 0.4145, -0.9006]
-Original Grad: 0.517, -lr * Pred Grad: -0.062, New P: 0.352
-Original Grad: 0.297, -lr * Pred Grad: 0.065, New P: -0.835
iter 11 loss: 0.333
Actual params: [ 0.3522, -0.8354]
-Original Grad: 0.406, -lr * Pred Grad: -0.062, New P: 0.290
-Original Grad: 0.253, -lr * Pred Grad: 0.065, New P: -0.770
iter 12 loss: 0.380
Actual params: [ 0.2899, -0.7702]
-Original Grad: 0.510, -lr * Pred Grad: -0.062, New P: 0.228
-Original Grad: 0.247, -lr * Pred Grad: 0.065, New P: -0.705
iter 13 loss: 0.328
Actual params: [ 0.2276, -0.7049]
-Original Grad: 0.422, -lr * Pred Grad: -0.062, New P: 0.165
-Original Grad: 0.240, -lr * Pred Grad: 0.065, New P: -0.640
iter 14 loss: 0.341
Actual params: [ 0.1653, -0.6397]
-Original Grad: 0.333, -lr * Pred Grad: -0.062, New P: 0.103
-Original Grad: 0.191, -lr * Pred Grad: 0.065, New P: -0.574
iter 15 loss: 0.350
Actual params: [ 0.103 , -0.5745]
-Original Grad: 0.322, -lr * Pred Grad: -0.062, New P: 0.041
-Original Grad: 0.240, -lr * Pred Grad: 0.065, New P: -0.509
iter 16 loss: 0.375
Actual params: [ 0.0407, -0.5092]
-Original Grad: 0.188, -lr * Pred Grad: -0.062, New P: -0.022
-Original Grad: 0.191, -lr * Pred Grad: 0.065, New P: -0.444
iter 17 loss: 0.360
Actual params: [-0.0216, -0.444 ]
-Original Grad: 0.130, -lr * Pred Grad: -0.062, New P: -0.084
-Original Grad: 0.184, -lr * Pred Grad: 0.065, New P: -0.379
iter 18 loss: 0.374
Actual params: [-0.0838, -0.3788]
-Original Grad: 0.140, -lr * Pred Grad: -0.062, New P: -0.146
-Original Grad: 0.175, -lr * Pred Grad: 0.065, New P: -0.314
iter 19 loss: 0.365
Actual params: [-0.1461, -0.3135]
-Original Grad: 0.142, -lr * Pred Grad: -0.062, New P: -0.208
-Original Grad: 0.149, -lr * Pred Grad: 0.065, New P: -0.248
iter 20 loss: 0.369
Actual params: [-0.2084, -0.2483]
Target params: [1.1812, 0.2779]
Actual params: [-0.7653,  1.3313]
-Original Grad: 0.392, -lr * Pred Grad: 0.050, New P: -0.715
-Original Grad: -0.128, -lr * Pred Grad: -0.040, New P: 1.291
iter 0 loss: 0.620
Actual params: [-0.7154,  1.2913]
-Original Grad: 0.439, -lr * Pred Grad: 0.063, New P: -0.652
-Original Grad: -0.176, -lr * Pred Grad: -0.059, New P: 1.233
iter 1 loss: 0.598
Actual params: [-0.6525,  1.2326]
-Original Grad: 0.503, -lr * Pred Grad: 0.065, New P: -0.588
-Original Grad: -0.253, -lr * Pred Grad: -0.062, New P: 1.171
iter 2 loss: 0.628
Actual params: [-0.5875,  1.1708]
-Original Grad: 0.452, -lr * Pred Grad: 0.065, New P: -0.522
-Original Grad: -0.186, -lr * Pred Grad: -0.062, New P: 1.109
iter 3 loss: 0.548
Actual params: [-0.5224,  1.1086]
-Original Grad: 0.442, -lr * Pred Grad: 0.065, New P: -0.457
-Original Grad: -0.072, -lr * Pred Grad: -0.062, New P: 1.046
iter 4 loss: 0.553
Actual params: [-0.4571,  1.0464]
-Original Grad: 0.383, -lr * Pred Grad: 0.065, New P: -0.392
-Original Grad: -0.090, -lr * Pred Grad: -0.062, New P: 0.984
iter 5 loss: 0.472
Actual params: [-0.3919,  0.9841]
-Original Grad: 0.275, -lr * Pred Grad: 0.065, New P: -0.327
-Original Grad: 0.034, -lr * Pred Grad: -0.062, New P: 0.922
iter 6 loss: 0.406
Actual params: [-0.3267,  0.9218]
-Original Grad: 0.292, -lr * Pred Grad: 0.065, New P: -0.261
-Original Grad: -0.015, -lr * Pred Grad: -0.062, New P: 0.859
iter 7 loss: 0.408
Actual params: [-0.2614,  0.8595]
-Original Grad: 0.272, -lr * Pred Grad: 0.065, New P: -0.196
-Original Grad: -0.052, -lr * Pred Grad: -0.062, New P: 0.797
iter 8 loss: 0.424
Actual params: [-0.1962,  0.7972]
-Original Grad: 0.258, -lr * Pred Grad: 0.065, New P: -0.131
-Original Grad: -0.091, -lr * Pred Grad: -0.062, New P: 0.735
iter 9 loss: 0.421
Actual params: [-0.131 ,  0.7349]
-Original Grad: 0.142, -lr * Pred Grad: 0.065, New P: -0.066
-Original Grad: -0.277, -lr * Pred Grad: -0.062, New P: 0.673
iter 10 loss: 0.301
Actual params: [-0.0657,  0.6726]
-Original Grad: 0.160, -lr * Pred Grad: 0.065, New P: -0.001
-Original Grad: -0.357, -lr * Pred Grad: -0.062, New P: 0.610
iter 11 loss: 0.343
Actual params: [-5.0181e-04,  6.1033e-01]
-Original Grad: 0.132, -lr * Pred Grad: 0.065, New P: 0.065
-Original Grad: -0.297, -lr * Pred Grad: -0.062, New P: 0.548
iter 12 loss: 0.291
Actual params: [0.0647, 0.548 ]
-Original Grad: 0.095, -lr * Pred Grad: 0.065, New P: 0.130
-Original Grad: -0.232, -lr * Pred Grad: -0.062, New P: 0.486
iter 13 loss: 0.256
Actual params: [0.13  , 0.4858]
-Original Grad: 0.115, -lr * Pred Grad: 0.065, New P: 0.195
-Original Grad: -0.149, -lr * Pred Grad: -0.062, New P: 0.423
iter 14 loss: 0.241
Actual params: [0.1952, 0.4235]
-Original Grad: 0.107, -lr * Pred Grad: 0.065, New P: 0.260
-Original Grad: -0.163, -lr * Pred Grad: -0.062, New P: 0.361
iter 15 loss: 0.222
Actual params: [0.2604, 0.3612]
-Original Grad: 0.058, -lr * Pred Grad: 0.065, New P: 0.326
-Original Grad: -0.082, -lr * Pred Grad: -0.062, New P: 0.299
iter 16 loss: 0.160
Actual params: [0.3257, 0.2989]
-Original Grad: 0.029, -lr * Pred Grad: 0.065, New P: 0.391
-Original Grad: -0.143, -lr * Pred Grad: -0.062, New P: 0.237
iter 17 loss: 0.219
Actual params: [0.3909, 0.2366]
-Original Grad: -0.036, -lr * Pred Grad: 0.065, New P: 0.456
-Original Grad: -0.111, -lr * Pred Grad: -0.062, New P: 0.174
iter 18 loss: 0.221
Actual params: [0.4561, 0.1743]
-Original Grad: 0.036, -lr * Pred Grad: 0.065, New P: 0.521
-Original Grad: -0.106, -lr * Pred Grad: -0.062, New P: 0.112
iter 19 loss: 0.213
Actual params: [0.5214, 0.112 ]
-Original Grad: 0.099, -lr * Pred Grad: 0.065, New P: 0.587
-Original Grad: -0.079, -lr * Pred Grad: -0.062, New P: 0.050
iter 20 loss: 0.178
Actual params: [0.5866, 0.0497]
Target params: [1.1812, 0.2779]
Actual params: [ 0.3094, -0.0048]
-Original Grad: 0.215, -lr * Pred Grad: 0.050, New P: 0.359
-Original Grad: 0.055, -lr * Pred Grad: 0.039, New P: 0.034
iter 0 loss: 0.209
Actual params: [0.3589, 0.0344]
-Original Grad: 0.265, -lr * Pred Grad: 0.063, New P: 0.422
-Original Grad: 0.033, -lr * Pred Grad: 0.060, New P: 0.094
iter 1 loss: 0.184
Actual params: [0.4218, 0.0944]
-Original Grad: 0.377, -lr * Pred Grad: 0.065, New P: 0.487
-Original Grad: -0.045, -lr * Pred Grad: 0.057, New P: 0.151
iter 2 loss: 0.176
Actual params: [0.4867, 0.1513]
-Original Grad: 0.338, -lr * Pred Grad: 0.065, New P: 0.552
-Original Grad: 0.003, -lr * Pred Grad: 0.054, New P: 0.205
iter 3 loss: 0.133
Actual params: [0.5519, 0.2053]
-Original Grad: 0.241, -lr * Pred Grad: 0.065, New P: 0.617
-Original Grad: -0.061, -lr * Pred Grad: 0.042, New P: 0.248
iter 4 loss: 0.097
Actual params: [0.6171, 0.2477]
-Original Grad: 0.261, -lr * Pred Grad: 0.065, New P: 0.682
-Original Grad: -0.092, -lr * Pred Grad: 0.016, New P: 0.263
iter 5 loss: 0.103
Actual params: [0.6823, 0.2634]
-Original Grad: 0.275, -lr * Pred Grad: 0.065, New P: 0.748
-Original Grad: -0.066, -lr * Pred Grad: -0.031, New P: 0.232
iter 6 loss: 0.075
Actual params: [0.7476, 0.232 ]
-Original Grad: 0.282, -lr * Pred Grad: 0.065, New P: 0.813
-Original Grad: -0.030, -lr * Pred Grad: -0.052, New P: 0.180
iter 7 loss: 0.060
Actual params: [0.8128, 0.18  ]
-Original Grad: 0.330, -lr * Pred Grad: 0.065, New P: 0.878
-Original Grad: 0.004, -lr * Pred Grad: -0.059, New P: 0.121
iter 8 loss: 0.051
Actual params: [0.878 , 0.1208]
-Original Grad: 0.242, -lr * Pred Grad: 0.065, New P: 0.943
-Original Grad: 0.009, -lr * Pred Grad: -0.062, New P: 0.059
iter 9 loss: 0.029
Actual params: [0.9433, 0.0591]
-Original Grad: 0.068, -lr * Pred Grad: 0.065, New P: 1.008
-Original Grad: 0.086, -lr * Pred Grad: -0.062, New P: -0.003
iter 10 loss: 0.027
Actual params: [ 1.0085, -0.0031]
-Original Grad: 0.039, -lr * Pred Grad: 0.065, New P: 1.074
-Original Grad: 0.033, -lr * Pred Grad: -0.062, New P: -0.065
iter 11 loss: 0.017
Actual params: [ 1.0737, -0.0654]
-Original Grad: -0.003, -lr * Pred Grad: 0.065, New P: 1.139
-Original Grad: 0.019, -lr * Pred Grad: -0.062, New P: -0.128
iter 12 loss: 0.019
Actual params: [ 1.139 , -0.1277]
-Original Grad: -0.026, -lr * Pred Grad: 0.065, New P: 1.204
-Original Grad: 0.087, -lr * Pred Grad: -0.062, New P: -0.190
iter 13 loss: 0.044
Actual params: [ 1.2042, -0.19  ]
-Original Grad: -0.030, -lr * Pred Grad: 0.065, New P: 1.269
-Original Grad: 0.040, -lr * Pred Grad: -0.062, New P: -0.252
iter 14 loss: 0.033
Actual params: [ 1.2694, -0.2522]
-Original Grad: -0.047, -lr * Pred Grad: 0.065, New P: 1.335
-Original Grad: 0.023, -lr * Pred Grad: -0.062, New P: -0.315
iter 15 loss: 0.015
Actual params: [ 1.3347, -0.3145]
-Original Grad: -0.059, -lr * Pred Grad: 0.065, New P: 1.400
-Original Grad: 0.023, -lr * Pred Grad: -0.062, New P: -0.377
iter 16 loss: 0.044
Actual params: [ 1.3999, -0.3768]
-Original Grad: -0.042, -lr * Pred Grad: 0.065, New P: 1.465
-Original Grad: 0.027, -lr * Pred Grad: -0.062, New P: -0.439
iter 17 loss: 0.066
Actual params: [ 1.4651, -0.4391]
-Original Grad: -0.044, -lr * Pred Grad: 0.065, New P: 1.530
-Original Grad: 0.032, -lr * Pred Grad: -0.062, New P: -0.501
iter 18 loss: 0.038
Actual params: [ 1.5304, -0.5014]
-Original Grad: -0.044, -lr * Pred Grad: 0.065, New P: 1.596
-Original Grad: 0.071, -lr * Pred Grad: -0.062, New P: -0.564
iter 19 loss: 0.049
Actual params: [ 1.5956, -0.5637]
-Original Grad: -0.017, -lr * Pred Grad: 0.065, New P: 1.661
-Original Grad: 0.057, -lr * Pred Grad: -0.062, New P: -0.626
iter 20 loss: 0.072
Actual params: [ 1.6608, -0.626 ]
Target params: [1.1812, 0.2779]
Actual params: [1.7812, 1.0158]
-Original Grad: -0.050, -lr * Pred Grad: -0.016, New P: 1.766
-Original Grad: -0.141, -lr * Pred Grad: -0.041, New P: 0.974
iter 0 loss: 0.183
Actual params: [1.7655, 0.9744]
-Original Grad: -0.049, -lr * Pred Grad: -0.053, New P: 1.713
-Original Grad: -0.220, -lr * Pred Grad: -0.059, New P: 0.915
iter 1 loss: 0.163
Actual params: [1.7125, 0.9154]
-Original Grad: -0.051, -lr * Pred Grad: -0.061, New P: 1.652
-Original Grad: -0.398, -lr * Pred Grad: -0.062, New P: 0.854
iter 2 loss: 0.125
Actual params: [1.6516, 0.8536]
-Original Grad: -0.013, -lr * Pred Grad: -0.062, New P: 1.589
-Original Grad: -0.205, -lr * Pred Grad: -0.062, New P: 0.791
iter 3 loss: 0.056
Actual params: [1.5895, 0.7914]
-Original Grad: -0.024, -lr * Pred Grad: -0.062, New P: 1.527
-Original Grad: -0.333, -lr * Pred Grad: -0.062, New P: 0.729
iter 4 loss: 0.057
Actual params: [1.5272, 0.7291]
-Original Grad: 0.002, -lr * Pred Grad: -0.062, New P: 1.465
-Original Grad: -0.077, -lr * Pred Grad: -0.062, New P: 0.667
iter 5 loss: 0.031
Actual params: [1.4649, 0.6668]
-Original Grad: 0.001, -lr * Pred Grad: -0.062, New P: 1.403
-Original Grad: -0.109, -lr * Pred Grad: -0.062, New P: 0.605
iter 6 loss: 0.040
Actual params: [1.4027, 0.6045]
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: 1.340
-Original Grad: -0.050, -lr * Pred Grad: -0.062, New P: 0.542
iter 7 loss: 0.028
Actual params: [1.3404, 0.5422]
-Original Grad: 0.012, -lr * Pred Grad: -0.062, New P: 1.278
-Original Grad: -0.042, -lr * Pred Grad: -0.062, New P: 0.480
iter 8 loss: 0.029
Actual params: [1.2781, 0.4799]
-Original Grad: 0.026, -lr * Pred Grad: -0.062, New P: 1.216
-Original Grad: 0.026, -lr * Pred Grad: -0.062, New P: 0.418
iter 9 loss: 0.033
Actual params: [1.2158, 0.4177]
-Original Grad: 0.027, -lr * Pred Grad: -0.062, New P: 1.154
-Original Grad: 0.067, -lr * Pred Grad: -0.062, New P: 0.355
iter 10 loss: 0.037
Actual params: [1.1535, 0.3554]
-Original Grad: 0.018, -lr * Pred Grad: -0.062, New P: 1.091
-Original Grad: 0.061, -lr * Pred Grad: -0.062, New P: 0.293
iter 11 loss: 0.036
Actual params: [1.0912, 0.2931]
-Original Grad: 0.024, -lr * Pred Grad: -0.062, New P: 1.029
-Original Grad: 0.072, -lr * Pred Grad: -0.062, New P: 0.231
iter 12 loss: 0.046
Actual params: [1.0289, 0.2308]
-Original Grad: 0.024, -lr * Pred Grad: -0.062, New P: 0.967
-Original Grad: 0.061, -lr * Pred Grad: -0.062, New P: 0.169
iter 13 loss: 0.050
Actual params: [0.9666, 0.1685]
-Original Grad: 0.022, -lr * Pred Grad: -0.062, New P: 0.904
-Original Grad: 0.058, -lr * Pred Grad: -0.062, New P: 0.106
iter 14 loss: 0.051
Actual params: [0.9044, 0.1062]
-Original Grad: 0.024, -lr * Pred Grad: -0.062, New P: 0.842
-Original Grad: 0.048, -lr * Pred Grad: -0.062, New P: 0.044
iter 15 loss: 0.059
Actual params: [0.8421, 0.0439]
-Original Grad: 0.015, -lr * Pred Grad: -0.062, New P: 0.780
-Original Grad: 0.043, -lr * Pred Grad: -0.062, New P: -0.018
iter 16 loss: 0.065
Actual params: [ 0.7798, -0.0184]
-Original Grad: 0.008, -lr * Pred Grad: -0.062, New P: 0.717
-Original Grad: 0.013, -lr * Pred Grad: -0.062, New P: -0.081
iter 17 loss: 0.074
Actual params: [ 0.7175, -0.0806]
-Original Grad: -0.024, -lr * Pred Grad: -0.062, New P: 0.655
-Original Grad: -0.011, -lr * Pred Grad: -0.062, New P: -0.143
iter 18 loss: 0.080
Actual params: [ 0.6552, -0.1429]
-Original Grad: -0.035, -lr * Pred Grad: -0.062, New P: 0.593
-Original Grad: -0.010, -lr * Pred Grad: -0.062, New P: -0.205
iter 19 loss: 0.068
Actual params: [ 0.5929, -0.2052]
-Original Grad: -0.063, -lr * Pred Grad: -0.062, New P: 0.531
-Original Grad: -0.019, -lr * Pred Grad: -0.062, New P: -0.268
iter 20 loss: 0.066
Actual params: [ 0.5306, -0.2675]
