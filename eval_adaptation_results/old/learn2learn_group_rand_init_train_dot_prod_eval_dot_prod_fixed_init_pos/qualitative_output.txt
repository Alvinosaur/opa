Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.275, -lr * Pred Grad: 0.050, New P: 0.644
-Original Grad: -0.518, -lr * Pred Grad: -0.045, New P: 0.549
iter 0 loss: 0.351
Actual params: [0.6439, 0.5492]
-Original Grad: 0.360, -lr * Pred Grad: 0.063, New P: 0.707
-Original Grad: -0.488, -lr * Pred Grad: -0.060, New P: 0.490
iter 1 loss: 0.332
Actual params: [0.7068, 0.4896]
-Original Grad: 0.283, -lr * Pred Grad: 0.065, New P: 0.772
-Original Grad: -0.355, -lr * Pred Grad: -0.062, New P: 0.428
iter 2 loss: 0.305
Actual params: [0.7717, 0.4277]
-Original Grad: 0.293, -lr * Pred Grad: 0.065, New P: 0.837
-Original Grad: -0.351, -lr * Pred Grad: -0.062, New P: 0.366
iter 3 loss: 0.277
Actual params: [0.8369, 0.3655]
-Original Grad: 0.327, -lr * Pred Grad: 0.065, New P: 0.902
-Original Grad: 0.003, -lr * Pred Grad: -0.062, New P: 0.303
iter 4 loss: 0.240
Actual params: [0.9021, 0.3032]
-Original Grad: 0.363, -lr * Pred Grad: 0.065, New P: 0.967
-Original Grad: 0.075, -lr * Pred Grad: -0.062, New P: 0.241
iter 5 loss: 0.173
Actual params: [0.9673, 0.2409]
-Original Grad: 0.217, -lr * Pred Grad: 0.065, New P: 1.033
-Original Grad: 0.148, -lr * Pred Grad: -0.062, New P: 0.179
iter 6 loss: 0.085
Actual params: [1.0326, 0.1787]
-Original Grad: -0.009, -lr * Pred Grad: 0.065, New P: 1.098
-Original Grad: 0.182, -lr * Pred Grad: -0.062, New P: 0.116
iter 7 loss: 0.068
Actual params: [1.0978, 0.1164]
-Original Grad: 0.362, -lr * Pred Grad: 0.065, New P: 1.163
-Original Grad: 0.125, -lr * Pred Grad: -0.062, New P: 0.054
iter 8 loss: 0.087
Actual params: [1.163 , 0.0541]
-Original Grad: -0.082, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: 0.144, -lr * Pred Grad: -0.062, New P: -0.008
iter 9 loss: 0.132
Actual params: [ 1.2283, -0.0082]
-Original Grad: -0.348, -lr * Pred Grad: 0.065, New P: 1.294
-Original Grad: 0.827, -lr * Pred Grad: -0.062, New P: -0.070
iter 10 loss: 0.178
Actual params: [ 1.2935, -0.0705]
-Original Grad: -0.204, -lr * Pred Grad: 0.065, New P: 1.359
-Original Grad: 1.829, -lr * Pred Grad: -0.062, New P: -0.133
iter 11 loss: 0.226
Actual params: [ 1.3587, -0.1328]
-Original Grad: 0.211, -lr * Pred Grad: 0.065, New P: 1.424
-Original Grad: 1.332, -lr * Pred Grad: -0.062, New P: -0.195
iter 12 loss: 0.266
Actual params: [ 1.424 , -0.1951]
-Original Grad: 0.262, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: 1.145, -lr * Pred Grad: -0.062, New P: -0.257
iter 13 loss: 0.299
Actual params: [ 1.4892, -0.2574]
-Original Grad: 0.040, -lr * Pred Grad: 0.065, New P: 1.554
-Original Grad: 0.701, -lr * Pred Grad: -0.062, New P: -0.320
iter 14 loss: 0.323
Actual params: [ 1.5544, -0.3196]
-Original Grad: 0.148, -lr * Pred Grad: 0.065, New P: 1.620
-Original Grad: 0.772, -lr * Pred Grad: -0.062, New P: -0.382
iter 15 loss: 0.342
Actual params: [ 1.6197, -0.3819]
-Original Grad: 0.074, -lr * Pred Grad: 0.065, New P: 1.685
-Original Grad: 0.742, -lr * Pred Grad: -0.062, New P: -0.444
iter 16 loss: 0.356
Actual params: [ 1.6849, -0.4442]
-Original Grad: 0.080, -lr * Pred Grad: 0.065, New P: 1.750
-Original Grad: 0.715, -lr * Pred Grad: -0.062, New P: -0.507
iter 17 loss: 0.075
Actual params: [ 1.7501, -0.5065]
-Original Grad: 0.115, -lr * Pred Grad: 0.065, New P: 1.815
-Original Grad: 0.975, -lr * Pred Grad: -0.062, New P: -0.569
iter 18 loss: 0.100
Actual params: [ 1.8154, -0.5688]
-Original Grad: 0.062, -lr * Pred Grad: 0.065, New P: 1.881
-Original Grad: 0.951, -lr * Pred Grad: -0.062, New P: -0.631
iter 19 loss: 0.152
Actual params: [ 1.8806, -0.6311]
-Original Grad: 0.095, -lr * Pred Grad: 0.065, New P: 1.946
-Original Grad: 0.822, -lr * Pred Grad: -0.062, New P: -0.693
iter 20 loss: 0.209
Actual params: [ 1.9458, -0.6934]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.248, -lr * Pred Grad: 0.050, New P: 0.644
-Original Grad: -0.239, -lr * Pred Grad: -0.045, New P: 0.549
iter 0 loss: 0.223
Actual params: [0.6438, 0.5487]
-Original Grad: 0.104, -lr * Pred Grad: 0.063, New P: 0.707
-Original Grad: -0.135, -lr * Pred Grad: -0.060, New P: 0.489
iter 1 loss: 0.180
Actual params: [0.7067, 0.489 ]
-Original Grad: 0.073, -lr * Pred Grad: 0.065, New P: 0.772
-Original Grad: -0.027, -lr * Pred Grad: -0.062, New P: 0.427
iter 2 loss: 0.133
Actual params: [0.7716, 0.4271]
-Original Grad: 0.034, -lr * Pred Grad: 0.065, New P: 0.837
-Original Grad: -0.022, -lr * Pred Grad: -0.062, New P: 0.365
iter 3 loss: 0.105
Actual params: [0.8368, 0.3648]
-Original Grad: 0.007, -lr * Pred Grad: 0.065, New P: 0.902
-Original Grad: 0.003, -lr * Pred Grad: -0.062, New P: 0.303
iter 4 loss: 0.092
Actual params: [0.902 , 0.3026]
-Original Grad: 0.013, -lr * Pred Grad: 0.065, New P: 0.967
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: 0.240
iter 5 loss: 0.093
Actual params: [0.9672, 0.2403]
-Original Grad: 0.031, -lr * Pred Grad: 0.065, New P: 1.032
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: 0.178
iter 6 loss: 0.084
Actual params: [1.0325, 0.178 ]
-Original Grad: 0.018, -lr * Pred Grad: 0.065, New P: 1.098
-Original Grad: 0.031, -lr * Pred Grad: -0.062, New P: 0.116
iter 7 loss: 0.078
Actual params: [1.0977, 0.1157]
-Original Grad: 0.011, -lr * Pred Grad: 0.065, New P: 1.163
-Original Grad: 0.014, -lr * Pred Grad: -0.062, New P: 0.053
iter 8 loss: 0.075
Actual params: [1.1629, 0.0534]
-Original Grad: 0.007, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: 0.014, -lr * Pred Grad: -0.062, New P: -0.009
iter 9 loss: 0.073
Actual params: [ 1.2282, -0.0089]
-Original Grad: 0.001, -lr * Pred Grad: 0.065, New P: 1.293
-Original Grad: 0.012, -lr * Pred Grad: -0.062, New P: -0.071
iter 10 loss: 0.071
Actual params: [ 1.2934, -0.0712]
-Original Grad: 0.002, -lr * Pred Grad: 0.065, New P: 1.359
-Original Grad: 0.012, -lr * Pred Grad: -0.062, New P: -0.133
iter 11 loss: 0.072
Actual params: [ 1.3586, -0.1335]
-Original Grad: 0.003, -lr * Pred Grad: 0.065, New P: 1.424
-Original Grad: 0.007, -lr * Pred Grad: -0.062, New P: -0.196
iter 12 loss: 0.073
Actual params: [ 1.4239, -0.1957]
-Original Grad: -0.002, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: 0.003, -lr * Pred Grad: -0.062, New P: -0.258
iter 13 loss: 0.074
Actual params: [ 1.4891, -0.258 ]
-Original Grad: -0.003, -lr * Pred Grad: 0.065, New P: 1.554
-Original Grad: 0.008, -lr * Pred Grad: -0.062, New P: -0.320
iter 14 loss: 0.074
Actual params: [ 1.5543, -0.3203]
-Original Grad: -0.010, -lr * Pred Grad: 0.065, New P: 1.620
-Original Grad: 0.013, -lr * Pred Grad: -0.062, New P: -0.383
iter 15 loss: 0.076
Actual params: [ 1.6196, -0.3826]
-Original Grad: -0.012, -lr * Pred Grad: 0.065, New P: 1.685
-Original Grad: 0.013, -lr * Pred Grad: -0.062, New P: -0.445
iter 16 loss: 0.080
Actual params: [ 1.6848, -0.4449]
-Original Grad: -0.020, -lr * Pred Grad: 0.065, New P: 1.750
-Original Grad: 0.016, -lr * Pred Grad: -0.062, New P: -0.507
iter 17 loss: 0.086
Actual params: [ 1.75  , -0.5072]
-Original Grad: -0.019, -lr * Pred Grad: 0.065, New P: 1.815
-Original Grad: 0.009, -lr * Pred Grad: -0.062, New P: -0.569
iter 18 loss: 0.093
Actual params: [ 1.8153, -0.5695]
-Original Grad: -0.025, -lr * Pred Grad: 0.065, New P: 1.880
-Original Grad: 0.013, -lr * Pred Grad: -0.062, New P: -0.632
iter 19 loss: 0.101
Actual params: [ 1.8805, -0.6318]
-Original Grad: -0.032, -lr * Pred Grad: 0.065, New P: 1.946
-Original Grad: 0.013, -lr * Pred Grad: -0.062, New P: -0.694
iter 20 loss: 0.108
Actual params: [ 1.9457, -0.6941]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.691, -lr * Pred Grad: 0.050, New P: 0.644
-Original Grad: 0.118, -lr * Pred Grad: 0.047, New P: 0.641
iter 0 loss: 0.342
Actual params: [0.6441, 0.6413]
-Original Grad: 0.827, -lr * Pred Grad: 0.063, New P: 0.707
-Original Grad: 0.087, -lr * Pred Grad: 0.062, New P: 0.704
iter 1 loss: 0.325
Actual params: [0.707 , 0.7037]
-Original Grad: 0.665, -lr * Pred Grad: 0.065, New P: 0.772
-Original Grad: -0.013, -lr * Pred Grad: 0.065, New P: 0.769
iter 2 loss: 0.268
Actual params: [0.7719, 0.7685]
-Original Grad: 0.478, -lr * Pred Grad: 0.065, New P: 0.837
-Original Grad: -0.090, -lr * Pred Grad: 0.065, New P: 0.834
iter 3 loss: 0.186
Actual params: [0.8371, 0.8337]
-Original Grad: 0.060, -lr * Pred Grad: 0.065, New P: 0.902
-Original Grad: -0.166, -lr * Pred Grad: 0.065, New P: 0.899
iter 4 loss: 0.068
Actual params: [0.9023, 0.8989]
-Original Grad: -0.316, -lr * Pred Grad: 0.065, New P: 0.968
-Original Grad: -0.118, -lr * Pred Grad: 0.065, New P: 0.964
iter 5 loss: 0.161
Actual params: [0.9676, 0.9642]
-Original Grad: -0.731, -lr * Pred Grad: 0.065, New P: 1.033
-Original Grad: -0.114, -lr * Pred Grad: 0.065, New P: 1.029
iter 6 loss: 0.382
Actual params: [1.0328, 1.0294]
-Original Grad: -0.389, -lr * Pred Grad: 0.065, New P: 1.098
-Original Grad: -0.088, -lr * Pred Grad: 0.065, New P: 1.095
iter 7 loss: 0.581
Actual params: [1.098 , 1.0946]
-Original Grad: -0.245, -lr * Pred Grad: 0.065, New P: 1.163
-Original Grad: -0.021, -lr * Pred Grad: 0.065, New P: 1.160
iter 8 loss: 0.695
Actual params: [1.1632, 1.1598]
-Original Grad: -0.203, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: 0.039, -lr * Pred Grad: 0.065, New P: 1.225
iter 9 loss: 0.720
Actual params: [1.2285, 1.2251]
-Original Grad: -0.041, -lr * Pred Grad: 0.065, New P: 1.294
-Original Grad: -0.012, -lr * Pred Grad: 0.065, New P: 1.290
iter 10 loss: 0.683
Actual params: [1.2937, 1.2903]
-Original Grad: 0.004, -lr * Pred Grad: 0.065, New P: 1.359
-Original Grad: 0.026, -lr * Pred Grad: 0.065, New P: 1.356
iter 11 loss: 0.634
Actual params: [1.3589, 1.3555]
-Original Grad: 0.079, -lr * Pred Grad: 0.065, New P: 1.424
-Original Grad: 0.030, -lr * Pred Grad: 0.065, New P: 1.421
iter 12 loss: 0.572
Actual params: [1.4242, 1.4207]
-Original Grad: 0.261, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: -0.017, -lr * Pred Grad: 0.065, New P: 1.486
iter 13 loss: 0.476
Actual params: [1.4894, 1.486 ]
-Original Grad: 0.510, -lr * Pred Grad: 0.065, New P: 1.555
-Original Grad: 0.056, -lr * Pred Grad: 0.065, New P: 1.551
iter 14 loss: 0.367
Actual params: [1.5546, 1.5512]
-Original Grad: 0.313, -lr * Pred Grad: 0.065, New P: 1.620
-Original Grad: -0.022, -lr * Pred Grad: 0.065, New P: 1.616
iter 15 loss: 0.283
Actual params: [1.6199, 1.6164]
-Original Grad: 0.295, -lr * Pred Grad: 0.065, New P: 1.685
-Original Grad: -0.039, -lr * Pred Grad: 0.065, New P: 1.682
iter 16 loss: 0.232
Actual params: [1.6851, 1.6816]
-Original Grad: 0.223, -lr * Pred Grad: 0.065, New P: 1.750
-Original Grad: 0.001, -lr * Pred Grad: 0.065, New P: 1.747
iter 17 loss: 0.205
Actual params: [1.7503, 1.7468]
-Original Grad: 0.224, -lr * Pred Grad: 0.065, New P: 1.816
-Original Grad: -0.018, -lr * Pred Grad: 0.065, New P: 1.812
iter 18 loss: 0.189
Actual params: [1.8156, 1.812 ]
-Original Grad: 0.127, -lr * Pred Grad: 0.065, New P: 1.881
-Original Grad: -0.043, -lr * Pred Grad: 0.065, New P: 1.877
iter 19 loss: 0.181
Actual params: [1.8808, 1.8772]
-Original Grad: 0.099, -lr * Pred Grad: 0.065, New P: 1.946
-Original Grad: -0.041, -lr * Pred Grad: 0.065, New P: 1.942
iter 20 loss: 0.178
Actual params: [1.946 , 1.9423]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.165, -lr * Pred Grad: 0.049, New P: 0.643
-Original Grad: -0.136, -lr * Pred Grad: -0.041, New P: 0.553
iter 0 loss: 0.156
Actual params: [0.643 , 0.5533]
-Original Grad: 0.086, -lr * Pred Grad: 0.063, New P: 0.706
-Original Grad: -0.054, -lr * Pred Grad: -0.059, New P: 0.494
iter 1 loss: 0.115
Actual params: [0.7057, 0.4944]
-Original Grad: 0.028, -lr * Pred Grad: 0.065, New P: 0.771
-Original Grad: -0.026, -lr * Pred Grad: -0.062, New P: 0.433
iter 2 loss: 0.092
Actual params: [0.7706, 0.4326]
-Original Grad: 0.014, -lr * Pred Grad: 0.065, New P: 0.836
-Original Grad: -0.005, -lr * Pred Grad: -0.062, New P: 0.370
iter 3 loss: 0.086
Actual params: [0.8358, 0.3704]
-Original Grad: 0.001, -lr * Pred Grad: 0.065, New P: 0.901
-Original Grad: 0.016, -lr * Pred Grad: -0.062, New P: 0.308
iter 4 loss: 0.102
Actual params: [0.901 , 0.3081]
-Original Grad: 0.002, -lr * Pred Grad: 0.065, New P: 0.966
-Original Grad: 0.028, -lr * Pred Grad: -0.062, New P: 0.246
iter 5 loss: 0.107
Actual params: [0.9662, 0.2458]
-Original Grad: 0.025, -lr * Pred Grad: 0.065, New P: 1.031
-Original Grad: 0.040, -lr * Pred Grad: -0.062, New P: 0.184
iter 6 loss: 0.105
Actual params: [1.0315, 0.1835]
-Original Grad: 0.025, -lr * Pred Grad: 0.065, New P: 1.097
-Original Grad: 0.038, -lr * Pred Grad: -0.062, New P: 0.121
iter 7 loss: 0.101
Actual params: [1.0967, 0.1212]
-Original Grad: 0.008, -lr * Pred Grad: 0.065, New P: 1.162
-Original Grad: 0.031, -lr * Pred Grad: -0.062, New P: 0.059
iter 8 loss: 0.100
Actual params: [1.1619, 0.0589]
-Original Grad: 0.006, -lr * Pred Grad: 0.065, New P: 1.227
-Original Grad: 0.022, -lr * Pred Grad: -0.062, New P: -0.003
iter 9 loss: 0.102
Actual params: [ 1.2272, -0.0033]
-Original Grad: 0.014, -lr * Pred Grad: 0.065, New P: 1.292
-Original Grad: 0.025, -lr * Pred Grad: -0.062, New P: -0.066
iter 10 loss: 0.102
Actual params: [ 1.2924, -0.0656]
-Original Grad: 0.014, -lr * Pred Grad: 0.065, New P: 1.358
-Original Grad: 0.047, -lr * Pred Grad: -0.062, New P: -0.128
iter 11 loss: 0.101
Actual params: [ 1.3576, -0.1279]
-Original Grad: 0.011, -lr * Pred Grad: 0.065, New P: 1.423
-Original Grad: 0.046, -lr * Pred Grad: -0.062, New P: -0.190
iter 12 loss: 0.103
Actual params: [ 1.4229, -0.1902]
-Original Grad: 0.012, -lr * Pred Grad: 0.065, New P: 1.488
-Original Grad: 0.072, -lr * Pred Grad: -0.062, New P: -0.252
iter 13 loss: 0.102
Actual params: [ 1.4881, -0.2525]
-Original Grad: 0.009, -lr * Pred Grad: 0.065, New P: 1.553
-Original Grad: 0.042, -lr * Pred Grad: -0.062, New P: -0.315
iter 14 loss: 0.101
Actual params: [ 1.5533, -0.3148]
-Original Grad: 0.010, -lr * Pred Grad: 0.065, New P: 1.619
-Original Grad: 0.079, -lr * Pred Grad: -0.062, New P: -0.377
iter 15 loss: 0.101
Actual params: [ 1.6186, -0.3771]
-Original Grad: 0.007, -lr * Pred Grad: 0.065, New P: 1.684
-Original Grad: 0.052, -lr * Pred Grad: -0.062, New P: -0.439
iter 16 loss: 0.101
Actual params: [ 1.6838, -0.4394]
-Original Grad: 0.015, -lr * Pred Grad: 0.065, New P: 1.749
-Original Grad: 0.116, -lr * Pred Grad: -0.062, New P: -0.502
iter 17 loss: 0.101
Actual params: [ 1.749 , -0.5016]
-Original Grad: 0.009, -lr * Pred Grad: 0.065, New P: 1.814
-Original Grad: 0.101, -lr * Pred Grad: -0.062, New P: -0.564
iter 18 loss: 0.102
Actual params: [ 1.8143, -0.5639]
-Original Grad: 0.007, -lr * Pred Grad: 0.065, New P: 1.879
-Original Grad: 0.165, -lr * Pred Grad: -0.062, New P: -0.626
iter 19 loss: 0.104
Actual params: [ 1.8795, -0.6262]
-Original Grad: 0.005, -lr * Pred Grad: 0.065, New P: 1.945
-Original Grad: 0.218, -lr * Pred Grad: -0.062, New P: -0.689
iter 20 loss: 0.106
Actual params: [ 1.9447, -0.6885]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.212, -lr * Pred Grad: 0.050, New P: 0.644
-Original Grad: -0.322, -lr * Pred Grad: -0.046, New P: 0.548
iter 0 loss: 0.269
Actual params: [0.6436, 0.5484]
-Original Grad: 0.305, -lr * Pred Grad: 0.063, New P: 0.706
-Original Grad: -0.336, -lr * Pred Grad: -0.060, New P: 0.489
iter 1 loss: 0.229
Actual params: [0.7064, 0.4886]
-Original Grad: 0.296, -lr * Pred Grad: 0.065, New P: 0.771
-Original Grad: -0.218, -lr * Pred Grad: -0.062, New P: 0.427
iter 2 loss: 0.174
Actual params: [0.7713, 0.4267]
-Original Grad: 0.187, -lr * Pred Grad: 0.065, New P: 0.837
-Original Grad: -0.143, -lr * Pred Grad: -0.062, New P: 0.364
iter 3 loss: 0.121
Actual params: [0.8365, 0.3645]
-Original Grad: 0.023, -lr * Pred Grad: 0.065, New P: 0.902
-Original Grad: -0.029, -lr * Pred Grad: -0.062, New P: 0.302
iter 4 loss: 0.085
Actual params: [0.9018, 0.3022]
-Original Grad: -0.003, -lr * Pred Grad: 0.065, New P: 0.967
-Original Grad: 0.060, -lr * Pred Grad: -0.062, New P: 0.240
iter 5 loss: 0.074
Actual params: [0.967 , 0.2399]
-Original Grad: 0.075, -lr * Pred Grad: 0.065, New P: 1.032
-Original Grad: 0.104, -lr * Pred Grad: -0.062, New P: 0.178
iter 6 loss: 0.070
Actual params: [1.0322, 0.1776]
-Original Grad: 0.103, -lr * Pred Grad: 0.065, New P: 1.097
-Original Grad: 0.090, -lr * Pred Grad: -0.062, New P: 0.115
iter 7 loss: 0.053
Actual params: [1.0975, 0.1153]
-Original Grad: 0.053, -lr * Pred Grad: 0.065, New P: 1.163
-Original Grad: 0.057, -lr * Pred Grad: -0.062, New P: 0.053
iter 8 loss: 0.048
Actual params: [1.1627, 0.0531]
-Original Grad: 0.039, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: 0.057, -lr * Pred Grad: -0.062, New P: -0.009
iter 9 loss: 0.044
Actual params: [ 1.2279, -0.0092]
-Original Grad: 0.026, -lr * Pred Grad: 0.065, New P: 1.293
-Original Grad: 0.045, -lr * Pred Grad: -0.062, New P: -0.072
iter 10 loss: 0.042
Actual params: [ 1.2932, -0.0715]
-Original Grad: 0.025, -lr * Pred Grad: 0.065, New P: 1.358
-Original Grad: 0.057, -lr * Pred Grad: -0.062, New P: -0.134
iter 11 loss: 0.038
Actual params: [ 1.3584, -0.1338]
-Original Grad: 0.026, -lr * Pred Grad: 0.065, New P: 1.424
-Original Grad: 0.048, -lr * Pred Grad: -0.062, New P: -0.196
iter 12 loss: 0.034
Actual params: [ 1.4236, -0.1961]
-Original Grad: 0.033, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: 0.066, -lr * Pred Grad: -0.062, New P: -0.258
iter 13 loss: 0.030
Actual params: [ 1.4889, -0.2584]
-Original Grad: 0.031, -lr * Pred Grad: 0.065, New P: 1.554
-Original Grad: 0.041, -lr * Pred Grad: -0.062, New P: -0.321
iter 14 loss: 0.027
Actual params: [ 1.5541, -0.3207]
-Original Grad: 0.038, -lr * Pred Grad: 0.065, New P: 1.619
-Original Grad: 0.080, -lr * Pred Grad: -0.062, New P: -0.383
iter 15 loss: 0.024
Actual params: [ 1.6193, -0.383 ]
-Original Grad: 0.039, -lr * Pred Grad: 0.065, New P: 1.685
-Original Grad: 0.189, -lr * Pred Grad: -0.062, New P: -0.445
iter 16 loss: 0.021
Actual params: [ 1.6846, -0.4453]
-Original Grad: 0.024, -lr * Pred Grad: 0.065, New P: 1.750
-Original Grad: 0.108, -lr * Pred Grad: -0.062, New P: -0.508
iter 17 loss: 0.020
Actual params: [ 1.7498, -0.5075]
-Original Grad: -0.001, -lr * Pred Grad: 0.065, New P: 1.815
-Original Grad: 0.126, -lr * Pred Grad: -0.062, New P: -0.570
iter 18 loss: 0.024
Actual params: [ 1.815 , -0.5698]
-Original Grad: 0.011, -lr * Pred Grad: 0.065, New P: 1.880
-Original Grad: 0.332, -lr * Pred Grad: -0.062, New P: -0.632
iter 19 loss: 0.031
Actual params: [ 1.8802, -0.6321]
-Original Grad: -0.047, -lr * Pred Grad: 0.065, New P: 1.945
-Original Grad: 0.229, -lr * Pred Grad: -0.062, New P: -0.694
iter 20 loss: 0.037
Actual params: [ 1.9455, -0.6944]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.043, -lr * Pred Grad: 0.036, New P: 0.630
-Original Grad: -0.044, -lr * Pred Grad: -0.012, New P: 0.582
iter 0 loss: 0.074
Actual params: [0.63  , 0.5824]
-Original Grad: 0.036, -lr * Pred Grad: 0.057, New P: 0.687
-Original Grad: -0.055, -lr * Pred Grad: -0.052, New P: 0.531
iter 1 loss: 0.047
Actual params: [0.6873, 0.5306]
-Original Grad: -0.009, -lr * Pred Grad: 0.053, New P: 0.740
-Original Grad: -0.036, -lr * Pred Grad: -0.061, New P: 0.470
iter 2 loss: 0.057
Actual params: [0.7401, 0.4699]
-Original Grad: -0.026, -lr * Pred Grad: 0.045, New P: 0.785
-Original Grad: -0.009, -lr * Pred Grad: -0.062, New P: 0.408
iter 3 loss: 0.093
Actual params: [0.7855, 0.4078]
-Original Grad: -0.019, -lr * Pred Grad: 0.020, New P: 0.805
-Original Grad: -0.007, -lr * Pred Grad: -0.062, New P: 0.346
iter 4 loss: 0.089
Actual params: [0.805 , 0.3455]
-Original Grad: -0.005, -lr * Pred Grad: -0.029, New P: 0.776
-Original Grad: 0.008, -lr * Pred Grad: -0.062, New P: 0.283
iter 5 loss: 0.071
Actual params: [0.7757, 0.2832]
-Original Grad: -0.006, -lr * Pred Grad: -0.054, New P: 0.722
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: 0.221
iter 6 loss: 0.055
Actual params: [0.7221, 0.221 ]
-Original Grad: -0.004, -lr * Pred Grad: -0.061, New P: 0.662
-Original Grad: 0.013, -lr * Pred Grad: -0.062, New P: 0.159
iter 7 loss: 0.045
Actual params: [0.6615, 0.1587]
-Original Grad: -0.001, -lr * Pred Grad: -0.062, New P: 0.599
-Original Grad: 0.010, -lr * Pred Grad: -0.062, New P: 0.096
iter 8 loss: 0.042
Actual params: [0.5995, 0.0964]
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: 0.537
-Original Grad: 0.008, -lr * Pred Grad: -0.062, New P: 0.034
iter 9 loss: 0.034
Actual params: [0.5372, 0.0341]
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: 0.475
-Original Grad: 0.014, -lr * Pred Grad: -0.062, New P: -0.028
iter 10 loss: 0.033
Actual params: [ 0.475 , -0.0282]
-Original Grad: 0.031, -lr * Pred Grad: -0.062, New P: 0.413
-Original Grad: 0.015, -lr * Pred Grad: -0.062, New P: -0.090
iter 11 loss: 0.046
Actual params: [ 0.4127, -0.0905]
-Original Grad: 0.045, -lr * Pred Grad: -0.062, New P: 0.350
-Original Grad: 0.019, -lr * Pred Grad: -0.062, New P: -0.153
iter 12 loss: 0.066
Actual params: [ 0.3504, -0.1528]
-Original Grad: 0.043, -lr * Pred Grad: -0.062, New P: 0.288
-Original Grad: 0.017, -lr * Pred Grad: -0.062, New P: -0.215
iter 13 loss: 0.083
Actual params: [ 0.2881, -0.2151]
-Original Grad: 0.043, -lr * Pred Grad: -0.062, New P: 0.226
-Original Grad: 0.010, -lr * Pred Grad: -0.062, New P: -0.277
iter 14 loss: 0.098
Actual params: [ 0.2258, -0.2773]
-Original Grad: 0.042, -lr * Pred Grad: -0.062, New P: 0.164
-Original Grad: 0.002, -lr * Pred Grad: -0.062, New P: -0.340
iter 15 loss: 0.110
Actual params: [ 0.1635, -0.3396]
-Original Grad: 0.048, -lr * Pred Grad: -0.062, New P: 0.101
-Original Grad: -0.011, -lr * Pred Grad: -0.062, New P: -0.402
iter 16 loss: 0.123
Actual params: [ 0.1012, -0.4019]
-Original Grad: 0.053, -lr * Pred Grad: -0.062, New P: 0.039
-Original Grad: -0.005, -lr * Pred Grad: -0.062, New P: -0.464
iter 17 loss: 0.136
Actual params: [ 0.0389, -0.4642]
-Original Grad: 0.058, -lr * Pred Grad: -0.062, New P: -0.023
-Original Grad: -0.020, -lr * Pred Grad: -0.062, New P: -0.526
iter 18 loss: 0.147
Actual params: [-0.0234, -0.5265]
-Original Grad: 0.058, -lr * Pred Grad: -0.062, New P: -0.086
-Original Grad: -0.006, -lr * Pred Grad: -0.062, New P: -0.589
iter 19 loss: 0.157
Actual params: [-0.0856, -0.5888]
-Original Grad: 0.033, -lr * Pred Grad: -0.062, New P: -0.148
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -0.651
iter 20 loss: 0.169
Actual params: [-0.1479, -0.6511]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.646, -lr * Pred Grad: 0.050, New P: 0.644
-Original Grad: -0.406, -lr * Pred Grad: -0.045, New P: 0.549
iter 0 loss: 0.571
Actual params: [0.6441, 0.5486]
-Original Grad: 1.016, -lr * Pred Grad: 0.063, New P: 0.707
-Original Grad: -0.404, -lr * Pred Grad: -0.060, New P: 0.489
iter 1 loss: 0.532
Actual params: [0.707 , 0.4889]
-Original Grad: 0.696, -lr * Pred Grad: 0.065, New P: 0.772
-Original Grad: -0.201, -lr * Pred Grad: -0.062, New P: 0.427
iter 2 loss: 0.461
Actual params: [0.7719, 0.427 ]
-Original Grad: 0.394, -lr * Pred Grad: 0.065, New P: 0.837
-Original Grad: -0.090, -lr * Pred Grad: -0.062, New P: 0.365
iter 3 loss: 0.381
Actual params: [0.8371, 0.3648]
-Original Grad: 0.231, -lr * Pred Grad: 0.065, New P: 0.902
-Original Grad: -0.032, -lr * Pred Grad: -0.062, New P: 0.303
iter 4 loss: 0.331
Actual params: [0.9023, 0.3025]
-Original Grad: 0.139, -lr * Pred Grad: 0.065, New P: 0.968
-Original Grad: 0.006, -lr * Pred Grad: -0.062, New P: 0.240
iter 5 loss: 0.292
Actual params: [0.9675, 0.2402]
-Original Grad: 0.069, -lr * Pred Grad: 0.065, New P: 1.033
-Original Grad: 0.014, -lr * Pred Grad: -0.062, New P: 0.178
iter 6 loss: 0.269
Actual params: [1.0328, 0.1779]
-Original Grad: 0.079, -lr * Pred Grad: 0.065, New P: 1.098
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: 0.116
iter 7 loss: 0.251
Actual params: [1.098 , 0.1156]
-Original Grad: 0.021, -lr * Pred Grad: 0.065, New P: 1.163
-Original Grad: 0.016, -lr * Pred Grad: -0.062, New P: 0.053
iter 8 loss: 0.245
Actual params: [1.1632, 0.0534]
-Original Grad: 0.003, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: 0.018, -lr * Pred Grad: -0.062, New P: -0.009
iter 9 loss: 0.232
Actual params: [ 1.2285, -0.0089]
-Original Grad: -0.003, -lr * Pred Grad: 0.065, New P: 1.294
-Original Grad: 0.016, -lr * Pred Grad: -0.062, New P: -0.071
iter 10 loss: 0.220
Actual params: [ 1.2937, -0.0712]
-Original Grad: 0.003, -lr * Pred Grad: 0.065, New P: 1.359
-Original Grad: 0.006, -lr * Pred Grad: -0.062, New P: -0.134
iter 11 loss: 0.219
Actual params: [ 1.3589, -0.1335]
-Original Grad: -0.007, -lr * Pred Grad: 0.065, New P: 1.424
-Original Grad: 0.044, -lr * Pred Grad: -0.062, New P: -0.196
iter 12 loss: 0.212
Actual params: [ 1.4242, -0.1958]
-Original Grad: 0.002, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: 0.030, -lr * Pred Grad: -0.062, New P: -0.258
iter 13 loss: 0.205
Actual params: [ 1.4894, -0.2581]
-Original Grad: 0.004, -lr * Pred Grad: 0.065, New P: 1.555
-Original Grad: 0.045, -lr * Pred Grad: -0.062, New P: -0.320
iter 14 loss: 0.197
Actual params: [ 1.5546, -0.3204]
-Original Grad: -0.006, -lr * Pred Grad: 0.065, New P: 1.620
-Original Grad: 0.060, -lr * Pred Grad: -0.062, New P: -0.383
iter 15 loss: 0.189
Actual params: [ 1.6199, -0.3827]
-Original Grad: -0.001, -lr * Pred Grad: 0.065, New P: 1.685
-Original Grad: 0.073, -lr * Pred Grad: -0.062, New P: -0.445
iter 16 loss: 0.179
Actual params: [ 1.6851, -0.4449]
-Original Grad: 0.000, -lr * Pred Grad: 0.065, New P: 1.750
-Original Grad: 0.096, -lr * Pred Grad: -0.062, New P: -0.507
iter 17 loss: 0.174
Actual params: [ 1.7503, -0.5072]
-Original Grad: 0.012, -lr * Pred Grad: 0.065, New P: 1.816
-Original Grad: 0.123, -lr * Pred Grad: -0.062, New P: -0.570
iter 18 loss: 0.159
Actual params: [ 1.8156, -0.5695]
-Original Grad: 0.004, -lr * Pred Grad: 0.065, New P: 1.881
-Original Grad: 0.147, -lr * Pred Grad: -0.062, New P: -0.632
iter 19 loss: 0.148
Actual params: [ 1.8808, -0.6318]
-Original Grad: 0.006, -lr * Pred Grad: 0.065, New P: 1.946
-Original Grad: 0.153, -lr * Pred Grad: -0.062, New P: -0.694
iter 20 loss: 0.137
Actual params: [ 1.946 , -0.6941]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.173, -lr * Pred Grad: 0.049, New P: 0.643
-Original Grad: -0.285, -lr * Pred Grad: -0.046, New P: 0.548
iter 0 loss: 0.526
Actual params: [0.6431, 0.5484]
-Original Grad: 0.102, -lr * Pred Grad: 0.063, New P: 0.706
-Original Grad: -0.266, -lr * Pred Grad: -0.060, New P: 0.489
iter 1 loss: 0.516
Actual params: [0.7059, 0.4887]
-Original Grad: 0.246, -lr * Pred Grad: 0.065, New P: 0.771
-Original Grad: -0.191, -lr * Pred Grad: -0.062, New P: 0.427
iter 2 loss: 0.497
Actual params: [0.7708, 0.4267]
-Original Grad: 0.137, -lr * Pred Grad: 0.065, New P: 0.836
-Original Grad: -0.260, -lr * Pred Grad: -0.062, New P: 0.365
iter 3 loss: 0.472
Actual params: [0.836 , 0.3645]
-Original Grad: 0.122, -lr * Pred Grad: 0.065, New P: 0.901
-Original Grad: -0.174, -lr * Pred Grad: -0.062, New P: 0.302
iter 4 loss: 0.450
Actual params: [0.9012, 0.3022]
-Original Grad: 0.072, -lr * Pred Grad: 0.065, New P: 0.966
-Original Grad: -0.164, -lr * Pred Grad: -0.062, New P: 0.240
iter 5 loss: 0.433
Actual params: [0.9664, 0.2399]
-Original Grad: 0.094, -lr * Pred Grad: 0.065, New P: 1.032
-Original Grad: -0.116, -lr * Pred Grad: -0.062, New P: 0.178
iter 6 loss: 0.415
Actual params: [1.0317, 0.1776]
-Original Grad: 0.064, -lr * Pred Grad: 0.065, New P: 1.097
-Original Grad: -0.101, -lr * Pred Grad: -0.062, New P: 0.115
iter 7 loss: 0.409
Actual params: [1.0969, 0.1154]
-Original Grad: 0.089, -lr * Pred Grad: 0.065, New P: 1.162
-Original Grad: -0.134, -lr * Pred Grad: -0.062, New P: 0.053
iter 8 loss: 0.397
Actual params: [1.1621, 0.0531]
-Original Grad: 0.069, -lr * Pred Grad: 0.065, New P: 1.227
-Original Grad: -0.066, -lr * Pred Grad: -0.062, New P: -0.009
iter 9 loss: 0.375
Actual params: [ 1.2273, -0.0092]
-Original Grad: 0.041, -lr * Pred Grad: 0.065, New P: 1.293
-Original Grad: -0.028, -lr * Pred Grad: -0.062, New P: -0.072
iter 10 loss: 0.347
Actual params: [ 1.2926, -0.0715]
-Original Grad: 0.042, -lr * Pred Grad: 0.065, New P: 1.358
-Original Grad: -0.045, -lr * Pred Grad: -0.062, New P: -0.134
iter 11 loss: 0.321
Actual params: [ 1.3578, -0.1338]
-Original Grad: 0.035, -lr * Pred Grad: 0.065, New P: 1.423
-Original Grad: -0.037, -lr * Pred Grad: -0.062, New P: -0.196
iter 12 loss: 0.306
Actual params: [ 1.423 , -0.1961]
-Original Grad: 0.032, -lr * Pred Grad: 0.065, New P: 1.488
-Original Grad: -0.067, -lr * Pred Grad: -0.062, New P: -0.258
iter 13 loss: 0.273
Actual params: [ 1.4883, -0.2584]
-Original Grad: 0.020, -lr * Pred Grad: 0.065, New P: 1.554
-Original Grad: -0.033, -lr * Pred Grad: -0.062, New P: -0.321
iter 14 loss: 0.254
Actual params: [ 1.5535, -0.3207]
-Original Grad: 0.007, -lr * Pred Grad: 0.065, New P: 1.619
-Original Grad: -0.052, -lr * Pred Grad: -0.062, New P: -0.383
iter 15 loss: 0.231
Actual params: [ 1.6187, -0.3829]
-Original Grad: -0.008, -lr * Pred Grad: 0.065, New P: 1.684
-Original Grad: -0.011, -lr * Pred Grad: -0.062, New P: -0.445
iter 16 loss: 0.207
Actual params: [ 1.684 , -0.4452]
-Original Grad: 0.033, -lr * Pred Grad: 0.065, New P: 1.749
-Original Grad: -0.017, -lr * Pred Grad: -0.062, New P: -0.508
iter 17 loss: 0.217
Actual params: [ 1.7492, -0.5075]
-Original Grad: 0.055, -lr * Pred Grad: 0.065, New P: 1.814
-Original Grad: 0.015, -lr * Pred Grad: -0.062, New P: -0.570
iter 18 loss: 0.228
Actual params: [ 1.8144, -0.5698]
-Original Grad: 0.013, -lr * Pred Grad: 0.065, New P: 1.880
-Original Grad: 0.048, -lr * Pred Grad: -0.062, New P: -0.632
iter 19 loss: 0.232
Actual params: [ 1.8797, -0.6321]
-Original Grad: -0.020, -lr * Pred Grad: 0.065, New P: 1.945
-Original Grad: 0.110, -lr * Pred Grad: -0.062, New P: -0.694
iter 20 loss: 0.240
Actual params: [ 1.9449, -0.6944]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.242, -lr * Pred Grad: 0.050, New P: 0.644
-Original Grad: -0.461, -lr * Pred Grad: -0.045, New P: 0.549
iter 0 loss: 0.503
Actual params: [0.6438, 0.5489]
-Original Grad: 0.276, -lr * Pred Grad: 0.063, New P: 0.707
-Original Grad: -0.369, -lr * Pred Grad: -0.060, New P: 0.489
iter 1 loss: 0.482
Actual params: [0.7067, 0.4893]
-Original Grad: 0.364, -lr * Pred Grad: 0.065, New P: 0.772
-Original Grad: -0.183, -lr * Pred Grad: -0.062, New P: 0.427
iter 2 loss: 0.447
Actual params: [0.7716, 0.4273]
-Original Grad: 0.358, -lr * Pred Grad: 0.065, New P: 0.837
-Original Grad: -0.145, -lr * Pred Grad: -0.062, New P: 0.365
iter 3 loss: 0.389
Actual params: [0.8368, 0.3651]
-Original Grad: 0.261, -lr * Pred Grad: 0.065, New P: 0.902
-Original Grad: -0.056, -lr * Pred Grad: -0.062, New P: 0.303
iter 4 loss: 0.299
Actual params: [0.902 , 0.3028]
-Original Grad: 0.194, -lr * Pred Grad: 0.065, New P: 0.967
-Original Grad: -0.015, -lr * Pred Grad: -0.062, New P: 0.241
iter 5 loss: 0.233
Actual params: [0.9672, 0.2405]
-Original Grad: 0.075, -lr * Pred Grad: 0.065, New P: 1.032
-Original Grad: 0.018, -lr * Pred Grad: -0.062, New P: 0.178
iter 6 loss: 0.182
Actual params: [1.0324, 0.1783]
-Original Grad: 0.033, -lr * Pred Grad: 0.065, New P: 1.098
-Original Grad: 0.050, -lr * Pred Grad: -0.062, New P: 0.116
iter 7 loss: 0.154
Actual params: [1.0977, 0.116 ]
-Original Grad: -0.005, -lr * Pred Grad: 0.065, New P: 1.163
-Original Grad: 0.077, -lr * Pred Grad: -0.062, New P: 0.054
iter 8 loss: 0.142
Actual params: [1.1629, 0.0537]
-Original Grad: -0.020, -lr * Pred Grad: 0.065, New P: 1.228
-Original Grad: 0.051, -lr * Pred Grad: -0.062, New P: -0.009
iter 9 loss: 0.145
Actual params: [ 1.2281, -0.0086]
-Original Grad: -0.027, -lr * Pred Grad: 0.065, New P: 1.293
-Original Grad: 0.102, -lr * Pred Grad: -0.062, New P: -0.071
iter 10 loss: 0.152
Actual params: [ 1.2934, -0.0709]
-Original Grad: -0.055, -lr * Pred Grad: 0.065, New P: 1.359
-Original Grad: 0.022, -lr * Pred Grad: -0.062, New P: -0.133
iter 11 loss: 0.164
Actual params: [ 1.3586, -0.1332]
-Original Grad: -0.082, -lr * Pred Grad: 0.065, New P: 1.424
-Original Grad: 0.037, -lr * Pred Grad: -0.062, New P: -0.195
iter 12 loss: 0.188
Actual params: [ 1.4238, -0.1955]
-Original Grad: -0.035, -lr * Pred Grad: 0.065, New P: 1.489
-Original Grad: 0.035, -lr * Pred Grad: -0.062, New P: -0.258
iter 13 loss: 0.207
Actual params: [ 1.4891, -0.2578]
-Original Grad: -0.065, -lr * Pred Grad: 0.065, New P: 1.554
-Original Grad: 0.034, -lr * Pred Grad: -0.062, New P: -0.320
iter 14 loss: 0.223
Actual params: [ 1.5543, -0.3201]
-Original Grad: -0.037, -lr * Pred Grad: 0.065, New P: 1.620
-Original Grad: 0.018, -lr * Pred Grad: -0.062, New P: -0.382
iter 15 loss: 0.235
Actual params: [ 1.6195, -0.3823]
-Original Grad: -0.018, -lr * Pred Grad: 0.065, New P: 1.685
-Original Grad: 0.027, -lr * Pred Grad: -0.062, New P: -0.445
iter 16 loss: 0.240
Actual params: [ 1.6848, -0.4446]
-Original Grad: -0.006, -lr * Pred Grad: 0.065, New P: 1.750
-Original Grad: 0.015, -lr * Pred Grad: -0.062, New P: -0.507
iter 17 loss: 0.244
Actual params: [ 1.75  , -0.5069]
-Original Grad: -0.008, -lr * Pred Grad: 0.065, New P: 1.815
-Original Grad: 0.034, -lr * Pred Grad: -0.062, New P: -0.569
iter 18 loss: 0.253
Actual params: [ 1.8152, -0.5692]
-Original Grad: 0.002, -lr * Pred Grad: 0.065, New P: 1.880
-Original Grad: 0.064, -lr * Pred Grad: -0.062, New P: -0.631
iter 19 loss: 0.260
Actual params: [ 1.8805, -0.6315]
-Original Grad: 0.011, -lr * Pred Grad: 0.065, New P: 1.946
-Original Grad: 0.058, -lr * Pred Grad: -0.062, New P: -0.694
iter 20 loss: 0.266
Actual params: [ 1.9457, -0.6938]
Target params: [1.1812, 0.2779]
Actual params: [0.5941, 0.5941]
-Original Grad: 0.014, -lr * Pred Grad: 0.024, New P: 0.618
-Original Grad: -0.063, -lr * Pred Grad: -0.022, New P: 0.572
iter 0 loss: 0.473
Actual params: [0.6181, 0.5721]
-Original Grad: 0.004, -lr * Pred Grad: -0.004, New P: 0.614
-Original Grad: -0.038, -lr * Pred Grad: -0.055, New P: 0.517
iter 1 loss: 0.475
Actual params: [0.6137, 0.5174]
-Original Grad: -0.022, -lr * Pred Grad: -0.042, New P: 0.571
-Original Grad: 0.010, -lr * Pred Grad: -0.061, New P: 0.456
iter 2 loss: 0.475
Actual params: [0.5713, 0.4562]
-Original Grad: -0.068, -lr * Pred Grad: -0.058, New P: 0.513
-Original Grad: 0.065, -lr * Pred Grad: -0.062, New P: 0.394
iter 3 loss: 0.487
Actual params: [0.5133, 0.3941]
-Original Grad: -0.082, -lr * Pred Grad: -0.062, New P: 0.452
-Original Grad: 0.053, -lr * Pred Grad: -0.062, New P: 0.332
iter 4 loss: 0.488
Actual params: [0.4517, 0.3318]
-Original Grad: -0.086, -lr * Pred Grad: -0.062, New P: 0.390
-Original Grad: 0.046, -lr * Pred Grad: -0.062, New P: 0.270
iter 5 loss: 0.488
Actual params: [0.3895, 0.2695]
-Original Grad: -0.043, -lr * Pred Grad: -0.062, New P: 0.327
-Original Grad: 0.050, -lr * Pred Grad: -0.062, New P: 0.207
iter 6 loss: 0.490
Actual params: [0.3272, 0.2072]
-Original Grad: -0.032, -lr * Pred Grad: -0.062, New P: 0.265
-Original Grad: 0.050, -lr * Pred Grad: -0.062, New P: 0.145
iter 7 loss: 0.492
Actual params: [0.2649, 0.1449]
-Original Grad: -0.029, -lr * Pred Grad: -0.062, New P: 0.203
-Original Grad: 0.040, -lr * Pred Grad: -0.062, New P: 0.083
iter 8 loss: 0.494
Actual params: [0.2027, 0.0826]
-Original Grad: -0.011, -lr * Pred Grad: -0.062, New P: 0.140
-Original Grad: 0.031, -lr * Pred Grad: -0.062, New P: 0.020
iter 9 loss: 0.500
Actual params: [0.1404, 0.0204]
-Original Grad: -0.039, -lr * Pred Grad: -0.062, New P: 0.078
-Original Grad: 0.048, -lr * Pred Grad: -0.062, New P: -0.042
iter 10 loss: 0.503
Actual params: [ 0.0781, -0.0419]
-Original Grad: -0.017, -lr * Pred Grad: -0.062, New P: 0.016
-Original Grad: 0.046, -lr * Pred Grad: -0.062, New P: -0.104
iter 11 loss: 0.504
Actual params: [ 0.0158, -0.1042]
-Original Grad: 0.014, -lr * Pred Grad: -0.062, New P: -0.046
-Original Grad: 0.041, -lr * Pred Grad: -0.062, New P: -0.167
iter 12 loss: 0.510
Actual params: [-0.0465, -0.1665]
-Original Grad: 0.014, -lr * Pred Grad: -0.062, New P: -0.109
-Original Grad: 0.024, -lr * Pred Grad: -0.062, New P: -0.229
iter 13 loss: 0.516
Actual params: [-0.1088, -0.2288]
-Original Grad: -0.006, -lr * Pred Grad: -0.062, New P: -0.171
-Original Grad: 0.006, -lr * Pred Grad: -0.062, New P: -0.291
iter 14 loss: 0.526
Actual params: [-0.1711, -0.2911]
-Original Grad: -0.004, -lr * Pred Grad: -0.062, New P: -0.233
-Original Grad: 0.002, -lr * Pred Grad: -0.062, New P: -0.353
iter 15 loss: 0.531
Actual params: [-0.2334, -0.3534]
-Original Grad: 0.007, -lr * Pred Grad: -0.062, New P: -0.296
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: -0.416
iter 16 loss: 0.531
Actual params: [-0.2957, -0.4157]
-Original Grad: 0.015, -lr * Pred Grad: -0.062, New P: -0.358
-Original Grad: 0.004, -lr * Pred Grad: -0.062, New P: -0.478
iter 17 loss: 0.537
Actual params: [-0.3579, -0.4779]
-Original Grad: 0.029, -lr * Pred Grad: -0.062, New P: -0.420
-Original Grad: 0.001, -lr * Pred Grad: -0.062, New P: -0.540
iter 18 loss: 0.538
Actual params: [-0.4202, -0.5402]
-Original Grad: 0.042, -lr * Pred Grad: -0.062, New P: -0.483
-Original Grad: 0.013, -lr * Pred Grad: -0.062, New P: -0.603
iter 19 loss: 0.539
Actual params: [-0.4825, -0.6025]
-Original Grad: 0.042, -lr * Pred Grad: -0.062, New P: -0.545
-Original Grad: 0.013, -lr * Pred Grad: -0.062, New P: -0.665
iter 20 loss: 0.538
Actual params: [-0.5448, -0.6648]
