Target params: [ 1.3344, -1.0472]
Actual params: [1.084 , 0.5507]
-Original Grad: 0.016, -lr * Pred Grad:  0.014, New P: 1.098
-Original Grad: 0.390, -lr * Pred Grad:  0.408, New P: 0.958
iter 0 loss: 0.440
Actual params: [1.0982, 0.9583]
-Original Grad: -0.008, -lr * Pred Grad:  -0.113, New P: 0.985
-Original Grad: 0.226, -lr * Pred Grad:  0.158, New P: 1.116
iter 1 loss: 0.290
Actual params: [0.9849, 1.1163]
-Original Grad: -0.024, -lr * Pred Grad:  -0.202, New P: 0.783
-Original Grad: 0.182, -lr * Pred Grad:  0.092, New P: 1.208
iter 2 loss: 0.233
Actual params: [0.7828, 1.2084]
-Original Grad: 0.086, -lr * Pred Grad:  0.307, New P: 1.090
-Original Grad: 0.140, -lr * Pred Grad:  0.043, New P: 1.251
iter 3 loss: 0.203
Actual params: [1.0896, 1.251 ]
-Original Grad: -0.034, -lr * Pred Grad:  -0.140, New P: 0.950
-Original Grad: 0.149, -lr * Pred Grad:  0.066, New P: 1.317
iter 4 loss: 0.192
Actual params: [0.9497, 1.317 ]
-Original Grad: 0.008, -lr * Pred Grad:  0.018, New P: 0.968
-Original Grad: 0.130, -lr * Pred Grad:  0.049, New P: 1.366
iter 5 loss: 0.159
Actual params: [0.9677, 1.3658]
-Original Grad: -0.003, -lr * Pred Grad:  0.002, New P: 0.970
-Original Grad: 0.120, -lr * Pred Grad:  0.040, New P: 1.406
iter 6 loss: 0.141
Actual params: [0.9701, 1.4061]
-Original Grad: 0.008, -lr * Pred Grad:  0.014, New P: 0.984
-Original Grad: 0.097, -lr * Pred Grad:  0.031, New P: 1.438
iter 7 loss: 0.125
Actual params: [0.9843, 1.4375]
-Original Grad: 0.010, -lr * Pred Grad:  0.013, New P: 0.998
-Original Grad: 0.091, -lr * Pred Grad:  0.028, New P: 1.466
iter 8 loss: 0.114
Actual params: [0.9976, 1.4658]
-Original Grad: -0.001, -lr * Pred Grad:  0.006, New P: 1.003
-Original Grad: 0.088, -lr * Pred Grad:  0.026, New P: 1.491
iter 9 loss: 0.104
Actual params: [1.0032, 1.4913]
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 1.010
-Original Grad: 0.082, -lr * Pred Grad:  0.023, New P: 1.514
iter 10 loss: 0.094
Actual params: [1.01  , 1.5143]
-Original Grad: 0.004, -lr * Pred Grad:  0.008, New P: 1.019
-Original Grad: 0.077, -lr * Pred Grad:  0.021, New P: 1.536
iter 11 loss: 0.086
Actual params: [1.0185, 1.5356]
-Original Grad: 0.014, -lr * Pred Grad:  0.013, New P: 1.031
-Original Grad: 0.075, -lr * Pred Grad:  0.020, New P: 1.556
iter 12 loss: 0.078
Actual params: [1.0313, 1.5559]
-Original Grad: 0.014, -lr * Pred Grad:  0.011, New P: 1.042
-Original Grad: 0.070, -lr * Pred Grad:  0.018, New P: 1.574
iter 13 loss: 0.070
Actual params: [1.0419, 1.574 ]
-Original Grad: 0.011, -lr * Pred Grad:  0.008, New P: 1.050
-Original Grad: 0.057, -lr * Pred Grad:  0.015, New P: 1.589
iter 14 loss: 0.064
Actual params: [1.0498, 1.5886]
-Original Grad: 0.004, -lr * Pred Grad:  0.005, New P: 1.055
-Original Grad: 0.055, -lr * Pred Grad:  0.014, New P: 1.602
iter 15 loss: 0.059
Actual params: [1.0546, 1.6021]
-Original Grad: 0.007, -lr * Pred Grad:  0.005, New P: 1.060
-Original Grad: 0.052, -lr * Pred Grad:  0.013, New P: 1.615
iter 16 loss: 0.057
Actual params: [1.0599, 1.6148]
-Original Grad: 0.006, -lr * Pred Grad:  0.004, New P: 1.064
-Original Grad: 0.042, -lr * Pred Grad:  0.010, New P: 1.625
iter 17 loss: 0.056
Actual params: [1.064 , 1.6252]
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: 1.067
-Original Grad: 0.040, -lr * Pred Grad:  0.010, New P: 1.635
iter 18 loss: 0.055
Actual params: [1.0668, 1.6352]
-Original Grad: -0.000, -lr * Pred Grad:  0.002, New P: 1.069
-Original Grad: 0.040, -lr * Pred Grad:  0.010, New P: 1.645
iter 19 loss: 0.055
Actual params: [1.0686, 1.645 ]
-Original Grad: -0.000, -lr * Pred Grad:  0.002, New P: 1.070
-Original Grad: 0.038, -lr * Pred Grad:  0.009, New P: 1.655
iter 20 loss: 0.055
Actual params: [1.0704, 1.6545]
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: 1.072
-Original Grad: 0.030, -lr * Pred Grad:  0.008, New P: 1.662
iter 21 loss: 0.055
Actual params: [1.0723, 1.6623]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: 1.074
-Original Grad: 0.029, -lr * Pred Grad:  0.008, New P: 1.670
iter 22 loss: 0.056
Actual params: [1.0735, 1.67  ]
-Original Grad: -0.001, -lr * Pred Grad:  0.001, New P: 1.075
-Original Grad: 0.028, -lr * Pred Grad:  0.008, New P: 1.678
iter 23 loss: 0.057
Actual params: [1.0747, 1.6776]
-Original Grad: -0.002, -lr * Pred Grad:  0.000, New P: 1.075
-Original Grad: 0.024, -lr * Pred Grad:  0.007, New P: 1.684
iter 24 loss: 0.057
Actual params: [1.0751, 1.6842]
-Original Grad: -0.002, -lr * Pred Grad:  0.000, New P: 1.076
-Original Grad: 0.023, -lr * Pred Grad:  0.007, New P: 1.691
iter 25 loss: 0.058
Actual params: [1.0756, 1.6907]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: 1.076
-Original Grad: 0.017, -lr * Pred Grad:  0.005, New P: 1.696
iter 26 loss: 0.059
Actual params: [1.0762, 1.6956]
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: 1.077
-Original Grad: 0.006, -lr * Pred Grad:  0.002, New P: 1.697
iter 27 loss: 0.059
Actual params: [1.0769, 1.6975]
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: 1.077
-Original Grad: 0.006, -lr * Pred Grad:  0.002, New P: 1.699
iter 28 loss: 0.060
Actual params: [1.0775, 1.6991]
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: 1.078
-Original Grad: 0.005, -lr * Pred Grad:  0.002, New P: 1.701
iter 29 loss: 0.060
Actual params: [1.078 , 1.7007]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 1.078
-Original Grad: 0.005, -lr * Pred Grad:  0.001, New P: 1.702
iter 30 loss: 0.060
Actual params: [1.0784, 1.7022]
Target params: [ 1.3344, -1.0472]
Actual params: [-1.0585,  1.4018]
-Original Grad: 0.052, -lr * Pred Grad:  0.491, New P: -0.567
-Original Grad: 0.032, -lr * Pred Grad:  0.301, New P: 1.703
iter 0 loss: 0.711
Actual params: [-0.5673,  1.7031]
-Original Grad: 0.269, -lr * Pred Grad:  0.672, New P: 0.105
-Original Grad: 0.027, -lr * Pred Grad:  -0.052, New P: 1.651
iter 1 loss: 0.653
Actual params: [0.105 , 1.6515]
-Original Grad: 0.437, -lr * Pred Grad:  0.180, New P: 0.285
-Original Grad: 0.193, -lr * Pred Grad:  0.202, New P: 1.853
iter 2 loss: 0.435
Actual params: [0.2846, 1.8533]
-Original Grad: 0.285, -lr * Pred Grad:  0.193, New P: 0.478
-Original Grad: 0.048, -lr * Pred Grad:  -0.273, New P: 1.580
iter 3 loss: 0.266
Actual params: [0.4781, 1.58  ]
-Original Grad: 0.265, -lr * Pred Grad:  -0.003, New P: 0.475
-Original Grad: 0.157, -lr * Pred Grad:  0.179, New P: 1.759
iter 4 loss: 0.237
Actual params: [0.4754, 1.7588]
-Original Grad: 0.182, -lr * Pred Grad:  0.097, New P: 0.572
-Original Grad: 0.031, -lr * Pred Grad:  -0.139, New P: 1.620
iter 5 loss: 0.187
Actual params: [0.5725, 1.6202]
-Original Grad: 0.203, -lr * Pred Grad:  0.041, New P: 0.614
-Original Grad: 0.077, -lr * Pred Grad:  -0.003, New P: 1.617
iter 6 loss: 0.172
Actual params: [0.6138, 1.6173]
-Original Grad: 0.191, -lr * Pred Grad:  0.046, New P: 0.660
-Original Grad: 0.060, -lr * Pred Grad:  -0.040, New P: 1.577
iter 7 loss: 0.151
Actual params: [0.6601, 1.577 ]
-Original Grad: 0.188, -lr * Pred Grad:  0.031, New P: 0.691
-Original Grad: 0.064, -lr * Pred Grad:  -0.015, New P: 1.562
iter 8 loss: 0.142
Actual params: [0.6913, 1.5617]
-Original Grad: 0.123, -lr * Pred Grad:  0.023, New P: 0.715
-Original Grad: 0.038, -lr * Pred Grad:  -0.020, New P: 1.542
iter 9 loss: 0.131
Actual params: [0.7147, 1.5418]
-Original Grad: 0.111, -lr * Pred Grad:  0.015, New P: 0.730
-Original Grad: 0.037, -lr * Pred Grad:  -0.000, New P: 1.541
iter 10 loss: 0.128
Actual params: [0.7297, 1.5415]
-Original Grad: 0.109, -lr * Pred Grad:  0.017, New P: 0.747
-Original Grad: 0.034, -lr * Pred Grad:  -0.007, New P: 1.535
iter 11 loss: 0.120
Actual params: [0.7466, 1.5348]
-Original Grad: 0.105, -lr * Pred Grad:  0.014, New P: 0.760
-Original Grad: 0.033, -lr * Pred Grad:  0.001, New P: 1.536
iter 12 loss: 0.115
Actual params: [0.7602, 1.5358]
-Original Grad: 0.117, -lr * Pred Grad:  0.015, New P: 0.776
-Original Grad: 0.032, -lr * Pred Grad:  -0.003, New P: 1.532
iter 13 loss: 0.108
Actual params: [0.7756, 1.5325]
-Original Grad: 0.091, -lr * Pred Grad:  0.009, New P: 0.784
-Original Grad: 0.028, -lr * Pred Grad:  0.009, New P: 1.542
iter 14 loss: 0.104
Actual params: [0.7845, 1.5417]
-Original Grad: 0.092, -lr * Pred Grad:  0.010, New P: 0.795
-Original Grad: 0.026, -lr * Pred Grad:  0.005, New P: 1.547
iter 15 loss: 0.097
Actual params: [0.7946, 1.5467]
-Original Grad: 0.092, -lr * Pred Grad:  0.011, New P: 0.805
-Original Grad: 0.024, -lr * Pred Grad:  0.003, New P: 1.550
iter 16 loss: 0.093
Actual params: [0.8052, 1.5495]
-Original Grad: 0.087, -lr * Pred Grad:  0.012, New P: 0.817
-Original Grad: 0.018, -lr * Pred Grad:  -0.008, New P: 1.542
iter 17 loss: 0.091
Actual params: [0.8173, 1.5418]
-Original Grad: 0.089, -lr * Pred Grad:  0.011, New P: 0.828
-Original Grad: 0.018, -lr * Pred Grad:  -0.004, New P: 1.538
iter 18 loss: 0.090
Actual params: [0.8283, 1.5377]
-Original Grad: 0.071, -lr * Pred Grad:  0.003, New P: 0.831
-Original Grad: 0.025, -lr * Pred Grad:  0.024, New P: 1.561
iter 19 loss: 0.089
Actual params: [0.8315, 1.5614]
-Original Grad: 0.051, -lr * Pred Grad:  0.008, New P: 0.840
-Original Grad: 0.008, -lr * Pred Grad:  -0.009, New P: 1.552
iter 20 loss: 0.085
Actual params: [0.8399, 1.5522]
-Original Grad: 0.053, -lr * Pred Grad:  0.010, New P: 0.850
-Original Grad: 0.008, -lr * Pred Grad:  -0.012, New P: 1.540
iter 21 loss: 0.086
Actual params: [0.8496, 1.5402]
-Original Grad: 0.076, -lr * Pred Grad:  0.015, New P: 0.865
-Original Grad: 0.005, -lr * Pred Grad:  -0.026, New P: 1.514
iter 22 loss: 0.086
Actual params: [0.8649, 1.514 ]
-Original Grad: 0.093, -lr * Pred Grad:  0.010, New P: 0.875
-Original Grad: 0.020, -lr * Pred Grad:  0.003, New P: 1.517
iter 23 loss: 0.088
Actual params: [0.8753, 1.5174]
-Original Grad: 0.068, -lr * Pred Grad:  0.008, New P: 0.883
-Original Grad: 0.011, -lr * Pred Grad:  -0.003, New P: 1.514
iter 24 loss: 0.087
Actual params: [0.8834, 1.5142]
-Original Grad: 0.065, -lr * Pred Grad:  0.007, New P: 0.890
-Original Grad: 0.010, -lr * Pred Grad:  -0.002, New P: 1.512
iter 25 loss: 0.088
Actual params: [0.8905, 1.5123]
-Original Grad: 0.046, -lr * Pred Grad:  0.005, New P: 0.895
-Original Grad: 0.009, -lr * Pred Grad:  0.000, New P: 1.513
iter 26 loss: 0.089
Actual params: [0.8954, 1.5126]
-Original Grad: 0.048, -lr * Pred Grad:  0.007, New P: 0.903
-Original Grad: 0.001, -lr * Pred Grad:  -0.008, New P: 1.504
iter 27 loss: 0.090
Actual params: [0.9026, 1.5044]
-Original Grad: 0.057, -lr * Pred Grad:  0.007, New P: 0.910
-Original Grad: 0.008, -lr * Pred Grad:  -0.003, New P: 1.501
iter 28 loss: 0.091
Actual params: [0.9099, 1.5012]
-Original Grad: 0.074, -lr * Pred Grad:  0.010, New P: 0.920
-Original Grad: 0.006, -lr * Pred Grad:  -0.007, New P: 1.494
iter 29 loss: 0.091
Actual params: [0.9198, 1.4939]
-Original Grad: 0.091, -lr * Pred Grad:  0.011, New P: 0.930
-Original Grad: 0.005, -lr * Pred Grad:  -0.008, New P: 1.486
iter 30 loss: 0.091
Actual params: [0.9305, 1.4857]
Target params: [ 1.3344, -1.0472]
Actual params: [1.5477, 0.5327]
-Original Grad: -0.159, -lr * Pred Grad:  -0.199, New P: 1.348
-Original Grad: 0.378, -lr * Pred Grad:  0.496, New P: 1.029
iter 0 loss: 0.893
Actual params: [1.3483, 1.0287]
-Original Grad: -0.289, -lr * Pred Grad:  -0.245, New P: 1.103
-Original Grad: 0.221, -lr * Pred Grad:  0.122, New P: 1.151
iter 1 loss: 0.639
Actual params: [1.1032, 1.151 ]
-Original Grad: 0.239, -lr * Pred Grad:  0.167, New P: 1.270
-Original Grad: 0.193, -lr * Pred Grad:  0.212, New P: 1.363
iter 2 loss: 0.523
Actual params: [1.2701, 1.3633]
-Original Grad: -0.117, -lr * Pred Grad:  -0.043, New P: 1.227
-Original Grad: 0.172, -lr * Pred Grad:  0.138, New P: 1.502
iter 3 loss: 0.521
Actual params: [1.2266, 1.5017]
-Original Grad: -0.047, -lr * Pred Grad:  0.006, New P: 1.233
-Original Grad: 0.172, -lr * Pred Grad:  0.134, New P: 1.636
iter 4 loss: 0.425
Actual params: [1.2325, 1.636 ]
-Original Grad: 0.038, -lr * Pred Grad:  0.049, New P: 1.281
-Original Grad: 0.154, -lr * Pred Grad:  0.122, New P: 1.758
iter 5 loss: 0.346
Actual params: [1.2811, 1.7578]
-Original Grad: 0.021, -lr * Pred Grad:  0.038, New P: 1.319
-Original Grad: 0.149, -lr * Pred Grad:  0.104, New P: 1.862
iter 6 loss: 0.304
Actual params: [1.3189, 1.8621]
-Original Grad: 0.028, -lr * Pred Grad:  0.036, New P: 1.355
-Original Grad: 0.148, -lr * Pred Grad:  0.092, New P: 1.955
iter 7 loss: 0.280
Actual params: [1.3552, 1.9545]
-Original Grad: 0.033, -lr * Pred Grad:  0.031, New P: 1.386
-Original Grad: 0.137, -lr * Pred Grad:  0.077, New P: 2.031
iter 8 loss: 0.269
Actual params: [1.3863, 2.0314]
-Original Grad: 0.030, -lr * Pred Grad:  0.026, New P: 1.412
-Original Grad: 0.134, -lr * Pred Grad:  0.066, New P: 2.097
iter 9 loss: 0.257
Actual params: [1.4121, 2.0974]
-Original Grad: 0.264, -lr * Pred Grad:  0.051, New P: 1.463
-Original Grad: -0.016, -lr * Pred Grad:  0.029, New P: 2.126
iter 10 loss: 0.247
Actual params: [1.4632, 2.1261]
-Original Grad: -0.085, -lr * Pred Grad:  0.005, New P: 1.468
-Original Grad: 0.142, -lr * Pred Grad:  0.046, New P: 2.172
iter 11 loss: 0.248
Actual params: [1.4678, 2.172 ]
-Original Grad: 0.038, -lr * Pred Grad:  0.020, New P: 1.487
-Original Grad: 0.090, -lr * Pred Grad:  0.041, New P: 2.213
iter 12 loss: 0.238
Actual params: [1.4873, 2.2127]
-Original Grad: 0.069, -lr * Pred Grad:  0.018, New P: 1.505
-Original Grad: 0.056, -lr * Pred Grad:  0.031, New P: 2.244
iter 13 loss: 0.238
Actual params: [1.5052, 2.2438]
-Original Grad: -0.047, -lr * Pred Grad:  -0.003, New P: 1.502
-Original Grad: 0.023, -lr * Pred Grad:  0.003, New P: 2.247
iter 14 loss: 0.242
Actual params: [1.5018, 2.2466]
-Original Grad: 0.058, -lr * Pred Grad:  0.005, New P: 1.507
-Original Grad: -0.000, -lr * Pred Grad:  0.004, New P: 2.251
iter 15 loss: 0.238
Actual params: [1.5072, 2.251 ]
-Original Grad: -0.049, -lr * Pred Grad:  -0.003, New P: 1.504
-Original Grad: 0.020, -lr * Pred Grad:  0.003, New P: 2.254
iter 16 loss: 0.241
Actual params: [1.5041, 2.2538]
-Original Grad: 0.019, -lr * Pred Grad:  -0.000, New P: 1.504
-Original Grad: -0.030, -lr * Pred Grad:  -0.007, New P: 2.247
iter 17 loss: 0.238
Actual params: [1.504 , 2.2468]
-Original Grad: -0.047, -lr * Pred Grad:  -0.003, New P: 1.501
-Original Grad: 0.022, -lr * Pred Grad:  0.004, New P: 2.251
iter 18 loss: 0.240
Actual params: [1.5014, 2.2508]
-Original Grad: 0.058, -lr * Pred Grad:  0.004, New P: 1.505
-Original Grad: -0.002, -lr * Pred Grad:  0.002, New P: 2.252
iter 19 loss: 0.237
Actual params: [1.5051, 2.2523]
-Original Grad: -0.049, -lr * Pred Grad:  -0.003, New P: 1.502
-Original Grad: 0.020, -lr * Pred Grad:  0.004, New P: 2.256
iter 20 loss: 0.239
Actual params: [1.5025, 2.256 ]
-Original Grad: 0.090, -lr * Pred Grad:  0.003, New P: 1.506
-Original Grad: -0.057, -lr * Pred Grad:  -0.010, New P: 2.246
iter 21 loss: 0.236
Actual params: [1.5057, 2.2457]
-Original Grad: -0.047, -lr * Pred Grad:  -0.002, New P: 1.504
-Original Grad: 0.022, -lr * Pred Grad:  0.004, New P: 2.250
iter 22 loss: 0.241
Actual params: [1.5036, 2.2497]
-Original Grad: -0.048, -lr * Pred Grad:  -0.002, New P: 1.501
-Original Grad: 0.021, -lr * Pred Grad:  0.004, New P: 2.254
iter 23 loss: 0.239
Actual params: [1.5012, 2.254 ]
-Original Grad: 0.136, -lr * Pred Grad:  0.006, New P: 1.507
-Original Grad: -0.035, -lr * Pred Grad:  -0.005, New P: 2.249
iter 24 loss: 0.236
Actual params: [1.5072, 2.2494]
-Original Grad: -0.049, -lr * Pred Grad:  -0.002, New P: 1.505
-Original Grad: 0.021, -lr * Pred Grad:  0.004, New P: 2.253
iter 25 loss: 0.242
Actual params: [1.5051, 2.2532]
-Original Grad: -0.049, -lr * Pred Grad:  -0.002, New P: 1.503
-Original Grad: 0.020, -lr * Pred Grad:  0.004, New P: 2.257
iter 26 loss: 0.239
Actual params: [1.5026, 2.2571]
-Original Grad: 0.091, -lr * Pred Grad:  0.003, New P: 1.506
-Original Grad: -0.057, -lr * Pred Grad:  -0.010, New P: 2.247
iter 27 loss: 0.236
Actual params: [1.5059, 2.2466]
-Original Grad: -0.048, -lr * Pred Grad:  -0.002, New P: 1.504
-Original Grad: 0.022, -lr * Pred Grad:  0.004, New P: 2.251
iter 28 loss: 0.241
Actual params: [1.5038, 2.2507]
-Original Grad: -0.048, -lr * Pred Grad:  -0.002, New P: 1.502
-Original Grad: 0.021, -lr * Pred Grad:  0.004, New P: 2.255
iter 29 loss: 0.239
Actual params: [1.5015, 2.2551]
-Original Grad: 0.136, -lr * Pred Grad:  0.006, New P: 1.507
-Original Grad: -0.035, -lr * Pred Grad:  -0.005, New P: 2.250
iter 30 loss: 0.236
Actual params: [1.5072, 2.2498]
Target params: [ 1.3344, -1.0472]
Actual params: [0.0029, 0.9353]
-Original Grad: 0.105, -lr * Pred Grad:  0.366, New P: 0.369
-Original Grad: 0.074, -lr * Pred Grad:  0.317, New P: 1.253
iter 0 loss: 0.426
Actual params: [0.3693, 1.2528]
-Original Grad: 0.123, -lr * Pred Grad:  0.346, New P: 0.716
-Original Grad: 0.022, -lr * Pred Grad:  -0.080, New P: 1.172
iter 1 loss: 0.256
Actual params: [0.7158, 1.1724]
-Original Grad: 0.103, -lr * Pred Grad:  0.146, New P: 0.862
-Original Grad: 0.031, -lr * Pred Grad:  0.066, New P: 1.238
iter 2 loss: 0.225
Actual params: [0.8616, 1.2385]
-Original Grad: -0.111, -lr * Pred Grad:  -0.021, New P: 0.840
-Original Grad: 0.082, -lr * Pred Grad:  0.100, New P: 1.339
iter 3 loss: 0.213
Actual params: [0.8402, 1.3385]
-Original Grad: -0.033, -lr * Pred Grad:  0.004, New P: 0.844
-Original Grad: 0.043, -lr * Pred Grad:  0.057, New P: 1.396
iter 4 loss: 0.187
Actual params: [0.844, 1.396]
-Original Grad: 0.004, -lr * Pred Grad:  0.010, New P: 0.854
-Original Grad: 0.029, -lr * Pred Grad:  0.046, New P: 1.442
iter 5 loss: 0.173
Actual params: [0.8535, 1.4415]
-Original Grad: 0.045, -lr * Pred Grad:  0.021, New P: 0.875
-Original Grad: 0.040, -lr * Pred Grad:  0.066, New P: 1.507
iter 6 loss: 0.163
Actual params: [0.8748, 1.5073]
-Original Grad: 0.067, -lr * Pred Grad:  0.020, New P: 0.895
-Original Grad: 0.022, -lr * Pred Grad:  0.045, New P: 1.552
iter 7 loss: 0.150
Actual params: [0.8953, 1.5524]
-Original Grad: 0.073, -lr * Pred Grad:  0.012, New P: 0.907
-Original Grad: -0.016, -lr * Pred Grad:  0.003, New P: 1.556
iter 8 loss: 0.150
Actual params: [0.907 , 1.5555]
-Original Grad: 0.093, -lr * Pred Grad:  0.016, New P: 0.923
-Original Grad: -0.019, -lr * Pred Grad:  0.006, New P: 1.561
iter 9 loss: 0.150
Actual params: [0.9226, 1.5611]
-Original Grad: 0.074, -lr * Pred Grad:  0.011, New P: 0.934
-Original Grad: -0.022, -lr * Pred Grad:  -0.001, New P: 1.560
iter 10 loss: 0.150
Actual params: [0.9339, 1.5601]
-Original Grad: 0.062, -lr * Pred Grad:  0.012, New P: 0.946
-Original Grad: -0.008, -lr * Pred Grad:  0.008, New P: 1.568
iter 11 loss: 0.149
Actual params: [0.9462, 1.5677]
-Original Grad: 0.065, -lr * Pred Grad:  0.010, New P: 0.956
-Original Grad: -0.024, -lr * Pred Grad:  -0.003, New P: 1.565
iter 12 loss: 0.150
Actual params: [0.9562, 1.5647]
-Original Grad: 0.055, -lr * Pred Grad:  0.011, New P: 0.968
-Original Grad: -0.009, -lr * Pred Grad:  0.005, New P: 1.570
iter 13 loss: 0.150
Actual params: [0.9676, 1.5701]
-Original Grad: 0.055, -lr * Pred Grad:  0.008, New P: 0.976
-Original Grad: -0.030, -lr * Pred Grad:  -0.008, New P: 1.563
iter 14 loss: 0.151
Actual params: [0.9756, 1.5626]
-Original Grad: 0.052, -lr * Pred Grad:  0.011, New P: 0.987
-Original Grad: -0.009, -lr * Pred Grad:  0.004, New P: 1.567
iter 15 loss: 0.149
Actual params: [0.9869, 1.5667]
-Original Grad: 0.050, -lr * Pred Grad:  0.011, New P: 0.998
-Original Grad: -0.011, -lr * Pred Grad:  0.003, New P: 1.569
iter 16 loss: 0.150
Actual params: [0.9977, 1.5694]
-Original Grad: 0.038, -lr * Pred Grad:  0.008, New P: 1.006
-Original Grad: -0.011, -lr * Pred Grad:  0.001, New P: 1.570
iter 17 loss: 0.150
Actual params: [1.0058, 1.5701]
-Original Grad: 0.036, -lr * Pred Grad:  0.005, New P: 1.011
-Original Grad: -0.029, -lr * Pred Grad:  -0.009, New P: 1.561
iter 18 loss: 0.150
Actual params: [1.0112, 1.5614]
-Original Grad: 0.042, -lr * Pred Grad:  0.010, New P: 1.021
-Original Grad: -0.012, -lr * Pred Grad:  0.000, New P: 1.562
iter 19 loss: 0.148
Actual params: [1.0211, 1.5616]
-Original Grad: 0.026, -lr * Pred Grad:  0.004, New P: 1.025
-Original Grad: -0.029, -lr * Pred Grad:  -0.010, New P: 1.552
iter 20 loss: 0.148
Actual params: [1.0248, 1.5519]
-Original Grad: 0.028, -lr * Pred Grad:  0.007, New P: 1.032
-Original Grad: -0.011, -lr * Pred Grad:  -0.001, New P: 1.551
iter 21 loss: 0.146
Actual params: [1.032 , 1.5505]
-Original Grad: 0.010, -lr * Pred Grad:  0.002, New P: 1.034
-Original Grad: -0.014, -lr * Pred Grad:  -0.005, New P: 1.546
iter 22 loss: 0.145
Actual params: [1.0336, 1.5458]
-Original Grad: 0.011, -lr * Pred Grad:  0.002, New P: 1.036
-Original Grad: -0.013, -lr * Pred Grad:  -0.004, New P: 1.541
iter 23 loss: 0.144
Actual params: [1.036 , 1.5415]
-Original Grad: 0.009, -lr * Pred Grad:  0.002, New P: 1.038
-Original Grad: -0.014, -lr * Pred Grad:  -0.005, New P: 1.536
iter 24 loss: 0.143
Actual params: [1.0382, 1.5364]
-Original Grad: 0.007, -lr * Pred Grad:  0.002, New P: 1.040
-Original Grad: -0.017, -lr * Pred Grad:  -0.006, New P: 1.530
iter 25 loss: 0.142
Actual params: [1.0399, 1.5302]
-Original Grad: 0.008, -lr * Pred Grad:  0.003, New P: 1.042
-Original Grad: -0.017, -lr * Pred Grad:  -0.006, New P: 1.524
iter 26 loss: 0.141
Actual params: [1.0425, 1.5238]
-Original Grad: 0.004, -lr * Pred Grad:  0.001, New P: 1.044
-Original Grad: -0.013, -lr * Pred Grad:  -0.005, New P: 1.519
iter 27 loss: 0.140
Actual params: [1.0439, 1.5187]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 1.044
-Original Grad: -0.014, -lr * Pred Grad:  -0.005, New P: 1.513
iter 28 loss: 0.139
Actual params: [1.0443, 1.5132]
-Original Grad: 0.003, -lr * Pred Grad:  0.002, New P: 1.046
-Original Grad: -0.012, -lr * Pred Grad:  -0.005, New P: 1.509
iter 29 loss: 0.138
Actual params: [1.0462, 1.5085]
-Original Grad: 0.007, -lr * Pred Grad:  0.004, New P: 1.051
-Original Grad: -0.010, -lr * Pred Grad:  -0.004, New P: 1.504
iter 30 loss: 0.137
Actual params: [1.0507, 1.5042]
Target params: [ 1.3344, -1.0472]
Actual params: [-0.6756, -1.5044]
-Original Grad: -0.037, -lr * Pred Grad:  -0.384, New P: -1.060
-Original Grad: 0.004, -lr * Pred Grad:  0.042, New P: -1.462
iter 0 loss: 0.728
Actual params: [-1.0598, -1.462 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -1.070
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -1.463
iter 1 loss: 0.726
Actual params: [-1.0702, -1.4626]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -1.081
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -1.463
iter 2 loss: 0.726
Actual params: [-1.0806, -1.4632]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -1.091
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -1.464
iter 3 loss: 0.726
Actual params: [-1.0908, -1.4638]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -1.101
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -1.464
iter 4 loss: 0.726
Actual params: [-1.101 , -1.4644]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -1.111
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -1.465
iter 5 loss: 0.726
Actual params: [-1.111 , -1.4649]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -1.121
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -1.465
iter 6 loss: 0.726
Actual params: [-1.1209, -1.4654]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.131
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.466
iter 7 loss: 0.726
Actual params: [-1.1309, -1.4659]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.141
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.466
iter 8 loss: 0.726
Actual params: [-1.1407, -1.4664]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.151
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.467
iter 9 loss: 0.726
Actual params: [-1.1506, -1.4669]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.161
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -1.467
iter 10 loss: 0.726
Actual params: [-1.1605, -1.4674]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.170
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.468
iter 11 loss: 0.726
Actual params: [-1.1704, -1.4679]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.180
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -1.468
iter 12 loss: 0.726
Actual params: [-1.1803, -1.4684]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.190
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.469
iter 13 loss: 0.726
Actual params: [-1.1902, -1.4689]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.200
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.469
iter 14 loss: 0.726
Actual params: [-1.2   , -1.4693]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.210
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.470
iter 15 loss: 0.726
Actual params: [-1.2099, -1.4698]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.220
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.470
iter 16 loss: 0.726
Actual params: [-1.2198, -1.4702]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.230
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.471
iter 17 loss: 0.726
Actual params: [-1.2296, -1.4706]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.240
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.471
iter 18 loss: 0.726
Actual params: [-1.2396, -1.471 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.250
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.471
iter 19 loss: 0.726
Actual params: [-1.2495, -1.4714]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.259
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.472
iter 20 loss: 0.726
Actual params: [-1.2595, -1.4718]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.269
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.472
iter 21 loss: 0.726
Actual params: [-1.2694, -1.4722]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.279
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.473
iter 22 loss: 0.726
Actual params: [-1.2794, -1.4726]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.289
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.473
iter 23 loss: 0.726
Actual params: [-1.2894, -1.4729]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.299
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.473
iter 24 loss: 0.726
Actual params: [-1.2995, -1.4733]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.310
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.474
iter 25 loss: 0.726
Actual params: [-1.3096, -1.4737]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.320
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.474
iter 26 loss: 0.726
Actual params: [-1.3198, -1.4741]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.330
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.474
iter 27 loss: 0.726
Actual params: [-1.33  , -1.4744]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.340
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.475
iter 28 loss: 0.726
Actual params: [-1.3401, -1.4748]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.350
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.475
iter 29 loss: 0.726
Actual params: [-1.3503, -1.4751]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.361
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.475
iter 30 loss: 0.726
Actual params: [-1.3606, -1.4754]
Target params: [ 1.3344, -1.0472]
Actual params: [-0.6634, -0.2295]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.659
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.229
iter 0 loss: 0.552
Actual params: [-0.6592, -0.2286]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.654
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.227
iter 1 loss: 0.552
Actual params: [-0.6544, -0.2274]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.649
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.226
iter 2 loss: 0.552
Actual params: [-0.6488, -0.2261]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.642
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.225
iter 3 loss: 0.552
Actual params: [-0.6423, -0.2246]
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: -0.635
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.223
iter 4 loss: 0.552
Actual params: [-0.6347, -0.2229]
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: -0.626
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.221
iter 5 loss: 0.552
Actual params: [-0.6255, -0.2207]
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: -0.614
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.218
iter 6 loss: 0.552
Actual params: [-0.6145, -0.2182]
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: -0.601
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.215
iter 7 loss: 0.552
Actual params: [-0.6009, -0.2151]
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: -0.584
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.211
iter 8 loss: 0.552
Actual params: [-0.5839, -0.2111]
-Original Grad: 0.001, -lr * Pred Grad:  0.022, New P: -0.562
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.206
iter 9 loss: 0.552
Actual params: [-0.5618, -0.206 ]
-Original Grad: 0.001, -lr * Pred Grad:  0.030, New P: -0.532
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.199
iter 10 loss: 0.552
Actual params: [-0.5317, -0.1991]
-Original Grad: 0.001, -lr * Pred Grad:  0.044, New P: -0.488
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: -0.189
iter 11 loss: 0.552
Actual params: [-0.4877, -0.189 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.075, New P: -0.413
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: -0.172
iter 12 loss: 0.552
Actual params: [-0.4132, -0.1715]
-Original Grad: 0.004, -lr * Pred Grad:  0.169, New P: -0.245
-Original Grad: 0.001, -lr * Pred Grad:  0.040, New P: -0.131
iter 13 loss: 0.552
Actual params: [-0.2447, -0.1311]
-Original Grad: 0.020, -lr * Pred Grad:  0.838, New P: 0.594
-Original Grad: 0.005, -lr * Pred Grad:  0.207, New P: 0.075
iter 14 loss: 0.551
Actual params: [0.5937, 0.0754]
-Original Grad: -1.001, -lr * Pred Grad:  -0.035, New P: 0.558
-Original Grad: 0.423, -lr * Pred Grad:  0.464, New P: 0.539
iter 15 loss: 0.506
Actual params: [0.5583, 0.539 ]
-Original Grad: -0.551, -lr * Pred Grad:  -0.051, New P: 0.507
-Original Grad: 0.226, -lr * Pred Grad:  0.087, New P: 0.626
iter 16 loss: 0.349
Actual params: [0.5071, 0.6256]
-Original Grad: -0.406, -lr * Pred Grad:  0.000, New P: 0.507
-Original Grad: 0.191, -lr * Pred Grad:  0.128, New P: 0.753
iter 17 loss: 0.312
Actual params: [0.5071, 0.7534]
-Original Grad: -0.161, -lr * Pred Grad:  0.044, New P: 0.551
-Original Grad: 0.118, -lr * Pred Grad:  0.158, New P: 0.911
iter 18 loss: 0.288
Actual params: [0.5511, 0.9111]
-Original Grad: -0.041, -lr * Pred Grad:  0.076, New P: 0.627
-Original Grad: 0.118, -lr * Pred Grad:  0.194, New P: 1.106
iter 19 loss: 0.263
Actual params: [0.6273, 1.1056]
-Original Grad: 0.085, -lr * Pred Grad:  0.055, New P: 0.682
-Original Grad: 0.044, -lr * Pred Grad:  0.108, New P: 1.213
iter 20 loss: 0.231
Actual params: [0.6819, 1.2134]
-Original Grad: 0.132, -lr * Pred Grad:  0.031, New P: 0.713
-Original Grad: -0.028, -lr * Pred Grad:  0.038, New P: 1.252
iter 21 loss: 0.209
Actual params: [0.7132, 1.2519]
-Original Grad: 0.132, -lr * Pred Grad:  0.033, New P: 0.746
-Original Grad: -0.024, -lr * Pred Grad:  0.043, New P: 1.295
iter 22 loss: 0.202
Actual params: [0.7464, 1.2945]
-Original Grad: 0.146, -lr * Pred Grad:  0.036, New P: 0.782
-Original Grad: -0.024, -lr * Pred Grad:  0.047, New P: 1.342
iter 23 loss: 0.194
Actual params: [0.7819, 1.3416]
-Original Grad: 0.086, -lr * Pred Grad:  0.015, New P: 0.797
-Original Grad: -0.015, -lr * Pred Grad:  0.018, New P: 1.359
iter 24 loss: 0.185
Actual params: [0.7972, 1.3592]
-Original Grad: 0.016, -lr * Pred Grad:  0.003, New P: 0.800
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 1.365
iter 25 loss: 0.182
Actual params: [0.8003, 1.3653]
-Original Grad: 0.013, -lr * Pred Grad:  0.001, New P: 0.802
-Original Grad: -0.003, -lr * Pred Grad:  0.001, New P: 1.366
iter 26 loss: 0.180
Actual params: [0.8016, 1.3658]
-Original Grad: 0.015, -lr * Pred Grad:  0.001, New P: 0.803
-Original Grad: -0.002, -lr * Pred Grad:  0.001, New P: 1.367
iter 27 loss: 0.180
Actual params: [0.803 , 1.3669]
-Original Grad: 0.021, -lr * Pred Grad:  0.002, New P: 0.805
-Original Grad: -0.001, -lr * Pred Grad:  0.004, New P: 1.371
iter 28 loss: 0.180
Actual params: [0.8052, 1.3709]
-Original Grad: 0.032, -lr * Pred Grad:  0.004, New P: 0.809
-Original Grad: 0.002, -lr * Pred Grad:  0.011, New P: 1.382
iter 29 loss: 0.179
Actual params: [0.8092, 1.3819]
-Original Grad: -0.007, -lr * Pred Grad:  -0.002, New P: 0.807
-Original Grad: -0.009, -lr * Pred Grad:  -0.014, New P: 1.368
iter 30 loss: 0.177
Actual params: [0.8068, 1.3679]
Target params: [ 1.3344, -1.0472]
Actual params: [-0.8962,  0.1733]
-Original Grad: 0.065, -lr * Pred Grad:  0.550, New P: -0.347
-Original Grad: -0.016, -lr * Pred Grad:  -0.122, New P: 0.052
iter 0 loss: 0.361
Actual params: [-0.3467,  0.0516]
-Original Grad: 0.174, -lr * Pred Grad:  0.535, New P: 0.188
-Original Grad: 0.025, -lr * Pred Grad:  0.113, New P: 0.164
iter 1 loss: 0.298
Actual params: [0.1881, 0.1642]
-Original Grad: 0.140, -lr * Pred Grad:  0.226, New P: 0.414
-Original Grad: 0.036, -lr * Pred Grad:  0.117, New P: 0.281
iter 2 loss: 0.173
Actual params: [0.4144, 0.281 ]
-Original Grad: -0.758, -lr * Pred Grad:  0.002, New P: 0.416
-Original Grad: -0.427, -lr * Pred Grad:  -0.226, New P: 0.055
iter 3 loss: 0.243
Actual params: [0.4162, 0.0546]
-Original Grad: -0.372, -lr * Pred Grad:  -0.087, New P: 0.329
-Original Grad: -0.082, -lr * Pred Grad:  0.126, New P: 0.180
iter 4 loss: 0.208
Actual params: [0.3295, 0.1804]
-Original Grad: -0.389, -lr * Pred Grad:  -0.033, New P: 0.296
-Original Grad: -0.148, -lr * Pred Grad:  0.015, New P: 0.196
iter 5 loss: 0.191
Actual params: [0.2964, 0.1956]
-Original Grad: -0.182, -lr * Pred Grad:  -0.019, New P: 0.278
-Original Grad: -0.063, -lr * Pred Grad:  0.019, New P: 0.215
iter 6 loss: 0.185
Actual params: [0.2778, 0.215 ]
-Original Grad: -0.171, -lr * Pred Grad:  -0.013, New P: 0.265
-Original Grad: -0.062, -lr * Pred Grad:  0.010, New P: 0.225
iter 7 loss: 0.182
Actual params: [0.265 , 0.2246]
-Original Grad: -0.059, -lr * Pred Grad:  0.009, New P: 0.274
-Original Grad: -0.035, -lr * Pred Grad:  -0.030, New P: 0.194
iter 8 loss: 0.180
Actual params: [0.2736, 0.1943]
-Original Grad: -0.155, -lr * Pred Grad:  -0.021, New P: 0.252
-Original Grad: -0.041, -lr * Pred Grad:  0.037, New P: 0.232
iter 9 loss: 0.179
Actual params: [0.2525, 0.2317]
-Original Grad: -0.052, -lr * Pred Grad:  0.011, New P: 0.263
-Original Grad: -0.034, -lr * Pred Grad:  -0.036, New P: 0.196
iter 10 loss: 0.179
Actual params: [0.2633, 0.1959]
-Original Grad: -0.032, -lr * Pred Grad:  0.005, New P: 0.268
-Original Grad: -0.019, -lr * Pred Grad:  -0.018, New P: 0.178
iter 11 loss: 0.178
Actual params: [0.2682, 0.1782]
-Original Grad: -0.024, -lr * Pred Grad:  -0.002, New P: 0.267
-Original Grad: -0.008, -lr * Pred Grad:  0.001, New P: 0.179
iter 12 loss: 0.177
Actual params: [0.2667, 0.1794]
-Original Grad: -0.024, -lr * Pred Grad:  -0.002, New P: 0.265
-Original Grad: -0.008, -lr * Pred Grad:  0.001, New P: 0.181
iter 13 loss: 0.177
Actual params: [0.2652, 0.1805]
-Original Grad: -0.019, -lr * Pred Grad:  0.002, New P: 0.267
-Original Grad: -0.009, -lr * Pred Grad:  -0.007, New P: 0.174
iter 14 loss: 0.177
Actual params: [0.2667, 0.1736]
-Original Grad: -0.021, -lr * Pred Grad:  -0.002, New P: 0.265
-Original Grad: -0.007, -lr * Pred Grad:  0.002, New P: 0.176
iter 15 loss: 0.177
Actual params: [0.2649, 0.1759]
-Original Grad: -0.021, -lr * Pred Grad:  -0.002, New P: 0.263
-Original Grad: -0.007, -lr * Pred Grad:  0.002, New P: 0.178
iter 16 loss: 0.177
Actual params: [0.2632, 0.1779]
-Original Grad: -0.018, -lr * Pred Grad:  0.001, New P: 0.264
-Original Grad: -0.008, -lr * Pred Grad:  -0.006, New P: 0.172
iter 17 loss: 0.177
Actual params: [0.2643, 0.172 ]
-Original Grad: 0.003, -lr * Pred Grad:  -0.023, New P: 0.241
-Original Grad: 0.031, -lr * Pred Grad:  0.070, New P: 0.242
iter 18 loss: 0.177
Actual params: [0.2413, 0.242 ]
-Original Grad: -0.039, -lr * Pred Grad:  0.019, New P: 0.261
-Original Grad: -0.041, -lr * Pred Grad:  -0.064, New P: 0.178
iter 19 loss: 0.178
Actual params: [0.2606, 0.1778]
-Original Grad: -0.017, -lr * Pred Grad:  0.000, New P: 0.261
-Original Grad: -0.007, -lr * Pred Grad:  -0.004, New P: 0.174
iter 20 loss: 0.176
Actual params: [0.2609, 0.1742]
-Original Grad: 0.007, -lr * Pred Grad:  -0.017, New P: 0.244
-Original Grad: 0.029, -lr * Pred Grad:  0.056, New P: 0.230
iter 21 loss: 0.177
Actual params: [0.2438, 0.2301]
-Original Grad: -0.044, -lr * Pred Grad:  0.009, New P: 0.253
-Original Grad: -0.031, -lr * Pred Grad:  -0.036, New P: 0.195
iter 22 loss: 0.178
Actual params: [0.2529, 0.1945]
-Original Grad: -0.023, -lr * Pred Grad:  -0.001, New P: 0.252
-Original Grad: -0.007, -lr * Pred Grad:  -0.000, New P: 0.194
iter 23 loss: 0.176
Actual params: [0.2518, 0.1942]
-Original Grad: -0.022, -lr * Pred Grad:  -0.001, New P: 0.251
-Original Grad: -0.007, -lr * Pred Grad:  -0.000, New P: 0.194
iter 24 loss: 0.176
Actual params: [0.2509, 0.1937]
-Original Grad: -0.022, -lr * Pred Grad:  -0.001, New P: 0.250
-Original Grad: -0.007, -lr * Pred Grad:  -0.000, New P: 0.194
iter 25 loss: 0.176
Actual params: [0.2498, 0.1936]
-Original Grad: -0.021, -lr * Pred Grad:  -0.001, New P: 0.249
-Original Grad: -0.006, -lr * Pred Grad:  0.000, New P: 0.194
iter 26 loss: 0.176
Actual params: [0.2487, 0.1939]
-Original Grad: -0.021, -lr * Pred Grad:  -0.001, New P: 0.248
-Original Grad: -0.006, -lr * Pred Grad:  0.000, New P: 0.194
iter 27 loss: 0.176
Actual params: [0.2476, 0.1944]
-Original Grad: -0.020, -lr * Pred Grad:  -0.001, New P: 0.246
-Original Grad: -0.006, -lr * Pred Grad:  0.001, New P: 0.195
iter 28 loss: 0.176
Actual params: [0.2464, 0.195 ]
-Original Grad: -0.020, -lr * Pred Grad:  -0.001, New P: 0.245
-Original Grad: -0.006, -lr * Pred Grad:  0.001, New P: 0.196
iter 29 loss: 0.176
Actual params: [0.2452, 0.1958]
-Original Grad: -0.019, -lr * Pred Grad:  -0.001, New P: 0.244
-Original Grad: -0.006, -lr * Pred Grad:  0.001, New P: 0.197
iter 30 loss: 0.176
Actual params: [0.244 , 0.1966]
Target params: [ 1.3344, -1.0472]
Actual params: [1.5544, 0.3381]
-Original Grad: 0.466, -lr * Pred Grad:  -0.045, New P: 1.509
-Original Grad: 1.529, -lr * Pred Grad:  0.169, New P: 0.507
iter 0 loss: 0.552
Actual params: [1.5092, 0.5067]
-Original Grad: 0.182, -lr * Pred Grad:  -0.306, New P: 1.203
-Original Grad: 1.172, -lr * Pred Grad:  0.144, New P: 0.651
iter 1 loss: 0.425
Actual params: [1.2033, 0.6506]
-Original Grad: 0.762, -lr * Pred Grad:  0.254, New P: 1.457
-Original Grad: -0.012, -lr * Pred Grad:  -0.059, New P: 0.591
iter 2 loss: 0.474
Actual params: [1.4569, 0.5911]
-Original Grad: 0.218, -lr * Pred Grad:  0.051, New P: 1.508
-Original Grad: 0.283, -lr * Pred Grad:  0.008, New P: 0.599
iter 3 loss: 0.397
Actual params: [1.5078, 0.5988]
-Original Grad: 0.199, -lr * Pred Grad:  0.047, New P: 1.554
-Original Grad: 0.258, -lr * Pred Grad:  0.008, New P: 0.606
iter 4 loss: 0.383
Actual params: [1.5545, 0.6064]
-Original Grad: 0.189, -lr * Pred Grad:  0.045, New P: 1.600
-Original Grad: 0.232, -lr * Pred Grad:  0.007, New P: 0.613
iter 5 loss: 0.372
Actual params: [1.5999, 0.613 ]
-Original Grad: 0.159, -lr * Pred Grad:  0.036, New P: 1.636
-Original Grad: 0.220, -lr * Pred Grad:  0.009, New P: 0.622
iter 6 loss: 0.362
Actual params: [1.6361, 0.6223]
-Original Grad: 0.153, -lr * Pred Grad:  0.038, New P: 1.674
-Original Grad: 0.192, -lr * Pred Grad:  0.007, New P: 0.630
iter 7 loss: 0.353
Actual params: [1.6739, 0.6296]
-Original Grad: 0.145, -lr * Pred Grad:  0.037, New P: 1.711
-Original Grad: 0.179, -lr * Pred Grad:  0.007, New P: 0.637
iter 8 loss: 0.345
Actual params: [1.7107, 0.637 ]
-Original Grad: 0.123, -lr * Pred Grad:  0.032, New P: 1.743
-Original Grad: 0.159, -lr * Pred Grad:  0.008, New P: 0.645
iter 9 loss: 0.338
Actual params: [1.7426, 0.645 ]
-Original Grad: 0.133, -lr * Pred Grad:  0.041, New P: 1.784
-Original Grad: 0.136, -lr * Pred Grad:  0.003, New P: 0.648
iter 10 loss: 0.332
Actual params: [1.7837, 0.6484]
-Original Grad: 0.128, -lr * Pred Grad:  0.039, New P: 1.822
-Original Grad: 0.144, -lr * Pred Grad:  0.006, New P: 0.655
iter 11 loss: 0.325
Actual params: [1.8225, 0.6546]
-Original Grad: 0.130, -lr * Pred Grad:  0.040, New P: 1.863
-Original Grad: 0.147, -lr * Pred Grad:  0.007, New P: 0.662
iter 12 loss: 0.319
Actual params: [1.8626, 0.662 ]
-Original Grad: 0.083, -lr * Pred Grad:  0.018, New P: 1.881
-Original Grad: 0.141, -lr * Pred Grad:  0.016, New P: 0.678
iter 13 loss: 0.313
Actual params: [1.8809, 0.6778]
-Original Grad: 0.085, -lr * Pred Grad:  0.027, New P: 1.908
-Original Grad: 0.111, -lr * Pred Grad:  0.009, New P: 0.687
iter 14 loss: 0.308
Actual params: [1.9082, 0.6872]
-Original Grad: 0.090, -lr * Pred Grad:  0.033, New P: 1.941
-Original Grad: 0.107, -lr * Pred Grad:  0.008, New P: 0.695
iter 15 loss: 0.303
Actual params: [1.9413, 0.6952]
-Original Grad: 0.095, -lr * Pred Grad:  0.040, New P: 1.981
-Original Grad: 0.101, -lr * Pred Grad:  0.006, New P: 0.701
iter 16 loss: 0.299
Actual params: [1.9813, 0.7011]
-Original Grad: 0.105, -lr * Pred Grad:  0.050, New P: 2.031
-Original Grad: 0.092, -lr * Pred Grad:  0.002, New P: 0.703
iter 17 loss: 0.294
Actual params: [2.0312, 0.7034]
-Original Grad: 0.102, -lr * Pred Grad:  0.050, New P: 2.081
-Original Grad: 0.091, -lr * Pred Grad:  0.004, New P: 0.707
iter 18 loss: 0.288
Actual params: [2.0808, 0.7069]
-Original Grad: 0.107, -lr * Pred Grad:  0.057, New P: 2.138
-Original Grad: 0.080, -lr * Pred Grad:  -0.000, New P: 0.707
iter 19 loss: 0.282
Actual params: [2.1377, 0.7068]
-Original Grad: 0.111, -lr * Pred Grad:  0.061, New P: 2.199
-Original Grad: 0.081, -lr * Pred Grad:  0.000, New P: 0.707
iter 20 loss: 0.275
Actual params: [2.1986, 0.7069]
-Original Grad: 0.201, -lr * Pred Grad:  0.115, New P: 2.314
-Original Grad: 0.023, -lr * Pred Grad:  -0.035, New P: 0.672
iter 21 loss: 0.267
Actual params: [2.3137, 0.6723]
-Original Grad: 0.185, -lr * Pred Grad:  0.080, New P: 2.394
-Original Grad: 0.097, -lr * Pred Grad:  -0.003, New P: 0.669
iter 22 loss: 0.250
Actual params: [2.3936, 0.6689]
-Original Grad: 0.163, -lr * Pred Grad:  0.068, New P: 2.462
-Original Grad: 0.074, -lr * Pred Grad:  -0.007, New P: 0.662
iter 23 loss: 0.235
Actual params: [2.4616, 0.6619]
-Original Grad: 0.140, -lr * Pred Grad:  0.058, New P: 2.519
-Original Grad: 0.059, -lr * Pred Grad:  -0.008, New P: 0.653
iter 24 loss: 0.222
Actual params: [2.5191, 0.6534]
-Original Grad: 0.128, -lr * Pred Grad:  0.052, New P: 2.571
-Original Grad: 0.048, -lr * Pred Grad:  -0.010, New P: 0.643
iter 25 loss: 0.212
Actual params: [2.5713, 0.643 ]
-Original Grad: 0.116, -lr * Pred Grad:  0.048, New P: 2.619
-Original Grad: 0.039, -lr * Pred Grad:  -0.012, New P: 0.631
iter 26 loss: 0.204
Actual params: [2.6191, 0.6311]
-Original Grad: 0.104, -lr * Pred Grad:  0.043, New P: 2.663
-Original Grad: 0.032, -lr * Pred Grad:  -0.013, New P: 0.619
iter 27 loss: 0.196
Actual params: [2.6625, 0.6185]
-Original Grad: 0.090, -lr * Pred Grad:  0.040, New P: 2.703
-Original Grad: 0.021, -lr * Pred Grad:  -0.016, New P: 0.603
iter 28 loss: 0.191
Actual params: [2.7027, 0.6029]
-Original Grad: 0.081, -lr * Pred Grad:  0.037, New P: 2.739
-Original Grad: 0.020, -lr * Pred Grad:  -0.013, New P: 0.590
iter 29 loss: 0.186
Actual params: [2.7393, 0.5896]
-Original Grad: 0.075, -lr * Pred Grad:  0.035, New P: 2.774
-Original Grad: 0.018, -lr * Pred Grad:  -0.014, New P: 0.576
iter 30 loss: 0.181
Actual params: [2.7742, 0.5761]
Target params: [ 1.3344, -1.0472]
Actual params: [-0.7899, -0.493 ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.028, New P: -0.818
-Original Grad: -0.002, -lr * Pred Grad:  -0.020, New P: -0.513
iter 0 loss: 0.915
Actual params: [-0.8182, -0.513 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -0.843
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.530
iter 1 loss: 0.915
Actual params: [-0.8426, -0.5303]
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: -0.864
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.546
iter 2 loss: 0.915
Actual params: [-0.8644, -0.546 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -0.884
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.561
iter 3 loss: 0.915
Actual params: [-0.8844, -0.5605]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.903
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.574
iter 4 loss: 0.915
Actual params: [-0.9028, -0.5741]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.920
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.587
iter 5 loss: 0.915
Actual params: [-0.9202, -0.5867]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.937
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.599
iter 6 loss: 0.915
Actual params: [-0.9366, -0.5988]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.952
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.611
iter 7 loss: 0.914
Actual params: [-0.9524, -0.6105]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.967
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.622
iter 8 loss: 0.914
Actual params: [-0.9675, -0.6217]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.982
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.633
iter 9 loss: 0.914
Actual params: [-0.9821, -0.6325]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.996
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.643
iter 10 loss: 0.914
Actual params: [-0.9963, -0.6431]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.010
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.653
iter 11 loss: 0.914
Actual params: [-1.0101, -0.6533]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.024
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.663
iter 12 loss: 0.914
Actual params: [-1.0236, -0.6633]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.037
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.673
iter 13 loss: 0.914
Actual params: [-1.0369, -0.6732]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.050
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.683
iter 14 loss: 0.914
Actual params: [-1.0499, -0.6828]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.063
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.692
iter 15 loss: 0.914
Actual params: [-1.0627, -0.6924]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.075
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.702
iter 16 loss: 0.914
Actual params: [-1.0754, -0.7019]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.088
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.711
iter 17 loss: 0.914
Actual params: [-1.0879, -0.7113]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.100
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.721
iter 18 loss: 0.914
Actual params: [-1.1003, -0.7205]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.113
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.730
iter 19 loss: 0.914
Actual params: [-1.1125, -0.7299]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.125
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.739
iter 20 loss: 0.914
Actual params: [-1.1247, -0.7392]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.137
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.749
iter 21 loss: 0.914
Actual params: [-1.1367, -0.7486]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.149
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.758
iter 22 loss: 0.914
Actual params: [-1.1486, -0.758 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.160
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.767
iter 23 loss: 0.914
Actual params: [-1.1604, -0.7674]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.172
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.777
iter 24 loss: 0.914
Actual params: [-1.1721, -0.7767]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.184
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.786
iter 25 loss: 0.914
Actual params: [-1.1837, -0.786 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.195
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.795
iter 26 loss: 0.914
Actual params: [-1.1952, -0.7954]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.207
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.805
iter 27 loss: 0.914
Actual params: [-1.2066, -0.8047]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.218
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.814
iter 28 loss: 0.914
Actual params: [-1.218 , -0.8139]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.229
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.823
iter 29 loss: 0.914
Actual params: [-1.2292, -0.8231]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.240
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.832
iter 30 loss: 0.914
Actual params: [-1.2404, -0.8323]
Target params: [ 1.3344, -1.0472]
Actual params: [0.3685, 0.155 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.018, New P: 0.387
-Original Grad: 0.003, -lr * Pred Grad:  0.029, New P: 0.184
iter 0 loss: 0.823
Actual params: [0.3867, 0.1843]
-Original Grad: 0.002, -lr * Pred Grad:  0.024, New P: 0.411
-Original Grad: 0.003, -lr * Pred Grad:  0.036, New P: 0.221
iter 1 loss: 0.810
Actual params: [0.4107, 0.2206]
-Original Grad: 0.002, -lr * Pred Grad:  0.033, New P: 0.444
-Original Grad: 0.003, -lr * Pred Grad:  0.045, New P: 0.266
iter 2 loss: 0.798
Actual params: [0.4435, 0.2659]
-Original Grad: 0.003, -lr * Pred Grad:  0.048, New P: 0.492
-Original Grad: 0.004, -lr * Pred Grad:  0.059, New P: 0.325
iter 3 loss: 0.784
Actual params: [0.4917, 0.3248]
-Original Grad: 0.005, -lr * Pred Grad:  0.087, New P: 0.579
-Original Grad: 0.005, -lr * Pred Grad:  0.085, New P: 0.410
iter 4 loss: 0.771
Actual params: [0.5791, 0.4097]
-Original Grad: 0.011, -lr * Pred Grad:  0.201, New P: 0.780
-Original Grad: 0.009, -lr * Pred Grad:  0.154, New P: 0.564
iter 5 loss: 0.761
Actual params: [0.7799, 0.5639]
-Original Grad: 0.095, -lr * Pred Grad:  0.674, New P: 1.454
-Original Grad: 0.047, -lr * Pred Grad:  0.286, New P: 0.850
iter 6 loss: 0.729
Actual params: [1.4538, 0.8496]
-Original Grad: 0.089, -lr * Pred Grad:  0.407, New P: 1.861
-Original Grad: -0.149, -lr * Pred Grad:  -0.197, New P: 0.652
iter 7 loss: 0.315
Actual params: [1.8612, 0.6525]
-Original Grad: 0.009, -lr * Pred Grad:  0.085, New P: 1.946
-Original Grad: 0.056, -lr * Pred Grad:  0.071, New P: 0.724
iter 8 loss: 0.400
Actual params: [1.9459, 0.7238]
-Original Grad: -0.011, -lr * Pred Grad:  -0.082, New P: 1.864
-Original Grad: -0.037, -lr * Pred Grad:  -0.043, New P: 0.681
iter 9 loss: 0.427
Actual params: [1.8641, 0.6809]
-Original Grad: 0.007, -lr * Pred Grad:  0.047, New P: 1.911
-Original Grad: -0.007, -lr * Pred Grad:  -0.006, New P: 0.675
iter 10 loss: 0.401
Actual params: [1.9106, 0.675 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.006, New P: 1.905
-Original Grad: -0.003, -lr * Pred Grad:  -0.003, New P: 0.672
iter 11 loss: 0.414
Actual params: [1.9047, 0.6715]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 1.905
-Original Grad: -0.003, -lr * Pred Grad:  -0.003, New P: 0.668
iter 12 loss: 0.413
Actual params: [1.9045, 0.6685]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 1.904
-Original Grad: -0.002, -lr * Pred Grad:  -0.003, New P: 0.666
iter 13 loss: 0.413
Actual params: [1.9044, 0.6658]
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: 1.904
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: 0.663
iter 14 loss: 0.412
Actual params: [1.9045, 0.6634]
-Original Grad: -0.013, -lr * Pred Grad:  -0.091, New P: 1.814
-Original Grad: 0.025, -lr * Pred Grad:  0.027, New P: 0.690
iter 15 loss: 0.412
Actual params: [1.8138, 0.6903]
-Original Grad: 0.020, -lr * Pred Grad:  0.152, New P: 1.966
-Original Grad: -0.009, -lr * Pred Grad:  -0.012, New P: 0.678
iter 16 loss: 0.387
Actual params: [1.9657, 0.6778]
-Original Grad: -0.016, -lr * Pred Grad:  -0.119, New P: 1.847
-Original Grad: 0.006, -lr * Pred Grad:  0.008, New P: 0.686
iter 17 loss: 0.427
Actual params: [1.8466, 0.6861]
-Original Grad: 0.008, -lr * Pred Grad:  0.063, New P: 1.910
-Original Grad: -0.008, -lr * Pred Grad:  -0.010, New P: 0.676
iter 18 loss: 0.397
Actual params: [1.9101, 0.6761]
-Original Grad: -0.001, -lr * Pred Grad:  -0.005, New P: 1.905
-Original Grad: -0.003, -lr * Pred Grad:  -0.004, New P: 0.673
iter 19 loss: 0.414
Actual params: [1.9048, 0.6726]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: 1.906
-Original Grad: -0.003, -lr * Pred Grad:  -0.003, New P: 0.669
iter 20 loss: 0.413
Actual params: [1.9057, 0.6692]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 1.904
-Original Grad: -0.003, -lr * Pred Grad:  -0.003, New P: 0.666
iter 21 loss: 0.413
Actual params: [1.9042, 0.6664]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: 1.905
-Original Grad: -0.002, -lr * Pred Grad:  -0.003, New P: 0.664
iter 22 loss: 0.412
Actual params: [1.905 , 0.6638]
-Original Grad: -0.013, -lr * Pred Grad:  -0.104, New P: 1.801
-Original Grad: 0.025, -lr * Pred Grad:  0.031, New P: 0.695
iter 23 loss: 0.413
Actual params: [1.8015, 0.6947]
-Original Grad: 0.028, -lr * Pred Grad:  0.229, New P: 2.030
-Original Grad: -0.011, -lr * Pred Grad:  -0.020, New P: 0.674
iter 24 loss: 0.384
Actual params: [2.0303, 0.6743]
-Original Grad: -0.022, -lr * Pred Grad:  -0.167, New P: 1.863
-Original Grad: 0.008, -lr * Pred Grad:  0.015, New P: 0.689
iter 25 loss: 0.442
Actual params: [1.8634, 0.6893]
-Original Grad: 0.008, -lr * Pred Grad:  0.069, New P: 1.932
-Original Grad: -0.006, -lr * Pred Grad:  -0.010, New P: 0.680
iter 26 loss: 0.402
Actual params: [1.9321, 0.6795]
-Original Grad: -0.014, -lr * Pred Grad:  -0.107, New P: 1.825
-Original Grad: 0.007, -lr * Pred Grad:  0.011, New P: 0.691
iter 27 loss: 0.420
Actual params: [1.8251, 0.6909]
-Original Grad: 0.019, -lr * Pred Grad:  0.155, New P: 1.980
-Original Grad: -0.009, -lr * Pred Grad:  -0.018, New P: 0.672
iter 28 loss: 0.391
Actual params: [1.9797, 0.6725]
-Original Grad: -0.017, -lr * Pred Grad:  -0.127, New P: 1.852
-Original Grad: 0.007, -lr * Pred Grad:  0.014, New P: 0.686
iter 29 loss: 0.430
Actual params: [1.8522, 0.6865]
-Original Grad: 0.007, -lr * Pred Grad:  0.058, New P: 1.910
-Original Grad: -0.007, -lr * Pred Grad:  -0.012, New P: 0.675
iter 30 loss: 0.398
Actual params: [1.9105, 0.6747]
