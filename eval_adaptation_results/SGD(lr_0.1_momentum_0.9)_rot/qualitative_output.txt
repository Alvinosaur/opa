Target params: [ 1.3344, -1.0472]
Actual params: [1.084 , 0.5507]
-Original Grad: 0.011, -lr * Pred Grad:  0.001, New P: 1.085
-Original Grad: 0.289, -lr * Pred Grad:  0.029, New P: 0.580
iter 0 loss: 0.176
Actual params: [1.0851, 0.5796]
-Original Grad: 0.010, -lr * Pred Grad:  0.002, New P: 1.087
-Original Grad: 0.286, -lr * Pred Grad:  0.055, New P: 0.634
iter 1 loss: 0.168
Actual params: [1.087 , 0.6342]
-Original Grad: 0.007, -lr * Pred Grad:  0.003, New P: 1.090
-Original Grad: 0.277, -lr * Pred Grad:  0.077, New P: 0.711
iter 2 loss: 0.152
Actual params: [1.0895, 0.7111]
-Original Grad: 0.005, -lr * Pred Grad:  0.003, New P: 1.092
-Original Grad: 0.262, -lr * Pred Grad:  0.095, New P: 0.806
iter 3 loss: 0.132
Actual params: [1.0923, 0.8064]
-Original Grad: 0.005, -lr * Pred Grad:  0.003, New P: 1.095
-Original Grad: 0.240, -lr * Pred Grad:  0.110, New P: 0.916
iter 4 loss: 0.108
Actual params: [1.0954, 0.9163]
-Original Grad: 0.007, -lr * Pred Grad:  0.003, New P: 1.099
-Original Grad: 0.216, -lr * Pred Grad:  0.120, New P: 1.037
iter 5 loss: 0.083
Actual params: [1.0988, 1.0368]
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: 1.102
-Original Grad: 0.176, -lr * Pred Grad:  0.126, New P: 1.163
iter 6 loss: 0.059
Actual params: [1.1022, 1.1628]
-Original Grad: -0.011, -lr * Pred Grad:  0.002, New P: 1.104
-Original Grad: 0.123, -lr * Pred Grad:  0.126, New P: 1.288
iter 7 loss: 0.040
Actual params: [1.104 , 1.2884]
-Original Grad: -0.025, -lr * Pred Grad:  -0.001, New P: 1.103
-Original Grad: 0.107, -lr * Pred Grad:  0.124, New P: 1.412
iter 8 loss: 0.026
Actual params: [1.1032, 1.4122]
-Original Grad: -0.026, -lr * Pred Grad:  -0.003, New P: 1.100
-Original Grad: 0.085, -lr * Pred Grad:  0.120, New P: 1.532
iter 9 loss: 0.014
Actual params: [1.0999, 1.5321]
-Original Grad: -0.016, -lr * Pred Grad:  -0.005, New P: 1.095
-Original Grad: 0.050, -lr * Pred Grad:  0.113, New P: 1.645
iter 10 loss: 0.006
Actual params: [1.0953, 1.645 ]
-Original Grad: -0.008, -lr * Pred Grad:  -0.005, New P: 1.090
-Original Grad: 0.004, -lr * Pred Grad:  0.102, New P: 1.747
iter 11 loss: 0.002
Actual params: [1.0903, 1.747 ]
-Original Grad: -0.005, -lr * Pred Grad:  -0.005, New P: 1.085
-Original Grad: -0.036, -lr * Pred Grad:  0.088, New P: 1.835
iter 12 loss: 0.004
Actual params: [1.0853, 1.8353]
-Original Grad: -0.007, -lr * Pred Grad:  -0.005, New P: 1.080
-Original Grad: -0.067, -lr * Pred Grad:  0.073, New P: 1.908
iter 13 loss: 0.009
Actual params: [1.0801, 1.9081]
-Original Grad: -0.012, -lr * Pred Grad:  -0.006, New P: 1.074
-Original Grad: -0.088, -lr * Pred Grad:  0.057, New P: 1.965
iter 14 loss: 0.014
Actual params: [1.0742, 1.9648]
-Original Grad: -0.015, -lr * Pred Grad:  -0.007, New P: 1.067
-Original Grad: -0.104, -lr * Pred Grad:  0.041, New P: 2.005
iter 15 loss: 0.020
Actual params: [1.0675, 2.0054]
-Original Grad: -0.017, -lr * Pred Grad:  -0.008, New P: 1.060
-Original Grad: -0.116, -lr * Pred Grad:  0.025, New P: 2.030
iter 16 loss: 0.024
Actual params: [1.0596, 2.0303]
-Original Grad: -0.022, -lr * Pred Grad:  -0.009, New P: 1.050
-Original Grad: -0.122, -lr * Pred Grad:  0.010, New P: 2.041
iter 17 loss: 0.027
Actual params: [1.0504, 2.0406]
-Original Grad: -0.021, -lr * Pred Grad:  -0.010, New P: 1.040
-Original Grad: -0.124, -lr * Pred Grad:  -0.003, New P: 2.038
iter 18 loss: 0.028
Actual params: [1.04  , 2.0375]
-Original Grad: -0.019, -lr * Pred Grad:  -0.011, New P: 1.029
-Original Grad: -0.122, -lr * Pred Grad:  -0.015, New P: 2.022
iter 19 loss: 0.027
Actual params: [1.0289, 2.0225]
-Original Grad: -0.019, -lr * Pred Grad:  -0.012, New P: 1.017
-Original Grad: -0.118, -lr * Pred Grad:  -0.025, New P: 1.997
iter 20 loss: 0.025
Actual params: [1.0169, 1.9972]
-Original Grad: -0.016, -lr * Pred Grad:  -0.012, New P: 1.005
-Original Grad: -0.110, -lr * Pred Grad:  -0.034, New P: 1.963
iter 21 loss: 0.022
Actual params: [1.0046, 1.9633]
-Original Grad: -0.014, -lr * Pred Grad:  -0.013, New P: 0.992
-Original Grad: -0.100, -lr * Pred Grad:  -0.040, New P: 1.923
iter 22 loss: 0.018
Actual params: [0.992 , 1.9228]
-Original Grad: -0.011, -lr * Pred Grad:  -0.012, New P: 0.980
-Original Grad: -0.088, -lr * Pred Grad:  -0.045, New P: 1.878
iter 23 loss: 0.014
Actual params: [0.9797, 1.8776]
-Original Grad: -0.007, -lr * Pred Grad:  -0.012, New P: 0.968
-Original Grad: -0.073, -lr * Pred Grad:  -0.048, New P: 1.830
iter 24 loss: 0.011
Actual params: [0.9678, 1.8295]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: 0.957
-Original Grad: -0.058, -lr * Pred Grad:  -0.049, New P: 1.780
iter 25 loss: 0.007
Actual params: [0.9571, 1.7804]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: 0.947
-Original Grad: -0.042, -lr * Pred Grad:  -0.048, New P: 1.732
iter 26 loss: 0.005
Actual params: [0.9474, 1.7321]
-Original Grad: 0.002, -lr * Pred Grad:  -0.009, New P: 0.939
-Original Grad: -0.026, -lr * Pred Grad:  -0.046, New P: 1.686
iter 27 loss: 0.003
Actual params: [0.9388, 1.6859]
-Original Grad: 0.005, -lr * Pred Grad:  -0.007, New P: 0.932
-Original Grad: -0.012, -lr * Pred Grad:  -0.043, New P: 1.643
iter 28 loss: 0.002
Actual params: [0.9316, 1.6432]
-Original Grad: 0.007, -lr * Pred Grad:  -0.006, New P: 0.926
-Original Grad: 0.002, -lr * Pred Grad:  -0.038, New P: 1.605
iter 29 loss: 0.002
Actual params: [0.9258, 1.605 ]
-Original Grad: 0.008, -lr * Pred Grad:  -0.004, New P: 0.921
-Original Grad: 0.014, -lr * Pred Grad:  -0.033, New P: 1.572
iter 30 loss: 0.003
Actual params: [0.9214, 1.572 ]
Target params: [ 1.3344, -1.0472]
Actual params: [-1.0585,  1.4018]
-Original Grad: 0.039, -lr * Pred Grad:  0.004, New P: -1.055
-Original Grad: 0.025, -lr * Pred Grad:  0.003, New P: 1.404
iter 0 loss: 0.436
Actual params: [-1.0545,  1.4044]
-Original Grad: 0.041, -lr * Pred Grad:  0.008, New P: -1.047
-Original Grad: 0.026, -lr * Pred Grad:  0.005, New P: 1.409
iter 1 loss: 0.436
Actual params: [-1.0469,  1.4093]
-Original Grad: 0.044, -lr * Pred Grad:  0.011, New P: -1.036
-Original Grad: 0.028, -lr * Pred Grad:  0.007, New P: 1.416
iter 2 loss: 0.435
Actual params: [-1.0357,  1.4164]
-Original Grad: 0.047, -lr * Pred Grad:  0.015, New P: -1.021
-Original Grad: 0.029, -lr * Pred Grad:  0.009, New P: 1.426
iter 3 loss: 0.434
Actual params: [-1.0209,  1.4258]
-Original Grad: 0.051, -lr * Pred Grad:  0.018, New P: -1.003
-Original Grad: 0.031, -lr * Pred Grad:  0.012, New P: 1.437
iter 4 loss: 0.433
Actual params: [-1.0026,  1.4374]
-Original Grad: 0.057, -lr * Pred Grad:  0.022, New P: -0.980
-Original Grad: 0.034, -lr * Pred Grad:  0.014, New P: 1.451
iter 5 loss: 0.432
Actual params: [-0.9803,  1.4512]
-Original Grad: 0.063, -lr * Pred Grad:  0.026, New P: -0.954
-Original Grad: 0.036, -lr * Pred Grad:  0.016, New P: 1.467
iter 6 loss: 0.430
Actual params: [-0.954 ,  1.4671]
-Original Grad: 0.074, -lr * Pred Grad:  0.031, New P: -0.923
-Original Grad: 0.040, -lr * Pred Grad:  0.018, New P: 1.486
iter 7 loss: 0.428
Actual params: [-0.9228,  1.4855]
-Original Grad: 0.088, -lr * Pred Grad:  0.037, New P: -0.886
-Original Grad: 0.045, -lr * Pred Grad:  0.021, New P: 1.507
iter 8 loss: 0.424
Actual params: [-0.886 ,  1.5066]
-Original Grad: 0.110, -lr * Pred Grad:  0.044, New P: -0.842
-Original Grad: 0.053, -lr * Pred Grad:  0.024, New P: 1.531
iter 9 loss: 0.420
Actual params: [-0.8419,  1.5309]
-Original Grad: 0.143, -lr * Pred Grad:  0.054, New P: -0.788
-Original Grad: 0.064, -lr * Pred Grad:  0.028, New P: 1.559
iter 10 loss: 0.413
Actual params: [-0.7879,  1.5592]
-Original Grad: 0.169, -lr * Pred Grad:  0.065, New P: -0.722
-Original Grad: 0.064, -lr * Pred Grad:  0.032, New P: 1.591
iter 11 loss: 0.402
Actual params: [-0.7224,  1.5911]
-Original Grad: 0.152, -lr * Pred Grad:  0.074, New P: -0.648
-Original Grad: 0.039, -lr * Pred Grad:  0.033, New P: 1.624
iter 12 loss: 0.391
Actual params: [-0.6483,  1.6237]
-Original Grad: 0.180, -lr * Pred Grad:  0.085, New P: -0.564
-Original Grad: 0.042, -lr * Pred Grad:  0.034, New P: 1.657
iter 13 loss: 0.377
Actual params: [-0.5636,  1.6573]
-Original Grad: 0.196, -lr * Pred Grad:  0.096, New P: -0.468
-Original Grad: 0.034, -lr * Pred Grad:  0.034, New P: 1.691
iter 14 loss: 0.361
Actual params: [-0.4678,  1.6909]
-Original Grad: 0.221, -lr * Pred Grad:  0.108, New P: -0.359
-Original Grad: 0.022, -lr * Pred Grad:  0.032, New P: 1.723
iter 15 loss: 0.341
Actual params: [-0.3594,  1.7233]
-Original Grad: 0.273, -lr * Pred Grad:  0.125, New P: -0.235
-Original Grad: 0.033, -lr * Pred Grad:  0.032, New P: 1.756
iter 16 loss: 0.313
Actual params: [-0.2347,  1.7558]
-Original Grad: 0.342, -lr * Pred Grad:  0.146, New P: -0.088
-Original Grad: 0.056, -lr * Pred Grad:  0.035, New P: 1.791
iter 17 loss: 0.274
Actual params: [-0.0882,  1.7906]
-Original Grad: 0.437, -lr * Pred Grad:  0.176, New P: 0.087
-Original Grad: 0.088, -lr * Pred Grad:  0.040, New P: 1.831
iter 18 loss: 0.214
Actual params: [0.0874, 1.8307]
-Original Grad: 0.426, -lr * Pred Grad:  0.201, New P: 0.288
-Original Grad: 0.116, -lr * Pred Grad:  0.048, New P: 1.878
iter 19 loss: 0.132
Actual params: [0.2881, 1.8785]
-Original Grad: 0.232, -lr * Pred Grad:  0.204, New P: 0.492
-Original Grad: 0.060, -lr * Pred Grad:  0.049, New P: 1.928
iter 20 loss: 0.060
Actual params: [0.4919, 1.9275]
-Original Grad: 0.068, -lr * Pred Grad:  0.190, New P: 0.682
-Original Grad: -0.029, -lr * Pred Grad:  0.041, New P: 1.969
iter 21 loss: 0.030
Actual params: [0.6821, 1.9687]
-Original Grad: -0.019, -lr * Pred Grad:  0.169, New P: 0.851
-Original Grad: -0.104, -lr * Pred Grad:  0.027, New P: 1.995
iter 22 loss: 0.030
Actual params: [0.8515, 1.9953]
-Original Grad: -0.106, -lr * Pred Grad:  0.142, New P: 0.993
-Original Grad: -0.157, -lr * Pred Grad:  0.008, New P: 2.004
iter 23 loss: 0.044
Actual params: [0.9932, 2.0036]
-Original Grad: -0.019, -lr * Pred Grad:  0.126, New P: 1.119
-Original Grad: -0.203, -lr * Pred Grad:  -0.013, New P: 1.991
iter 24 loss: 0.056
Actual params: [1.119 , 1.9908]
-Original Grad: 0.053, -lr * Pred Grad:  0.118, New P: 1.237
-Original Grad: -0.181, -lr * Pred Grad:  -0.030, New P: 1.961
iter 25 loss: 0.054
Actual params: [1.2374, 1.9612]
-Original Grad: 0.072, -lr * Pred Grad:  0.114, New P: 1.351
-Original Grad: -0.136, -lr * Pred Grad:  -0.040, New P: 1.921
iter 26 loss: 0.041
Actual params: [1.3512, 1.9209]
-Original Grad: 0.081, -lr * Pred Grad:  0.111, New P: 1.462
-Original Grad: -0.094, -lr * Pred Grad:  -0.046, New P: 1.875
iter 27 loss: 0.027
Actual params: [1.4618, 1.8753]
-Original Grad: 0.053, -lr * Pred Grad:  0.105, New P: 1.567
-Original Grad: -0.068, -lr * Pred Grad:  -0.048, New P: 1.827
iter 28 loss: 0.016
Actual params: [1.5666, 1.8275]
-Original Grad: 0.023, -lr * Pred Grad:  0.097, New P: 1.663
-Original Grad: -0.036, -lr * Pred Grad:  -0.047, New P: 1.781
iter 29 loss: 0.009
Actual params: [1.6634, 1.7808]
-Original Grad: 0.003, -lr * Pred Grad:  0.087, New P: 1.751
-Original Grad: 0.004, -lr * Pred Grad:  -0.042, New P: 1.739
iter 30 loss: 0.007
Actual params: [1.7507, 1.7392]
Target params: [ 1.3344, -1.0472]
Actual params: [1.5477, 0.5327]
-Original Grad: -0.149, -lr * Pred Grad:  -0.015, New P: 1.533
-Original Grad: 0.399, -lr * Pred Grad:  0.040, New P: 0.573
iter 0 loss: 0.487
Actual params: [1.5327, 0.5725]
-Original Grad: -0.160, -lr * Pred Grad:  -0.029, New P: 1.503
-Original Grad: 0.405, -lr * Pred Grad:  0.076, New P: 0.649
iter 1 loss: 0.469
Actual params: [1.5033, 0.6489]
-Original Grad: -0.166, -lr * Pred Grad:  -0.043, New P: 1.460
-Original Grad: 0.394, -lr * Pred Grad:  0.108, New P: 0.757
iter 2 loss: 0.433
Actual params: [1.4602, 0.7571]
-Original Grad: -0.209, -lr * Pred Grad:  -0.060, New P: 1.400
-Original Grad: 0.402, -lr * Pred Grad:  0.138, New P: 0.895
iter 3 loss: 0.383
Actual params: [1.4005, 0.8946]
-Original Grad: -0.303, -lr * Pred Grad:  -0.084, New P: 1.316
-Original Grad: 0.341, -lr * Pred Grad:  0.158, New P: 1.053
iter 4 loss: 0.311
Actual params: [1.3165, 1.0526]
-Original Grad: -0.260, -lr * Pred Grad:  -0.102, New P: 1.215
-Original Grad: 0.229, -lr * Pred Grad:  0.165, New P: 1.218
iter 5 loss: 0.244
Actual params: [1.2149, 1.2176]
-Original Grad: -0.187, -lr * Pred Grad:  -0.110, New P: 1.105
-Original Grad: 0.192, -lr * Pred Grad:  0.168, New P: 1.385
iter 6 loss: 0.188
Actual params: [1.1048, 1.3853]
-Original Grad: -0.004, -lr * Pred Grad:  -0.099, New P: 1.005
-Original Grad: 0.220, -lr * Pred Grad:  0.173, New P: 1.558
iter 7 loss: 0.136
Actual params: [1.0054, 1.5582]
-Original Grad: 0.247, -lr * Pred Grad:  -0.065, New P: 0.941
-Original Grad: 0.083, -lr * Pred Grad:  0.164, New P: 1.722
iter 8 loss: 0.126
Actual params: [0.9407, 1.7222]
-Original Grad: 0.436, -lr * Pred Grad:  -0.015, New P: 0.926
-Original Grad: -0.000, -lr * Pred Grad:  0.148, New P: 1.870
iter 9 loss: 0.140
Actual params: [0.926 , 1.8697]
-Original Grad: 0.511, -lr * Pred Grad:  0.038, New P: 0.964
-Original Grad: -0.075, -lr * Pred Grad:  0.125, New P: 1.995
iter 10 loss: 0.153
Actual params: [0.9639, 1.995 ]
-Original Grad: 0.448, -lr * Pred Grad:  0.079, New P: 1.043
-Original Grad: -0.140, -lr * Pred Grad:  0.099, New P: 2.094
iter 11 loss: 0.148
Actual params: [1.0428, 2.0937]
-Original Grad: 0.307, -lr * Pred Grad:  0.102, New P: 1.145
-Original Grad: -0.187, -lr * Pred Grad:  0.070, New P: 2.164
iter 12 loss: 0.133
Actual params: [1.1445, 2.1639]
-Original Grad: 0.311, -lr * Pred Grad:  0.123, New P: 1.267
-Original Grad: -0.134, -lr * Pred Grad:  0.050, New P: 2.214
iter 13 loss: 0.111
Actual params: [1.2672, 2.2136]
-Original Grad: 0.248, -lr * Pred Grad:  0.135, New P: 1.402
-Original Grad: -0.085, -lr * Pred Grad:  0.036, New P: 2.250
iter 14 loss: 0.084
Actual params: [1.4025, 2.2498]
-Original Grad: 0.371, -lr * Pred Grad:  0.159, New P: 1.561
-Original Grad: -0.099, -lr * Pred Grad:  0.023, New P: 2.273
iter 15 loss: 0.047
Actual params: [1.5613, 2.2725]
-Original Grad: -0.436, -lr * Pred Grad:  0.099, New P: 1.661
-Original Grad: 0.096, -lr * Pred Grad:  0.030, New P: 2.303
iter 16 loss: 0.045
Actual params: [1.6606, 2.3026]
-Original Grad: -0.964, -lr * Pred Grad:  -0.007, New P: 1.654
-Original Grad: 0.018, -lr * Pred Grad:  0.029, New P: 2.331
iter 17 loss: 0.112
Actual params: [1.6536, 2.3315]
-Original Grad: -0.984, -lr * Pred Grad:  -0.105, New P: 1.549
-Original Grad: -0.012, -lr * Pred Grad:  0.025, New P: 2.356
iter 18 loss: 0.105
Actual params: [1.5489, 2.3563]
-Original Grad: -0.251, -lr * Pred Grad:  -0.119, New P: 1.430
-Original Grad: -0.012, -lr * Pred Grad:  0.021, New P: 2.377
iter 19 loss: 0.036
Actual params: [1.4296, 2.3775]
-Original Grad: 0.242, -lr * Pred Grad:  -0.083, New P: 1.347
-Original Grad: -0.113, -lr * Pred Grad:  0.008, New P: 2.385
iter 20 loss: 0.053
Actual params: [1.3465, 2.3852]
-Original Grad: 0.281, -lr * Pred Grad:  -0.047, New P: 1.300
-Original Grad: -0.102, -lr * Pred Grad:  -0.003, New P: 2.382
iter 21 loss: 0.076
Actual params: [1.2998, 2.382 ]
-Original Grad: 0.286, -lr * Pred Grad:  -0.013, New P: 1.286
-Original Grad: -0.101, -lr * Pred Grad:  -0.013, New P: 2.369
iter 22 loss: 0.089
Actual params: [1.2864, 2.369 ]
-Original Grad: 0.310, -lr * Pred Grad:  0.019, New P: 1.305
-Original Grad: -0.091, -lr * Pred Grad:  -0.021, New P: 2.348
iter 23 loss: 0.091
Actual params: [1.3053, 2.3482]
-Original Grad: 0.275, -lr * Pred Grad:  0.045, New P: 1.350
-Original Grad: -0.076, -lr * Pred Grad:  -0.026, New P: 2.322
iter 24 loss: 0.084
Actual params: [1.3498, 2.3219]
-Original Grad: 0.278, -lr * Pred Grad:  0.068, New P: 1.418
-Original Grad: -0.054, -lr * Pred Grad:  -0.029, New P: 2.293
iter 25 loss: 0.071
Actual params: [1.4177, 2.2929]
-Original Grad: 0.397, -lr * Pred Grad:  0.101, New P: 1.518
-Original Grad: -0.123, -lr * Pred Grad:  -0.038, New P: 2.254
iter 26 loss: 0.046
Actual params: [1.5184, 2.2544]
-Original Grad: -0.183, -lr * Pred Grad:  0.072, New P: 1.591
-Original Grad: 0.068, -lr * Pred Grad:  -0.028, New P: 2.227
iter 27 loss: 0.033
Actual params: [1.5908, 2.2266]
-Original Grad: -0.590, -lr * Pred Grad:  0.006, New P: 1.597
-Original Grad: 0.135, -lr * Pred Grad:  -0.012, New P: 2.215
iter 28 loss: 0.065
Actual params: [1.597, 2.215]
-Original Grad: -0.605, -lr * Pred Grad:  -0.055, New P: 1.542
-Original Grad: 0.138, -lr * Pred Grad:  0.003, New P: 2.218
iter 29 loss: 0.071
Actual params: [1.542 , 2.2184]
-Original Grad: -0.361, -lr * Pred Grad:  -0.086, New P: 1.456
-Original Grad: 0.122, -lr * Pred Grad:  0.015, New P: 2.234
iter 30 loss: 0.043
Actual params: [1.4565, 2.2337]
Target params: [ 1.3344, -1.0472]
Actual params: [0.0029, 0.9353]
-Original Grad: 0.212, -lr * Pred Grad:  0.021, New P: 0.024
-Original Grad: 0.097, -lr * Pred Grad:  0.010, New P: 0.945
iter 0 loss: 0.137
Actual params: [0.0241, 0.945 ]
-Original Grad: 0.224, -lr * Pred Grad:  0.041, New P: 0.066
-Original Grad: 0.097, -lr * Pred Grad:  0.018, New P: 0.963
iter 1 loss: 0.132
Actual params: [0.0656, 0.9634]
-Original Grad: 0.223, -lr * Pred Grad:  0.060, New P: 0.125
-Original Grad: 0.083, -lr * Pred Grad:  0.025, New P: 0.988
iter 2 loss: 0.121
Actual params: [0.1252, 0.9883]
-Original Grad: 0.206, -lr * Pred Grad:  0.074, New P: 0.200
-Original Grad: 0.053, -lr * Pred Grad:  0.028, New P: 1.016
iter 3 loss: 0.106
Actual params: [0.1995, 1.016 ]
-Original Grad: 0.185, -lr * Pred Grad:  0.085, New P: 0.285
-Original Grad: 0.030, -lr * Pred Grad:  0.028, New P: 1.044
iter 4 loss: 0.091
Actual params: [0.2849, 1.044 ]
-Original Grad: 0.160, -lr * Pred Grad:  0.093, New P: 0.378
-Original Grad: 0.016, -lr * Pred Grad:  0.027, New P: 1.071
iter 5 loss: 0.075
Actual params: [0.3777, 1.0706]
-Original Grad: 0.128, -lr * Pred Grad:  0.096, New P: 0.474
-Original Grad: 0.009, -lr * Pred Grad:  0.025, New P: 1.096
iter 6 loss: 0.061
Actual params: [0.4741, 1.0956]
-Original Grad: 0.096, -lr * Pred Grad:  0.096, New P: 0.570
-Original Grad: 0.007, -lr * Pred Grad:  0.023, New P: 1.119
iter 7 loss: 0.050
Actual params: [0.5705, 1.1187]
-Original Grad: 0.044, -lr * Pred Grad:  0.091, New P: 0.662
-Original Grad: 0.038, -lr * Pred Grad:  0.025, New P: 1.143
iter 8 loss: 0.043
Actual params: [0.6615, 1.1433]
-Original Grad: 0.016, -lr * Pred Grad:  0.084, New P: 0.745
-Original Grad: 0.070, -lr * Pred Grad:  0.029, New P: 1.172
iter 9 loss: 0.039
Actual params: [0.7452, 1.1724]
-Original Grad: 0.005, -lr * Pred Grad:  0.076, New P: 0.821
-Original Grad: 0.096, -lr * Pred Grad:  0.036, New P: 1.208
iter 10 loss: 0.035
Actual params: [0.8209, 1.2082]
-Original Grad: -0.024, -lr * Pred Grad:  0.066, New P: 0.887
-Original Grad: 0.105, -lr * Pred Grad:  0.043, New P: 1.251
iter 11 loss: 0.032
Actual params: [0.8867, 1.2509]
-Original Grad: -0.005, -lr * Pred Grad:  0.059, New P: 0.945
-Original Grad: 0.097, -lr * Pred Grad:  0.048, New P: 1.299
iter 12 loss: 0.029
Actual params: [0.9453, 1.2991]
-Original Grad: 0.045, -lr * Pred Grad:  0.057, New P: 1.003
-Original Grad: 0.083, -lr * Pred Grad:  0.052, New P: 1.351
iter 13 loss: 0.023
Actual params: [1.0027, 1.3508]
-Original Grad: 0.036, -lr * Pred Grad:  0.055, New P: 1.058
-Original Grad: 0.054, -lr * Pred Grad:  0.052, New P: 1.403
iter 14 loss: 0.017
Actual params: [1.0579, 1.4026]
-Original Grad: 0.022, -lr * Pred Grad:  0.052, New P: 1.110
-Original Grad: 0.024, -lr * Pred Grad:  0.049, New P: 1.452
iter 15 loss: 0.013
Actual params: [1.1098, 1.4518]
-Original Grad: 0.004, -lr * Pred Grad:  0.047, New P: 1.157
-Original Grad: -0.001, -lr * Pred Grad:  0.044, New P: 1.496
iter 16 loss: 0.012
Actual params: [1.1569, 1.496 ]
-Original Grad: -0.011, -lr * Pred Grad:  0.041, New P: 1.198
-Original Grad: -0.014, -lr * Pred Grad:  0.038, New P: 1.534
iter 17 loss: 0.012
Actual params: [1.1983, 1.5343]
-Original Grad: -0.018, -lr * Pred Grad:  0.035, New P: 1.234
-Original Grad: -0.025, -lr * Pred Grad:  0.032, New P: 1.566
iter 18 loss: 0.014
Actual params: [1.2337, 1.5663]
-Original Grad: -0.025, -lr * Pred Grad:  0.029, New P: 1.263
-Original Grad: -0.035, -lr * Pred Grad:  0.025, New P: 1.592
iter 19 loss: 0.016
Actual params: [1.263 , 1.5916]
-Original Grad: -0.031, -lr * Pred Grad:  0.023, New P: 1.286
-Original Grad: -0.044, -lr * Pred Grad:  0.018, New P: 1.610
iter 20 loss: 0.017
Actual params: [1.2862, 1.61  ]
-Original Grad: -0.036, -lr * Pred Grad:  0.017, New P: 1.303
-Original Grad: -0.050, -lr * Pred Grad:  0.012, New P: 1.622
iter 21 loss: 0.019
Actual params: [1.3035, 1.6215]
-Original Grad: -0.039, -lr * Pred Grad:  0.012, New P: 1.315
-Original Grad: -0.055, -lr * Pred Grad:  0.005, New P: 1.626
iter 22 loss: 0.020
Actual params: [1.3152, 1.6264]
-Original Grad: -0.041, -lr * Pred Grad:  0.006, New P: 1.322
-Original Grad: -0.057, -lr * Pred Grad:  -0.001, New P: 1.625
iter 23 loss: 0.021
Actual params: [1.3215, 1.6251]
-Original Grad: -0.043, -lr * Pred Grad:  0.001, New P: 1.323
-Original Grad: -0.057, -lr * Pred Grad:  -0.007, New P: 1.618
iter 24 loss: 0.021
Actual params: [1.323 , 1.6183]
-Original Grad: -0.042, -lr * Pred Grad:  -0.003, New P: 1.320
-Original Grad: -0.055, -lr * Pred Grad:  -0.012, New P: 1.607
iter 25 loss: 0.021
Actual params: [1.3201, 1.6067]
-Original Grad: -0.041, -lr * Pred Grad:  -0.007, New P: 1.313
-Original Grad: -0.051, -lr * Pred Grad:  -0.016, New P: 1.591
iter 26 loss: 0.020
Actual params: [1.3135, 1.5911]
-Original Grad: -0.040, -lr * Pred Grad:  -0.010, New P: 1.304
-Original Grad: -0.045, -lr * Pred Grad:  -0.019, New P: 1.573
iter 27 loss: 0.019
Actual params: [1.3035, 1.5726]
-Original Grad: -0.038, -lr * Pred Grad:  -0.013, New P: 1.291
-Original Grad: -0.039, -lr * Pred Grad:  -0.021, New P: 1.552
iter 28 loss: 0.018
Actual params: [1.2908, 1.5521]
-Original Grad: -0.034, -lr * Pred Grad:  -0.015, New P: 1.276
-Original Grad: -0.031, -lr * Pred Grad:  -0.022, New P: 1.530
iter 29 loss: 0.017
Actual params: [1.2759, 1.5304]
-Original Grad: -0.032, -lr * Pred Grad:  -0.017, New P: 1.259
-Original Grad: -0.025, -lr * Pred Grad:  -0.022, New P: 1.508
iter 30 loss: 0.016
Actual params: [1.2592, 1.5084]
Target params: [ 1.3344, -1.0472]
Actual params: [-0.6756, -1.5044]
-Original Grad: -0.021, -lr * Pred Grad:  -0.002, New P: -0.678
-Original Grad: 0.005, -lr * Pred Grad:  0.001, New P: -1.504
iter 0 loss: 0.469
Actual params: [-0.6777, -1.5039]
-Original Grad: -0.020, -lr * Pred Grad:  -0.004, New P: -0.682
-Original Grad: 0.005, -lr * Pred Grad:  0.001, New P: -1.503
iter 1 loss: 0.469
Actual params: [-0.6816, -1.5029]
-Original Grad: -0.020, -lr * Pred Grad:  -0.005, New P: -0.687
-Original Grad: 0.005, -lr * Pred Grad:  0.001, New P: -1.502
iter 2 loss: 0.469
Actual params: [-0.6871, -1.5015]
-Original Grad: -0.019, -lr * Pred Grad:  -0.007, New P: -0.694
-Original Grad: 0.005, -lr * Pred Grad:  0.002, New P: -1.500
iter 3 loss: 0.468
Actual params: [-0.6939, -1.4998]
-Original Grad: -0.018, -lr * Pred Grad:  -0.008, New P: -0.702
-Original Grad: 0.004, -lr * Pred Grad:  0.002, New P: -1.498
iter 4 loss: 0.468
Actual params: [-0.7019, -1.4979]
-Original Grad: -0.017, -lr * Pred Grad:  -0.009, New P: -0.711
-Original Grad: 0.004, -lr * Pred Grad:  0.002, New P: -1.496
iter 5 loss: 0.468
Actual params: [-0.7106, -1.4957]
-Original Grad: -0.015, -lr * Pred Grad:  -0.009, New P: -0.720
-Original Grad: 0.004, -lr * Pred Grad:  0.002, New P: -1.493
iter 6 loss: 0.468
Actual params: [-0.7201, -1.4933]
-Original Grad: -0.014, -lr * Pred Grad:  -0.010, New P: -0.730
-Original Grad: 0.004, -lr * Pred Grad:  0.003, New P: -1.491
iter 7 loss: 0.468
Actual params: [-0.7299, -1.4908]
-Original Grad: -0.013, -lr * Pred Grad:  -0.010, New P: -0.740
-Original Grad: 0.003, -lr * Pred Grad:  0.003, New P: -1.488
iter 8 loss: 0.468
Actual params: [-0.7401, -1.4882]
-Original Grad: -0.012, -lr * Pred Grad:  -0.010, New P: -0.750
-Original Grad: 0.003, -lr * Pred Grad:  0.003, New P: -1.486
iter 9 loss: 0.468
Actual params: [-0.7505, -1.4855]
-Original Grad: -0.011, -lr * Pred Grad:  -0.010, New P: -0.761
-Original Grad: 0.003, -lr * Pred Grad:  0.003, New P: -1.483
iter 10 loss: 0.467
Actual params: [-0.7609, -1.4829]
-Original Grad: -0.010, -lr * Pred Grad:  -0.010, New P: -0.771
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: -1.480
iter 11 loss: 0.467
Actual params: [-0.7712, -1.4802]
-Original Grad: -0.009, -lr * Pred Grad:  -0.010, New P: -0.781
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: -1.478
iter 12 loss: 0.467
Actual params: [-0.7814, -1.4776]
-Original Grad: -0.008, -lr * Pred Grad:  -0.010, New P: -0.791
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: -1.475
iter 13 loss: 0.467
Actual params: [-0.7914, -1.4751]
-Original Grad: -0.008, -lr * Pred Grad:  -0.010, New P: -0.801
-Original Grad: 0.002, -lr * Pred Grad:  0.002, New P: -1.473
iter 14 loss: 0.467
Actual params: [-0.8012, -1.4726]
-Original Grad: -0.007, -lr * Pred Grad:  -0.009, New P: -0.811
-Original Grad: 0.002, -lr * Pred Grad:  0.002, New P: -1.470
iter 15 loss: 0.467
Actual params: [-0.8107, -1.4702]
-Original Grad: -0.006, -lr * Pred Grad:  -0.009, New P: -0.820
-Original Grad: 0.002, -lr * Pred Grad:  0.002, New P: -1.468
iter 16 loss: 0.467
Actual params: [-0.8199, -1.4679]
-Original Grad: -0.006, -lr * Pred Grad:  -0.009, New P: -0.829
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: -1.466
iter 17 loss: 0.467
Actual params: [-0.8287, -1.4657]
-Original Grad: -0.005, -lr * Pred Grad:  -0.009, New P: -0.837
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: -1.464
iter 18 loss: 0.467
Actual params: [-0.8372, -1.4636]
-Original Grad: -0.005, -lr * Pred Grad:  -0.008, New P: -0.845
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: -1.462
iter 19 loss: 0.467
Actual params: [-0.8454, -1.4616]
-Original Grad: -0.005, -lr * Pred Grad:  -0.008, New P: -0.853
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: -1.460
iter 20 loss: 0.467
Actual params: [-0.8533, -1.4596]
-Original Grad: -0.004, -lr * Pred Grad:  -0.007, New P: -0.861
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: -1.458
iter 21 loss: 0.467
Actual params: [-0.8607, -1.4577]
-Original Grad: -0.004, -lr * Pred Grad:  -0.007, New P: -0.868
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: -1.456
iter 22 loss: 0.467
Actual params: [-0.8679, -1.456 ]
-Original Grad: -0.004, -lr * Pred Grad:  -0.007, New P: -0.875
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: -1.454
iter 23 loss: 0.467
Actual params: [-0.8747, -1.4543]
-Original Grad: -0.004, -lr * Pred Grad:  -0.007, New P: -0.881
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: -1.453
iter 24 loss: 0.467
Actual params: [-0.8812, -1.4526]
-Original Grad: -0.003, -lr * Pred Grad:  -0.006, New P: -0.887
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: -1.451
iter 25 loss: 0.467
Actual params: [-0.8874, -1.4511]
-Original Grad: -0.003, -lr * Pred Grad:  -0.006, New P: -0.893
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: -1.450
iter 26 loss: 0.466
Actual params: [-0.8933, -1.4496]
-Original Grad: -0.003, -lr * Pred Grad:  -0.006, New P: -0.899
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: -1.448
iter 27 loss: 0.466
Actual params: [-0.8989, -1.4482]
-Original Grad: -0.003, -lr * Pred Grad:  -0.005, New P: -0.904
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: -1.447
iter 28 loss: 0.466
Actual params: [-0.9043, -1.4469]
-Original Grad: -0.003, -lr * Pred Grad:  -0.005, New P: -0.909
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: -1.446
iter 29 loss: 0.466
Actual params: [-0.9094, -1.4456]
-Original Grad: -0.003, -lr * Pred Grad:  -0.005, New P: -0.914
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: -1.444
iter 30 loss: 0.466
Actual params: [-0.9143, -1.4444]
Target params: [ 1.3344, -1.0472]
Actual params: [-0.6634, -0.2295]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.230
iter 0 loss: 0.228
Actual params: [-0.6633, -0.2295]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.230
iter 1 loss: 0.228
Actual params: [-0.6633, -0.2295]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.230
iter 2 loss: 0.228
Actual params: [-0.6633, -0.2295]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 3 loss: 0.228
Actual params: [-0.6632, -0.2295]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 4 loss: 0.228
Actual params: [-0.6631, -0.2295]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 5 loss: 0.228
Actual params: [-0.663 , -0.2294]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 6 loss: 0.228
Actual params: [-0.6629, -0.2294]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 7 loss: 0.228
Actual params: [-0.6628, -0.2294]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 8 loss: 0.228
Actual params: [-0.6627, -0.2293]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 9 loss: 0.228
Actual params: [-0.6626, -0.2293]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.662
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 10 loss: 0.228
Actual params: [-0.6625, -0.2293]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.662
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 11 loss: 0.228
Actual params: [-0.6623, -0.2292]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.662
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 12 loss: 0.228
Actual params: [-0.6622, -0.2292]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.662
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 13 loss: 0.228
Actual params: [-0.6621, -0.2292]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.662
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 14 loss: 0.228
Actual params: [-0.6619, -0.2291]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.662
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 15 loss: 0.228
Actual params: [-0.6618, -0.2291]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.662
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 16 loss: 0.228
Actual params: [-0.6616, -0.229 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.661
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 17 loss: 0.228
Actual params: [-0.6615, -0.229 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.661
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 18 loss: 0.228
Actual params: [-0.6613, -0.2289]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.661
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 19 loss: 0.228
Actual params: [-0.6611, -0.2289]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.661
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 20 loss: 0.228
Actual params: [-0.661 , -0.2288]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.661
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 21 loss: 0.228
Actual params: [-0.6608, -0.2288]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.661
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 22 loss: 0.228
Actual params: [-0.6607, -0.2288]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.660
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 23 loss: 0.228
Actual params: [-0.6605, -0.2287]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.660
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 24 loss: 0.228
Actual params: [-0.6603, -0.2287]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.660
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 25 loss: 0.228
Actual params: [-0.6601, -0.2286]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.660
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 26 loss: 0.228
Actual params: [-0.66  , -0.2286]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.660
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.229
iter 27 loss: 0.228
Actual params: [-0.6598, -0.2285]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.660
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.228
iter 28 loss: 0.228
Actual params: [-0.6596, -0.2285]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.659
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.228
iter 29 loss: 0.228
Actual params: [-0.6594, -0.2284]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.659
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.228
iter 30 loss: 0.228
Actual params: [-0.6593, -0.2283]
Target params: [ 1.3344, -1.0472]
Actual params: [-0.8962,  0.1733]
-Original Grad: 0.049, -lr * Pred Grad:  0.005, New P: -0.891
-Original Grad: -0.018, -lr * Pred Grad:  -0.002, New P: 0.171
iter 0 loss: 0.152
Actual params: [-0.8913,  0.1715]
-Original Grad: 0.051, -lr * Pred Grad:  0.010, New P: -0.882
-Original Grad: -0.019, -lr * Pred Grad:  -0.004, New P: 0.168
iter 1 loss: 0.152
Actual params: [-0.8818,  0.1679]
-Original Grad: 0.053, -lr * Pred Grad:  0.014, New P: -0.868
-Original Grad: -0.020, -lr * Pred Grad:  -0.005, New P: 0.163
iter 2 loss: 0.151
Actual params: [-0.8679,  0.1627]
-Original Grad: 0.054, -lr * Pred Grad:  0.018, New P: -0.850
-Original Grad: -0.021, -lr * Pred Grad:  -0.007, New P: 0.156
iter 3 loss: 0.151
Actual params: [-0.85  ,  0.1559]
-Original Grad: 0.058, -lr * Pred Grad:  0.022, New P: -0.828
-Original Grad: -0.022, -lr * Pred Grad:  -0.008, New P: 0.148
iter 4 loss: 0.149
Actual params: [-0.8281,  0.1476]
-Original Grad: 0.060, -lr * Pred Grad:  0.026, New P: -0.802
-Original Grad: -0.021, -lr * Pred Grad:  -0.010, New P: 0.138
iter 5 loss: 0.148
Actual params: [-0.8024,  0.138 ]
-Original Grad: 0.065, -lr * Pred Grad:  0.030, New P: -0.773
-Original Grad: -0.023, -lr * Pred Grad:  -0.011, New P: 0.127
iter 6 loss: 0.146
Actual params: [-0.7728,  0.1271]
-Original Grad: 0.070, -lr * Pred Grad:  0.034, New P: -0.739
-Original Grad: -0.025, -lr * Pred Grad:  -0.012, New P: 0.115
iter 7 loss: 0.144
Actual params: [-0.7392,  0.1146]
-Original Grad: 0.075, -lr * Pred Grad:  0.038, New P: -0.701
-Original Grad: -0.024, -lr * Pred Grad:  -0.014, New P: 0.101
iter 8 loss: 0.141
Actual params: [-0.7014,  0.1011]
-Original Grad: 0.072, -lr * Pred Grad:  0.041, New P: -0.660
-Original Grad: -0.014, -lr * Pred Grad:  -0.014, New P: 0.087
iter 9 loss: 0.138
Actual params: [-0.6602,  0.0874]
-Original Grad: 0.077, -lr * Pred Grad:  0.045, New P: -0.615
-Original Grad: -0.011, -lr * Pred Grad:  -0.013, New P: 0.074
iter 10 loss: 0.135
Actual params: [-0.6154,  0.074 ]
-Original Grad: 0.083, -lr * Pred Grad:  0.049, New P: -0.567
-Original Grad: -0.009, -lr * Pred Grad:  -0.013, New P: 0.061
iter 11 loss: 0.131
Actual params: [-0.5668,  0.061 ]
-Original Grad: 0.087, -lr * Pred Grad:  0.052, New P: -0.514
-Original Grad: -0.003, -lr * Pred Grad:  -0.012, New P: 0.049
iter 12 loss: 0.127
Actual params: [-0.5143,  0.049 ]
-Original Grad: 0.091, -lr * Pred Grad:  0.056, New P: -0.458
-Original Grad: 0.003, -lr * Pred Grad:  -0.010, New P: 0.039
iter 13 loss: 0.122
Actual params: [-0.4579,  0.0385]
-Original Grad: 0.100, -lr * Pred Grad:  0.061, New P: -0.397
-Original Grad: 0.006, -lr * Pred Grad:  -0.009, New P: 0.030
iter 14 loss: 0.117
Actual params: [-0.3972,  0.0297]
-Original Grad: 0.107, -lr * Pred Grad:  0.065, New P: -0.332
-Original Grad: 0.010, -lr * Pred Grad:  -0.007, New P: 0.023
iter 15 loss: 0.111
Actual params: [-0.3318,  0.0228]
-Original Grad: 0.115, -lr * Pred Grad:  0.070, New P: -0.262
-Original Grad: 0.015, -lr * Pred Grad:  -0.005, New P: 0.018
iter 16 loss: 0.104
Actual params: [-0.2616,  0.018 ]
-Original Grad: 0.122, -lr * Pred Grad:  0.075, New P: -0.186
-Original Grad: 0.019, -lr * Pred Grad:  -0.002, New P: 0.016
iter 17 loss: 0.095
Actual params: [-0.1861,  0.0156]
-Original Grad: 0.128, -lr * Pred Grad:  0.081, New P: -0.105
-Original Grad: 0.022, -lr * Pred Grad:  0.000, New P: 0.016
iter 18 loss: 0.086
Actual params: [-0.1054,  0.0157]
-Original Grad: 0.136, -lr * Pred Grad:  0.086, New P: -0.019
-Original Grad: 0.025, -lr * Pred Grad:  0.003, New P: 0.018
iter 19 loss: 0.075
Actual params: [-0.0191,  0.0183]
-Original Grad: 0.137, -lr * Pred Grad:  0.091, New P: 0.072
-Original Grad: 0.025, -lr * Pred Grad:  0.005, New P: 0.023
iter 20 loss: 0.063
Actual params: [0.0722, 0.0232]
-Original Grad: 0.124, -lr * Pred Grad:  0.095, New P: 0.167
-Original Grad: 0.030, -lr * Pred Grad:  0.007, New P: 0.031
iter 21 loss: 0.051
Actual params: [0.1668, 0.0305]
-Original Grad: 0.082, -lr * Pred Grad:  0.093, New P: 0.260
-Original Grad: 0.032, -lr * Pred Grad:  0.010, New P: 0.040
iter 22 loss: 0.041
Actual params: [0.2602, 0.0403]
-Original Grad: 0.022, -lr * Pred Grad:  0.086, New P: 0.346
-Original Grad: 0.029, -lr * Pred Grad:  0.012, New P: 0.052
iter 23 loss: 0.035
Actual params: [0.3464, 0.052 ]
-Original Grad: -0.036, -lr * Pred Grad:  0.074, New P: 0.420
-Original Grad: 0.014, -lr * Pred Grad:  0.012, New P: 0.064
iter 24 loss: 0.036
Actual params: [0.4205, 0.064 ]
-Original Grad: -0.114, -lr * Pred Grad:  0.055, New P: 0.476
-Original Grad: -0.020, -lr * Pred Grad:  0.009, New P: 0.073
iter 25 loss: 0.041
Actual params: [0.4757, 0.0728]
-Original Grad: -0.147, -lr * Pred Grad:  0.035, New P: 0.511
-Original Grad: -0.048, -lr * Pred Grad:  0.003, New P: 0.076
iter 26 loss: 0.049
Actual params: [0.5107, 0.0759]
-Original Grad: -0.161, -lr * Pred Grad:  0.015, New P: 0.526
-Original Grad: -0.081, -lr * Pred Grad:  -0.005, New P: 0.071
iter 27 loss: 0.054
Actual params: [0.5261, 0.0706]
-Original Grad: -0.140, -lr * Pred Grad:  -0.000, New P: 0.526
-Original Grad: -0.082, -lr * Pred Grad:  -0.013, New P: 0.058
iter 28 loss: 0.056
Actual params: [0.5261, 0.0577]
-Original Grad: -0.133, -lr * Pred Grad:  -0.013, New P: 0.513
-Original Grad: -0.064, -lr * Pred Grad:  -0.018, New P: 0.040
iter 29 loss: 0.055
Actual params: [0.5127, 0.0396]
-Original Grad: -0.139, -lr * Pred Grad:  -0.026, New P: 0.487
-Original Grad: -0.058, -lr * Pred Grad:  -0.022, New P: 0.018
iter 30 loss: 0.053
Actual params: [0.4868, 0.0176]
Target params: [ 1.3344, -1.0472]
Actual params: [1.5544, 0.3381]
-Original Grad: 0.270, -lr * Pred Grad:  0.027, New P: 1.581
-Original Grad: 0.892, -lr * Pred Grad:  0.089, New P: 0.427
iter 0 loss: 0.258
Actual params: [1.5814, 0.4273]
-Original Grad: 0.181, -lr * Pred Grad:  0.042, New P: 1.624
-Original Grad: 0.733, -lr * Pred Grad:  0.154, New P: 0.581
iter 1 loss: 0.176
Actual params: [1.6239, 0.581 ]
-Original Grad: 0.103, -lr * Pred Grad:  0.048, New P: 1.672
-Original Grad: 0.196, -lr * Pred Grad:  0.158, New P: 0.739
iter 2 loss: 0.097
Actual params: [1.6724, 0.7388]
-Original Grad: 0.104, -lr * Pred Grad:  0.054, New P: 1.726
-Original Grad: 0.045, -lr * Pred Grad:  0.147, New P: 0.885
iter 3 loss: 0.077
Actual params: [1.7264, 0.8854]
-Original Grad: 0.102, -lr * Pred Grad:  0.059, New P: 1.785
-Original Grad: 0.049, -lr * Pred Grad:  0.137, New P: 1.022
iter 4 loss: 0.065
Actual params: [1.7852, 1.0222]
-Original Grad: 0.087, -lr * Pred Grad:  0.062, New P: 1.847
-Original Grad: 0.054, -lr * Pred Grad:  0.128, New P: 1.151
iter 5 loss: 0.052
Actual params: [1.8467, 1.1507]
-Original Grad: 0.068, -lr * Pred Grad:  0.062, New P: 1.909
-Original Grad: 0.053, -lr * Pred Grad:  0.121, New P: 1.272
iter 6 loss: 0.040
Actual params: [1.909 , 1.2716]
-Original Grad: 0.047, -lr * Pred Grad:  0.061, New P: 1.970
-Original Grad: 0.046, -lr * Pred Grad:  0.113, New P: 1.385
iter 7 loss: 0.030
Actual params: [1.9697, 1.3851]
-Original Grad: 0.028, -lr * Pred Grad:  0.057, New P: 2.027
-Original Grad: 0.033, -lr * Pred Grad:  0.105, New P: 1.491
iter 8 loss: 0.024
Actual params: [2.0271, 1.4906]
-Original Grad: 0.019, -lr * Pred Grad:  0.054, New P: 2.081
-Original Grad: 0.022, -lr * Pred Grad:  0.097, New P: 1.588
iter 9 loss: 0.019
Actual params: [2.0807, 1.5878]
-Original Grad: 0.014, -lr * Pred Grad:  0.050, New P: 2.130
-Original Grad: 0.011, -lr * Pred Grad:  0.089, New P: 1.676
iter 10 loss: 0.017
Actual params: [2.1303, 1.6763]
-Original Grad: 0.012, -lr * Pred Grad:  0.046, New P: 2.176
-Original Grad: 0.002, -lr * Pred Grad:  0.080, New P: 1.756
iter 11 loss: 0.016
Actual params: [2.1761, 1.7562]
-Original Grad: 0.009, -lr * Pred Grad:  0.042, New P: 2.218
-Original Grad: -0.007, -lr * Pred Grad:  0.071, New P: 1.827
iter 12 loss: 0.016
Actual params: [2.2183, 1.8273]
-Original Grad: 0.007, -lr * Pred Grad:  0.039, New P: 2.257
-Original Grad: -0.015, -lr * Pred Grad:  0.063, New P: 1.890
iter 13 loss: 0.016
Actual params: [2.2569, 1.8899]
-Original Grad: 0.006, -lr * Pred Grad:  0.035, New P: 2.292
-Original Grad: -0.022, -lr * Pred Grad:  0.054, New P: 1.944
iter 14 loss: 0.017
Actual params: [2.2924, 1.9439]
-Original Grad: 0.005, -lr * Pred Grad:  0.032, New P: 2.325
-Original Grad: -0.027, -lr * Pred Grad:  0.046, New P: 1.990
iter 15 loss: 0.018
Actual params: [2.3247, 1.9898]
-Original Grad: 0.005, -lr * Pred Grad:  0.030, New P: 2.354
-Original Grad: -0.032, -lr * Pred Grad:  0.038, New P: 2.028
iter 16 loss: 0.019
Actual params: [2.3544, 2.0279]
-Original Grad: 0.005, -lr * Pred Grad:  0.027, New P: 2.381
-Original Grad: -0.036, -lr * Pred Grad:  0.031, New P: 2.059
iter 17 loss: 0.020
Actual params: [2.3815, 2.0586]
-Original Grad: 0.005, -lr * Pred Grad:  0.025, New P: 2.406
-Original Grad: -0.039, -lr * Pred Grad:  0.024, New P: 2.082
iter 18 loss: 0.021
Actual params: [2.4064, 2.0822]
-Original Grad: 0.005, -lr * Pred Grad:  0.023, New P: 2.429
-Original Grad: -0.041, -lr * Pred Grad:  0.017, New P: 2.099
iter 19 loss: 0.022
Actual params: [2.4293, 2.0994]
-Original Grad: 0.005, -lr * Pred Grad:  0.021, New P: 2.450
-Original Grad: -0.043, -lr * Pred Grad:  0.011, New P: 2.111
iter 20 loss: 0.023
Actual params: [2.4504, 2.1107]
-Original Grad: 0.005, -lr * Pred Grad:  0.020, New P: 2.470
-Original Grad: -0.043, -lr * Pred Grad:  0.006, New P: 2.116
iter 21 loss: 0.023
Actual params: [2.4699, 2.1164]
-Original Grad: 0.005, -lr * Pred Grad:  0.018, New P: 2.488
-Original Grad: -0.044, -lr * Pred Grad:  0.001, New P: 2.117
iter 22 loss: 0.023
Actual params: [2.488 , 2.1172]
-Original Grad: 0.005, -lr * Pred Grad:  0.017, New P: 2.505
-Original Grad: -0.044, -lr * Pred Grad:  -0.004, New P: 2.114
iter 23 loss: 0.023
Actual params: [2.5048, 2.1136]
-Original Grad: 0.005, -lr * Pred Grad:  0.016, New P: 2.521
-Original Grad: -0.043, -lr * Pred Grad:  -0.008, New P: 2.106
iter 24 loss: 0.023
Actual params: [2.5205, 2.106 ]
-Original Grad: 0.005, -lr * Pred Grad:  0.015, New P: 2.535
-Original Grad: -0.042, -lr * Pred Grad:  -0.011, New P: 2.095
iter 25 loss: 0.023
Actual params: [2.5352, 2.0949]
-Original Grad: 0.005, -lr * Pred Grad:  0.014, New P: 2.549
-Original Grad: -0.041, -lr * Pred Grad:  -0.014, New P: 2.081
iter 26 loss: 0.022
Actual params: [2.5489, 2.0808]
-Original Grad: 0.005, -lr * Pred Grad:  0.013, New P: 2.562
-Original Grad: -0.040, -lr * Pred Grad:  -0.017, New P: 2.064
iter 27 loss: 0.021
Actual params: [2.5618, 2.0641]
-Original Grad: 0.005, -lr * Pred Grad:  0.012, New P: 2.574
-Original Grad: -0.038, -lr * Pred Grad:  -0.019, New P: 2.045
iter 28 loss: 0.021
Actual params: [2.574 , 2.0453]
-Original Grad: 0.005, -lr * Pred Grad:  0.011, New P: 2.585
-Original Grad: -0.037, -lr * Pred Grad:  -0.021, New P: 2.025
iter 29 loss: 0.020
Actual params: [2.5854, 2.0247]
-Original Grad: 0.005, -lr * Pred Grad:  0.011, New P: 2.596
-Original Grad: -0.035, -lr * Pred Grad:  -0.022, New P: 2.003
iter 30 loss: 0.019
Actual params: [2.5962, 2.0026]
Target params: [ 1.3344, -1.0472]
Actual params: [-0.7899, -0.493 ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.000, New P: -0.790
-Original Grad: -0.002, -lr * Pred Grad:  -0.000, New P: -0.493
iter 0 loss: 0.537
Actual params: [-0.7902, -0.4932]
-Original Grad: -0.003, -lr * Pred Grad:  -0.001, New P: -0.791
-Original Grad: -0.002, -lr * Pred Grad:  -0.000, New P: -0.494
iter 1 loss: 0.537
Actual params: [-0.7907, -0.4935]
-Original Grad: -0.003, -lr * Pred Grad:  -0.001, New P: -0.791
-Original Grad: -0.002, -lr * Pred Grad:  -0.000, New P: -0.494
iter 2 loss: 0.537
Actual params: [-0.7915, -0.494 ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.001, New P: -0.792
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.495
iter 3 loss: 0.537
Actual params: [-0.7924, -0.4946]
-Original Grad: -0.003, -lr * Pred Grad:  -0.001, New P: -0.793
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.495
iter 4 loss: 0.537
Actual params: [-0.7935, -0.4953]
-Original Grad: -0.003, -lr * Pred Grad:  -0.001, New P: -0.795
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.496
iter 5 loss: 0.537
Actual params: [-0.7947, -0.4961]
-Original Grad: -0.003, -lr * Pred Grad:  -0.001, New P: -0.796
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.497
iter 6 loss: 0.537
Actual params: [-0.7961, -0.497 ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.002, New P: -0.798
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.498
iter 7 loss: 0.537
Actual params: [-0.7976, -0.498 ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.002, New P: -0.799
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.499
iter 8 loss: 0.537
Actual params: [-0.7992, -0.4991]
-Original Grad: -0.003, -lr * Pred Grad:  -0.002, New P: -0.801
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.500
iter 9 loss: 0.537
Actual params: [-0.8009, -0.5002]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.803
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.501
iter 10 loss: 0.537
Actual params: [-0.8027, -0.5013]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.805
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.503
iter 11 loss: 0.537
Actual params: [-0.8045, -0.5026]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.806
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.504
iter 12 loss: 0.537
Actual params: [-0.8064, -0.5038]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.808
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.505
iter 13 loss: 0.537
Actual params: [-0.8084, -0.5051]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.810
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.506
iter 14 loss: 0.537
Actual params: [-0.8104, -0.5064]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.812
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.508
iter 15 loss: 0.537
Actual params: [-0.8124, -0.5077]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.814
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.509
iter 16 loss: 0.537
Actual params: [-0.8144, -0.5091]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.816
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.510
iter 17 loss: 0.537
Actual params: [-0.8165, -0.5105]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.819
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.512
iter 18 loss: 0.537
Actual params: [-0.8186, -0.5118]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.821
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.513
iter 19 loss: 0.537
Actual params: [-0.8207, -0.5132]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.823
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.515
iter 20 loss: 0.537
Actual params: [-0.8227, -0.5146]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.825
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.516
iter 21 loss: 0.537
Actual params: [-0.8248, -0.516 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.827
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.517
iter 22 loss: 0.537
Actual params: [-0.8269, -0.5174]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.829
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.519
iter 23 loss: 0.537
Actual params: [-0.829 , -0.5188]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.831
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.520
iter 24 loss: 0.537
Actual params: [-0.831 , -0.5202]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.833
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.522
iter 25 loss: 0.537
Actual params: [-0.8331, -0.5215]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.835
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.523
iter 26 loss: 0.537
Actual params: [-0.8351, -0.5229]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.837
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.524
iter 27 loss: 0.537
Actual params: [-0.8371, -0.5242]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.839
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.526
iter 28 loss: 0.537
Actual params: [-0.8391, -0.5256]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.841
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.527
iter 29 loss: 0.537
Actual params: [-0.8411, -0.5269]
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: -0.843
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.528
iter 30 loss: 0.537
Actual params: [-0.843 , -0.5282]
Target params: [ 1.3344, -1.0472]
Actual params: [0.3685, 0.155 ]
-Original Grad: 0.280, -lr * Pred Grad:  0.028, New P: 0.396
-Original Grad: 0.089, -lr * Pred Grad:  0.009, New P: 0.164
iter 0 loss: 0.457
Actual params: [0.3965, 0.1639]
-Original Grad: 0.195, -lr * Pred Grad:  0.045, New P: 0.441
-Original Grad: 0.098, -lr * Pred Grad:  0.018, New P: 0.182
iter 1 loss: 0.450
Actual params: [0.4412, 0.1817]
-Original Grad: 0.045, -lr * Pred Grad:  0.045, New P: 0.486
-Original Grad: 0.115, -lr * Pred Grad:  0.028, New P: 0.209
iter 2 loss: 0.443
Actual params: [0.486 , 0.2092]
-Original Grad: -0.075, -lr * Pred Grad:  0.033, New P: 0.519
-Original Grad: 0.134, -lr * Pred Grad:  0.038, New P: 0.247
iter 3 loss: 0.440
Actual params: [0.5187, 0.2474]
-Original Grad: -0.146, -lr * Pred Grad:  0.015, New P: 0.534
-Original Grad: 0.140, -lr * Pred Grad:  0.048, New P: 0.296
iter 4 loss: 0.438
Actual params: [0.5336, 0.2957]
-Original Grad: -0.151, -lr * Pred Grad:  -0.002, New P: 0.532
-Original Grad: 0.137, -lr * Pred Grad:  0.057, New P: 0.353
iter 5 loss: 0.434
Actual params: [0.5318, 0.3529]
-Original Grad: -0.121, -lr * Pred Grad:  -0.014, New P: 0.518
-Original Grad: 0.123, -lr * Pred Grad:  0.064, New P: 0.417
iter 6 loss: 0.427
Actual params: [0.5181, 0.4167]
-Original Grad: -0.052, -lr * Pred Grad:  -0.018, New P: 0.501
-Original Grad: 0.099, -lr * Pred Grad:  0.067, New P: 0.484
iter 7 loss: 0.418
Actual params: [0.5006, 0.484 ]
-Original Grad: 0.058, -lr * Pred Grad:  -0.010, New P: 0.491
-Original Grad: 0.075, -lr * Pred Grad:  0.068, New P: 0.552
iter 8 loss: 0.413
Actual params: [0.4906, 0.5521]
-Original Grad: 0.072, -lr * Pred Grad:  -0.002, New P: 0.489
-Original Grad: 0.083, -lr * Pred Grad:  0.070, New P: 0.622
iter 9 loss: 0.408
Actual params: [0.4888, 0.6217]
-Original Grad: 0.072, -lr * Pred Grad:  0.006, New P: 0.494
-Original Grad: 0.085, -lr * Pred Grad:  0.071, New P: 0.693
iter 10 loss: 0.403
Actual params: [0.4944, 0.6927]
-Original Grad: 0.080, -lr * Pred Grad:  0.013, New P: 0.507
-Original Grad: 0.084, -lr * Pred Grad:  0.072, New P: 0.765
iter 11 loss: 0.396
Actual params: [0.5074, 0.7651]
-Original Grad: 0.082, -lr * Pred Grad:  0.020, New P: 0.527
-Original Grad: 0.081, -lr * Pred Grad:  0.073, New P: 0.838
iter 12 loss: 0.389
Actual params: [0.5274, 0.8382]
-Original Grad: 0.067, -lr * Pred Grad:  0.025, New P: 0.552
-Original Grad: 0.046, -lr * Pred Grad:  0.070, New P: 0.909
iter 13 loss: 0.383
Actual params: [0.5519, 0.9087]
-Original Grad: 0.070, -lr * Pred Grad:  0.029, New P: 0.581
-Original Grad: 0.054, -lr * Pred Grad:  0.069, New P: 0.977
iter 14 loss: 0.378
Actual params: [0.581 , 0.9775]
-Original Grad: 0.100, -lr * Pred Grad:  0.036, New P: 0.617
-Original Grad: 0.073, -lr * Pred Grad:  0.069, New P: 1.047
iter 15 loss: 0.371
Actual params: [0.6173, 1.0467]
-Original Grad: 0.127, -lr * Pred Grad:  0.045, New P: 0.663
-Original Grad: 0.068, -lr * Pred Grad:  0.069, New P: 1.116
iter 16 loss: 0.362
Actual params: [0.6626, 1.1158]
-Original Grad: 0.180, -lr * Pred Grad:  0.059, New P: 0.721
-Original Grad: 0.067, -lr * Pred Grad:  0.069, New P: 1.185
iter 17 loss: 0.350
Actual params: [0.7213, 1.1846]
-Original Grad: 0.267, -lr * Pred Grad:  0.080, New P: 0.801
-Original Grad: 0.057, -lr * Pred Grad:  0.068, New P: 1.252
iter 18 loss: 0.332
Actual params: [0.8009, 1.2523]
-Original Grad: 0.370, -lr * Pred Grad:  0.109, New P: 0.910
-Original Grad: 0.015, -lr * Pred Grad:  0.062, New P: 1.315
iter 19 loss: 0.304
Actual params: [0.9095, 1.3147]
-Original Grad: 0.397, -lr * Pred Grad:  0.138, New P: 1.047
-Original Grad: -0.046, -lr * Pred Grad:  0.052, New P: 1.366
iter 20 loss: 0.262
Actual params: [1.0471, 1.3662]
-Original Grad: 0.299, -lr * Pred Grad:  0.154, New P: 1.201
-Original Grad: -0.145, -lr * Pred Grad:  0.032, New P: 1.398
iter 21 loss: 0.219
Actual params: [1.2007, 1.3981]
-Original Grad: 0.058, -lr * Pred Grad:  0.144, New P: 1.345
-Original Grad: -0.283, -lr * Pred Grad:  0.000, New P: 1.399
iter 22 loss: 0.199
Actual params: [1.3448, 1.3985]
-Original Grad: -0.023, -lr * Pred Grad:  0.127, New P: 1.472
-Original Grad: -0.371, -lr * Pred Grad:  -0.037, New P: 1.362
iter 23 loss: 0.198
Actual params: [1.4722, 1.3618]
-Original Grad: -0.006, -lr * Pred Grad:  0.114, New P: 1.586
-Original Grad: -0.377, -lr * Pred Grad:  -0.071, New P: 1.291
iter 24 loss: 0.186
Actual params: [1.5863, 1.291 ]
-Original Grad: -0.007, -lr * Pred Grad:  0.102, New P: 1.688
-Original Grad: -0.260, -lr * Pred Grad:  -0.090, New P: 1.201
iter 25 loss: 0.164
Actual params: [1.6883, 1.2014]
-Original Grad: -0.033, -lr * Pred Grad:  0.088, New P: 1.777
-Original Grad: -0.205, -lr * Pred Grad:  -0.101, New P: 1.100
iter 26 loss: 0.145
Actual params: [1.7767, 1.1002]
-Original Grad: -0.065, -lr * Pred Grad:  0.073, New P: 1.850
-Original Grad: -0.155, -lr * Pred Grad:  -0.107, New P: 0.994
iter 27 loss: 0.132
Actual params: [1.8498, 0.9936]
-Original Grad: -0.083, -lr * Pred Grad:  0.058, New P: 1.907
-Original Grad: -0.110, -lr * Pred Grad:  -0.107, New P: 0.887
iter 28 loss: 0.123
Actual params: [1.9074, 0.8866]
-Original Grad: -0.083, -lr * Pred Grad:  0.043, New P: 1.951
-Original Grad: -0.070, -lr * Pred Grad:  -0.103, New P: 0.783
iter 29 loss: 0.117
Actual params: [1.9508, 0.7834]
-Original Grad: -0.082, -lr * Pred Grad:  0.031, New P: 1.982
-Original Grad: -0.025, -lr * Pred Grad:  -0.095, New P: 0.688
iter 30 loss: 0.117
Actual params: [1.9818, 0.6881]
