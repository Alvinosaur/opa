Target params: [1.1812, 0.2779]
iter 0 loss: 0.779
Actual params: [0.5941, 0.5941]
-Original Grad: 0.039, -lr * Pred Grad:  0.534, New P: 1.128
-Original Grad: -0.007, -lr * Pred Grad:  -0.097, New P: 0.497
iter 1 loss: 0.736
Actual params: [1.1283, 0.4967]
-Original Grad: 0.289, -lr * Pred Grad:  0.035, New P: 1.164
-Original Grad: -0.572, -lr * Pred Grad:  -0.336, New P: 0.161
iter 2 loss: 0.388
Actual params: [1.1635, 0.1606]
-Original Grad: -0.295, -lr * Pred Grad:  -0.848, New P: 0.316
-Original Grad: 0.216, -lr * Pred Grad:  -0.386, New P: -0.226
iter 3 loss: 0.734
Actual params: [ 0.3158, -0.2259]
-Original Grad: 0.009, -lr * Pred Grad:  0.184, New P: 0.499
-Original Grad: 0.060, -lr * Pred Grad:  0.140, New P: -0.086
iter 4 loss: 0.693
Actual params: [ 0.4995, -0.086 ]
-Original Grad: 0.027, -lr * Pred Grad:  0.092, New P: 0.591
-Original Grad: -0.000, -lr * Pred Grad:  0.051, New P: -0.035
iter 5 loss: 0.662
Actual params: [ 0.5911, -0.035 ]
-Original Grad: 0.297, -lr * Pred Grad:  0.136, New P: 0.727
-Original Grad: 0.052, -lr * Pred Grad:  0.081, New P: 0.045
iter 6 loss: 0.589
Actual params: [0.7271, 0.0455]
-Original Grad: 0.310, -lr * Pred Grad:  0.046, New P: 0.773
-Original Grad: -0.377, -lr * Pred Grad:  -0.140, New P: -0.094
iter 7 loss: 0.531
Actual params: [ 0.7734, -0.0943]
-Original Grad: 0.375, -lr * Pred Grad:  0.076, New P: 0.849
-Original Grad: -0.316, -lr * Pred Grad:  -0.074, New P: -0.169
iter 8 loss: 0.468
Actual params: [ 0.8492, -0.1685]
-Original Grad: -0.127, -lr * Pred Grad:  -0.046, New P: 0.803
-Original Grad: -0.080, -lr * Pred Grad:  -0.048, New P: -0.217
iter 9 loss: 0.509
Actual params: [ 0.8033, -0.2167]
-Original Grad: -0.013, -lr * Pred Grad:  -0.004, New P: 0.800
-Original Grad: -0.006, -lr * Pred Grad:  -0.003, New P: -0.220
iter 10 loss: 0.513
Actual params: [ 0.7995, -0.2199]
-Original Grad: 0.360, -lr * Pred Grad:  0.088, New P: 0.888
-Original Grad: 0.089, -lr * Pred Grad:  0.055, New P: -0.165
iter 11 loss: 0.444
Actual params: [ 0.8875, -0.1653]
-Original Grad: -1.610, -lr * Pred Grad:  -0.105, New P: 0.782
-Original Grad: -0.201, -lr * Pred Grad:  -0.043, New P: -0.208
iter 12 loss: 0.524
Actual params: [ 0.782, -0.208]
-Original Grad: -0.065, -lr * Pred Grad:  -0.011, New P: 0.771
-Original Grad: 0.221, -lr * Pred Grad:  0.105, New P: -0.103
iter 13 loss: 0.534
Actual params: [ 0.7706, -0.1035]
-Original Grad: 1.697, -lr * Pred Grad:  0.069, New P: 0.839
-Original Grad: 0.209, -lr * Pred Grad:  0.026, New P: -0.077
iter 14 loss: 0.460
Actual params: [ 0.8392, -0.0773]
-Original Grad: -1.422, -lr * Pred Grad:  -0.017, New P: 0.822
-Original Grad: -0.621, -lr * Pred Grad:  -0.118, New P: -0.195
iter 15 loss: 0.492
Actual params: [ 0.822 , -0.1951]
-Original Grad: -0.110, -lr * Pred Grad:  0.000, New P: 0.822
-Original Grad: -0.065, -lr * Pred Grad:  -0.016, New P: -0.211
iter 16 loss: 0.496
Actual params: [ 0.8221, -0.2112]
-Original Grad: 0.209, -lr * Pred Grad:  0.004, New P: 0.826
-Original Grad: 0.085, -lr * Pred Grad:  0.016, New P: -0.195
iter 17 loss: 0.489
Actual params: [ 0.8258, -0.1948]
-Original Grad: 0.100, -lr * Pred Grad:  0.023, New P: 0.849
-Original Grad: -0.169, -lr * Pred Grad:  -0.086, New P: -0.281
iter 18 loss: 1.626
Actual params: [ 0.8491, -0.2811]
-Original Grad: -1.434, -lr * Pred Grad:  -0.055, New P: 0.794
-Original Grad: 0.220, -lr * Pred Grad:  0.120, New P: -0.161
iter 19 loss: 0.509
Actual params: [ 0.7942, -0.1607]
-Original Grad: -0.031, -lr * Pred Grad:  -0.001, New P: 0.794
-Original Grad: -0.016, -lr * Pred Grad:  -0.004, New P: -0.165
iter 20 loss: 0.510
Actual params: [ 0.7936, -0.1651]
-Original Grad: -0.018, -lr * Pred Grad:  -0.001, New P: 0.793
-Original Grad: -0.005, -lr * Pred Grad:  -0.001, New P: -0.166
Target params: [1.1812, 0.2779]
iter 0 loss: 0.343
Actual params: [0.5941, 0.5941]
-Original Grad: -0.619, -lr * Pred Grad:  -0.395, New P: 0.199
-Original Grad: -0.017, -lr * Pred Grad:  -0.011, New P: 0.583
iter 1 loss: 0.399
Actual params: [0.1988, 0.5831]
-Original Grad: -0.067, -lr * Pred Grad:  -0.030, New P: 0.169
-Original Grad: -0.015, -lr * Pred Grad:  -0.486, New P: 0.098
iter 2 loss: 0.359
Actual params: [0.1685, 0.0975]
-Original Grad: -0.028, -lr * Pred Grad:  0.002, New P: 0.171
-Original Grad: -0.023, -lr * Pred Grad:  -0.280, New P: -0.182
iter 3 loss: 0.358
Actual params: [ 0.171 , -0.1824]
-Original Grad: 0.037, -lr * Pred Grad:  -0.039, New P: 0.132
-Original Grad: 0.072, -lr * Pred Grad:  0.746, New P: 0.564
iter 4 loss: 0.416
Actual params: [0.1316, 0.564 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: 0.134
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: 0.550
iter 5 loss: 0.412
Actual params: [0.1344, 0.5498]
-Original Grad: 0.065, -lr * Pred Grad:  0.024, New P: 0.159
-Original Grad: -0.057, -lr * Pred Grad:  -0.145, New P: 0.405
iter 6 loss: 0.373
Actual params: [0.1586, 0.4047]
-Original Grad: -0.005, -lr * Pred Grad:  -0.004, New P: 0.154
-Original Grad: -0.001, -lr * Pred Grad:  -0.005, New P: 0.400
iter 7 loss: 0.373
Actual params: [0.1544, 0.3998]
-Original Grad: -0.197, -lr * Pred Grad:  -0.098, New P: 0.056
-Original Grad: 0.074, -lr * Pred Grad:  0.128, New P: 0.528
iter 8 loss: 0.425
Actual params: [0.056 , 0.5277]
-Original Grad: -0.258, -lr * Pred Grad:  -0.005, New P: 0.051
-Original Grad: 0.277, -lr * Pred Grad:  0.426, New P: 0.953
iter 9 loss: 0.528
Actual params: [0.0513, 0.9532]
-Original Grad: -0.017, -lr * Pred Grad:  -0.011, New P: 0.040
-Original Grad: -0.129, -lr * Pred Grad:  -0.029, New P: 0.924
iter 10 loss: 0.521
Actual params: [0.0405, 0.9241]
-Original Grad: -0.190, -lr * Pred Grad:  -0.100, New P: -0.059
-Original Grad: 0.140, -lr * Pred Grad:  0.027, New P: 0.951
iter 11 loss: 0.542
Actual params: [-0.059 ,  0.9506]
-Original Grad: -0.036, -lr * Pred Grad:  -0.019, New P: -0.078
-Original Grad: 0.016, -lr * Pred Grad:  0.002, New P: 0.953
iter 12 loss: 0.544
Actual params: [-0.0776,  0.9529]
-Original Grad: -0.077, -lr * Pred Grad:  -0.034, New P: -0.112
-Original Grad: 0.166, -lr * Pred Grad:  0.042, New P: 0.995
iter 13 loss: 0.559
Actual params: [-0.1115,  0.9952]
-Original Grad: 0.097, -lr * Pred Grad:  0.047, New P: -0.065
-Original Grad: 0.007, -lr * Pred Grad:  0.007, New P: 1.002
iter 14 loss: 0.555
Actual params: [-0.0646,  1.002 ]
-Original Grad: -0.001, -lr * Pred Grad:  0.000, New P: -0.065
-Original Grad: 0.014, -lr * Pred Grad:  0.005, New P: 1.007
iter 15 loss: 0.557
Actual params: [-0.0646,  1.0067]
-Original Grad: 0.056, -lr * Pred Grad:  0.027, New P: -0.037
-Original Grad: -0.078, -lr * Pred Grad:  -0.024, New P: 0.983
iter 16 loss: 0.548
Actual params: [-0.0374,  0.9829]
-Original Grad: 0.018, -lr * Pred Grad:  0.009, New P: -0.028
-Original Grad: -0.010, -lr * Pred Grad:  -0.002, New P: 0.981
iter 17 loss: 0.546
Actual params: [-0.0282,  0.9806]
-Original Grad: -0.019, -lr * Pred Grad:  -0.012, New P: -0.040
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: 0.978
iter 18 loss: 0.547
Actual params: [-0.0404,  0.9781]
-Original Grad: -0.028, -lr * Pred Grad:  0.020, New P: -0.020
-Original Grad: 2.273, -lr * Pred Grad:  0.090, New P: 1.068
iter 19 loss: 0.564
Actual params: [-0.0203,  1.0682]
-Original Grad: -0.166, -lr * Pred Grad:  -0.110, New P: -0.130
-Original Grad: 0.145, -lr * Pred Grad:  0.003, New P: 1.071
iter 20 loss: 0.576
Actual params: [-0.13  ,  1.0711]
-Original Grad: -0.256, -lr * Pred Grad:  -0.136, New P: -0.266
-Original Grad: -0.003, -lr * Pred Grad:  -0.004, New P: 1.067
Target params: [1.1812, 0.2779]
iter 0 loss: 0.542
Actual params: [0.5941, 0.5941]
-Original Grad: 0.193, -lr * Pred Grad:  0.974, New P: 1.568
-Original Grad: 0.061, -lr * Pred Grad:  0.307, New P: 0.901
iter 1 loss: 1.239
Actual params: [1.5677, 0.9012]
-Original Grad: 0.001, -lr * Pred Grad:  -0.008, New P: 1.559
-Original Grad: 0.002, -lr * Pred Grad:  0.060, New P: 0.961
iter 2 loss: 1.287
Actual params: [1.5592, 0.9612]
-Original Grad: -0.027, -lr * Pred Grad:  -0.007, New P: 1.553
-Original Grad: 0.292, -lr * Pred Grad:  0.023, New P: 0.985
iter 3 loss: 1.298
Actual params: [1.5526, 0.9846]
-Original Grad: 0.044, -lr * Pred Grad:  0.263, New P: 1.815
-Original Grad: 0.020, -lr * Pred Grad:  0.025, New P: 1.010
iter 4 loss: 1.243
Actual params: [1.8155, 1.0098]
-Original Grad: 0.336, -lr * Pred Grad:  0.519, New P: 2.335
-Original Grad: 0.080, -lr * Pred Grad:  0.048, New P: 1.058
iter 5 loss: 1.117
Actual params: [2.3348, 1.0579]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 2.336
-Original Grad: 0.005, -lr * Pred Grad:  0.001, New P: 1.058
iter 6 loss: 1.117
Actual params: [2.3361, 1.0585]
-Original Grad: 0.100, -lr * Pred Grad:  0.007, New P: 2.343
-Original Grad: -1.160, -lr * Pred Grad:  -0.041, New P: 1.018
iter 7 loss: 1.126
Actual params: [2.3427, 1.0177]
-Original Grad: -0.013, -lr * Pred Grad:  -0.019, New P: 2.324
-Original Grad: 0.045, -lr * Pred Grad:  0.000, New P: 1.018
iter 8 loss: 1.132
Actual params: [2.3237, 1.0179]
-Original Grad: 0.288, -lr * Pred Grad:  0.110, New P: 2.433
-Original Grad: -1.157, -lr * Pred Grad:  -0.003, New P: 1.014
iter 9 loss: 1.099
Actual params: [2.4332, 1.0144]
-Original Grad: -0.012, -lr * Pred Grad:  -0.005, New P: 2.428
-Original Grad: 0.051, -lr * Pred Grad:  0.000, New P: 1.015
iter 10 loss: 1.100
Actual params: [2.4284, 1.0147]
-Original Grad: 0.008, -lr * Pred Grad:  0.008, New P: 2.437
-Original Grad: -0.013, -lr * Pred Grad:  0.001, New P: 1.016
iter 11 loss: 1.098
Actual params: [2.4367, 1.0158]
-Original Grad: -0.005, -lr * Pred Grad:  -0.010, New P: 2.427
-Original Grad: -0.111, -lr * Pred Grad:  -0.002, New P: 1.013
iter 12 loss: 1.101
Actual params: [2.4268, 1.0134]
-Original Grad: -0.345, -lr * Pred Grad:  -0.148, New P: 2.279
-Original Grad: 1.068, -lr * Pred Grad:  0.001, New P: 1.015
iter 13 loss: 1.147
Actual params: [2.2787, 1.0146]
-Original Grad: 0.004, -lr * Pred Grad:  0.004, New P: 2.283
-Original Grad: 0.019, -lr * Pred Grad:  0.001, New P: 1.015
iter 14 loss: 1.146
Actual params: [2.2825, 1.0153]
-Original Grad: 0.004, -lr * Pred Grad:  0.004, New P: 2.287
-Original Grad: 0.019, -lr * Pred Grad:  0.001, New P: 1.016
iter 15 loss: 1.144
Actual params: [2.2869, 1.0161]
-Original Grad: -0.116, -lr * Pred Grad:  -0.091, New P: 2.196
-Original Grad: -0.031, -lr * Pred Grad:  -0.010, New P: 1.006
iter 16 loss: 1.172
Actual params: [2.1961, 1.0065]
-Original Grad: 0.115, -lr * Pred Grad:  0.089, New P: 2.285
-Original Grad: -0.059, -lr * Pred Grad:  0.008, New P: 1.014
iter 17 loss: 1.145
Actual params: [2.2848, 1.014 ]
-Original Grad: -0.424, -lr * Pred Grad:  -0.088, New P: 2.197
-Original Grad: -0.697, -lr * Pred Grad:  -0.012, New P: 1.002
iter 18 loss: 1.171
Actual params: [2.1968, 1.0017]
-Original Grad: 0.007, -lr * Pred Grad:  0.002, New P: 2.199
-Original Grad: -0.013, -lr * Pred Grad:  -0.000, New P: 1.001
iter 19 loss: 1.171
Actual params: [2.1985, 1.0014]
-Original Grad: -1.294, -lr * Pred Grad:  -0.120, New P: 2.079
-Original Grad: 0.142, -lr * Pred Grad:  0.004, New P: 1.006
iter 20 loss: 1.200
Actual params: [2.0787, 1.0058]
-Original Grad: 0.038, -lr * Pred Grad:  0.004, New P: 2.082
-Original Grad: 0.206, -lr * Pred Grad:  0.006, New P: 1.012
Target params: [1.1812, 0.2779]
iter 0 loss: 1.449
Actual params: [0.5941, 0.5941]
-Original Grad: -0.041, -lr * Pred Grad:  -0.546, New P: 0.048
-Original Grad: -0.023, -lr * Pred Grad:  -0.304, New P: 0.291
iter 1 loss: 1.463
Actual params: [0.0479, 0.2905]
-Original Grad: -0.022, -lr * Pred Grad:  0.197, New P: 0.245
-Original Grad: -0.192, -lr * Pred Grad:  -0.787, New P: -0.496
iter 2 loss: 1.431
Actual params: [ 0.2451, -0.496 ]
-Original Grad: -0.090, -lr * Pred Grad:  -0.838, New P: -0.593
-Original Grad: -0.130, -lr * Pred Grad:  -0.189, New P: -0.685
iter 3 loss: 1.470
Actual params: [-0.5927, -0.6854]
-Original Grad: 0.001, -lr * Pred Grad:  0.070, New P: -0.523
-Original Grad: -0.013, -lr * Pred Grad:  -0.061, New P: -0.747
iter 4 loss: 1.470
Actual params: [-0.523 , -0.7468]
-Original Grad: 0.014, -lr * Pred Grad:  0.532, New P: 0.010
-Original Grad: -0.095, -lr * Pred Grad:  -0.401, New P: -1.148
iter 5 loss: 1.458
Actual params: [ 0.0095, -1.1478]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: 0.003
-Original Grad: 0.001, -lr * Pred Grad:  0.004, New P: -1.144
iter 6 loss: 1.458
Actual params: [ 0.0032, -1.1443]
-Original Grad: -0.027, -lr * Pred Grad:  -0.609, New P: -0.606
-Original Grad: 0.195, -lr * Pred Grad:  0.488, New P: -0.656
iter 7 loss: 1.470
Actual params: [-0.6059, -0.6564]
-Original Grad: -0.024, -lr * Pred Grad:  -0.323, New P: -0.929
-Original Grad: -0.044, -lr * Pred Grad:  -0.076, New P: -0.733
iter 8 loss: 1.502
Actual params: [-0.9288, -0.7328]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.941
-Original Grad: 0.002, -lr * Pred Grad:  0.007, New P: -0.725
iter 9 loss: 1.502
Actual params: [-0.9408, -0.7255]
-Original Grad: -0.009, -lr * Pred Grad:  -0.121, New P: -1.062
-Original Grad: -0.030, -lr * Pred Grad:  -0.071, New P: -0.796
iter 10 loss: 1.518
Actual params: [-1.0621, -0.7964]
-Original Grad: 0.008, -lr * Pred Grad:  0.089, New P: -0.973
-Original Grad: 0.002, -lr * Pred Grad:  -0.006, New P: -0.802
iter 11 loss: 1.504
Actual params: [-0.973, -0.802]
-Original Grad: 0.022, -lr * Pred Grad:  0.162, New P: -0.811
-Original Grad: 0.048, -lr * Pred Grad:  0.116, New P: -0.686
iter 12 loss: 1.490
Actual params: [-0.8113, -0.6863]
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.801
-Original Grad: 0.001, -lr * Pred Grad:  -0.001, New P: -0.687
iter 13 loss: 1.489
Actual params: [-0.8014, -0.687 ]
-Original Grad: 0.008, -lr * Pred Grad:  0.050, New P: -0.752
-Original Grad: 0.013, -lr * Pred Grad:  0.034, New P: -0.653
iter 14 loss: 1.484
Actual params: [-0.7517, -0.6527]
-Original Grad: 0.025, -lr * Pred Grad:  0.129, New P: -0.622
-Original Grad: 0.047, -lr * Pred Grad:  0.140, New P: -0.513
iter 15 loss: 1.470
Actual params: [-0.6223, -0.5127]
-Original Grad: 0.033, -lr * Pred Grad:  0.412, New P: -0.210
-Original Grad: -0.023, -lr * Pred Grad:  -0.195, New P: -0.708
iter 16 loss: 1.454
Actual params: [-0.2101, -0.7077]
-Original Grad: 0.073, -lr * Pred Grad:  0.595, New P: 0.385
-Original Grad: 0.046, -lr * Pred Grad:  0.020, New P: -0.687
iter 17 loss: 1.418
Actual params: [ 0.3846, -0.6873]
-Original Grad: -0.071, -lr * Pred Grad:  -0.318, New P: 0.066
-Original Grad: -0.017, -lr * Pred Grad:  0.059, New P: -0.628
iter 18 loss: 1.444
Actual params: [ 0.0663, -0.6283]
-Original Grad: 0.044, -lr * Pred Grad:  0.100, New P: 0.166
-Original Grad: 0.053, -lr * Pred Grad:  0.194, New P: -0.434
iter 19 loss: 1.438
Actual params: [ 0.1659, -0.4343]
-Original Grad: 0.045, -lr * Pred Grad:  0.133, New P: 0.299
-Original Grad: 0.041, -lr * Pred Grad:  0.122, New P: -0.313
iter 20 loss: 1.423
Actual params: [ 0.2991, -0.3127]
-Original Grad: -0.273, -lr * Pred Grad:  -0.530, New P: -0.231
-Original Grad: -0.131, -lr * Pred Grad:  0.011, New P: -0.302
Target params: [1.1812, 0.2779]
iter 0 loss: 0.945
Actual params: [0.5941, 0.5941]
-Original Grad: 0.199, -lr * Pred Grad:  0.047, New P: 0.641
-Original Grad: -0.519, -lr * Pred Grad:  -0.122, New P: 0.472
iter 1 loss: 0.911
Actual params: [0.641 , 0.4716]
-Original Grad: 0.242, -lr * Pred Grad:  0.323, New P: 0.964
-Original Grad: -0.145, -lr * Pred Grad:  0.121, New P: 0.592
iter 2 loss: 0.712
Actual params: [0.9642, 0.5924]
-Original Grad: 0.039, -lr * Pred Grad:  0.019, New P: 0.983
-Original Grad: -0.058, -lr * Pred Grad:  -0.007, New P: 0.585
iter 3 loss: 0.690
Actual params: [0.9831, 0.5854]
-Original Grad: 0.760, -lr * Pred Grad:  0.205, New P: 1.188
-Original Grad: -0.734, -lr * Pred Grad:  0.051, New P: 0.636
iter 4 loss: 0.680
Actual params: [1.1884, 0.6363]
-Original Grad: -0.361, -lr * Pred Grad:  -0.143, New P: 1.045
-Original Grad: -1.122, -lr * Pred Grad:  -0.130, New P: 0.507
iter 5 loss: 0.636
Actual params: [1.0451, 0.5066]
-Original Grad: 0.008, -lr * Pred Grad:  0.001, New P: 1.046
-Original Grad: -0.008, -lr * Pred Grad:  -0.000, New P: 0.506
iter 6 loss: 0.635
Actual params: [1.0465, 0.5062]
-Original Grad: -0.206, -lr * Pred Grad:  -0.046, New P: 1.000
-Original Grad: 0.056, -lr * Pred Grad:  -0.008, New P: 0.499
iter 7 loss: 0.654
Actual params: [1.0004, 0.4987]
-Original Grad: -0.178, -lr * Pred Grad:  -0.006, New P: 0.995
-Original Grad: 0.536, -lr * Pred Grad:  0.049, New P: 0.547
iter 8 loss: 0.677
Actual params: [0.9946, 0.5473]
-Original Grad: 0.051, -lr * Pred Grad:  0.012, New P: 1.006
-Original Grad: -0.015, -lr * Pred Grad:  0.002, New P: 0.549
iter 9 loss: 0.666
Actual params: [1.0064, 0.5494]
-Original Grad: 0.116, -lr * Pred Grad:  0.049, New P: 1.056
-Original Grad: 0.233, -lr * Pred Grad:  0.040, New P: 0.590
iter 10 loss: 0.644
Actual params: [1.0558, 0.5896]
-Original Grad: -0.025, -lr * Pred Grad:  0.000, New P: 1.056
-Original Grad: 0.090, -lr * Pred Grad:  0.008, New P: 0.598
iter 11 loss: 0.647
Actual params: [1.056 , 0.5977]
-Original Grad: 0.226, -lr * Pred Grad:  0.032, New P: 1.088
-Original Grad: -0.425, -lr * Pred Grad:  -0.029, New P: 0.569
iter 12 loss: 0.646
Actual params: [1.0884, 0.5687]
-Original Grad: 0.002, -lr * Pred Grad:  0.004, New P: 1.093
-Original Grad: 0.034, -lr * Pred Grad:  0.005, New P: 0.573
iter 13 loss: 0.647
Actual params: [1.0927, 0.5735]
-Original Grad: 0.300, -lr * Pred Grad:  0.034, New P: 1.127
-Original Grad: 1.807, -lr * Pred Grad:  0.030, New P: 0.604
iter 14 loss: 0.657
Actual params: [1.1267, 0.6039]
-Original Grad: -0.107, -lr * Pred Grad:  -0.011, New P: 1.115
-Original Grad: -0.693, -lr * Pred Grad:  -0.012, New P: 0.592
iter 15 loss: 0.648
Actual params: [1.1154, 0.5917]
-Original Grad: 0.003, -lr * Pred Grad:  0.001, New P: 1.116
-Original Grad: -0.007, -lr * Pred Grad:  -0.000, New P: 0.591
iter 16 loss: 0.648
Actual params: [1.1164, 0.5915]
-Original Grad: 0.101, -lr * Pred Grad:  -0.005, New P: 1.112
-Original Grad: 1.285, -lr * Pred Grad:  0.026, New P: 0.617
iter 17 loss: 0.659
Actual params: [1.1116, 0.6174]
-Original Grad: 0.007, -lr * Pred Grad:  -0.003, New P: 1.109
-Original Grad: 0.180, -lr * Pred Grad:  0.004, New P: 0.621
iter 18 loss: 0.660
Actual params: [1.1086, 0.6211]
-Original Grad: 0.005, -lr * Pred Grad:  0.003, New P: 1.111
-Original Grad: -0.025, -lr * Pred Grad:  -0.001, New P: 0.620
iter 19 loss: 0.660
Actual params: [1.1113, 0.6204]
-Original Grad: -0.127, -lr * Pred Grad:  -0.040, New P: 1.071
-Original Grad: -0.124, -lr * Pred Grad:  0.001, New P: 0.621
iter 20 loss: 0.656
Actual params: [1.0715, 0.6214]
-Original Grad: -0.101, -lr * Pred Grad:  -0.030, New P: 1.042
-Original Grad: -0.106, -lr * Pred Grad:  0.001, New P: 0.622
Target params: [1.1812, 0.2779]
iter 0 loss: 0.804
Actual params: [0.5941, 0.5941]
-Original Grad: 0.021, -lr * Pred Grad:  0.254, New P: 0.848
-Original Grad: -0.061, -lr * Pred Grad:  -0.724, New P: -0.130
iter 1 loss: 0.506
Actual params: [ 0.8483, -0.1299]
-Original Grad: 0.816, -lr * Pred Grad:  0.238, New P: 1.086
-Original Grad: 0.513, -lr * Pred Grad:  0.104, New P: -0.026
iter 2 loss: 0.387
Actual params: [ 1.0859, -0.0256]
-Original Grad: 0.004, -lr * Pred Grad:  0.109, New P: 1.195
-Original Grad: -0.042, -lr * Pred Grad:  -0.177, New P: -0.202
iter 3 loss: 0.398
Actual params: [ 1.1952, -0.2021]
-Original Grad: 0.007, -lr * Pred Grad:  0.100, New P: 1.296
-Original Grad: -0.033, -lr * Pred Grad:  -0.160, New P: -0.362
iter 4 loss: 0.424
Actual params: [ 1.2955, -0.3617]
-Original Grad: 0.003, -lr * Pred Grad:  -0.011, New P: 1.284
-Original Grad: 0.006, -lr * Pred Grad:  0.021, New P: -0.341
iter 5 loss: 0.421
Actual params: [ 1.2842, -0.3407]
-Original Grad: -0.052, -lr * Pred Grad:  -0.304, New P: 0.980
-Original Grad: 0.100, -lr * Pred Grad:  0.466, New P: 0.126
iter 6 loss: 0.405
Actual params: [0.9799, 0.1257]
-Original Grad: -0.007, -lr * Pred Grad:  -0.049, New P: 0.931
-Original Grad: 0.017, -lr * Pred Grad:  0.076, New P: 0.202
iter 7 loss: 0.432
Actual params: [0.9306, 0.2021]
-Original Grad: -0.000, -lr * Pred Grad:  0.104, New P: 1.035
-Original Grad: -0.046, -lr * Pred Grad:  -0.178, New P: 0.024
iter 8 loss: 0.391
Actual params: [1.0348, 0.0238]
-Original Grad: -0.010, -lr * Pred Grad:  -0.091, New P: 0.943
-Original Grad: 0.031, -lr * Pred Grad:  0.144, New P: 0.168
iter 9 loss: 0.422
Actual params: [0.9434, 0.1681]
-Original Grad: 0.005, -lr * Pred Grad:  0.110, New P: 1.053
-Original Grad: -0.040, -lr * Pred Grad:  -0.182, New P: -0.014
iter 10 loss: 0.388
Actual params: [ 1.0531, -0.0136]
-Original Grad: 0.022, -lr * Pred Grad:  0.166, New P: 1.220
-Original Grad: -0.046, -lr * Pred Grad:  -0.254, New P: -0.267
iter 11 loss: 0.405
Actual params: [ 1.2196, -0.2674]
-Original Grad: -0.003, -lr * Pred Grad:  -0.045, New P: 1.175
-Original Grad: 0.013, -lr * Pred Grad:  0.073, New P: -0.195
iter 12 loss: 0.395
Actual params: [ 1.1746, -0.1948]
-Original Grad: 0.009, -lr * Pred Grad:  0.046, New P: 1.220
-Original Grad: -0.007, -lr * Pred Grad:  -0.063, New P: -0.258
iter 13 loss: 0.405
Actual params: [ 1.2204, -0.2578]
-Original Grad: 0.001, -lr * Pred Grad:  -0.072, New P: 1.149
-Original Grad: 0.023, -lr * Pred Grad:  0.127, New P: -0.131
iter 14 loss: 0.390
Actual params: [ 1.1487, -0.1305]
-Original Grad: -0.023, -lr * Pred Grad:  -0.138, New P: 1.011
-Original Grad: 0.019, -lr * Pred Grad:  0.186, New P: 0.055
iter 15 loss: 0.395
Actual params: [1.0107, 0.0554]
-Original Grad: 0.006, -lr * Pred Grad:  0.099, New P: 1.110
-Original Grad: -0.026, -lr * Pred Grad:  -0.162, New P: -0.107
iter 16 loss: 0.387
Actual params: [ 1.11  , -0.1068]
-Original Grad: -0.077, -lr * Pred Grad:  -0.399, New P: 0.711
-Original Grad: 0.086, -lr * Pred Grad:  0.551, New P: 0.444
iter 17 loss: 0.668
Actual params: [0.7109, 0.4441]
-Original Grad: -0.009, -lr * Pred Grad:  0.083, New P: 0.794
-Original Grad: -0.052, -lr * Pred Grad:  -0.201, New P: 0.244
iter 18 loss: 0.560
Actual params: [0.7938, 0.2436]
-Original Grad: -0.037, -lr * Pred Grad:  -0.154, New P: 0.640
-Original Grad: 0.016, -lr * Pred Grad:  0.157, New P: 0.401
iter 19 loss: 0.721
Actual params: [0.64 , 0.401]
-Original Grad: -0.002, -lr * Pred Grad:  0.007, New P: 0.647
-Original Grad: -0.005, -lr * Pred Grad:  -0.022, New P: 0.379
iter 20 loss: 0.711
Actual params: [0.6468, 0.3787]
-Original Grad: -0.011, -lr * Pred Grad:  -0.087, New P: 0.560
-Original Grad: 0.028, -lr * Pred Grad:  0.136, New P: 0.514
Target params: [1.1812, 0.2779]
iter 0 loss: 0.465
Actual params: [0.5941, 0.5941]
-Original Grad: 0.181, -lr * Pred Grad:  0.612, New P: 1.206
-Original Grad: 0.184, -lr * Pred Grad:  0.622, New P: 1.216
iter 1 loss: 0.616
Actual params: [1.2056, 1.2158]
-Original Grad: -0.081, -lr * Pred Grad:  0.415, New P: 1.621
-Original Grad: -0.487, -lr * Pred Grad:  -0.535, New P: 0.680
iter 2 loss: 0.417
Actual params: [1.6208, 0.6804]
-Original Grad: 0.068, -lr * Pred Grad:  0.135, New P: 1.756
-Original Grad: -1.152, -lr * Pred Grad:  -0.073, New P: 0.608
iter 3 loss: 0.428
Actual params: [1.756 , 0.6079]
-Original Grad: 0.021, -lr * Pred Grad:  0.225, New P: 1.981
-Original Grad: 0.551, -lr * Pred Grad:  0.045, New P: 0.653
iter 4 loss: 0.441
Actual params: [1.9809, 0.6527]
-Original Grad: 0.148, -lr * Pred Grad:  0.280, New P: 2.261
-Original Grad: -0.034, -lr * Pred Grad:  0.008, New P: 0.661
iter 5 loss: 0.472
Actual params: [2.2613, 0.6608]
-Original Grad: 0.125, -lr * Pred Grad:  0.039, New P: 2.301
-Original Grad: -1.921, -lr * Pred Grad:  -0.056, New P: 0.605
iter 6 loss: 0.477
Actual params: [2.3006, 0.6047]
-Original Grad: 0.001, -lr * Pred Grad:  -0.002, New P: 2.299
-Original Grad: -0.023, -lr * Pred Grad:  -0.001, New P: 0.604
iter 7 loss: 0.476
Actual params: [2.2988, 0.6038]
-Original Grad: -0.154, -lr * Pred Grad:  -0.276, New P: 2.023
-Original Grad: 0.078, -lr * Pred Grad:  -0.013, New P: 0.591
iter 8 loss: 0.446
Actual params: [2.0229, 0.5906]
-Original Grad: 0.104, -lr * Pred Grad:  0.218, New P: 2.241
-Original Grad: 0.312, -lr * Pred Grad:  0.024, New P: 0.615
iter 9 loss: 0.470
Actual params: [2.2414, 0.6147]
-Original Grad: 0.007, -lr * Pred Grad:  0.022, New P: 2.264
-Original Grad: 0.073, -lr * Pred Grad:  0.004, New P: 0.619
iter 10 loss: 0.472
Actual params: [2.2636, 0.6191]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 2.264
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 0.619
iter 11 loss: 0.472
Actual params: [2.2643, 0.6192]
-Original Grad: 0.018, -lr * Pred Grad:  0.033, New P: 2.297
-Original Grad: 0.025, -lr * Pred Grad:  0.003, New P: 0.622
iter 12 loss: 0.475
Actual params: [2.2972, 0.6218]
-Original Grad: 0.131, -lr * Pred Grad:  0.225, New P: 2.522
-Original Grad: 0.067, -lr * Pred Grad:  0.012, New P: 0.634
iter 13 loss: 0.489
Actual params: [2.5225, 0.6336]
-Original Grad: 0.098, -lr * Pred Grad:  0.185, New P: 2.707
-Original Grad: 0.480, -lr * Pred Grad:  0.034, New P: 0.668
iter 14 loss: 2.798
Actual params: [2.7074, 0.6677]
-Original Grad: 0.156, -lr * Pred Grad:  0.244, New P: 2.952
-Original Grad: 0.457, -lr * Pred Grad:  0.030, New P: 0.698
iter 15 loss: 2.923
Actual params: [2.9516, 0.6975]
-Original Grad: 0.123, -lr * Pred Grad:  0.115, New P: 3.066
-Original Grad: 0.327, -lr * Pred Grad:  0.013, New P: 0.710
iter 16 loss: 2.983
Actual params: [3.0664, 0.7103]
-Original Grad: -0.003, -lr * Pred Grad:  -0.001, New P: 3.065
-Original Grad: -0.031, -lr * Pred Grad:  -0.002, New P: 0.708
iter 17 loss: 2.980
Actual params: [3.0655, 0.7085]
-Original Grad: 0.227, -lr * Pred Grad:  0.208, New P: 3.274
-Original Grad: 0.522, -lr * Pred Grad:  0.014, New P: 0.722
iter 18 loss: 3.037
Actual params: [3.2736, 0.7222]
-Original Grad: -0.034, -lr * Pred Grad:  -0.005, New P: 3.268
-Original Grad: -0.322, -lr * Pred Grad:  -0.020, New P: 0.702
iter 19 loss: 3.014
Actual params: [3.2684, 0.7023]
-Original Grad: -1.349, -lr * Pred Grad:  -0.007, New P: 3.261
-Original Grad: -9.061, -lr * Pred Grad:  -0.005, New P: 0.697
iter 20 loss: 2.996
Actual params: [3.2613, 0.6969]
-Original Grad: -0.003, -lr * Pred Grad:  -0.020, New P: 3.242
-Original Grad: 0.074, -lr * Pred Grad:  0.003, New P: 0.700
Target params: [1.1812, 0.2779]
iter 0 loss: 0.551
Actual params: [0.5941, 0.5941]
-Original Grad: -0.050, -lr * Pred Grad:  -0.036, New P: 0.558
-Original Grad: -0.183, -lr * Pred Grad:  -0.132, New P: 0.462
iter 1 loss: 0.590
Actual params: [0.5582, 0.4624]
-Original Grad: 0.182, -lr * Pred Grad:  0.766, New P: 1.324
-Original Grad: -0.021, -lr * Pred Grad:  -0.206, New P: 0.256
iter 2 loss: 0.463
Actual params: [1.3238, 0.2562]
-Original Grad: -0.006, -lr * Pred Grad:  -0.014, New P: 1.310
-Original Grad: -0.011, -lr * Pred Grad:  -0.007, New P: 0.250
iter 3 loss: 0.459
Actual params: [1.3097, 0.2495]
-Original Grad: 0.131, -lr * Pred Grad:  -0.089, New P: 1.221
-Original Grad: 1.857, -lr * Pred Grad:  0.127, New P: 0.376
iter 4 loss: 0.461
Actual params: [1.2205, 0.3764]
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 1.229
-Original Grad: -0.018, -lr * Pred Grad:  -0.002, New P: 0.374
iter 5 loss: 0.462
Actual params: [1.229 , 0.3745]
-Original Grad: 0.007, -lr * Pred Grad:  0.035, New P: 1.264
-Original Grad: -0.014, -lr * Pred Grad:  -0.004, New P: 0.371
iter 6 loss: 0.464
Actual params: [1.2642, 0.3707]
-Original Grad: 0.030, -lr * Pred Grad:  0.330, New P: 1.594
-Original Grad: -1.075, -lr * Pred Grad:  -0.081, New P: 0.289
iter 7 loss: 0.494
Actual params: [1.5938, 0.2892]
-Original Grad: 0.092, -lr * Pred Grad:  0.367, New P: 1.961
-Original Grad: -0.097, -lr * Pred Grad:  -0.022, New P: 0.267
iter 8 loss: 0.504
Actual params: [1.9606, 0.2672]
-Original Grad: -0.038, -lr * Pred Grad:  -0.155, New P: 1.806
-Original Grad: -0.027, -lr * Pred Grad:  0.005, New P: 0.272
iter 9 loss: 0.504
Actual params: [1.8059, 0.2717]
-Original Grad: 0.108, -lr * Pred Grad:  0.392, New P: 2.198
-Original Grad: 0.034, -lr * Pred Grad:  -0.014, New P: 0.257
iter 10 loss: 0.496
Actual params: [2.1983, 0.2574]
-Original Grad: -0.049, -lr * Pred Grad:  -0.069, New P: 2.129
-Original Grad: -0.663, -lr * Pred Grad:  -0.050, New P: 0.207
iter 11 loss: 0.497
Actual params: [2.1295, 0.2072]
-Original Grad: 0.028, -lr * Pred Grad:  0.125, New P: 2.254
-Original Grad: -0.010, -lr * Pred Grad:  -0.007, New P: 0.200
iter 12 loss: 0.494
Actual params: [2.254 , 0.2004]
-Original Grad: 0.071, -lr * Pred Grad:  0.311, New P: 2.565
-Original Grad: 0.018, -lr * Pred Grad:  -0.013, New P: 0.187
iter 13 loss: 0.474
Actual params: [2.5649, 0.1871]
-Original Grad: 0.025, -lr * Pred Grad:  0.112, New P: 2.677
-Original Grad: 0.022, -lr * Pred Grad:  -0.003, New P: 0.184
iter 14 loss: 0.470
Actual params: [2.6765, 0.1841]
-Original Grad: -0.004, -lr * Pred Grad:  -0.019, New P: 2.658
-Original Grad: 0.002, -lr * Pred Grad:  0.001, New P: 0.185
iter 15 loss: 0.471
Actual params: [2.6575, 0.1853]
-Original Grad: 0.100, -lr * Pred Grad:  0.411, New P: 3.068
-Original Grad: 0.014, -lr * Pred Grad:  -0.019, New P: 0.167
iter 16 loss: 0.456
Actual params: [3.0681, 0.1667]
-Original Grad: -0.111, -lr * Pred Grad:  -0.400, New P: 2.668
-Original Grad: -0.017, -lr * Pred Grad:  0.018, New P: 0.185
iter 17 loss: 0.471
Actual params: [2.6682, 0.1847]
-Original Grad: -0.007, -lr * Pred Grad:  -0.024, New P: 2.644
-Original Grad: -0.001, -lr * Pred Grad:  0.001, New P: 0.186
iter 18 loss: 0.472
Actual params: [2.6439, 0.1858]
-Original Grad: 0.025, -lr * Pred Grad:  0.094, New P: 2.738
-Original Grad: 0.022, -lr * Pred Grad:  -0.001, New P: 0.185
iter 19 loss: 0.468
Actual params: [2.7384, 0.1849]
-Original Grad: -0.064, -lr * Pred Grad:  -0.206, New P: 2.532
-Original Grad: -0.216, -lr * Pred Grad:  -0.030, New P: 0.155
iter 20 loss: 0.471
Actual params: [2.532, 0.155]
-Original Grad: -0.056, -lr * Pred Grad:  -0.224, New P: 2.308
-Original Grad: -0.015, -lr * Pred Grad:  0.011, New P: 0.166
Target params: [1.1812, 0.2779]
iter 0 loss: 1.367
Actual params: [0.5941, 0.5941]
-Original Grad: -0.071, -lr * Pred Grad:  -0.126, New P: 0.468
-Original Grad: 0.049, -lr * Pred Grad:  0.087, New P: 0.681
iter 1 loss: 1.352
Actual params: [0.4676, 0.6811]
-Original Grad: -0.444, -lr * Pred Grad:  -0.248, New P: 0.219
-Original Grad: -0.181, -lr * Pred Grad:  -0.336, New P: 0.345
iter 2 loss: 1.336
Actual params: [0.2192, 0.3447]
-Original Grad: 0.031, -lr * Pred Grad:  0.031, New P: 0.251
-Original Grad: -0.035, -lr * Pred Grad:  -0.095, New P: 0.250
iter 3 loss: 1.338
Actual params: [0.2506, 0.2501]
-Original Grad: -0.149, -lr * Pred Grad:  -0.124, New P: 0.127
-Original Grad: 0.102, -lr * Pred Grad:  0.266, New P: 0.516
iter 4 loss: 1.326
Actual params: [0.127, 0.516]
-Original Grad: 0.010, -lr * Pred Grad:  0.003, New P: 0.130
-Original Grad: -0.176, -lr * Pred Grad:  -0.045, New P: 0.471
iter 5 loss: 1.326
Actual params: [0.1299, 0.4709]
-Original Grad: -0.016, -lr * Pred Grad:  -0.005, New P: 0.125
-Original Grad: 0.165, -lr * Pred Grad:  0.027, New P: 0.498
iter 6 loss: 1.326
Actual params: [0.1247, 0.4978]
-Original Grad: -0.038, -lr * Pred Grad:  -0.033, New P: 0.092
-Original Grad: 0.053, -lr * Pred Grad:  0.007, New P: 0.505
iter 7 loss: 1.324
Actual params: [0.0917, 0.5047]
-Original Grad: 0.020, -lr * Pred Grad:  -0.002, New P: 0.090
-Original Grad: -0.287, -lr * Pred Grad:  -0.037, New P: 0.467
iter 8 loss: 1.324
Actual params: [0.0901, 0.4675]
-Original Grad: -0.061, -lr * Pred Grad:  -0.052, New P: 0.038
-Original Grad: 0.192, -lr * Pred Grad:  0.022, New P: 0.489
iter 9 loss: 1.323
Actual params: [0.0382, 0.4893]
-Original Grad: -0.035, -lr * Pred Grad:  -0.039, New P: -0.001
-Original Grad: 0.051, -lr * Pred Grad:  0.004, New P: 0.494
iter 10 loss: 1.321
Actual params: [-0.0013,  0.4936]
-Original Grad: 0.094, -lr * Pred Grad:  0.106, New P: 0.104
-Original Grad: 0.033, -lr * Pred Grad:  0.013, New P: 0.507
iter 11 loss: 1.325
Actual params: [0.1044, 0.507 ]
-Original Grad: -0.006, -lr * Pred Grad:  -0.006, New P: 0.098
-Original Grad: 0.010, -lr * Pred Grad:  0.001, New P: 0.508
iter 12 loss: 1.325
Actual params: [0.0979, 0.5082]
-Original Grad: -0.114, -lr * Pred Grad:  -0.130, New P: -0.032
-Original Grad: 0.079, -lr * Pred Grad:  0.004, New P: 0.513
iter 13 loss: 1.320
Actual params: [-0.0321,  0.5125]
-Original Grad: -0.009, -lr * Pred Grad:  0.044, New P: 0.011
-Original Grad: 0.616, -lr * Pred Grad:  0.100, New P: 0.612
iter 14 loss: 1.320
Actual params: [0.0115, 0.6125]
-Original Grad: -0.006, -lr * Pred Grad:  -0.007, New P: 0.005
-Original Grad: 0.023, -lr * Pred Grad:  0.003, New P: 0.616
iter 15 loss: 1.320
Actual params: [0.0048, 0.616 ]
-Original Grad: 0.047, -lr * Pred Grad:  0.064, New P: 0.069
-Original Grad: -0.106, -lr * Pred Grad:  -0.015, New P: 0.601
iter 16 loss: 1.322
Actual params: [0.0686, 0.6006]
-Original Grad: -0.065, -lr * Pred Grad:  -0.016, New P: 0.053
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: 0.599
iter 17 loss: 1.322
Actual params: [0.0531, 0.5994]
-Original Grad: 0.028, -lr * Pred Grad:  0.008, New P: 0.061
-Original Grad: 0.023, -lr * Pred Grad:  0.006, New P: 0.605
iter 18 loss: 1.322
Actual params: [0.0608, 0.6052]
-Original Grad: -0.062, -lr * Pred Grad:  -0.019, New P: 0.042
-Original Grad: -0.055, -lr * Pred Grad:  -0.015, New P: 0.590
iter 19 loss: 1.321
Actual params: [0.0421, 0.5901]
-Original Grad: -0.041, -lr * Pred Grad:  -0.012, New P: 0.030
-Original Grad: 0.112, -lr * Pred Grad:  0.031, New P: 0.621
iter 20 loss: 1.320
Actual params: [0.0305, 0.6212]
-Original Grad: 0.002, -lr * Pred Grad:  -0.003, New P: 0.027
-Original Grad: -0.278, -lr * Pred Grad:  -0.062, New P: 0.560
Target params: [1.1812, 0.2779]
iter 0 loss: 0.857
Actual params: [0.5941, 0.5941]
-Original Grad: -0.077, -lr * Pred Grad:  -0.638, New P: -0.043
-Original Grad: 0.035, -lr * Pred Grad:  0.291, New P: 0.885
iter 1 loss: 1.036
Actual params: [-0.0435,  0.8847]
-Original Grad: 0.355, -lr * Pred Grad:  0.362, New P: 0.319
-Original Grad: 0.169, -lr * Pred Grad:  0.507, New P: 1.391
iter 2 loss: 0.897
Actual params: [0.3187, 1.3913]
-Original Grad: -0.086, -lr * Pred Grad:  -0.098, New P: 0.221
-Original Grad: -0.039, -lr * Pred Grad:  -0.096, New P: 1.296
iter 3 loss: 0.968
Actual params: [0.2208, 1.2957]
-Original Grad: -0.035, -lr * Pred Grad:  -0.168, New P: 0.052
-Original Grad: 0.511, -lr * Pred Grad:  0.449, New P: 1.745
iter 4 loss: 0.759
Actual params: [0.0524, 1.7449]
-Original Grad: -0.185, -lr * Pred Grad:  -0.255, New P: -0.203
-Original Grad: 0.026, -lr * Pred Grad:  0.047, New P: 1.792
iter 5 loss: 0.731
Actual params: [-0.203 ,  1.7919]
-Original Grad: -0.030, -lr * Pred Grad:  -0.048, New P: -0.251
-Original Grad: 0.118, -lr * Pred Grad:  0.101, New P: 1.893
iter 6 loss: 0.717
Actual params: [-0.2515,  1.8928]
-Original Grad: 0.080, -lr * Pred Grad:  0.109, New P: -0.142
-Original Grad: -0.086, -lr * Pred Grad:  -0.069, New P: 1.824
iter 7 loss: 0.733
Actual params: [-0.1421,  1.8241]
-Original Grad: 0.045, -lr * Pred Grad:  0.020, New P: -0.122
-Original Grad: -0.417, -lr * Pred Grad:  -0.200, New P: 1.624
iter 8 loss: 0.753
Actual params: [-0.1225,  1.6243]
-Original Grad: 0.006, -lr * Pred Grad:  0.010, New P: -0.112
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: 1.625
iter 9 loss: 0.754
Actual params: [-0.1122,  1.625 ]
-Original Grad: -0.112, -lr * Pred Grad:  -0.137, New P: -0.250
-Original Grad: 0.260, -lr * Pred Grad:  0.114, New P: 1.739
iter 10 loss: 0.730
Actual params: [-0.2495,  1.7393]
-Original Grad: 0.032, -lr * Pred Grad:  0.059, New P: -0.191
-Original Grad: 0.005, -lr * Pred Grad:  0.010, New P: 1.750
iter 11 loss: 0.736
Actual params: [-0.191 ,  1.7498]
-Original Grad: -0.057, -lr * Pred Grad:  -0.094, New P: -0.284
-Original Grad: -0.011, -lr * Pred Grad:  -0.018, New P: 1.732
iter 12 loss: 0.726
Actual params: [-0.2845,  1.7323]
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: -0.284
-Original Grad: -0.005, -lr * Pred Grad:  -0.003, New P: 1.729
iter 13 loss: 0.727
Actual params: [-0.2837,  1.7288]
-Original Grad: 0.026, -lr * Pred Grad:  0.006, New P: -0.278
-Original Grad: -0.189, -lr * Pred Grad:  -0.105, New P: 1.623
iter 14 loss: 0.736
Actual params: [-0.278 ,  1.6235]
-Original Grad: -0.002, -lr * Pred Grad:  -0.006, New P: -0.284
-Original Grad: -0.012, -lr * Pred Grad:  -0.008, New P: 1.616
iter 15 loss: 0.736
Actual params: [-0.2844,  1.6155]
-Original Grad: 0.019, -lr * Pred Grad:  0.050, New P: -0.235
-Original Grad: 0.020, -lr * Pred Grad:  0.019, New P: 1.635
iter 16 loss: 0.740
Actual params: [-0.2347,  1.6348]
-Original Grad: 0.100, -lr * Pred Grad:  0.131, New P: -0.103
-Original Grad: -0.226, -lr * Pred Grad:  -0.099, New P: 1.536
iter 17 loss: 0.785
Actual params: [-0.1035,  1.5355]
-Original Grad: 0.092, -lr * Pred Grad:  0.184, New P: 0.081
-Original Grad: 0.001, -lr * Pred Grad:  0.037, New P: 1.573
iter 18 loss: 0.807
Actual params: [0.0809, 1.573 ]
-Original Grad: 0.050, -lr * Pred Grad:  -0.045, New P: 0.036
-Original Grad: -0.544, -lr * Pred Grad:  -0.138, New P: 1.435
iter 19 loss: 0.887
Actual params: [0.0361, 1.4347]
-Original Grad: 0.132, -lr * Pred Grad:  0.164, New P: 0.200
-Original Grad: 0.064, -lr * Pred Grad:  0.030, New P: 1.464
iter 20 loss: 0.878
Actual params: [0.1997, 1.4643]
-Original Grad: -0.021, -lr * Pred Grad:  -0.022, New P: 0.178
-Original Grad: 0.013, -lr * Pred Grad:  0.001, New P: 1.466
Target params: [1.1812, 0.2779]
iter 0 loss: 0.595
Actual params: [0.5941, 0.5941]
-Original Grad: 0.022, -lr * Pred Grad:  0.029, New P: 0.623
-Original Grad: -0.434, -lr * Pred Grad:  -0.554, New P: 0.040
iter 1 loss: 1.050
Actual params: [0.6227, 0.0398]
-Original Grad: -0.000, -lr * Pred Grad:  -0.051, New P: 0.572
-Original Grad: -0.022, -lr * Pred Grad:  -0.031, New P: 0.009
iter 2 loss: 1.057
Actual params: [0.5715, 0.0093]
-Original Grad: -0.145, -lr * Pred Grad:  -0.120, New P: 0.451
-Original Grad: -0.049, -lr * Pred Grad:  -0.007, New P: 0.003
iter 3 loss: 1.033
Actual params: [0.4513, 0.0026]
-Original Grad: -0.197, -lr * Pred Grad:  -0.125, New P: 0.326
-Original Grad: -0.126, -lr * Pred Grad:  -0.083, New P: -0.080
iter 4 loss: 1.030
Actual params: [ 0.3263, -0.08  ]
-Original Grad: 0.426, -lr * Pred Grad:  0.169, New P: 0.495
-Original Grad: 0.180, -lr * Pred Grad:  0.021, New P: -0.059
iter 5 loss: 1.061
Actual params: [ 0.4955, -0.0591]
-Original Grad: 0.006, -lr * Pred Grad:  -0.039, New P: 0.457
-Original Grad: 0.083, -lr * Pred Grad:  0.103, New P: 0.044
iter 6 loss: 1.019
Actual params: [0.4569, 0.0444]
-Original Grad: 0.185, -lr * Pred Grad:  0.072, New P: 0.529
-Original Grad: 0.088, -lr * Pred Grad:  0.017, New P: 0.061
iter 7 loss: 1.023
Actual params: [0.5291, 0.0613]
-Original Grad: -0.084, -lr * Pred Grad:  0.015, New P: 0.545
-Original Grad: -0.123, -lr * Pred Grad:  -0.124, New P: -0.063
iter 8 loss: 1.076
Actual params: [ 0.5446, -0.063 ]
-Original Grad: -0.071, -lr * Pred Grad:  0.067, New P: 0.612
-Original Grad: -0.202, -lr * Pred Grad:  -0.217, New P: -0.280
iter 9 loss: 1.165
Actual params: [ 0.6115, -0.2801]
-Original Grad: 0.263, -lr * Pred Grad:  0.164, New P: 0.776
-Original Grad: 0.053, -lr * Pred Grad:  -0.085, New P: -0.365
iter 10 loss: 1.311
Actual params: [ 0.776 , -0.3654]
-Original Grad: 0.022, -lr * Pred Grad:  -0.053, New P: 0.723
-Original Grad: 0.112, -lr * Pred Grad:  0.148, New P: -0.217
iter 11 loss: 1.208
Actual params: [ 0.7229, -0.2174]
-Original Grad: -0.230, -lr * Pred Grad:  -0.200, New P: 0.523
-Original Grad: 0.216, -lr * Pred Grad:  0.282, New P: 0.065
iter 12 loss: 1.021
Actual params: [0.5228, 0.0649]
-Original Grad: -0.063, -lr * Pred Grad:  -0.002, New P: 0.521
-Original Grad: -0.127, -lr * Pred Grad:  -0.093, New P: -0.028
iter 13 loss: 1.058
Actual params: [ 0.5207, -0.0285]
-Original Grad: 0.009, -lr * Pred Grad:  -0.004, New P: 0.517
-Original Grad: 0.031, -lr * Pred Grad:  0.026, New P: -0.003
iter 14 loss: 1.047
Actual params: [ 0.517 , -0.0027]
-Original Grad: 0.279, -lr * Pred Grad:  0.153, New P: 0.670
-Original Grad: 0.117, -lr * Pred Grad:  0.012, New P: 0.010
iter 15 loss: 1.084
Actual params: [0.6704, 0.0097]
-Original Grad: -0.533, -lr * Pred Grad:  -0.165, New P: 0.505
-Original Grad: -0.257, -lr * Pred Grad:  -0.034, New P: -0.025
iter 16 loss: 1.052
Actual params: [ 0.505 , -0.0247]
-Original Grad: 0.366, -lr * Pred Grad:  0.142, New P: 0.647
-Original Grad: 0.074, -lr * Pred Grad:  -0.081, New P: -0.106
iter 17 loss: 1.130
Actual params: [ 0.6474, -0.1058]
-Original Grad: 0.971, -lr * Pred Grad:  0.140, New P: 0.787
-Original Grad: 0.170, -lr * Pred Grad:  -0.089, New P: -0.195
iter 18 loss: 1.240
Actual params: [ 0.7875, -0.1951]
-Original Grad: 0.022, -lr * Pred Grad:  0.018, New P: 0.805
-Original Grad: -0.046, -lr * Pred Grad:  -0.060, New P: -0.255
iter 19 loss: 1.274
Actual params: [ 0.8051, -0.2555]
-Original Grad: 0.014, -lr * Pred Grad:  0.043, New P: 0.848
-Original Grad: -0.137, -lr * Pred Grad:  -0.168, New P: -0.423
iter 20 loss: 1.390
Actual params: [ 0.8478, -0.4231]
-Original Grad: -0.037, -lr * Pred Grad:  0.001, New P: 0.848
-Original Grad: -0.030, -lr * Pred Grad:  -0.025, New P: -0.448
Target params: [1.1812, 0.2779]
iter 0 loss: 0.832
Actual params: [0.5941, 0.5941]
-Original Grad: -0.176, -lr * Pred Grad:  -0.332, New P: 0.262
-Original Grad: -0.240, -lr * Pred Grad:  -0.454, New P: 0.140
iter 1 loss: 0.605
Actual params: [0.2616, 0.1396]
-Original Grad: -0.005, -lr * Pred Grad:  0.023, New P: 0.285
-Original Grad: -0.010, -lr * Pred Grad:  -0.035, New P: 0.104
iter 2 loss: 0.596
Actual params: [0.285 , 0.1041]
-Original Grad: 0.052, -lr * Pred Grad:  0.246, New P: 0.531
-Original Grad: 0.063, -lr * Pred Grad:  -0.043, New P: 0.061
iter 3 loss: 0.552
Actual params: [0.5308, 0.0613]
-Original Grad: 0.240, -lr * Pred Grad:  0.052, New P: 0.583
-Original Grad: -0.102, -lr * Pred Grad:  -0.036, New P: 0.025
iter 4 loss: 0.528
Actual params: [0.5828, 0.0253]
-Original Grad: -0.016, -lr * Pred Grad:  0.022, New P: 0.605
-Original Grad: 0.061, -lr * Pred Grad:  0.084, New P: 0.109
iter 5 loss: 0.536
Actual params: [0.6047, 0.109 ]
-Original Grad: -0.152, -lr * Pred Grad:  -0.072, New P: 0.533
-Original Grad: -0.149, -lr * Pred Grad:  -0.152, New P: -0.043
iter 6 loss: 0.533
Actual params: [ 0.5332, -0.0433]
-Original Grad: 0.776, -lr * Pred Grad:  0.128, New P: 0.661
-Original Grad: 0.092, -lr * Pred Grad:  0.098, New P: 0.054
iter 7 loss: 0.493
Actual params: [0.6611, 0.0545]
-Original Grad: -0.075, -lr * Pred Grad:  -0.010, New P: 0.651
-Original Grad: 0.025, -lr * Pred Grad:  0.017, New P: 0.072
iter 8 loss: 0.505
Actual params: [0.6506, 0.072 ]
-Original Grad: 0.229, -lr * Pred Grad:  0.023, New P: 0.674
-Original Grad: -0.199, -lr * Pred Grad:  -0.152, New P: -0.081
iter 9 loss: 0.459
Actual params: [ 0.6735, -0.0805]
-Original Grad: 0.033, -lr * Pred Grad:  0.004, New P: 0.677
-Original Grad: -0.006, -lr * Pred Grad:  -0.002, New P: -0.082
iter 10 loss: 0.456
Actual params: [ 0.677 , -0.0823]
-Original Grad: -0.017, -lr * Pred Grad:  -0.011, New P: 0.666
-Original Grad: -0.062, -lr * Pred Grad:  -0.066, New P: -0.148
iter 11 loss: 0.455
Actual params: [ 0.666 , -0.1481]
-Original Grad: -0.157, -lr * Pred Grad:  -0.022, New P: 0.644
-Original Grad: 0.021, -lr * Pred Grad:  -0.000, New P: -0.148
iter 12 loss: 0.470
Actual params: [ 0.6442, -0.1485]
-Original Grad: 0.011, -lr * Pred Grad:  -0.011, New P: 0.633
-Original Grad: -0.081, -lr * Pred Grad:  -0.095, New P: -0.244
iter 13 loss: 0.469
Actual params: [ 0.6329, -0.2439]
-Original Grad: -0.089, -lr * Pred Grad:  -0.018, New P: 0.615
-Original Grad: -0.013, -lr * Pred Grad:  -0.029, New P: -0.273
iter 14 loss: 0.473
Actual params: [ 0.6153, -0.2733]
-Original Grad: 0.075, -lr * Pred Grad:  0.024, New P: 0.640
-Original Grad: 0.132, -lr * Pred Grad:  0.134, New P: -0.139
iter 15 loss: 0.473
Actual params: [ 0.6397, -0.1393]
-Original Grad: -0.523, -lr * Pred Grad:  -0.055, New P: 0.584
-Original Grad: -0.148, -lr * Pred Grad:  -0.097, New P: -0.236
iter 16 loss: 0.492
Actual params: [ 0.5845, -0.2361]
-Original Grad: 0.533, -lr * Pred Grad:  0.056, New P: 0.640
-Original Grad: 0.053, -lr * Pred Grad:  0.016, New P: -0.220
iter 17 loss: 0.467
Actual params: [ 0.6401, -0.2205]
-Original Grad: -1.029, -lr * Pred Grad:  -0.054, New P: 0.586
-Original Grad: -0.360, -lr * Pred Grad:  -0.159, New P: -0.380
iter 18 loss: 0.483
Actual params: [ 0.5858, -0.3798]
-Original Grad: 1.275, -lr * Pred Grad:  0.067, New P: 0.653
-Original Grad: 0.210, -lr * Pred Grad:  0.001, New P: -0.379
iter 19 loss: 0.448
Actual params: [ 0.653 , -0.3787]
-Original Grad: -0.040, -lr * Pred Grad:  -0.013, New P: 0.640
-Original Grad: 0.059, -lr * Pred Grad:  0.063, New P: -0.316
iter 20 loss: 0.461
Actual params: [ 0.6405, -0.3161]
-Original Grad: -0.896, -lr * Pred Grad:  -0.032, New P: 0.608
-Original Grad: -0.185, -lr * Pred Grad:  -0.027, New P: -0.343
Target params: [1.1812, 0.2779]
iter 0 loss: 0.667
Actual params: [0.5941, 0.5941]
-Original Grad: 0.014, -lr * Pred Grad:  0.431, New P: 1.025
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: 0.582
iter 1 loss: 0.895
Actual params: [1.0246, 0.5819]
-Original Grad: -0.088, -lr * Pred Grad:  -0.679, New P: 0.345
-Original Grad: 0.029, -lr * Pred Grad:  0.263, New P: 0.844
iter 2 loss: 0.688
Actual params: [0.3454, 0.8444]
-Original Grad: -0.030, -lr * Pred Grad:  -0.054, New P: 0.291
-Original Grad: 0.029, -lr * Pred Grad:  0.787, New P: 1.632
iter 3 loss: 0.733
Actual params: [0.2913, 1.6317]
-Original Grad: -0.048, -lr * Pred Grad:  -0.444, New P: -0.153
-Original Grad: 0.012, -lr * Pred Grad:  -0.036, New P: 1.596
iter 4 loss: 0.789
Actual params: [-0.1531,  1.596 ]
-Original Grad: 0.050, -lr * Pred Grad:  -0.037, New P: -0.190
-Original Grad: -0.325, -lr * Pred Grad:  -0.155, New P: 1.441
iter 5 loss: 0.793
Actual params: [-0.1904,  1.4412]
-Original Grad: -0.039, -lr * Pred Grad:  0.082, New P: -0.109
-Original Grad: 0.291, -lr * Pred Grad:  0.129, New P: 1.570
iter 6 loss: 0.787
Actual params: [-0.1089,  1.5699]
-Original Grad: 0.007, -lr * Pred Grad:  0.061, New P: -0.048
-Original Grad: -0.008, -lr * Pred Grad:  0.007, New P: 1.577
iter 7 loss: 0.781
Actual params: [-0.0477,  1.5766]
-Original Grad: -0.004, -lr * Pred Grad:  -0.022, New P: -0.070
-Original Grad: 0.005, -lr * Pred Grad:  -0.002, New P: 1.574
iter 8 loss: 0.783
Actual params: [-0.0701,  1.5742]
-Original Grad: -0.182, -lr * Pred Grad:  -0.379, New P: -0.449
-Original Grad: 0.228, -lr * Pred Grad:  -0.039, New P: 1.535
iter 9 loss: 0.806
Actual params: [-0.4486,  1.5351]
-Original Grad: 0.020, -lr * Pred Grad:  0.041, New P: -0.408
-Original Grad: -0.026, -lr * Pred Grad:  0.003, New P: 1.538
iter 10 loss: 0.803
Actual params: [-0.4076,  1.5384]
-Original Grad: -0.147, -lr * Pred Grad:  -0.342, New P: -0.750
-Original Grad: 0.015, -lr * Pred Grad:  -0.116, New P: 1.422
iter 11 loss: 0.837
Actual params: [-0.7501,  1.4222]
-Original Grad: -0.059, -lr * Pred Grad:  0.003, New P: -0.747
-Original Grad: 0.166, -lr * Pred Grad:  0.083, New P: 1.505
iter 12 loss: 0.835
Actual params: [-0.7475,  1.5049]
-Original Grad: 0.010, -lr * Pred Grad:  -0.023, New P: -0.771
-Original Grad: -0.050, -lr * Pred Grad:  -0.035, New P: 1.470
iter 13 loss: 0.838
Actual params: [-0.7708,  1.4695]
-Original Grad: -0.041, -lr * Pred Grad:  -0.085, New P: -0.856
-Original Grad: 0.025, -lr * Pred Grad:  -0.018, New P: 1.451
iter 14 loss: 0.850
Actual params: [-0.8559,  1.451 ]
-Original Grad: 0.011, -lr * Pred Grad:  -0.022, New P: -0.878
-Original Grad: -0.049, -lr * Pred Grad:  -0.040, New P: 1.411
iter 15 loss: 0.853
Actual params: [-0.8783,  1.4109]
-Original Grad: 0.028, -lr * Pred Grad:  -0.001, New P: -0.879
-Original Grad: -0.073, -lr * Pred Grad:  -0.050, New P: 1.361
iter 16 loss: 0.855
Actual params: [-0.8793,  1.3608]
-Original Grad: 0.099, -lr * Pred Grad:  0.244, New P: -0.635
-Original Grad: -0.042, -lr * Pred Grad:  0.069, New P: 1.430
iter 17 loss: 0.825
Actual params: [-0.6351,  1.4296]
-Original Grad: 0.004, -lr * Pred Grad:  0.004, New P: -0.631
-Original Grad: -0.004, -lr * Pred Grad:  0.000, New P: 1.430
iter 18 loss: 0.825
Actual params: [-0.6311,  1.4297]
-Original Grad: 0.099, -lr * Pred Grad:  0.101, New P: -0.531
-Original Grad: -0.095, -lr * Pred Grad:  -0.002, New P: 1.428
iter 19 loss: 0.815
Actual params: [-0.5306,  1.4281]
-Original Grad: 0.188, -lr * Pred Grad:  0.359, New P: -0.172
-Original Grad: -0.010, -lr * Pred Grad:  0.212, New P: 1.640
iter 20 loss: 0.788
Actual params: [-0.1719,  1.6397]
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.173
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: 1.640
Target params: [1.1812, 0.2779]
iter 0 loss: 0.903
Actual params: [0.5941, 0.5941]
-Original Grad: 0.058, -lr * Pred Grad:  0.079, New P: 0.673
-Original Grad: 0.316, -lr * Pred Grad:  0.428, New P: 1.022
iter 1 loss: 1.668
Actual params: [0.6729, 1.0217]
-Original Grad: -0.040, -lr * Pred Grad:  -0.223, New P: 0.450
-Original Grad: 0.113, -lr * Pred Grad:  0.056, New P: 1.078
iter 2 loss: 1.343
Actual params: [0.4503, 1.0781]
-Original Grad: -0.040, -lr * Pred Grad:  -0.241, New P: 0.209
-Original Grad: 0.064, -lr * Pred Grad:  0.015, New P: 1.093
iter 3 loss: 0.958
Actual params: [0.2095, 1.0929]
-Original Grad: 0.216, -lr * Pred Grad:  0.529, New P: 0.739
-Original Grad: -0.174, -lr * Pred Grad:  0.026, New P: 1.119
iter 4 loss: 1.716
Actual params: [0.7387, 1.119 ]
-Original Grad: 0.033, -lr * Pred Grad:  0.166, New P: 0.905
-Original Grad: 0.102, -lr * Pred Grad:  0.110, New P: 1.229
iter 5 loss: 1.670
Actual params: [0.9048, 1.2286]
-Original Grad: 0.011, -lr * Pred Grad:  0.055, New P: 0.960
-Original Grad: 0.121, -lr * Pred Grad:  0.055, New P: 1.283
iter 6 loss: 1.599
Actual params: [0.96  , 1.2832]
-Original Grad: 0.070, -lr * Pred Grad:  0.200, New P: 1.160
-Original Grad: 0.058, -lr * Pred Grad:  0.038, New P: 1.321
iter 7 loss: 1.474
Actual params: [1.1604, 1.3215]
-Original Grad: 0.328, -lr * Pred Grad:  0.357, New P: 1.518
-Original Grad: 0.175, -lr * Pred Grad:  0.052, New P: 1.373
iter 8 loss: 1.279
Actual params: [1.5175, 1.3732]
-Original Grad: -0.659, -lr * Pred Grad:  -0.243, New P: 1.275
-Original Grad: 0.276, -lr * Pred Grad:  0.062, New P: 1.435
iter 9 loss: 1.331
Actual params: [1.2745, 1.4348]
-Original Grad: -0.196, -lr * Pred Grad:  -0.034, New P: 1.241
-Original Grad: 0.352, -lr * Pred Grad:  0.115, New P: 1.550
iter 10 loss: 1.295
Actual params: [1.2407, 1.5499]
-Original Grad: 0.072, -lr * Pred Grad:  0.033, New P: 1.274
-Original Grad: -0.014, -lr * Pred Grad:  0.006, New P: 1.556
iter 11 loss: 1.277
Actual params: [1.2736, 1.5556]
-Original Grad: -0.170, -lr * Pred Grad:  -0.098, New P: 1.176
-Original Grad: -0.117, -lr * Pred Grad:  -0.078, New P: 1.478
iter 12 loss: 1.359
Actual params: [1.1756, 1.4777]
-Original Grad: -0.278, -lr * Pred Grad:  -0.055, New P: 1.120
-Original Grad: 0.456, -lr * Pred Grad:  0.134, New P: 1.611
iter 13 loss: 1.340
Actual params: [1.1203, 1.6114]
-Original Grad: -0.131, -lr * Pred Grad:  0.010, New P: 1.131
-Original Grad: 0.410, -lr * Pred Grad:  0.122, New P: 1.733
iter 14 loss: 1.298
Actual params: [1.1307, 1.7335]
-Original Grad: 0.109, -lr * Pred Grad:  0.075, New P: 1.206
-Original Grad: 0.041, -lr * Pred Grad:  0.040, New P: 1.773
iter 15 loss: 1.247
Actual params: [1.2059, 1.7734]
-Original Grad: -0.352, -lr * Pred Grad:  -0.179, New P: 1.027
-Original Grad: -0.170, -lr * Pred Grad:  -0.102, New P: 1.671
iter 16 loss: 1.371
Actual params: [1.0265, 1.6709]
-Original Grad: -0.004, -lr * Pred Grad:  0.002, New P: 1.028
-Original Grad: 0.029, -lr * Pred Grad:  0.011, New P: 1.682
iter 17 loss: 1.367
Actual params: [1.0284, 1.6822]
-Original Grad: 0.116, -lr * Pred Grad:  0.062, New P: 1.091
-Original Grad: -0.004, -lr * Pred Grad:  0.014, New P: 1.696
iter 18 loss: 1.332
Actual params: [1.0907, 1.6962]
-Original Grad: -0.250, -lr * Pred Grad:  -0.108, New P: 0.983
-Original Grad: -0.221, -lr * Pred Grad:  -0.086, New P: 1.611
iter 19 loss: 1.409
Actual params: [0.9829, 1.6106]
-Original Grad: 0.168, -lr * Pred Grad:  0.054, New P: 1.037
-Original Grad: 0.513, -lr * Pred Grad:  0.140, New P: 1.750
iter 20 loss: 1.349
Actual params: [1.0366, 1.7504]
-Original Grad: -0.325, -lr * Pred Grad:  -0.119, New P: 0.917
-Original Grad: 0.453, -lr * Pred Grad:  0.103, New P: 1.854
Target params: [1.1812, 0.2779]
iter 0 loss: 0.874
Actual params: [0.5941, 0.5941]
-Original Grad: 0.979, -lr * Pred Grad:  0.115, New P: 0.709
-Original Grad: 1.073, -lr * Pred Grad:  0.126, New P: 0.721
iter 1 loss: 0.535
Actual params: [0.7095, 0.7205]
-Original Grad: -0.299, -lr * Pred Grad:  0.118, New P: 0.828
-Original Grad: -2.617, -lr * Pred Grad:  -0.109, New P: 0.612
iter 2 loss: 0.386
Actual params: [0.8275, 0.6118]
-Original Grad: -0.023, -lr * Pred Grad:  -0.017, New P: 0.810
-Original Grad: 0.326, -lr * Pred Grad:  0.008, New P: 0.620
iter 3 loss: 0.400
Actual params: [0.8104, 0.6199]
-Original Grad: -0.187, -lr * Pred Grad:  -0.067, New P: 0.743
-Original Grad: 0.212, -lr * Pred Grad:  0.011, New P: 0.631
iter 4 loss: 0.467
Actual params: [0.7433, 0.6305]
-Original Grad: 0.085, -lr * Pred Grad:  0.019, New P: 0.762
-Original Grad: 0.361, -lr * Pred Grad:  0.007, New P: 0.638
iter 5 loss: 0.440
Actual params: [0.7621, 0.6375]
-Original Grad: 0.114, -lr * Pred Grad:  0.055, New P: 0.817
-Original Grad: -1.053, -lr * Pred Grad:  -0.023, New P: 0.614
iter 6 loss: 0.395
Actual params: [0.8167, 0.6142]
-Original Grad: -0.271, -lr * Pred Grad:  -0.088, New P: 0.728
-Original Grad: 1.693, -lr * Pred Grad:  0.029, New P: 0.643
iter 7 loss: 0.471
Actual params: [0.7282, 0.6431]
-Original Grad: -0.006, -lr * Pred Grad:  -0.002, New P: 0.726
-Original Grad: -0.038, -lr * Pred Grad:  -0.001, New P: 0.642
iter 8 loss: 0.475
Actual params: [0.7259, 0.6424]
-Original Grad: -0.038, -lr * Pred Grad:  -0.010, New P: 0.716
-Original Grad: -0.525, -lr * Pred Grad:  -0.007, New P: 0.635
iter 9 loss: 0.495
Actual params: [0.7156, 0.6352]
-Original Grad: -0.007, -lr * Pred Grad:  0.002, New P: 0.718
-Original Grad: -0.608, -lr * Pred Grad:  -0.007, New P: 0.628
iter 10 loss: 0.501
Actual params: [0.7179, 0.6278]
-Original Grad: -0.008, -lr * Pred Grad:  -0.004, New P: 0.714
-Original Grad: 0.006, -lr * Pred Grad:  0.000, New P: 0.628
iter 11 loss: 0.506
Actual params: [0.7138, 0.628 ]
-Original Grad: 1.853, -lr * Pred Grad:  0.119, New P: 0.833
-Original Grad: 0.336, -lr * Pred Grad:  -0.002, New P: 0.626
iter 12 loss: 0.377
Actual params: [0.8328, 0.6261]
-Original Grad: -0.008, -lr * Pred Grad:  -0.001, New P: 0.832
-Original Grad: 0.007, -lr * Pred Grad:  0.000, New P: 0.626
iter 13 loss: 0.378
Actual params: [0.8322, 0.6263]
-Original Grad: 0.082, -lr * Pred Grad:  0.006, New P: 0.838
-Original Grad: 0.158, -lr * Pred Grad:  0.003, New P: 0.629
iter 14 loss: 0.374
Actual params: [0.838 , 0.6288]
-Original Grad: 0.026, -lr * Pred Grad:  0.007, New P: 0.845
-Original Grad: -1.020, -lr * Pred Grad:  -0.018, New P: 0.611
iter 15 loss: 0.371
Actual params: [0.8446, 0.6107]
-Original Grad: -0.246, -lr * Pred Grad:  -0.012, New P: 0.833
-Original Grad: -0.068, -lr * Pred Grad:  -0.000, New P: 0.611
iter 16 loss: 0.383
Actual params: [0.8326, 0.6106]
-Original Grad: 0.186, -lr * Pred Grad:  0.015, New P: 0.847
-Original Grad: -1.135, -lr * Pred Grad:  -0.023, New P: 0.588
iter 17 loss: 0.380
Actual params: [0.8473, 0.588 ]
-Original Grad: -1.153, -lr * Pred Grad:  -0.042, New P: 0.805
-Original Grad: -0.113, -lr * Pred Grad:  0.002, New P: 0.590
iter 18 loss: 0.432
Actual params: [0.8055, 0.5897]
-Original Grad: 0.021, -lr * Pred Grad:  0.003, New P: 0.808
-Original Grad: -0.556, -lr * Pred Grad:  -0.012, New P: 0.578
iter 19 loss: 0.441
Actual params: [0.8082, 0.5778]
-Original Grad: -0.287, -lr * Pred Grad:  -0.014, New P: 0.794
-Original Grad: 2.078, -lr * Pred Grad:  0.034, New P: 0.612
iter 20 loss: 0.425
Actual params: [0.794 , 0.6121]
-Original Grad: 0.178, -lr * Pred Grad:  0.008, New P: 0.802
-Original Grad: -0.951, -lr * Pred Grad:  -0.015, New P: 0.597
Target params: [1.1812, 0.2779]
iter 0 loss: 0.447
Actual params: [0.5941, 0.5941]
-Original Grad: -0.019, -lr * Pred Grad:  -0.385, New P: 0.209
-Original Grad: -0.057, -lr * Pred Grad:  -1.174, New P: -0.580
iter 1 loss: 0.512
Actual params: [ 0.2086, -0.5802]
-Original Grad: 0.008, -lr * Pred Grad:  0.327, New P: 0.536
-Original Grad: -0.022, -lr * Pred Grad:  -0.465, New P: -1.045
iter 2 loss: 0.448
Actual params: [ 0.5359, -1.0449]
-Original Grad: -0.046, -lr * Pred Grad:  -0.435, New P: 0.101
-Original Grad: -0.022, -lr * Pred Grad:  -0.097, New P: -1.142
iter 3 loss: 0.445
Actual params: [ 0.101 , -1.1418]
-Original Grad: 0.022, -lr * Pred Grad:  -0.338, New P: -0.237
-Original Grad: 0.157, -lr * Pred Grad:  1.122, New P: -0.020
iter 4 loss: 0.513
Actual params: [-0.2368, -0.02  ]
-Original Grad: 0.007, -lr * Pred Grad:  0.038, New P: -0.199
-Original Grad: 0.014, -lr * Pred Grad:  0.082, New P: 0.062
iter 5 loss: 0.519
Actual params: [-0.1985,  0.0623]
-Original Grad: 0.050, -lr * Pred Grad:  0.731, New P: 0.533
-Original Grad: -0.125, -lr * Pred Grad:  -0.704, New P: -0.641
iter 6 loss: 0.462
Actual params: [ 0.5325, -0.6414]
-Original Grad: -0.096, -lr * Pred Grad:  -0.792, New P: -0.259
-Original Grad: -0.062, -lr * Pred Grad:  -0.164, New P: -0.805
iter 7 loss: 0.465
Actual params: [-0.2594, -0.8054]
-Original Grad: 0.001, -lr * Pred Grad:  -0.077, New P: -0.337
-Original Grad: 0.041, -lr * Pred Grad:  0.244, New P: -0.562
iter 8 loss: 0.481
Actual params: [-0.3369, -0.5619]
-Original Grad: -0.044, -lr * Pred Grad:  -0.373, New P: -0.710
-Original Grad: -0.012, -lr * Pred Grad:  0.017, New P: -0.545
iter 9 loss: 0.470
Actual params: [-0.7104, -0.545 ]
-Original Grad: -0.009, -lr * Pred Grad:  -0.065, New P: -0.775
-Original Grad: -0.011, -lr * Pred Grad:  -0.057, New P: -0.602
iter 10 loss: 0.465
Actual params: [-0.7752, -0.6017]
-Original Grad: 0.042, -lr * Pred Grad:  0.404, New P: -0.371
-Original Grad: 0.007, -lr * Pred Grad:  -0.055, New P: -0.656
iter 11 loss: 0.473
Actual params: [-0.3712, -0.6563]
-Original Grad: -0.003, -lr * Pred Grad:  -0.063, New P: -0.434
-Original Grad: 0.014, -lr * Pred Grad:  0.115, New P: -0.541
iter 12 loss: 0.479
Actual params: [-0.4342, -0.541 ]
-Original Grad: -0.026, -lr * Pred Grad:  -0.312, New P: -0.746
-Original Grad: 0.005, -lr * Pred Grad:  0.105, New P: -0.436
iter 13 loss: 0.473
Actual params: [-0.746 , -0.4364]
-Original Grad: 0.002, -lr * Pred Grad:  0.112, New P: -0.633
-Original Grad: -0.035, -lr * Pred Grad:  -0.323, New P: -0.759
iter 14 loss: 0.459
Actual params: [-0.6335, -0.7595]
-Original Grad: -0.244, -lr * Pred Grad:  -0.177, New P: -0.811
-Original Grad: -0.478, -lr * Pred Grad:  -0.340, New P: -1.099
iter 15 loss: 0.435
Actual params: [-0.8108, -1.0995]
-Original Grad: -0.000, -lr * Pred Grad:  0.479, New P: -0.332
-Original Grad: -0.123, -lr * Pred Grad:  -0.302, New P: -1.401
iter 16 loss: 0.444
Actual params: [-0.3316, -1.4014]
-Original Grad: -0.041, -lr * Pred Grad:  -0.677, New P: -1.009
-Original Grad: 0.114, -lr * Pred Grad:  0.357, New P: -1.044
iter 17 loss: 0.438
Actual params: [-1.009, -1.044]
-Original Grad: 0.064, -lr * Pred Grad:  0.454, New P: -0.555
-Original Grad: 0.020, -lr * Pred Grad:  -0.153, New P: -1.197
iter 18 loss: 0.436
Actual params: [-0.5547, -1.1971]
-Original Grad: 0.002, -lr * Pred Grad:  -0.008, New P: -0.563
-Original Grad: 0.017, -lr * Pred Grad:  0.007, New P: -1.190
iter 19 loss: 0.436
Actual params: [-0.563 , -1.1898]
-Original Grad: -0.038, -lr * Pred Grad:  0.129, New P: -0.434
-Original Grad: -0.279, -lr * Pred Grad:  -0.119, New P: -1.309
iter 20 loss: 0.441
Actual params: [-0.4338, -1.3086]
-Original Grad: 0.002, -lr * Pred Grad:  -0.057, New P: -0.491
-Original Grad: 0.107, -lr * Pred Grad:  0.026, New P: -1.283
Target params: [1.1812, 0.2779]
iter 0 loss: 0.409
Actual params: [0.5941, 0.5941]
-Original Grad: 0.002, -lr * Pred Grad:  0.080, New P: 0.674
-Original Grad: -0.004, -lr * Pred Grad:  -0.140, New P: 0.454
iter 1 loss: 0.304
Actual params: [0.6742, 0.454 ]
-Original Grad: 0.604, -lr * Pred Grad:  0.100, New P: 0.775
-Original Grad: 0.051, -lr * Pred Grad:  0.010, New P: 0.464
iter 2 loss: 0.304
Actual params: [0.7745, 0.4639]
-Original Grad: -0.823, -lr * Pred Grad:  -0.145, New P: 0.629
-Original Grad: -0.041, -lr * Pred Grad:  0.679, New P: 1.143
iter 3 loss: 0.708
Actual params: [0.629 , 1.1428]
-Original Grad: 0.143, -lr * Pred Grad:  -0.053, New P: 0.576
-Original Grad: 0.268, -lr * Pred Grad:  0.768, New P: 1.911
iter 4 loss: 0.626
Actual params: [0.5756, 1.9105]
-Original Grad: -0.246, -lr * Pred Grad:  -0.044, New P: 0.532
-Original Grad: 0.011, -lr * Pred Grad:  0.108, New P: 2.019
iter 5 loss: 0.656
Actual params: [0.5317, 2.0186]
-Original Grad: 0.919, -lr * Pred Grad:  0.045, New P: 0.576
-Original Grad: 0.135, -lr * Pred Grad:  0.070, New P: 2.089
iter 6 loss: 0.631
Actual params: [0.5763, 2.0887]
-Original Grad: 2.293, -lr * Pred Grad:  0.067, New P: 0.644
-Original Grad: -0.030, -lr * Pred Grad:  -0.274, New P: 1.815
iter 7 loss: 0.576
Actual params: [0.6435, 1.8149]
-Original Grad: -0.179, -lr * Pred Grad:  -0.004, New P: 0.640
-Original Grad: -0.018, -lr * Pred Grad:  -0.028, New P: 1.787
iter 8 loss: 0.578
Actual params: [0.6398, 1.7868]
-Original Grad: -0.150, -lr * Pred Grad:  -0.000, New P: 0.640
-Original Grad: -0.046, -lr * Pred Grad:  -0.104, New P: 1.682
iter 9 loss: 0.580
Actual params: [0.6397, 1.6825]
-Original Grad: 0.162, -lr * Pred Grad:  -0.003, New P: 0.637
-Original Grad: 0.077, -lr * Pred Grad:  0.179, New P: 1.861
iter 10 loss: 0.583
Actual params: [0.6368, 1.8611]
-Original Grad: 0.001, -lr * Pred Grad:  -0.000, New P: 0.637
-Original Grad: 0.001, -lr * Pred Grad:  0.003, New P: 1.864
iter 11 loss: 0.583
Actual params: [0.6367, 1.864 ]
-Original Grad: -0.132, -lr * Pred Grad:  0.008, New P: 0.644
-Original Grad: -0.147, -lr * Pred Grad:  -0.226, New P: 1.638
iter 12 loss: 0.578
Actual params: [0.6444, 1.6377]
-Original Grad: -0.389, -lr * Pred Grad:  -0.019, New P: 0.625
-Original Grad: 0.002, -lr * Pred Grad:  0.043, New P: 1.680
iter 13 loss: 0.589
Actual params: [0.6251, 1.6802]
-Original Grad: 0.257, -lr * Pred Grad:  0.016, New P: 0.641
-Original Grad: -0.025, -lr * Pred Grad:  -0.076, New P: 1.604
iter 14 loss: 0.582
Actual params: [0.6412, 1.6044]
-Original Grad: 1.080, -lr * Pred Grad:  0.005, New P: 0.646
-Original Grad: 0.447, -lr * Pred Grad:  0.324, New P: 1.928
iter 15 loss: 0.577
Actual params: [0.646 , 1.9283]
-Original Grad: -0.004, -lr * Pred Grad:  0.000, New P: 0.646
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: 1.926
iter 16 loss: 0.577
Actual params: [0.6461, 1.9263]
-Original Grad: -1.147, -lr * Pred Grad:  -0.043, New P: 0.603
-Original Grad: 0.404, -lr * Pred Grad:  0.205, New P: 2.132
iter 17 loss: 0.616
Actual params: [0.6028, 2.1316]
-Original Grad: 0.136, -lr * Pred Grad:  0.006, New P: 0.609
-Original Grad: 0.022, -lr * Pred Grad:  0.013, New P: 2.144
iter 18 loss: 0.613
Actual params: [0.6085, 2.1443]
-Original Grad: -0.045, -lr * Pred Grad:  -0.002, New P: 0.606
-Original Grad: 0.005, -lr * Pred Grad:  0.003, New P: 2.147
iter 19 loss: 0.615
Actual params: [0.6064, 2.1475]
-Original Grad: -0.127, -lr * Pred Grad:  -0.006, New P: 0.601
-Original Grad: -0.003, -lr * Pred Grad:  -0.002, New P: 2.145
iter 20 loss: 0.618
Actual params: [0.6005, 2.1455]
-Original Grad: -0.133, -lr * Pred Grad:  -0.007, New P: 0.594
-Original Grad: 0.007, -lr * Pred Grad:  0.006, New P: 2.151
Target params: [1.1812, 0.2779]
iter 0 loss: 0.798
Actual params: [0.5941, 0.5941]
-Original Grad: -0.106, -lr * Pred Grad:  -0.058, New P: 0.536
-Original Grad: 0.013, -lr * Pred Grad:  0.007, New P: 0.601
iter 1 loss: 0.802
Actual params: [0.5359, 0.6012]
-Original Grad: -0.359, -lr * Pred Grad:  -0.067, New P: 0.469
-Original Grad: -0.435, -lr * Pred Grad:  -0.503, New P: 0.098
iter 2 loss: 0.734
Actual params: [0.469, 0.098]
-Original Grad: 0.038, -lr * Pred Grad:  -0.000, New P: 0.469
-Original Grad: 0.073, -lr * Pred Grad:  0.089, New P: 0.187
iter 3 loss: 0.738
Actual params: [0.4686, 0.1872]
-Original Grad: -0.139, -lr * Pred Grad:  -0.007, New P: 0.462
-Original Grad: -0.231, -lr * Pred Grad:  -0.202, New P: -0.015
iter 4 loss: 0.736
Actual params: [ 0.4618, -0.0148]
-Original Grad: -0.507, -lr * Pred Grad:  -0.200, New P: 0.261
-Original Grad: -0.098, -lr * Pred Grad:  0.052, New P: 0.037
iter 5 loss: 0.765
Actual params: [0.2614, 0.0369]
-Original Grad: -0.024, -lr * Pred Grad:  0.002, New P: 0.264
-Original Grad: -0.039, -lr * Pred Grad:  -0.041, New P: -0.004
iter 6 loss: 0.765
Actual params: [ 0.2635, -0.0038]
-Original Grad: 0.121, -lr * Pred Grad:  0.053, New P: 0.316
-Original Grad: 0.035, -lr * Pred Grad:  0.000, New P: -0.004
iter 7 loss: 0.758
Actual params: [ 0.3164, -0.0037]
-Original Grad: 0.167, -lr * Pred Grad:  0.083, New P: 0.399
-Original Grad: 0.035, -lr * Pred Grad:  -0.020, New P: -0.024
iter 8 loss: 0.746
Actual params: [ 0.3993, -0.0241]
-Original Grad: 0.218, -lr * Pred Grad:  0.086, New P: 0.485
-Original Grad: 0.076, -lr * Pred Grad:  0.018, New P: -0.006
iter 9 loss: 0.732
Actual params: [ 0.4849, -0.0058]
-Original Grad: -0.031, -lr * Pred Grad:  -0.008, New P: 0.477
-Original Grad: -0.000, -lr * Pred Grad:  0.007, New P: 0.001
iter 10 loss: 0.733
Actual params: [0.4774, 0.0009]
-Original Grad: -0.156, -lr * Pred Grad:  -0.019, New P: 0.459
-Original Grad: -0.087, -lr * Pred Grad:  -0.104, New P: -0.103
iter 11 loss: 0.739
Actual params: [ 0.4586, -0.103 ]
-Original Grad: -0.005, -lr * Pred Grad:  -0.015, New P: 0.444
-Original Grad: 0.063, -lr * Pred Grad:  0.088, New P: -0.015
iter 12 loss: 0.739
Actual params: [ 0.4435, -0.015 ]
-Original Grad: 0.519, -lr * Pred Grad:  0.119, New P: 0.563
-Original Grad: -0.004, -lr * Pred Grad:  -0.094, New P: -0.109
iter 13 loss: 0.714
Actual params: [ 0.5629, -0.1085]
-Original Grad: 0.864, -lr * Pred Grad:  0.123, New P: 0.686
-Original Grad: -0.048, -lr * Pred Grad:  -0.129, New P: -0.238
iter 14 loss: 0.681
Actual params: [ 0.6861, -0.2378]
-Original Grad: 1.918, -lr * Pred Grad:  0.086, New P: 0.772
-Original Grad: 0.144, -lr * Pred Grad:  0.030, New P: -0.208
iter 15 loss: 0.638
Actual params: [ 0.772, -0.208]
-Original Grad: -0.653, -lr * Pred Grad:  -0.022, New P: 0.750
-Original Grad: -0.111, -lr * Pred Grad:  -0.113, New P: -0.321
iter 16 loss: 0.658
Actual params: [ 0.7497, -0.321 ]
-Original Grad: 0.510, -lr * Pred Grad:  0.003, New P: 0.753
-Original Grad: 0.191, -lr * Pred Grad:  0.230, New P: -0.091
iter 17 loss: 0.635
Actual params: [ 0.7526, -0.0915]
-Original Grad: -0.207, -lr * Pred Grad:  0.016, New P: 0.769
-Original Grad: -0.243, -lr * Pred Grad:  -0.241, New P: -0.332
iter 18 loss: 0.653
Actual params: [ 0.7687, -0.3323]
-Original Grad: 0.022, -lr * Pred Grad:  -0.005, New P: 0.763
-Original Grad: 0.051, -lr * Pred Grad:  0.058, New P: -0.274
iter 19 loss: 0.650
Actual params: [ 0.7635, -0.2739]
-Original Grad: -0.013, -lr * Pred Grad:  -0.002, New P: 0.762
-Original Grad: 0.005, -lr * Pred Grad:  0.009, New P: -0.265
iter 20 loss: 0.650
Actual params: [ 0.7616, -0.2648]
-Original Grad: 0.736, -lr * Pred Grad:  0.033, New P: 0.795
-Original Grad: 0.109, -lr * Pred Grad:  0.026, New P: -0.238
Target params: [1.1812, 0.2779]
iter 0 loss: 0.910
Actual params: [0.5941, 0.5941]
-Original Grad: -0.075, -lr * Pred Grad:  -0.808, New P: -0.214
-Original Grad: -0.093, -lr * Pred Grad:  -1.004, New P: -0.410
iter 1 loss: 0.387
Actual params: [-0.2144, -0.4096]
-Original Grad: -0.008, -lr * Pred Grad:  -0.486, New P: -0.700
-Original Grad: 0.239, -lr * Pred Grad:  0.782, New P: 0.373
iter 2 loss: 0.652
Actual params: [-0.6999,  0.3728]
-Original Grad: 0.040, -lr * Pred Grad:  0.403, New P: -0.297
-Original Grad: 0.186, -lr * Pred Grad:  0.373, New P: 0.745
iter 3 loss: 0.989
Actual params: [-0.2974,  0.7454]
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: -0.289
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: 0.746
iter 4 loss: 0.990
Actual params: [-0.2893,  0.7463]
-Original Grad: 0.028, -lr * Pred Grad:  -0.044, New P: -0.333
-Original Grad: 0.401, -lr * Pred Grad:  0.101, New P: 0.847
iter 5 loss: 0.982
Actual params: [-0.3329,  0.8475]
-Original Grad: -0.108, -lr * Pred Grad:  -1.223, New P: -1.556
-Original Grad: 0.137, -lr * Pred Grad:  0.105, New P: 0.952
iter 6 loss: 0.555
Actual params: [-1.5555,  0.9522]
-Original Grad: -0.262, -lr * Pred Grad:  -0.394, New P: -1.950
-Original Grad: -0.014, -lr * Pred Grad:  0.021, New P: 0.974
iter 7 loss: 0.442
Actual params: [-1.9498,  0.9737]
-Original Grad: 0.013, -lr * Pred Grad:  0.016, New P: -1.934
-Original Grad: -0.003, -lr * Pred Grad:  -0.002, New P: 0.972
iter 8 loss: 0.444
Actual params: [-1.9337,  0.9719]
-Original Grad: -1.040, -lr * Pred Grad:  -0.190, New P: -2.123
-Original Grad: -0.014, -lr * Pred Grad:  0.008, New P: 0.980
iter 9 loss: 0.428
Actual params: [-2.1234,  0.9801]
-Original Grad: -0.706, -lr * Pred Grad:  -0.049, New P: -2.172
-Original Grad: 0.490, -lr * Pred Grad:  0.068, New P: 1.048
iter 10 loss: 0.420
Actual params: [-2.1724,  1.0479]
-Original Grad: 0.570, -lr * Pred Grad:  0.076, New P: -2.096
-Original Grad: 0.242, -lr * Pred Grad:  0.091, New P: 1.139
iter 11 loss: 0.416
Actual params: [-2.0962,  1.1388]
-Original Grad: 0.142, -lr * Pred Grad:  0.008, New P: -2.088
-Original Grad: -0.094, -lr * Pred Grad:  -0.012, New P: 1.127
iter 12 loss: 0.417
Actual params: [-2.088,  1.127]
-Original Grad: -0.333, -lr * Pred Grad:  -0.025, New P: -2.113
-Original Grad: 0.165, -lr * Pred Grad:  0.015, New P: 1.142
iter 13 loss: 0.415
Actual params: [-2.113,  1.142]
-Original Grad: 0.203, -lr * Pred Grad:  -0.003, New P: -2.116
-Original Grad: -0.315, -lr * Pred Grad:  -0.044, New P: 1.098
iter 14 loss: 0.418
Actual params: [-2.1156,  1.098 ]
-Original Grad: 0.379, -lr * Pred Grad:  0.020, New P: -2.096
-Original Grad: -0.329, -lr * Pred Grad:  -0.031, New P: 1.067
iter 15 loss: 0.421
Actual params: [-2.0957,  1.0668]
-Original Grad: 0.060, -lr * Pred Grad:  0.025, New P: -2.070
-Original Grad: 0.141, -lr * Pred Grad:  0.039, New P: 1.106
iter 16 loss: 0.420
Actual params: [-2.0704,  1.1063]
-Original Grad: 0.168, -lr * Pred Grad:  0.005, New P: -2.065
-Original Grad: -0.058, -lr * Pred Grad:  0.001, New P: 1.108
iter 17 loss: 0.420
Actual params: [-2.0653,  1.1077]
-Original Grad: 0.261, -lr * Pred Grad:  0.051, New P: -2.014
-Original Grad: 0.611, -lr * Pred Grad:  0.123, New P: 1.230
iter 18 loss: 0.417
Actual params: [-2.0139,  1.2305]
-Original Grad: -0.165, -lr * Pred Grad:  -0.011, New P: -2.025
-Original Grad: -0.024, -lr * Pred Grad:  -0.016, New P: 1.215
iter 19 loss: 0.417
Actual params: [-2.0249,  1.2148]
-Original Grad: -0.237, -lr * Pred Grad:  -0.022, New P: -2.047
-Original Grad: -0.112, -lr * Pred Grad:  -0.040, New P: 1.175
iter 20 loss: 0.416
Actual params: [-2.0473,  1.1749]
-Original Grad: 0.039, -lr * Pred Grad:  0.040, New P: -2.008
-Original Grad: 0.704, -lr * Pred Grad:  0.112, New P: 1.287
Target params: [1.1812, 0.2779]
iter 0 loss: 0.374
Actual params: [0.5941, 0.5941]
-Original Grad: -0.220, -lr * Pred Grad:  -0.085, New P: 0.509
-Original Grad: -0.302, -lr * Pred Grad:  -0.117, New P: 0.477
iter 1 loss: 0.380
Actual params: [0.5093, 0.4773]
-Original Grad: 0.057, -lr * Pred Grad:  0.609, New P: 1.118
-Original Grad: -0.438, -lr * Pred Grad:  -0.453, New P: 0.024
iter 2 loss: 0.410
Actual params: [1.1184, 0.024 ]
-Original Grad: -0.136, -lr * Pred Grad:  0.175, New P: 1.294
-Original Grad: -0.579, -lr * Pred Grad:  -0.232, New P: -0.208
iter 3 loss: 0.392
Actual params: [ 1.2936, -0.2077]
-Original Grad: -0.510, -lr * Pred Grad:  -0.382, New P: 0.912
-Original Grad: -0.346, -lr * Pred Grad:  0.096, New P: -0.112
iter 4 loss: 0.412
Actual params: [ 0.9118, -0.1118]
-Original Grad: 0.066, -lr * Pred Grad:  0.081, New P: 0.993
-Original Grad: -0.014, -lr * Pred Grad:  -0.045, New P: -0.157
iter 5 loss: 0.407
Actual params: [ 0.9933, -0.1568]
-Original Grad: -0.074, -lr * Pred Grad:  -0.047, New P: 0.946
-Original Grad: -0.072, -lr * Pred Grad:  -0.001, New P: -0.158
iter 6 loss: 0.409
Actual params: [ 0.9463, -0.1581]
-Original Grad: 0.070, -lr * Pred Grad:  0.102, New P: 1.048
-Original Grad: -0.040, -lr * Pred Grad:  -0.064, New P: -0.222
iter 7 loss: 0.401
Actual params: [ 1.0485, -0.2221]
-Original Grad: -0.057, -lr * Pred Grad:  -0.125, New P: 0.923
-Original Grad: 0.109, -lr * Pred Grad:  0.101, New P: -0.121
iter 8 loss: 0.412
Actual params: [ 0.9233, -0.121 ]
-Original Grad: -0.044, -lr * Pred Grad:  -0.120, New P: 0.803
-Original Grad: 0.127, -lr * Pred Grad:  0.106, New P: -0.015
iter 9 loss: 0.419
Actual params: [ 0.8034, -0.0148]
-Original Grad: 0.056, -lr * Pred Grad:  -0.067, New P: 0.737
-Original Grad: 0.272, -lr * Pred Grad:  0.138, New P: 0.123
iter 10 loss: 0.428
Actual params: [0.7367, 0.1229]
-Original Grad: 0.012, -lr * Pred Grad:  0.050, New P: 0.787
-Original Grad: -0.087, -lr * Pred Grad:  -0.049, New P: 0.074
iter 11 loss: 0.424
Actual params: [0.7869, 0.0741]
-Original Grad: 0.066, -lr * Pred Grad:  0.162, New P: 0.949
-Original Grad: -0.237, -lr * Pred Grad:  -0.132, New P: -0.058
iter 12 loss: 0.414
Actual params: [ 0.9493, -0.0577]
-Original Grad: 0.023, -lr * Pred Grad:  0.064, New P: 1.013
-Original Grad: -0.091, -lr * Pred Grad:  -0.054, New P: -0.111
iter 13 loss: 0.409
Actual params: [ 1.0135, -0.1115]
-Original Grad: 0.056, -lr * Pred Grad:  0.050, New P: 1.064
-Original Grad: 0.097, -lr * Pred Grad:  0.032, New P: -0.079
iter 14 loss: 0.409
Actual params: [ 1.0637, -0.0792]
-Original Grad: 0.190, -lr * Pred Grad:  0.184, New P: 1.248
-Original Grad: 0.239, -lr * Pred Grad:  0.053, New P: -0.027
iter 15 loss: 0.404
Actual params: [ 1.248 , -0.0266]
-Original Grad: -0.283, -lr * Pred Grad:  -0.167, New P: 1.081
-Original Grad: -0.310, -lr * Pred Grad:  -0.029, New P: -0.055
iter 16 loss: 0.409
Actual params: [ 1.0806, -0.0552]
-Original Grad: 0.052, -lr * Pred Grad:  0.106, New P: 1.187
-Original Grad: -0.058, -lr * Pred Grad:  -0.073, New P: -0.129
iter 17 loss: 0.402
Actual params: [ 1.1868, -0.1286]
-Original Grad: 0.036, -lr * Pred Grad:  0.014, New P: 1.201
-Original Grad: 0.056, -lr * Pred Grad:  0.017, New P: -0.112
iter 18 loss: 0.402
Actual params: [ 1.2006, -0.1121]
-Original Grad: 0.045, -lr * Pred Grad:  0.032, New P: 1.232
-Original Grad: 0.048, -lr * Pred Grad:  0.004, New P: -0.108
iter 19 loss: 0.401
Actual params: [ 1.2321, -0.108 ]
-Original Grad: 0.010, -lr * Pred Grad:  0.033, New P: 1.265
-Original Grad: -0.025, -lr * Pred Grad:  -0.027, New P: -0.135
iter 20 loss: 0.398
Actual params: [ 1.2651, -0.1348]
-Original Grad: -0.034, -lr * Pred Grad:  -0.002, New P: 1.263
-Original Grad: -0.070, -lr * Pred Grad:  -0.034, New P: -0.169
Target params: [1.1812, 0.2779]
iter 0 loss: 0.660
Actual params: [0.5941, 0.5941]
-Original Grad: -0.267, -lr * Pred Grad:  -0.395, New P: 0.199
-Original Grad: 0.301, -lr * Pred Grad:  0.445, New P: 1.039
iter 1 loss: 0.974
Actual params: [0.1992, 1.0395]
-Original Grad: -0.133, -lr * Pred Grad:  0.551, New P: 0.750
-Original Grad: 0.215, -lr * Pred Grad:  0.785, New P: 1.825
iter 2 loss: 0.898
Actual params: [0.75  , 1.8249]
-Original Grad: 0.011, -lr * Pred Grad:  0.059, New P: 0.809
-Original Grad: -0.012, -lr * Pred Grad:  0.021, New P: 1.846
iter 3 loss: 0.903
Actual params: [0.8093, 1.8463]
-Original Grad: -0.073, -lr * Pred Grad:  -0.480, New P: 0.329
-Original Grad: -0.072, -lr * Pred Grad:  -0.381, New P: 1.466
iter 4 loss: 0.962
Actual params: [0.329 , 1.4656]
-Original Grad: 0.302, -lr * Pred Grad:  0.364, New P: 0.693
-Original Grad: -0.233, -lr * Pred Grad:  -0.012, New P: 1.454
iter 5 loss: 0.857
Actual params: [0.6932, 1.454 ]
-Original Grad: -0.127, -lr * Pred Grad:  0.244, New P: 0.938
-Original Grad: 0.418, -lr * Pred Grad:  0.398, New P: 1.852
iter 6 loss: 0.931
Actual params: [0.9377, 1.852 ]
-Original Grad: 0.071, -lr * Pred Grad:  0.221, New P: 1.159
-Original Grad: 0.021, -lr * Pred Grad:  0.130, New P: 1.982
iter 7 loss: 1.066
Actual params: [1.1589, 1.982 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: 1.155
-Original Grad: 0.001, -lr * Pred Grad:  -0.002, New P: 1.980
iter 8 loss: 1.065
Actual params: [1.1548, 1.9803]
-Original Grad: -0.318, -lr * Pred Grad:  -0.385, New P: 0.770
-Original Grad: -0.069, -lr * Pred Grad:  -0.226, New P: 1.754
iter 9 loss: 0.881
Actual params: [0.7695, 1.754 ]
-Original Grad: -0.268, -lr * Pred Grad:  -0.199, New P: 0.570
-Original Grad: -0.266, -lr * Pred Grad:  -0.194, New P: 1.560
iter 10 loss: 0.894
Actual params: [0.5703, 1.5599]
-Original Grad: -0.033, -lr * Pred Grad:  -0.026, New P: 0.544
-Original Grad: -0.013, -lr * Pred Grad:  -0.011, New P: 1.549
iter 11 loss: 0.902
Actual params: [0.5441, 1.5486]
-Original Grad: 0.142, -lr * Pred Grad:  0.082, New P: 0.626
-Original Grad: -0.015, -lr * Pred Grad:  -0.005, New P: 1.543
iter 12 loss: 0.875
Actual params: [0.6264, 1.5434]
-Original Grad: 0.623, -lr * Pred Grad:  0.079, New P: 0.706
-Original Grad: -0.037, -lr * Pred Grad:  0.000, New P: 1.543
iter 13 loss: 0.861
Actual params: [0.7055, 1.5435]
-Original Grad: -0.036, -lr * Pred Grad:  -0.001, New P: 0.705
-Original Grad: 0.058, -lr * Pred Grad:  0.031, New P: 1.574
iter 14 loss: 0.865
Actual params: [0.7047, 1.5741]
-Original Grad: -0.048, -lr * Pred Grad:  -0.003, New P: 0.701
-Original Grad: 0.050, -lr * Pred Grad:  0.028, New P: 1.602
iter 15 loss: 0.867
Actual params: [0.7014, 1.6016]
-Original Grad: 0.171, -lr * Pred Grad:  0.036, New P: 0.738
-Original Grad: 0.089, -lr * Pred Grad:  0.074, New P: 1.676
iter 16 loss: 0.870
Actual params: [0.7377, 1.6755]
-Original Grad: 0.037, -lr * Pred Grad:  0.005, New P: 0.743
-Original Grad: -0.014, -lr * Pred Grad:  -0.007, New P: 1.669
iter 17 loss: 0.869
Actual params: [0.7428, 1.6688]
-Original Grad: 0.165, -lr * Pred Grad:  0.036, New P: 0.779
-Original Grad: 0.043, -lr * Pred Grad:  0.052, New P: 1.721
iter 18 loss: 0.874
Actual params: [0.7792, 1.7206]
-Original Grad: -0.084, -lr * Pred Grad:  -0.024, New P: 0.755
-Original Grad: -0.058, -lr * Pred Grad:  -0.059, New P: 1.662
iter 19 loss: 0.867
Actual params: [0.7554, 1.662 ]
-Original Grad: 0.157, -lr * Pred Grad:  0.040, New P: 0.796
-Original Grad: 0.069, -lr * Pred Grad:  0.076, New P: 1.738
iter 20 loss: 0.878
Actual params: [0.7959, 1.7384]
-Original Grad: -0.050, -lr * Pred Grad:  0.012, New P: 0.808
-Original Grad: 0.299, -lr * Pred Grad:  0.204, New P: 1.942
Target params: [1.1812, 0.2779]
iter 0 loss: 0.880
Actual params: [0.5941, 0.5941]
-Original Grad: 0.293, -lr * Pred Grad:  0.041, New P: 0.635
-Original Grad: 0.098, -lr * Pred Grad:  0.014, New P: 0.608
iter 1 loss: 0.873
Actual params: [0.6349, 0.6076]
-Original Grad: -0.218, -lr * Pred Grad:  -0.500, New P: 0.135
-Original Grad: 0.001, -lr * Pred Grad:  1.447, New P: 2.054
iter 2 loss: 0.717
Actual params: [0.1353, 2.0545]
-Original Grad: -0.018, -lr * Pred Grad:  -0.029, New P: 0.106
-Original Grad: -0.002, -lr * Pred Grad:  0.082, New P: 2.137
iter 3 loss: 0.717
Actual params: [0.106 , 2.1365]
-Original Grad: 0.027, -lr * Pred Grad:  0.054, New P: 0.160
-Original Grad: 0.001, -lr * Pred Grad:  -0.155, New P: 1.982
iter 4 loss: 0.717
Actual params: [0.1602, 1.9818]
-Original Grad: 0.062, -lr * Pred Grad:  0.042, New P: 0.202
-Original Grad: 0.016, -lr * Pred Grad:  -0.089, New P: 1.892
iter 5 loss: 0.714
Actual params: [0.202 , 1.8924]
-Original Grad: -0.139, -lr * Pred Grad:  -0.267, New P: -0.065
-Original Grad: -0.012, -lr * Pred Grad:  0.753, New P: 2.645
iter 6 loss: 0.719
Actual params: [-0.0654,  2.645 ]
-Original Grad: -0.082, -lr * Pred Grad:  -0.193, New P: -0.258
-Original Grad: 0.003, -lr * Pred Grad:  0.567, New P: 3.212
iter 7 loss: 0.704
Actual params: [-0.2579,  3.212 ]
-Original Grad: 0.091, -lr * Pred Grad:  0.101, New P: -0.157
-Original Grad: 0.015, -lr * Pred Grad:  -0.255, New P: 2.957
iter 8 loss: 0.717
Actual params: [-0.1568,  2.957 ]
-Original Grad: -0.066, -lr * Pred Grad:  -0.232, New P: -0.388
-Original Grad: 0.075, -lr * Pred Grad:  0.758, New P: 3.715
iter 9 loss: 0.681
Actual params: [-0.3884,  3.7145]
-Original Grad: 0.014, -lr * Pred Grad:  0.134, New P: -0.255
-Original Grad: -0.084, -lr * Pred Grad:  -0.469, New P: 3.246
iter 10 loss: 0.703
Actual params: [-0.2547,  3.2456]
-Original Grad: 0.003, -lr * Pred Grad:  -0.024, New P: -0.279
-Original Grad: 0.024, -lr * Pred Grad:  0.090, New P: 3.336
iter 11 loss: 0.700
Actual params: [-0.2788,  3.3355]
-Original Grad: -0.012, -lr * Pred Grad:  -0.009, New P: -0.288
-Original Grad: 0.003, -lr * Pred Grad:  0.020, New P: 3.355
iter 12 loss: 0.700
Actual params: [-0.2882,  3.3554]
-Original Grad: 0.009, -lr * Pred Grad:  0.001, New P: -0.287
-Original Grad: 0.005, -lr * Pred Grad:  0.012, New P: 3.367
iter 13 loss: 0.700
Actual params: [-0.2872,  3.3669]
-Original Grad: -0.013, -lr * Pred Grad:  -0.106, New P: -0.393
-Original Grad: 0.137, -lr * Pred Grad:  0.414, New P: 3.781
iter 14 loss: 0.680
Actual params: [-0.3933,  3.7809]
-Original Grad: 0.167, -lr * Pred Grad:  0.083, New P: -0.310
-Original Grad: 0.039, -lr * Pred Grad:  -0.005, New P: 3.776
iter 15 loss: 0.685
Actual params: [-0.3105,  3.7757]
-Original Grad: -0.086, -lr * Pred Grad:  0.007, New P: -0.303
-Original Grad: -0.082, -lr * Pred Grad:  -0.209, New P: 3.566
iter 16 loss: 0.693
Actual params: [-0.3034,  3.5665]
-Original Grad: 0.039, -lr * Pred Grad:  0.090, New P: -0.214
-Original Grad: -0.076, -lr * Pred Grad:  -0.277, New P: 3.289
iter 17 loss: 0.704
Actual params: [-0.2136,  3.2894]
-Original Grad: 0.044, -lr * Pred Grad:  -0.002, New P: -0.216
-Original Grad: 0.047, -lr * Pred Grad:  0.128, New P: 3.418
iter 18 loss: 0.699
Actual params: [-0.2161,  3.4175]
-Original Grad: 0.060, -lr * Pred Grad:  0.067, New P: -0.149
-Original Grad: -0.013, -lr * Pred Grad:  -0.106, New P: 3.312
iter 19 loss: 0.706
Actual params: [-0.1486,  3.312 ]
-Original Grad: -0.004, -lr * Pred Grad:  -0.022, New P: -0.171
-Original Grad: 0.018, -lr * Pred Grad:  0.080, New P: 3.392
iter 20 loss: 0.701
Actual params: [-0.1708,  3.392 ]
-Original Grad: 0.186, -lr * Pred Grad:  0.122, New P: -0.049
-Original Grad: 0.066, -lr * Pred Grad:  0.092, New P: 3.484
Target params: [1.1812, 0.2779]
iter 0 loss: 0.776
Actual params: [0.5941, 0.5941]
-Original Grad: -0.057, -lr * Pred Grad:  -0.467, New P: 0.127
-Original Grad: -0.042, -lr * Pred Grad:  -0.347, New P: 0.247
iter 1 loss: 0.796
Actual params: [0.1267, 0.2472]
-Original Grad: 0.048, -lr * Pred Grad:  0.433, New P: 0.559
-Original Grad: 0.022, -lr * Pred Grad:  -0.050, New P: 0.197
iter 2 loss: 0.734
Actual params: [0.5592, 0.1971]
-Original Grad: 0.356, -lr * Pred Grad:  0.268, New P: 0.827
-Original Grad: -0.008, -lr * Pred Grad:  -0.250, New P: -0.053
iter 3 loss: 0.527
Actual params: [ 0.8272, -0.053 ]
-Original Grad: 1.423, -lr * Pred Grad:  0.070, New P: 0.897
-Original Grad: 0.329, -lr * Pred Grad:  0.388, New P: 0.335
iter 4 loss: 0.463
Actual params: [0.8969, 0.3345]
-Original Grad: -0.471, -lr * Pred Grad:  -0.158, New P: 0.739
-Original Grad: 0.007, -lr * Pred Grad:  0.629, New P: 0.964
iter 5 loss: 1.955
Actual params: [0.7385, 0.9636]
-Original Grad: -0.924, -lr * Pred Grad:  -0.125, New P: 0.614
-Original Grad: -0.087, -lr * Pred Grad:  0.353, New P: 1.317
iter 6 loss: 1.897
Actual params: [0.6137, 1.3166]
-Original Grad: 1.453, -lr * Pred Grad:  0.019, New P: 0.633
-Original Grad: -1.659, -lr * Pred Grad:  -0.121, New P: 1.196
iter 7 loss: 1.915
Actual params: [0.6329, 1.1959]
-Original Grad: -0.583, -lr * Pred Grad:  -0.037, New P: 0.596
-Original Grad: -0.014, -lr * Pred Grad:  -0.027, New P: 1.169
iter 8 loss: 1.886
Actual params: [0.5964, 1.169 ]
-Original Grad: -0.249, -lr * Pred Grad:  -0.017, New P: 0.579
-Original Grad: -0.020, -lr * Pred Grad:  -0.014, New P: 1.155
iter 9 loss: 1.872
Actual params: [0.5792, 1.1551]
-Original Grad: -0.265, -lr * Pred Grad:  -0.022, New P: 0.558
-Original Grad: -0.085, -lr * Pred Grad:  -0.024, New P: 1.131
iter 10 loss: 1.852
Actual params: [0.5575, 1.1313]
-Original Grad: 1.349, -lr * Pred Grad:  0.054, New P: 0.611
-Original Grad: 0.035, -lr * Pred Grad:  0.038, New P: 1.169
iter 11 loss: 1.895
Actual params: [0.6112, 1.1691]
-Original Grad: 0.356, -lr * Pred Grad:  0.014, New P: 0.626
-Original Grad: -0.013, -lr * Pred Grad:  0.007, New P: 1.176
iter 12 loss: 1.908
Actual params: [0.6255, 1.1764]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 0.626
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 1.177
iter 13 loss: 1.908
Actual params: [0.6256, 1.1766]
-Original Grad: -0.004, -lr * Pred Grad:  -0.000, New P: 0.625
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 1.176
iter 14 loss: 1.908
Actual params: [0.6254, 1.1765]
-Original Grad: 0.284, -lr * Pred Grad:  0.014, New P: 0.640
-Original Grad: 0.027, -lr * Pred Grad:  0.013, New P: 1.190
iter 15 loss: 1.921
Actual params: [0.6396, 1.1897]
-Original Grad: -0.581, -lr * Pred Grad:  -0.027, New P: 0.613
-Original Grad: -0.015, -lr * Pred Grad:  -0.018, New P: 1.172
iter 16 loss: 1.897
Actual params: [0.6129, 1.1717]
-Original Grad: -0.699, -lr * Pred Grad:  -0.034, New P: 0.579
-Original Grad: -0.181, -lr * Pred Grad:  -0.054, New P: 1.118
iter 17 loss: 1.854
Actual params: [0.579 , 1.1178]
-Original Grad: -0.299, -lr * Pred Grad:  -0.015, New P: 0.564
-Original Grad: -0.022, -lr * Pred Grad:  -0.011, New P: 1.107
iter 18 loss: 1.839
Actual params: [0.5645, 1.1067]
-Original Grad: 0.186, -lr * Pred Grad:  0.009, New P: 0.574
-Original Grad: -0.017, -lr * Pred Grad:  -0.001, New P: 1.106
iter 19 loss: 1.844
Actual params: [0.5738, 1.1059]
-Original Grad: 0.679, -lr * Pred Grad:  0.036, New P: 0.609
-Original Grad: 0.015, -lr * Pred Grad:  0.018, New P: 1.124
iter 20 loss: 1.884
Actual params: [0.6094, 1.1243]
-Original Grad: -0.439, -lr * Pred Grad:  -0.020, New P: 0.589
-Original Grad: -0.049, -lr * Pred Grad:  -0.021, New P: 1.104
Target params: [1.1812, 0.2779]
iter 0 loss: 0.599
Actual params: [0.5941, 0.5941]
-Original Grad: 0.208, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.067, -lr * Pred Grad:  -0.032, New P: 0.562
iter 1 loss: 0.537
Actual params: [0.6938, 0.5621]
-Original Grad: -0.033, -lr * Pred Grad:  0.130, New P: 0.824
-Original Grad: 0.036, -lr * Pred Grad:  0.440, New P: 1.002
iter 2 loss: 0.784
Actual params: [0.8236, 1.0022]
-Original Grad: -0.113, -lr * Pred Grad:  0.198, New P: 1.022
-Original Grad: 0.085, -lr * Pred Grad:  0.763, New P: 1.765
iter 3 loss: 1.145
Actual params: [1.0216, 1.7653]
-Original Grad: -0.423, -lr * Pred Grad:  -0.100, New P: 0.921
-Original Grad: 0.018, -lr * Pred Grad:  -0.214, New P: 1.552
iter 4 loss: 1.055
Actual params: [0.9212, 1.5516]
-Original Grad: -1.486, -lr * Pred Grad:  0.003, New P: 0.924
-Original Grad: 0.539, -lr * Pred Grad:  0.359, New P: 1.911
iter 5 loss: 1.141
Actual params: [0.9242, 1.9108]
-Original Grad: -0.541, -lr * Pred Grad:  -0.135, New P: 0.790
-Original Grad: -0.044, -lr * Pred Grad:  -0.393, New P: 1.518
iter 6 loss: 1.025
Actual params: [0.7896, 1.518 ]
-Original Grad: -0.006, -lr * Pred Grad:  0.049, New P: 0.839
-Original Grad: 0.184, -lr * Pred Grad:  0.198, New P: 1.716
iter 7 loss: 1.068
Actual params: [0.8389, 1.7159]
-Original Grad: -0.117, -lr * Pred Grad:  -0.025, New P: 0.814
-Original Grad: -0.074, -lr * Pred Grad:  -0.077, New P: 1.639
iter 8 loss: 1.073
Actual params: [0.8137, 1.6393]
-Original Grad: -0.901, -lr * Pred Grad:  -0.008, New P: 0.806
-Original Grad: 0.147, -lr * Pred Grad:  -0.003, New P: 1.636
iter 9 loss: 1.052
Actual params: [0.8061, 1.6361]
-Original Grad: 0.883, -lr * Pred Grad:  -0.023, New P: 0.783
-Original Grad: -0.405, -lr * Pred Grad:  -0.170, New P: 1.466
iter 10 loss: 0.687
Actual params: [0.7831, 1.4661]
-Original Grad: -0.090, -lr * Pred Grad:  0.006, New P: 0.789
-Original Grad: 0.094, -lr * Pred Grad:  0.037, New P: 1.503
iter 11 loss: 1.027
Actual params: [0.7891, 1.5028]
-Original Grad: -4.437, -lr * Pred Grad:  -0.022, New P: 0.767
-Original Grad: 0.869, -lr * Pred Grad:  0.011, New P: 1.514
iter 12 loss: 0.989
Actual params: [0.767 , 1.5138]
-Original Grad: 1.152, -lr * Pred Grad:  0.031, New P: 0.798
-Original Grad: 0.092, -lr * Pred Grad:  0.136, New P: 1.650
iter 13 loss: 1.047
Actual params: [0.7982, 1.6498]
-Original Grad: 0.065, -lr * Pred Grad:  0.016, New P: 0.815
-Original Grad: 0.177, -lr * Pred Grad:  0.089, New P: 1.738
iter 14 loss: 1.061
Actual params: [0.8145, 1.7383]
-Original Grad: 0.953, -lr * Pred Grad:  -0.016, New P: 0.798
-Original Grad: -0.489, -lr * Pred Grad:  -0.116, New P: 1.622
iter 15 loss: 1.046
Actual params: [0.7984, 1.622 ]
-Original Grad: 0.982, -lr * Pred Grad:  -0.006, New P: 0.793
-Original Grad: -0.357, -lr * Pred Grad:  -0.065, New P: 1.557
iter 16 loss: 1.035
Actual params: [0.7927, 1.5567]
-Original Grad: -0.366, -lr * Pred Grad:  -0.004, New P: 0.789
-Original Grad: -0.039, -lr * Pred Grad:  -0.014, New P: 1.543
iter 17 loss: 1.037
Actual params: [0.789 , 1.5427]
-Original Grad: 1.030, -lr * Pred Grad:  0.008, New P: 0.797
-Original Grad: 0.099, -lr * Pred Grad:  0.029, New P: 1.572
iter 18 loss: 1.040
Actual params: [0.7968, 1.5719]
-Original Grad: -0.068, -lr * Pred Grad:  -0.000, New P: 0.797
-Original Grad: 0.034, -lr * Pred Grad:  0.005, New P: 1.577
iter 19 loss: 1.041
Actual params: [0.7968, 1.5767]
-Original Grad: -0.192, -lr * Pred Grad:  -0.002, New P: 0.795
-Original Grad: -0.020, -lr * Pred Grad:  -0.007, New P: 1.570
iter 20 loss: 1.039
Actual params: [0.795, 1.57 ]
-Original Grad: 0.526, -lr * Pred Grad:  0.005, New P: 0.800
-Original Grad: 0.092, -lr * Pred Grad:  0.025, New P: 1.595
Target params: [1.1812, 0.2779]
iter 0 loss: 0.511
Actual params: [0.5941, 0.5941]
-Original Grad: 0.044, -lr * Pred Grad:  0.214, New P: 0.808
-Original Grad: -0.163, -lr * Pred Grad:  -0.789, New P: -0.195
iter 1 loss: 0.338
Actual params: [ 0.808 , -0.1952]
-Original Grad: 0.066, -lr * Pred Grad:  0.899, New P: 1.707
-Original Grad: -0.044, -lr * Pred Grad:  0.100, New P: -0.095
iter 2 loss: 0.360
Actual params: [ 1.7069, -0.0951]
-Original Grad: -0.020, -lr * Pred Grad:  -0.456, New P: 1.251
-Original Grad: -0.070, -lr * Pred Grad:  -0.369, New P: -0.464
iter 3 loss: 0.336
Actual params: [ 1.2506, -0.4637]
-Original Grad: -0.049, -lr * Pred Grad:  -0.382, New P: 0.869
-Original Grad: -0.212, -lr * Pred Grad:  -0.340, New P: -0.804
iter 4 loss: 0.325
Actual params: [ 0.8686, -0.8037]
-Original Grad: -0.028, -lr * Pred Grad:  -0.396, New P: 0.472
-Original Grad: 0.106, -lr * Pred Grad:  0.205, New P: -0.598
iter 5 loss: 0.350
Actual params: [ 0.4722, -0.5984]
-Original Grad: 0.126, -lr * Pred Grad:  0.652, New P: 1.124
-Original Grad: -0.071, -lr * Pred Grad:  -0.082, New P: -0.681
iter 6 loss: 0.322
Actual params: [ 1.1238, -0.6806]
-Original Grad: 0.167, -lr * Pred Grad:  0.533, New P: 1.657
-Original Grad: 0.125, -lr * Pred Grad:  0.169, New P: -0.512
iter 7 loss: 0.342
Actual params: [ 1.6566, -0.5121]
-Original Grad: 0.007, -lr * Pred Grad:  0.001, New P: 1.658
-Original Grad: 0.063, -lr * Pred Grad:  0.077, New P: -0.435
iter 8 loss: 0.346
Actual params: [ 1.6578, -0.4354]
-Original Grad: 0.009, -lr * Pred Grad:  0.001, New P: 1.659
-Original Grad: 0.083, -lr * Pred Grad:  0.075, New P: -0.361
iter 9 loss: 0.350
Actual params: [ 1.659 , -0.3608]
-Original Grad: -0.002, -lr * Pred Grad:  -0.007, New P: 1.652
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: -0.362
iter 10 loss: 0.350
Actual params: [ 1.6522, -0.3619]
-Original Grad: 0.014, -lr * Pred Grad:  0.053, New P: 1.705
-Original Grad: 0.014, -lr * Pred Grad:  0.006, New P: -0.356
iter 11 loss: 0.351
Actual params: [ 1.7052, -0.3562]
-Original Grad: -0.044, -lr * Pred Grad:  -0.011, New P: 1.694
-Original Grad: -0.244, -lr * Pred Grad:  -0.161, New P: -0.517
iter 12 loss: 0.342
Actual params: [ 1.6945, -0.5171]
-Original Grad: 0.016, -lr * Pred Grad:  0.089, New P: 1.784
-Original Grad: -0.006, -lr * Pred Grad:  -0.019, New P: -0.536
iter 13 loss: 0.342
Actual params: [ 1.7838, -0.5363]
-Original Grad: 0.016, -lr * Pred Grad:  0.094, New P: 1.878
-Original Grad: -0.005, -lr * Pred Grad:  -0.020, New P: -0.556
iter 14 loss: 0.343
Actual params: [ 1.8783, -0.5565]
-Original Grad: -0.195, -lr * Pred Grad:  -0.416, New P: 1.462
-Original Grad: -0.376, -lr * Pred Grad:  -0.107, New P: -0.663
iter 15 loss: 0.334
Actual params: [ 1.4621, -0.6633]
-Original Grad: 0.040, -lr * Pred Grad:  0.036, New P: 1.498
-Original Grad: 0.114, -lr * Pred Grad:  0.061, New P: -0.602
iter 16 loss: 0.339
Actual params: [ 1.4976, -0.6022]
-Original Grad: 0.009, -lr * Pred Grad:  0.030, New P: 1.528
-Original Grad: 0.009, -lr * Pred Grad:  -0.003, New P: -0.606
iter 17 loss: 0.339
Actual params: [ 1.5281, -0.6056]
-Original Grad: 0.146, -lr * Pred Grad:  0.522, New P: 2.051
-Original Grad: 0.040, -lr * Pred Grad:  -0.143, New P: -0.749
iter 18 loss: 0.335
Actual params: [ 2.0506, -0.7491]
-Original Grad: -0.011, -lr * Pred Grad:  -0.155, New P: 1.895
-Original Grad: 0.116, -lr * Pred Grad:  0.121, New P: -0.629
iter 19 loss: 0.340
Actual params: [ 1.8954, -0.6286]
-Original Grad: 0.003, -lr * Pred Grad:  0.117, New P: 2.013
-Original Grad: -0.146, -lr * Pred Grad:  -0.107, New P: -0.736
iter 20 loss: 0.335
Actual params: [ 2.0126, -0.7358]
-Original Grad: 0.012, -lr * Pred Grad:  -0.059, New P: 1.954
-Original Grad: 0.137, -lr * Pred Grad:  0.095, New P: -0.641
Target params: [1.1812, 0.2779]
iter 0 loss: 0.674
Actual params: [0.5941, 0.5941]
-Original Grad: -0.081, -lr * Pred Grad:  -0.063, New P: 0.531
-Original Grad: 0.006, -lr * Pred Grad:  0.005, New P: 0.599
iter 1 loss: 0.745
Actual params: [0.5312, 0.5989]
-Original Grad: 0.003, -lr * Pred Grad:  -0.008, New P: 0.523
-Original Grad: -0.006, -lr * Pred Grad:  -0.126, New P: 0.473
iter 2 loss: 0.742
Actual params: [0.523 , 0.4729]
-Original Grad: 0.183, -lr * Pred Grad:  0.168, New P: 0.691
-Original Grad: 0.077, -lr * Pred Grad:  1.007, New P: 1.480
iter 3 loss: 0.461
Actual params: [0.6914, 1.4803]
-Original Grad: -0.062, -lr * Pred Grad:  -0.056, New P: 0.635
-Original Grad: 0.002, -lr * Pred Grad:  0.008, New P: 1.488
iter 4 loss: 0.503
Actual params: [0.6353, 1.4882]
-Original Grad: 0.295, -lr * Pred Grad:  0.092, New P: 0.728
-Original Grad: -0.146, -lr * Pred Grad:  -0.777, New P: 0.711
iter 5 loss: 0.555
Actual params: [0.7277, 0.7114]
-Original Grad: -0.178, -lr * Pred Grad:  -0.179, New P: 0.549
-Original Grad: -0.030, -lr * Pred Grad:  -0.406, New P: 0.306
iter 6 loss: 0.720
Actual params: [0.549 , 0.3058]
-Original Grad: 0.330, -lr * Pred Grad:  0.218, New P: 0.767
-Original Grad: 0.061, -lr * Pred Grad:  0.511, New P: 0.816
iter 7 loss: 0.473
Actual params: [0.7672, 0.8164]
-Original Grad: -0.650, -lr * Pred Grad:  -0.093, New P: 0.675
-Original Grad: 0.216, -lr * Pred Grad:  0.359, New P: 1.175
iter 8 loss: 0.462
Actual params: [0.6747, 1.1754]
-Original Grad: 0.096, -lr * Pred Grad:  -0.010, New P: 0.665
-Original Grad: -0.143, -lr * Pred Grad:  -0.056, New P: 1.120
iter 9 loss: 0.478
Actual params: [0.6649, 1.1197]
-Original Grad: -0.636, -lr * Pred Grad:  -0.145, New P: 0.520
-Original Grad: 0.077, -lr * Pred Grad:  -0.105, New P: 1.014
iter 10 loss: 0.707
Actual params: [0.5203, 1.0144]
-Original Grad: 0.922, -lr * Pred Grad:  0.129, New P: 0.649
-Original Grad: -0.133, -lr * Pred Grad:  0.088, New P: 1.103
iter 11 loss: 0.500
Actual params: [0.6495, 1.1028]
-Original Grad: -0.006, -lr * Pred Grad:  -0.002, New P: 0.648
-Original Grad: -0.005, -lr * Pred Grad:  -0.004, New P: 1.099
iter 12 loss: 0.503
Actual params: [0.6478, 1.0993]
-Original Grad: -0.081, -lr * Pred Grad:  -0.023, New P: 0.624
-Original Grad: -0.054, -lr * Pred Grad:  -0.046, New P: 1.053
iter 13 loss: 0.543
Actual params: [0.6245, 1.0528]
-Original Grad: 0.014, -lr * Pred Grad:  0.001, New P: 0.626
-Original Grad: -0.003, -lr * Pred Grad:  0.000, New P: 1.053
iter 14 loss: 0.541
Actual params: [0.6258, 1.0531]
-Original Grad: -0.373, -lr * Pred Grad:  -0.047, New P: 0.578
-Original Grad: -0.016, -lr * Pred Grad:  -0.067, New P: 0.986
iter 15 loss: 0.632
Actual params: [0.5784, 0.9858]
-Original Grad: -0.056, -lr * Pred Grad:  -0.019, New P: 0.560
-Original Grad: -0.084, -lr * Pred Grad:  -0.064, New P: 0.922
iter 16 loss: 0.677
Actual params: [0.5597, 0.9218]
-Original Grad: 1.254, -lr * Pred Grad:  0.074, New P: 0.634
-Original Grad: -0.003, -lr * Pred Grad:  0.080, New P: 1.002
iter 17 loss: 0.544
Actual params: [0.6337, 1.0022]
-Original Grad: 0.006, -lr * Pred Grad:  -0.001, New P: 0.633
-Original Grad: -0.015, -lr * Pred Grad:  -0.010, New P: 0.992
iter 18 loss: 0.548
Actual params: [0.633 , 0.9919]
-Original Grad: -0.419, -lr * Pred Grad:  -0.024, New P: 0.609
-Original Grad: 0.029, -lr * Pred Grad:  -0.009, New P: 0.983
iter 19 loss: 0.586
Actual params: [0.609, 0.983]
-Original Grad: 0.019, -lr * Pred Grad:  0.002, New P: 0.611
-Original Grad: 0.050, -lr * Pred Grad:  0.018, New P: 1.001
iter 20 loss: 0.577
Actual params: [0.6112, 1.0009]
-Original Grad: -0.408, -lr * Pred Grad:  -0.025, New P: 0.586
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: 0.993
Target params: [1.1812, 0.2779]
iter 0 loss: 0.556
Actual params: [0.5941, 0.5941]
-Original Grad: -0.079, -lr * Pred Grad:  -0.141, New P: 0.453
-Original Grad: -0.168, -lr * Pred Grad:  -0.301, New P: 0.293
iter 1 loss: 0.464
Actual params: [0.4526, 0.2928]
-Original Grad: -0.053, -lr * Pred Grad:  -0.413, New P: 0.039
-Original Grad: 0.297, -lr * Pred Grad:  0.231, New P: 0.524
iter 2 loss: 0.732
Actual params: [0.0394, 0.5241]
-Original Grad: -0.175, -lr * Pred Grad:  -0.349, New P: -0.310
-Original Grad: 0.141, -lr * Pred Grad:  0.035, New P: 0.559
iter 3 loss: 0.761
Actual params: [-0.3099,  0.559 ]
-Original Grad: 0.012, -lr * Pred Grad:  0.045, New P: -0.265
-Original Grad: 0.031, -lr * Pred Grad:  0.032, New P: 0.591
iter 4 loss: 0.772
Actual params: [-0.265,  0.591]
-Original Grad: 0.096, -lr * Pred Grad:  0.174, New P: -0.091
-Original Grad: 0.207, -lr * Pred Grad:  0.112, New P: 0.703
iter 5 loss: 0.809
Actual params: [-0.0913,  0.7033]
-Original Grad: 0.085, -lr * Pred Grad:  0.069, New P: -0.023
-Original Grad: 0.307, -lr * Pred Grad:  0.081, New P: 0.784
iter 6 loss: 0.827
Actual params: [-0.0226,  0.7841]
-Original Grad: -1.206, -lr * Pred Grad:  -0.154, New P: -0.176
-Original Grad: 1.132, -lr * Pred Grad:  0.044, New P: 0.828
iter 7 loss: 0.829
Actual params: [-0.1764,  0.8282]
-Original Grad: 0.108, -lr * Pred Grad:  0.065, New P: -0.112
-Original Grad: 0.660, -lr * Pred Grad:  0.074, New P: 0.902
iter 8 loss: 0.834
Actual params: [-0.1115,  0.9019]
-Original Grad: -0.044, -lr * Pred Grad:  -0.008, New P: -0.120
-Original Grad: 0.030, -lr * Pred Grad:  0.000, New P: 0.902
iter 9 loss: 0.834
Actual params: [-0.1197,  0.9021]
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: -0.120
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.902
iter 10 loss: 0.834
Actual params: [-0.1197,  0.9021]
-Original Grad: 0.024, -lr * Pred Grad:  0.005, New P: -0.114
-Original Grad: -0.016, -lr * Pred Grad:  0.000, New P: 0.902
iter 11 loss: 0.834
Actual params: [-0.1142,  0.9024]
-Original Grad: 0.095, -lr * Pred Grad:  0.060, New P: -0.054
-Original Grad: 0.611, -lr * Pred Grad:  0.058, New P: 0.961
iter 12 loss: 0.836
Actual params: [-0.0539,  0.9606]
-Original Grad: -0.090, -lr * Pred Grad:  -0.027, New P: -0.080
-Original Grad: 0.009, -lr * Pred Grad:  -0.005, New P: 0.956
iter 13 loss: 0.834
Actual params: [-0.0805,  0.956 ]
-Original Grad: -0.022, -lr * Pred Grad:  -0.008, New P: -0.088
-Original Grad: -0.006, -lr * Pred Grad:  -0.002, New P: 0.954
iter 14 loss: 0.834
Actual params: [-0.0881,  0.9539]
-Original Grad: 0.133, -lr * Pred Grad:  0.039, New P: -0.049
-Original Grad: -0.123, -lr * Pred Grad:  -0.005, New P: 0.949
iter 15 loss: 0.836
Actual params: [-0.0492,  0.9493]
-Original Grad: 0.010, -lr * Pred Grad:  0.002, New P: -0.047
-Original Grad: -0.019, -lr * Pred Grad:  -0.001, New P: 0.948
iter 16 loss: 0.837
Actual params: [-0.0473,  0.9481]
-Original Grad: 0.240, -lr * Pred Grad:  0.096, New P: 0.049
-Original Grad: 0.070, -lr * Pred Grad:  0.032, New P: 0.980
iter 17 loss: 0.837
Actual params: [0.0489, 0.9803]
-Original Grad: 0.393, -lr * Pred Grad:  0.093, New P: 0.142
-Original Grad: -0.439, -lr * Pred Grad:  -0.016, New P: 0.964
iter 18 loss: 0.831
Actual params: [0.1422, 0.9639]
-Original Grad: -0.011, -lr * Pred Grad:  0.001, New P: 0.144
-Original Grad: 0.046, -lr * Pred Grad:  0.006, New P: 0.970
iter 19 loss: 0.831
Actual params: [0.1437, 0.9696]
-Original Grad: -2.016, -lr * Pred Grad:  -0.045, New P: 0.098
-Original Grad: 3.546, -lr * Pred Grad:  0.036, New P: 1.006
iter 20 loss: 0.832
Actual params: [0.0984, 1.0061]
-Original Grad: -0.069, -lr * Pred Grad:  -0.023, New P: 0.076
-Original Grad: 0.028, -lr * Pred Grad:  -0.012, New P: 0.994
Target params: [1.1812, 0.2779]
iter 0 loss: 0.399
Actual params: [0.5941, 0.5941]
-Original Grad: -0.130, -lr * Pred Grad:  -1.043, New P: -0.449
-Original Grad: 0.082, -lr * Pred Grad:  0.658, New P: 1.252
iter 1 loss: 0.341
Actual params: [-0.4486,  1.2519]
-Original Grad: -0.111, -lr * Pred Grad:  -0.520, New P: -0.969
-Original Grad: 0.077, -lr * Pred Grad:  0.488, New P: 1.740
iter 2 loss: 0.323
Actual params: [-0.9688,  1.74  ]
-Original Grad: -0.073, -lr * Pred Grad:  -0.685, New P: -1.654
-Original Grad: -0.015, -lr * Pred Grad:  -0.788, New P: 0.952
iter 3 loss: 0.369
Actual params: [-1.6542,  0.9522]
-Original Grad: 0.080, -lr * Pred Grad:  0.266, New P: -1.388
-Original Grad: -0.003, -lr * Pred Grad:  0.206, New P: 1.158
iter 4 loss: 0.350
Actual params: [-1.3879,  1.1583]
-Original Grad: 0.088, -lr * Pred Grad:  0.207, New P: -1.181
-Original Grad: 0.006, -lr * Pred Grad:  0.262, New P: 1.420
iter 5 loss: 0.351
Actual params: [-1.1811,  1.4203]
-Original Grad: 0.084, -lr * Pred Grad:  0.074, New P: -1.107
-Original Grad: -0.055, -lr * Pred Grad:  -0.645, New P: 0.775
iter 6 loss: 0.291
Actual params: [-1.1066,  0.7753]
-Original Grad: 0.133, -lr * Pred Grad:  0.091, New P: -1.016
-Original Grad: 0.453, -lr * Pred Grad:  0.477, New P: 1.252
iter 7 loss: 0.297
Actual params: [-1.0161,  1.2521]
-Original Grad: 0.209, -lr * Pred Grad:  0.324, New P: -0.692
-Original Grad: 0.084, -lr * Pred Grad:  0.010, New P: 1.262
iter 8 loss: 0.314
Actual params: [-0.6918,  1.2617]
-Original Grad: 0.001, -lr * Pred Grad:  0.003, New P: -0.689
-Original Grad: -0.003, -lr * Pred Grad:  -0.005, New P: 1.256
iter 9 loss: 0.315
Actual params: [-0.6886,  1.2564]
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: -0.692
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 1.258
iter 10 loss: 0.314
Actual params: [-0.6922,  1.2576]
-Original Grad: 0.161, -lr * Pred Grad:  0.247, New P: -0.445
-Original Grad: -0.121, -lr * Pred Grad:  -0.187, New P: 1.070
iter 11 loss: 0.352
Actual params: [-0.4455,  1.0704]
-Original Grad: 0.079, -lr * Pred Grad:  0.125, New P: -0.321
-Original Grad: -0.074, -lr * Pred Grad:  -0.114, New P: 0.956
iter 12 loss: 0.375
Actual params: [-0.3206,  0.9559]
-Original Grad: 0.089, -lr * Pred Grad:  0.130, New P: -0.191
-Original Grad: -0.147, -lr * Pred Grad:  -0.203, New P: 0.753
iter 13 loss: 0.399
Actual params: [-0.1911,  0.7529]
-Original Grad: -0.020, -lr * Pred Grad:  -0.037, New P: -0.228
-Original Grad: -0.009, -lr * Pred Grad:  -0.016, New P: 0.737
iter 14 loss: 0.394
Actual params: [-0.2284,  0.7368]
-Original Grad: 0.026, -lr * Pred Grad:  0.044, New P: -0.184
-Original Grad: -0.011, -lr * Pred Grad:  -0.014, New P: 0.723
iter 15 loss: 0.399
Actual params: [-0.1844,  0.7233]
-Original Grad: 0.016, -lr * Pred Grad:  0.031, New P: -0.153
-Original Grad: 0.005, -lr * Pred Grad:  0.013, New P: 0.736
iter 16 loss: 0.403
Actual params: [-0.153 ,  0.7359]
-Original Grad: -0.061, -lr * Pred Grad:  -0.103, New P: -0.256
-Original Grad: 0.048, -lr * Pred Grad:  0.080, New P: 0.816
iter 17 loss: 0.392
Actual params: [-0.2562,  0.8157]
-Original Grad: 0.011, -lr * Pred Grad:  0.013, New P: -0.243
-Original Grad: -0.028, -lr * Pred Grad:  -0.058, New P: 0.758
iter 18 loss: 0.394
Actual params: [-0.2434,  0.7579]
-Original Grad: 0.007, -lr * Pred Grad:  0.012, New P: -0.231
-Original Grad: -0.008, -lr * Pred Grad:  -0.016, New P: 0.742
iter 19 loss: 0.394
Actual params: [-0.2313,  0.7423]
-Original Grad: 0.006, -lr * Pred Grad:  0.012, New P: -0.219
-Original Grad: -0.008, -lr * Pred Grad:  -0.016, New P: 0.726
iter 20 loss: 0.395
Actual params: [-0.219 ,  0.7265]
-Original Grad: 0.084, -lr * Pred Grad:  0.247, New P: 0.028
-Original Grad: 0.112, -lr * Pred Grad:  0.295, New P: 1.022
Target params: [1.1812, 0.2779]
iter 0 loss: 1.456
Actual params: [0.5941, 0.5941]
-Original Grad: 0.743, -lr * Pred Grad:  0.226, New P: 0.820
-Original Grad: -0.004, -lr * Pred Grad:  -0.001, New P: 0.593
iter 1 loss: 1.247
Actual params: [0.8201, 0.5928]
-Original Grad: -0.400, -lr * Pred Grad:  -0.018, New P: 0.802
-Original Grad: -0.183, -lr * Pred Grad:  -0.741, New P: -0.148
iter 2 loss: 1.200
Actual params: [ 0.8022, -0.1483]
-Original Grad: 0.062, -lr * Pred Grad:  0.051, New P: 0.853
-Original Grad: -0.050, -lr * Pred Grad:  -0.313, New P: -0.461
iter 3 loss: 1.150
Actual params: [ 0.8534, -0.4609]
-Original Grad: -0.063, -lr * Pred Grad:  -0.007, New P: 0.846
-Original Grad: -0.000, -lr * Pred Grad:  0.013, New P: -0.448
iter 4 loss: 1.159
Actual params: [ 0.8459, -0.4483]
-Original Grad: -0.760, -lr * Pred Grad:  -0.076, New P: 0.770
-Original Grad: 0.038, -lr * Pred Grad:  0.284, New P: -0.164
iter 5 loss: 1.222
Actual params: [ 0.7696, -0.1641]
-Original Grad: -0.014, -lr * Pred Grad:  0.000, New P: 0.770
-Original Grad: -0.017, -lr * Pred Grad:  -0.094, New P: -0.258
iter 6 loss: 1.214
Actual params: [ 0.77  , -0.2582]
-Original Grad: 0.307, -lr * Pred Grad:  0.036, New P: 0.806
-Original Grad: -0.013, -lr * Pred Grad:  -0.114, New P: -0.372
iter 7 loss: 1.188
Actual params: [ 0.8058, -0.3723]
-Original Grad: -0.004, -lr * Pred Grad:  -0.003, New P: 0.803
-Original Grad: 0.030, -lr * Pred Grad:  0.161, New P: -0.211
iter 8 loss: 1.197
Actual params: [ 0.8026, -0.2113]
-Original Grad: 0.016, -lr * Pred Grad:  0.006, New P: 0.809
-Original Grad: -0.044, -lr * Pred Grad:  -0.238, New P: -0.449
iter 9 loss: 1.179
Actual params: [ 0.8086, -0.4494]
-Original Grad: -0.042, -lr * Pred Grad:  -0.008, New P: 0.801
-Original Grad: 0.251, -lr * Pred Grad:  0.378, New P: -0.072
iter 10 loss: 1.210
Actual params: [ 0.801 , -0.0719]
-Original Grad: -0.543, -lr * Pred Grad:  -0.069, New P: 0.732
-Original Grad: -0.108, -lr * Pred Grad:  -0.136, New P: -0.208
iter 11 loss: 1.244
Actual params: [ 0.7315, -0.2082]
-Original Grad: -0.019, -lr * Pred Grad:  -0.012, New P: 0.719
-Original Grad: 0.194, -lr * Pred Grad:  0.268, New P: 0.059
iter 12 loss: 1.279
Actual params: [0.7191, 0.0595]
-Original Grad: -0.098, -lr * Pred Grad:  -0.016, New P: 0.704
-Original Grad: -0.019, -lr * Pred Grad:  -0.024, New P: 0.035
iter 13 loss: 1.285
Actual params: [0.7036, 0.0354]
-Original Grad: -0.002, -lr * Pred Grad:  0.004, New P: 0.707
-Original Grad: -0.172, -lr * Pred Grad:  -0.105, New P: -0.070
iter 14 loss: 1.262
Actual params: [ 0.7074, -0.0697]
-Original Grad: 0.076, -lr * Pred Grad:  0.014, New P: 0.721
-Original Grad: 0.056, -lr * Pred Grad:  0.035, New P: -0.035
iter 15 loss: 1.259
Actual params: [ 0.7215, -0.0346]
-Original Grad: -0.119, -lr * Pred Grad:  -0.027, New P: 0.694
-Original Grad: 0.007, -lr * Pred Grad:  0.009, New P: -0.026
iter 16 loss: 1.278
Actual params: [ 0.6944, -0.0258]
-Original Grad: 0.027, -lr * Pred Grad:  0.007, New P: 0.701
-Original Grad: -0.002, -lr * Pred Grad:  -0.003, New P: -0.029
iter 17 loss: 1.273
Actual params: [ 0.7013, -0.0286]
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: 0.700
-Original Grad: 0.006, -lr * Pred Grad:  0.005, New P: -0.023
iter 18 loss: 1.275
Actual params: [ 0.7005, -0.0234]
-Original Grad: 0.022, -lr * Pred Grad:  0.007, New P: 0.707
-Original Grad: -0.002, -lr * Pred Grad:  -0.003, New P: -0.026
iter 19 loss: 1.270
Actual params: [ 0.7071, -0.026 ]
-Original Grad: -0.051, -lr * Pred Grad:  -0.012, New P: 0.695
-Original Grad: -0.076, -lr * Pred Grad:  -0.073, New P: -0.099
iter 20 loss: 1.265
Actual params: [ 0.695, -0.099]
-Original Grad: -0.058, -lr * Pred Grad:  -0.025, New P: 0.670
-Original Grad: 0.067, -lr * Pred Grad:  0.076, New P: -0.023
Target params: [1.1812, 0.2779]
iter 0 loss: 0.391
Actual params: [0.5941, 0.5941]
-Original Grad: 0.010, -lr * Pred Grad:  0.045, New P: 0.639
-Original Grad: -0.105, -lr * Pred Grad:  -0.461, New P: 0.133
iter 1 loss: 0.395
Actual params: [0.6393, 0.1334]
-Original Grad: -0.070, -lr * Pred Grad:  -0.719, New P: -0.080
-Original Grad: -0.348, -lr * Pred Grad:  -0.378, New P: -0.245
iter 2 loss: 0.364
Actual params: [-0.0801, -0.2449]
-Original Grad: 0.110, -lr * Pred Grad:  0.650, New P: 0.570
-Original Grad: -0.473, -lr * Pred Grad:  -0.172, New P: -0.417
iter 3 loss: 0.422
Actual params: [ 0.5704, -0.417 ]
-Original Grad: 0.182, -lr * Pred Grad:  0.372, New P: 0.942
-Original Grad: -0.035, -lr * Pred Grad:  0.037, New P: -0.380
iter 4 loss: 0.395
Actual params: [ 0.942 , -0.3796]
-Original Grad: 0.016, -lr * Pred Grad:  0.062, New P: 1.004
-Original Grad: 0.072, -lr * Pred Grad:  0.054, New P: -0.326
iter 5 loss: 0.392
Actual params: [ 1.0041, -0.3259]
-Original Grad: 0.042, -lr * Pred Grad:  0.085, New P: 1.089
-Original Grad: -0.055, -lr * Pred Grad:  -0.024, New P: -0.350
iter 6 loss: 0.384
Actual params: [ 1.0888, -0.3502]
-Original Grad: 0.073, -lr * Pred Grad:  0.222, New P: 1.311
-Original Grad: 0.112, -lr * Pred Grad:  0.108, New P: -0.243
iter 7 loss: 0.368
Actual params: [ 1.3109, -0.2426]
-Original Grad: 0.002, -lr * Pred Grad:  0.013, New P: 1.324
-Original Grad: 0.022, -lr * Pred Grad:  0.018, New P: -0.224
iter 8 loss: 0.366
Actual params: [ 1.3238, -0.2241]
-Original Grad: 0.088, -lr * Pred Grad:  0.265, New P: 1.588
-Original Grad: 0.061, -lr * Pred Grad:  0.075, New P: -0.149
iter 9 loss: 0.355
Actual params: [ 1.5884, -0.1491]
-Original Grad: 0.045, -lr * Pred Grad:  0.150, New P: 1.739
-Original Grad: 0.058, -lr * Pred Grad:  0.065, New P: -0.084
iter 10 loss: 0.350
Actual params: [ 1.7387, -0.0841]
-Original Grad: 0.011, -lr * Pred Grad:  0.065, New P: 1.804
-Original Grad: 0.132, -lr * Pred Grad:  0.131, New P: 0.047
iter 11 loss: 0.345
Actual params: [1.8042, 0.0469]
-Original Grad: 0.013, -lr * Pred Grad:  0.089, New P: 1.893
-Original Grad: 0.344, -lr * Pred Grad:  0.246, New P: 0.293
iter 12 loss: 0.345
Actual params: [1.8933, 0.293 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.007, New P: 1.900
-Original Grad: 0.003, -lr * Pred Grad:  0.003, New P: 0.296
iter 13 loss: 0.344
Actual params: [1.8999, 0.2959]
-Original Grad: 0.001, -lr * Pred Grad:  0.005, New P: 1.905
-Original Grad: 0.003, -lr * Pred Grad:  0.003, New P: 0.299
iter 14 loss: 0.344
Actual params: [1.9049, 0.2985]
-Original Grad: 0.009, -lr * Pred Grad:  0.037, New P: 1.942
-Original Grad: -0.031, -lr * Pred Grad:  -0.027, New P: 0.271
iter 15 loss: 0.344
Actual params: [1.942 , 0.2713]
-Original Grad: 0.002, -lr * Pred Grad:  0.009, New P: 1.951
-Original Grad: 0.003, -lr * Pred Grad:  0.004, New P: 0.275
iter 16 loss: 0.344
Actual params: [1.9507, 0.275 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.010, New P: 1.941
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: 0.276
iter 17 loss: 0.344
Actual params: [1.9411, 0.276 ]
-Original Grad: -0.042, -lr * Pred Grad:  -0.267, New P: 1.674
-Original Grad: -0.046, -lr * Pred Grad:  -0.061, New P: 0.215
iter 18 loss: 0.346
Actual params: [1.6738, 0.2147]
-Original Grad: 0.019, -lr * Pred Grad:  0.109, New P: 1.783
-Original Grad: -0.010, -lr * Pred Grad:  -0.010, New P: 0.205
iter 19 loss: 0.345
Actual params: [1.7831, 0.2052]
-Original Grad: -0.001, -lr * Pred Grad:  -0.007, New P: 1.776
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.204
iter 20 loss: 0.345
Actual params: [1.7765, 0.2044]
-Original Grad: 0.015, -lr * Pred Grad:  0.092, New P: 1.868
-Original Grad: 0.003, -lr * Pred Grad:  0.008, New P: 0.212
