Target params: [1.1812, 0.2779]
iter 0 loss: 0.779
Actual params: [0.5941, 0.5941]
-Original Grad: 0.395, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.051, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.772
Actual params: [0.6941, 0.6941]
-Original Grad: 0.036, -lr * Pred Grad:  0.073, New P: 0.767
-Original Grad: -0.170, -lr * Pred Grad:  -0.052, New P: 0.642
iter 2 loss: 0.767
Actual params: [0.7675, 0.6418]
-Original Grad: 0.088, -lr * Pred Grad:  0.069, New P: 0.837
-Original Grad: -0.146, -lr * Pred Grad:  -0.072, New P: 0.570
iter 3 loss: 0.747
Actual params: [0.8368, 0.57  ]
-Original Grad: 0.347, -lr * Pred Grad:  0.081, New P: 0.918
-Original Grad: -0.330, -lr * Pred Grad:  -0.081, New P: 0.489
iter 4 loss: 0.690
Actual params: [0.9177, 0.4888]
-Original Grad: -0.146, -lr * Pred Grad:  0.052, New P: 0.969
-Original Grad: 0.005, -lr * Pred Grad:  -0.068, New P: 0.421
iter 5 loss: 0.623
Actual params: [0.9692, 0.4207]
-Original Grad: -0.015, -lr * Pred Grad:  0.043, New P: 1.012
-Original Grad: -0.084, -lr * Pred Grad:  -0.068, New P: 0.353
iter 6 loss: 0.540
Actual params: [1.0122, 0.3527]
-Original Grad: 0.233, -lr * Pred Grad:  0.054, New P: 1.066
-Original Grad: 0.003, -lr * Pred Grad:  -0.059, New P: 0.294
iter 7 loss: 0.464
Actual params: [1.0664, 0.2936]
-Original Grad: -0.002, -lr * Pred Grad:  0.048, New P: 1.114
-Original Grad: -0.194, -lr * Pred Grad:  -0.068, New P: 0.225
iter 8 loss: 0.390
Actual params: [1.114 , 0.2253]
-Original Grad: 0.156, -lr * Pred Grad:  0.053, New P: 1.167
-Original Grad: -0.281, -lr * Pred Grad:  -0.077, New P: 0.148
iter 9 loss: 0.386
Actual params: [1.1671, 0.1481]
-Original Grad: -0.083, -lr * Pred Grad:  0.041, New P: 1.208
-Original Grad: 0.142, -lr * Pred Grad:  -0.054, New P: 0.094
iter 10 loss: 0.401
Actual params: [1.2077, 0.0939]
-Original Grad: -0.121, -lr * Pred Grad:  0.027, New P: 1.234
-Original Grad: -0.130, -lr * Pred Grad:  -0.058, New P: 0.036
iter 11 loss: 0.416
Actual params: [1.2343, 0.0355]
-Original Grad: 0.029, -lr * Pred Grad:  0.026, New P: 1.260
-Original Grad: -0.088, -lr * Pred Grad:  -0.059, New P: -0.024
iter 12 loss: 0.437
Actual params: [ 1.2603, -0.0237]
-Original Grad: 0.239, -lr * Pred Grad:  0.039, New P: 1.299
-Original Grad: 0.580, -lr * Pred Grad:  -0.003, New P: -0.027
iter 13 loss: 0.460
Actual params: [ 1.2992, -0.027 ]
-Original Grad: -0.073, -lr * Pred Grad:  0.030, New P: 1.329
-Original Grad: 0.312, -lr * Pred Grad:  0.015, New P: -0.012
iter 14 loss: 0.472
Actual params: [ 1.329 , -0.0124]
-Original Grad: 0.117, -lr * Pred Grad:  0.035, New P: 1.364
-Original Grad: 0.135, -lr * Pred Grad:  0.020, New P: 0.008
iter 15 loss: 0.490
Actual params: [1.3638, 0.0081]
-Original Grad: -0.015, -lr * Pred Grad:  0.031, New P: 1.394
-Original Grad: 0.206, -lr * Pred Grad:  0.029, New P: 0.037
iter 16 loss: 0.504
Actual params: [1.3943, 0.0373]
-Original Grad: -0.056, -lr * Pred Grad:  0.024, New P: 1.418
-Original Grad: 0.050, -lr * Pred Grad:  0.029, New P: 0.066
iter 17 loss: 0.516
Actual params: [1.4179, 0.0665]
-Original Grad: -0.096, -lr * Pred Grad:  0.014, New P: 1.432
-Original Grad: -0.080, -lr * Pred Grad:  0.022, New P: 0.088
iter 18 loss: 0.524
Actual params: [1.4323, 0.0885]
-Original Grad: -0.031, -lr * Pred Grad:  0.011, New P: 1.443
-Original Grad: 0.092, -lr * Pred Grad:  0.025, New P: 0.113
iter 19 loss: 0.537
Actual params: [1.4432, 0.1135]
-Original Grad: -0.064, -lr * Pred Grad:  0.005, New P: 1.448
-Original Grad: -0.084, -lr * Pred Grad:  0.018, New P: 0.131
iter 20 loss: 0.543
Actual params: [1.4484, 0.1315]
-Original Grad: -0.125, -lr * Pred Grad:  -0.004, New P: 1.444
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.148
Target params: [1.1812, 0.2779]
iter 0 loss: 0.343
Actual params: [0.5941, 0.5941]
-Original Grad: -0.142, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: 0.078, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.349
Actual params: [0.4941, 0.6941]
-Original Grad: -0.012, -lr * Pred Grad:  -0.073, New P: 0.421
-Original Grad: -0.040, -lr * Pred Grad:  0.026, New P: 0.720
iter 2 loss: 0.365
Actual params: [0.4209, 0.72  ]
-Original Grad: -0.059, -lr * Pred Grad:  -0.077, New P: 0.344
-Original Grad: 0.079, -lr * Pred Grad:  0.058, New P: 0.778
iter 3 loss: 0.395
Actual params: [0.3444, 0.7779]
-Original Grad: 0.027, -lr * Pred Grad:  -0.052, New P: 0.292
-Original Grad: -0.024, -lr * Pred Grad:  0.035, New P: 0.813
iter 4 loss: 0.417
Actual params: [0.2924, 0.8128]
-Original Grad: -0.080, -lr * Pred Grad:  -0.064, New P: 0.229
-Original Grad: -0.004, -lr * Pred Grad:  0.028, New P: 0.840
iter 5 loss: 0.449
Actual params: [0.2286, 0.8404]
-Original Grad: 0.027, -lr * Pred Grad:  -0.046, New P: 0.182
-Original Grad: -0.074, -lr * Pred Grad:  -0.007, New P: 0.833
iter 6 loss: 0.464
Actual params: [0.1821, 0.8332]
-Original Grad: -0.022, -lr * Pred Grad:  -0.046, New P: 0.136
-Original Grad: 0.065, -lr * Pred Grad:  0.015, New P: 0.848
iter 7 loss: 0.481
Actual params: [0.1357, 0.8485]
-Original Grad: 0.077, -lr * Pred Grad:  -0.018, New P: 0.118
-Original Grad: -0.035, -lr * Pred Grad:  0.002, New P: 0.851
iter 8 loss: 0.485
Actual params: [0.1175, 0.8508]
-Original Grad: -0.022, -lr * Pred Grad:  -0.021, New P: 0.096
-Original Grad: -0.078, -lr * Pred Grad:  -0.020, New P: 0.831
iter 9 loss: 0.485
Actual params: [0.0962, 0.8312]
-Original Grad: 0.053, -lr * Pred Grad:  -0.006, New P: 0.090
-Original Grad: 0.056, -lr * Pred Grad:  -0.002, New P: 0.829
iter 10 loss: 0.486
Actual params: [0.0903, 0.8291]
-Original Grad: -0.019, -lr * Pred Grad:  -0.010, New P: 0.081
-Original Grad: 0.051, -lr * Pred Grad:  0.011, New P: 0.840
iter 11 loss: 0.490
Actual params: [0.0807, 0.8401]
-Original Grad: 0.081, -lr * Pred Grad:  0.010, New P: 0.091
-Original Grad: -0.090, -lr * Pred Grad:  -0.011, New P: 0.829
iter 12 loss: 0.486
Actual params: [0.0905, 0.8287]
-Original Grad: -0.031, -lr * Pred Grad:  0.002, New P: 0.093
-Original Grad: -0.036, -lr * Pred Grad:  -0.018, New P: 0.810
iter 13 loss: 0.482
Actual params: [0.0925, 0.8105]
-Original Grad: 0.091, -lr * Pred Grad:  0.020, New P: 0.113
-Original Grad: -0.071, -lr * Pred Grad:  -0.031, New P: 0.780
iter 14 loss: 0.472
Actual params: [0.1126, 0.7797]
-Original Grad: 0.073, -lr * Pred Grad:  0.032, New P: 0.144
-Original Grad: -0.034, -lr * Pred Grad:  -0.035, New P: 0.745
iter 15 loss: 0.456
Actual params: [0.1442, 0.7449]
-Original Grad: -0.057, -lr * Pred Grad:  0.017, New P: 0.161
-Original Grad: -0.020, -lr * Pred Grad:  -0.036, New P: 0.709
iter 16 loss: 0.443
Actual params: [0.1612, 0.7093]
-Original Grad: 0.037, -lr * Pred Grad:  0.022, New P: 0.183
-Original Grad: -0.048, -lr * Pred Grad:  -0.042, New P: 0.668
iter 17 loss: 0.426
Actual params: [0.1834, 0.6676]
-Original Grad: 0.031, -lr * Pred Grad:  0.026, New P: 0.209
-Original Grad: -0.055, -lr * Pred Grad:  -0.048, New P: 0.619
iter 18 loss: 0.404
Actual params: [0.2095, 0.6194]
-Original Grad: -0.003, -lr * Pred Grad:  0.023, New P: 0.233
-Original Grad: 0.046, -lr * Pred Grad:  -0.034, New P: 0.586
iter 19 loss: 0.390
Actual params: [0.2327, 0.5858]
-Original Grad: -0.001, -lr * Pred Grad:  0.021, New P: 0.254
-Original Grad: 0.008, -lr * Pred Grad:  -0.029, New P: 0.557
iter 20 loss: 0.379
Actual params: [0.2535, 0.557 ]
-Original Grad: -0.033, -lr * Pred Grad:  0.012, New P: 0.266
-Original Grad: 0.011, -lr * Pred Grad:  -0.024, New P: 0.533
Target params: [1.1812, 0.2779]
iter 0 loss: 0.542
Actual params: [0.5941, 0.5941]
-Original Grad: 0.195, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.007, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.804
Actual params: [0.6941, 0.6941]
-Original Grad: 0.013, -lr * Pred Grad:  0.072, New P: 0.766
-Original Grad: 0.034, -lr * Pred Grad:  0.086, New P: 0.780
iter 2 loss: 1.200
Actual params: [0.7657, 0.7799]
-Original Grad: -0.038, -lr * Pred Grad:  0.042, New P: 0.808
-Original Grad: 0.082, -lr * Pred Grad:  0.085, New P: 0.865
iter 3 loss: 1.390
Actual params: [0.8077, 0.8646]
-Original Grad: 0.035, -lr * Pred Grad:  0.044, New P: 0.852
-Original Grad: 0.002, -lr * Pred Grad:  0.071, New P: 0.935
iter 4 loss: 1.476
Actual params: [0.8518, 0.9354]
-Original Grad: -0.020, -lr * Pred Grad:  0.032, New P: 0.884
-Original Grad: 0.061, -lr * Pred Grad:  0.080, New P: 1.016
iter 5 loss: 1.512
Actual params: [0.8835, 1.0157]
-Original Grad: 0.049, -lr * Pred Grad:  0.039, New P: 0.922
-Original Grad: -0.040, -lr * Pred Grad:  0.046, New P: 1.062
iter 6 loss: 1.509
Actual params: [0.9224, 1.0621]
-Original Grad: 0.011, -lr * Pred Grad:  0.037, New P: 0.959
-Original Grad: 0.023, -lr * Pred Grad:  0.050, New P: 1.112
iter 7 loss: 1.479
Actual params: [0.9589, 1.1118]
-Original Grad: -0.055, -lr * Pred Grad:  0.018, New P: 0.977
-Original Grad: -0.202, -lr * Pred Grad:  -0.021, New P: 1.091
iter 8 loss: 1.486
Actual params: [0.9773, 1.0908]
-Original Grad: 0.020, -lr * Pred Grad:  0.021, New P: 0.998
-Original Grad: -0.024, -lr * Pred Grad:  -0.024, New P: 1.067
iter 9 loss: 1.493
Actual params: [0.9981, 1.0673]
-Original Grad: 0.063, -lr * Pred Grad:  0.031, New P: 1.029
-Original Grad: 0.119, -lr * Pred Grad:  0.003, New P: 1.070
iter 10 loss: 1.483
Actual params: [1.0295, 1.0705]
-Original Grad: -0.044, -lr * Pred Grad:  0.018, New P: 1.048
-Original Grad: -0.147, -lr * Pred Grad:  -0.021, New P: 1.049
iter 11 loss: 1.485
Actual params: [1.0478, 1.0495]
-Original Grad: 0.015, -lr * Pred Grad:  0.020, New P: 1.068
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: 1.031
iter 12 loss: 1.482
Actual params: [1.0675, 1.0306]
-Original Grad: 0.113, -lr * Pred Grad:  0.037, New P: 1.105
-Original Grad: 0.283, -lr * Pred Grad:  0.021, New P: 1.051
iter 13 loss: 1.465
Actual params: [1.1048, 1.0512]
-Original Grad: -0.044, -lr * Pred Grad:  0.025, New P: 1.130
-Original Grad: -0.150, -lr * Pred Grad:  0.001, New P: 1.052
iter 14 loss: 1.456
Actual params: [1.1296, 1.0523]
-Original Grad: 0.056, -lr * Pred Grad:  0.032, New P: 1.162
-Original Grad: -0.098, -lr * Pred Grad:  -0.010, New P: 1.043
iter 15 loss: 1.446
Actual params: [1.1619, 1.0426]
-Original Grad: 0.084, -lr * Pred Grad:  0.043, New P: 1.205
-Original Grad: 0.058, -lr * Pred Grad:  -0.002, New P: 1.040
iter 16 loss: 1.429
Actual params: [1.2045, 1.0402]
-Original Grad: -0.091, -lr * Pred Grad:  0.021, New P: 1.226
-Original Grad: -0.173, -lr * Pred Grad:  -0.020, New P: 1.021
iter 17 loss: 1.422
Actual params: [1.226 , 1.0205]
-Original Grad: -0.003, -lr * Pred Grad:  0.019, New P: 1.245
-Original Grad: 0.090, -lr * Pred Grad:  -0.008, New P: 1.012
iter 18 loss: 1.415
Actual params: [1.2449, 1.0121]
-Original Grad: 0.072, -lr * Pred Grad:  0.029, New P: 1.274
-Original Grad: -0.030, -lr * Pred Grad:  -0.011, New P: 1.001
iter 19 loss: 1.403
Actual params: [1.2736, 1.0013]
-Original Grad: -0.331, -lr * Pred Grad:  -0.020, New P: 1.254
-Original Grad: -0.318, -lr * Pred Grad:  -0.036, New P: 0.966
iter 20 loss: 1.402
Actual params: [1.2537, 0.9657]
-Original Grad: -0.063, -lr * Pred Grad:  -0.025, New P: 1.229
-Original Grad: -0.278, -lr * Pred Grad:  -0.051, New P: 0.914
Target params: [1.1812, 0.2779]
iter 0 loss: 1.449
Actual params: [0.5941, 0.5941]
-Original Grad: -0.073, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.070, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 1.468
Actual params: [0.4941, 0.4941]
-Original Grad: 0.041, -lr * Pred Grad:  -0.022, New P: 0.472
-Original Grad: -0.095, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 1.455
Actual params: [0.4718, 0.3944]
-Original Grad: 0.083, -lr * Pred Grad:  0.033, New P: 0.505
-Original Grad: -0.045, -lr * Pred Grad:  -0.095, New P: 0.300
iter 3 loss: 1.440
Actual params: [0.5048, 0.2997]
-Original Grad: 0.118, -lr * Pred Grad:  0.060, New P: 0.565
-Original Grad: -0.078, -lr * Pred Grad:  -0.097, New P: 0.203
iter 4 loss: 1.414
Actual params: [0.5651, 0.2031]
-Original Grad: 0.106, -lr * Pred Grad:  0.072, New P: 0.637
-Original Grad: -0.030, -lr * Pred Grad:  -0.091, New P: 0.112
iter 5 loss: 1.370
Actual params: [0.6373, 0.1122]
-Original Grad: -0.013, -lr * Pred Grad:  0.059, New P: 0.696
-Original Grad: -0.023, -lr * Pred Grad:  -0.085, New P: 0.027
iter 6 loss: 1.315
Actual params: [0.6958, 0.0269]
-Original Grad: 0.063, -lr * Pred Grad:  0.064, New P: 0.760
-Original Grad: -0.027, -lr * Pred Grad:  -0.082, New P: -0.055
iter 7 loss: 1.250
Actual params: [ 0.7599, -0.0553]
-Original Grad: 0.034, -lr * Pred Grad:  0.064, New P: 0.824
-Original Grad: 0.007, -lr * Pred Grad:  -0.070, New P: -0.125
iter 8 loss: 1.110
Actual params: [ 0.8236, -0.1254]
-Original Grad: 0.260, -lr * Pred Grad:  0.074, New P: 0.897
-Original Grad: 0.012, -lr * Pred Grad:  -0.058, New P: -0.184
iter 9 loss: 0.847
Actual params: [ 0.8973, -0.1837]
-Original Grad: -0.022, -lr * Pred Grad:  0.062, New P: 0.960
-Original Grad: -0.002, -lr * Pred Grad:  -0.053, New P: -0.236
iter 10 loss: 0.685
Actual params: [ 0.9596, -0.2365]
-Original Grad: -0.052, -lr * Pred Grad:  0.048, New P: 1.007
-Original Grad: 0.093, -lr * Pred Grad:  -0.016, New P: -0.252
iter 11 loss: 0.598
Actual params: [ 1.0074, -0.2523]
-Original Grad: 0.175, -lr * Pred Grad:  0.060, New P: 1.068
-Original Grad: 0.031, -lr * Pred Grad:  -0.006, New P: -0.258
iter 12 loss: 0.528
Actual params: [ 1.0677, -0.2583]
-Original Grad: 0.083, -lr * Pred Grad:  0.063, New P: 1.131
-Original Grad: 0.065, -lr * Pred Grad:  0.011, New P: -0.247
iter 13 loss: 0.500
Actual params: [ 1.1311, -0.2472]
-Original Grad: 0.088, -lr * Pred Grad:  0.067, New P: 1.198
-Original Grad: 0.021, -lr * Pred Grad:  0.015, New P: -0.232
iter 14 loss: 0.513
Actual params: [ 1.1977, -0.2322]
-Original Grad: -0.124, -lr * Pred Grad:  0.043, New P: 1.241
-Original Grad: 0.035, -lr * Pred Grad:  0.022, New P: -0.210
iter 15 loss: 0.529
Actual params: [ 1.2408, -0.2103]
-Original Grad: 0.010, -lr * Pred Grad:  0.040, New P: 1.281
-Original Grad: 0.021, -lr * Pred Grad:  0.025, New P: -0.185
iter 16 loss: 0.553
Actual params: [ 1.281 , -0.1854]
-Original Grad: 0.040, -lr * Pred Grad:  0.041, New P: 1.322
-Original Grad: 0.011, -lr * Pred Grad:  0.025, New P: -0.160
iter 17 loss: 0.585
Actual params: [ 1.322, -0.16 ]
-Original Grad: 0.156, -lr * Pred Grad:  0.052, New P: 1.374
-Original Grad: 0.042, -lr * Pred Grad:  0.033, New P: -0.127
iter 18 loss: 0.633
Actual params: [ 1.3742, -0.1274]
-Original Grad: -0.021, -lr * Pred Grad:  0.045, New P: 1.419
-Original Grad: 0.015, -lr * Pred Grad:  0.033, New P: -0.094
iter 19 loss: 0.675
Actual params: [ 1.4192, -0.094 ]
-Original Grad: -0.019, -lr * Pred Grad:  0.039, New P: 1.458
-Original Grad: -0.019, -lr * Pred Grad:  0.026, New P: -0.068
iter 20 loss: 0.702
Actual params: [ 1.458 , -0.0684]
-Original Grad: 0.049, -lr * Pred Grad:  0.041, New P: 1.499
-Original Grad: 0.027, -lr * Pred Grad:  0.030, New P: -0.039
Target params: [1.1812, 0.2779]
iter 0 loss: 0.945
Actual params: [0.5941, 0.5941]
-Original Grad: 0.058, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.237, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.891
Actual params: [0.6941, 0.4941]
-Original Grad: 0.231, -lr * Pred Grad:  0.088, New P: 0.783
-Original Grad: -0.077, -lr * Pred Grad:  -0.087, New P: 0.407
iter 2 loss: 0.820
Actual params: [0.7825, 0.4074]
-Original Grad: 0.132, -lr * Pred Grad:  0.091, New P: 0.873
-Original Grad: -0.214, -lr * Pred Grad:  -0.092, New P: 0.315
iter 3 loss: 0.725
Actual params: [0.8734, 0.3149]
-Original Grad: 0.128, -lr * Pred Grad:  0.092, New P: 0.965
-Original Grad: 0.023, -lr * Pred Grad:  -0.071, New P: 0.243
iter 4 loss: 0.654
Actual params: [0.9654, 0.2435]
-Original Grad: 0.016, -lr * Pred Grad:  0.081, New P: 1.046
-Original Grad: 0.058, -lr * Pred Grad:  -0.050, New P: 0.194
iter 5 loss: 0.602
Actual params: [1.0461, 0.1936]
-Original Grad: -0.033, -lr * Pred Grad:  0.063, New P: 1.109
-Original Grad: 0.047, -lr * Pred Grad:  -0.035, New P: 0.158
iter 6 loss: 0.597
Actual params: [1.1094, 0.1582]
-Original Grad: 0.030, -lr * Pred Grad:  0.060, New P: 1.170
-Original Grad: 0.002, -lr * Pred Grad:  -0.031, New P: 0.128
iter 7 loss: 0.610
Actual params: [1.1695, 0.1276]
-Original Grad: -0.008, -lr * Pred Grad:  0.052, New P: 1.221
-Original Grad: -0.010, -lr * Pred Grad:  -0.028, New P: 0.099
iter 8 loss: 0.625
Actual params: [1.2211, 0.0992]
-Original Grad: -0.025, -lr * Pred Grad:  0.042, New P: 1.263
-Original Grad: -0.028, -lr * Pred Grad:  -0.029, New P: 0.070
iter 9 loss: 0.638
Actual params: [1.2628, 0.07  ]
-Original Grad: -0.042, -lr * Pred Grad:  0.030, New P: 1.293
-Original Grad: -0.012, -lr * Pred Grad:  -0.028, New P: 0.042
iter 10 loss: 0.652
Actual params: [1.293 , 0.0423]
-Original Grad: -0.006, -lr * Pred Grad:  0.026, New P: 1.319
-Original Grad: 0.007, -lr * Pred Grad:  -0.024, New P: 0.018
iter 11 loss: 0.662
Actual params: [1.3193, 0.0185]
-Original Grad: -0.074, -lr * Pred Grad:  0.012, New P: 1.331
-Original Grad: -0.007, -lr * Pred Grad:  -0.022, New P: -0.004
iter 12 loss: 0.661
Actual params: [ 1.3309, -0.0039]
-Original Grad: -0.027, -lr * Pred Grad:  0.006, New P: 1.337
-Original Grad: -0.048, -lr * Pred Grad:  -0.027, New P: -0.031
iter 13 loss: 0.661
Actual params: [ 1.3373, -0.0307]
-Original Grad: -0.026, -lr * Pred Grad:  0.002, New P: 1.339
-Original Grad: 0.015, -lr * Pred Grad:  -0.022, New P: -0.053
iter 14 loss: 0.663
Actual params: [ 1.3391, -0.0527]
-Original Grad: -0.068, -lr * Pred Grad:  -0.009, New P: 1.330
-Original Grad: 0.008, -lr * Pred Grad:  -0.019, New P: -0.072
iter 15 loss: 0.660
Actual params: [ 1.3305, -0.0715]
-Original Grad: -0.029, -lr * Pred Grad:  -0.012, New P: 1.318
-Original Grad: -0.062, -lr * Pred Grad:  -0.025, New P: -0.097
iter 16 loss: 0.651
Actual params: [ 1.3183, -0.097 ]
-Original Grad: -0.052, -lr * Pred Grad:  -0.019, New P: 1.300
-Original Grad: -0.004, -lr * Pred Grad:  -0.024, New P: -0.121
iter 17 loss: 0.638
Actual params: [ 1.2996, -0.1206]
-Original Grad: -0.021, -lr * Pred Grad:  -0.020, New P: 1.279
-Original Grad: -0.039, -lr * Pred Grad:  -0.027, New P: -0.147
iter 18 loss: 0.632
Actual params: [ 1.2794, -0.1475]
-Original Grad: -0.012, -lr * Pred Grad:  -0.020, New P: 1.259
-Original Grad: 0.019, -lr * Pred Grad:  -0.022, New P: -0.169
iter 19 loss: 0.625
Actual params: [ 1.2594, -0.169 ]
-Original Grad: 0.022, -lr * Pred Grad:  -0.015, New P: 1.245
-Original Grad: 0.004, -lr * Pred Grad:  -0.019, New P: -0.188
iter 20 loss: 0.616
Actual params: [ 1.2446, -0.188 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: 1.231
-Original Grad: -0.016, -lr * Pred Grad:  -0.020, New P: -0.208
Target params: [1.1812, 0.2779]
iter 0 loss: 0.804
Actual params: [0.5941, 0.5941]
-Original Grad: -0.056, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.200, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.819
Actual params: [0.4941, 0.4941]
-Original Grad: 0.098, -lr * Pred Grad:  0.031, New P: 0.526
-Original Grad: -0.129, -lr * Pred Grad:  -0.097, New P: 0.397
iter 2 loss: 0.779
Actual params: [0.5255, 0.3974]
-Original Grad: 0.096, -lr * Pred Grad:  0.060, New P: 0.585
-Original Grad: -0.074, -lr * Pred Grad:  -0.090, New P: 0.307
iter 3 loss: 0.732
Actual params: [0.5854, 0.3071]
-Original Grad: 0.023, -lr * Pred Grad:  0.057, New P: 0.643
-Original Grad: -0.092, -lr * Pred Grad:  -0.089, New P: 0.218
iter 4 loss: 0.685
Actual params: [0.6427, 0.2176]
-Original Grad: -0.038, -lr * Pred Grad:  0.033, New P: 0.676
-Original Grad: -0.137, -lr * Pred Grad:  -0.092, New P: 0.125
iter 5 loss: 0.654
Actual params: [0.6762, 0.1253]
-Original Grad: 0.109, -lr * Pred Grad:  0.054, New P: 0.730
-Original Grad: -0.067, -lr * Pred Grad:  -0.089, New P: 0.036
iter 6 loss: 0.610
Actual params: [0.7299, 0.0363]
-Original Grad: 0.055, -lr * Pred Grad:  0.059, New P: 0.789
-Original Grad: -0.044, -lr * Pred Grad:  -0.084, New P: -0.048
iter 7 loss: 0.559
Actual params: [ 0.7891, -0.0479]
-Original Grad: 0.098, -lr * Pred Grad:  0.069, New P: 0.858
-Original Grad: -0.005, -lr * Pred Grad:  -0.075, New P: -0.123
iter 8 loss: 0.495
Actual params: [ 0.8579, -0.1229]
-Original Grad: -0.018, -lr * Pred Grad:  0.057, New P: 0.915
-Original Grad: 0.012, -lr * Pred Grad:  -0.065, New P: -0.187
iter 9 loss: 0.440
Actual params: [ 0.9148, -0.1874]
-Original Grad: 0.065, -lr * Pred Grad:  0.062, New P: 0.977
-Original Grad: 0.020, -lr * Pred Grad:  -0.054, New P: -0.242
iter 10 loss: 0.405
Actual params: [ 0.9773, -0.2418]
-Original Grad: 0.047, -lr * Pred Grad:  0.064, New P: 1.042
-Original Grad: -0.010, -lr * Pred Grad:  -0.050, New P: -0.292
iter 11 loss: 0.393
Actual params: [ 1.0418, -0.292 ]
-Original Grad: 0.065, -lr * Pred Grad:  0.069, New P: 1.110
-Original Grad: -0.007, -lr * Pred Grad:  -0.046, New P: -0.338
iter 12 loss: 0.394
Actual params: [ 1.1105, -0.3382]
-Original Grad: -0.004, -lr * Pred Grad:  0.061, New P: 1.172
-Original Grad: 0.039, -lr * Pred Grad:  -0.035, New P: -0.374
iter 13 loss: 0.403
Actual params: [ 1.1717, -0.3735]
-Original Grad: -0.035, -lr * Pred Grad:  0.048, New P: 1.220
-Original Grad: -0.048, -lr * Pred Grad:  -0.039, New P: -0.412
iter 14 loss: 0.413
Actual params: [ 1.2196, -0.4124]
-Original Grad: -0.056, -lr * Pred Grad:  0.031, New P: 1.251
-Original Grad: -0.005, -lr * Pred Grad:  -0.036, New P: -0.448
iter 15 loss: 0.419
Actual params: [ 1.2511, -0.4483]
-Original Grad: -0.040, -lr * Pred Grad:  0.020, New P: 1.272
-Original Grad: -0.026, -lr * Pred Grad:  -0.037, New P: -0.485
iter 16 loss: 0.423
Actual params: [ 1.2715, -0.4848]
-Original Grad: -0.087, -lr * Pred Grad:  0.002, New P: 1.273
-Original Grad: -0.045, -lr * Pred Grad:  -0.040, New P: -0.525
iter 17 loss: 0.426
Actual params: [ 1.2731, -0.5246]
-Original Grad: -0.027, -lr * Pred Grad:  -0.004, New P: 1.270
-Original Grad: 0.000, -lr * Pred Grad:  -0.036, New P: -0.561
iter 18 loss: 0.425
Actual params: [ 1.2696, -0.5606]
-Original Grad: -0.009, -lr * Pred Grad:  -0.005, New P: 1.265
-Original Grad: 0.007, -lr * Pred Grad:  -0.032, New P: -0.592
iter 19 loss: 0.425
Actual params: [ 1.2647, -0.5922]
-Original Grad: -0.051, -lr * Pred Grad:  -0.014, New P: 1.251
-Original Grad: 0.016, -lr * Pred Grad:  -0.026, New P: -0.618
iter 20 loss: 0.424
Actual params: [ 1.2509, -0.6184]
-Original Grad: -0.045, -lr * Pred Grad:  -0.021, New P: 1.230
-Original Grad: 0.029, -lr * Pred Grad:  -0.019, New P: -0.637
Target params: [1.1812, 0.2779]
iter 0 loss: 0.465
Actual params: [0.5941, 0.5941]
-Original Grad: 0.041, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.039, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.437
Actual params: [0.6941, 0.4941]
-Original Grad: 0.039, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: 0.007, -lr * Pred Grad:  -0.052, New P: 0.442
iter 2 loss: 0.420
Actual params: [0.7939, 0.4419]
-Original Grad: 0.040, -lr * Pred Grad:  0.100, New P: 0.894
-Original Grad: -0.085, -lr * Pred Grad:  -0.075, New P: 0.367
iter 3 loss: 0.405
Actual params: [0.8938, 0.367 ]
-Original Grad: 0.018, -lr * Pred Grad:  0.094, New P: 0.988
-Original Grad: 0.010, -lr * Pred Grad:  -0.055, New P: 0.312
iter 4 loss: 0.400
Actual params: [0.9877, 0.3122]
-Original Grad: 0.032, -lr * Pred Grad:  0.095, New P: 1.082
-Original Grad: -0.060, -lr * Pred Grad:  -0.068, New P: 0.244
iter 5 loss: 0.401
Actual params: [1.0825, 0.2439]
-Original Grad: 0.014, -lr * Pred Grad:  0.090, New P: 1.172
-Original Grad: -0.040, -lr * Pred Grad:  -0.073, New P: 0.171
iter 6 loss: 0.404
Actual params: [1.172 , 0.1709]
-Original Grad: 0.029, -lr * Pred Grad:  0.091, New P: 1.263
-Original Grad: -0.027, -lr * Pred Grad:  -0.073, New P: 0.097
iter 7 loss: 0.402
Actual params: [1.2629, 0.0975]
-Original Grad: 0.002, -lr * Pred Grad:  0.081, New P: 1.344
-Original Grad: -0.075, -lr * Pred Grad:  -0.081, New P: 0.016
iter 8 loss: 0.405
Actual params: [1.3443, 0.0163]
-Original Grad: 0.026, -lr * Pred Grad:  0.083, New P: 1.428
-Original Grad: 0.003, -lr * Pred Grad:  -0.071, New P: -0.055
iter 9 loss: 0.405
Actual params: [ 1.4278, -0.0548]
-Original Grad: 0.006, -lr * Pred Grad:  0.078, New P: 1.505
-Original Grad: 0.016, -lr * Pred Grad:  -0.058, New P: -0.113
iter 10 loss: 0.407
Actual params: [ 1.5055, -0.1126]
-Original Grad: 0.014, -lr * Pred Grad:  0.076, New P: 1.582
-Original Grad: -0.011, -lr * Pred Grad:  -0.055, New P: -0.168
iter 11 loss: 0.414
Actual params: [ 1.5818, -0.1681]
-Original Grad: 0.030, -lr * Pred Grad:  0.080, New P: 1.662
-Original Grad: 0.018, -lr * Pred Grad:  -0.044, New P: -0.212
iter 12 loss: 0.415
Actual params: [ 1.6622, -0.2116]
-Original Grad: 0.003, -lr * Pred Grad:  0.074, New P: 1.736
-Original Grad: -0.034, -lr * Pred Grad:  -0.049, New P: -0.261
iter 13 loss: 0.416
Actual params: [ 1.7361, -0.2609]
-Original Grad: 0.016, -lr * Pred Grad:  0.074, New P: 1.810
-Original Grad: -0.068, -lr * Pred Grad:  -0.061, New P: -0.321
iter 14 loss: 0.416
Actual params: [ 1.8101, -0.3215]
-Original Grad: 0.011, -lr * Pred Grad:  0.072, New P: 1.882
-Original Grad: -0.023, -lr * Pred Grad:  -0.061, New P: -0.382
iter 15 loss: 0.417
Actual params: [ 1.8821, -0.3825]
-Original Grad: 0.047, -lr * Pred Grad:  0.080, New P: 1.962
-Original Grad: 0.021, -lr * Pred Grad:  -0.049, New P: -0.431
iter 16 loss: 0.417
Actual params: [ 1.9623, -0.4312]
-Original Grad: 0.012, -lr * Pred Grad:  0.078, New P: 2.040
-Original Grad: 0.017, -lr * Pred Grad:  -0.039, New P: -0.470
iter 17 loss: 0.417
Actual params: [ 2.04  , -0.4701]
-Original Grad: 0.010, -lr * Pred Grad:  0.075, New P: 2.115
-Original Grad: -0.035, -lr * Pred Grad:  -0.045, New P: -0.515
iter 18 loss: 0.418
Actual params: [ 2.1147, -0.5149]
-Original Grad: 0.033, -lr * Pred Grad:  0.080, New P: 2.194
-Original Grad: 0.011, -lr * Pred Grad:  -0.037, New P: -0.552
iter 19 loss: 0.418
Actual params: [ 2.1942, -0.5523]
-Original Grad: 0.020, -lr * Pred Grad:  0.080, New P: 2.274
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -0.586
iter 20 loss: 0.419
Actual params: [ 2.2742, -0.5865]
-Original Grad: 0.039, -lr * Pred Grad:  0.085, New P: 2.360
-Original Grad: -0.009, -lr * Pred Grad:  -0.034, New P: -0.620
Target params: [1.1812, 0.2779]
iter 0 loss: 0.551
Actual params: [0.5941, 0.5941]
-Original Grad: 0.003, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.045, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.538
Actual params: [0.6941, 0.4941]
-Original Grad: -0.084, -lr * Pred Grad:  -0.072, New P: 0.622
-Original Grad: -0.075, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.584
Actual params: [0.6219, 0.3957]
-Original Grad: -0.086, -lr * Pred Grad:  -0.085, New P: 0.537
-Original Grad: -0.074, -lr * Pred Grad:  -0.099, New P: 0.296
iter 3 loss: 0.629
Actual params: [0.5372, 0.2965]
-Original Grad: -0.029, -lr * Pred Grad:  -0.081, New P: 0.456
-Original Grad: -0.046, -lr * Pred Grad:  -0.097, New P: 0.199
iter 4 loss: 0.648
Actual params: [0.4562, 0.1994]
-Original Grad: 0.054, -lr * Pred Grad:  -0.041, New P: 0.415
-Original Grad: -0.020, -lr * Pred Grad:  -0.090, New P: 0.110
iter 5 loss: 0.662
Actual params: [0.4151, 0.1096]
-Original Grad: 0.045, -lr * Pred Grad:  -0.017, New P: 0.398
-Original Grad: -0.055, -lr * Pred Grad:  -0.092, New P: 0.018
iter 6 loss: 0.672
Actual params: [0.3979, 0.0177]
-Original Grad: 0.052, -lr * Pred Grad:  0.003, New P: 0.401
-Original Grad: -0.026, -lr * Pred Grad:  -0.088, New P: -0.071
iter 7 loss: 0.669
Actual params: [ 0.4014, -0.0707]
-Original Grad: -0.075, -lr * Pred Grad:  -0.019, New P: 0.382
-Original Grad: -0.064, -lr * Pred Grad:  -0.092, New P: -0.162
iter 8 loss: 0.669
Actual params: [ 0.3822, -0.1622]
-Original Grad: -0.008, -lr * Pred Grad:  -0.019, New P: 0.363
-Original Grad: -0.014, -lr * Pred Grad:  -0.085, New P: -0.248
iter 9 loss: 0.668
Actual params: [ 0.3629, -0.2477]
-Original Grad: 0.147, -lr * Pred Grad:  0.019, New P: 0.382
-Original Grad: -0.013, -lr * Pred Grad:  -0.080, New P: -0.328
iter 10 loss: 0.661
Actual params: [ 0.3818, -0.3278]
-Original Grad: 0.016, -lr * Pred Grad:  0.020, New P: 0.402
-Original Grad: 0.094, -lr * Pred Grad:  -0.036, New P: -0.364
iter 11 loss: 0.656
Actual params: [ 0.4021, -0.364 ]
-Original Grad: 0.026, -lr * Pred Grad:  0.024, New P: 0.426
-Original Grad: 0.070, -lr * Pred Grad:  -0.013, New P: -0.377
iter 12 loss: 0.652
Actual params: [ 0.4258, -0.3772]
-Original Grad: -0.053, -lr * Pred Grad:  0.010, New P: 0.436
-Original Grad: -0.047, -lr * Pred Grad:  -0.023, New P: -0.400
iter 13 loss: 0.649
Actual params: [ 0.4355, -0.4   ]
-Original Grad: 0.098, -lr * Pred Grad:  0.027, New P: 0.462
-Original Grad: 0.040, -lr * Pred Grad:  -0.011, New P: -0.411
iter 14 loss: 0.640
Actual params: [ 0.4625, -0.4107]
-Original Grad: -0.043, -lr * Pred Grad:  0.016, New P: 0.478
-Original Grad: -0.047, -lr * Pred Grad:  -0.020, New P: -0.431
iter 15 loss: 0.635
Actual params: [ 0.4784, -0.4311]
-Original Grad: 0.076, -lr * Pred Grad:  0.028, New P: 0.506
-Original Grad: -0.037, -lr * Pred Grad:  -0.027, New P: -0.458
iter 16 loss: 0.629
Actual params: [ 0.5062, -0.4578]
-Original Grad: 0.093, -lr * Pred Grad:  0.040, New P: 0.546
-Original Grad: 0.015, -lr * Pred Grad:  -0.021, New P: -0.478
iter 17 loss: 0.621
Actual params: [ 0.5463, -0.4784]
-Original Grad: -0.015, -lr * Pred Grad:  0.034, New P: 0.580
-Original Grad: -0.013, -lr * Pred Grad:  -0.022, New P: -0.500
iter 18 loss: 0.613
Actual params: [ 0.58  , -0.5002]
-Original Grad: 0.001, -lr * Pred Grad:  0.031, New P: 0.611
-Original Grad: -0.023, -lr * Pred Grad:  -0.025, New P: -0.525
iter 19 loss: 0.604
Actual params: [ 0.6109, -0.5252]
-Original Grad: -0.097, -lr * Pred Grad:  0.010, New P: 0.621
-Original Grad: -0.050, -lr * Pred Grad:  -0.034, New P: -0.559
iter 20 loss: 0.600
Actual params: [ 0.621 , -0.5589]
-Original Grad: -0.102, -lr * Pred Grad:  -0.008, New P: 0.613
-Original Grad: -0.015, -lr * Pred Grad:  -0.034, New P: -0.593
Target params: [1.1812, 0.2779]
iter 0 loss: 1.367
Actual params: [0.5941, 0.5941]
-Original Grad: 0.256, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.093, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 1.412
Actual params: [0.6941, 0.4941]
-Original Grad: -0.246, -lr * Pred Grad:  -0.003, New P: 0.691
-Original Grad: -0.022, -lr * Pred Grad:  -0.082, New P: 0.412
iter 2 loss: 1.425
Actual params: [0.6908, 0.4116]
-Original Grad: -0.006, -lr * Pred Grad:  -0.004, New P: 0.687
-Original Grad: 0.020, -lr * Pred Grad:  -0.049, New P: 0.362
iter 3 loss: 1.428
Actual params: [0.6873, 0.3624]
-Original Grad: 0.170, -lr * Pred Grad:  0.022, New P: 0.710
-Original Grad: 0.053, -lr * Pred Grad:  -0.008, New P: 0.355
iter 4 loss: 1.452
Actual params: [0.7098, 0.3547]
-Original Grad: 0.080, -lr * Pred Grad:  0.029, New P: 0.739
-Original Grad: -0.047, -lr * Pred Grad:  -0.027, New P: 0.327
iter 5 loss: 1.487
Actual params: [0.7392, 0.3274]
-Original Grad: -0.109, -lr * Pred Grad:  0.011, New P: 0.750
-Original Grad: -0.059, -lr * Pred Grad:  -0.044, New P: 0.283
iter 6 loss: 1.503
Actual params: [0.75  , 0.2833]
-Original Grad: -0.048, -lr * Pred Grad:  0.004, New P: 0.754
-Original Grad: -0.081, -lr * Pred Grad:  -0.059, New P: 0.224
iter 7 loss: 1.517
Actual params: [0.7536, 0.2241]
-Original Grad: 0.086, -lr * Pred Grad:  0.013, New P: 0.767
-Original Grad: 0.037, -lr * Pred Grad:  -0.039, New P: 0.185
iter 8 loss: 1.527
Actual params: [0.7666, 0.1849]
-Original Grad: -0.302, -lr * Pred Grad:  -0.019, New P: 0.748
-Original Grad: 0.020, -lr * Pred Grad:  -0.028, New P: 0.156
iter 9 loss: 1.511
Actual params: [0.7478, 0.1564]
-Original Grad: -0.207, -lr * Pred Grad:  -0.033, New P: 0.714
-Original Grad: -0.012, -lr * Pred Grad:  -0.029, New P: 0.127
iter 10 loss: 1.488
Actual params: [0.7144, 0.1274]
-Original Grad: -0.104, -lr * Pred Grad:  -0.038, New P: 0.676
-Original Grad: 0.052, -lr * Pred Grad:  -0.010, New P: 0.117
iter 11 loss: 1.458
Actual params: [0.6762, 0.1173]
-Original Grad: -0.287, -lr * Pred Grad:  -0.052, New P: 0.624
-Original Grad: -0.025, -lr * Pred Grad:  -0.016, New P: 0.101
iter 12 loss: 1.420
Actual params: [0.6238, 0.1011]
-Original Grad: -0.016, -lr * Pred Grad:  -0.048, New P: 0.575
-Original Grad: 0.039, -lr * Pred Grad:  -0.004, New P: 0.097
iter 13 loss: 1.390
Actual params: [0.5755, 0.0974]
-Original Grad: 0.311, -lr * Pred Grad:  -0.018, New P: 0.557
-Original Grad: 0.065, -lr * Pred Grad:  0.014, New P: 0.111
iter 14 loss: 1.381
Actual params: [0.5574, 0.1109]
-Original Grad: 0.165, -lr * Pred Grad:  -0.005, New P: 0.552
-Original Grad: 0.071, -lr * Pred Grad:  0.029, New P: 0.139
iter 15 loss: 1.377
Actual params: [0.5525, 0.1395]
-Original Grad: 0.113, -lr * Pred Grad:  0.003, New P: 0.556
-Original Grad: 0.061, -lr * Pred Grad:  0.039, New P: 0.178
iter 16 loss: 1.374
Actual params: [0.5555, 0.1785]
-Original Grad: 0.157, -lr * Pred Grad:  0.013, New P: 0.569
-Original Grad: 0.107, -lr * Pred Grad:  0.054, New P: 0.232
iter 17 loss: 1.374
Actual params: [0.5685, 0.2324]
-Original Grad: 0.124, -lr * Pred Grad:  0.020, New P: 0.588
-Original Grad: 0.098, -lr * Pred Grad:  0.064, New P: 0.297
iter 18 loss: 1.374
Actual params: [0.5883, 0.2967]
-Original Grad: 0.039, -lr * Pred Grad:  0.020, New P: 0.609
-Original Grad: 0.070, -lr * Pred Grad:  0.070, New P: 0.366
iter 19 loss: 1.377
Actual params: [0.6087, 0.3663]
-Original Grad: 0.131, -lr * Pred Grad:  0.027, New P: 0.636
-Original Grad: 0.024, -lr * Pred Grad:  0.068, New P: 0.434
iter 20 loss: 1.386
Actual params: [0.6356, 0.4341]
-Original Grad: 0.319, -lr * Pred Grad:  0.042, New P: 0.678
-Original Grad: 0.015, -lr * Pred Grad:  0.064, New P: 0.498
Target params: [1.1812, 0.2779]
iter 0 loss: 0.857
Actual params: [0.5941, 0.5941]
-Original Grad: -0.033, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: 0.206, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 1.005
Actual params: [0.4941, 0.6941]
-Original Grad: -0.017, -lr * Pred Grad:  -0.093, New P: 0.401
-Original Grad: -0.075, -lr * Pred Grad:  0.038, New P: 0.732
iter 2 loss: 1.041
Actual params: [0.4008, 0.7318]
-Original Grad: 0.014, -lr * Pred Grad:  -0.045, New P: 0.356
-Original Grad: -0.101, -lr * Pred Grad:  -0.000, New P: 0.732
iter 3 loss: 1.045
Actual params: [0.3557, 0.7316]
-Original Grad: -0.020, -lr * Pred Grad:  -0.059, New P: 0.296
-Original Grad: -0.097, -lr * Pred Grad:  -0.022, New P: 0.710
iter 4 loss: 1.045
Actual params: [0.2963, 0.7098]
-Original Grad: 0.020, -lr * Pred Grad:  -0.023, New P: 0.273
-Original Grad: 0.007, -lr * Pred Grad:  -0.017, New P: 0.693
iter 5 loss: 1.041
Actual params: [0.2729, 0.6928]
-Original Grad: 0.017, -lr * Pred Grad:  -0.002, New P: 0.271
-Original Grad: -0.106, -lr * Pred Grad:  -0.033, New P: 0.660
iter 6 loss: 1.021
Actual params: [0.2709, 0.6595]
-Original Grad: 0.017, -lr * Pred Grad:  0.014, New P: 0.285
-Original Grad: 0.182, -lr * Pred Grad:  0.003, New P: 0.663
iter 7 loss: 1.023
Actual params: [0.285 , 0.6628]
-Original Grad: 0.027, -lr * Pred Grad:  0.034, New P: 0.319
-Original Grad: 0.124, -lr * Pred Grad:  0.020, New P: 0.683
iter 8 loss: 1.032
Actual params: [0.3186, 0.6827]
-Original Grad: 0.051, -lr * Pred Grad:  0.054, New P: 0.373
-Original Grad: 0.061, -lr * Pred Grad:  0.026, New P: 0.708
iter 9 loss: 1.037
Actual params: [0.3728, 0.7084]
-Original Grad: -0.006, -lr * Pred Grad:  0.045, New P: 0.418
-Original Grad: 0.384, -lr * Pred Grad:  0.051, New P: 0.759
iter 10 loss: 1.040
Actual params: [0.4176, 0.7594]
-Original Grad: 0.010, -lr * Pred Grad:  0.046, New P: 0.463
-Original Grad: 0.056, -lr * Pred Grad:  0.051, New P: 0.810
iter 11 loss: 1.036
Actual params: [0.4634, 0.81  ]
-Original Grad: 0.000, -lr * Pred Grad:  0.041, New P: 0.505
-Original Grad: -0.064, -lr * Pred Grad:  0.039, New P: 0.849
iter 12 loss: 1.024
Actual params: [0.5048, 0.8494]
-Original Grad: -0.037, -lr * Pred Grad:  0.014, New P: 0.518
-Original Grad: -0.035, -lr * Pred Grad:  0.032, New P: 0.882
iter 13 loss: 1.013
Actual params: [0.5184, 0.8817]
-Original Grad: -0.028, -lr * Pred Grad:  -0.003, New P: 0.515
-Original Grad: -0.057, -lr * Pred Grad:  0.024, New P: 0.906
iter 14 loss: 1.009
Actual params: [0.5154, 0.9056]
-Original Grad: -0.032, -lr * Pred Grad:  -0.018, New P: 0.497
-Original Grad: -0.018, -lr * Pred Grad:  0.020, New P: 0.926
iter 15 loss: 1.010
Actual params: [0.497 , 0.9256]
-Original Grad: -0.041, -lr * Pred Grad:  -0.034, New P: 0.463
-Original Grad: -0.079, -lr * Pred Grad:  0.011, New P: 0.936
iter 16 loss: 1.019
Actual params: [0.4627, 0.9364]
-Original Grad: -0.049, -lr * Pred Grad:  -0.049, New P: 0.414
-Original Grad: -0.113, -lr * Pred Grad:  -0.000, New P: 0.936
iter 17 loss: 1.030
Actual params: [0.4137, 0.936 ]
-Original Grad: 0.009, -lr * Pred Grad:  -0.041, New P: 0.373
-Original Grad: -0.062, -lr * Pred Grad:  -0.006, New P: 0.930
iter 18 loss: 1.035
Actual params: [0.3731, 0.9302]
-Original Grad: -0.013, -lr * Pred Grad:  -0.042, New P: 0.331
-Original Grad: -0.083, -lr * Pred Grad:  -0.013, New P: 0.917
iter 19 loss: 1.044
Actual params: [0.3311, 0.9175]
-Original Grad: -0.045, -lr * Pred Grad:  -0.054, New P: 0.277
-Original Grad: -0.124, -lr * Pred Grad:  -0.022, New P: 0.895
iter 20 loss: 1.055
Actual params: [0.2772, 0.8954]
-Original Grad: 0.043, -lr * Pred Grad:  -0.030, New P: 0.248
-Original Grad: -0.029, -lr * Pred Grad:  -0.023, New P: 0.873
Target params: [1.1812, 0.2779]
iter 0 loss: 0.595
Actual params: [0.5941, 0.5941]
-Original Grad: 0.133, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.006, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.565
Actual params: [0.6941, 0.4941]
-Original Grad: 0.031, -lr * Pred Grad:  0.082, New P: 0.776
-Original Grad: -0.041, -lr * Pred Grad:  -0.083, New P: 0.411
iter 2 loss: 0.518
Actual params: [0.776 , 0.4113]
-Original Grad: -0.017, -lr * Pred Grad:  0.055, New P: 0.831
-Original Grad: -0.124, -lr * Pred Grad:  -0.081, New P: 0.330
iter 3 loss: 0.501
Actual params: [0.8308, 0.3304]
-Original Grad: -0.047, -lr * Pred Grad:  0.023, New P: 0.854
-Original Grad: -0.033, -lr * Pred Grad:  -0.078, New P: 0.252
iter 4 loss: 0.535
Actual params: [0.8543, 0.2519]
-Original Grad: 0.204, -lr * Pred Grad:  0.056, New P: 0.910
-Original Grad: 0.021, -lr * Pred Grad:  -0.057, New P: 0.195
iter 5 loss: 0.505
Actual params: [0.9102, 0.1946]
-Original Grad: 0.100, -lr * Pred Grad:  0.064, New P: 0.974
-Original Grad: 0.054, -lr * Pred Grad:  -0.027, New P: 0.168
iter 6 loss: 0.480
Actual params: [0.9744, 0.168 ]
-Original Grad: -0.039, -lr * Pred Grad:  0.048, New P: 1.022
-Original Grad: 0.142, -lr * Pred Grad:  0.019, New P: 0.187
iter 7 loss: 0.467
Actual params: [1.0225, 0.1866]
-Original Grad: 0.035, -lr * Pred Grad:  0.048, New P: 1.071
-Original Grad: 0.085, -lr * Pred Grad:  0.034, New P: 0.221
iter 8 loss: 0.456
Actual params: [1.0708, 0.2207]
-Original Grad: 0.002, -lr * Pred Grad:  0.043, New P: 1.114
-Original Grad: -0.066, -lr * Pred Grad:  0.015, New P: 0.236
iter 9 loss: 0.454
Actual params: [1.1141, 0.2358]
-Original Grad: 0.054, -lr * Pred Grad:  0.047, New P: 1.161
-Original Grad: 0.049, -lr * Pred Grad:  0.023, New P: 0.259
iter 10 loss: 0.454
Actual params: [1.1614, 0.2591]
-Original Grad: 0.055, -lr * Pred Grad:  0.051, New P: 1.212
-Original Grad: -0.071, -lr * Pred Grad:  0.006, New P: 0.265
iter 11 loss: 0.456
Actual params: [1.2123, 0.2652]
-Original Grad: -0.011, -lr * Pred Grad:  0.044, New P: 1.256
-Original Grad: 0.207, -lr * Pred Grad:  0.035, New P: 0.300
iter 12 loss: 0.459
Actual params: [1.2562, 0.3004]
-Original Grad: 0.001, -lr * Pred Grad:  0.040, New P: 1.296
-Original Grad: -0.060, -lr * Pred Grad:  0.022, New P: 0.323
iter 13 loss: 0.462
Actual params: [1.2959, 0.3228]
-Original Grad: 0.005, -lr * Pred Grad:  0.037, New P: 1.333
-Original Grad: 0.241, -lr * Pred Grad:  0.045, New P: 0.368
iter 14 loss: 0.462
Actual params: [1.3327, 0.3678]
-Original Grad: 0.010, -lr * Pred Grad:  0.035, New P: 1.368
-Original Grad: -0.219, -lr * Pred Grad:  0.013, New P: 0.381
iter 15 loss: 0.463
Actual params: [1.3676, 0.3806]
-Original Grad: 0.006, -lr * Pred Grad:  0.033, New P: 1.400
-Original Grad: 0.213, -lr * Pred Grad:  0.031, New P: 0.412
iter 16 loss: 0.464
Actual params: [1.4003, 0.4116]
-Original Grad: 0.011, -lr * Pred Grad:  0.031, New P: 1.432
-Original Grad: 0.105, -lr * Pred Grad:  0.038, New P: 0.449
iter 17 loss: 0.470
Actual params: [1.4318, 0.4492]
-Original Grad: 0.005, -lr * Pred Grad:  0.029, New P: 1.461
-Original Grad: 0.259, -lr * Pred Grad:  0.053, New P: 0.502
iter 18 loss: 0.486
Actual params: [1.4612, 0.502 ]
-Original Grad: 0.027, -lr * Pred Grad:  0.031, New P: 1.493
-Original Grad: 0.149, -lr * Pred Grad:  0.059, New P: 0.561
iter 19 loss: 0.514
Actual params: [1.4927, 0.561 ]
-Original Grad: 0.014, -lr * Pred Grad:  0.031, New P: 1.524
-Original Grad: -0.055, -lr * Pred Grad:  0.049, New P: 0.610
iter 20 loss: 0.553
Actual params: [1.5237, 0.6097]
-Original Grad: 0.007, -lr * Pred Grad:  0.029, New P: 1.553
-Original Grad: -0.170, -lr * Pred Grad:  0.029, New P: 0.638
Target params: [1.1812, 0.2779]
iter 0 loss: 0.832
Actual params: [0.5941, 0.5941]
-Original Grad: 0.038, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.013, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.962
Actual params: [0.6941, 0.6941]
-Original Grad: -0.191, -lr * Pred Grad:  -0.060, New P: 0.634
-Original Grad: 0.049, -lr * Pred Grad:  0.089, New P: 0.784
iter 2 loss: 0.948
Actual params: [0.6343, 0.7835]
-Original Grad: -0.187, -lr * Pred Grad:  -0.078, New P: 0.557
-Original Grad: -0.042, -lr * Pred Grad:  0.013, New P: 0.796
iter 3 loss: 0.895
Actual params: [0.5567, 0.7962]
-Original Grad: 0.091, -lr * Pred Grad:  -0.042, New P: 0.515
-Original Grad: -0.017, -lr * Pred Grad:  -0.004, New P: 0.792
iter 4 loss: 0.874
Actual params: [0.515, 0.792]
-Original Grad: -0.096, -lr * Pred Grad:  -0.051, New P: 0.464
-Original Grad: -0.029, -lr * Pred Grad:  -0.025, New P: 0.767
iter 5 loss: 0.847
Actual params: [0.4641, 0.7674]
-Original Grad: -0.038, -lr * Pred Grad:  -0.050, New P: 0.414
-Original Grad: -0.035, -lr * Pred Grad:  -0.042, New P: 0.726
iter 6 loss: 0.820
Actual params: [0.414 , 0.7258]
-Original Grad: 0.035, -lr * Pred Grad:  -0.038, New P: 0.377
-Original Grad: -0.068, -lr * Pred Grad:  -0.060, New P: 0.665
iter 7 loss: 0.800
Actual params: [0.3765, 0.6655]
-Original Grad: 0.062, -lr * Pred Grad:  -0.023, New P: 0.354
-Original Grad: 0.082, -lr * Pred Grad:  -0.012, New P: 0.654
iter 8 loss: 0.794
Actual params: [0.3539, 0.6538]
-Original Grad: -0.028, -lr * Pred Grad:  -0.024, New P: 0.330
-Original Grad: -0.003, -lr * Pred Grad:  -0.012, New P: 0.642
iter 9 loss: 0.786
Actual params: [0.3296, 0.6423]
-Original Grad: 0.067, -lr * Pred Grad:  -0.011, New P: 0.319
-Original Grad: 0.061, -lr * Pred Grad:  0.011, New P: 0.653
iter 10 loss: 0.787
Actual params: [0.3186, 0.6532]
-Original Grad: 0.033, -lr * Pred Grad:  -0.005, New P: 0.314
-Original Grad: 0.062, -lr * Pred Grad:  0.028, New P: 0.681
iter 11 loss: 0.794
Actual params: [0.3138, 0.6808]
-Original Grad: 0.080, -lr * Pred Grad:  0.008, New P: 0.321
-Original Grad: -0.032, -lr * Pred Grad:  0.015, New P: 0.696
iter 12 loss: 0.799
Actual params: [0.3213, 0.6958]
-Original Grad: -0.005, -lr * Pred Grad:  0.006, New P: 0.327
-Original Grad: -0.056, -lr * Pred Grad:  -0.003, New P: 0.693
iter 13 loss: 0.799
Actual params: [0.3273, 0.6931]
-Original Grad: 0.102, -lr * Pred Grad:  0.020, New P: 0.347
-Original Grad: -0.039, -lr * Pred Grad:  -0.013, New P: 0.680
iter 14 loss: 0.800
Actual params: [0.3468, 0.6799]
-Original Grad: 0.032, -lr * Pred Grad:  0.022, New P: 0.369
-Original Grad: -0.034, -lr * Pred Grad:  -0.021, New P: 0.659
iter 15 loss: 0.798
Actual params: [0.3689, 0.6591]
-Original Grad: 0.105, -lr * Pred Grad:  0.033, New P: 0.402
-Original Grad: 0.032, -lr * Pred Grad:  -0.010, New P: 0.649
iter 16 loss: 0.798
Actual params: [0.4022, 0.649 ]
-Original Grad: 0.129, -lr * Pred Grad:  0.045, New P: 0.447
-Original Grad: -0.102, -lr * Pred Grad:  -0.032, New P: 0.617
iter 17 loss: 0.793
Actual params: [0.4473, 0.6168]
-Original Grad: 0.041, -lr * Pred Grad:  0.046, New P: 0.493
-Original Grad: -0.026, -lr * Pred Grad:  -0.035, New P: 0.582
iter 18 loss: 0.789
Actual params: [0.4932, 0.5817]
-Original Grad: -0.078, -lr * Pred Grad:  0.031, New P: 0.524
-Original Grad: 0.020, -lr * Pred Grad:  -0.027, New P: 0.555
iter 19 loss: 0.785
Actual params: [0.5242, 0.5545]
-Original Grad: 0.035, -lr * Pred Grad:  0.033, New P: 0.557
-Original Grad: -0.073, -lr * Pred Grad:  -0.040, New P: 0.515
iter 20 loss: 0.773
Actual params: [0.5568, 0.5146]
-Original Grad: -0.085, -lr * Pred Grad:  0.018, New P: 0.575
-Original Grad: -0.034, -lr * Pred Grad:  -0.044, New P: 0.471
Target params: [1.1812, 0.2779]
iter 0 loss: 0.667
Actual params: [0.5941, 0.5941]
-Original Grad: -0.084, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.193, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.707
Actual params: [0.4941, 0.4941]
-Original Grad: -0.061, -lr * Pred Grad:  -0.098, New P: 0.396
-Original Grad: -0.040, -lr * Pred Grad:  -0.081, New P: 0.413
iter 2 loss: 0.725
Actual params: [0.396 , 0.4133]
-Original Grad: -0.051, -lr * Pred Grad:  -0.096, New P: 0.300
-Original Grad: 0.040, -lr * Pred Grad:  -0.049, New P: 0.365
iter 3 loss: 0.743
Actual params: [0.2998, 0.3647]
-Original Grad: -0.017, -lr * Pred Grad:  -0.086, New P: 0.213
-Original Grad: 0.020, -lr * Pred Grad:  -0.034, New P: 0.331
iter 4 loss: 0.754
Actual params: [0.2134, 0.3308]
-Original Grad: -0.055, -lr * Pred Grad:  -0.089, New P: 0.124
-Original Grad: 0.020, -lr * Pred Grad:  -0.023, New P: 0.308
iter 5 loss: 0.764
Actual params: [0.1241, 0.3078]
-Original Grad: -0.069, -lr * Pred Grad:  -0.093, New P: 0.032
-Original Grad: 0.009, -lr * Pred Grad:  -0.018, New P: 0.290
iter 6 loss: 0.777
Actual params: [0.0315, 0.2902]
-Original Grad: -0.009, -lr * Pred Grad:  -0.084, New P: -0.052
-Original Grad: -0.011, -lr * Pred Grad:  -0.018, New P: 0.272
iter 7 loss: 0.792
Actual params: [-0.0522,  0.2721]
-Original Grad: 0.053, -lr * Pred Grad:  -0.053, New P: -0.105
-Original Grad: -0.004, -lr * Pred Grad:  -0.017, New P: 0.255
iter 8 loss: 0.800
Actual params: [-0.1048,  0.2551]
-Original Grad: 0.053, -lr * Pred Grad:  -0.029, New P: -0.133
-Original Grad: -0.026, -lr * Pred Grad:  -0.021, New P: 0.234
iter 9 loss: 0.804
Actual params: [-0.1333,  0.2338]
-Original Grad: 0.045, -lr * Pred Grad:  -0.012, New P: -0.145
-Original Grad: 0.061, -lr * Pred Grad:  -0.004, New P: 0.229
iter 10 loss: 0.805
Actual params: [-0.1452,  0.2293]
-Original Grad: 0.017, -lr * Pred Grad:  -0.006, New P: -0.151
-Original Grad: 0.022, -lr * Pred Grad:  0.001, New P: 0.230
iter 11 loss: 0.806
Actual params: [-0.151 ,  0.2304]
-Original Grad: 0.059, -lr * Pred Grad:  0.011, New P: -0.140
-Original Grad: 0.019, -lr * Pred Grad:  0.005, New P: 0.236
iter 12 loss: 0.804
Actual params: [-0.1403,  0.2357]
-Original Grad: 0.045, -lr * Pred Grad:  0.021, New P: -0.119
-Original Grad: 0.044, -lr * Pred Grad:  0.014, New P: 0.250
iter 13 loss: 0.802
Actual params: [-0.1193,  0.25  ]
-Original Grad: 0.053, -lr * Pred Grad:  0.031, New P: -0.088
-Original Grad: 0.013, -lr * Pred Grad:  0.016, New P: 0.266
iter 14 loss: 0.797
Actual params: [-0.0878,  0.2658]
-Original Grad: -0.031, -lr * Pred Grad:  0.020, New P: -0.067
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.280
iter 15 loss: 0.794
Actual params: [-0.0675,  0.2802]
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: -0.049
-Original Grad: -0.001, -lr * Pred Grad:  0.013, New P: 0.293
iter 16 loss: 0.791
Actual params: [-0.049,  0.293]
-Original Grad: 0.009, -lr * Pred Grad:  0.019, New P: -0.030
-Original Grad: 0.028, -lr * Pred Grad:  0.018, New P: 0.311
iter 17 loss: 0.786
Actual params: [-0.0298,  0.3109]
-Original Grad: 0.051, -lr * Pred Grad:  0.029, New P: -0.000
-Original Grad: 0.009, -lr * Pred Grad:  0.018, New P: 0.329
iter 18 loss: 0.781
Actual params: [-0.0004,  0.3292]
-Original Grad: 0.100, -lr * Pred Grad:  0.046, New P: 0.046
-Original Grad: -0.013, -lr * Pred Grad:  0.014, New P: 0.343
iter 19 loss: 0.773
Actual params: [0.0459, 0.3429]
-Original Grad: -0.052, -lr * Pred Grad:  0.030, New P: 0.075
-Original Grad: 0.047, -lr * Pred Grad:  0.023, New P: 0.366
iter 20 loss: 0.768
Actual params: [0.0754, 0.3658]
-Original Grad: -0.050, -lr * Pred Grad:  0.015, New P: 0.091
-Original Grad: 0.014, -lr * Pred Grad:  0.024, New P: 0.390
Target params: [1.1812, 0.2779]
iter 0 loss: 0.903
Actual params: [0.5941, 0.5941]
-Original Grad: -0.040, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.164, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.724
Actual params: [0.4941, 0.4941]
-Original Grad: -0.012, -lr * Pred Grad:  -0.086, New P: 0.408
-Original Grad: -0.123, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.603
Actual params: [0.4083, 0.3958]
-Original Grad: -0.038, -lr * Pred Grad:  -0.092, New P: 0.316
-Original Grad: -0.119, -lr * Pred Grad:  -0.098, New P: 0.298
iter 3 loss: 0.517
Actual params: [0.3163, 0.298 ]
-Original Grad: -0.012, -lr * Pred Grad:  -0.086, New P: 0.230
-Original Grad: -0.096, -lr * Pred Grad:  -0.096, New P: 0.202
iter 4 loss: 0.476
Actual params: [0.2303, 0.202 ]
-Original Grad: -0.026, -lr * Pred Grad:  -0.089, New P: 0.142
-Original Grad: -0.067, -lr * Pred Grad:  -0.092, New P: 0.110
iter 5 loss: 0.461
Actual params: [0.1417, 0.1096]
-Original Grad: -0.000, -lr * Pred Grad:  -0.076, New P: 0.065
-Original Grad: -0.077, -lr * Pred Grad:  -0.091, New P: 0.018
iter 6 loss: 0.461
Actual params: [0.0653, 0.0185]
-Original Grad: 0.003, -lr * Pred Grad:  -0.064, New P: 0.001
-Original Grad: 0.007, -lr * Pred Grad:  -0.078, New P: -0.060
iter 7 loss: 0.456
Actual params: [ 0.0011, -0.0597]
-Original Grad: 0.011, -lr * Pred Grad:  -0.048, New P: -0.047
-Original Grad: -0.024, -lr * Pred Grad:  -0.073, New P: -0.133
iter 8 loss: 0.453
Actual params: [-0.0465, -0.1328]
-Original Grad: -0.034, -lr * Pred Grad:  -0.060, New P: -0.107
-Original Grad: -0.019, -lr * Pred Grad:  -0.068, New P: -0.201
iter 9 loss: 0.453
Actual params: [-0.1068, -0.2008]
-Original Grad: 0.031, -lr * Pred Grad:  -0.030, New P: -0.137
-Original Grad: -0.021, -lr * Pred Grad:  -0.064, New P: -0.265
iter 10 loss: 0.454
Actual params: [-0.137, -0.265]
-Original Grad: -0.013, -lr * Pred Grad:  -0.035, New P: -0.172
-Original Grad: -0.018, -lr * Pred Grad:  -0.061, New P: -0.326
iter 11 loss: 0.460
Actual params: [-0.1715, -0.3256]
-Original Grad: -0.025, -lr * Pred Grad:  -0.044, New P: -0.216
-Original Grad: 0.007, -lr * Pred Grad:  -0.053, New P: -0.379
iter 12 loss: 0.473
Actual params: [-0.2157, -0.3789]
-Original Grad: 0.032, -lr * Pred Grad:  -0.020, New P: -0.236
-Original Grad: -0.001, -lr * Pred Grad:  -0.048, New P: -0.427
iter 13 loss: 0.481
Actual params: [-0.236 , -0.4271]
-Original Grad: -0.003, -lr * Pred Grad:  -0.020, New P: -0.256
-Original Grad: 0.029, -lr * Pred Grad:  -0.038, New P: -0.465
iter 14 loss: 0.489
Actual params: [-0.2559, -0.4654]
-Original Grad: 0.022, -lr * Pred Grad:  -0.006, New P: -0.262
-Original Grad: 0.030, -lr * Pred Grad:  -0.029, New P: -0.495
iter 15 loss: 0.495
Actual params: [-0.262 , -0.4947]
-Original Grad: 0.003, -lr * Pred Grad:  -0.004, New P: -0.266
-Original Grad: 0.061, -lr * Pred Grad:  -0.016, New P: -0.510
iter 16 loss: 0.496
Actual params: [-0.2661, -0.5102]
-Original Grad: 0.019, -lr * Pred Grad:  0.006, New P: -0.260
-Original Grad: -0.032, -lr * Pred Grad:  -0.019, New P: -0.530
iter 17 loss: 0.496
Actual params: [-0.2597, -0.5297]
-Original Grad: -0.000, -lr * Pred Grad:  0.006, New P: -0.254
-Original Grad: -0.006, -lr * Pred Grad:  -0.019, New P: -0.549
iter 18 loss: 0.496
Actual params: [-0.2539, -0.5485]
-Original Grad: -0.008, -lr * Pred Grad:  0.001, New P: -0.253
-Original Grad: -0.014, -lr * Pred Grad:  -0.020, New P: -0.568
iter 19 loss: 0.497
Actual params: [-0.253 , -0.5681]
-Original Grad: 0.022, -lr * Pred Grad:  0.012, New P: -0.241
-Original Grad: -0.014, -lr * Pred Grad:  -0.020, New P: -0.588
iter 20 loss: 0.497
Actual params: [-0.2406, -0.5884]
-Original Grad: -0.004, -lr * Pred Grad:  0.009, New P: -0.231
-Original Grad: 0.012, -lr * Pred Grad:  -0.016, New P: -0.605
Target params: [1.1812, 0.2779]
iter 0 loss: 0.874
Actual params: [0.5941, 0.5941]
-Original Grad: 0.062, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.156, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.743
Actual params: [0.6941, 0.4941]
-Original Grad: 0.209, -lr * Pred Grad:  0.090, New P: 0.784
-Original Grad: 0.010, -lr * Pred Grad:  -0.062, New P: 0.432
iter 2 loss: 0.621
Actual params: [0.7844, 0.4319]
-Original Grad: 0.131, -lr * Pred Grad:  0.093, New P: 0.877
-Original Grad: 0.025, -lr * Pred Grad:  -0.037, New P: 0.395
iter 3 loss: 0.484
Actual params: [0.8771, 0.3946]
-Original Grad: -0.042, -lr * Pred Grad:  0.065, New P: 0.942
-Original Grad: -0.063, -lr * Pred Grad:  -0.050, New P: 0.345
iter 4 loss: 0.430
Actual params: [0.9425, 0.3449]
-Original Grad: 0.015, -lr * Pred Grad:  0.058, New P: 1.001
-Original Grad: -0.041, -lr * Pred Grad:  -0.054, New P: 0.291
iter 5 loss: 0.416
Actual params: [1.0008, 0.2912]
-Original Grad: -0.043, -lr * Pred Grad:  0.041, New P: 1.042
-Original Grad: -0.101, -lr * Pred Grad:  -0.066, New P: 0.225
iter 6 loss: 0.426
Actual params: [1.0418, 0.225 ]
-Original Grad: -0.028, -lr * Pred Grad:  0.030, New P: 1.072
-Original Grad: 0.011, -lr * Pred Grad:  -0.055, New P: 0.170
iter 7 loss: 0.435
Actual params: [1.0721, 0.1701]
-Original Grad: -0.024, -lr * Pred Grad:  0.022, New P: 1.094
-Original Grad: -0.004, -lr * Pred Grad:  -0.049, New P: 0.121
iter 8 loss: 0.445
Actual params: [1.0941, 0.1207]
-Original Grad: -0.265, -lr * Pred Grad:  -0.021, New P: 1.073
-Original Grad: 0.049, -lr * Pred Grad:  -0.031, New P: 0.090
iter 9 loss: 0.448
Actual params: [1.0732, 0.0896]
-Original Grad: -0.190, -lr * Pred Grad:  -0.039, New P: 1.035
-Original Grad: -0.013, -lr * Pred Grad:  -0.031, New P: 0.059
iter 10 loss: 0.452
Actual params: [1.0346, 0.0588]
-Original Grad: -0.091, -lr * Pred Grad:  -0.044, New P: 0.991
-Original Grad: 0.010, -lr * Pred Grad:  -0.025, New P: 0.033
iter 11 loss: 0.469
Actual params: [0.9907, 0.0335]
-Original Grad: -0.077, -lr * Pred Grad:  -0.047, New P: 0.943
-Original Grad: -0.055, -lr * Pred Grad:  -0.034, New P: -0.001
iter 12 loss: 0.503
Actual params: [ 9.4328e-01, -8.3940e-04]
-Original Grad: -0.058, -lr * Pred Grad:  -0.049, New P: 0.895
-Original Grad: -0.050, -lr * Pred Grad:  -0.041, New P: -0.042
iter 13 loss: 0.553
Actual params: [ 0.8946, -0.0419]
-Original Grad: 0.006, -lr * Pred Grad:  -0.043, New P: 0.851
-Original Grad: -0.005, -lr * Pred Grad:  -0.038, New P: -0.080
iter 14 loss: 0.607
Actual params: [ 0.8512, -0.0801]
-Original Grad: 0.171, -lr * Pred Grad:  -0.019, New P: 0.832
-Original Grad: 0.041, -lr * Pred Grad:  -0.025, New P: -0.105
iter 15 loss: 0.632
Actual params: [ 0.8324, -0.1053]
-Original Grad: -0.013, -lr * Pred Grad:  -0.018, New P: 0.814
-Original Grad: -0.012, -lr * Pred Grad:  -0.025, New P: -0.131
iter 16 loss: 0.658
Actual params: [ 0.8139, -0.1308]
-Original Grad: 0.127, -lr * Pred Grad:  -0.003, New P: 0.811
-Original Grad: -0.018, -lr * Pred Grad:  -0.027, New P: -0.158
iter 17 loss: 0.666
Actual params: [ 0.8106, -0.1577]
-Original Grad: 0.092, -lr * Pred Grad:  0.006, New P: 0.817
-Original Grad: -0.008, -lr * Pred Grad:  -0.026, New P: -0.184
iter 18 loss: 0.661
Actual params: [ 0.8169, -0.1839]
-Original Grad: 0.108, -lr * Pred Grad:  0.016, New P: 0.833
-Original Grad: 0.011, -lr * Pred Grad:  -0.021, New P: -0.205
iter 19 loss: 0.643
Actual params: [ 0.8332, -0.2052]
-Original Grad: 0.034, -lr * Pred Grad:  0.018, New P: 0.851
-Original Grad: 0.033, -lr * Pred Grad:  -0.012, New P: -0.217
iter 20 loss: 0.626
Actual params: [ 0.8514, -0.2171]
-Original Grad: 0.263, -lr * Pred Grad:  0.038, New P: 0.890
-Original Grad: 0.035, -lr * Pred Grad:  -0.003, New P: -0.220
Target params: [1.1812, 0.2779]
iter 0 loss: 0.447
Actual params: [0.5941, 0.5941]
-Original Grad: -0.096, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.027, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.440
Actual params: [0.4941, 0.4941]
-Original Grad: 0.024, -lr * Pred Grad:  -0.047, New P: 0.447
-Original Grad: 0.027, -lr * Pred Grad:  0.005, New P: 0.499
iter 2 loss: 0.444
Actual params: [0.4474, 0.4994]
-Original Grad: -0.027, -lr * Pred Grad:  -0.052, New P: 0.396
-Original Grad: 0.123, -lr * Pred Grad:  0.062, New P: 0.562
iter 3 loss: 0.441
Actual params: [0.3956, 0.5616]
-Original Grad: 0.017, -lr * Pred Grad:  -0.033, New P: 0.363
-Original Grad: -0.099, -lr * Pred Grad:  0.005, New P: 0.567
iter 4 loss: 0.443
Actual params: [0.3631, 0.5665]
-Original Grad: -0.021, -lr * Pred Grad:  -0.038, New P: 0.325
-Original Grad: -0.002, -lr * Pred Grad:  0.003, New P: 0.570
iter 5 loss: 0.444
Actual params: [0.3251, 0.57  ]
-Original Grad: -0.012, -lr * Pred Grad:  -0.038, New P: 0.287
-Original Grad: -0.048, -lr * Pred Grad:  -0.012, New P: 0.558
iter 6 loss: 0.447
Actual params: [0.2869, 0.5582]
-Original Grad: -0.034, -lr * Pred Grad:  -0.047, New P: 0.240
-Original Grad: -0.022, -lr * Pred Grad:  -0.017, New P: 0.542
iter 7 loss: 0.450
Actual params: [0.2398, 0.5415]
-Original Grad: -0.028, -lr * Pred Grad:  -0.052, New P: 0.188
-Original Grad: -0.020, -lr * Pred Grad:  -0.020, New P: 0.521
iter 8 loss: 0.456
Actual params: [0.1876, 0.521 ]
-Original Grad: -0.054, -lr * Pred Grad:  -0.063, New P: 0.125
-Original Grad: 0.070, -lr * Pred Grad:  0.002, New P: 0.523
iter 9 loss: 0.454
Actual params: [0.1247, 0.5228]
-Original Grad: -0.008, -lr * Pred Grad:  -0.059, New P: 0.066
-Original Grad: 0.038, -lr * Pred Grad:  0.011, New P: 0.534
iter 10 loss: 0.449
Actual params: [0.0655, 0.5342]
-Original Grad: -0.005, -lr * Pred Grad:  -0.055, New P: 0.011
-Original Grad: 0.006, -lr * Pred Grad:  0.012, New P: 0.546
iter 11 loss: 0.442
Actual params: [0.0106, 0.5461]
-Original Grad: -0.035, -lr * Pred Grad:  -0.060, New P: -0.050
-Original Grad: -0.003, -lr * Pred Grad:  0.010, New P: 0.556
iter 12 loss: 0.436
Actual params: [-0.0499,  0.5558]
-Original Grad: -0.050, -lr * Pred Grad:  -0.068, New P: -0.118
-Original Grad: 0.153, -lr * Pred Grad:  0.037, New P: 0.593
iter 13 loss: 0.423
Actual params: [-0.1179,  0.593 ]
-Original Grad: -0.026, -lr * Pred Grad:  -0.069, New P: -0.187
-Original Grad: 0.012, -lr * Pred Grad:  0.036, New P: 0.629
iter 14 loss: 0.417
Actual params: [-0.187,  0.629]
-Original Grad: -0.021, -lr * Pred Grad:  -0.069, New P: -0.256
-Original Grad: -0.027, -lr * Pred Grad:  0.027, New P: 0.656
iter 15 loss: 0.414
Actual params: [-0.256,  0.656]
-Original Grad: -0.016, -lr * Pred Grad:  -0.068, New P: -0.324
-Original Grad: 0.054, -lr * Pred Grad:  0.034, New P: 0.690
iter 16 loss: 0.415
Actual params: [-0.3235,  0.6904]
-Original Grad: -0.013, -lr * Pred Grad:  -0.065, New P: -0.389
-Original Grad: 0.098, -lr * Pred Grad:  0.047, New P: 0.737
iter 17 loss: 0.421
Actual params: [-0.3889,  0.7374]
-Original Grad: 0.002, -lr * Pred Grad:  -0.059, New P: -0.448
-Original Grad: -0.016, -lr * Pred Grad:  0.040, New P: 0.777
iter 18 loss: 0.427
Actual params: [-0.4476,  0.7771]
-Original Grad: -0.015, -lr * Pred Grad:  -0.058, New P: -0.506
-Original Grad: -0.072, -lr * Pred Grad:  0.022, New P: 0.799
iter 19 loss: 0.429
Actual params: [-0.5058,  0.7992]
-Original Grad: 0.004, -lr * Pred Grad:  -0.051, New P: -0.557
-Original Grad: 0.126, -lr * Pred Grad:  0.039, New P: 0.838
iter 20 loss: 0.435
Actual params: [-0.5572,  0.8384]
-Original Grad: -0.014, -lr * Pred Grad:  -0.051, New P: -0.608
-Original Grad: 0.222, -lr * Pred Grad:  0.059, New P: 0.897
Target params: [1.1812, 0.2779]
iter 0 loss: 0.409
Actual params: [0.5941, 0.5941]
-Original Grad: 0.205, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.110, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.298
Actual params: [0.6941, 0.4941]
-Original Grad: -0.075, -lr * Pred Grad:  0.037, New P: 0.731
-Original Grad: -0.203, -lr * Pred Grad:  -0.097, New P: 0.397
iter 2 loss: 0.285
Actual params: [0.7315, 0.3967]
-Original Grad: -0.003, -lr * Pred Grad:  0.028, New P: 0.760
-Original Grad: -0.153, -lr * Pred Grad:  -0.098, New P: 0.299
iter 3 loss: 0.288
Actual params: [0.7597, 0.2986]
-Original Grad: 0.005, -lr * Pred Grad:  0.025, New P: 0.784
-Original Grad: -0.188, -lr * Pred Grad:  -0.099, New P: 0.200
iter 4 loss: 0.286
Actual params: [0.7842, 0.1995]
-Original Grad: 0.103, -lr * Pred Grad:  0.042, New P: 0.826
-Original Grad: -0.061, -lr * Pred Grad:  -0.092, New P: 0.107
iter 5 loss: 0.269
Actual params: [0.8262, 0.1073]
-Original Grad: 0.029, -lr * Pred Grad:  0.042, New P: 0.868
-Original Grad: -0.083, -lr * Pred Grad:  -0.090, New P: 0.018
iter 6 loss: 0.261
Actual params: [0.8685, 0.0177]
-Original Grad: 0.013, -lr * Pred Grad:  0.040, New P: 0.908
-Original Grad: -0.039, -lr * Pred Grad:  -0.083, New P: -0.066
iter 7 loss: 0.257
Actual params: [ 0.908 , -0.0657]
-Original Grad: 0.057, -lr * Pred Grad:  0.045, New P: 0.953
-Original Grad: 0.013, -lr * Pred Grad:  -0.072, New P: -0.137
iter 8 loss: 0.257
Actual params: [ 0.9533, -0.1373]
-Original Grad: 0.004, -lr * Pred Grad:  0.041, New P: 0.994
-Original Grad: 0.021, -lr * Pred Grad:  -0.061, New P: -0.198
iter 9 loss: 0.261
Actual params: [ 0.9944, -0.1979]
-Original Grad: -0.003, -lr * Pred Grad:  0.036, New P: 1.031
-Original Grad: 0.014, -lr * Pred Grad:  -0.052, New P: -0.250
iter 10 loss: 0.276
Actual params: [ 1.0306, -0.2499]
-Original Grad: -0.017, -lr * Pred Grad:  0.029, New P: 1.060
-Original Grad: -0.007, -lr * Pred Grad:  -0.048, New P: -0.297
iter 11 loss: 0.297
Actual params: [ 1.0596, -0.2975]
-Original Grad: -0.000, -lr * Pred Grad:  0.026, New P: 1.086
-Original Grad: -0.056, -lr * Pred Grad:  -0.050, New P: -0.347
iter 12 loss: 0.317
Actual params: [ 1.0856, -0.3473]
-Original Grad: -0.088, -lr * Pred Grad:  0.006, New P: 1.092
-Original Grad: 0.061, -lr * Pred Grad:  -0.036, New P: -0.383
iter 13 loss: 0.328
Actual params: [ 1.0917, -0.3834]
-Original Grad: -0.043, -lr * Pred Grad:  -0.002, New P: 1.089
-Original Grad: 0.004, -lr * Pred Grad:  -0.032, New P: -0.416
iter 14 loss: 0.336
Actual params: [ 1.0893, -0.4156]
-Original Grad: -0.008, -lr * Pred Grad:  -0.004, New P: 1.086
-Original Grad: -0.039, -lr * Pred Grad:  -0.034, New P: -0.450
iter 15 loss: 0.345
Actual params: [ 1.0857, -0.4497]
-Original Grad: -0.024, -lr * Pred Grad:  -0.008, New P: 1.078
-Original Grad: 0.013, -lr * Pred Grad:  -0.029, New P: -0.479
iter 16 loss: 0.353
Actual params: [ 1.078 , -0.4788]
-Original Grad: 0.023, -lr * Pred Grad:  -0.003, New P: 1.075
-Original Grad: -0.104, -lr * Pred Grad:  -0.039, New P: -0.518
iter 17 loss: 0.364
Actual params: [ 1.0753, -0.5177]
-Original Grad: -0.097, -lr * Pred Grad:  -0.019, New P: 1.056
-Original Grad: 0.034, -lr * Pred Grad:  -0.031, New P: -0.549
iter 18 loss: 0.374
Actual params: [ 1.0561, -0.5485]
-Original Grad: 0.002, -lr * Pred Grad:  -0.017, New P: 1.039
-Original Grad: -0.026, -lr * Pred Grad:  -0.031, New P: -0.580
iter 19 loss: 0.384
Actual params: [ 1.039 , -0.5798]
-Original Grad: 0.013, -lr * Pred Grad:  -0.013, New P: 1.026
-Original Grad: -0.004, -lr * Pred Grad:  -0.029, New P: -0.609
iter 20 loss: 0.395
Actual params: [ 1.0257, -0.6088]
-Original Grad: -0.027, -lr * Pred Grad:  -0.017, New P: 1.009
-Original Grad: 0.078, -lr * Pred Grad:  -0.016, New P: -0.624
Target params: [1.1812, 0.2779]
iter 0 loss: 0.798
Actual params: [0.5941, 0.5941]
-Original Grad: -0.030, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.213, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.776
Actual params: [0.4941, 0.4941]
-Original Grad: -0.010, -lr * Pred Grad:  -0.088, New P: 0.406
-Original Grad: -0.038, -lr * Pred Grad:  -0.079, New P: 0.415
iter 2 loss: 0.774
Actual params: [0.4061, 0.415 ]
-Original Grad: -0.004, -lr * Pred Grad:  -0.076, New P: 0.330
-Original Grad: -0.027, -lr * Pred Grad:  -0.069, New P: 0.346
iter 3 loss: 0.775
Actual params: [0.33  , 0.3464]
-Original Grad: 0.028, -lr * Pred Grad:  -0.009, New P: 0.321
-Original Grad: -0.058, -lr * Pred Grad:  -0.069, New P: 0.277
iter 4 loss: 0.770
Actual params: [0.3214, 0.277 ]
-Original Grad: 0.036, -lr * Pred Grad:  0.030, New P: 0.351
-Original Grad: 0.013, -lr * Pred Grad:  -0.055, New P: 0.222
iter 5 loss: 0.761
Actual params: [0.3513, 0.2217]
-Original Grad: 0.071, -lr * Pred Grad:  0.057, New P: 0.408
-Original Grad: 0.005, -lr * Pred Grad:  -0.046, New P: 0.175
iter 6 loss: 0.748
Actual params: [0.4083, 0.1752]
-Original Grad: 0.049, -lr * Pred Grad:  0.068, New P: 0.476
-Original Grad: -0.040, -lr * Pred Grad:  -0.049, New P: 0.127
iter 7 loss: 0.733
Actual params: [0.4763, 0.1265]
-Original Grad: 0.216, -lr * Pred Grad:  0.070, New P: 0.547
-Original Grad: 0.007, -lr * Pred Grad:  -0.041, New P: 0.085
iter 8 loss: 0.715
Actual params: [0.5468, 0.0851]
-Original Grad: 0.105, -lr * Pred Grad:  0.077, New P: 0.624
-Original Grad: -0.012, -lr * Pred Grad:  -0.039, New P: 0.046
iter 9 loss: 0.692
Actual params: [0.6238, 0.0459]
-Original Grad: 0.149, -lr * Pred Grad:  0.084, New P: 0.708
-Original Grad: -0.074, -lr * Pred Grad:  -0.048, New P: -0.002
iter 10 loss: 0.657
Actual params: [ 0.7075, -0.0023]
-Original Grad: 0.136, -lr * Pred Grad:  0.088, New P: 0.796
-Original Grad: -0.045, -lr * Pred Grad:  -0.051, New P: -0.054
iter 11 loss: 0.612
Actual params: [ 0.7957, -0.0536]
-Original Grad: 0.541, -lr * Pred Grad:  0.082, New P: 0.878
-Original Grad: 0.004, -lr * Pred Grad:  -0.045, New P: -0.099
iter 12 loss: 0.554
Actual params: [ 0.8781, -0.0989]
-Original Grad: 0.271, -lr * Pred Grad:  0.087, New P: 0.965
-Original Grad: -0.083, -lr * Pred Grad:  -0.054, New P: -0.153
iter 13 loss: 0.544
Actual params: [ 0.9654, -0.1531]
-Original Grad: 0.041, -lr * Pred Grad:  0.082, New P: 1.047
-Original Grad: -0.027, -lr * Pred Grad:  -0.054, New P: -0.207
iter 14 loss: 0.555
Actual params: [ 1.047 , -0.2069]
-Original Grad: 0.194, -lr * Pred Grad:  0.084, New P: 1.131
-Original Grad: 0.031, -lr * Pred Grad:  -0.042, New P: -0.249
iter 15 loss: 0.545
Actual params: [ 1.1312, -0.2493]
-Original Grad: 0.086, -lr * Pred Grad:  0.082, New P: 1.213
-Original Grad: 0.064, -lr * Pred Grad:  -0.026, New P: -0.275
iter 16 loss: 0.514
Actual params: [ 1.2128, -0.2751]
-Original Grad: 0.001, -lr * Pred Grad:  0.074, New P: 1.287
-Original Grad: 0.133, -lr * Pred Grad:  0.001, New P: -0.274
iter 17 loss: 0.477
Actual params: [ 1.2869, -0.2742]
-Original Grad: -0.327, -lr * Pred Grad:  0.041, New P: 1.328
-Original Grad: 0.250, -lr * Pred Grad:  0.032, New P: -0.242
iter 18 loss: 0.455
Actual params: [ 1.3276, -0.2418]
-Original Grad: -0.008, -lr * Pred Grad:  0.036, New P: 1.364
-Original Grad: 0.086, -lr * Pred Grad:  0.040, New P: -0.202
iter 19 loss: 0.441
Actual params: [ 1.3641, -0.2022]
-Original Grad: -0.046, -lr * Pred Grad:  0.030, New P: 1.394
-Original Grad: 0.268, -lr * Pred Grad:  0.058, New P: -0.144
iter 20 loss: 0.431
Actual params: [ 1.3943, -0.1441]
-Original Grad: -0.070, -lr * Pred Grad:  0.023, New P: 1.417
-Original Grad: 0.264, -lr * Pred Grad:  0.071, New P: -0.073
Target params: [1.1812, 0.2779]
iter 0 loss: 0.910
Actual params: [0.5941, 0.5941]
-Original Grad: -0.013, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.129, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.765
Actual params: [0.4941, 0.4941]
-Original Grad: -0.071, -lr * Pred Grad:  -0.086, New P: 0.408
-Original Grad: 0.189, -lr * Pred Grad:  0.024, New P: 0.518
iter 2 loss: 0.832
Actual params: [0.4085, 0.5179]
-Original Grad: 0.016, -lr * Pred Grad:  -0.051, New P: 0.358
-Original Grad: -0.041, -lr * Pred Grad:  0.007, New P: 0.525
iter 3 loss: 0.856
Actual params: [0.3578, 0.5247]
-Original Grad: -0.034, -lr * Pred Grad:  -0.062, New P: 0.296
-Original Grad: -0.118, -lr * Pred Grad:  -0.021, New P: 0.503
iter 4 loss: 0.831
Actual params: [0.2959, 0.5033]
-Original Grad: -0.012, -lr * Pred Grad:  -0.060, New P: 0.236
-Original Grad: -0.239, -lr * Pred Grad:  -0.050, New P: 0.453
iter 5 loss: 0.760
Actual params: [0.2362, 0.4531]
-Original Grad: -0.081, -lr * Pred Grad:  -0.073, New P: 0.163
-Original Grad: 0.144, -lr * Pred Grad:  -0.020, New P: 0.433
iter 6 loss: 0.737
Actual params: [0.1629, 0.4327]
-Original Grad: -0.086, -lr * Pred Grad:  -0.082, New P: 0.081
-Original Grad: 0.046, -lr * Pred Grad:  -0.012, New P: 0.421
iter 7 loss: 0.728
Actual params: [0.0813, 0.4212]
-Original Grad: -0.022, -lr * Pred Grad:  -0.079, New P: 0.003
-Original Grad: -0.035, -lr * Pred Grad:  -0.015, New P: 0.407
iter 8 loss: 0.710
Actual params: [0.0027, 0.4065]
-Original Grad: -0.027, -lr * Pred Grad:  -0.078, New P: -0.075
-Original Grad: -0.116, -lr * Pred Grad:  -0.027, New P: 0.380
iter 9 loss: 0.682
Actual params: [-0.0748,  0.38  ]
-Original Grad: -0.006, -lr * Pred Grad:  -0.071, New P: -0.146
-Original Grad: -0.205, -lr * Pred Grad:  -0.043, New P: 0.337
iter 10 loss: 0.650
Actual params: [-0.1457,  0.3369]
-Original Grad: 0.005, -lr * Pred Grad:  -0.062, New P: -0.208
-Original Grad: -0.030, -lr * Pred Grad:  -0.042, New P: 0.295
iter 11 loss: 0.628
Actual params: [-0.2076,  0.2951]
-Original Grad: -0.012, -lr * Pred Grad:  -0.059, New P: -0.267
-Original Grad: -0.024, -lr * Pred Grad:  -0.040, New P: 0.255
iter 12 loss: 0.605
Actual params: [-0.267,  0.255]
-Original Grad: -0.041, -lr * Pred Grad:  -0.064, New P: -0.331
-Original Grad: -0.091, -lr * Pred Grad:  -0.045, New P: 0.210
iter 13 loss: 0.586
Actual params: [-0.3315,  0.2101]
-Original Grad: -0.013, -lr * Pred Grad:  -0.062, New P: -0.394
-Original Grad: -0.077, -lr * Pred Grad:  -0.048, New P: 0.162
iter 14 loss: 0.571
Actual params: [-0.3936,  0.1622]
-Original Grad: 0.005, -lr * Pred Grad:  -0.054, New P: -0.448
-Original Grad: -0.097, -lr * Pred Grad:  -0.052, New P: 0.110
iter 15 loss: 0.557
Actual params: [-0.448 ,  0.1098]
-Original Grad: -0.016, -lr * Pred Grad:  -0.054, New P: -0.502
-Original Grad: -0.082, -lr * Pred Grad:  -0.055, New P: 0.055
iter 16 loss: 0.539
Actual params: [-0.5022,  0.0546]
-Original Grad: -0.022, -lr * Pred Grad:  -0.056, New P: -0.558
-Original Grad: -0.090, -lr * Pred Grad:  -0.058, New P: -0.004
iter 17 loss: 0.522
Actual params: [-0.558 , -0.0035]
-Original Grad: -0.035, -lr * Pred Grad:  -0.060, New P: -0.618
-Original Grad: -0.057, -lr * Pred Grad:  -0.058, New P: -0.062
iter 18 loss: 0.511
Actual params: [-0.6182, -0.0617]
-Original Grad: -0.042, -lr * Pred Grad:  -0.066, New P: -0.684
-Original Grad: -0.078, -lr * Pred Grad:  -0.060, New P: -0.122
iter 19 loss: 0.491
Actual params: [-0.6838, -0.1218]
-Original Grad: -0.035, -lr * Pred Grad:  -0.069, New P: -0.753
-Original Grad: -0.072, -lr * Pred Grad:  -0.061, New P: -0.183
iter 20 loss: 0.466
Actual params: [-0.7528, -0.1831]
-Original Grad: -0.003, -lr * Pred Grad:  -0.064, New P: -0.816
-Original Grad: 0.027, -lr * Pred Grad:  -0.053, New P: -0.236
Target params: [1.1812, 0.2779]
iter 0 loss: 0.374
Actual params: [0.5941, 0.5941]
-Original Grad: 0.093, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.081, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.461
Actual params: [0.6941, 0.6941]
-Original Grad: -0.045, -lr * Pred Grad:  0.027, New P: 0.721
-Original Grad: 0.382, -lr * Pred Grad:  0.087, New P: 0.781
iter 2 loss: 1.026
Actual params: [0.7214, 0.7807]
-Original Grad: -0.188, -lr * Pred Grad:  -0.046, New P: 0.676
-Original Grad: -0.215, -lr * Pred Grad:  0.028, New P: 0.809
iter 3 loss: 0.352
Actual params: [0.6755, 0.8086]
-Original Grad: -0.248, -lr * Pred Grad:  -0.069, New P: 0.607
-Original Grad: 1.239, -lr * Pred Grad:  0.062, New P: 0.871
iter 4 loss: 0.404
Actual params: [0.607 , 0.8709]
-Original Grad: -0.005, -lr * Pred Grad:  -0.059, New P: 0.548
-Original Grad: -0.126, -lr * Pred Grad:  0.047, New P: 0.918
iter 5 loss: 0.545
Actual params: [0.5482, 0.9183]
-Original Grad: -0.171, -lr * Pred Grad:  -0.069, New P: 0.479
-Original Grad: 0.349, -lr * Pred Grad:  0.053, New P: 0.971
iter 6 loss: 0.701
Actual params: [0.4791, 0.971 ]
-Original Grad: -0.017, -lr * Pred Grad:  -0.063, New P: 0.417
-Original Grad: 0.007, -lr * Pred Grad:  0.046, New P: 1.017
iter 7 loss: 0.810
Actual params: [0.4166, 1.0173]
-Original Grad: -0.012, -lr * Pred Grad:  -0.057, New P: 0.360
-Original Grad: -0.150, -lr * Pred Grad:  0.035, New P: 1.053
iter 8 loss: 0.877
Actual params: [0.3598, 1.0525]
-Original Grad: -0.091, -lr * Pred Grad:  -0.061, New P: 0.299
-Original Grad: 0.114, -lr * Pred Grad:  0.035, New P: 1.088
iter 9 loss: 0.940
Actual params: [0.2993, 1.0877]
-Original Grad: -0.108, -lr * Pred Grad:  -0.065, New P: 0.234
-Original Grad: -0.026, -lr * Pred Grad:  0.030, New P: 1.118
iter 10 loss: 0.989
Actual params: [0.234 , 1.1182]
-Original Grad: -0.084, -lr * Pred Grad:  -0.067, New P: 0.167
-Original Grad: 0.011, -lr * Pred Grad:  0.028, New P: 1.146
iter 11 loss: 1.026
Actual params: [0.1668, 1.1458]
-Original Grad: -0.064, -lr * Pred Grad:  -0.067, New P: 0.100
-Original Grad: 0.024, -lr * Pred Grad:  0.026, New P: 1.172
iter 12 loss: 1.060
Actual params: [0.0995, 1.1716]
-Original Grad: -0.100, -lr * Pred Grad:  -0.070, New P: 0.029
-Original Grad: 0.025, -lr * Pred Grad:  0.024, New P: 1.196
iter 13 loss: 1.086
Actual params: [0.0292, 1.1957]
-Original Grad: 0.051, -lr * Pred Grad:  -0.057, New P: -0.028
-Original Grad: -0.000, -lr * Pred Grad:  0.022, New P: 1.217
iter 14 loss: 1.111
Actual params: [-0.0282,  1.2174]
-Original Grad: 0.168, -lr * Pred Grad:  -0.030, New P: -0.059
-Original Grad: -0.025, -lr * Pred Grad:  0.019, New P: 1.236
iter 15 loss: 1.123
Actual params: [-0.0585,  1.2362]
-Original Grad: 0.250, -lr * Pred Grad:  -0.001, New P: -0.059
-Original Grad: -0.044, -lr * Pred Grad:  0.015, New P: 1.252
iter 16 loss: 1.124
Actual params: [-0.059 ,  1.2516]
-Original Grad: 0.195, -lr * Pred Grad:  0.017, New P: -0.042
-Original Grad: -0.070, -lr * Pred Grad:  0.011, New P: 1.263
iter 17 loss: 1.119
Actual params: [-0.0421,  1.2631]
-Original Grad: 0.023, -lr * Pred Grad:  0.017, New P: -0.025
-Original Grad: 0.016, -lr * Pred Grad:  0.011, New P: 1.274
iter 18 loss: 1.113
Actual params: [-0.0247,  1.274 ]
-Original Grad: 0.095, -lr * Pred Grad:  0.024, New P: -0.001
-Original Grad: 0.020, -lr * Pred Grad:  0.011, New P: 1.285
iter 19 loss: 1.104
Actual params: [-6.4221e-04,  1.2848e+00]
-Original Grad: 0.275, -lr * Pred Grad:  0.042, New P: 0.041
-Original Grad: -0.044, -lr * Pred Grad:  0.008, New P: 1.293
iter 20 loss: 1.089
Actual params: [0.0413, 1.2929]
-Original Grad: 0.190, -lr * Pred Grad:  0.051, New P: 0.093
-Original Grad: -0.076, -lr * Pred Grad:  0.005, New P: 1.297
Target params: [1.1812, 0.2779]
iter 0 loss: 0.660
Actual params: [0.5941, 0.5941]
-Original Grad: 0.095, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.087, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.492
Actual params: [0.6941, 0.4941]
-Original Grad: 0.017, -lr * Pred Grad:  0.079, New P: 0.773
-Original Grad: -0.033, -lr * Pred Grad:  -0.089, New P: 0.405
iter 2 loss: 0.410
Actual params: [0.773 , 0.4049]
-Original Grad: -0.042, -lr * Pred Grad:  0.030, New P: 0.803
-Original Grad: -0.059, -lr * Pred Grad:  -0.092, New P: 0.312
iter 3 loss: 0.386
Actual params: [0.8034, 0.3125]
-Original Grad: -0.005, -lr * Pred Grad:  0.022, New P: 0.825
-Original Grad: -0.007, -lr * Pred Grad:  -0.079, New P: 0.233
iter 4 loss: 0.381
Actual params: [0.8255, 0.2334]
-Original Grad: 0.003, -lr * Pred Grad:  0.020, New P: 0.845
-Original Grad: -0.023, -lr * Pred Grad:  -0.077, New P: 0.157
iter 5 loss: 0.372
Actual params: [0.8454, 0.1567]
-Original Grad: 0.027, -lr * Pred Grad:  0.030, New P: 0.875
-Original Grad: -0.064, -lr * Pred Grad:  -0.083, New P: 0.073
iter 6 loss: 0.365
Actual params: [0.8753, 0.0734]
-Original Grad: 0.002, -lr * Pred Grad:  0.027, New P: 0.902
-Original Grad: -0.019, -lr * Pred Grad:  -0.079, New P: -0.006
iter 7 loss: 0.354
Actual params: [ 0.9021, -0.0057]
-Original Grad: -0.020, -lr * Pred Grad:  0.014, New P: 0.916
-Original Grad: 0.009, -lr * Pred Grad:  -0.066, New P: -0.072
iter 8 loss: 0.348
Actual params: [ 0.9164, -0.0718]
-Original Grad: -0.042, -lr * Pred Grad:  -0.006, New P: 0.911
-Original Grad: -0.032, -lr * Pred Grad:  -0.069, New P: -0.140
iter 9 loss: 0.344
Actual params: [ 0.9108, -0.1404]
-Original Grad: -0.023, -lr * Pred Grad:  -0.014, New P: 0.897
-Original Grad: -0.073, -lr * Pred Grad:  -0.077, New P: -0.217
iter 10 loss: 0.339
Actual params: [ 0.8967, -0.2173]
-Original Grad: -0.017, -lr * Pred Grad:  -0.019, New P: 0.877
-Original Grad: -0.067, -lr * Pred Grad:  -0.082, New P: -0.300
iter 11 loss: 0.333
Actual params: [ 0.8775, -0.2997]
-Original Grad: 0.036, -lr * Pred Grad:  -0.003, New P: 0.875
-Original Grad: -0.052, -lr * Pred Grad:  -0.085, New P: -0.385
iter 12 loss: 0.321
Actual params: [ 0.8746, -0.3847]
-Original Grad: -0.022, -lr * Pred Grad:  -0.011, New P: 0.864
-Original Grad: -0.046, -lr * Pred Grad:  -0.086, New P: -0.471
iter 13 loss: 0.321
Actual params: [ 0.8638, -0.4711]
-Original Grad: -0.062, -lr * Pred Grad:  -0.030, New P: 0.834
-Original Grad: -0.086, -lr * Pred Grad:  -0.091, New P: -0.562
iter 14 loss: 0.316
Actual params: [ 0.834 , -0.5624]
-Original Grad: -0.058, -lr * Pred Grad:  -0.043, New P: 0.791
-Original Grad: -0.066, -lr * Pred Grad:  -0.094, New P: -0.656
iter 15 loss: 0.301
Actual params: [ 0.7906, -0.656 ]
-Original Grad: -0.102, -lr * Pred Grad:  -0.060, New P: 0.731
-Original Grad: -0.077, -lr * Pred Grad:  -0.096, New P: -0.752
iter 16 loss: 1.143
Actual params: [ 0.7309, -0.7524]
-Original Grad: -0.070, -lr * Pred Grad:  -0.068, New P: 0.663
-Original Grad: 0.178, -lr * Pred Grad:  -0.038, New P: -0.790
iter 17 loss: 1.146
Actual params: [ 0.6627, -0.7904]
-Original Grad: -0.265, -lr * Pred Grad:  -0.077, New P: 0.586
-Original Grad: -1.065, -lr * Pred Grad:  -0.057, New P: -0.847
iter 18 loss: 1.144
Actual params: [ 0.5859, -0.8473]
-Original Grad: -0.213, -lr * Pred Grad:  -0.086, New P: 0.500
-Original Grad: -0.683, -lr * Pred Grad:  -0.070, New P: -0.918
iter 19 loss: 1.150
Actual params: [ 0.5   , -0.9178]
-Original Grad: 0.097, -lr * Pred Grad:  -0.064, New P: 0.436
-Original Grad: -0.450, -lr * Pred Grad:  -0.077, New P: -0.995
iter 20 loss: 1.156
Actual params: [ 0.4363, -0.9949]
-Original Grad: 0.234, -lr * Pred Grad:  -0.025, New P: 0.412
-Original Grad: 0.248, -lr * Pred Grad:  -0.060, New P: -1.055
Target params: [1.1812, 0.2779]
iter 0 loss: 0.880
Actual params: [0.5941, 0.5941]
-Original Grad: -0.060, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: 0.004, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.850
Actual params: [0.4941, 0.6941]
-Original Grad: 0.027, -lr * Pred Grad:  -0.031, New P: 0.463
-Original Grad: -0.030, -lr * Pred Grad:  -0.065, New P: 0.629
iter 2 loss: 0.872
Actual params: [0.4632, 0.6289]
-Original Grad: -0.004, -lr * Pred Grad:  -0.028, New P: 0.435
-Original Grad: -0.050, -lr * Pred Grad:  -0.081, New P: 0.548
iter 3 loss: 0.891
Actual params: [0.4355, 0.5481]
-Original Grad: 0.037, -lr * Pred Grad:  0.009, New P: 0.444
-Original Grad: -0.105, -lr * Pred Grad:  -0.083, New P: 0.465
iter 4 loss: 0.904
Actual params: [0.444, 0.465]
-Original Grad: 0.013, -lr * Pred Grad:  0.016, New P: 0.460
-Original Grad: 0.008, -lr * Pred Grad:  -0.066, New P: 0.399
iter 5 loss: 0.916
Actual params: [0.4603, 0.3988]
-Original Grad: 0.020, -lr * Pred Grad:  0.027, New P: 0.487
-Original Grad: -0.083, -lr * Pred Grad:  -0.077, New P: 0.322
iter 6 loss: 0.932
Actual params: [0.4872, 0.3222]
-Original Grad: 0.020, -lr * Pred Grad:  0.035, New P: 0.522
-Original Grad: -0.015, -lr * Pred Grad:  -0.072, New P: 0.251
iter 7 loss: 0.945
Actual params: [0.5224, 0.2505]
-Original Grad: 0.097, -lr * Pred Grad:  0.058, New P: 0.580
-Original Grad: 0.027, -lr * Pred Grad:  -0.053, New P: 0.197
iter 8 loss: 0.963
Actual params: [0.5804, 0.1974]
-Original Grad: 0.120, -lr * Pred Grad:  0.071, New P: 0.651
-Original Grad: -0.039, -lr * Pred Grad:  -0.058, New P: 0.139
iter 9 loss: 0.992
Actual params: [0.6513, 0.1394]
-Original Grad: 0.073, -lr * Pred Grad:  0.077, New P: 0.728
-Original Grad: 0.014, -lr * Pred Grad:  -0.047, New P: 0.092
iter 10 loss: 1.026
Actual params: [0.7284, 0.0922]
-Original Grad: -0.013, -lr * Pred Grad:  0.066, New P: 0.794
-Original Grad: 0.050, -lr * Pred Grad:  -0.025, New P: 0.067
iter 11 loss: 1.047
Actual params: [0.7942, 0.067 ]
-Original Grad: -0.010, -lr * Pred Grad:  0.057, New P: 0.851
-Original Grad: 0.063, -lr * Pred Grad:  -0.004, New P: 0.063
iter 12 loss: 1.072
Actual params: [0.8509, 0.0634]
-Original Grad: 0.060, -lr * Pred Grad:  0.063, New P: 0.914
-Original Grad: 0.082, -lr * Pred Grad:  0.018, New P: 0.081
iter 13 loss: 1.103
Actual params: [0.914, 0.081]
-Original Grad: -0.013, -lr * Pred Grad:  0.054, New P: 0.968
-Original Grad: 0.090, -lr * Pred Grad:  0.035, New P: 0.116
iter 14 loss: 1.123
Actual params: [0.9677, 0.1158]
-Original Grad: -0.059, -lr * Pred Grad:  0.033, New P: 1.000
-Original Grad: -0.016, -lr * Pred Grad:  0.028, New P: 0.144
iter 15 loss: 1.141
Actual params: [1.0004, 0.1437]
-Original Grad: -0.033, -lr * Pred Grad:  0.021, New P: 1.022
-Original Grad: 0.118, -lr * Pred Grad:  0.046, New P: 0.190
iter 16 loss: 1.419
Actual params: [1.0218, 0.1896]
-Original Grad: 0.123, -lr * Pred Grad:  0.042, New P: 1.063
-Original Grad: -0.273, -lr * Pred Grad:  -0.009, New P: 0.181
iter 17 loss: 1.388
Actual params: [1.0635, 0.1806]
-Original Grad: 0.083, -lr * Pred Grad:  0.052, New P: 1.115
-Original Grad: -0.072, -lr * Pred Grad:  -0.018, New P: 0.163
iter 18 loss: 1.443
Actual params: [1.1154, 0.1628]
-Original Grad: -0.044, -lr * Pred Grad:  0.038, New P: 1.153
-Original Grad: -0.041, -lr * Pred Grad:  -0.022, New P: 0.141
iter 19 loss: 1.493
Actual params: [1.1534, 0.1412]
-Original Grad: -0.055, -lr * Pred Grad:  0.023, New P: 1.177
-Original Grad: 0.054, -lr * Pred Grad:  -0.012, New P: 0.129
iter 20 loss: 1.516
Actual params: [1.1767, 0.1291]
-Original Grad: 0.024, -lr * Pred Grad:  0.026, New P: 1.203
-Original Grad: 0.070, -lr * Pred Grad:  -0.002, New P: 0.128
Target params: [1.1812, 0.2779]
iter 0 loss: 0.776
Actual params: [0.5941, 0.5941]
-Original Grad: 0.034, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.114, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.708
Actual params: [0.6941, 0.4941]
-Original Grad: 0.014, -lr * Pred Grad:  0.091, New P: 0.785
-Original Grad: -0.053, -lr * Pred Grad:  -0.092, New P: 0.402
iter 2 loss: 0.590
Actual params: [0.7849, 0.4019]
-Original Grad: 0.142, -lr * Pred Grad:  0.079, New P: 0.864
-Original Grad: 0.067, -lr * Pred Grad:  -0.033, New P: 0.369
iter 3 loss: 0.482
Actual params: [0.8641, 0.3692]
-Original Grad: 0.069, -lr * Pred Grad:  0.083, New P: 0.948
-Original Grad: -0.139, -lr * Pred Grad:  -0.060, New P: 0.309
iter 4 loss: 0.446
Actual params: [0.9476, 0.3095]
-Original Grad: -0.102, -lr * Pred Grad:  0.031, New P: 0.978
-Original Grad: -0.214, -lr * Pred Grad:  -0.074, New P: 0.235
iter 5 loss: 0.439
Actual params: [0.9784, 0.2352]
-Original Grad: -0.012, -lr * Pred Grad:  0.023, New P: 1.002
-Original Grad: -0.044, -lr * Pred Grad:  -0.071, New P: 0.164
iter 6 loss: 0.431
Actual params: [1.0015, 0.1641]
-Original Grad: 0.114, -lr * Pred Grad:  0.043, New P: 1.045
-Original Grad: 0.200, -lr * Pred Grad:  -0.023, New P: 0.141
iter 7 loss: 0.416
Actual params: [1.0448, 0.1411]
-Original Grad: 0.021, -lr * Pred Grad:  0.043, New P: 1.087
-Original Grad: 0.022, -lr * Pred Grad:  -0.017, New P: 0.124
iter 8 loss: 0.410
Actual params: [1.0875, 0.124 ]
-Original Grad: -0.009, -lr * Pred Grad:  0.036, New P: 1.123
-Original Grad: -0.017, -lr * Pred Grad:  -0.018, New P: 0.106
iter 9 loss: 0.409
Actual params: [1.1233, 0.1064]
-Original Grad: 0.043, -lr * Pred Grad:  0.041, New P: 1.164
-Original Grad: -0.021, -lr * Pred Grad:  -0.019, New P: 0.088
iter 10 loss: 0.411
Actual params: [1.1639, 0.0878]
-Original Grad: -0.004, -lr * Pred Grad:  0.035, New P: 1.199
-Original Grad: -0.058, -lr * Pred Grad:  -0.024, New P: 0.064
iter 11 loss: 0.418
Actual params: [1.1993, 0.0636]
-Original Grad: 0.064, -lr * Pred Grad:  0.044, New P: 1.243
-Original Grad: 0.080, -lr * Pred Grad:  -0.011, New P: 0.053
iter 12 loss: 0.429
Actual params: [1.243 , 0.0528]
-Original Grad: 0.021, -lr * Pred Grad:  0.043, New P: 1.286
-Original Grad: -0.305, -lr * Pred Grad:  -0.038, New P: 0.015
iter 13 loss: 0.442
Actual params: [1.2865, 0.0147]
-Original Grad: -0.068, -lr * Pred Grad:  0.024, New P: 1.311
-Original Grad: 0.155, -lr * Pred Grad:  -0.018, New P: -0.003
iter 14 loss: 0.453
Actual params: [ 1.3109, -0.0032]
-Original Grad: -0.021, -lr * Pred Grad:  0.018, New P: 1.329
-Original Grad: -0.147, -lr * Pred Grad:  -0.029, New P: -0.032
iter 15 loss: 0.457
Actual params: [ 1.3289, -0.0324]
-Original Grad: 0.024, -lr * Pred Grad:  0.021, New P: 1.350
-Original Grad: -0.182, -lr * Pred Grad:  -0.041, New P: -0.073
iter 16 loss: 0.468
Actual params: [ 1.3498, -0.0734]
-Original Grad: -0.082, -lr * Pred Grad:  0.003, New P: 1.352
-Original Grad: -0.069, -lr * Pred Grad:  -0.043, New P: -0.116
iter 17 loss: 0.474
Actual params: [ 1.3523, -0.1164]
-Original Grad: -0.076, -lr * Pred Grad:  -0.012, New P: 1.341
-Original Grad: 0.125, -lr * Pred Grad:  -0.027, New P: -0.144
iter 18 loss: 0.475
Actual params: [ 1.3407, -0.1438]
-Original Grad: -0.068, -lr * Pred Grad:  -0.022, New P: 1.318
-Original Grad: -0.017, -lr * Pred Grad:  -0.026, New P: -0.170
iter 19 loss: 0.473
Actual params: [ 1.3183, -0.1701]
-Original Grad: -0.059, -lr * Pred Grad:  -0.030, New P: 1.288
-Original Grad: -0.009, -lr * Pred Grad:  -0.025, New P: -0.195
iter 20 loss: 0.470
Actual params: [ 1.2879, -0.1948]
-Original Grad: -0.029, -lr * Pred Grad:  -0.033, New P: 1.255
-Original Grad: -0.032, -lr * Pred Grad:  -0.025, New P: -0.220
Target params: [1.1812, 0.2779]
iter 0 loss: 0.599
Actual params: [0.5941, 0.5941]
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: 0.010, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.710
Actual params: [0.4941, 0.6941]
-Original Grad: 0.086, -lr * Pred Grad:  0.074, New P: 0.568
-Original Grad: 0.028, -lr * Pred Grad:  0.093, New P: 0.787
iter 2 loss: 0.752
Actual params: [0.568 , 0.7869]
-Original Grad: 0.439, -lr * Pred Grad:  0.074, New P: 0.642
-Original Grad: -0.087, -lr * Pred Grad:  -0.037, New P: 0.750
iter 3 loss: 0.661
Actual params: [0.6417, 0.7497]
-Original Grad: 0.095, -lr * Pred Grad:  0.071, New P: 0.713
-Original Grad: 0.059, -lr * Pred Grad:  0.006, New P: 0.755
iter 4 loss: 0.619
Actual params: [0.7128, 0.7554]
-Original Grad: 0.091, -lr * Pred Grad:  0.070, New P: 0.782
-Original Grad: 0.106, -lr * Pred Grad:  0.041, New P: 0.797
iter 5 loss: 0.637
Actual params: [0.7824, 0.7968]
-Original Grad: -0.056, -lr * Pred Grad:  0.053, New P: 0.836
-Original Grad: 0.013, -lr * Pred Grad:  0.040, New P: 0.837
iter 6 loss: 0.672
Actual params: [0.8356, 0.8368]
-Original Grad: -0.545, -lr * Pred Grad:  -0.008, New P: 0.828
-Original Grad: -0.262, -lr * Pred Grad:  -0.026, New P: 0.811
iter 7 loss: 0.633
Actual params: [0.8275, 0.8106]
-Original Grad: -0.258, -lr * Pred Grad:  -0.023, New P: 0.804
-Original Grad: -0.048, -lr * Pred Grad:  -0.031, New P: 0.780
iter 8 loss: 0.597
Actual params: [0.8041, 0.7801]
-Original Grad: -0.426, -lr * Pred Grad:  -0.042, New P: 0.762
-Original Grad: -0.060, -lr * Pred Grad:  -0.036, New P: 0.744
iter 9 loss: 0.569
Actual params: [0.762 , 0.7442]
-Original Grad: -0.502, -lr * Pred Grad:  -0.057, New P: 0.705
-Original Grad: -0.177, -lr * Pred Grad:  -0.052, New P: 0.692
iter 10 loss: 0.568
Actual params: [0.7054, 0.6925]
-Original Grad: -0.075, -lr * Pred Grad:  -0.054, New P: 0.651
-Original Grad: 0.061, -lr * Pred Grad:  -0.038, New P: 0.655
iter 11 loss: 0.585
Actual params: [0.6512, 0.6549]
-Original Grad: 0.351, -lr * Pred Grad:  -0.030, New P: 0.621
-Original Grad: -0.095, -lr * Pred Grad:  -0.045, New P: 0.610
iter 12 loss: 0.586
Actual params: [0.6209, 0.61  ]
-Original Grad: 0.029, -lr * Pred Grad:  -0.026, New P: 0.595
-Original Grad: 0.018, -lr * Pred Grad:  -0.038, New P: 0.572
iter 13 loss: 0.592
Actual params: [0.595 , 0.5719]
-Original Grad: 0.125, -lr * Pred Grad:  -0.018, New P: 0.577
-Original Grad: 0.119, -lr * Pred Grad:  -0.018, New P: 0.554
iter 14 loss: 0.596
Actual params: [0.5773, 0.5535]
-Original Grad: 0.205, -lr * Pred Grad:  -0.007, New P: 0.571
-Original Grad: -0.149, -lr * Pred Grad:  -0.033, New P: 0.521
iter 15 loss: 0.594
Actual params: [0.5708, 0.5208]
-Original Grad: 0.446, -lr * Pred Grad:  0.013, New P: 0.584
-Original Grad: -0.038, -lr * Pred Grad:  -0.034, New P: 0.487
iter 16 loss: 0.580
Actual params: [0.5837, 0.4869]
-Original Grad: 0.096, -lr * Pred Grad:  0.016, New P: 0.599
-Original Grad: -0.198, -lr * Pred Grad:  -0.049, New P: 0.438
iter 17 loss: 0.560
Actual params: [0.5994, 0.4382]
-Original Grad: -0.032, -lr * Pred Grad:  0.013, New P: 0.612
-Original Grad: -0.170, -lr * Pred Grad:  -0.059, New P: 0.380
iter 18 loss: 0.542
Actual params: [0.6123, 0.3796]
-Original Grad: 0.295, -lr * Pred Grad:  0.024, New P: 0.636
-Original Grad: -0.026, -lr * Pred Grad:  -0.056, New P: 0.324
iter 19 loss: 0.518
Actual params: [0.6358, 0.3237]
-Original Grad: 0.093, -lr * Pred Grad:  0.025, New P: 0.661
-Original Grad: -0.149, -lr * Pred Grad:  -0.063, New P: 0.260
iter 20 loss: 0.492
Actual params: [0.6609, 0.2605]
-Original Grad: -0.021, -lr * Pred Grad:  0.022, New P: 0.683
-Original Grad: -0.100, -lr * Pred Grad:  -0.066, New P: 0.194
Target params: [1.1812, 0.2779]
iter 0 loss: 0.511
Actual params: [0.5941, 0.5941]
-Original Grad: 0.105, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.057, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.422
Actual params: [0.6941, 0.4941]
-Original Grad: -0.155, -lr * Pred Grad:  -0.024, New P: 0.670
-Original Grad: -0.054, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 0.395
Actual params: [0.6703, 0.3942]
-Original Grad: -0.121, -lr * Pred Grad:  -0.050, New P: 0.620
-Original Grad: -0.101, -lr * Pred Grad:  -0.098, New P: 0.296
iter 3 loss: 0.388
Actual params: [0.6201, 0.2964]
-Original Grad: -0.124, -lr * Pred Grad:  -0.064, New P: 0.556
-Original Grad: -0.064, -lr * Pred Grad:  -0.098, New P: 0.199
iter 4 loss: 0.394
Actual params: [0.5558, 0.1987]
-Original Grad: -0.048, -lr * Pred Grad:  -0.063, New P: 0.492
-Original Grad: -0.071, -lr * Pred Grad:  -0.098, New P: 0.100
iter 5 loss: 0.407
Actual params: [0.4924, 0.1004]
-Original Grad: 0.225, -lr * Pred Grad:  -0.007, New P: 0.485
-Original Grad: -0.069, -lr * Pred Grad:  -0.098, New P: 0.002
iter 6 loss: 0.395
Actual params: [0.4854, 0.002 ]
-Original Grad: 0.132, -lr * Pred Grad:  0.012, New P: 0.498
-Original Grad: -0.045, -lr * Pred Grad:  -0.096, New P: -0.094
iter 7 loss: 0.384
Actual params: [ 0.4978, -0.0939]
-Original Grad: -0.017, -lr * Pred Grad:  0.009, New P: 0.506
-Original Grad: -0.010, -lr * Pred Grad:  -0.087, New P: -0.181
iter 8 loss: 0.369
Actual params: [ 0.5065, -0.181 ]
-Original Grad: -0.006, -lr * Pred Grad:  0.007, New P: 0.513
-Original Grad: 0.008, -lr * Pred Grad:  -0.075, New P: -0.256
iter 9 loss: 0.358
Actual params: [ 0.5133, -0.2559]
-Original Grad: 0.110, -lr * Pred Grad:  0.020, New P: 0.533
-Original Grad: 0.019, -lr * Pred Grad:  -0.062, New P: -0.317
iter 10 loss: 0.350
Actual params: [ 0.5331, -0.3174]
-Original Grad: -0.022, -lr * Pred Grad:  0.015, New P: 0.548
-Original Grad: 0.014, -lr * Pred Grad:  -0.051, New P: -0.369
iter 11 loss: 0.349
Actual params: [ 0.548 , -0.3686]
-Original Grad: -0.019, -lr * Pred Grad:  0.011, New P: 0.559
-Original Grad: -0.000, -lr * Pred Grad:  -0.046, New P: -0.415
iter 12 loss: 0.347
Actual params: [ 0.5591, -0.4147]
-Original Grad: 0.078, -lr * Pred Grad:  0.019, New P: 0.578
-Original Grad: 0.075, -lr * Pred Grad:  -0.020, New P: -0.434
iter 13 loss: 0.346
Actual params: [ 0.5784, -0.4345]
-Original Grad: -0.112, -lr * Pred Grad:  0.004, New P: 0.582
-Original Grad: 0.088, -lr * Pred Grad:  0.004, New P: -0.431
iter 14 loss: 0.345
Actual params: [ 0.5819, -0.431 ]
-Original Grad: 0.065, -lr * Pred Grad:  0.011, New P: 0.593
-Original Grad: 0.007, -lr * Pred Grad:  0.005, New P: -0.426
iter 15 loss: 0.345
Actual params: [ 0.5927, -0.4263]
-Original Grad: -0.180, -lr * Pred Grad:  -0.011, New P: 0.582
-Original Grad: 0.024, -lr * Pred Grad:  0.010, New P: -0.416
iter 16 loss: 0.346
Actual params: [ 0.5821, -0.4165]
-Original Grad: -0.002, -lr * Pred Grad:  -0.010, New P: 0.572
-Original Grad: -0.019, -lr * Pred Grad:  0.004, New P: -0.412
iter 17 loss: 0.347
Actual params: [ 0.5722, -0.4121]
-Original Grad: 0.039, -lr * Pred Grad:  -0.005, New P: 0.568
-Original Grad: 0.040, -lr * Pred Grad:  0.013, New P: -0.399
iter 18 loss: 0.347
Actual params: [ 0.5676, -0.399 ]
-Original Grad: -0.132, -lr * Pred Grad:  -0.018, New P: 0.549
-Original Grad: 0.017, -lr * Pred Grad:  0.016, New P: -0.383
iter 19 loss: 0.348
Actual params: [ 0.5495, -0.3834]
-Original Grad: 0.015, -lr * Pred Grad:  -0.015, New P: 0.535
-Original Grad: -0.020, -lr * Pred Grad:  0.010, New P: -0.374
iter 20 loss: 0.349
Actual params: [ 0.5346, -0.3738]
-Original Grad: -0.216, -lr * Pred Grad:  -0.034, New P: 0.501
-Original Grad: 0.032, -lr * Pred Grad:  0.016, New P: -0.358
Target params: [1.1812, 0.2779]
iter 0 loss: 0.674
Actual params: [0.5941, 0.5941]
-Original Grad: 0.305, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.485, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.637
Actual params: [0.6941, 0.4941]
-Original Grad: 0.182, -lr * Pred Grad:  0.096, New P: 0.790
-Original Grad: -0.260, -lr * Pred Grad:  -0.094, New P: 0.400
iter 2 loss: 0.635
Actual params: [0.7898, 0.3998]
-Original Grad: 0.220, -lr * Pred Grad:  0.097, New P: 0.886
-Original Grad: -0.126, -lr * Pred Grad:  -0.085, New P: 0.315
iter 3 loss: 0.630
Actual params: [0.8863, 0.3145]
-Original Grad: 0.085, -lr * Pred Grad:  0.089, New P: 0.975
-Original Grad: -0.121, -lr * Pred Grad:  -0.080, New P: 0.234
iter 4 loss: 0.631
Actual params: [0.9754, 0.2341]
-Original Grad: -0.023, -lr * Pred Grad:  0.072, New P: 1.048
-Original Grad: -0.028, -lr * Pred Grad:  -0.071, New P: 0.163
iter 5 loss: 0.629
Actual params: [1.0477, 0.1635]
-Original Grad: 0.014, -lr * Pred Grad:  0.064, New P: 1.112
-Original Grad: 0.037, -lr * Pred Grad:  -0.057, New P: 0.106
iter 6 loss: 0.633
Actual params: [1.1117, 0.1061]
-Original Grad: 0.082, -lr * Pred Grad:  0.064, New P: 1.176
-Original Grad: -0.049, -lr * Pred Grad:  -0.054, New P: 0.052
iter 7 loss: 0.641
Actual params: [1.1761, 0.0518]
-Original Grad: -0.068, -lr * Pred Grad:  0.048, New P: 1.224
-Original Grad: -0.061, -lr * Pred Grad:  -0.053, New P: -0.001
iter 8 loss: 0.642
Actual params: [ 1.2245e+00, -9.1092e-04]
-Original Grad: -0.095, -lr * Pred Grad:  0.032, New P: 1.256
-Original Grad: -0.005, -lr * Pred Grad:  -0.047, New P: -0.048
iter 9 loss: 0.646
Actual params: [ 1.2562, -0.0481]
-Original Grad: -0.111, -lr * Pred Grad:  0.016, New P: 1.272
-Original Grad: -0.034, -lr * Pred Grad:  -0.045, New P: -0.093
iter 10 loss: 0.647
Actual params: [ 1.272, -0.093]
-Original Grad: -0.006, -lr * Pred Grad:  0.013, New P: 1.285
-Original Grad: -0.077, -lr * Pred Grad:  -0.046, New P: -0.139
iter 11 loss: 0.649
Actual params: [ 1.2854, -0.1392]
-Original Grad: -0.167, -lr * Pred Grad:  -0.005, New P: 1.281
-Original Grad: 0.009, -lr * Pred Grad:  -0.041, New P: -0.180
iter 12 loss: 0.650
Actual params: [ 1.2805, -0.1799]
-Original Grad: -0.104, -lr * Pred Grad:  -0.014, New P: 1.266
-Original Grad: -0.027, -lr * Pred Grad:  -0.039, New P: -0.219
iter 13 loss: 0.649
Actual params: [ 1.2662, -0.2189]
-Original Grad: -0.177, -lr * Pred Grad:  -0.028, New P: 1.238
-Original Grad: -0.111, -lr * Pred Grad:  -0.044, New P: -0.262
iter 14 loss: 0.647
Actual params: [ 1.2379, -0.2624]
-Original Grad: -0.132, -lr * Pred Grad:  -0.037, New P: 1.201
-Original Grad: -0.055, -lr * Pred Grad:  -0.044, New P: -0.306
iter 15 loss: 0.643
Actual params: [ 1.2013, -0.306 ]
-Original Grad: -0.143, -lr * Pred Grad:  -0.044, New P: 1.157
-Original Grad: -0.012, -lr * Pred Grad:  -0.041, New P: -0.347
iter 16 loss: 0.639
Actual params: [ 1.1569, -0.3466]
-Original Grad: -0.229, -lr * Pred Grad:  -0.056, New P: 1.101
-Original Grad: -0.076, -lr * Pred Grad:  -0.043, New P: -0.389
iter 17 loss: 0.634
Actual params: [ 1.1011, -0.3893]
-Original Grad: -0.102, -lr * Pred Grad:  -0.058, New P: 1.043
-Original Grad: 0.002, -lr * Pred Grad:  -0.039, New P: -0.428
iter 18 loss: 0.634
Actual params: [ 1.0429, -0.4278]
-Original Grad: -0.052, -lr * Pred Grad:  -0.057, New P: 0.986
-Original Grad: 0.045, -lr * Pred Grad:  -0.031, New P: -0.459
iter 19 loss: 0.630
Actual params: [ 0.986 , -0.4591]
-Original Grad: 0.098, -lr * Pred Grad:  -0.043, New P: 0.943
-Original Grad: -0.056, -lr * Pred Grad:  -0.033, New P: -0.492
iter 20 loss: 0.628
Actual params: [ 0.9428, -0.4921]
-Original Grad: 0.107, -lr * Pred Grad:  -0.030, New P: 0.913
-Original Grad: -0.112, -lr * Pred Grad:  -0.039, New P: -0.531
Target params: [1.1812, 0.2779]
iter 0 loss: 0.556
Actual params: [0.5941, 0.5941]
-Original Grad: 0.036, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.270, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.412
Actual params: [0.6941, 0.4941]
-Original Grad: -0.010, -lr * Pred Grad:  0.045, New P: 0.739
-Original Grad: -0.143, -lr * Pred Grad:  -0.094, New P: 0.400
iter 2 loss: 0.352
Actual params: [0.7387, 0.4   ]
-Original Grad: -0.026, -lr * Pred Grad:  -0.009, New P: 0.730
-Original Grad: -0.117, -lr * Pred Grad:  -0.091, New P: 0.309
iter 3 loss: 0.332
Actual params: [0.7299, 0.3092]
-Original Grad: 0.038, -lr * Pred Grad:  0.032, New P: 0.762
-Original Grad: -0.042, -lr * Pred Grad:  -0.081, New P: 0.228
iter 4 loss: 0.307
Actual params: [0.7619, 0.2281]
-Original Grad: 0.026, -lr * Pred Grad:  0.047, New P: 0.808
-Original Grad: -0.055, -lr * Pred Grad:  -0.077, New P: 0.151
iter 5 loss: 0.290
Actual params: [0.8085, 0.1514]
-Original Grad: 0.016, -lr * Pred Grad:  0.051, New P: 0.860
-Original Grad: -0.080, -lr * Pred Grad:  -0.076, New P: 0.075
iter 6 loss: 0.278
Actual params: [0.86 , 0.075]
-Original Grad: -0.008, -lr * Pred Grad:  0.038, New P: 0.898
-Original Grad: -0.035, -lr * Pred Grad:  -0.071, New P: 0.004
iter 7 loss: 0.271
Actual params: [0.8984, 0.0035]
-Original Grad: 0.025, -lr * Pred Grad:  0.049, New P: 0.947
-Original Grad: -0.048, -lr * Pred Grad:  -0.069, New P: -0.066
iter 8 loss: 0.267
Actual params: [ 0.9475, -0.0658]
-Original Grad: 0.002, -lr * Pred Grad:  0.045, New P: 0.992
-Original Grad: -0.074, -lr * Pred Grad:  -0.070, New P: -0.136
iter 9 loss: 0.265
Actual params: [ 0.992 , -0.1361]
-Original Grad: -0.034, -lr * Pred Grad:  0.015, New P: 1.007
-Original Grad: -0.059, -lr * Pred Grad:  -0.070, New P: -0.206
iter 10 loss: 0.266
Actual params: [ 1.0072, -0.2059]
-Original Grad: -0.019, -lr * Pred Grad:  0.002, New P: 1.009
-Original Grad: -0.025, -lr * Pred Grad:  -0.066, New P: -0.272
iter 11 loss: 0.268
Actual params: [ 1.009 , -0.2717]
-Original Grad: -0.007, -lr * Pred Grad:  -0.003, New P: 1.006
-Original Grad: -0.034, -lr * Pred Grad:  -0.063, New P: -0.335
iter 12 loss: 0.270
Actual params: [ 1.0064, -0.3351]
-Original Grad: -0.019, -lr * Pred Grad:  -0.013, New P: 0.993
-Original Grad: -0.014, -lr * Pred Grad:  -0.059, New P: -0.394
iter 13 loss: 0.272
Actual params: [ 0.993 , -0.3941]
-Original Grad: -0.018, -lr * Pred Grad:  -0.022, New P: 0.971
-Original Grad: -0.060, -lr * Pred Grad:  -0.061, New P: -0.455
iter 14 loss: 0.274
Actual params: [ 0.9709, -0.4546]
-Original Grad: 0.013, -lr * Pred Grad:  -0.012, New P: 0.958
-Original Grad: -0.016, -lr * Pred Grad:  -0.057, New P: -0.511
iter 15 loss: 0.275
Actual params: [ 0.9584, -0.5115]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: 0.946
-Original Grad: -0.014, -lr * Pred Grad:  -0.053, New P: -0.565
iter 16 loss: 0.277
Actual params: [ 0.9465, -0.5647]
-Original Grad: -0.042, -lr * Pred Grad:  -0.031, New P: 0.915
-Original Grad: -0.144, -lr * Pred Grad:  -0.063, New P: -0.628
iter 17 loss: 0.280
Actual params: [ 0.9153, -0.6277]
-Original Grad: 0.066, -lr * Pred Grad:  0.005, New P: 0.920
-Original Grad: 0.102, -lr * Pred Grad:  -0.043, New P: -0.671
iter 18 loss: 0.279
Actual params: [ 0.9202, -0.6707]
-Original Grad: -0.003, -lr * Pred Grad:  0.003, New P: 0.923
-Original Grad: -0.011, -lr * Pred Grad:  -0.040, New P: -0.711
iter 19 loss: 0.435
Actual params: [ 0.9233, -0.7111]
-Original Grad: 0.026, -lr * Pred Grad:  0.014, New P: 0.937
-Original Grad: -0.005, -lr * Pred Grad:  -0.037, New P: -0.748
iter 20 loss: 0.880
Actual params: [ 0.9371, -0.7484]
-Original Grad: 0.053, -lr * Pred Grad:  0.032, New P: 0.969
-Original Grad: 0.062, -lr * Pred Grad:  -0.026, New P: -0.774
Target params: [1.1812, 0.2779]
iter 0 loss: 0.399
Actual params: [0.5941, 0.5941]
-Original Grad: 0.003, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.025, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.348
Actual params: [0.6941, 0.4941]
-Original Grad: -0.009, -lr * Pred Grad:  -0.053, New P: 0.641
-Original Grad: -0.084, -lr * Pred Grad:  -0.090, New P: 0.404
iter 2 loss: 0.317
Actual params: [0.641 , 0.4036]
-Original Grad: -0.049, -lr * Pred Grad:  -0.071, New P: 0.570
-Original Grad: -0.050, -lr * Pred Grad:  -0.092, New P: 0.311
iter 3 loss: 0.302
Actual params: [0.5705, 0.3112]
-Original Grad: 0.035, -lr * Pred Grad:  -0.014, New P: 0.556
-Original Grad: -0.011, -lr * Pred Grad:  -0.081, New P: 0.230
iter 4 loss: 0.324
Actual params: [0.5561, 0.2298]
-Original Grad: 0.089, -lr * Pred Grad:  0.038, New P: 0.594
-Original Grad: -0.028, -lr * Pred Grad:  -0.081, New P: 0.149
iter 5 loss: 0.386
Actual params: [0.5943, 0.1488]
-Original Grad: -0.043, -lr * Pred Grad:  0.011, New P: 0.605
-Original Grad: 0.071, -lr * Pred Grad:  -0.028, New P: 0.120
iter 6 loss: 0.411
Actual params: [0.6055, 0.1204]
-Original Grad: -0.082, -lr * Pred Grad:  -0.021, New P: 0.584
-Original Grad: 0.099, -lr * Pred Grad:  0.011, New P: 0.132
iter 7 loss: 0.393
Actual params: [0.5842, 0.1319]
-Original Grad: -0.140, -lr * Pred Grad:  -0.048, New P: 0.536
-Original Grad: 0.178, -lr * Pred Grad:  0.044, New P: 0.175
iter 8 loss: 0.349
Actual params: [0.5359, 0.1755]
-Original Grad: -0.018, -lr * Pred Grad:  -0.047, New P: 0.489
-Original Grad: -0.030, -lr * Pred Grad:  0.032, New P: 0.208
iter 9 loss: 0.326
Actual params: [0.4888, 0.2079]
-Original Grad: 0.127, -lr * Pred Grad:  -0.010, New P: 0.479
-Original Grad: -0.158, -lr * Pred Grad:  -0.002, New P: 0.206
iter 10 loss: 0.327
Actual params: [0.4792, 0.2056]
-Original Grad: 0.036, -lr * Pred Grad:  -0.001, New P: 0.478
-Original Grad: 0.098, -lr * Pred Grad:  0.013, New P: 0.219
iter 11 loss: 0.322
Actual params: [0.4779, 0.2191]
-Original Grad: 0.055, -lr * Pred Grad:  0.010, New P: 0.488
-Original Grad: -0.008, -lr * Pred Grad:  0.011, New P: 0.230
iter 12 loss: 0.319
Actual params: [0.4876, 0.2299]
-Original Grad: -0.035, -lr * Pred Grad:  0.002, New P: 0.489
-Original Grad: 0.217, -lr * Pred Grad:  0.036, New P: 0.266
iter 13 loss: 0.311
Actual params: [0.4894, 0.2659]
-Original Grad: -0.037, -lr * Pred Grad:  -0.006, New P: 0.484
-Original Grad: -0.051, -lr * Pred Grad:  0.026, New P: 0.291
iter 14 loss: 0.311
Actual params: [0.4838, 0.2915]
-Original Grad: 0.006, -lr * Pred Grad:  -0.004, New P: 0.480
-Original Grad: 0.244, -lr * Pred Grad:  0.046, New P: 0.337
iter 15 loss: 0.321
Actual params: [0.48  , 0.3373]
-Original Grad: -0.015, -lr * Pred Grad:  -0.006, New P: 0.474
-Original Grad: -0.140, -lr * Pred Grad:  0.025, New P: 0.362
iter 16 loss: 0.331
Actual params: [0.4737, 0.3625]
-Original Grad: 0.032, -lr * Pred Grad:  0.001, New P: 0.474
-Original Grad: -0.054, -lr * Pred Grad:  0.017, New P: 0.379
iter 17 loss: 0.338
Actual params: [0.4742, 0.3795]
-Original Grad: 0.038, -lr * Pred Grad:  0.008, New P: 0.482
-Original Grad: -0.074, -lr * Pred Grad:  0.008, New P: 0.387
iter 18 loss: 0.340
Actual params: [0.4821, 0.3871]
-Original Grad: 0.039, -lr * Pred Grad:  0.015, New P: 0.497
-Original Grad: -0.035, -lr * Pred Grad:  0.003, New P: 0.390
iter 19 loss: 0.339
Actual params: [0.4967, 0.3903]
-Original Grad: 0.087, -lr * Pred Grad:  0.029, New P: 0.525
-Original Grad: -0.079, -lr * Pred Grad:  -0.005, New P: 0.385
iter 20 loss: 0.332
Actual params: [0.5254, 0.385 ]
-Original Grad: -0.036, -lr * Pred Grad:  0.019, New P: 0.545
-Original Grad: -0.058, -lr * Pred Grad:  -0.011, New P: 0.374
Target params: [1.1812, 0.2779]
iter 0 loss: 1.456
Actual params: [0.5941, 0.5941]
-Original Grad: -0.105, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.030, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 1.473
Actual params: [0.4941, 0.4941]
-Original Grad: -0.100, -lr * Pred Grad:  -0.100, New P: 0.394
-Original Grad: 0.023, -lr * Pred Grad:  -0.008, New P: 0.486
iter 2 loss: 1.503
Actual params: [0.3942, 0.4862]
-Original Grad: 0.143, -lr * Pred Grad:  -0.010, New P: 0.384
-Original Grad: -0.004, -lr * Pred Grad:  -0.013, New P: 0.474
iter 3 loss: 1.498
Actual params: [0.3842, 0.4736]
-Original Grad: 0.009, -lr * Pred Grad:  -0.006, New P: 0.379
-Original Grad: 0.021, -lr * Pred Grad:  0.019, New P: 0.492
iter 4 loss: 1.509
Actual params: [0.3786, 0.4923]
-Original Grad: -0.029, -lr * Pred Grad:  -0.012, New P: 0.366
-Original Grad: -0.039, -lr * Pred Grad:  -0.024, New P: 0.468
iter 5 loss: 1.500
Actual params: [0.3661, 0.468 ]
-Original Grad: 0.046, -lr * Pred Grad:  0.001, New P: 0.367
-Original Grad: -0.019, -lr * Pred Grad:  -0.036, New P: 0.432
iter 6 loss: 1.482
Actual params: [0.3671, 0.4316]
-Original Grad: 0.038, -lr * Pred Grad:  0.010, New P: 0.377
-Original Grad: -0.068, -lr * Pred Grad:  -0.059, New P: 0.373
iter 7 loss: 1.463
Actual params: [0.377 , 0.3728]
-Original Grad: 0.006, -lr * Pred Grad:  0.010, New P: 0.387
-Original Grad: -0.077, -lr * Pred Grad:  -0.072, New P: 0.301
iter 8 loss: 1.442
Actual params: [0.3871, 0.3012]
-Original Grad: 0.027, -lr * Pred Grad:  0.015, New P: 0.402
-Original Grad: -0.061, -lr * Pred Grad:  -0.079, New P: 0.222
iter 9 loss: 1.430
Actual params: [0.4021, 0.2223]
-Original Grad: 0.095, -lr * Pred Grad:  0.032, New P: 0.434
-Original Grad: -0.044, -lr * Pred Grad:  -0.082, New P: 0.140
iter 10 loss: 1.396
Actual params: [0.434 , 0.1404]
-Original Grad: 0.053, -lr * Pred Grad:  0.038, New P: 0.472
-Original Grad: -0.012, -lr * Pred Grad:  -0.077, New P: 0.063
iter 11 loss: 1.373
Actual params: [0.4725, 0.0632]
-Original Grad: 0.011, -lr * Pred Grad:  0.037, New P: 0.509
-Original Grad: -0.044, -lr * Pred Grad:  -0.081, New P: -0.017
iter 12 loss: 1.357
Actual params: [ 0.5092, -0.0174]
-Original Grad: 0.104, -lr * Pred Grad:  0.050, New P: 0.559
-Original Grad: -0.058, -lr * Pred Grad:  -0.085, New P: -0.103
iter 13 loss: 1.325
Actual params: [ 0.5587, -0.1027]
-Original Grad: -0.031, -lr * Pred Grad:  0.039, New P: 0.597
-Original Grad: -0.019, -lr * Pred Grad:  -0.082, New P: -0.185
iter 14 loss: 1.308
Actual params: [ 0.5975, -0.1848]
-Original Grad: -0.013, -lr * Pred Grad:  0.033, New P: 0.630
-Original Grad: 0.010, -lr * Pred Grad:  -0.071, New P: -0.256
iter 15 loss: 1.284
Actual params: [ 0.6301, -0.256 ]
-Original Grad: 0.052, -lr * Pred Grad:  0.038, New P: 0.668
-Original Grad: 0.037, -lr * Pred Grad:  -0.052, New P: -0.308
iter 16 loss: 1.262
Actual params: [ 0.6685, -0.3078]
-Original Grad: -0.048, -lr * Pred Grad:  0.026, New P: 0.694
-Original Grad: -0.012, -lr * Pred Grad:  -0.051, New P: -0.358
iter 17 loss: 1.255
Actual params: [ 0.6941, -0.3584]
-Original Grad: 0.013, -lr * Pred Grad:  0.026, New P: 0.720
-Original Grad: 0.052, -lr * Pred Grad:  -0.029, New P: -0.387
iter 18 loss: 1.237
Actual params: [ 0.7199, -0.3873]
-Original Grad: 0.028, -lr * Pred Grad:  0.028, New P: 0.748
-Original Grad: 0.041, -lr * Pred Grad:  -0.014, New P: -0.401
iter 19 loss: 1.217
Actual params: [ 0.7482, -0.4011]
-Original Grad: -0.028, -lr * Pred Grad:  0.020, New P: 0.769
-Original Grad: -0.005, -lr * Pred Grad:  -0.014, New P: -0.415
iter 20 loss: 1.212
Actual params: [ 0.7686, -0.4151]
-Original Grad: 0.055, -lr * Pred Grad:  0.028, New P: 0.797
-Original Grad: 0.045, -lr * Pred Grad:  0.000, New P: -0.415
Target params: [1.1812, 0.2779]
iter 0 loss: 0.391
Actual params: [0.5941, 0.5941]
-Original Grad: -0.004, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.075, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.386
Actual params: [0.4941, 0.4941]
-Original Grad: 0.027, -lr * Pred Grad:  0.065, New P: 0.559
-Original Grad: -0.104, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 0.397
Actual params: [0.559 , 0.3945]
-Original Grad: 0.029, -lr * Pred Grad:  0.081, New P: 0.640
-Original Grad: -0.094, -lr * Pred Grad:  -0.100, New P: 0.295
iter 3 loss: 0.399
Actual params: [0.6399, 0.2946]
-Original Grad: -0.022, -lr * Pred Grad:  0.029, New P: 0.669
-Original Grad: -0.054, -lr * Pred Grad:  -0.096, New P: 0.198
iter 4 loss: 0.397
Actual params: [0.6692, 0.1984]
-Original Grad: -0.010, -lr * Pred Grad:  0.013, New P: 0.682
-Original Grad: -0.052, -lr * Pred Grad:  -0.094, New P: 0.105
iter 5 loss: 0.395
Actual params: [0.6818, 0.1046]
-Original Grad: -0.002, -lr * Pred Grad:  0.009, New P: 0.691
-Original Grad: -0.029, -lr * Pred Grad:  -0.088, New P: 0.016
iter 6 loss: 0.397
Actual params: [0.6908, 0.0164]
-Original Grad: 0.009, -lr * Pred Grad:  0.018, New P: 0.708
-Original Grad: -0.037, -lr * Pred Grad:  -0.086, New P: -0.069
iter 7 loss: 0.399
Actual params: [ 0.7084, -0.0694]
-Original Grad: -0.010, -lr * Pred Grad:  0.005, New P: 0.713
-Original Grad: -0.002, -lr * Pred Grad:  -0.076, New P: -0.146
iter 8 loss: 0.402
Actual params: [ 0.7129, -0.1455]
-Original Grad: -0.001, -lr * Pred Grad:  0.003, New P: 0.716
-Original Grad: -0.058, -lr * Pred Grad:  -0.079, New P: -0.225
iter 9 loss: 0.407
Actual params: [ 0.7162, -0.2248]
-Original Grad: -0.003, -lr * Pred Grad:  -0.000, New P: 0.716
-Original Grad: -0.020, -lr * Pred Grad:  -0.076, New P: -0.300
iter 10 loss: 0.413
Actual params: [ 0.716 , -0.3003]
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: 0.717
-Original Grad: 0.027, -lr * Pred Grad:  -0.060, New P: -0.361
iter 11 loss: 0.415
Actual params: [ 0.7169, -0.3605]
-Original Grad: -0.007, -lr * Pred Grad:  -0.006, New P: 0.711
-Original Grad: 0.019, -lr * Pred Grad:  -0.049, New P: -0.410
iter 12 loss: 0.417
Actual params: [ 0.7107, -0.4097]
-Original Grad: 0.001, -lr * Pred Grad:  -0.004, New P: 0.706
-Original Grad: 0.024, -lr * Pred Grad:  -0.038, New P: -0.448
iter 13 loss: 0.420
Actual params: [ 0.7064, -0.4478]
-Original Grad: 0.002, -lr * Pred Grad:  -0.002, New P: 0.704
-Original Grad: -0.009, -lr * Pred Grad:  -0.037, New P: -0.484
iter 14 loss: 0.424
Actual params: [ 0.7044, -0.4844]
-Original Grad: 0.017, -lr * Pred Grad:  0.014, New P: 0.719
-Original Grad: 0.001, -lr * Pred Grad:  -0.033, New P: -0.517
iter 15 loss: 0.426
Actual params: [ 0.7189, -0.5173]
-Original Grad: 0.005, -lr * Pred Grad:  0.018, New P: 0.737
-Original Grad: 0.009, -lr * Pred Grad:  -0.027, New P: -0.545
iter 16 loss: 0.428
Actual params: [ 0.7368, -0.5448]
-Original Grad: -0.006, -lr * Pred Grad:  0.010, New P: 0.747
-Original Grad: -0.026, -lr * Pred Grad:  -0.031, New P: -0.576
iter 17 loss: 0.431
Actual params: [ 0.7471, -0.5761]
-Original Grad: 0.011, -lr * Pred Grad:  0.019, New P: 0.766
-Original Grad: 0.004, -lr * Pred Grad:  -0.027, New P: -0.603
iter 18 loss: 0.433
Actual params: [ 0.7661, -0.6035]
-Original Grad: 0.020, -lr * Pred Grad:  0.034, New P: 0.800
-Original Grad: 0.012, -lr * Pred Grad:  -0.022, New P: -0.625
iter 19 loss: 0.433
Actual params: [ 0.7998, -0.6252]
-Original Grad: 0.018, -lr * Pred Grad:  0.045, New P: 0.844
-Original Grad: -0.005, -lr * Pred Grad:  -0.021, New P: -0.646
iter 20 loss: 0.432
Actual params: [ 0.8443, -0.6462]
-Original Grad: 0.010, -lr * Pred Grad:  0.048, New P: 0.893
-Original Grad: -0.032, -lr * Pred Grad:  -0.027, New P: -0.673
