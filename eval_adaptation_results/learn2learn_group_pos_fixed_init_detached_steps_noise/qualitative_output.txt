Target params: [1.1812, 0.2779]
iter 0 loss: 0.779
Actual params: [0.5941, 0.5941]
-Original Grad: 0.141, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: 0.159, -lr * Pred Grad: 0.067, New P: 0.661
iter 1 loss: 0.776
Actual params: [0.6608, 0.6611]
-Original Grad: 0.268, -lr * Pred Grad: 0.084, New P: 0.745
-Original Grad: -0.073, -lr * Pred Grad: 0.074, New P: 0.735
iter 2 loss: 0.778
Actual params: [0.7452, 0.735 ]
-Original Grad: -0.038, -lr * Pred Grad: 0.086, New P: 0.832
-Original Grad: -0.029, -lr * Pred Grad: -0.032, New P: 0.703
iter 3 loss: 0.797
Actual params: [0.8317, 0.7029]
-Original Grad: -0.439, -lr * Pred Grad: -0.035, New P: 0.797
-Original Grad: -0.322, -lr * Pred Grad: -0.067, New P: 0.636
iter 4 loss: 0.774
Actual params: [0.7971, 0.6358]
-Original Grad: 0.127, -lr * Pred Grad: -0.051, New P: 0.746
-Original Grad: -0.280, -lr * Pred Grad: -0.078, New P: 0.558
iter 5 loss: 0.742
Actual params: [0.7461, 0.5579]
-Original Grad: -0.142, -lr * Pred Grad: -0.043, New P: 0.703
-Original Grad: -0.265, -lr * Pred Grad: -0.082, New P: 0.476
iter 6 loss: 0.721
Actual params: [0.7028, 0.476 ]
-Original Grad: 0.417, -lr * Pred Grad: 0.008, New P: 0.711
-Original Grad: 0.238, -lr * Pred Grad: -0.074, New P: 0.402
iter 7 loss: 0.686
Actual params: [0.7106, 0.4016]
-Original Grad: 0.634, -lr * Pred Grad: 0.059, New P: 0.770
-Original Grad: 0.008, -lr * Pred Grad: -0.066, New P: 0.336
iter 8 loss: 0.622
Actual params: [0.7697, 0.3356]
-Original Grad: 0.333, -lr * Pred Grad: 0.082, New P: 0.852
-Original Grad: -0.109, -lr * Pred Grad: -0.061, New P: 0.274
iter 9 loss: 0.519
Actual params: [0.8521, 0.2742]
-Original Grad: 0.453, -lr * Pred Grad: 0.087, New P: 0.940
-Original Grad: -0.210, -lr * Pred Grad: -0.065, New P: 0.209
iter 10 loss: 0.376
Actual params: [0.9395, 0.2092]
-Original Grad: -0.028, -lr * Pred Grad: 0.088, New P: 1.028
-Original Grad: -0.236, -lr * Pred Grad: -0.070, New P: 0.139
iter 11 loss: 0.331
Actual params: [1.0277, 0.1389]
-Original Grad: 0.622, -lr * Pred Grad: 0.088, New P: 1.116
-Original Grad: -0.182, -lr * Pred Grad: -0.074, New P: 0.065
iter 12 loss: 0.341
Actual params: [1.116 , 0.0647]
-Original Grad: 0.265, -lr * Pred Grad: 0.088, New P: 1.204
-Original Grad: 0.095, -lr * Pred Grad: -0.069, New P: -0.004
iter 13 loss: 0.402
Actual params: [ 1.2043, -0.0041]
-Original Grad: 0.912, -lr * Pred Grad: 0.088, New P: 1.293
-Original Grad: 1.403, -lr * Pred Grad: -0.058, New P: -0.062
iter 14 loss: 0.461
Actual params: [ 1.2927, -0.0619]
-Original Grad: 0.193, -lr * Pred Grad: 0.088, New P: 1.381
-Original Grad: 0.266, -lr * Pred Grad: -0.038, New P: -0.100
iter 15 loss: 0.510
Actual params: [ 1.381 , -0.0998]
-Original Grad: -0.237, -lr * Pred Grad: 0.088, New P: 1.469
-Original Grad: -0.057, -lr * Pred Grad: -0.025, New P: -0.124
iter 16 loss: 0.545
Actual params: [ 1.4693, -0.1243]
-Original Grad: -0.056, -lr * Pred Grad: 0.088, New P: 1.558
-Original Grad: 0.183, -lr * Pred Grad: -0.017, New P: -0.141
iter 17 loss: 0.570
Actual params: [ 1.5577, -0.1414]
-Original Grad: -0.109, -lr * Pred Grad: 0.088, New P: 1.646
-Original Grad: -0.012, -lr * Pred Grad: -0.005, New P: -0.147
iter 18 loss: 0.587
Actual params: [ 1.646 , -0.1469]
-Original Grad: -0.026, -lr * Pred Grad: 0.088, New P: 1.734
-Original Grad: 0.197, -lr * Pred Grad: 0.022, New P: -0.125
iter 19 loss: 0.597
Actual params: [ 1.7341, -0.1246]
-Original Grad: -0.041, -lr * Pred Grad: 0.087, New P: 1.822
-Original Grad: 0.342, -lr * Pred Grad: 0.058, New P: -0.067
iter 20 loss: 0.614
Actual params: [ 1.8216, -0.0671]
-Original Grad: -0.022, -lr * Pred Grad: 0.080, New P: 1.902
-Original Grad: 0.182, -lr * Pred Grad: 0.079, New P: 0.012
Target params: [1.1812, 0.2779]
iter 0 loss: 0.336
Actual params: [0.5941, 0.5941]
-Original Grad: -0.090, -lr * Pred Grad: -0.044, New P: 0.550
-Original Grad: 0.090, -lr * Pred Grad: 0.064, New P: 0.658
iter 1 loss: 0.369
Actual params: [0.5504, 0.6584]
-Original Grad: 0.037, -lr * Pred Grad: -0.057, New P: 0.493
-Original Grad: -0.034, -lr * Pred Grad: 0.039, New P: 0.697
iter 2 loss: 0.417
Actual params: [0.493, 0.697]
-Original Grad: -0.096, -lr * Pred Grad: -0.066, New P: 0.427
-Original Grad: 0.007, -lr * Pred Grad: -0.035, New P: 0.662
iter 3 loss: 0.431
Actual params: [0.4267, 0.662 ]
-Original Grad: 0.039, -lr * Pred Grad: -0.063, New P: 0.364
-Original Grad: -0.054, -lr * Pred Grad: -0.052, New P: 0.610
iter 4 loss: 0.431
Actual params: [0.364 , 0.6098]
-Original Grad: -0.068, -lr * Pred Grad: -0.058, New P: 0.306
-Original Grad: 0.007, -lr * Pred Grad: -0.031, New P: 0.579
iter 5 loss: 0.444
Actual params: [0.3064, 0.5789]
-Original Grad: -0.131, -lr * Pred Grad: -0.062, New P: 0.244
-Original Grad: 0.122, -lr * Pred Grad: 0.029, New P: 0.607
iter 6 loss: 0.499
Actual params: [0.244 , 0.6075]
-Original Grad: -0.062, -lr * Pred Grad: -0.063, New P: 0.181
-Original Grad: -0.035, -lr * Pred Grad: 0.034, New P: 0.642
iter 7 loss: 0.543
Actual params: [0.1807, 0.6419]
-Original Grad: -0.003, -lr * Pred Grad: -0.057, New P: 0.124
-Original Grad: -0.021, -lr * Pred Grad: -0.031, New P: 0.611
iter 8 loss: 0.547
Actual params: [0.1238, 0.6109]
-Original Grad: -0.019, -lr * Pred Grad: -0.043, New P: 0.081
-Original Grad: 0.046, -lr * Pred Grad: 0.011, New P: 0.622
iter 9 loss: 0.564
Actual params: [0.0808, 0.622 ]
-Original Grad: 0.041, -lr * Pred Grad: -0.019, New P: 0.062
-Original Grad: -0.007, -lr * Pred Grad: -0.005, New P: 0.617
iter 10 loss: 0.565
Actual params: [0.0622, 0.6166]
-Original Grad: -0.022, -lr * Pred Grad: -0.010, New P: 0.052
-Original Grad: -0.026, -lr * Pred Grad: -0.023, New P: 0.594
iter 11 loss: 0.556
Actual params: [0.0523, 0.5938]
-Original Grad: 0.162, -lr * Pred Grad: 0.027, New P: 0.080
-Original Grad: -0.130, -lr * Pred Grad: -0.049, New P: 0.544
iter 12 loss: 0.527
Actual params: [0.0797, 0.5444]
-Original Grad: 0.016, -lr * Pred Grad: 0.049, New P: 0.129
-Original Grad: 0.095, -lr * Pred Grad: -0.018, New P: 0.527
iter 13 loss: 0.502
Actual params: [0.1289, 0.5266]
-Original Grad: -0.062, -lr * Pred Grad: -0.019, New P: 0.110
-Original Grad: 0.122, -lr * Pred Grad: 0.036, New P: 0.563
iter 14 loss: 0.524
Actual params: [0.1096, 0.5625]
-Original Grad: 0.222, -lr * Pred Grad: 0.042, New P: 0.152
-Original Grad: -0.004, -lr * Pred Grad: 0.058, New P: 0.621
iter 15 loss: 0.540
Actual params: [0.1517, 0.6209]
-Original Grad: -0.074, -lr * Pred Grad: 0.029, New P: 0.180
-Original Grad: 0.013, -lr * Pred Grad: -0.012, New P: 0.609
iter 16 loss: 0.525
Actual params: [0.1804, 0.6092]
-Original Grad: 0.026, -lr * Pred Grad: -0.012, New P: 0.168
-Original Grad: 0.040, -lr * Pred Grad: 0.033, New P: 0.642
iter 17 loss: 0.547
Actual params: [0.1679, 0.6421]
-Original Grad: 0.056, -lr * Pred Grad: 0.026, New P: 0.194
-Original Grad: -0.010, -lr * Pred Grad: -0.019, New P: 0.623
iter 18 loss: 0.528
Actual params: [0.1942, 0.6229]
-Original Grad: -0.034, -lr * Pred Grad: -0.021, New P: 0.173
-Original Grad: 0.042, -lr * Pred Grad: 0.024, New P: 0.647
iter 19 loss: 0.548
Actual params: [0.1727, 0.6474]
-Original Grad: -0.077, -lr * Pred Grad: -0.049, New P: 0.124
-Original Grad: 0.014, -lr * Pred Grad: -0.001, New P: 0.646
iter 20 loss: 0.566
Actual params: [0.124 , 0.6465]
-Original Grad: 0.085, -lr * Pred Grad: -0.009, New P: 0.115
-Original Grad: -0.013, -lr * Pred Grad: -0.012, New P: 0.635
Target params: [1.1812, 0.2779]
iter 0 loss: 0.467
Actual params: [0.5941, 0.5941]
-Original Grad: -0.071, -lr * Pred Grad: -0.032, New P: 0.562
-Original Grad: 0.033, -lr * Pred Grad: 0.053, New P: 0.647
iter 1 loss: 0.645
Actual params: [0.5617, 0.6474]
-Original Grad: 0.001, -lr * Pred Grad: -0.059, New P: 0.502
-Original Grad: 0.009, -lr * Pred Grad: 0.008, New P: 0.656
iter 2 loss: 0.605
Actual params: [0.5025, 0.6557]
-Original Grad: 0.021, -lr * Pred Grad: -0.062, New P: 0.440
-Original Grad: -0.363, -lr * Pred Grad: -0.061, New P: 0.595
iter 3 loss: 0.315
Actual params: [0.4404, 0.5949]
-Original Grad: -0.260, -lr * Pred Grad: -0.068, New P: 0.373
-Original Grad: 0.587, -lr * Pred Grad: 0.007, New P: 0.601
iter 4 loss: 0.331
Actual params: [0.3725, 0.6015]
-Original Grad: -0.092, -lr * Pred Grad: -0.069, New P: 0.304
-Original Grad: 0.234, -lr * Pred Grad: 0.069, New P: 0.670
iter 5 loss: 0.579
Actual params: [0.304 , 0.6704]
-Original Grad: 0.685, -lr * Pred Grad: -0.050, New P: 0.254
-Original Grad: 0.048, -lr * Pred Grad: 0.085, New P: 0.755
iter 6 loss: 0.700
Actual params: [0.2543, 0.755 ]
-Original Grad: 4.433, -lr * Pred Grad: -0.024, New P: 0.230
-Original Grad: 0.603, -lr * Pred Grad: 0.088, New P: 0.843
iter 7 loss: 0.774
Actual params: [0.2303, 0.8426]
-Original Grad: -0.131, -lr * Pred Grad: 0.010, New P: 0.240
-Original Grad: -0.548, -lr * Pred Grad: 0.023, New P: 0.866
iter 8 loss: 0.787
Actual params: [0.2399, 0.8661]
-Original Grad: 1.522, -lr * Pred Grad: 0.058, New P: 0.298
-Original Grad: -0.488, -lr * Pred Grad: -0.047, New P: 0.819
iter 9 loss: 0.736
Actual params: [0.2977, 0.8189]
-Original Grad: 0.832, -lr * Pred Grad: 0.083, New P: 0.380
-Original Grad: -1.049, -lr * Pred Grad: -0.066, New P: 0.753
iter 10 loss: 0.868
Actual params: [0.3802, 0.7528]
-Original Grad: 0.221, -lr * Pred Grad: 0.087, New P: 0.468
-Original Grad: -0.427, -lr * Pred Grad: -0.075, New P: 0.678
iter 11 loss: 0.666
Actual params: [0.4677, 0.6778]
-Original Grad: -0.190, -lr * Pred Grad: 0.088, New P: 0.556
-Original Grad: -0.695, -lr * Pred Grad: -0.079, New P: 0.599
iter 12 loss: 0.442
Actual params: [0.5559, 0.599 ]
-Original Grad: -0.008, -lr * Pred Grad: 0.088, New P: 0.644
-Original Grad: -0.321, -lr * Pred Grad: -0.082, New P: 0.517
iter 13 loss: 0.343
Actual params: [0.6442, 0.5167]
-Original Grad: -0.162, -lr * Pred Grad: 0.088, New P: 0.733
-Original Grad: -0.431, -lr * Pred Grad: -0.084, New P: 0.433
iter 14 loss: 0.288
Actual params: [0.7325, 0.4328]
-Original Grad: -0.033, -lr * Pred Grad: 0.088, New P: 0.821
-Original Grad: -0.168, -lr * Pred Grad: -0.084, New P: 0.349
iter 15 loss: 0.277
Actual params: [0.8209, 0.3485]
-Original Grad: -0.019, -lr * Pred Grad: 0.088, New P: 0.909
-Original Grad: -0.184, -lr * Pred Grad: -0.084, New P: 0.264
iter 16 loss: 0.278
Actual params: [0.9092, 0.2641]
-Original Grad: -0.109, -lr * Pred Grad: 0.088, New P: 0.997
-Original Grad: 0.037, -lr * Pred Grad: -0.084, New P: 0.180
iter 17 loss: 0.284
Actual params: [0.9974, 0.1803]
-Original Grad: -0.042, -lr * Pred Grad: 0.088, New P: 1.085
-Original Grad: -0.130, -lr * Pred Grad: -0.083, New P: 0.097
iter 18 loss: 0.290
Actual params: [1.0852, 0.0971]
-Original Grad: -0.072, -lr * Pred Grad: 0.083, New P: 1.168
-Original Grad: 0.002, -lr * Pred Grad: -0.077, New P: 0.021
iter 19 loss: 0.297
Actual params: [1.1683, 0.0205]
-Original Grad: -0.038, -lr * Pred Grad: 0.062, New P: 1.231
-Original Grad: -0.072, -lr * Pred Grad: -0.071, New P: -0.051
iter 20 loss: 0.303
Actual params: [ 1.2307, -0.0506]
-Original Grad: -0.003, -lr * Pred Grad: 0.013, New P: 1.244
-Original Grad: -0.030, -lr * Pred Grad: -0.069, New P: -0.119
Target params: [1.1812, 0.2779]
iter 0 loss: 1.460
Actual params: [0.5941, 0.5941]
-Original Grad: -0.005, -lr * Pred Grad: 0.031, New P: 0.625
-Original Grad: -0.055, -lr * Pred Grad: -0.019, New P: 0.575
iter 1 loss: 1.450
Actual params: [0.6253, 0.5754]
-Original Grad: -0.039, -lr * Pred Grad: -0.045, New P: 0.580
-Original Grad: -0.073, -lr * Pred Grad: -0.067, New P: 0.508
iter 2 loss: 1.456
Actual params: [0.5801, 0.5083]
-Original Grad: -0.230, -lr * Pred Grad: -0.075, New P: 0.505
-Original Grad: -0.048, -lr * Pred Grad: -0.072, New P: 0.436
iter 3 loss: 1.463
Actual params: [0.5049, 0.4363]
-Original Grad: 0.032, -lr * Pred Grad: -0.073, New P: 0.432
-Original Grad: -0.154, -lr * Pred Grad: -0.071, New P: 0.365
iter 4 loss: 1.464
Actual params: [0.4321, 0.3653]
-Original Grad: -0.013, -lr * Pred Grad: -0.068, New P: 0.365
-Original Grad: -0.000, -lr * Pred Grad: -0.068, New P: 0.297
iter 5 loss: 1.464
Actual params: [0.3645, 0.2973]
-Original Grad: -0.002, -lr * Pred Grad: -0.057, New P: 0.308
-Original Grad: -0.048, -lr * Pred Grad: -0.065, New P: 0.232
iter 6 loss: 1.464
Actual params: [0.3077, 0.2324]
-Original Grad: 0.136, -lr * Pred Grad: -0.025, New P: 0.282
-Original Grad: 0.012, -lr * Pred Grad: -0.055, New P: 0.177
iter 7 loss: 1.462
Actual params: [0.2822, 0.1771]
-Original Grad: 0.032, -lr * Pred Grad: 0.018, New P: 0.301
-Original Grad: 0.010, -lr * Pred Grad: -0.038, New P: 0.139
iter 8 loss: 1.458
Actual params: [0.3006, 0.1386]
-Original Grad: 0.097, -lr * Pred Grad: 0.059, New P: 0.360
-Original Grad: -0.016, -lr * Pred Grad: -0.029, New P: 0.110
iter 9 loss: 1.449
Actual params: [0.3597, 0.1099]
-Original Grad: -0.169, -lr * Pred Grad: -0.038, New P: 0.321
-Original Grad: 0.040, -lr * Pred Grad: -0.005, New P: 0.105
iter 10 loss: 1.454
Actual params: [0.3214, 0.1052]
-Original Grad: 0.151, -lr * Pred Grad: 0.014, New P: 0.336
-Original Grad: 0.021, -lr * Pred Grad: 0.016, New P: 0.121
iter 11 loss: 1.454
Actual params: [0.3356, 0.1211]
-Original Grad: -0.140, -lr * Pred Grad: -0.042, New P: 0.294
-Original Grad: -0.022, -lr * Pred Grad: -0.022, New P: 0.099
iter 12 loss: 1.455
Actual params: [0.2938, 0.0987]
-Original Grad: 0.092, -lr * Pred Grad: 0.002, New P: 0.296
-Original Grad: 0.008, -lr * Pred Grad: -0.023, New P: 0.076
iter 13 loss: 1.453
Actual params: [0.2963, 0.0759]
-Original Grad: 0.195, -lr * Pred Grad: 0.061, New P: 0.357
-Original Grad: 0.007, -lr * Pred Grad: -0.017, New P: 0.059
iter 14 loss: 1.438
Actual params: [0.3571, 0.0587]
-Original Grad: 0.157, -lr * Pred Grad: 0.081, New P: 0.438
-Original Grad: -0.038, -lr * Pred Grad: -0.032, New P: 0.026
iter 15 loss: 1.419
Actual params: [0.4385, 0.0265]
-Original Grad: 0.046, -lr * Pred Grad: 0.083, New P: 0.522
-Original Grad: 0.052, -lr * Pred Grad: -0.014, New P: 0.012
iter 16 loss: 1.394
Actual params: [0.522 , 0.0125]
-Original Grad: -0.033, -lr * Pred Grad: 0.004, New P: 0.526
-Original Grad: 0.026, -lr * Pred Grad: 0.008, New P: 0.021
iter 17 loss: 1.394
Actual params: [0.5261, 0.0205]
-Original Grad: 0.095, -lr * Pred Grad: 0.055, New P: 0.581
-Original Grad: 0.061, -lr * Pred Grad: 0.029, New P: 0.049
iter 18 loss: 1.378
Actual params: [0.581 , 0.0492]
-Original Grad: 0.077, -lr * Pred Grad: 0.068, New P: 0.649
-Original Grad: -0.028, -lr * Pred Grad: -0.007, New P: 0.042
iter 19 loss: 1.344
Actual params: [0.6487, 0.0421]
-Original Grad: -0.134, -lr * Pred Grad: -0.034, New P: 0.615
-Original Grad: -0.015, -lr * Pred Grad: -0.033, New P: 0.009
iter 20 loss: 1.358
Actual params: [0.6147, 0.0094]
-Original Grad: -0.024, -lr * Pred Grad: -0.049, New P: 0.566
-Original Grad: -0.014, -lr * Pred Grad: -0.038, New P: -0.029
Target params: [1.1812, 0.2779]
iter 0 loss: 0.919
Actual params: [0.5941, 0.5941]
-Original Grad: 0.066, -lr * Pred Grad: 0.062, New P: 0.656
-Original Grad: -0.071, -lr * Pred Grad: -0.032, New P: 0.562
iter 1 loss: 0.884
Actual params: [0.6557, 0.562 ]
-Original Grad: -0.024, -lr * Pred Grad: 0.017, New P: 0.673
-Original Grad: 0.100, -lr * Pred Grad: -0.005, New P: 0.557
iter 2 loss: 0.873
Actual params: [0.6732, 0.5566]
-Original Grad: -0.056, -lr * Pred Grad: -0.052, New P: 0.622
-Original Grad: 0.179, -lr * Pred Grad: 0.054, New P: 0.611
iter 3 loss: 0.909
Actual params: [0.6215, 0.6111]
-Original Grad: -0.212, -lr * Pred Grad: -0.072, New P: 0.550
-Original Grad: 0.179, -lr * Pred Grad: 0.080, New P: 0.691
iter 4 loss: 0.956
Actual params: [0.5497, 0.6914]
-Original Grad: -0.072, -lr * Pred Grad: -0.071, New P: 0.479
-Original Grad: 0.026, -lr * Pred Grad: 0.079, New P: 0.770
iter 5 loss: 0.996
Actual params: [0.4791, 0.7703]
-Original Grad: 0.020, -lr * Pred Grad: -0.066, New P: 0.413
-Original Grad: 0.020, -lr * Pred Grad: 0.021, New P: 0.791
iter 6 loss: 1.023
Actual params: [0.4127, 0.7914]
-Original Grad: 0.247, -lr * Pred Grad: -0.046, New P: 0.367
-Original Grad: 0.135, -lr * Pred Grad: 0.068, New P: 0.859
iter 7 loss: 1.050
Actual params: [0.3671, 0.8591]
-Original Grad: 0.110, -lr * Pred Grad: -0.014, New P: 0.353
-Original Grad: 0.137, -lr * Pred Grad: 0.081, New P: 0.940
iter 8 loss: 1.064
Actual params: [0.3532, 0.9403]
-Original Grad: 0.232, -lr * Pred Grad: 0.032, New P: 0.385
-Original Grad: 0.064, -lr * Pred Grad: 0.080, New P: 1.020
iter 9 loss: 1.049
Actual params: [0.3853, 1.0201]
-Original Grad: 0.092, -lr * Pred Grad: 0.073, New P: 0.458
-Original Grad: -0.007, -lr * Pred Grad: 0.013, New P: 1.033
iter 10 loss: 1.024
Actual params: [0.458 , 1.0329]
-Original Grad: 0.084, -lr * Pred Grad: 0.083, New P: 0.541
-Original Grad: 0.024, -lr * Pred Grad: 0.035, New P: 1.068
iter 11 loss: 0.991
Actual params: [0.5411, 1.068 ]
-Original Grad: -0.017, -lr * Pred Grad: 0.031, New P: 0.572
-Original Grad: -0.054, -lr * Pred Grad: -0.036, New P: 1.032
iter 12 loss: 0.984
Actual params: [0.5718, 1.0324]
-Original Grad: -0.020, -lr * Pred Grad: 0.002, New P: 0.574
-Original Grad: -0.038, -lr * Pred Grad: -0.042, New P: 0.990
iter 13 loss: 0.988
Actual params: [0.5737, 0.99  ]
-Original Grad: -0.062, -lr * Pred Grad: -0.039, New P: 0.535
-Original Grad: -0.028, -lr * Pred Grad: -0.038, New P: 0.952
iter 14 loss: 1.003
Actual params: [0.5349, 0.9518]
-Original Grad: 0.095, -lr * Pred Grad: 0.017, New P: 0.552
-Original Grad: 0.078, -lr * Pred Grad: 0.010, New P: 0.961
iter 15 loss: 0.996
Actual params: [0.5522, 0.9614]
-Original Grad: 0.005, -lr * Pred Grad: 0.026, New P: 0.578
-Original Grad: 0.062, -lr * Pred Grad: 0.050, New P: 1.012
iter 16 loss: 0.984
Actual params: [0.5781, 1.0117]
-Original Grad: -0.014, -lr * Pred Grad: -0.027, New P: 0.551
-Original Grad: 0.015, -lr * Pred Grad: 0.019, New P: 1.031
iter 17 loss: 0.992
Actual params: [0.5514, 1.0308]
-Original Grad: 0.078, -lr * Pred Grad: 0.027, New P: 0.579
-Original Grad: 0.146, -lr * Pred Grad: 0.067, New P: 1.098
iter 18 loss: 0.971
Actual params: [0.5788, 1.098 ]
-Original Grad: -0.051, -lr * Pred Grad: -0.027, New P: 0.552
-Original Grad: -0.020, -lr * Pred Grad: 0.017, New P: 1.115
iter 19 loss: 0.979
Actual params: [0.5517, 1.115 ]
-Original Grad: -0.073, -lr * Pred Grad: -0.047, New P: 0.505
-Original Grad: -0.039, -lr * Pred Grad: -0.025, New P: 1.090
iter 20 loss: 1.000
Actual params: [0.505 , 1.0898]
-Original Grad: 0.022, -lr * Pred Grad: -0.028, New P: 0.477
-Original Grad: -0.063, -lr * Pred Grad: -0.047, New P: 1.043
Target params: [1.1812, 0.2779]
iter 0 loss: 0.968
Actual params: [0.5941, 0.5941]
-Original Grad: 0.149, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.182, -lr * Pred Grad: -0.061, New P: 0.533
iter 1 loss: 0.914
Actual params: [0.6609, 0.5327]
-Original Grad: -0.052, -lr * Pred Grad: 0.076, New P: 0.736
-Original Grad: -0.401, -lr * Pred Grad: -0.080, New P: 0.453
iter 2 loss: 0.822
Actual params: [0.7365, 0.4528]
-Original Grad: -0.025, -lr * Pred Grad: -0.029, New P: 0.708
-Original Grad: -0.067, -lr * Pred Grad: -0.083, New P: 0.369
iter 3 loss: 0.828
Actual params: [0.7079, 0.3694]
-Original Grad: -0.188, -lr * Pred Grad: -0.060, New P: 0.648
-Original Grad: -0.236, -lr * Pred Grad: -0.084, New P: 0.285
iter 4 loss: 0.848
Actual params: [0.6476, 0.2854]
-Original Grad: 0.044, -lr * Pred Grad: -0.058, New P: 0.590
-Original Grad: -0.119, -lr * Pred Grad: -0.084, New P: 0.201
iter 5 loss: 0.861
Actual params: [0.5896, 0.2012]
-Original Grad: 0.003, -lr * Pred Grad: -0.031, New P: 0.559
-Original Grad: 0.046, -lr * Pred Grad: -0.081, New P: 0.120
iter 6 loss: 0.864
Actual params: [0.5587, 0.1198]
-Original Grad: 0.093, -lr * Pred Grad: 0.022, New P: 0.581
-Original Grad: -0.051, -lr * Pred Grad: -0.074, New P: 0.046
iter 7 loss: 0.846
Actual params: [0.5806, 0.0457]
-Original Grad: 0.073, -lr * Pred Grad: 0.063, New P: 0.644
-Original Grad: 0.008, -lr * Pred Grad: -0.070, New P: -0.024
iter 8 loss: 0.805
Actual params: [ 0.6436, -0.024 ]
-Original Grad: -0.103, -lr * Pred Grad: -0.031, New P: 0.613
-Original Grad: 0.037, -lr * Pred Grad: -0.065, New P: -0.089
iter 9 loss: 0.821
Actual params: [ 0.6126, -0.0889]
-Original Grad: 0.077, -lr * Pred Grad: 0.012, New P: 0.624
-Original Grad: 0.000, -lr * Pred Grad: -0.049, New P: -0.138
iter 10 loss: 0.811
Actual params: [ 0.6243, -0.138 ]
-Original Grad: 0.046, -lr * Pred Grad: 0.044, New P: 0.669
-Original Grad: 0.005, -lr * Pred Grad: -0.037, New P: -0.175
iter 11 loss: 0.786
Actual params: [ 0.6687, -0.175 ]
-Original Grad: -0.027, -lr * Pred Grad: -0.020, New P: 0.649
-Original Grad: 0.004, -lr * Pred Grad: -0.032, New P: -0.207
iter 12 loss: 0.795
Actual params: [ 0.6489, -0.207 ]
-Original Grad: 0.016, -lr * Pred Grad: 0.007, New P: 0.656
-Original Grad: 0.014, -lr * Pred Grad: -0.030, New P: -0.237
iter 13 loss: 0.792
Actual params: [ 0.6558, -0.2369]
-Original Grad: -0.221, -lr * Pred Grad: -0.046, New P: 0.610
-Original Grad: -0.044, -lr * Pred Grad: -0.042, New P: -0.279
iter 14 loss: 0.815
Actual params: [ 0.6098, -0.2786]
-Original Grad: -0.020, -lr * Pred Grad: -0.056, New P: 0.554
-Original Grad: -0.019, -lr * Pred Grad: -0.042, New P: -0.320
iter 15 loss: 0.844
Actual params: [ 0.5539, -0.3205]
-Original Grad: 0.082, -lr * Pred Grad: -0.024, New P: 0.530
-Original Grad: -0.008, -lr * Pred Grad: -0.031, New P: -0.352
iter 16 loss: 0.854
Actual params: [ 0.5302, -0.3518]
-Original Grad: 0.032, -lr * Pred Grad: 0.027, New P: 0.558
-Original Grad: -0.039, -lr * Pred Grad: -0.035, New P: -0.387
iter 17 loss: 0.839
Actual params: [ 0.5577, -0.3872]
-Original Grad: 0.135, -lr * Pred Grad: 0.067, New P: 0.625
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -0.419
iter 18 loss: 0.801
Actual params: [ 0.6249, -0.4187]
-Original Grad: -0.051, -lr * Pred Grad: 0.002, New P: 0.627
-Original Grad: -0.036, -lr * Pred Grad: -0.036, New P: -0.454
iter 19 loss: 0.799
Actual params: [ 0.6271, -0.4542]
-Original Grad: 0.028, -lr * Pred Grad: 0.019, New P: 0.646
-Original Grad: -0.065, -lr * Pred Grad: -0.047, New P: -0.501
iter 20 loss: 0.784
Actual params: [ 0.6457, -0.5011]
-Original Grad: -0.073, -lr * Pred Grad: -0.040, New P: 0.606
-Original Grad: -0.085, -lr * Pred Grad: -0.056, New P: -0.557
Target params: [1.1812, 0.2779]
iter 0 loss: 0.496
Actual params: [0.5941, 0.5941]
-Original Grad: 0.034, -lr * Pred Grad: 0.054, New P: 0.648
-Original Grad: -0.141, -lr * Pred Grad: -0.058, New P: 0.536
iter 1 loss: 0.474
Actual params: [0.648 , 0.5362]
-Original Grad: -0.057, -lr * Pred Grad: -0.033, New P: 0.615
-Original Grad: -0.144, -lr * Pred Grad: -0.079, New P: 0.457
iter 2 loss: 0.475
Actual params: [0.6148, 0.4568]
-Original Grad: -0.016, -lr * Pred Grad: -0.060, New P: 0.555
-Original Grad: -0.215, -lr * Pred Grad: -0.083, New P: 0.374
iter 3 loss: 0.485
Actual params: [0.555 , 0.3737]
-Original Grad: -0.088, -lr * Pred Grad: -0.064, New P: 0.491
-Original Grad: -0.119, -lr * Pred Grad: -0.083, New P: 0.290
iter 4 loss: 0.497
Actual params: [0.4906, 0.2903]
-Original Grad: -0.073, -lr * Pred Grad: -0.064, New P: 0.426
-Original Grad: -0.019, -lr * Pred Grad: -0.080, New P: 0.211
iter 5 loss: 0.505
Actual params: [0.4263, 0.2108]
-Original Grad: -0.047, -lr * Pred Grad: -0.062, New P: 0.364
-Original Grad: -0.070, -lr * Pred Grad: -0.073, New P: 0.137
iter 6 loss: 0.513
Actual params: [0.364 , 0.1374]
-Original Grad: -0.026, -lr * Pred Grad: -0.057, New P: 0.307
-Original Grad: 0.044, -lr * Pred Grad: -0.069, New P: 0.068
iter 7 loss: 0.517
Actual params: [0.3071, 0.0681]
-Original Grad: -0.006, -lr * Pred Grad: -0.043, New P: 0.264
-Original Grad: -0.006, -lr * Pred Grad: -0.064, New P: 0.004
iter 8 loss: 0.520
Actual params: [0.2637, 0.0041]
-Original Grad: -0.048, -lr * Pred Grad: -0.038, New P: 0.226
-Original Grad: 0.059, -lr * Pred Grad: -0.046, New P: -0.041
iter 9 loss: 0.523
Actual params: [ 0.2259, -0.0414]
-Original Grad: 0.037, -lr * Pred Grad: -0.014, New P: 0.212
-Original Grad: 0.018, -lr * Pred Grad: -0.032, New P: -0.073
iter 10 loss: 0.523
Actual params: [ 0.212, -0.073]
-Original Grad: 0.013, -lr * Pred Grad: 0.003, New P: 0.215
-Original Grad: 0.049, -lr * Pred Grad: -0.025, New P: -0.098
iter 11 loss: 0.522
Actual params: [ 0.2151, -0.0979]
-Original Grad: 0.016, -lr * Pred Grad: 0.003, New P: 0.218
-Original Grad: -0.000, -lr * Pred Grad: -0.021, New P: -0.119
iter 12 loss: 0.521
Actual params: [ 0.2182, -0.119 ]
-Original Grad: 0.010, -lr * Pred Grad: -0.006, New P: 0.213
-Original Grad: 0.043, -lr * Pred Grad: -0.006, New P: -0.125
iter 13 loss: 0.522
Actual params: [ 0.2126, -0.1246]
-Original Grad: 0.091, -lr * Pred Grad: 0.033, New P: 0.245
-Original Grad: 0.025, -lr * Pred Grad: 0.020, New P: -0.105
iter 14 loss: 0.520
Actual params: [ 0.2454, -0.1045]
-Original Grad: -0.015, -lr * Pred Grad: 0.011, New P: 0.257
-Original Grad: 0.034, -lr * Pred Grad: 0.011, New P: -0.094
iter 15 loss: 0.519
Actual params: [ 0.2569, -0.0936]
-Original Grad: 0.009, -lr * Pred Grad: -0.010, New P: 0.247
-Original Grad: 0.021, -lr * Pred Grad: 0.007, New P: -0.087
iter 16 loss: 0.520
Actual params: [ 0.247 , -0.0869]
-Original Grad: 0.040, -lr * Pred Grad: 0.005, New P: 0.252
-Original Grad: -0.005, -lr * Pred Grad: -0.017, New P: -0.104
iter 17 loss: 0.519
Actual params: [ 0.2517, -0.1035]
-Original Grad: -0.000, -lr * Pred Grad: -0.002, New P: 0.250
-Original Grad: 0.006, -lr * Pred Grad: -0.020, New P: -0.124
iter 18 loss: 0.519
Actual params: [ 0.25  , -0.1237]
-Original Grad: -0.045, -lr * Pred Grad: -0.030, New P: 0.220
-Original Grad: 0.051, -lr * Pred Grad: 0.006, New P: -0.118
iter 19 loss: 0.521
Actual params: [ 0.2198, -0.1178]
-Original Grad: 0.042, -lr * Pred Grad: -0.016, New P: 0.204
-Original Grad: 0.018, -lr * Pred Grad: 0.011, New P: -0.107
iter 20 loss: 0.523
Actual params: [ 0.2036, -0.1067]
-Original Grad: 0.036, -lr * Pred Grad: 0.013, New P: 0.216
-Original Grad: 0.012, -lr * Pred Grad: -0.003, New P: -0.110
Target params: [1.1812, 0.2779]
iter 0 loss: 0.649
Actual params: [0.5941, 0.5941]
-Original Grad: -0.211, -lr * Pred Grad: -0.062, New P: 0.532
-Original Grad: -0.253, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.675
Actual params: [0.5317, 0.5311]
-Original Grad: 0.280, -lr * Pred Grad: -0.020, New P: 0.511
-Original Grad: 0.032, -lr * Pred Grad: -0.078, New P: 0.453
iter 2 loss: 0.692
Actual params: [0.5114, 0.4527]
-Original Grad: -0.115, -lr * Pred Grad: -0.058, New P: 0.454
-Original Grad: -0.025, -lr * Pred Grad: -0.074, New P: 0.378
iter 3 loss: 0.712
Actual params: [0.4538, 0.3785]
-Original Grad: 0.157, -lr * Pred Grad: -0.012, New P: 0.442
-Original Grad: -0.029, -lr * Pred Grad: -0.070, New P: 0.309
iter 4 loss: 0.725
Actual params: [0.4418, 0.3088]
-Original Grad: 0.033, -lr * Pred Grad: 0.047, New P: 0.488
-Original Grad: 0.008, -lr * Pred Grad: -0.066, New P: 0.243
iter 5 loss: 0.728
Actual params: [0.4883, 0.2427]
-Original Grad: -0.037, -lr * Pred Grad: -0.018, New P: 0.470
-Original Grad: -0.074, -lr * Pred Grad: -0.062, New P: 0.180
iter 6 loss: 0.736
Actual params: [0.4705, 0.1803]
-Original Grad: 0.183, -lr * Pred Grad: 0.051, New P: 0.521
-Original Grad: 0.112, -lr * Pred Grad: -0.042, New P: 0.139
iter 7 loss: 0.734
Actual params: [0.5214, 0.1386]
-Original Grad: 0.075, -lr * Pred Grad: 0.077, New P: 0.599
-Original Grad: 0.048, -lr * Pred Grad: -0.024, New P: 0.114
iter 8 loss: 0.726
Actual params: [0.5989, 0.1144]
-Original Grad: -0.107, -lr * Pred Grad: -0.027, New P: 0.572
-Original Grad: -0.090, -lr * Pred Grad: -0.022, New P: 0.092
iter 9 loss: 0.721
Actual params: [0.5724, 0.0921]
-Original Grad: -0.001, -lr * Pred Grad: -0.032, New P: 0.540
-Original Grad: -0.034, -lr * Pred Grad: -0.048, New P: 0.044
iter 10 loss: 0.728
Actual params: [0.5399, 0.044 ]
-Original Grad: 0.334, -lr * Pred Grad: 0.034, New P: 0.574
-Original Grad: 0.040, -lr * Pred Grad: -0.024, New P: 0.020
iter 11 loss: 0.723
Actual params: [0.5741, 0.0204]
-Original Grad: -0.063, -lr * Pred Grad: 0.074, New P: 0.648
-Original Grad: -0.040, -lr * Pred Grad: -0.022, New P: -0.002
iter 12 loss: 0.711
Actual params: [ 0.6482, -0.0017]
-Original Grad: -0.030, -lr * Pred Grad: -0.024, New P: 0.624
-Original Grad: -0.021, -lr * Pred Grad: -0.033, New P: -0.035
iter 13 loss: 0.714
Actual params: [ 0.6243, -0.0351]
-Original Grad: -0.190, -lr * Pred Grad: -0.053, New P: 0.571
-Original Grad: -0.024, -lr * Pred Grad: -0.038, New P: -0.073
iter 14 loss: 0.721
Actual params: [ 0.5715, -0.0727]
-Original Grad: 0.119, -lr * Pred Grad: -0.026, New P: 0.545
-Original Grad: 0.075, -lr * Pred Grad: -0.011, New P: -0.083
iter 15 loss: 0.724
Actual params: [ 0.5451, -0.0834]
-Original Grad: 0.028, -lr * Pred Grad: 0.028, New P: 0.573
-Original Grad: -0.019, -lr * Pred Grad: 0.002, New P: -0.081
iter 16 loss: 0.720
Actual params: [ 0.5728, -0.0812]
-Original Grad: 0.106, -lr * Pred Grad: 0.066, New P: 0.639
-Original Grad: -0.094, -lr * Pred Grad: -0.036, New P: -0.118
iter 17 loss: 0.707
Actual params: [ 0.6392, -0.1177]
-Original Grad: -0.213, -lr * Pred Grad: -0.038, New P: 0.602
-Original Grad: -0.099, -lr * Pred Grad: -0.054, New P: -0.172
iter 18 loss: 0.717
Actual params: [ 0.6015, -0.1718]
-Original Grad: 0.011, -lr * Pred Grad: -0.052, New P: 0.549
-Original Grad: -0.029, -lr * Pred Grad: -0.058, New P: -0.230
iter 19 loss: 0.724
Actual params: [ 0.5493, -0.2301]
-Original Grad: 0.371, -lr * Pred Grad: -0.004, New P: 0.545
-Original Grad: 0.014, -lr * Pred Grad: -0.044, New P: -0.274
iter 20 loss: 0.724
Actual params: [ 0.5451, -0.2742]
-Original Grad: 0.234, -lr * Pred Grad: 0.055, New P: 0.600
-Original Grad: -0.014, -lr * Pred Grad: -0.028, New P: -0.302
Target params: [1.1812, 0.2779]
iter 0 loss: 0.615
Actual params: [0.5941, 0.5941]
-Original Grad: 0.147, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: 0.032, -lr * Pred Grad: 0.053, New P: 0.647
iter 1 loss: 0.589
Actual params: [0.6609, 0.6471]
-Original Grad: -0.262, -lr * Pred Grad: -0.039, New P: 0.621
-Original Grad: 0.102, -lr * Pred Grad: 0.074, New P: 0.721
iter 2 loss: 0.617
Actual params: [0.6214, 0.7215]
-Original Grad: -0.267, -lr * Pred Grad: -0.073, New P: 0.549
-Original Grad: -0.086, -lr * Pred Grad: -0.034, New P: 0.688
iter 3 loss: 0.641
Actual params: [0.5487, 0.6878]
-Original Grad: 0.200, -lr * Pred Grad: -0.068, New P: 0.481
-Original Grad: 0.018, -lr * Pred Grad: -0.042, New P: 0.645
iter 4 loss: 0.660
Actual params: [0.4809, 0.6455]
-Original Grad: 0.121, -lr * Pred Grad: -0.034, New P: 0.447
-Original Grad: -0.118, -lr * Pred Grad: -0.053, New P: 0.592
iter 5 loss: 0.667
Actual params: [0.4467, 0.5924]
-Original Grad: -0.048, -lr * Pred Grad: 0.026, New P: 0.472
-Original Grad: 0.043, -lr * Pred Grad: -0.031, New P: 0.561
iter 6 loss: 0.657
Actual params: [0.4724, 0.5614]
-Original Grad: 0.233, -lr * Pred Grad: 0.071, New P: 0.543
-Original Grad: -0.111, -lr * Pred Grad: -0.045, New P: 0.517
iter 7 loss: 0.627
Actual params: [0.5434, 0.5166]
-Original Grad: -0.035, -lr * Pred Grad: 0.058, New P: 0.601
-Original Grad: 0.131, -lr * Pred Grad: 0.002, New P: 0.518
iter 8 loss: 0.602
Actual params: [0.6014, 0.5183]
-Original Grad: 0.041, -lr * Pred Grad: 0.013, New P: 0.615
-Original Grad: -0.105, -lr * Pred Grad: -0.020, New P: 0.499
iter 9 loss: 0.594
Actual params: [0.6146, 0.4987]
-Original Grad: 0.018, -lr * Pred Grad: 0.029, New P: 0.643
-Original Grad: 0.093, -lr * Pred Grad: 0.016, New P: 0.515
iter 10 loss: 0.578
Actual params: [0.6434, 0.5148]
-Original Grad: -0.152, -lr * Pred Grad: -0.041, New P: 0.602
-Original Grad: -0.097, -lr * Pred Grad: -0.031, New P: 0.484
iter 11 loss: 0.600
Actual params: [0.6019, 0.4841]
-Original Grad: -0.296, -lr * Pred Grad: -0.061, New P: 0.541
-Original Grad: -0.100, -lr * Pred Grad: -0.050, New P: 0.434
iter 12 loss: 0.628
Actual params: [0.5414, 0.4336]
-Original Grad: 0.212, -lr * Pred Grad: -0.040, New P: 0.502
-Original Grad: -0.010, -lr * Pred Grad: -0.049, New P: 0.385
iter 13 loss: 0.650
Actual params: [0.5018, 0.3846]
-Original Grad: 0.220, -lr * Pred Grad: 0.017, New P: 0.519
-Original Grad: -0.078, -lr * Pred Grad: -0.049, New P: 0.335
iter 14 loss: 0.646
Actual params: [0.5189, 0.3354]
-Original Grad: 0.042, -lr * Pred Grad: 0.066, New P: 0.585
-Original Grad: 0.186, -lr * Pred Grad: -0.010, New P: 0.325
iter 15 loss: 0.620
Actual params: [0.5847, 0.3251]
-Original Grad: 0.036, -lr * Pred Grad: 0.072, New P: 0.657
-Original Grad: 0.166, -lr * Pred Grad: 0.025, New P: 0.351
iter 16 loss: 0.573
Actual params: [0.6571, 0.3506]
-Original Grad: 0.230, -lr * Pred Grad: 0.083, New P: 0.740
-Original Grad: -0.037, -lr * Pred Grad: 0.056, New P: 0.406
iter 17 loss: 0.498
Actual params: [0.74  , 0.4064]
-Original Grad: -0.014, -lr * Pred Grad: 0.075, New P: 0.815
-Original Grad: 0.093, -lr * Pred Grad: 0.059, New P: 0.465
iter 18 loss: 0.429
Actual params: [0.8148, 0.465 ]
-Original Grad: -0.146, -lr * Pred Grad: -0.032, New P: 0.782
-Original Grad: -0.000, -lr * Pred Grad: 0.008, New P: 0.473
iter 19 loss: 0.463
Actual params: [0.7823, 0.4731]
-Original Grad: 0.612, -lr * Pred Grad: 0.034, New P: 0.816
-Original Grad: 0.015, -lr * Pred Grad: 0.020, New P: 0.493
iter 20 loss: 0.437
Actual params: [0.8162, 0.4933]
-Original Grad: -0.037, -lr * Pred Grad: 0.076, New P: 0.892
-Original Grad: -0.141, -lr * Pred Grad: -0.041, New P: 0.453
Target params: [1.1812, 0.2779]
iter 0 loss: 0.400
Actual params: [0.5941, 0.5941]
-Original Grad: -0.021, -lr * Pred Grad: 0.016, New P: 0.610
-Original Grad: -0.109, -lr * Pred Grad: -0.051, New P: 0.543
iter 1 loss: 0.318
Actual params: [0.6097, 0.543 ]
-Original Grad: 0.034, -lr * Pred Grad: -0.019, New P: 0.591
-Original Grad: 0.005, -lr * Pred Grad: -0.067, New P: 0.476
iter 2 loss: 0.336
Actual params: [0.5909, 0.4756]
-Original Grad: -0.090, -lr * Pred Grad: -0.060, New P: 0.531
-Original Grad: -0.107, -lr * Pred Grad: -0.070, New P: 0.406
iter 3 loss: 0.364
Actual params: [0.5308, 0.4058]
-Original Grad: -0.053, -lr * Pred Grad: -0.066, New P: 0.465
-Original Grad: -0.080, -lr * Pred Grad: -0.068, New P: 0.337
iter 4 loss: 0.376
Actual params: [0.4653, 0.3373]
-Original Grad: 0.010, -lr * Pred Grad: -0.058, New P: 0.407
-Original Grad: -0.254, -lr * Pred Grad: -0.073, New P: 0.265
iter 5 loss: 0.387
Actual params: [0.4072, 0.2648]
-Original Grad: 0.017, -lr * Pred Grad: -0.031, New P: 0.377
-Original Grad: -0.166, -lr * Pred Grad: -0.076, New P: 0.189
iter 6 loss: 0.401
Actual params: [0.3767, 0.1892]
-Original Grad: -0.048, -lr * Pred Grad: -0.020, New P: 0.356
-Original Grad: -0.030, -lr * Pred Grad: -0.071, New P: 0.118
iter 7 loss: 0.414
Actual params: [0.3562, 0.118 ]
-Original Grad: -0.023, -lr * Pred Grad: -0.036, New P: 0.320
-Original Grad: 0.025, -lr * Pred Grad: -0.067, New P: 0.051
iter 8 loss: 0.418
Actual params: [0.3198, 0.0505]
-Original Grad: -0.001, -lr * Pred Grad: -0.027, New P: 0.292
-Original Grad: -0.057, -lr * Pred Grad: -0.064, New P: -0.013
iter 9 loss: 0.425
Actual params: [ 0.2924, -0.0131]
-Original Grad: -0.024, -lr * Pred Grad: -0.028, New P: 0.265
-Original Grad: -0.072, -lr * Pred Grad: -0.062, New P: -0.075
iter 10 loss: 0.434
Actual params: [ 0.2648, -0.0751]
-Original Grad: -0.046, -lr * Pred Grad: -0.039, New P: 0.226
-Original Grad: -0.045, -lr * Pred Grad: -0.060, New P: -0.135
iter 11 loss: 0.437
Actual params: [ 0.2262, -0.135 ]
-Original Grad: -0.066, -lr * Pred Grad: -0.050, New P: 0.176
-Original Grad: -0.024, -lr * Pred Grad: -0.054, New P: -0.189
iter 12 loss: 0.433
Actual params: [ 0.1764, -0.1891]
-Original Grad: -0.026, -lr * Pred Grad: -0.050, New P: 0.127
-Original Grad: -0.133, -lr * Pred Grad: -0.061, New P: -0.250
iter 13 loss: 0.427
Actual params: [ 0.1265, -0.2503]
-Original Grad: -0.052, -lr * Pred Grad: -0.049, New P: 0.078
-Original Grad: 0.028, -lr * Pred Grad: -0.054, New P: -0.305
iter 14 loss: 0.424
Actual params: [ 0.0778, -0.3046]
-Original Grad: -0.061, -lr * Pred Grad: -0.051, New P: 0.027
-Original Grad: 0.096, -lr * Pred Grad: -0.032, New P: -0.336
iter 15 loss: 0.418
Actual params: [ 0.0268, -0.3362]
-Original Grad: -0.122, -lr * Pred Grad: -0.059, New P: -0.032
-Original Grad: 0.021, -lr * Pred Grad: -0.022, New P: -0.358
iter 16 loss: 0.408
Actual params: [-0.0323, -0.3583]
-Original Grad: -0.068, -lr * Pred Grad: -0.062, New P: -0.095
-Original Grad: -0.037, -lr * Pred Grad: -0.026, New P: -0.384
iter 17 loss: 0.401
Actual params: [-0.0945, -0.3838]
-Original Grad: -0.029, -lr * Pred Grad: -0.058, New P: -0.153
-Original Grad: 0.013, -lr * Pred Grad: -0.029, New P: -0.412
iter 18 loss: 0.399
Actual params: [-0.153 , -0.4125]
-Original Grad: -0.073, -lr * Pred Grad: -0.056, New P: -0.209
-Original Grad: 0.022, -lr * Pred Grad: -0.017, New P: -0.429
iter 19 loss: 0.397
Actual params: [-0.209 , -0.4292]
-Original Grad: 0.005, -lr * Pred Grad: -0.044, New P: -0.253
-Original Grad: -0.001, -lr * Pred Grad: -0.015, New P: -0.444
iter 20 loss: 0.397
Actual params: [-0.2528, -0.4438]
-Original Grad: -0.035, -lr * Pred Grad: -0.036, New P: -0.289
-Original Grad: 0.039, -lr * Pred Grad: -0.004, New P: -0.448
Target params: [1.1812, 0.2779]
iter 0 loss: 0.863
Actual params: [0.5941, 0.5941]
-Original Grad: 0.137, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: 0.176, -lr * Pred Grad: 0.067, New P: 0.661
iter 1 loss: 0.714
Actual params: [0.6607, 0.6613]
-Original Grad: -0.324, -lr * Pred Grad: -0.044, New P: 0.617
-Original Grad: -0.004, -lr * Pred Grad: 0.083, New P: 0.744
iter 2 loss: 0.679
Actual params: [0.617, 0.744]
-Original Grad: 0.020, -lr * Pred Grad: -0.068, New P: 0.549
-Original Grad: 0.235, -lr * Pred Grad: 0.086, New P: 0.830
iter 3 loss: 0.715
Actual params: [0.5493, 0.8299]
-Original Grad: 0.293, -lr * Pred Grad: -0.026, New P: 0.523
-Original Grad: 0.061, -lr * Pred Grad: 0.085, New P: 0.915
iter 4 loss: 0.728
Actual params: [0.5231, 0.9151]
-Original Grad: -0.104, -lr * Pred Grad: 0.036, New P: 0.560
-Original Grad: -0.133, -lr * Pred Grad: -0.017, New P: 0.899
iter 5 loss: 0.676
Actual params: [0.5596, 0.8986]
-Original Grad: 0.219, -lr * Pred Grad: 0.075, New P: 0.634
-Original Grad: -0.069, -lr * Pred Grad: -0.047, New P: 0.852
iter 6 loss: 0.567
Actual params: [0.6343, 0.8517]
-Original Grad: 0.438, -lr * Pred Grad: 0.086, New P: 0.720
-Original Grad: 0.082, -lr * Pred Grad: 0.001, New P: 0.852
iter 7 loss: 0.429
Actual params: [0.72  , 0.8525]
-Original Grad: -0.128, -lr * Pred Grad: 0.086, New P: 0.806
-Original Grad: -0.173, -lr * Pred Grad: -0.046, New P: 0.806
iter 8 loss: 0.386
Actual params: [0.8057, 0.8061]
-Original Grad: 0.009, -lr * Pred Grad: 0.021, New P: 0.826
-Original Grad: 0.043, -lr * Pred Grad: -0.038, New P: 0.768
iter 9 loss: 0.382
Actual params: [0.8264, 0.7676]
-Original Grad: 0.181, -lr * Pred Grad: 0.067, New P: 0.894
-Original Grad: 0.292, -lr * Pred Grad: 0.019, New P: 0.786
iter 10 loss: 0.486
Actual params: [0.8937, 0.7864]
-Original Grad: -0.168, -lr * Pred Grad: -0.028, New P: 0.866
-Original Grad: -0.178, -lr * Pred Grad: 0.040, New P: 0.827
iter 11 loss: 0.498
Actual params: [0.8657, 0.8269]
-Original Grad: -0.146, -lr * Pred Grad: -0.053, New P: 0.813
-Original Grad: -0.224, -lr * Pred Grad: -0.043, New P: 0.784
iter 12 loss: 0.384
Actual params: [0.8126, 0.7842]
-Original Grad: 0.054, -lr * Pred Grad: -0.043, New P: 0.770
-Original Grad: -0.039, -lr * Pred Grad: -0.057, New P: 0.728
iter 13 loss: 0.453
Actual params: [0.7698, 0.7275]
-Original Grad: -0.661, -lr * Pred Grad: -0.061, New P: 0.709
-Original Grad: -0.292, -lr * Pred Grad: -0.063, New P: 0.665
iter 14 loss: 0.636
Actual params: [0.7088, 0.6648]
-Original Grad: -0.817, -lr * Pred Grad: -0.072, New P: 0.637
-Original Grad: -0.227, -lr * Pred Grad: -0.070, New P: 0.595
iter 15 loss: 0.830
Actual params: [0.6372, 0.5951]
-Original Grad: 0.192, -lr * Pred Grad: -0.068, New P: 0.569
-Original Grad: 0.188, -lr * Pred Grad: -0.060, New P: 0.535
iter 16 loss: 0.948
Actual params: [0.5695, 0.5349]
-Original Grad: 0.059, -lr * Pred Grad: -0.060, New P: 0.509
-Original Grad: 0.077, -lr * Pred Grad: -0.034, New P: 0.501
iter 17 loss: 1.006
Actual params: [0.5092, 0.5009]
-Original Grad: 0.260, -lr * Pred Grad: -0.036, New P: 0.473
-Original Grad: 0.215, -lr * Pred Grad: -0.012, New P: 0.488
iter 18 loss: 1.021
Actual params: [0.4727, 0.4884]
-Original Grad: -0.020, -lr * Pred Grad: -0.014, New P: 0.458
-Original Grad: 0.060, -lr * Pred Grad: 0.018, New P: 0.506
iter 19 loss: 1.002
Actual params: [0.4583, 0.5064]
-Original Grad: 0.064, -lr * Pred Grad: 0.012, New P: 0.470
-Original Grad: 0.086, -lr * Pred Grad: 0.060, New P: 0.566
iter 20 loss: 0.931
Actual params: [0.4702, 0.5664]
-Original Grad: 0.156, -lr * Pred Grad: 0.052, New P: 0.522
-Original Grad: 0.245, -lr * Pred Grad: 0.082, New P: 0.648
Target params: [1.1812, 0.2779]
iter 0 loss: 0.858
Actual params: [0.5941, 0.5941]
-Original Grad: -0.142, -lr * Pred Grad: -0.058, New P: 0.536
-Original Grad: 0.071, -lr * Pred Grad: 0.062, New P: 0.656
iter 1 loss: 0.885
Actual params: [0.5361, 0.6564]
-Original Grad: -0.289, -lr * Pred Grad: -0.079, New P: 0.457
-Original Grad: -0.023, -lr * Pred Grad: 0.024, New P: 0.681
iter 2 loss: 0.873
Actual params: [0.4566, 0.6805]
-Original Grad: 0.006, -lr * Pred Grad: -0.082, New P: 0.374
-Original Grad: -0.002, -lr * Pred Grad: -0.039, New P: 0.642
iter 3 loss: 0.840
Actual params: [0.3741, 0.6419]
-Original Grad: -0.052, -lr * Pred Grad: -0.079, New P: 0.295
-Original Grad: -0.072, -lr * Pred Grad: -0.056, New P: 0.586
iter 4 loss: 0.811
Actual params: [0.2949, 0.5861]
-Original Grad: -0.006, -lr * Pred Grad: -0.072, New P: 0.223
-Original Grad: -0.097, -lr * Pred Grad: -0.061, New P: 0.525
iter 5 loss: 0.788
Actual params: [0.2233, 0.5254]
-Original Grad: 0.033, -lr * Pred Grad: -0.068, New P: 0.155
-Original Grad: -0.068, -lr * Pred Grad: -0.062, New P: 0.463
iter 6 loss: 0.766
Actual params: [0.155 , 0.4633]
-Original Grad: 0.017, -lr * Pred Grad: -0.057, New P: 0.098
-Original Grad: -0.044, -lr * Pred Grad: -0.061, New P: 0.403
iter 7 loss: 0.741
Actual params: [0.0976, 0.4027]
-Original Grad: -0.032, -lr * Pred Grad: -0.044, New P: 0.053
-Original Grad: -0.138, -lr * Pred Grad: -0.064, New P: 0.339
iter 8 loss: 0.706
Actual params: [0.0533, 0.3392]
-Original Grad: 0.000, -lr * Pred Grad: -0.037, New P: 0.016
-Original Grad: -0.023, -lr * Pred Grad: -0.062, New P: 0.277
iter 9 loss: 0.675
Actual params: [0.0159, 0.277 ]
-Original Grad: -0.046, -lr * Pred Grad: -0.040, New P: -0.024
-Original Grad: -0.069, -lr * Pred Grad: -0.060, New P: 0.217
iter 10 loss: 0.647
Actual params: [-0.0238,  0.2165]
-Original Grad: -0.022, -lr * Pred Grad: -0.036, New P: -0.060
-Original Grad: -0.188, -lr * Pred Grad: -0.064, New P: 0.152
iter 11 loss: 0.627
Actual params: [-0.0595,  0.1524]
-Original Grad: -0.020, -lr * Pred Grad: -0.033, New P: -0.093
-Original Grad: -0.160, -lr * Pred Grad: -0.066, New P: 0.086
iter 12 loss: 0.613
Actual params: [-0.0928,  0.0864]
-Original Grad: -0.010, -lr * Pred Grad: -0.030, New P: -0.122
-Original Grad: -0.072, -lr * Pred Grad: -0.066, New P: 0.020
iter 13 loss: 0.596
Actual params: [-0.1225,  0.0203]
-Original Grad: -0.033, -lr * Pred Grad: -0.035, New P: -0.157
-Original Grad: -0.073, -lr * Pred Grad: -0.065, New P: -0.045
iter 14 loss: 0.589
Actual params: [-0.157 , -0.0452]
-Original Grad: -0.011, -lr * Pred Grad: -0.034, New P: -0.191
-Original Grad: 0.012, -lr * Pred Grad: -0.060, New P: -0.105
iter 15 loss: 0.579
Actual params: [-0.1912, -0.1048]
-Original Grad: -0.062, -lr * Pred Grad: -0.043, New P: -0.234
-Original Grad: -0.041, -lr * Pred Grad: -0.052, New P: -0.157
iter 16 loss: 0.571
Actual params: [-0.2337, -0.1566]
-Original Grad: -0.012, -lr * Pred Grad: -0.043, New P: -0.276
-Original Grad: -0.002, -lr * Pred Grad: -0.042, New P: -0.199
iter 17 loss: 0.561
Actual params: [-0.2763, -0.1989]
-Original Grad: -0.025, -lr * Pred Grad: -0.039, New P: -0.316
-Original Grad: -0.031, -lr * Pred Grad: -0.041, New P: -0.240
iter 18 loss: 0.560
Actual params: [-0.3157, -0.2398]
-Original Grad: -0.017, -lr * Pred Grad: -0.036, New P: -0.352
-Original Grad: 0.035, -lr * Pred Grad: -0.025, New P: -0.265
iter 19 loss: 0.554
Actual params: [-0.3517, -0.2652]
-Original Grad: -0.020, -lr * Pred Grad: -0.034, New P: -0.386
-Original Grad: -0.011, -lr * Pred Grad: -0.022, New P: -0.287
iter 20 loss: 0.549
Actual params: [-0.386, -0.287]
-Original Grad: -0.008, -lr * Pred Grad: -0.031, New P: -0.417
-Original Grad: 0.010, -lr * Pred Grad: -0.019, New P: -0.306
Target params: [1.1812, 0.2779]
iter 0 loss: 0.536
Actual params: [0.5941, 0.5941]
-Original Grad: -0.244, -lr * Pred Grad: -0.063, New P: 0.531
-Original Grad: -0.216, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.581
Actual params: [0.5312, 0.5316]
-Original Grad: -0.144, -lr * Pred Grad: -0.081, New P: 0.451
-Original Grad: -0.054, -lr * Pred Grad: -0.080, New P: 0.452
iter 2 loss: 0.620
Actual params: [0.4506, 0.4516]
-Original Grad: -0.082, -lr * Pred Grad: -0.083, New P: 0.367
-Original Grad: -0.079, -lr * Pred Grad: -0.081, New P: 0.370
iter 3 loss: 0.654
Actual params: [0.3673, 0.3702]
-Original Grad: -0.014, -lr * Pred Grad: -0.081, New P: 0.287
-Original Grad: -0.075, -lr * Pred Grad: -0.077, New P: 0.293
iter 4 loss: 0.675
Actual params: [0.2868, 0.2933]
-Original Grad: -0.032, -lr * Pred Grad: -0.073, New P: 0.214
-Original Grad: -0.017, -lr * Pred Grad: -0.071, New P: 0.222
iter 5 loss: 0.692
Actual params: [0.2139, 0.2225]
-Original Grad: 0.044, -lr * Pred Grad: -0.069, New P: 0.145
-Original Grad: -0.046, -lr * Pred Grad: -0.068, New P: 0.154
iter 6 loss: 0.705
Actual params: [0.1449, 0.1542]
-Original Grad: 0.078, -lr * Pred Grad: -0.058, New P: 0.087
-Original Grad: 0.052, -lr * Pred Grad: -0.060, New P: 0.095
iter 7 loss: 0.719
Actual params: [0.0872, 0.0945]
-Original Grad: 0.083, -lr * Pred Grad: -0.038, New P: 0.049
-Original Grad: 0.024, -lr * Pred Grad: -0.041, New P: 0.054
iter 8 loss: 0.728
Actual params: [0.0494, 0.0539]
-Original Grad: -0.117, -lr * Pred Grad: -0.036, New P: 0.013
-Original Grad: -0.030, -lr * Pred Grad: -0.034, New P: 0.020
iter 9 loss: 0.739
Actual params: [0.0134, 0.0197]
-Original Grad: -0.110, -lr * Pred Grad: -0.061, New P: -0.047
-Original Grad: -0.009, -lr * Pred Grad: -0.036, New P: -0.017
iter 10 loss: 0.753
Actual params: [-0.0472, -0.0166]
-Original Grad: 0.106, -lr * Pred Grad: -0.039, New P: -0.086
-Original Grad: 0.079, -lr * Pred Grad: -0.010, New P: -0.027
iter 11 loss: 0.760
Actual params: [-0.0864, -0.0268]
-Original Grad: 0.043, -lr * Pred Grad: -0.007, New P: -0.094
-Original Grad: 0.050, -lr * Pred Grad: 0.024, New P: -0.003
iter 12 loss: 0.757
Actual params: [-0.0936, -0.0027]
-Original Grad: 0.010, -lr * Pred Grad: 0.022, New P: -0.072
-Original Grad: 0.027, -lr * Pred Grad: 0.039, New P: 0.037
iter 13 loss: 0.750
Actual params: [-0.0717,  0.0367]
-Original Grad: 0.052, -lr * Pred Grad: 0.027, New P: -0.045
-Original Grad: 0.023, -lr * Pred Grad: -0.002, New P: 0.035
iter 14 loss: 0.747
Actual params: [-0.0446,  0.035 ]
-Original Grad: -0.020, -lr * Pred Grad: -0.017, New P: -0.062
-Original Grad: -0.041, -lr * Pred Grad: -0.028, New P: 0.007
iter 15 loss: 0.752
Actual params: [-0.0616,  0.0066]
-Original Grad: 0.100, -lr * Pred Grad: 0.036, New P: -0.025
-Original Grad: 0.058, -lr * Pred Grad: 0.004, New P: 0.010
iter 16 loss: 0.747
Actual params: [-0.0255,  0.0103]
-Original Grad: 0.028, -lr * Pred Grad: 0.040, New P: 0.015
-Original Grad: 0.003, -lr * Pred Grad: 0.005, New P: 0.015
iter 17 loss: 0.739
Actual params: [0.0146, 0.0153]
-Original Grad: 0.118, -lr * Pred Grad: 0.067, New P: 0.082
-Original Grad: 0.111, -lr * Pred Grad: 0.050, New P: 0.065
iter 18 loss: 0.723
Actual params: [0.082 , 0.0648]
-Original Grad: 0.145, -lr * Pred Grad: 0.081, New P: 0.163
-Original Grad: 0.067, -lr * Pred Grad: 0.065, New P: 0.129
iter 19 loss: 0.705
Actual params: [0.1626, 0.1294]
-Original Grad: 0.017, -lr * Pred Grad: 0.065, New P: 0.228
-Original Grad: 0.065, -lr * Pred Grad: 0.057, New P: 0.186
iter 20 loss: 0.693
Actual params: [0.228 , 0.1859]
-Original Grad: -0.003, -lr * Pred Grad: -0.005, New P: 0.223
-Original Grad: 0.053, -lr * Pred Grad: 0.051, New P: 0.237
Target params: [1.1812, 0.2779]
iter 0 loss: 0.904
Actual params: [0.5941, 0.5941]
-Original Grad: -0.006, -lr * Pred Grad: 0.030, New P: 0.624
-Original Grad: 0.043, -lr * Pred Grad: 0.057, New P: 0.651
iter 1 loss: 0.992
Actual params: [0.6237, 0.6509]
-Original Grad: -0.041, -lr * Pred Grad: -0.046, New P: 0.577
-Original Grad: -0.139, -lr * Pred Grad: -0.044, New P: 0.607
iter 2 loss: 0.912
Actual params: [0.5774, 0.6073]
-Original Grad: 0.031, -lr * Pred Grad: -0.062, New P: 0.516
-Original Grad: -0.247, -lr * Pred Grad: -0.074, New P: 0.533
iter 3 loss: 0.801
Actual params: [0.5157, 0.5329]
-Original Grad: -0.111, -lr * Pred Grad: -0.065, New P: 0.451
-Original Grad: -0.183, -lr * Pred Grad: -0.081, New P: 0.452
iter 4 loss: 0.696
Actual params: [0.4508, 0.4521]
-Original Grad: -0.099, -lr * Pred Grad: -0.065, New P: 0.386
-Original Grad: 0.043, -lr * Pred Grad: -0.074, New P: 0.378
iter 5 loss: 0.601
Actual params: [0.3855, 0.378 ]
-Original Grad: 0.017, -lr * Pred Grad: -0.060, New P: 0.326
-Original Grad: -0.003, -lr * Pred Grad: -0.069, New P: 0.309
iter 6 loss: 0.541
Actual params: [0.3259, 0.3089]
-Original Grad: 0.011, -lr * Pred Grad: -0.041, New P: 0.285
-Original Grad: -0.045, -lr * Pred Grad: -0.065, New P: 0.244
iter 7 loss: 0.492
Actual params: [0.285 , 0.2437]
-Original Grad: -0.059, -lr * Pred Grad: -0.036, New P: 0.249
-Original Grad: -0.140, -lr * Pred Grad: -0.065, New P: 0.179
iter 8 loss: 0.461
Actual params: [0.2492, 0.1787]
-Original Grad: -0.063, -lr * Pred Grad: -0.048, New P: 0.201
-Original Grad: -0.008, -lr * Pred Grad: -0.062, New P: 0.116
iter 9 loss: 0.426
Actual params: [0.2015, 0.1164]
-Original Grad: 0.071, -lr * Pred Grad: -0.018, New P: 0.183
-Original Grad: -0.033, -lr * Pred Grad: -0.056, New P: 0.060
iter 10 loss: 0.413
Actual params: [0.1833, 0.0604]
-Original Grad: 0.021, -lr * Pred Grad: 0.014, New P: 0.198
-Original Grad: 0.014, -lr * Pred Grad: -0.042, New P: 0.019
iter 11 loss: 0.416
Actual params: [0.1977, 0.0186]
-Original Grad: 0.054, -lr * Pred Grad: 0.035, New P: 0.233
-Original Grad: 0.004, -lr * Pred Grad: -0.030, New P: -0.011
iter 12 loss: 0.430
Actual params: [ 0.2325, -0.0113]
-Original Grad: 0.048, -lr * Pred Grad: 0.037, New P: 0.269
-Original Grad: 0.107, -lr * Pred Grad: -0.011, New P: -0.022
iter 13 loss: 0.441
Actual params: [ 0.2695, -0.0222]
-Original Grad: 0.019, -lr * Pred Grad: 0.018, New P: 0.288
-Original Grad: 0.069, -lr * Pred Grad: 0.010, New P: -0.012
iter 14 loss: 0.449
Actual params: [ 0.2877, -0.0123]
-Original Grad: -0.003, -lr * Pred Grad: -0.007, New P: 0.281
-Original Grad: 0.020, -lr * Pred Grad: 0.031, New P: 0.019
iter 15 loss: 0.450
Actual params: [0.2812, 0.0189]
-Original Grad: -0.104, -lr * Pred Grad: -0.045, New P: 0.237
-Original Grad: 0.019, -lr * Pred Grad: 0.009, New P: 0.027
iter 16 loss: 0.432
Actual params: [0.2365, 0.0275]
-Original Grad: -0.024, -lr * Pred Grad: -0.054, New P: 0.183
-Original Grad: -0.031, -lr * Pred Grad: -0.027, New P: 0.000
iter 17 loss: 0.405
Actual params: [0.1829, 0.0003]
-Original Grad: 0.032, -lr * Pred Grad: -0.028, New P: 0.155
-Original Grad: -0.109, -lr * Pred Grad: -0.052, New P: -0.052
iter 18 loss: 0.386
Actual params: [ 0.1551, -0.0518]
-Original Grad: 0.066, -lr * Pred Grad: 0.018, New P: 0.173
-Original Grad: -0.098, -lr * Pred Grad: -0.062, New P: -0.113
iter 19 loss: 0.388
Actual params: [ 0.1728, -0.1134]
-Original Grad: -0.124, -lr * Pred Grad: -0.034, New P: 0.138
-Original Grad: -0.038, -lr * Pred Grad: -0.062, New P: -0.175
iter 20 loss: 0.368
Actual params: [ 0.1384, -0.175 ]
-Original Grad: -0.018, -lr * Pred Grad: -0.050, New P: 0.089
-Original Grad: 0.023, -lr * Pred Grad: -0.046, New P: -0.221
Target params: [1.1812, 0.2779]
iter 0 loss: 0.835
Actual params: [0.5941, 0.5941]
-Original Grad: -0.295, -lr * Pred Grad: -0.063, New P: 0.531
-Original Grad: -0.451, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.921
Actual params: [0.5309, 0.5311]
-Original Grad: 0.001, -lr * Pred Grad: -0.080, New P: 0.451
-Original Grad: -0.047, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.941
Actual params: [0.4507, 0.4504]
-Original Grad: -0.244, -lr * Pred Grad: -0.083, New P: 0.367
-Original Grad: -0.131, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.949
Actual params: [0.3674, 0.3669]
-Original Grad: -0.169, -lr * Pred Grad: -0.084, New P: 0.284
-Original Grad: -0.050, -lr * Pred Grad: -0.083, New P: 0.283
iter 4 loss: 0.946
Actual params: [0.2836, 0.2835]
-Original Grad: 0.152, -lr * Pred Grad: -0.078, New P: 0.206
-Original Grad: 0.120, -lr * Pred Grad: -0.076, New P: 0.208
iter 5 loss: 0.941
Actual params: [0.2061, 0.2077]
-Original Grad: -0.093, -lr * Pred Grad: -0.071, New P: 0.135
-Original Grad: -0.051, -lr * Pred Grad: -0.070, New P: 0.138
iter 6 loss: 0.948
Actual params: [0.1351, 0.1377]
-Original Grad: 0.051, -lr * Pred Grad: -0.068, New P: 0.067
-Original Grad: 0.015, -lr * Pred Grad: -0.066, New P: 0.072
iter 7 loss: 0.943
Actual params: [0.0675, 0.0718]
-Original Grad: 0.001, -lr * Pred Grad: -0.056, New P: 0.011
-Original Grad: -0.067, -lr * Pred Grad: -0.060, New P: 0.012
iter 8 loss: 0.940
Actual params: [0.0113, 0.0119]
-Original Grad: 0.067, -lr * Pred Grad: -0.037, New P: -0.026
-Original Grad: 0.050, -lr * Pred Grad: -0.043, New P: -0.031
iter 9 loss: 0.939
Actual params: [-0.0258, -0.0311]
-Original Grad: -0.008, -lr * Pred Grad: -0.028, New P: -0.054
-Original Grad: -0.001, -lr * Pred Grad: -0.032, New P: -0.063
iter 10 loss: 0.941
Actual params: [-0.0536, -0.063 ]
-Original Grad: 0.027, -lr * Pred Grad: -0.023, New P: -0.077
-Original Grad: 0.013, -lr * Pred Grad: -0.023, New P: -0.086
iter 11 loss: 0.944
Actual params: [-0.0766, -0.0863]
-Original Grad: -0.006, -lr * Pred Grad: -0.016, New P: -0.092
-Original Grad: 0.018, -lr * Pred Grad: -0.009, New P: -0.095
iter 12 loss: 0.945
Actual params: [-0.0923, -0.095 ]
-Original Grad: 0.017, -lr * Pred Grad: -0.001, New P: -0.093
-Original Grad: 0.009, -lr * Pred Grad: -0.000, New P: -0.095
iter 13 loss: 0.945
Actual params: [-0.0933, -0.095 ]
-Original Grad: 0.014, -lr * Pred Grad: -0.008, New P: -0.101
-Original Grad: 0.027, -lr * Pred Grad: -0.001, New P: -0.096
iter 14 loss: 0.951
Actual params: [-0.1014, -0.0963]
-Original Grad: -0.022, -lr * Pred Grad: -0.026, New P: -0.128
-Original Grad: 0.019, -lr * Pred Grad: 0.000, New P: -0.096
iter 15 loss: 0.951
Actual params: [-0.1278, -0.0959]
-Original Grad: 0.016, -lr * Pred Grad: -0.022, New P: -0.149
-Original Grad: -0.000, -lr * Pred Grad: -0.013, New P: -0.109
iter 16 loss: 0.951
Actual params: [-0.1493, -0.1088]
-Original Grad: 0.019, -lr * Pred Grad: -0.010, New P: -0.159
-Original Grad: 0.021, -lr * Pred Grad: -0.012, New P: -0.121
iter 17 loss: 0.951
Actual params: [-0.1592, -0.1205]
-Original Grad: 0.071, -lr * Pred Grad: 0.022, New P: -0.138
-Original Grad: 0.027, -lr * Pred Grad: -0.003, New P: -0.123
iter 18 loss: 0.951
Actual params: [-0.1376, -0.1231]
-Original Grad: 0.066, -lr * Pred Grad: 0.045, New P: -0.092
-Original Grad: 0.018, -lr * Pred Grad: -0.001, New P: -0.124
iter 19 loss: 0.944
Actual params: [-0.0923, -0.1242]
-Original Grad: 0.042, -lr * Pred Grad: 0.038, New P: -0.054
-Original Grad: -0.002, -lr * Pred Grad: -0.015, New P: -0.139
iter 20 loss: 0.943
Actual params: [-0.054 , -0.1387]
-Original Grad: -0.063, -lr * Pred Grad: -0.027, New P: -0.081
-Original Grad: 0.003, -lr * Pred Grad: -0.023, New P: -0.162
Target params: [1.1812, 0.2779]
iter 0 loss: 0.500
Actual params: [0.5941, 0.5941]
-Original Grad: -0.032, -lr * Pred Grad: 0.004, New P: 0.598
-Original Grad: 0.079, -lr * Pred Grad: 0.063, New P: 0.657
iter 1 loss: 0.496
Actual params: [0.5982, 0.6574]
-Original Grad: 0.113, -lr * Pred Grad: 0.045, New P: 0.643
-Original Grad: 0.062, -lr * Pred Grad: 0.079, New P: 0.736
iter 2 loss: 0.480
Actual params: [0.643, 0.736]
-Original Grad: 0.270, -lr * Pred Grad: 0.079, New P: 0.722
-Original Grad: 0.088, -lr * Pred Grad: 0.067, New P: 0.803
iter 3 loss: 0.475
Actual params: [0.7222, 0.8028]
-Original Grad: 0.045, -lr * Pred Grad: 0.082, New P: 0.804
-Original Grad: -0.011, -lr * Pred Grad: -0.008, New P: 0.795
iter 4 loss: 0.512
Actual params: [0.8044, 0.795 ]
-Original Grad: -0.238, -lr * Pred Grad: -0.034, New P: 0.770
-Original Grad: -0.132, -lr * Pred Grad: -0.049, New P: 0.746
iter 5 loss: 0.477
Actual params: [0.77  , 0.7461]
-Original Grad: 0.026, -lr * Pred Grad: -0.052, New P: 0.718
-Original Grad: 0.076, -lr * Pred Grad: -0.017, New P: 0.729
iter 6 loss: 0.471
Actual params: [0.7179, 0.7287]
-Original Grad: 0.019, -lr * Pred Grad: -0.017, New P: 0.701
-Original Grad: -0.107, -lr * Pred Grad: -0.038, New P: 0.691
iter 7 loss: 0.477
Actual params: [0.7009, 0.6911]
-Original Grad: 0.167, -lr * Pred Grad: 0.043, New P: 0.744
-Original Grad: 0.017, -lr * Pred Grad: -0.026, New P: 0.665
iter 8 loss: 0.469
Actual params: [0.7436, 0.6649]
-Original Grad: -0.049, -lr * Pred Grad: 0.046, New P: 0.790
-Original Grad: 0.032, -lr * Pred Grad: 0.009, New P: 0.674
iter 9 loss: 0.465
Actual params: [0.79  , 0.6736]
-Original Grad: -0.132, -lr * Pred Grad: -0.040, New P: 0.750
-Original Grad: -0.050, -lr * Pred Grad: -0.025, New P: 0.649
iter 10 loss: 0.468
Actual params: [0.7499, 0.6485]
-Original Grad: -0.040, -lr * Pred Grad: -0.054, New P: 0.696
-Original Grad: 0.001, -lr * Pred Grad: -0.020, New P: 0.628
iter 11 loss: 0.483
Actual params: [0.6956, 0.6282]
-Original Grad: -0.133, -lr * Pred Grad: -0.058, New P: 0.637
-Original Grad: -0.171, -lr * Pred Grad: -0.047, New P: 0.581
iter 12 loss: 0.493
Actual params: [0.6373, 0.5808]
-Original Grad: 0.234, -lr * Pred Grad: -0.025, New P: 0.613
-Original Grad: 0.110, -lr * Pred Grad: -0.021, New P: 0.559
iter 13 loss: 0.499
Actual params: [0.6126, 0.5593]
-Original Grad: -0.122, -lr * Pred Grad: 0.007, New P: 0.620
-Original Grad: -0.033, -lr * Pred Grad: 0.001, New P: 0.560
iter 14 loss: 0.498
Actual params: [0.6196, 0.5602]
-Original Grad: 0.117, -lr * Pred Grad: 0.046, New P: 0.666
-Original Grad: 0.034, -lr * Pred Grad: 0.009, New P: 0.570
iter 15 loss: 0.491
Actual params: [0.6655, 0.5697]
-Original Grad: -0.115, -lr * Pred Grad: -0.036, New P: 0.630
-Original Grad: -0.008, -lr * Pred Grad: -0.006, New P: 0.564
iter 16 loss: 0.495
Actual params: [0.6298, 0.5638]
-Original Grad: 0.023, -lr * Pred Grad: -0.026, New P: 0.604
-Original Grad: 0.010, -lr * Pred Grad: -0.008, New P: 0.556
iter 17 loss: 0.502
Actual params: [0.6039, 0.5557]
-Original Grad: -0.037, -lr * Pred Grad: -0.023, New P: 0.581
-Original Grad: 0.116, -lr * Pred Grad: 0.036, New P: 0.592
iter 18 loss: 0.503
Actual params: [0.5806, 0.5922]
-Original Grad: 0.089, -lr * Pred Grad: 0.029, New P: 0.609
-Original Grad: 0.014, -lr * Pred Grad: 0.044, New P: 0.637
iter 19 loss: 0.496
Actual params: [0.6092, 0.6367]
-Original Grad: 0.259, -lr * Pred Grad: 0.069, New P: 0.678
-Original Grad: 0.027, -lr * Pred Grad: 0.012, New P: 0.648
iter 20 loss: 0.486
Actual params: [0.6781, 0.6484]
-Original Grad: 0.120, -lr * Pred Grad: 0.083, New P: 0.761
-Original Grad: 0.086, -lr * Pred Grad: 0.045, New P: 0.694
Target params: [1.1812, 0.2779]
iter 0 loss: 0.371
Actual params: [0.5941, 0.5941]
-Original Grad: 0.402, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.302, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.336
Actual params: [0.6617, 0.5309]
-Original Grad: 0.173, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.164, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.359
Actual params: [0.7467, 0.4503]
-Original Grad: -0.003, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.214, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.355
Actual params: [0.8344, 0.3667]
-Original Grad: -0.252, -lr * Pred Grad: 0.085, New P: 0.919
-Original Grad: -0.103, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.337
Actual params: [0.9194, 0.2826]
-Original Grad: 0.042, -lr * Pred Grad: 0.018, New P: 0.937
-Original Grad: -0.107, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.328
Actual params: [0.9372, 0.1987]
-Original Grad: 0.048, -lr * Pred Grad: 0.062, New P: 0.999
-Original Grad: -0.045, -lr * Pred Grad: -0.082, New P: 0.116
iter 6 loss: 0.322
Actual params: [0.9989, 0.1164]
-Original Grad: -0.012, -lr * Pred Grad: 0.037, New P: 1.036
-Original Grad: -0.080, -lr * Pred Grad: -0.078, New P: 0.039
iter 7 loss: 0.322
Actual params: [1.0363, 0.0388]
-Original Grad: -0.063, -lr * Pred Grad: -0.007, New P: 1.030
-Original Grad: -0.043, -lr * Pred Grad: -0.072, New P: -0.033
iter 8 loss: 0.323
Actual params: [ 1.0297, -0.0329]
-Original Grad: 0.001, -lr * Pred Grad: -0.010, New P: 1.020
-Original Grad: -0.072, -lr * Pred Grad: -0.069, New P: -0.102
iter 9 loss: 0.323
Actual params: [ 1.0201, -0.1021]
-Original Grad: -0.119, -lr * Pred Grad: -0.047, New P: 0.973
-Original Grad: -0.098, -lr * Pred Grad: -0.068, New P: -0.170
iter 10 loss: 0.323
Actual params: [ 0.9727, -0.1698]
-Original Grad: -0.121, -lr * Pred Grad: -0.057, New P: 0.916
-Original Grad: -0.028, -lr * Pred Grad: -0.066, New P: -0.235
iter 11 loss: 0.323
Actual params: [ 0.9161, -0.2353]
-Original Grad: 0.010, -lr * Pred Grad: -0.051, New P: 0.865
-Original Grad: -0.064, -lr * Pred Grad: -0.063, New P: -0.299
iter 12 loss: 0.324
Actual params: [ 0.8649, -0.2986]
-Original Grad: -0.188, -lr * Pred Grad: -0.057, New P: 0.808
-Original Grad: -0.087, -lr * Pred Grad: -0.063, New P: -0.362
iter 13 loss: 0.326
Actual params: [ 0.8076, -0.3616]
-Original Grad: 0.139, -lr * Pred Grad: -0.034, New P: 0.774
-Original Grad: -0.050, -lr * Pred Grad: -0.062, New P: -0.423
iter 14 loss: 0.327
Actual params: [ 0.774 , -0.4233]
-Original Grad: 0.038, -lr * Pred Grad: -0.002, New P: 0.772
-Original Grad: -0.138, -lr * Pred Grad: -0.064, New P: -0.487
iter 15 loss: 0.329
Actual params: [ 0.7718, -0.4872]
-Original Grad: -0.050, -lr * Pred Grad: 0.010, New P: 0.782
-Original Grad: -0.009, -lr * Pred Grad: -0.061, New P: -0.549
iter 16 loss: 0.335
Actual params: [ 0.7818, -0.5486]
-Original Grad: 0.013, -lr * Pred Grad: -0.017, New P: 0.765
-Original Grad: -0.156, -lr * Pred Grad: -0.064, New P: -0.612
iter 17 loss: 0.339
Actual params: [ 0.7649, -0.6125]
-Original Grad: 0.068, -lr * Pred Grad: 0.025, New P: 0.790
-Original Grad: -0.137, -lr * Pred Grad: -0.065, New P: -0.678
iter 18 loss: 0.350
Actual params: [ 0.7897, -0.6775]
-Original Grad: -0.087, -lr * Pred Grad: -0.028, New P: 0.762
-Original Grad: -0.257, -lr * Pred Grad: -0.069, New P: -0.747
iter 19 loss: 0.357
Actual params: [ 0.7618, -0.7468]
-Original Grad: 0.101, -lr * Pred Grad: 0.016, New P: 0.778
-Original Grad: -0.066, -lr * Pred Grad: -0.068, New P: -0.815
iter 20 loss: 0.374
Actual params: [ 0.7777, -0.8153]
-Original Grad: -0.066, -lr * Pred Grad: -0.021, New P: 0.756
-Original Grad: 0.205, -lr * Pred Grad: -0.051, New P: -0.866
Target params: [1.1812, 0.2779]
iter 0 loss: 0.510
Actual params: [0.5941, 0.5941]
-Original Grad: -0.216, -lr * Pred Grad: -0.062, New P: 0.532
-Original Grad: 0.155, -lr * Pred Grad: 0.067, New P: 0.661
iter 1 loss: 0.554
Actual params: [0.5316, 0.661 ]
-Original Grad: 0.112, -lr * Pred Grad: -0.071, New P: 0.461
-Original Grad: 0.103, -lr * Pred Grad: 0.084, New P: 0.745
iter 2 loss: 0.604
Actual params: [0.4608, 0.7448]
-Original Grad: 0.158, -lr * Pred Grad: -0.047, New P: 0.414
-Original Grad: -0.110, -lr * Pred Grad: 0.037, New P: 0.782
iter 3 loss: 0.625
Actual params: [0.4135, 0.7823]
-Original Grad: 0.117, -lr * Pred Grad: 0.016, New P: 0.430
-Original Grad: -0.039, -lr * Pred Grad: -0.041, New P: 0.741
iter 4 loss: 0.609
Actual params: [0.4296, 0.7413]
-Original Grad: 0.138, -lr * Pred Grad: 0.068, New P: 0.497
-Original Grad: -0.047, -lr * Pred Grad: -0.046, New P: 0.696
iter 5 loss: 0.576
Actual params: [0.4971, 0.6955]
-Original Grad: 0.088, -lr * Pred Grad: 0.081, New P: 0.578
-Original Grad: -0.007, -lr * Pred Grad: -0.019, New P: 0.676
iter 6 loss: 0.543
Actual params: [0.5783, 0.6761]
-Original Grad: 0.110, -lr * Pred Grad: 0.081, New P: 0.660
-Original Grad: -0.036, -lr * Pred Grad: -0.023, New P: 0.653
iter 7 loss: 0.496
Actual params: [0.6595, 0.6526]
-Original Grad: 0.141, -lr * Pred Grad: 0.083, New P: 0.743
-Original Grad: -0.115, -lr * Pred Grad: -0.050, New P: 0.603
iter 8 loss: 0.439
Actual params: [0.743 , 0.6029]
-Original Grad: 0.010, -lr * Pred Grad: 0.065, New P: 0.808
-Original Grad: -0.001, -lr * Pred Grad: -0.046, New P: 0.557
iter 9 loss: 0.391
Actual params: [0.8082, 0.557 ]
-Original Grad: 0.112, -lr * Pred Grad: 0.074, New P: 0.883
-Original Grad: -0.020, -lr * Pred Grad: -0.028, New P: 0.529
iter 10 loss: 0.336
Actual params: [0.8827, 0.5292]
-Original Grad: 0.046, -lr * Pred Grad: 0.056, New P: 0.939
-Original Grad: -0.075, -lr * Pred Grad: -0.038, New P: 0.491
iter 11 loss: 0.306
Actual params: [0.9389, 0.4911]
-Original Grad: 0.084, -lr * Pred Grad: 0.071, New P: 1.010
-Original Grad: -0.119, -lr * Pred Grad: -0.052, New P: 0.439
iter 12 loss: 0.285
Actual params: [1.0098, 0.4388]
-Original Grad: 0.022, -lr * Pred Grad: 0.018, New P: 1.028
-Original Grad: -0.189, -lr * Pred Grad: -0.062, New P: 0.377
iter 13 loss: 0.284
Actual params: [1.0276, 0.3767]
-Original Grad: -0.039, -lr * Pred Grad: -0.013, New P: 1.015
-Original Grad: -0.154, -lr * Pred Grad: -0.067, New P: 0.310
iter 14 loss: 0.288
Actual params: [1.015, 0.31 ]
-Original Grad: 0.048, -lr * Pred Grad: 0.004, New P: 1.019
-Original Grad: -0.025, -lr * Pred Grad: -0.066, New P: 0.244
iter 15 loss: 0.289
Actual params: [1.0189, 0.2441]
-Original Grad: 0.024, -lr * Pred Grad: 0.011, New P: 1.030
-Original Grad: -0.053, -lr * Pred Grad: -0.064, New P: 0.180
iter 16 loss: 0.289
Actual params: [1.03  , 0.1801]
-Original Grad: 0.004, -lr * Pred Grad: -0.008, New P: 1.022
-Original Grad: -0.039, -lr * Pred Grad: -0.060, New P: 0.120
iter 17 loss: 0.293
Actual params: [1.0216, 0.1201]
-Original Grad: -0.028, -lr * Pred Grad: -0.028, New P: 0.994
-Original Grad: 0.014, -lr * Pred Grad: -0.046, New P: 0.074
iter 18 loss: 0.299
Actual params: [0.9937, 0.0743]
-Original Grad: 0.003, -lr * Pred Grad: -0.022, New P: 0.971
-Original Grad: -0.007, -lr * Pred Grad: -0.032, New P: 0.042
iter 19 loss: 0.304
Actual params: [0.9713, 0.0424]
-Original Grad: -0.038, -lr * Pred Grad: -0.029, New P: 0.942
-Original Grad: -0.104, -lr * Pred Grad: -0.049, New P: -0.007
iter 20 loss: 0.310
Actual params: [ 0.942, -0.007]
-Original Grad: -0.084, -lr * Pred Grad: -0.043, New P: 0.899
-Original Grad: -0.059, -lr * Pred Grad: -0.055, New P: -0.062
Target params: [1.1812, 0.2779]
iter 0 loss: 0.935
Actual params: [0.5941, 0.5941]
-Original Grad: 0.085, -lr * Pred Grad: 0.064, New P: 0.658
-Original Grad: -0.270, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.806
Actual params: [0.658, 0.531]
-Original Grad: 0.217, -lr * Pred Grad: 0.083, New P: 0.741
-Original Grad: -0.272, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.567
Actual params: [0.7414, 0.4505]
-Original Grad: 0.041, -lr * Pred Grad: 0.085, New P: 0.827
-Original Grad: 0.052, -lr * Pred Grad: -0.083, New P: 0.368
iter 3 loss: 0.472
Actual params: [0.8269, 0.3678]
-Original Grad: -0.088, -lr * Pred Grad: -0.008, New P: 0.819
-Original Grad: -0.172, -lr * Pred Grad: -0.083, New P: 0.284
iter 4 loss: 0.418
Actual params: [0.8189, 0.2845]
-Original Grad: -0.011, -lr * Pred Grad: -0.020, New P: 0.799
-Original Grad: -0.023, -lr * Pred Grad: -0.080, New P: 0.205
iter 5 loss: 0.392
Actual params: [0.7988, 0.2046]
-Original Grad: -0.035, -lr * Pred Grad: -0.021, New P: 0.777
-Original Grad: 0.056, -lr * Pred Grad: -0.072, New P: 0.133
iter 6 loss: 0.374
Actual params: [0.7774, 0.133 ]
-Original Grad: -0.063, -lr * Pred Grad: -0.040, New P: 0.737
-Original Grad: -0.041, -lr * Pred Grad: -0.069, New P: 0.064
iter 7 loss: 0.364
Actual params: [0.7371, 0.0644]
-Original Grad: 0.071, -lr * Pred Grad: -0.000, New P: 0.737
-Original Grad: -0.080, -lr * Pred Grad: -0.066, New P: -0.002
iter 8 loss: 0.356
Actual params: [ 0.7368, -0.0019]
-Original Grad: 0.018, -lr * Pred Grad: 0.023, New P: 0.760
-Original Grad: -0.031, -lr * Pred Grad: -0.063, New P: -0.064
iter 9 loss: 0.346
Actual params: [ 0.7597, -0.0644]
-Original Grad: -0.019, -lr * Pred Grad: -0.018, New P: 0.742
-Original Grad: -0.087, -lr * Pred Grad: -0.062, New P: -0.126
iter 10 loss: 0.342
Actual params: [ 0.7421, -0.1261]
-Original Grad: -0.087, -lr * Pred Grad: -0.044, New P: 0.698
-Original Grad: 0.013, -lr * Pred Grad: -0.053, New P: -0.179
iter 11 loss: 0.341
Actual params: [ 0.6984, -0.1791]
-Original Grad: -0.021, -lr * Pred Grad: -0.044, New P: 0.654
-Original Grad: -0.092, -lr * Pred Grad: -0.056, New P: -0.235
iter 12 loss: 0.342
Actual params: [ 0.6542, -0.235 ]
-Original Grad: 0.017, -lr * Pred Grad: -0.019, New P: 0.635
-Original Grad: -0.109, -lr * Pred Grad: -0.062, New P: -0.297
iter 13 loss: 0.341
Actual params: [ 0.6349, -0.2968]
-Original Grad: 0.059, -lr * Pred Grad: 0.018, New P: 0.653
-Original Grad: -0.056, -lr * Pred Grad: -0.062, New P: -0.359
iter 14 loss: 0.343
Actual params: [ 0.6532, -0.3591]
-Original Grad: 0.007, -lr * Pred Grad: 0.023, New P: 0.676
-Original Grad: -0.064, -lr * Pred Grad: -0.061, New P: -0.421
iter 15 loss: 0.345
Actual params: [ 0.6761, -0.4206]
-Original Grad: -0.029, -lr * Pred Grad: -0.020, New P: 0.657
-Original Grad: 0.045, -lr * Pred Grad: -0.046, New P: -0.467
iter 16 loss: 0.346
Actual params: [ 0.6565, -0.4666]
-Original Grad: 0.022, -lr * Pred Grad: -0.012, New P: 0.644
-Original Grad: -0.033, -lr * Pred Grad: -0.037, New P: -0.504
iter 17 loss: 0.348
Actual params: [ 0.644 , -0.5038]
-Original Grad: -0.029, -lr * Pred Grad: -0.021, New P: 0.623
-Original Grad: -0.040, -lr * Pred Grad: -0.043, New P: -0.547
iter 18 loss: 0.350
Actual params: [ 0.6232, -0.5468]
-Original Grad: 0.023, -lr * Pred Grad: -0.009, New P: 0.614
-Original Grad: 0.017, -lr * Pred Grad: -0.029, New P: -0.576
iter 19 loss: 0.346
Actual params: [ 0.6144, -0.5762]
-Original Grad: 0.026, -lr * Pred Grad: 0.007, New P: 0.621
-Original Grad: -0.069, -lr * Pred Grad: -0.041, New P: -0.618
iter 20 loss: 0.351
Actual params: [ 0.6211, -0.6175]
-Original Grad: -0.067, -lr * Pred Grad: -0.026, New P: 0.595
-Original Grad: 0.024, -lr * Pred Grad: -0.033, New P: -0.651
Target params: [1.1812, 0.2779]
iter 0 loss: 0.315
Actual params: [0.5941, 0.5941]
-Original Grad: -0.100, -lr * Pred Grad: -0.048, New P: 0.546
-Original Grad: -0.502, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.317
Actual params: [0.5461, 0.5313]
-Original Grad: 0.002, -lr * Pred Grad: -0.066, New P: 0.480
-Original Grad: 0.001, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.319
Actual params: [0.4798, 0.4506]
-Original Grad: -0.006, -lr * Pred Grad: -0.067, New P: 0.413
-Original Grad: -0.056, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.311
Actual params: [0.4128, 0.3675]
-Original Grad: -0.011, -lr * Pred Grad: -0.063, New P: 0.349
-Original Grad: -0.163, -lr * Pred Grad: -0.084, New P: 0.284
iter 4 loss: 0.306
Actual params: [0.3494, 0.2839]
-Original Grad: -0.094, -lr * Pred Grad: -0.062, New P: 0.287
-Original Grad: -0.150, -lr * Pred Grad: -0.083, New P: 0.200
iter 5 loss: 0.306
Actual params: [0.287 , 0.2004]
-Original Grad: -0.077, -lr * Pred Grad: -0.063, New P: 0.224
-Original Grad: -0.076, -lr * Pred Grad: -0.082, New P: 0.119
iter 6 loss: 0.312
Actual params: [0.2242, 0.1186]
-Original Grad: 0.070, -lr * Pred Grad: -0.045, New P: 0.179
-Original Grad: -0.054, -lr * Pred Grad: -0.076, New P: 0.043
iter 7 loss: 0.321
Actual params: [0.1791, 0.0426]
-Original Grad: -0.122, -lr * Pred Grad: -0.048, New P: 0.131
-Original Grad: -0.096, -lr * Pred Grad: -0.072, New P: -0.030
iter 8 loss: 0.334
Actual params: [ 0.1307, -0.0296]
-Original Grad: 0.021, -lr * Pred Grad: -0.037, New P: 0.093
-Original Grad: -0.087, -lr * Pred Grad: -0.070, New P: -0.100
iter 9 loss: 0.349
Actual params: [ 0.0932, -0.0995]
-Original Grad: 0.010, -lr * Pred Grad: -0.018, New P: 0.075
-Original Grad: -0.038, -lr * Pred Grad: -0.068, New P: -0.167
iter 10 loss: 0.359
Actual params: [ 0.0751, -0.1672]
-Original Grad: 0.049, -lr * Pred Grad: 0.008, New P: 0.083
-Original Grad: -0.070, -lr * Pred Grad: -0.066, New P: -0.233
iter 11 loss: 0.361
Actual params: [ 0.0831, -0.2332]
-Original Grad: -0.157, -lr * Pred Grad: -0.040, New P: 0.043
-Original Grad: -0.104, -lr * Pred Grad: -0.065, New P: -0.299
iter 12 loss: 0.376
Actual params: [ 0.0431, -0.2985]
-Original Grad: 0.262, -lr * Pred Grad: 0.007, New P: 0.050
-Original Grad: 0.052, -lr * Pred Grad: -0.056, New P: -0.354
iter 13 loss: 0.378
Actual params: [ 0.0503, -0.3545]
-Original Grad: 0.428, -lr * Pred Grad: 0.051, New P: 0.102
-Original Grad: 0.112, -lr * Pred Grad: -0.035, New P: -0.389
iter 14 loss: 0.360
Actual params: [ 0.1016, -0.3895]
-Original Grad: 0.479, -lr * Pred Grad: 0.075, New P: 0.177
-Original Grad: -0.106, -lr * Pred Grad: -0.034, New P: -0.423
iter 15 loss: 0.340
Actual params: [ 0.1766, -0.4232]
-Original Grad: -0.158, -lr * Pred Grad: 0.085, New P: 0.261
-Original Grad: -0.042, -lr * Pred Grad: -0.055, New P: -0.478
iter 16 loss: 0.324
Actual params: [ 0.2613, -0.4783]
-Original Grad: -0.103, -lr * Pred Grad: 0.074, New P: 0.335
-Original Grad: 0.178, -lr * Pred Grad: -0.028, New P: -0.507
iter 17 loss: 0.317
Actual params: [ 0.3353, -0.5066]
-Original Grad: 0.031, -lr * Pred Grad: -0.006, New P: 0.329
-Original Grad: 0.059, -lr * Pred Grad: -0.021, New P: -0.528
iter 18 loss: 0.321
Actual params: [ 0.3291, -0.5276]
-Original Grad: 0.138, -lr * Pred Grad: 0.053, New P: 0.383
-Original Grad: 0.308, -lr * Pred Grad: -0.018, New P: -0.546
iter 19 loss: 0.318
Actual params: [ 0.3826, -0.546 ]
-Original Grad: -0.033, -lr * Pred Grad: 0.059, New P: 0.442
-Original Grad: 0.451, -lr * Pred Grad: -0.016, New P: -0.562
iter 20 loss: 0.314
Actual params: [ 0.442 , -0.5622]
-Original Grad: 0.049, -lr * Pred Grad: 0.005, New P: 0.447
-Original Grad: -0.031, -lr * Pred Grad: -0.013, New P: -0.575
Target params: [1.1812, 0.2779]
iter 0 loss: 0.660
Actual params: [0.5941, 0.5941]
-Original Grad: 0.100, -lr * Pred Grad: 0.065, New P: 0.659
-Original Grad: -0.123, -lr * Pred Grad: -0.055, New P: 0.539
iter 1 loss: 0.497
Actual params: [0.6591, 0.5394]
-Original Grad: -0.001, -lr * Pred Grad: 0.071, New P: 0.730
-Original Grad: -0.048, -lr * Pred Grad: -0.074, New P: 0.465
iter 2 loss: 0.395
Actual params: [0.7298, 0.4649]
-Original Grad: 0.110, -lr * Pred Grad: 0.055, New P: 0.785
-Original Grad: -0.080, -lr * Pred Grad: -0.074, New P: 0.391
iter 3 loss: 0.386
Actual params: [0.7851, 0.3909]
-Original Grad: 0.050, -lr * Pred Grad: 0.052, New P: 0.837
-Original Grad: 0.003, -lr * Pred Grad: -0.069, New P: 0.321
iter 4 loss: 0.393
Actual params: [0.8371, 0.3215]
-Original Grad: 0.014, -lr * Pred Grad: 0.017, New P: 0.854
-Original Grad: 0.127, -lr * Pred Grad: -0.057, New P: 0.264
iter 5 loss: 0.401
Actual params: [0.8543, 0.2644]
-Original Grad: -0.058, -lr * Pred Grad: -0.028, New P: 0.826
-Original Grad: -0.018, -lr * Pred Grad: -0.033, New P: 0.231
iter 6 loss: 0.406
Actual params: [0.8259, 0.2312]
-Original Grad: 0.014, -lr * Pred Grad: -0.017, New P: 0.809
-Original Grad: -0.095, -lr * Pred Grad: -0.043, New P: 0.188
iter 7 loss: 0.411
Actual params: [0.8093, 0.1881]
-Original Grad: 0.005, -lr * Pred Grad: -0.006, New P: 0.804
-Original Grad: -0.036, -lr * Pred Grad: -0.051, New P: 0.137
iter 8 loss: 0.415
Actual params: [0.8038, 0.1367]
-Original Grad: -0.072, -lr * Pred Grad: -0.039, New P: 0.765
-Original Grad: 0.020, -lr * Pred Grad: -0.031, New P: 0.106
iter 9 loss: 0.414
Actual params: [0.7648, 0.1058]
-Original Grad: -0.001, -lr * Pred Grad: -0.033, New P: 0.731
-Original Grad: 0.057, -lr * Pred Grad: 0.012, New P: 0.118
iter 10 loss: 0.409
Actual params: [0.7313, 0.1177]
-Original Grad: -0.091, -lr * Pred Grad: -0.044, New P: 0.687
-Original Grad: -0.018, -lr * Pred Grad: 0.005, New P: 0.122
iter 11 loss: 0.403
Actual params: [0.6873, 0.1223]
-Original Grad: -0.125, -lr * Pred Grad: -0.054, New P: 0.634
-Original Grad: -0.009, -lr * Pred Grad: -0.023, New P: 0.100
iter 12 loss: 0.398
Actual params: [0.6337, 0.0998]
-Original Grad: -0.071, -lr * Pred Grad: -0.056, New P: 0.578
-Original Grad: 0.055, -lr * Pred Grad: 0.004, New P: 0.104
iter 13 loss: 0.395
Actual params: [0.5777, 0.1037]
-Original Grad: 0.034, -lr * Pred Grad: -0.039, New P: 0.538
-Original Grad: 0.070, -lr * Pred Grad: 0.043, New P: 0.146
iter 14 loss: 0.395
Actual params: [0.5384, 0.1463]
-Original Grad: 0.001, -lr * Pred Grad: -0.016, New P: 0.523
-Original Grad: 0.018, -lr * Pred Grad: 0.028, New P: 0.174
iter 15 loss: 0.396
Actual params: [0.5226, 0.1743]
-Original Grad: -0.162, -lr * Pred Grad: -0.046, New P: 0.476
-Original Grad: 0.153, -lr * Pred Grad: 0.067, New P: 0.241
iter 16 loss: 0.417
Actual params: [0.4762, 0.2413]
-Original Grad: -0.131, -lr * Pred Grad: -0.055, New P: 0.421
-Original Grad: 0.068, -lr * Pred Grad: 0.075, New P: 0.316
iter 17 loss: 0.504
Actual params: [0.421 , 0.3164]
-Original Grad: 0.012, -lr * Pred Grad: -0.051, New P: 0.370
-Original Grad: -0.050, -lr * Pred Grad: -0.014, New P: 0.303
iter 18 loss: 0.545
Actual params: [0.3699, 0.3026]
-Original Grad: -0.236, -lr * Pred Grad: -0.060, New P: 0.309
-Original Grad: 0.258, -lr * Pred Grad: 0.052, New P: 0.355
iter 19 loss: 0.672
Actual params: [0.3095, 0.355 ]
-Original Grad: -0.019, -lr * Pred Grad: -0.063, New P: 0.247
-Original Grad: -0.080, -lr * Pred Grad: 0.060, New P: 0.415
iter 20 loss: 0.772
Actual params: [0.2467, 0.4152]
-Original Grad: 0.015, -lr * Pred Grad: -0.053, New P: 0.194
-Original Grad: 0.265, -lr * Pred Grad: 0.081, New P: 0.496
Target params: [1.1812, 0.2779]
iter 0 loss: 0.972
Actual params: [0.5941, 0.5941]
-Original Grad: -0.204, -lr * Pred Grad: -0.062, New P: 0.532
-Original Grad: -0.204, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.984
Actual params: [0.5319, 0.5319]
-Original Grad: -0.047, -lr * Pred Grad: -0.080, New P: 0.452
-Original Grad: -0.159, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.989
Actual params: [0.4523, 0.4515]
-Original Grad: -0.012, -lr * Pred Grad: -0.077, New P: 0.375
-Original Grad: -0.103, -lr * Pred Grad: -0.083, New P: 0.368
iter 3 loss: 0.990
Actual params: [0.3753, 0.3682]
-Original Grad: 0.059, -lr * Pred Grad: -0.070, New P: 0.305
-Original Grad: 0.016, -lr * Pred Grad: -0.080, New P: 0.289
iter 4 loss: 0.989
Actual params: [0.3051, 0.2887]
-Original Grad: -0.072, -lr * Pred Grad: -0.067, New P: 0.238
-Original Grad: -0.025, -lr * Pred Grad: -0.072, New P: 0.217
iter 5 loss: 0.987
Actual params: [0.238 , 0.2168]
-Original Grad: -0.055, -lr * Pred Grad: -0.064, New P: 0.174
-Original Grad: 0.080, -lr * Pred Grad: -0.068, New P: 0.149
iter 6 loss: 0.989
Actual params: [0.1741, 0.1491]
-Original Grad: 0.024, -lr * Pred Grad: -0.052, New P: 0.122
-Original Grad: 0.088, -lr * Pred Grad: -0.052, New P: 0.097
iter 7 loss: 0.984
Actual params: [0.122 , 0.0974]
-Original Grad: 0.002, -lr * Pred Grad: -0.036, New P: 0.086
-Original Grad: 0.054, -lr * Pred Grad: -0.033, New P: 0.065
iter 8 loss: 0.981
Actual params: [0.0862, 0.0647]
-Original Grad: 0.004, -lr * Pred Grad: -0.020, New P: 0.066
-Original Grad: 0.106, -lr * Pred Grad: -0.018, New P: 0.047
iter 9 loss: 0.979
Actual params: [0.0662, 0.0469]
-Original Grad: -0.002, -lr * Pred Grad: -0.009, New P: 0.058
-Original Grad: 0.076, -lr * Pred Grad: 0.012, New P: 0.059
iter 10 loss: 0.979
Actual params: [0.0577, 0.0587]
-Original Grad: -0.050, -lr * Pred Grad: -0.038, New P: 0.020
-Original Grad: -0.001, -lr * Pred Grad: 0.050, New P: 0.108
iter 11 loss: 0.980
Actual params: [0.0199, 0.1083]
-Original Grad: 0.011, -lr * Pred Grad: -0.032, New P: -0.012
-Original Grad: 0.045, -lr * Pred Grad: 0.002, New P: 0.110
iter 12 loss: 0.978
Actual params: [-0.0121,  0.1098]
-Original Grad: 0.008, -lr * Pred Grad: -0.018, New P: -0.030
-Original Grad: 0.056, -lr * Pred Grad: 0.052, New P: 0.161
iter 13 loss: 0.980
Actual params: [-0.0296,  0.1614]
-Original Grad: 0.062, -lr * Pred Grad: 0.013, New P: -0.016
-Original Grad: 0.074, -lr * Pred Grad: 0.050, New P: 0.211
iter 14 loss: 0.979
Actual params: [-0.0163,  0.2114]
-Original Grad: -0.002, -lr * Pred Grad: 0.008, New P: -0.008
-Original Grad: 0.021, -lr * Pred Grad: 0.026, New P: 0.238
iter 15 loss: 0.971
Actual params: [-0.0079,  0.2378]
-Original Grad: -0.052, -lr * Pred Grad: -0.029, New P: -0.037
-Original Grad: -0.098, -lr * Pred Grad: -0.039, New P: 0.199
iter 16 loss: 0.980
Actual params: [-0.0372,  0.1986]
-Original Grad: -0.014, -lr * Pred Grad: -0.042, New P: -0.079
-Original Grad: -0.032, -lr * Pred Grad: -0.050, New P: 0.149
iter 17 loss: 0.977
Actual params: [-0.0793,  0.1485]
-Original Grad: 0.015, -lr * Pred Grad: -0.030, New P: -0.109
-Original Grad: 0.046, -lr * Pred Grad: -0.014, New P: 0.134
iter 18 loss: 0.975
Actual params: [-0.1093,  0.1341]
-Original Grad: -0.007, -lr * Pred Grad: -0.021, New P: -0.130
-Original Grad: -0.028, -lr * Pred Grad: 0.002, New P: 0.136
iter 19 loss: 0.975
Actual params: [-0.1303,  0.1361]
-Original Grad: 0.039, -lr * Pred Grad: -0.003, New P: -0.134
-Original Grad: 0.121, -lr * Pred Grad: 0.049, New P: 0.185
iter 20 loss: 0.976
Actual params: [-0.1337,  0.1853]
-Original Grad: 0.012, -lr * Pred Grad: 0.003, New P: -0.131
-Original Grad: -0.020, -lr * Pred Grad: 0.006, New P: 0.192
Target params: [1.1812, 0.2779]
iter 0 loss: 0.757
Actual params: [0.5941, 0.5941]
-Original Grad: 0.035, -lr * Pred Grad: 0.054, New P: 0.648
-Original Grad: -0.311, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.742
Actual params: [0.6482, 0.5309]
-Original Grad: 0.161, -lr * Pred Grad: 0.080, New P: 0.729
-Original Grad: -0.093, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.699
Actual params: [0.7285, 0.4502]
-Original Grad: -0.312, -lr * Pred Grad: -0.038, New P: 0.690
-Original Grad: -0.157, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.727
Actual params: [0.6903, 0.3667]
-Original Grad: -0.050, -lr * Pred Grad: -0.065, New P: 0.625
-Original Grad: -0.092, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.745
Actual params: [0.6249, 0.283 ]
-Original Grad: 0.012, -lr * Pred Grad: -0.063, New P: 0.562
-Original Grad: -0.188, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.752
Actual params: [0.5618, 0.1991]
-Original Grad: -0.138, -lr * Pred Grad: -0.064, New P: 0.498
-Original Grad: -0.097, -lr * Pred Grad: -0.083, New P: 0.116
iter 6 loss: 0.758
Actual params: [0.4978, 0.1159]
-Original Grad: 0.024, -lr * Pred Grad: -0.058, New P: 0.440
-Original Grad: -0.048, -lr * Pred Grad: -0.079, New P: 0.037
iter 7 loss: 0.762
Actual params: [0.4401, 0.0366]
-Original Grad: 0.120, -lr * Pred Grad: -0.028, New P: 0.412
-Original Grad: -0.049, -lr * Pred Grad: -0.073, New P: -0.036
iter 8 loss: 0.765
Actual params: [ 0.4124, -0.036 ]
-Original Grad: 0.013, -lr * Pred Grad: 0.009, New P: 0.422
-Original Grad: 0.119, -lr * Pred Grad: -0.069, New P: -0.105
iter 9 loss: 0.766
Actual params: [ 0.4218, -0.1046]
-Original Grad: 0.218, -lr * Pred Grad: 0.052, New P: 0.473
-Original Grad: 0.154, -lr * Pred Grad: -0.057, New P: -0.161
iter 10 loss: 0.763
Actual params: [ 0.4734, -0.1614]
-Original Grad: 0.275, -lr * Pred Grad: 0.079, New P: 0.552
-Original Grad: 0.001, -lr * Pred Grad: -0.038, New P: -0.200
iter 11 loss: 0.756
Actual params: [ 0.552 , -0.1997]
-Original Grad: 0.034, -lr * Pred Grad: 0.085, New P: 0.637
-Original Grad: 0.013, -lr * Pred Grad: -0.028, New P: -0.228
iter 12 loss: 0.740
Actual params: [ 0.6374, -0.2276]
-Original Grad: 0.021, -lr * Pred Grad: 0.076, New P: 0.714
-Original Grad: 0.063, -lr * Pred Grad: -0.025, New P: -0.253
iter 13 loss: 0.702
Actual params: [ 0.7138, -0.2527]
-Original Grad: 0.106, -lr * Pred Grad: 0.070, New P: 0.783
-Original Grad: 0.067, -lr * Pred Grad: -0.020, New P: -0.273
iter 14 loss: 0.669
Actual params: [ 0.7833, -0.273 ]
-Original Grad: 0.321, -lr * Pred Grad: 0.084, New P: 0.868
-Original Grad: 0.135, -lr * Pred Grad: -0.010, New P: -0.283
iter 15 loss: 0.625
Actual params: [ 0.8677, -0.2835]
-Original Grad: -0.012, -lr * Pred Grad: 0.085, New P: 0.953
-Original Grad: -0.001, -lr * Pred Grad: 0.021, New P: -0.262
iter 16 loss: 0.582
Actual params: [ 0.9529, -0.2624]
-Original Grad: 0.108, -lr * Pred Grad: 0.081, New P: 1.034
-Original Grad: 0.114, -lr * Pred Grad: 0.062, New P: -0.200
iter 17 loss: 0.554
Actual params: [ 1.0337, -0.2002]
-Original Grad: -0.030, -lr * Pred Grad: 0.009, New P: 1.043
-Original Grad: -0.102, -lr * Pred Grad: -0.033, New P: -0.233
iter 18 loss: 0.556
Actual params: [ 1.0425, -0.2332]
-Original Grad: 0.070, -lr * Pred Grad: 0.053, New P: 1.096
-Original Grad: 0.031, -lr * Pred Grad: -0.023, New P: -0.257
iter 19 loss: 0.544
Actual params: [ 1.0957, -0.2566]
-Original Grad: 0.066, -lr * Pred Grad: 0.051, New P: 1.147
-Original Grad: 0.021, -lr * Pred Grad: 0.007, New P: -0.249
iter 20 loss: 0.545
Actual params: [ 1.1465, -0.2492]
-Original Grad: 0.077, -lr * Pred Grad: 0.065, New P: 1.211
-Original Grad: 0.024, -lr * Pred Grad: 0.007, New P: -0.242
Target params: [1.1812, 0.2779]
iter 0 loss: 0.609
Actual params: [0.5941, 0.5941]
-Original Grad: 0.081, -lr * Pred Grad: 0.064, New P: 0.658
-Original Grad: -0.042, -lr * Pred Grad: -0.006, New P: 0.588
iter 1 loss: 0.567
Actual params: [0.6576, 0.5876]
-Original Grad: 0.194, -lr * Pred Grad: 0.083, New P: 0.741
-Original Grad: -0.134, -lr * Pred Grad: -0.066, New P: 0.522
iter 2 loss: 0.501
Actual params: [0.7408, 0.5218]
-Original Grad: -0.344, -lr * Pred Grad: -0.035, New P: 0.706
-Original Grad: 0.154, -lr * Pred Grad: -0.067, New P: 0.455
iter 3 loss: 0.503
Actual params: [0.7061, 0.4551]
-Original Grad: -0.203, -lr * Pred Grad: -0.067, New P: 0.639
-Original Grad: -0.120, -lr * Pred Grad: -0.066, New P: 0.389
iter 4 loss: 0.529
Actual params: [0.6389, 0.3894]
-Original Grad: 0.291, -lr * Pred Grad: -0.032, New P: 0.606
-Original Grad: -0.163, -lr * Pred Grad: -0.067, New P: 0.323
iter 5 loss: 0.531
Actual params: [0.6064, 0.3229]
-Original Grad: 0.170, -lr * Pred Grad: 0.035, New P: 0.641
-Original Grad: 0.022, -lr * Pred Grad: -0.064, New P: 0.259
iter 6 loss: 0.500
Actual params: [0.641 , 0.2594]
-Original Grad: 0.150, -lr * Pred Grad: 0.076, New P: 0.717
-Original Grad: -0.034, -lr * Pred Grad: -0.056, New P: 0.203
iter 7 loss: 0.445
Actual params: [0.7166, 0.2033]
-Original Grad: -0.218, -lr * Pred Grad: -0.024, New P: 0.692
-Original Grad: -0.188, -lr * Pred Grad: -0.063, New P: 0.140
iter 8 loss: 0.447
Actual params: [0.6923, 0.1402]
-Original Grad: 0.056, -lr * Pred Grad: -0.020, New P: 0.672
-Original Grad: -0.163, -lr * Pred Grad: -0.065, New P: 0.075
iter 9 loss: 0.450
Actual params: [0.6723, 0.0747]
-Original Grad: 0.275, -lr * Pred Grad: 0.047, New P: 0.720
-Original Grad: -0.126, -lr * Pred Grad: -0.067, New P: 0.008
iter 10 loss: 0.413
Actual params: [0.7197, 0.008 ]
-Original Grad: 0.399, -lr * Pred Grad: 0.080, New P: 0.800
-Original Grad: -0.076, -lr * Pred Grad: -0.066, New P: -0.058
iter 11 loss: 0.363
Actual params: [ 0.7998, -0.0584]
-Original Grad: 0.020, -lr * Pred Grad: 0.086, New P: 0.886
-Original Grad: -0.073, -lr * Pred Grad: -0.066, New P: -0.124
iter 12 loss: 0.317
Actual params: [ 0.8862, -0.124 ]
-Original Grad: -0.751, -lr * Pred Grad: -0.035, New P: 0.851
-Original Grad: 0.081, -lr * Pred Grad: -0.052, New P: -0.176
iter 13 loss: 0.319
Actual params: [ 0.8514, -0.1759]
-Original Grad: -0.002, -lr * Pred Grad: -0.056, New P: 0.796
-Original Grad: -0.038, -lr * Pred Grad: -0.039, New P: -0.215
iter 14 loss: 0.331
Actual params: [ 0.7956, -0.2151]
-Original Grad: 0.211, -lr * Pred Grad: -0.014, New P: 0.781
-Original Grad: -0.085, -lr * Pred Grad: -0.054, New P: -0.269
iter 15 loss: 0.321
Actual params: [ 0.7812, -0.269 ]
-Original Grad: -0.119, -lr * Pred Grad: 0.020, New P: 0.801
-Original Grad: -0.031, -lr * Pred Grad: -0.055, New P: -0.324
iter 16 loss: 0.318
Actual params: [ 0.801 , -0.3236]
-Original Grad: -0.195, -lr * Pred Grad: -0.045, New P: 0.756
-Original Grad: -0.082, -lr * Pred Grad: -0.058, New P: -0.381
iter 17 loss: 0.317
Actual params: [ 0.7555, -0.3815]
-Original Grad: 0.215, -lr * Pred Grad: 0.008, New P: 0.764
-Original Grad: -0.063, -lr * Pred Grad: -0.059, New P: -0.440
iter 18 loss: 0.319
Actual params: [ 0.7639, -0.4405]
-Original Grad: 0.124, -lr * Pred Grad: 0.065, New P: 0.829
-Original Grad: -0.045, -lr * Pred Grad: -0.057, New P: -0.497
iter 19 loss: 0.314
Actual params: [ 0.8294, -0.4972]
-Original Grad: 0.218, -lr * Pred Grad: 0.083, New P: 0.913
-Original Grad: -0.098, -lr * Pred Grad: -0.059, New P: -0.556
iter 20 loss: 0.278
Actual params: [ 0.9128, -0.5565]
-Original Grad: -0.115, -lr * Pred Grad: 0.046, New P: 0.959
-Original Grad: 0.110, -lr * Pred Grad: -0.034, New P: -0.590
Target params: [1.1812, 0.2779]
iter 0 loss: 0.539
Actual params: [0.5941, 0.5941]
-Original Grad: 0.353, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.229, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.410
Actual params: [0.6617, 0.5314]
-Original Grad: -0.050, -lr * Pred Grad: 0.084, New P: 0.746
-Original Grad: -0.088, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.332
Actual params: [0.7461, 0.4509]
-Original Grad: -0.019, -lr * Pred Grad: 0.085, New P: 0.831
-Original Grad: -0.123, -lr * Pred Grad: -0.083, New P: 0.368
iter 3 loss: 0.301
Actual params: [0.8309, 0.3678]
-Original Grad: -0.220, -lr * Pred Grad: -0.033, New P: 0.798
-Original Grad: -0.045, -lr * Pred Grad: -0.081, New P: 0.287
iter 4 loss: 0.298
Actual params: [0.7983, 0.2869]
-Original Grad: -0.017, -lr * Pred Grad: -0.052, New P: 0.746
-Original Grad: -0.107, -lr * Pred Grad: -0.077, New P: 0.210
iter 5 loss: 0.299
Actual params: [0.7461, 0.2101]
-Original Grad: -0.329, -lr * Pred Grad: -0.061, New P: 0.685
-Original Grad: -0.087, -lr * Pred Grad: -0.073, New P: 0.137
iter 6 loss: 0.304
Actual params: [0.6849, 0.1373]
-Original Grad: 0.034, -lr * Pred Grad: -0.063, New P: 0.622
-Original Grad: -0.090, -lr * Pred Grad: -0.070, New P: 0.067
iter 7 loss: 0.312
Actual params: [0.6222, 0.0671]
-Original Grad: 0.340, -lr * Pred Grad: -0.035, New P: 0.588
-Original Grad: -0.034, -lr * Pred Grad: -0.068, New P: -0.001
iter 8 loss: 0.315
Actual params: [ 0.5876, -0.0008]
-Original Grad: 0.233, -lr * Pred Grad: -0.007, New P: 0.581
-Original Grad: -0.056, -lr * Pred Grad: -0.066, New P: -0.067
iter 9 loss: 0.311
Actual params: [ 0.5809, -0.0666]
-Original Grad: 0.130, -lr * Pred Grad: 0.034, New P: 0.615
-Original Grad: -0.104, -lr * Pred Grad: -0.065, New P: -0.132
iter 10 loss: 0.302
Actual params: [ 0.6147, -0.1317]
-Original Grad: 0.512, -lr * Pred Grad: 0.074, New P: 0.689
-Original Grad: -0.051, -lr * Pred Grad: -0.064, New P: -0.196
iter 11 loss: 0.291
Actual params: [ 0.6885, -0.1955]
-Original Grad: 0.352, -lr * Pred Grad: 0.086, New P: 0.774
-Original Grad: -0.069, -lr * Pred Grad: -0.063, New P: -0.258
iter 12 loss: 0.290
Actual params: [ 0.7744, -0.258 ]
-Original Grad: -0.060, -lr * Pred Grad: 0.088, New P: 0.862
-Original Grad: -0.033, -lr * Pred Grad: -0.059, New P: -0.317
iter 13 loss: 0.294
Actual params: [ 0.8622, -0.317 ]
-Original Grad: 0.043, -lr * Pred Grad: 0.087, New P: 0.950
-Original Grad: -0.053, -lr * Pred Grad: -0.056, New P: -0.373
iter 14 loss: 0.298
Actual params: [ 0.9496, -0.3732]
-Original Grad: 0.057, -lr * Pred Grad: 0.081, New P: 1.030
-Original Grad: 0.001, -lr * Pred Grad: -0.047, New P: -0.420
iter 15 loss: 0.300
Actual params: [ 1.0303, -0.4202]
-Original Grad: 0.022, -lr * Pred Grad: 0.019, New P: 1.049
-Original Grad: -0.015, -lr * Pred Grad: -0.040, New P: -0.460
iter 16 loss: 0.301
Actual params: [ 1.0491, -0.4603]
-Original Grad: 0.091, -lr * Pred Grad: 0.064, New P: 1.113
-Original Grad: -0.059, -lr * Pred Grad: -0.050, New P: -0.510
iter 17 loss: 0.303
Actual params: [ 1.1126, -0.5099]
-Original Grad: 0.157, -lr * Pred Grad: 0.081, New P: 1.193
-Original Grad: -0.137, -lr * Pred Grad: -0.061, New P: -0.571
iter 18 loss: 0.300
Actual params: [ 1.1934, -0.5713]
-Original Grad: -0.004, -lr * Pred Grad: 0.067, New P: 1.260
-Original Grad: -0.107, -lr * Pred Grad: -0.064, New P: -0.635
iter 19 loss: 0.297
Actual params: [ 1.2602, -0.6351]
-Original Grad: 0.039, -lr * Pred Grad: 0.022, New P: 1.282
-Original Grad: -0.026, -lr * Pred Grad: -0.062, New P: -0.698
iter 20 loss: 0.296
Actual params: [ 1.2818, -0.6976]
-Original Grad: -0.012, -lr * Pred Grad: 0.005, New P: 1.287
-Original Grad: -0.061, -lr * Pred Grad: -0.060, New P: -0.758
Target params: [1.1812, 0.2779]
iter 0 loss: 0.436
Actual params: [0.5941, 0.5941]
-Original Grad: -0.068, -lr * Pred Grad: -0.030, New P: 0.564
-Original Grad: -0.332, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.416
Actual params: [0.5643, 0.5309]
-Original Grad: -0.069, -lr * Pred Grad: -0.069, New P: 0.495
-Original Grad: -0.334, -lr * Pred Grad: -0.080, New P: 0.450
iter 2 loss: 0.455
Actual params: [0.4951, 0.4504]
-Original Grad: -0.098, -lr * Pred Grad: -0.074, New P: 0.421
-Original Grad: 0.043, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.502
Actual params: [0.4206, 0.3672]
-Original Grad: 0.125, -lr * Pred Grad: -0.067, New P: 0.354
-Original Grad: -0.082, -lr * Pred Grad: -0.083, New P: 0.284
iter 4 loss: 0.523
Actual params: [0.3537, 0.2841]
-Original Grad: 0.162, -lr * Pred Grad: -0.043, New P: 0.311
-Original Grad: -0.107, -lr * Pred Grad: -0.082, New P: 0.202
iter 5 loss: 0.531
Actual params: [0.3109, 0.202 ]
-Original Grad: 0.013, -lr * Pred Grad: 0.010, New P: 0.320
-Original Grad: -0.004, -lr * Pred Grad: -0.075, New P: 0.127
iter 6 loss: 0.517
Actual params: [0.3205, 0.1273]
-Original Grad: -0.092, -lr * Pred Grad: -0.023, New P: 0.297
-Original Grad: -0.165, -lr * Pred Grad: -0.074, New P: 0.054
iter 7 loss: 0.517
Actual params: [0.2971, 0.0537]
-Original Grad: -0.067, -lr * Pred Grad: -0.054, New P: 0.243
-Original Grad: -0.060, -lr * Pred Grad: -0.070, New P: -0.017
iter 8 loss: 0.529
Actual params: [ 0.2432, -0.0166]
-Original Grad: 0.162, -lr * Pred Grad: -0.004, New P: 0.239
-Original Grad: -0.026, -lr * Pred Grad: -0.068, New P: -0.084
iter 9 loss: 0.532
Actual params: [ 0.2393, -0.0844]
-Original Grad: 0.024, -lr * Pred Grad: 0.051, New P: 0.290
-Original Grad: -0.017, -lr * Pred Grad: -0.064, New P: -0.148
iter 10 loss: 0.524
Actual params: [ 0.2904, -0.1482]
-Original Grad: 0.126, -lr * Pred Grad: 0.075, New P: 0.366
-Original Grad: 0.046, -lr * Pred Grad: -0.047, New P: -0.196
iter 11 loss: 0.503
Actual params: [ 0.3657, -0.1955]
-Original Grad: -0.003, -lr * Pred Grad: 0.022, New P: 0.387
-Original Grad: -0.115, -lr * Pred Grad: -0.054, New P: -0.249
iter 12 loss: 0.500
Actual params: [ 0.3872, -0.2492]
-Original Grad: 0.092, -lr * Pred Grad: 0.064, New P: 0.451
-Original Grad: -0.095, -lr * Pred Grad: -0.061, New P: -0.310
iter 13 loss: 0.463
Actual params: [ 0.4508, -0.3104]
-Original Grad: -0.074, -lr * Pred Grad: -0.031, New P: 0.420
-Original Grad: -0.114, -lr * Pred Grad: -0.064, New P: -0.374
iter 14 loss: 0.494
Actual params: [ 0.42  , -0.3741]
-Original Grad: 0.146, -lr * Pred Grad: 0.038, New P: 0.458
-Original Grad: -0.017, -lr * Pred Grad: -0.062, New P: -0.436
iter 15 loss: 0.468
Actual params: [ 0.4582, -0.4358]
-Original Grad: 0.380, -lr * Pred Grad: 0.078, New P: 0.536
-Original Grad: -0.068, -lr * Pred Grad: -0.060, New P: -0.496
iter 16 loss: 0.414
Actual params: [ 0.5358, -0.4957]
-Original Grad: -0.235, -lr * Pred Grad: 0.040, New P: 0.576
-Original Grad: -0.015, -lr * Pred Grad: -0.054, New P: -0.549
iter 17 loss: 0.398
Actual params: [ 0.5757, -0.5495]
-Original Grad: -0.033, -lr * Pred Grad: -0.040, New P: 0.536
-Original Grad: -0.225, -lr * Pred Grad: -0.063, New P: -0.613
iter 18 loss: 0.418
Actual params: [ 0.536 , -0.6126]
-Original Grad: -0.059, -lr * Pred Grad: -0.049, New P: 0.487
-Original Grad: -0.034, -lr * Pred Grad: -0.064, New P: -0.677
iter 19 loss: 0.463
Actual params: [ 0.4866, -0.6766]
-Original Grad: 0.052, -lr * Pred Grad: -0.011, New P: 0.475
-Original Grad: 0.056, -lr * Pred Grad: -0.050, New P: -0.727
iter 20 loss: 0.487
Actual params: [ 0.4754, -0.7269]
-Original Grad: 0.174, -lr * Pred Grad: 0.048, New P: 0.524
-Original Grad: 0.063, -lr * Pred Grad: -0.030, New P: -0.757
Target params: [1.1812, 0.2779]
iter 0 loss: 0.318
Actual params: [0.5941, 0.5941]
-Original Grad: -0.131, -lr * Pred Grad: -0.056, New P: 0.538
-Original Grad: -0.162, -lr * Pred Grad: -0.060, New P: 0.534
iter 1 loss: 0.326
Actual params: [0.5377, 0.534 ]
-Original Grad: 0.018, -lr * Pred Grad: -0.070, New P: 0.468
-Original Grad: 0.043, -lr * Pred Grad: -0.071, New P: 0.463
iter 2 loss: 0.348
Actual params: [0.468 , 0.4629]
-Original Grad: -0.044, -lr * Pred Grad: -0.069, New P: 0.399
-Original Grad: -0.084, -lr * Pred Grad: -0.070, New P: 0.393
iter 3 loss: 0.386
Actual params: [0.3992, 0.3929]
-Original Grad: 0.084, -lr * Pred Grad: -0.061, New P: 0.339
-Original Grad: 0.035, -lr * Pred Grad: -0.067, New P: 0.326
iter 4 loss: 0.412
Actual params: [0.3386, 0.3261]
-Original Grad: 0.024, -lr * Pred Grad: -0.032, New P: 0.306
-Original Grad: -0.003, -lr * Pred Grad: -0.057, New P: 0.269
iter 5 loss: 0.414
Actual params: [0.3064, 0.2686]
-Original Grad: -0.018, -lr * Pred Grad: 0.000, New P: 0.306
-Original Grad: -0.104, -lr * Pred Grad: -0.058, New P: 0.211
iter 6 loss: 0.395
Actual params: [0.3065, 0.211 ]
-Original Grad: 0.012, -lr * Pred Grad: -0.007, New P: 0.300
-Original Grad: -0.076, -lr * Pred Grad: -0.061, New P: 0.150
iter 7 loss: 0.382
Actual params: [0.2998, 0.1502]
-Original Grad: 0.039, -lr * Pred Grad: 0.013, New P: 0.313
-Original Grad: -0.116, -lr * Pred Grad: -0.063, New P: 0.087
iter 8 loss: 0.360
Actual params: [0.3132, 0.0867]
-Original Grad: 0.001, -lr * Pred Grad: -0.002, New P: 0.311
-Original Grad: -0.159, -lr * Pred Grad: -0.065, New P: 0.022
iter 9 loss: 0.350
Actual params: [0.311 , 0.0216]
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: 0.304
-Original Grad: -0.064, -lr * Pred Grad: -0.065, New P: -0.044
iter 10 loss: 0.345
Actual params: [ 0.3039, -0.0436]
-Original Grad: -0.024, -lr * Pred Grad: -0.027, New P: 0.277
-Original Grad: 0.007, -lr * Pred Grad: -0.060, New P: -0.104
iter 11 loss: 0.352
Actual params: [ 0.2768, -0.1041]
-Original Grad: 0.060, -lr * Pred Grad: 0.002, New P: 0.279
-Original Grad: -0.027, -lr * Pred Grad: -0.051, New P: -0.155
iter 12 loss: 0.347
Actual params: [ 0.2787, -0.1549]
-Original Grad: -0.036, -lr * Pred Grad: -0.013, New P: 0.266
-Original Grad: -0.086, -lr * Pred Grad: -0.055, New P: -0.210
iter 13 loss: 0.350
Actual params: [ 0.2655, -0.2098]
-Original Grad: 0.085, -lr * Pred Grad: 0.020, New P: 0.286
-Original Grad: -0.049, -lr * Pred Grad: -0.056, New P: -0.265
iter 14 loss: 0.338
Actual params: [ 0.286 , -0.2654]
-Original Grad: -0.075, -lr * Pred Grad: -0.024, New P: 0.262
-Original Grad: -0.039, -lr * Pred Grad: -0.052, New P: -0.317
iter 15 loss: 0.346
Actual params: [ 0.2623, -0.3172]
-Original Grad: -0.052, -lr * Pred Grad: -0.047, New P: 0.215
-Original Grad: -0.055, -lr * Pred Grad: -0.050, New P: -0.367
iter 16 loss: 0.370
Actual params: [ 0.2151, -0.3671]
-Original Grad: 0.004, -lr * Pred Grad: -0.043, New P: 0.172
-Original Grad: -0.065, -lr * Pred Grad: -0.052, New P: -0.419
iter 17 loss: 0.397
Actual params: [ 0.1723, -0.4191]
-Original Grad: -0.082, -lr * Pred Grad: -0.048, New P: 0.124
-Original Grad: -0.019, -lr * Pred Grad: -0.047, New P: -0.466
iter 18 loss: 0.431
Actual params: [ 0.1242, -0.4664]
-Original Grad: -0.109, -lr * Pred Grad: -0.057, New P: 0.068
-Original Grad: -0.037, -lr * Pred Grad: -0.042, New P: -0.509
iter 19 loss: 0.471
Actual params: [ 0.0676, -0.5089]
-Original Grad: 0.042, -lr * Pred Grad: -0.048, New P: 0.020
-Original Grad: -0.083, -lr * Pred Grad: -0.048, New P: -0.557
iter 20 loss: 0.497
Actual params: [ 0.02 , -0.557]
-Original Grad: 0.050, -lr * Pred Grad: -0.021, New P: -0.001
-Original Grad: 0.096, -lr * Pred Grad: -0.029, New P: -0.586
Target params: [1.1812, 0.2779]
iter 0 loss: 0.419
Actual params: [0.5941, 0.5941]
-Original Grad: -0.122, -lr * Pred Grad: -0.054, New P: 0.540
-Original Grad: -0.038, -lr * Pred Grad: -0.002, New P: 0.592
iter 1 loss: 0.429
Actual params: [0.5396, 0.592 ]
-Original Grad: -0.038, -lr * Pred Grad: -0.073, New P: 0.466
-Original Grad: -0.021, -lr * Pred Grad: -0.055, New P: 0.537
iter 2 loss: 0.429
Actual params: [0.4662, 0.5372]
-Original Grad: -0.030, -lr * Pred Grad: -0.071, New P: 0.395
-Original Grad: -0.046, -lr * Pred Grad: -0.067, New P: 0.471
iter 3 loss: 0.419
Actual params: [0.3952, 0.4707]
-Original Grad: -0.198, -lr * Pred Grad: -0.072, New P: 0.323
-Original Grad: 0.090, -lr * Pred Grad: -0.051, New P: 0.420
iter 4 loss: 0.409
Actual params: [0.3231, 0.4198]
-Original Grad: 0.069, -lr * Pred Grad: -0.068, New P: 0.255
-Original Grad: -0.014, -lr * Pred Grad: -0.011, New P: 0.409
iter 5 loss: 0.408
Actual params: [0.2552, 0.4092]
-Original Grad: 0.064, -lr * Pred Grad: -0.053, New P: 0.202
-Original Grad: -0.016, -lr * Pred Grad: -0.011, New P: 0.399
iter 6 loss: 0.406
Actual params: [0.2018, 0.3986]
-Original Grad: 0.189, -lr * Pred Grad: -0.033, New P: 0.169
-Original Grad: -0.051, -lr * Pred Grad: -0.042, New P: 0.357
iter 7 loss: 0.394
Actual params: [0.1692, 0.357 ]
-Original Grad: -0.010, -lr * Pred Grad: -0.015, New P: 0.154
-Original Grad: 0.020, -lr * Pred Grad: -0.026, New P: 0.331
iter 8 loss: 0.386
Actual params: [0.1544, 0.3309]
-Original Grad: 0.085, -lr * Pred Grad: 0.019, New P: 0.174
-Original Grad: -0.047, -lr * Pred Grad: -0.033, New P: 0.298
iter 9 loss: 0.375
Actual params: [0.1735, 0.2984]
-Original Grad: -0.010, -lr * Pred Grad: 0.035, New P: 0.208
-Original Grad: -0.079, -lr * Pred Grad: -0.050, New P: 0.249
iter 10 loss: 0.351
Actual params: [0.2085, 0.2485]
-Original Grad: 0.023, -lr * Pred Grad: -0.017, New P: 0.192
-Original Grad: -0.117, -lr * Pred Grad: -0.061, New P: 0.188
iter 11 loss: 0.330
Actual params: [0.1919, 0.1878]
-Original Grad: 0.007, -lr * Pred Grad: 0.008, New P: 0.200
-Original Grad: 0.055, -lr * Pred Grad: -0.047, New P: 0.141
iter 12 loss: 0.317
Actual params: [0.2001, 0.141 ]
-Original Grad: 0.030, -lr * Pred Grad: 0.002, New P: 0.202
-Original Grad: -0.035, -lr * Pred Grad: -0.030, New P: 0.111
iter 13 loss: 0.313
Actual params: [0.2023, 0.1111]
-Original Grad: 0.087, -lr * Pred Grad: 0.048, New P: 0.250
-Original Grad: -0.123, -lr * Pred Grad: -0.050, New P: 0.061
iter 14 loss: 0.306
Actual params: [0.2502, 0.0611]
-Original Grad: 0.160, -lr * Pred Grad: 0.076, New P: 0.326
-Original Grad: -0.079, -lr * Pred Grad: -0.060, New P: 0.001
iter 15 loss: 0.305
Actual params: [0.3258, 0.0015]
-Original Grad: -0.039, -lr * Pred Grad: 0.026, New P: 0.352
-Original Grad: -0.058, -lr * Pred Grad: -0.061, New P: -0.060
iter 16 loss: 0.316
Actual params: [ 0.3516, -0.0595]
-Original Grad: -0.030, -lr * Pred Grad: -0.027, New P: 0.325
-Original Grad: 0.076, -lr * Pred Grad: -0.036, New P: -0.096
iter 17 loss: 0.319
Actual params: [ 0.3249, -0.0959]
-Original Grad: 0.016, -lr * Pred Grad: -0.013, New P: 0.312
-Original Grad: -0.003, -lr * Pred Grad: -0.014, New P: -0.110
iter 18 loss: 0.321
Actual params: [ 0.312 , -0.1102]
-Original Grad: 0.046, -lr * Pred Grad: 0.027, New P: 0.339
-Original Grad: -0.003, -lr * Pred Grad: -0.012, New P: -0.122
iter 19 loss: 0.326
Actual params: [ 0.3388, -0.1221]
-Original Grad: 0.062, -lr * Pred Grad: 0.044, New P: 0.383
-Original Grad: -0.045, -lr * Pred Grad: -0.031, New P: -0.153
iter 20 loss: 0.338
Actual params: [ 0.383 , -0.1527]
-Original Grad: -0.015, -lr * Pred Grad: -0.011, New P: 0.372
-Original Grad: -0.040, -lr * Pred Grad: -0.041, New P: -0.194
Target params: [1.1812, 0.2779]
iter 0 loss: 1.204
Actual params: [0.5941, 0.5941]
-Original Grad: -0.067, -lr * Pred Grad: -0.029, New P: 0.565
-Original Grad: -0.034, -lr * Pred Grad: 0.002, New P: 0.596
iter 1 loss: 1.220
Actual params: [0.5649, 0.596 ]
-Original Grad: 0.112, -lr * Pred Grad: 0.008, New P: 0.573
-Original Grad: 0.046, -lr * Pred Grad: -0.019, New P: 0.577
iter 2 loss: 1.213
Actual params: [0.573 , 0.5771]
-Original Grad: -0.162, -lr * Pred Grad: -0.058, New P: 0.515
-Original Grad: -0.063, -lr * Pred Grad: -0.056, New P: 0.521
iter 3 loss: 1.216
Actual params: [0.5154, 0.5206]
-Original Grad: -0.009, -lr * Pred Grad: -0.067, New P: 0.448
-Original Grad: 0.004, -lr * Pred Grad: -0.058, New P: 0.462
iter 4 loss: 1.226
Actual params: [0.448 , 0.4625]
-Original Grad: 0.048, -lr * Pred Grad: -0.052, New P: 0.396
-Original Grad: -0.083, -lr * Pred Grad: -0.058, New P: 0.405
iter 5 loss: 1.225
Actual params: [0.3956, 0.4048]
-Original Grad: 0.071, -lr * Pred Grad: -0.001, New P: 0.394
-Original Grad: -0.082, -lr * Pred Grad: -0.060, New P: 0.345
iter 6 loss: 1.209
Actual params: [0.3941, 0.3445]
-Original Grad: -0.118, -lr * Pred Grad: -0.034, New P: 0.360
-Original Grad: -0.237, -lr * Pred Grad: -0.065, New P: 0.279
iter 7 loss: 1.200
Actual params: [0.3603, 0.2792]
-Original Grad: -0.015, -lr * Pred Grad: -0.048, New P: 0.312
-Original Grad: -0.035, -lr * Pred Grad: -0.066, New P: 0.214
iter 8 loss: 1.191
Actual params: [0.3124, 0.2135]
-Original Grad: 0.062, -lr * Pred Grad: -0.008, New P: 0.304
-Original Grad: -0.042, -lr * Pred Grad: -0.064, New P: 0.150
iter 9 loss: 1.181
Actual params: [0.304 , 0.1496]
-Original Grad: 0.058, -lr * Pred Grad: 0.041, New P: 0.345
-Original Grad: 0.017, -lr * Pred Grad: -0.053, New P: 0.096
iter 10 loss: 1.173
Actual params: [0.3446, 0.0963]
-Original Grad: 0.132, -lr * Pred Grad: 0.071, New P: 0.416
-Original Grad: 0.050, -lr * Pred Grad: -0.031, New P: 0.065
iter 11 loss: 1.158
Actual params: [0.4157, 0.0651]
-Original Grad: 0.307, -lr * Pred Grad: 0.084, New P: 0.499
-Original Grad: -0.068, -lr * Pred Grad: -0.029, New P: 0.036
iter 12 loss: 1.136
Actual params: [0.4992, 0.0361]
-Original Grad: 0.141, -lr * Pred Grad: 0.087, New P: 0.586
-Original Grad: -0.049, -lr * Pred Grad: -0.045, New P: -0.009
iter 13 loss: 1.100
Actual params: [ 0.5861, -0.0087]
-Original Grad: 0.041, -lr * Pred Grad: 0.087, New P: 0.673
-Original Grad: -0.060, -lr * Pred Grad: -0.052, New P: -0.061
iter 14 loss: 1.066
Actual params: [ 0.6727, -0.0607]
-Original Grad: -0.070, -lr * Pred Grad: 0.010, New P: 0.683
-Original Grad: -0.121, -lr * Pred Grad: -0.060, New P: -0.120
iter 15 loss: 1.066
Actual params: [ 0.6831, -0.1204]
-Original Grad: 0.023, -lr * Pred Grad: 0.022, New P: 0.705
-Original Grad: 0.064, -lr * Pred Grad: -0.045, New P: -0.166
iter 16 loss: 1.053
Actual params: [ 0.7054, -0.1655]
-Original Grad: 0.023, -lr * Pred Grad: 0.009, New P: 0.714
-Original Grad: 0.035, -lr * Pred Grad: -0.016, New P: -0.182
iter 17 loss: 1.048
Actual params: [ 0.7144, -0.1818]
-Original Grad: 0.300, -lr * Pred Grad: 0.067, New P: 0.782
-Original Grad: 0.025, -lr * Pred Grad: 0.000, New P: -0.182
iter 18 loss: 1.017
Actual params: [ 0.7815, -0.1817]
-Original Grad: 0.117, -lr * Pred Grad: 0.084, New P: 0.865
-Original Grad: -0.059, -lr * Pred Grad: -0.025, New P: -0.207
iter 19 loss: 0.971
Actual params: [ 0.8652, -0.2067]
-Original Grad: 0.044, -lr * Pred Grad: 0.083, New P: 0.949
-Original Grad: 0.007, -lr * Pred Grad: -0.032, New P: -0.239
iter 20 loss: 0.924
Actual params: [ 0.9486, -0.2392]
-Original Grad: 0.195, -lr * Pred Grad: 0.085, New P: 1.033
-Original Grad: -0.005, -lr * Pred Grad: -0.029, New P: -0.268
Target params: [1.1812, 0.2779]
iter 0 loss: 0.429
Actual params: [0.5941, 0.5941]
-Original Grad: 0.045, -lr * Pred Grad: 0.057, New P: 0.651
-Original Grad: -0.174, -lr * Pred Grad: -0.061, New P: 0.533
iter 1 loss: 0.441
Actual params: [0.6513, 0.5332]
-Original Grad: -0.009, -lr * Pred Grad: 0.005, New P: 0.656
-Original Grad: -0.039, -lr * Pred Grad: -0.078, New P: 0.455
iter 2 loss: 0.417
Actual params: [0.6564, 0.455 ]
-Original Grad: 0.009, -lr * Pred Grad: -0.037, New P: 0.620
-Original Grad: -0.047, -lr * Pred Grad: -0.076, New P: 0.379
iter 3 loss: 0.401
Actual params: [0.6197, 0.3793]
-Original Grad: 0.016, -lr * Pred Grad: -0.021, New P: 0.599
-Original Grad: -0.109, -lr * Pred Grad: -0.072, New P: 0.307
iter 4 loss: 0.393
Actual params: [0.5991, 0.3069]
-Original Grad: 0.029, -lr * Pred Grad: 0.021, New P: 0.620
-Original Grad: -0.059, -lr * Pred Grad: -0.069, New P: 0.238
iter 5 loss: 0.388
Actual params: [0.6202, 0.2376]
-Original Grad: -0.009, -lr * Pred Grad: -0.009, New P: 0.611
-Original Grad: -0.061, -lr * Pred Grad: -0.067, New P: 0.170
iter 6 loss: 0.383
Actual params: [0.6115, 0.1702]
-Original Grad: -0.007, -lr * Pred Grad: -0.010, New P: 0.601
-Original Grad: -0.144, -lr * Pred Grad: -0.067, New P: 0.103
iter 7 loss: 0.379
Actual params: [0.6011, 0.1035]
-Original Grad: -0.003, -lr * Pred Grad: -0.018, New P: 0.583
-Original Grad: -0.114, -lr * Pred Grad: -0.067, New P: 0.037
iter 8 loss: 0.380
Actual params: [0.5833, 0.0369]
-Original Grad: -0.035, -lr * Pred Grad: -0.028, New P: 0.555
-Original Grad: -0.057, -lr * Pred Grad: -0.066, New P: -0.029
iter 9 loss: 0.382
Actual params: [ 0.555 , -0.0289]
-Original Grad: -0.020, -lr * Pred Grad: -0.029, New P: 0.526
-Original Grad: 0.048, -lr * Pred Grad: -0.056, New P: -0.085
iter 10 loss: 0.384
Actual params: [ 0.5259, -0.0853]
-Original Grad: -0.015, -lr * Pred Grad: -0.025, New P: 0.501
-Original Grad: -0.003, -lr * Pred Grad: -0.040, New P: -0.126
iter 11 loss: 0.384
Actual params: [ 0.501 , -0.1257]
-Original Grad: 0.007, -lr * Pred Grad: -0.014, New P: 0.487
-Original Grad: -0.037, -lr * Pred Grad: -0.041, New P: -0.166
iter 12 loss: 0.385
Actual params: [ 0.4868, -0.1665]
-Original Grad: -0.041, -lr * Pred Grad: -0.024, New P: 0.463
-Original Grad: -0.013, -lr * Pred Grad: -0.042, New P: -0.208
iter 13 loss: 0.385
Actual params: [ 0.4627, -0.2083]
-Original Grad: -0.043, -lr * Pred Grad: -0.034, New P: 0.429
-Original Grad: 0.031, -lr * Pred Grad: -0.030, New P: -0.238
iter 14 loss: 0.383
Actual params: [ 0.4286, -0.2381]
-Original Grad: 0.044, -lr * Pred Grad: -0.016, New P: 0.412
-Original Grad: -0.052, -lr * Pred Grad: -0.042, New P: -0.280
iter 15 loss: 0.385
Actual params: [ 0.4122, -0.2798]
-Original Grad: -0.002, -lr * Pred Grad: -0.002, New P: 0.410
-Original Grad: -0.013, -lr * Pred Grad: -0.041, New P: -0.321
iter 16 loss: 0.387
Actual params: [ 0.4098, -0.3209]
-Original Grad: 0.017, -lr * Pred Grad: 0.002, New P: 0.412
-Original Grad: -0.051, -lr * Pred Grad: -0.045, New P: -0.366
iter 17 loss: 0.391
Actual params: [ 0.4116, -0.3659]
-Original Grad: -0.025, -lr * Pred Grad: -0.015, New P: 0.396
-Original Grad: 0.024, -lr * Pred Grad: -0.029, New P: -0.395
iter 18 loss: 0.392
Actual params: [ 0.3964, -0.3949]
-Original Grad: -0.013, -lr * Pred Grad: -0.024, New P: 0.372
-Original Grad: -0.036, -lr * Pred Grad: -0.028, New P: -0.423
iter 19 loss: 0.392
Actual params: [ 0.3722, -0.4231]
-Original Grad: -0.015, -lr * Pred Grad: -0.025, New P: 0.348
-Original Grad: 0.053, -lr * Pred Grad: -0.010, New P: -0.433
iter 20 loss: 0.389
Actual params: [ 0.3476, -0.4335]
-Original Grad: -0.082, -lr * Pred Grad: -0.037, New P: 0.310
-Original Grad: 0.043, -lr * Pred Grad: 0.014, New P: -0.420
