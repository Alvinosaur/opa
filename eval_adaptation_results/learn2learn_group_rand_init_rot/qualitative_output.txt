Target params: [1.3344, 1.5708]
Actual params: [1.084 , 0.5507]
-Original Grad: 0.011, -lr * Pred Grad: 0.754, New P: 1.838
-Original Grad: 0.578, -lr * Pred Grad: 2.272, New P: 2.823
iter 0 loss: 0.176
Actual params: [1.8376, 2.8227]
-Original Grad: 0.100, -lr * Pred Grad: 1.270, New P: 3.108
-Original Grad: -0.200, -lr * Pred Grad: -0.697, New P: 2.125
iter 1 loss: 0.059
Actual params: [3.1076, 2.1253]
-Original Grad: -0.029, -lr * Pred Grad: -0.061, New P: 3.047
-Original Grad: 0.064, -lr * Pred Grad: 0.123, New P: 2.248
iter 2 loss: 0.017
Actual params: [3.047 , 2.2482]
-Original Grad: -0.025, -lr * Pred Grad: 0.062, New P: 3.109
-Original Grad: 0.045, -lr * Pred Grad: 0.094, New P: 2.342
iter 3 loss: 0.012
Actual params: [3.1087, 2.3422]
-Original Grad: -0.024, -lr * Pred Grad: -0.056, New P: 3.052
-Original Grad: 0.037, -lr * Pred Grad: 0.108, New P: 2.450
iter 4 loss: 0.011
Actual params: [3.0524, 2.4501]
-Original Grad: -0.018, -lr * Pred Grad: -0.074, New P: 2.978
-Original Grad: 0.021, -lr * Pred Grad: 0.068, New P: 2.518
iter 5 loss: 0.009
Actual params: [2.978 , 2.5178]
-Original Grad: -0.007, -lr * Pred Grad: -0.068, New P: 2.910
-Original Grad: 0.006, -lr * Pred Grad: 0.025, New P: 2.543
iter 6 loss: 0.007
Actual params: [2.9103, 2.5433]
-Original Grad: 0.003, -lr * Pred Grad: -0.043, New P: 2.867
-Original Grad: -0.005, -lr * Pred Grad: -0.015, New P: 2.528
iter 7 loss: 0.007
Actual params: [2.8674, 2.5281]
-Original Grad: 0.008, -lr * Pred Grad: -0.019, New P: 2.849
-Original Grad: -0.011, -lr * Pred Grad: -0.043, New P: 2.485
iter 8 loss: 0.007
Actual params: [2.8488, 2.4847]
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: 2.842
-Original Grad: -0.010, -lr * Pred Grad: -0.055, New P: 2.430
iter 9 loss: 0.007
Actual params: [2.8421, 2.4299]
-Original Grad: 0.003, -lr * Pred Grad: -0.010, New P: 2.832
-Original Grad: -0.005, -lr * Pred Grad: -0.049, New P: 2.381
iter 10 loss: 0.007
Actual params: [2.8323, 2.3806]
-Original Grad: -0.000, -lr * Pred Grad: -0.020, New P: 2.813
-Original Grad: 0.000, -lr * Pred Grad: -0.036, New P: 2.344
iter 11 loss: 0.007
Actual params: [2.8125, 2.3442]
-Original Grad: -0.001, -lr * Pred Grad: -0.027, New P: 2.786
-Original Grad: 0.002, -lr * Pred Grad: -0.026, New P: 2.318
iter 12 loss: 0.007
Actual params: [2.786 , 2.3178]
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 2.758
-Original Grad: 0.002, -lr * Pred Grad: -0.024, New P: 2.294
iter 13 loss: 0.007
Actual params: [2.7577, 2.2939]
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 2.728
-Original Grad: 0.002, -lr * Pred Grad: -0.024, New P: 2.270
iter 14 loss: 0.007
Actual params: [2.7275, 2.2701]
-Original Grad: 0.001, -lr * Pred Grad: -0.031, New P: 2.696
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 2.246
iter 15 loss: 0.007
Actual params: [2.6964, 2.2456]
-Original Grad: 0.000, -lr * Pred Grad: -0.034, New P: 2.663
-Original Grad: 0.002, -lr * Pred Grad: -0.023, New P: 2.222
iter 16 loss: 0.007
Actual params: [2.6629, 2.2222]
-Original Grad: -0.000, -lr * Pred Grad: -0.037, New P: 2.626
-Original Grad: 0.003, -lr * Pred Grad: -0.021, New P: 2.201
iter 17 loss: 0.007
Actual params: [2.6259, 2.2011]
-Original Grad: -0.001, -lr * Pred Grad: -0.041, New P: 2.585
-Original Grad: 0.004, -lr * Pred Grad: -0.018, New P: 2.183
iter 18 loss: 0.007
Actual params: [2.5846, 2.1828]
-Original Grad: -0.001, -lr * Pred Grad: -0.044, New P: 2.540
-Original Grad: 0.004, -lr * Pred Grad: -0.017, New P: 2.166
iter 19 loss: 0.007
Actual params: [2.5402, 2.1657]
-Original Grad: -0.001, -lr * Pred Grad: -0.047, New P: 2.493
-Original Grad: 0.004, -lr * Pred Grad: -0.016, New P: 2.150
iter 20 loss: 0.007
Actual params: [2.4931, 2.1498]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: 2.445
-Original Grad: 0.004, -lr * Pred Grad: -0.015, New P: 2.135
iter 21 loss: 0.007
Actual params: [2.4449, 2.135 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.049, New P: 2.396
-Original Grad: 0.004, -lr * Pred Grad: -0.014, New P: 2.121
iter 22 loss: 0.007
Actual params: [2.3961, 2.121 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.049, New P: 2.347
-Original Grad: 0.005, -lr * Pred Grad: -0.013, New P: 2.108
iter 23 loss: 0.007
Actual params: [2.3466, 2.1082]
-Original Grad: -0.001, -lr * Pred Grad: -0.050, New P: 2.297
-Original Grad: 0.005, -lr * Pred Grad: -0.012, New P: 2.096
iter 24 loss: 0.007
Actual params: [2.2966, 2.0965]
-Original Grad: -0.001, -lr * Pred Grad: -0.051, New P: 2.246
-Original Grad: 0.005, -lr * Pred Grad: -0.011, New P: 2.085
iter 25 loss: 0.007
Actual params: [2.246 , 2.0854]
-Original Grad: -0.001, -lr * Pred Grad: -0.051, New P: 2.195
-Original Grad: 0.005, -lr * Pred Grad: -0.011, New P: 2.074
iter 26 loss: 0.007
Actual params: [2.1951, 2.0744]
-Original Grad: -0.001, -lr * Pred Grad: -0.051, New P: 2.145
-Original Grad: 0.005, -lr * Pred Grad: -0.012, New P: 2.063
iter 27 loss: 0.007
Actual params: [2.1446, 2.0628]
-Original Grad: -0.001, -lr * Pred Grad: -0.050, New P: 2.094
-Original Grad: 0.004, -lr * Pred Grad: -0.013, New P: 2.050
iter 28 loss: 0.007
Actual params: [2.0941, 2.0499]
-Original Grad: -0.001, -lr * Pred Grad: -0.052, New P: 2.042
-Original Grad: 0.004, -lr * Pred Grad: -0.013, New P: 2.037
iter 29 loss: 0.007
Actual params: [2.0421, 2.0367]
-Original Grad: -0.002, -lr * Pred Grad: -0.055, New P: 1.987
-Original Grad: 0.005, -lr * Pred Grad: -0.013, New P: 2.024
iter 30 loss: 0.007
Actual params: [1.9872, 2.0238]
Target params: [1.3344, 1.5708]
Actual params: [-1.0585,  1.4018]
-Original Grad: 0.039, -lr * Pred Grad: 1.029, New P: -0.030
-Original Grad: 0.051, -lr * Pred Grad: 0.872, New P: 2.274
iter 0 loss: 0.436
Actual params: [-0.0296,  2.2741]
-Original Grad: 0.566, -lr * Pred Grad: 2.507, New P: 2.477
-Original Grad: -0.328, -lr * Pred Grad: -0.753, New P: 1.522
iter 1 loss: 0.193
Actual params: [2.4775, 1.5215]
-Original Grad: -0.067, -lr * Pred Grad: -1.003, New P: 1.475
-Original Grad: 0.334, -lr * Pred Grad: 0.492, New P: 2.013
iter 2 loss: 0.102
Actual params: [1.4749, 2.0134]
-Original Grad: 0.070, -lr * Pred Grad: 0.413, New P: 1.888
-Original Grad: -0.196, -lr * Pred Grad: -0.344, New P: 1.669
iter 3 loss: 0.026
Actual params: [1.8883, 1.669 ]
-Original Grad: -0.084, -lr * Pred Grad: -0.168, New P: 1.721
-Original Grad: 0.298, -lr * Pred Grad: 0.530, New P: 2.199
iter 4 loss: 0.024
Actual params: [1.7207, 2.1989]
-Original Grad: 0.076, -lr * Pred Grad: 0.203, New P: 1.923
-Original Grad: -0.264, -lr * Pred Grad: -0.378, New P: 1.821
iter 5 loss: 0.030
Actual params: [1.9234, 1.8214]
-Original Grad: -0.043, -lr * Pred Grad: -0.037, New P: 1.887
-Original Grad: 0.117, -lr * Pred Grad: 0.031, New P: 1.853
iter 6 loss: 0.010
Actual params: [1.8868, 1.8526]
-Original Grad: -0.023, -lr * Pred Grad: -0.049, New P: 1.838
-Original Grad: 0.063, -lr * Pred Grad: 0.130, New P: 1.983
iter 7 loss: 0.008
Actual params: [1.8381, 1.9827]
-Original Grad: 0.021, -lr * Pred Grad: 0.023, New P: 1.861
-Original Grad: -0.044, -lr * Pred Grad: -0.073, New P: 1.910
iter 8 loss: 0.008
Actual params: [1.8606, 1.9096]
-Original Grad: 0.001, -lr * Pred Grad: 0.001, New P: 1.862
-Original Grad: -0.002, -lr * Pred Grad: -0.040, New P: 1.870
iter 9 loss: 0.007
Actual params: [1.8619, 1.8697]
-Original Grad: -0.010, -lr * Pred Grad: -0.034, New P: 1.828
-Original Grad: 0.031, -lr * Pred Grad: 0.035, New P: 1.904
iter 10 loss: 0.007
Actual params: [1.8281, 1.9044]
-Original Grad: 0.007, -lr * Pred Grad: -0.012, New P: 1.816
-Original Grad: -0.014, -lr * Pred Grad: -0.039, New P: 1.866
iter 11 loss: 0.007
Actual params: [1.8164, 1.8659]
-Original Grad: -0.001, -lr * Pred Grad: -0.027, New P: 1.790
-Original Grad: 0.009, -lr * Pred Grad: -0.008, New P: 1.858
iter 12 loss: 0.007
Actual params: [1.7897, 1.8583]
-Original Grad: 0.002, -lr * Pred Grad: -0.024, New P: 1.766
-Original Grad: 0.001, -lr * Pred Grad: -0.021, New P: 1.837
iter 13 loss: 0.007
Actual params: [1.766 , 1.8371]
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 1.738
-Original Grad: 0.007, -lr * Pred Grad: -0.009, New P: 1.828
iter 14 loss: 0.007
Actual params: [1.7381, 1.828 ]
-Original Grad: 0.002, -lr * Pred Grad: -0.026, New P: 1.712
-Original Grad: 0.002, -lr * Pred Grad: -0.019, New P: 1.809
iter 15 loss: 0.007
Actual params: [1.7122, 1.8086]
-Original Grad: 0.002, -lr * Pred Grad: -0.025, New P: 1.688
-Original Grad: 0.006, -lr * Pred Grad: -0.010, New P: 1.798
iter 16 loss: 0.007
Actual params: [1.6876, 1.7983]
-Original Grad: 0.003, -lr * Pred Grad: -0.022, New P: 1.666
-Original Grad: 0.005, -lr * Pred Grad: -0.011, New P: 1.787
iter 17 loss: 0.007
Actual params: [1.6659, 1.7869]
-Original Grad: 0.004, -lr * Pred Grad: -0.017, New P: 1.649
-Original Grad: 0.005, -lr * Pred Grad: -0.010, New P: 1.777
iter 18 loss: 0.007
Actual params: [1.6486, 1.7769]
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: 1.631
-Original Grad: 0.006, -lr * Pred Grad: -0.008, New P: 1.769
iter 19 loss: 0.007
Actual params: [1.6314, 1.7691]
-Original Grad: 0.004, -lr * Pred Grad: -0.014, New P: 1.617
-Original Grad: 0.006, -lr * Pred Grad: -0.006, New P: 1.763
iter 20 loss: 0.007
Actual params: [1.6171, 1.7631]
-Original Grad: 0.003, -lr * Pred Grad: -0.015, New P: 1.602
-Original Grad: 0.006, -lr * Pred Grad: -0.006, New P: 1.757
iter 21 loss: 0.007
Actual params: [1.6017, 1.7566]
-Original Grad: 0.003, -lr * Pred Grad: -0.014, New P: 1.588
-Original Grad: 0.005, -lr * Pred Grad: -0.009, New P: 1.748
iter 22 loss: 0.007
Actual params: [1.5875, 1.7476]
-Original Grad: 0.003, -lr * Pred Grad: -0.015, New P: 1.573
-Original Grad: 0.008, -lr * Pred Grad: -0.003, New P: 1.744
iter 23 loss: 0.008
Actual params: [1.5729, 1.7441]
-Original Grad: 0.003, -lr * Pred Grad: -0.014, New P: 1.559
-Original Grad: 0.005, -lr * Pred Grad: -0.009, New P: 1.735
iter 24 loss: 0.008
Actual params: [1.5593, 1.735 ]
-Original Grad: 0.003, -lr * Pred Grad: -0.016, New P: 1.544
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 1.728
iter 25 loss: 0.008
Actual params: [1.5437, 1.7283]
-Original Grad: 0.002, -lr * Pred Grad: -0.018, New P: 1.526
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 1.721
iter 26 loss: 0.008
Actual params: [1.5259, 1.7213]
-Original Grad: 0.002, -lr * Pred Grad: -0.018, New P: 1.507
-Original Grad: 0.005, -lr * Pred Grad: -0.010, New P: 1.711
iter 27 loss: 0.008
Actual params: [1.5074, 1.711 ]
-Original Grad: 0.002, -lr * Pred Grad: -0.021, New P: 1.486
-Original Grad: 0.008, -lr * Pred Grad: -0.002, New P: 1.709
iter 28 loss: 0.008
Actual params: [1.4861, 1.7087]
-Original Grad: 0.003, -lr * Pred Grad: -0.018, New P: 1.468
-Original Grad: -0.004, -lr * Pred Grad: -0.030, New P: 1.679
iter 29 loss: 0.008
Actual params: [1.4678, 1.6787]
-Original Grad: -0.004, -lr * Pred Grad: -0.037, New P: 1.431
-Original Grad: 0.020, -lr * Pred Grad: 0.020, New P: 1.699
iter 30 loss: 0.008
Actual params: [1.4306, 1.6991]
Target params: [1.3344, 1.5708]
Actual params: [1.5477, 0.5327]
-Original Grad: -0.149, -lr * Pred Grad: -0.948, New P: 0.600
-Original Grad: 0.797, -lr * Pred Grad: 2.460, New P: 2.993
iter 0 loss: 0.487
Actual params: [0.6002, 2.9928]
-Original Grad: -0.078, -lr * Pred Grad: -0.776, New P: -0.176
-Original Grad: -1.032, -lr * Pred Grad: -0.941, New P: 2.052
iter 1 loss: 0.824
Actual params: [-0.1762,  2.0518]
-Original Grad: 0.088, -lr * Pred Grad: 0.070, New P: -0.106
-Original Grad: -0.133, -lr * Pred Grad: -0.207, New P: 1.845
iter 2 loss: 0.700
Actual params: [-0.1057,  1.8447]
-Original Grad: 0.132, -lr * Pred Grad: 0.402, New P: 0.296
-Original Grad: -0.206, -lr * Pred Grad: -0.526, New P: 1.319
iter 3 loss: 0.674
Actual params: [0.2958, 1.3191]
-Original Grad: 0.574, -lr * Pred Grad: 1.497, New P: 1.793
-Original Grad: -0.076, -lr * Pred Grad: -0.401, New P: 0.918
iter 4 loss: 0.476
Actual params: [1.7927, 0.9179]
-Original Grad: -0.214, -lr * Pred Grad: -0.744, New P: 1.049
-Original Grad: 0.695, -lr * Pred Grad: 1.320, New P: 2.238
iter 5 loss: 0.387
Actual params: [1.0491, 2.2377]
-Original Grad: 0.429, -lr * Pred Grad: 0.700, New P: 1.749
-Original Grad: -0.452, -lr * Pred Grad: -0.684, New P: 1.554
iter 6 loss: 0.160
Actual params: [1.7493, 1.5539]
-Original Grad: -0.289, -lr * Pred Grad: -0.474, New P: 1.275
-Original Grad: 0.246, -lr * Pred Grad: 0.148, New P: 1.702
iter 7 loss: 0.243
Actual params: [1.2754, 1.7021]
-Original Grad: -0.086, -lr * Pred Grad: -0.368, New P: 0.908
-Original Grad: 0.413, -lr * Pred Grad: 1.356, New P: 3.058
iter 8 loss: 0.081
Actual params: [0.9078, 3.0576]
-Original Grad: -0.189, -lr * Pred Grad: -0.633, New P: 0.275
-Original Grad: -1.017, -lr * Pred Grad: -0.576, New P: 2.481
iter 9 loss: 0.902
Actual params: [0.2746, 2.4814]
-Original Grad: 1.961, -lr * Pred Grad: 1.037, New P: 1.312
-Original Grad: -0.687, -lr * Pred Grad: -0.437, New P: 2.044
iter 10 loss: 0.647
Actual params: [1.3116, 2.0444]
-Original Grad: 0.222, -lr * Pred Grad: 0.022, New P: 1.334
-Original Grad: -0.124, -lr * Pred Grad: -0.573, New P: 1.471
iter 11 loss: 0.058
Actual params: [1.3335, 1.4712]
-Original Grad: -0.321, -lr * Pred Grad: -0.263, New P: 1.071
-Original Grad: 0.588, -lr * Pred Grad: 0.457, New P: 1.928
iter 12 loss: 0.163
Actual params: [1.0707, 1.9278]
-Original Grad: 0.295, -lr * Pred Grad: 0.329, New P: 1.400
-Original Grad: -0.136, -lr * Pred Grad: -0.337, New P: 1.590
iter 13 loss: 0.103
Actual params: [1.4001, 1.5905]
-Original Grad: -0.302, -lr * Pred Grad: -0.388, New P: 1.012
-Original Grad: 0.651, -lr * Pred Grad: 1.349, New P: 2.939
iter 14 loss: 0.145
Actual params: [1.0118, 2.9395]
-Original Grad: -0.120, -lr * Pred Grad: -0.485, New P: 0.527
-Original Grad: -1.772, -lr * Pred Grad: -0.542, New P: 2.397
iter 15 loss: 0.826
Actual params: [0.5268, 2.397 ]
-Original Grad: 0.381, -lr * Pred Grad: 0.318, New P: 0.844
-Original Grad: -0.783, -lr * Pred Grad: -0.471, New P: 1.926
iter 16 loss: 0.474
Actual params: [0.8445, 1.9261]
-Original Grad: 0.511, -lr * Pred Grad: 0.782, New P: 1.627
-Original Grad: -0.269, -lr * Pred Grad: -0.596, New P: 1.330
iter 17 loss: 0.203
Actual params: [1.6265, 1.3304]
-Original Grad: -0.195, -lr * Pred Grad: -0.241, New P: 1.385
-Original Grad: 0.379, -lr * Pred Grad: -0.189, New P: 1.141
iter 18 loss: 0.248
Actual params: [1.3852, 1.1412]
-Original Grad: -0.232, -lr * Pred Grad: -0.495, New P: 0.890
-Original Grad: 0.414, -lr * Pred Grad: 0.742, New P: 1.883
iter 19 loss: 0.242
Actual params: [0.8898, 1.8831]
-Original Grad: 0.538, -lr * Pred Grad: 0.569, New P: 1.459
-Original Grad: -0.196, -lr * Pred Grad: -0.425, New P: 1.458
iter 20 loss: 0.173
Actual params: [1.4586, 1.458 ]
-Original Grad: -0.173, -lr * Pred Grad: -0.263, New P: 1.195
-Original Grad: 0.403, -lr * Pred Grad: 0.685, New P: 2.143
iter 21 loss: 0.193
Actual params: [1.1952, 2.1427]
-Original Grad: 0.239, -lr * Pred Grad: 0.339, New P: 1.534
-Original Grad: -0.208, -lr * Pred Grad: -0.357, New P: 1.786
iter 22 loss: 0.095
Actual params: [1.5342, 1.7856]
-Original Grad: -0.359, -lr * Pred Grad: -0.515, New P: 1.019
-Original Grad: 0.537, -lr * Pred Grad: 1.224, New P: 3.010
iter 23 loss: 0.128
Actual params: [1.0189, 3.0097]
-Original Grad: -0.109, -lr * Pred Grad: -0.581, New P: 0.438
-Original Grad: -1.386, -lr * Pred Grad: -0.508, New P: 2.502
iter 24 loss: 0.882
Actual params: [0.4376, 2.5021]
-Original Grad: 0.574, -lr * Pred Grad: 0.480, New P: 0.917
-Original Grad: -0.781, -lr * Pred Grad: -0.470, New P: 2.032
iter 25 loss: 0.546
Actual params: [0.9172, 2.0319]
-Original Grad: 0.611, -lr * Pred Grad: 0.882, New P: 1.799
-Original Grad: -0.427, -lr * Pred Grad: -0.598, New P: 1.434
iter 26 loss: 0.178
Actual params: [1.7988, 1.4337]
-Original Grad: -0.259, -lr * Pred Grad: -0.358, New P: 1.441
-Original Grad: 0.272, -lr * Pred Grad: -0.367, New P: 1.066
iter 27 loss: 0.272
Actual params: [1.4409, 1.0663]
-Original Grad: -0.232, -lr * Pred Grad: -0.496, New P: 0.944
-Original Grad: 0.502, -lr * Pred Grad: 0.768, New P: 1.835
iter 28 loss: 0.272
Actual params: [0.9445, 1.8345]
-Original Grad: 0.418, -lr * Pred Grad: 0.412, New P: 1.356
-Original Grad: -0.118, -lr * Pred Grad: -0.366, New P: 1.469
iter 29 loss: 0.141
Actual params: [1.3564, 1.4687]
-Original Grad: -0.259, -lr * Pred Grad: -0.369, New P: 0.987
-Original Grad: 0.537, -lr * Pred Grad: 1.195, New P: 2.663
iter 30 loss: 0.171
Actual params: [0.9875, 2.6632]
Target params: [1.3344, 1.5708]
Actual params: [0.0029, 0.9353]
-Original Grad: 0.212, -lr * Pred Grad: 2.012, New P: 2.015
-Original Grad: 0.193, -lr * Pred Grad: 1.459, New P: 2.395
iter 0 loss: 0.137
Actual params: [2.0151, 2.3946]
-Original Grad: -0.022, -lr * Pred Grad: -0.327, New P: 1.688
-Original Grad: -0.469, -lr * Pred Grad: -0.918, New P: 1.476
iter 1 loss: 0.177
Actual params: [1.688 , 1.4764]
-Original Grad: -0.041, -lr * Pred Grad: 0.113, New P: 1.801
-Original Grad: -0.077, -lr * Pred Grad: -0.163, New P: 1.314
iter 2 loss: 0.029
Actual params: [1.8015, 1.3138]
-Original Grad: -0.039, -lr * Pred Grad: -0.102, New P: 1.699
-Original Grad: -0.061, -lr * Pred Grad: -0.385, New P: 0.929
iter 3 loss: 0.028
Actual params: [1.6991, 0.929 ]
-Original Grad: 0.068, -lr * Pred Grad: 0.251, New P: 1.950
-Original Grad: 0.188, -lr * Pred Grad: 0.204, New P: 1.133
iter 4 loss: 0.033
Actual params: [1.9505, 1.1329]
-Original Grad: -0.024, -lr * Pred Grad: -0.005, New P: 1.946
-Original Grad: -0.063, -lr * Pred Grad: -0.147, New P: 0.985
iter 5 loss: 0.027
Actual params: [1.9457, 0.9854]
-Original Grad: -0.012, -lr * Pred Grad: -0.010, New P: 1.935
-Original Grad: 0.001, -lr * Pred Grad: -0.061, New P: 0.924
iter 6 loss: 0.024
Actual params: [1.9352, 0.9243]
-Original Grad: 0.003, -lr * Pred Grad: 0.000, New P: 1.936
-Original Grad: 0.042, -lr * Pred Grad: 0.033, New P: 0.958
iter 7 loss: 0.025
Actual params: [1.9356, 0.9577]
-Original Grad: -0.008, -lr * Pred Grad: -0.033, New P: 1.903
-Original Grad: 0.016, -lr * Pred Grad: 0.032, New P: 0.989
iter 8 loss: 0.024
Actual params: [1.903 , 0.9894]
-Original Grad: -0.008, -lr * Pred Grad: -0.051, New P: 1.852
-Original Grad: 0.012, -lr * Pred Grad: 0.024, New P: 1.013
iter 9 loss: 0.024
Actual params: [1.8519, 1.0131]
-Original Grad: -0.009, -lr * Pred Grad: -0.067, New P: 1.785
-Original Grad: 0.017, -lr * Pred Grad: 0.032, New P: 1.045
iter 10 loss: 0.023
Actual params: [1.7845, 1.0454]
-Original Grad: 0.008, -lr * Pred Grad: -0.032, New P: 1.752
-Original Grad: 0.042, -lr * Pred Grad: 0.107, New P: 1.152
iter 11 loss: 0.023
Actual params: [1.7524, 1.1522]
-Original Grad: -0.026, -lr * Pred Grad: -0.113, New P: 1.639
-Original Grad: -0.024, -lr * Pred Grad: -0.048, New P: 1.104
iter 12 loss: 0.022
Actual params: [1.6392, 1.1041]
-Original Grad: 0.025, -lr * Pred Grad: -0.010, New P: 1.629
-Original Grad: 0.101, -lr * Pred Grad: 0.257, New P: 1.362
iter 13 loss: 0.023
Actual params: [1.629 , 1.3616]
-Original Grad: -0.045, -lr * Pred Grad: -0.160, New P: 1.469
-Original Grad: -0.079, -lr * Pred Grad: -0.161, New P: 1.200
iter 14 loss: 0.022
Actual params: [1.4691, 1.2005]
-Original Grad: 0.038, -lr * Pred Grad: -0.003, New P: 1.466
-Original Grad: 0.194, -lr * Pred Grad: 0.492, New P: 1.692
iter 15 loss: 0.022
Actual params: [1.4658, 1.6922]
-Original Grad: -0.042, -lr * Pred Grad: -0.152, New P: 1.313
-Original Grad: -0.168, -lr * Pred Grad: -0.325, New P: 1.367
iter 16 loss: 0.033
Actual params: [1.3133, 1.3673]
-Original Grad: -0.015, -lr * Pred Grad: -0.150, New P: 1.163
-Original Grad: -0.009, -lr * Pred Grad: -0.099, New P: 1.268
iter 17 loss: 0.014
Actual params: [1.163 , 1.2681]
-Original Grad: 0.008, -lr * Pred Grad: -0.092, New P: 1.071
-Original Grad: 0.209, -lr * Pred Grad: 0.493, New P: 1.761
iter 18 loss: 0.020
Actual params: [1.0708, 1.761 ]
-Original Grad: 0.002, -lr * Pred Grad: -0.081, New P: 0.990
-Original Grad: -0.191, -lr * Pred Grad: -0.355, New P: 1.406
iter 19 loss: 0.026
Actual params: [0.9902, 1.4061]
-Original Grad: 0.039, -lr * Pred Grad: 0.038, New P: 1.028
-Original Grad: 0.063, -lr * Pred Grad: 0.009, New P: 1.415
iter 20 loss: 0.015
Actual params: [1.0277, 1.4149]
-Original Grad: 0.025, -lr * Pred Grad: 0.065, New P: 1.093
-Original Grad: 0.044, -lr * Pred Grad: 0.065, New P: 1.480
iter 21 loss: 0.014
Actual params: [1.0927, 1.4797]
-Original Grad: 0.008, -lr * Pred Grad: 0.048, New P: 1.140
-Original Grad: -0.014, -lr * Pred Grad: -0.021, New P: 1.459
iter 22 loss: 0.012
Actual params: [1.1405, 1.4591]
-Original Grad: -0.006, -lr * Pred Grad: 0.003, New P: 1.143
-Original Grad: -0.012, -lr * Pred Grad: -0.050, New P: 1.409
iter 23 loss: 0.012
Actual params: [1.1431, 1.4093]
-Original Grad: -0.001, -lr * Pred Grad: -0.008, New P: 1.135
-Original Grad: 0.021, -lr * Pred Grad: 0.010, New P: 1.419
iter 24 loss: 0.012
Actual params: [1.1346, 1.4191]
-Original Grad: 0.001, -lr * Pred Grad: -0.013, New P: 1.122
-Original Grad: 0.014, -lr * Pred Grad: 0.019, New P: 1.439
iter 25 loss: 0.012
Actual params: [1.1218, 1.4386]
-Original Grad: 0.000, -lr * Pred Grad: -0.020, New P: 1.101
-Original Grad: 0.002, -lr * Pred Grad: -0.007, New P: 1.432
iter 26 loss: 0.012
Actual params: [1.1014, 1.4318]
-Original Grad: 0.009, -lr * Pred Grad: -0.001, New P: 1.101
-Original Grad: 0.011, -lr * Pred Grad: 0.008, New P: 1.440
iter 27 loss: 0.012
Actual params: [1.1006, 1.4401]
-Original Grad: 0.008, -lr * Pred Grad: 0.007, New P: 1.107
-Original Grad: 0.008, -lr * Pred Grad: 0.004, New P: 1.444
iter 28 loss: 0.012
Actual params: [1.1074, 1.4438]
-Original Grad: 0.005, -lr * Pred Grad: 0.004, New P: 1.112
-Original Grad: 0.003, -lr * Pred Grad: -0.010, New P: 1.434
iter 29 loss: 0.012
Actual params: [1.1115, 1.4339]
-Original Grad: 0.006, -lr * Pred Grad: 0.007, New P: 1.119
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: 1.435
iter 30 loss: 0.012
Actual params: [1.1189, 1.4346]
Target params: [1.3344, 1.5708]
Actual params: [-0.6756, -1.5044]
-Original Grad: -0.021, -lr * Pred Grad: 0.423, New P: -0.253
-Original Grad: 0.010, -lr * Pred Grad: 0.682, New P: -0.822
iter 0 loss: 0.469
Actual params: [-0.253 , -0.8222]
-Original Grad: -0.059, -lr * Pred Grad: -0.290, New P: -0.544
-Original Grad: 0.052, -lr * Pred Grad: 0.417, New P: -0.406
iter 1 loss: 0.473
Actual params: [-0.5435, -0.4056]
-Original Grad: -0.001, -lr * Pred Grad: 0.021, New P: -0.522
-Original Grad: 0.001, -lr * Pred Grad: -0.084, New P: -0.489
iter 2 loss: 0.466
Actual params: [-0.5222, -0.4893]
-Original Grad: -0.002, -lr * Pred Grad: -0.031, New P: -0.554
-Original Grad: 0.002, -lr * Pred Grad: -0.010, New P: -0.499
iter 3 loss: 0.466
Actual params: [-0.5536, -0.4992]
-Original Grad: -0.002, -lr * Pred Grad: -0.029, New P: -0.583
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: -0.536
iter 4 loss: 0.466
Actual params: [-0.5829, -0.5365]
-Original Grad: -0.001, -lr * Pred Grad: -0.033, New P: -0.616
-Original Grad: 0.001, -lr * Pred Grad: -0.035, New P: -0.572
iter 5 loss: 0.466
Actual params: [-0.6159, -0.5715]
-Original Grad: -0.001, -lr * Pred Grad: -0.035, New P: -0.651
-Original Grad: 0.001, -lr * Pred Grad: -0.033, New P: -0.605
iter 6 loss: 0.466
Actual params: [-0.6511, -0.6049]
-Original Grad: -0.001, -lr * Pred Grad: -0.037, New P: -0.688
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: -0.634
iter 7 loss: 0.466
Actual params: [-0.6882, -0.6343]
-Original Grad: -0.001, -lr * Pred Grad: -0.039, New P: -0.727
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: -0.662
iter 8 loss: 0.466
Actual params: [-0.7268, -0.662 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.040, New P: -0.767
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: -0.689
iter 9 loss: 0.466
Actual params: [-0.7666, -0.689 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.041, New P: -0.807
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: -0.716
iter 10 loss: 0.466
Actual params: [-0.8073, -0.7162]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -0.849
-Original Grad: 0.000, -lr * Pred Grad: -0.028, New P: -0.744
iter 11 loss: 0.466
Actual params: [-0.8486, -0.744 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -0.890
-Original Grad: 0.000, -lr * Pred Grad: -0.028, New P: -0.772
iter 12 loss: 0.466
Actual params: [-0.8905, -0.7725]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -0.933
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: -0.802
iter 13 loss: 0.466
Actual params: [-0.9328, -0.8016]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.975
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: -0.831
iter 14 loss: 0.466
Actual params: [-0.9754, -0.8314]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.018
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: -0.862
iter 15 loss: 0.466
Actual params: [-1.0183, -0.8617]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.061
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: -0.892
iter 16 loss: 0.466
Actual params: [-1.0613, -0.8925]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.105
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: -0.924
iter 17 loss: 0.466
Actual params: [-1.1046, -0.9237]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.148
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: -0.955
iter 18 loss: 0.466
Actual params: [-1.1479, -0.9551]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.191
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -0.987
iter 19 loss: 0.466
Actual params: [-1.1913, -0.9869]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.235
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -1.019
iter 20 loss: 0.466
Actual params: [-1.2348, -1.0188]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.278
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -1.051
iter 21 loss: 0.466
Actual params: [-1.2783, -1.0509]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.322
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -1.083
iter 22 loss: 0.466
Actual params: [-1.3219, -1.083 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.365
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -1.115
iter 23 loss: 0.466
Actual params: [-1.3655, -1.1153]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.409
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -1.148
iter 24 loss: 0.466
Actual params: [-1.409 , -1.1477]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.453
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -1.180
iter 25 loss: 0.466
Actual params: [-1.4526, -1.1801]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.496
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -1.213
iter 26 loss: 0.466
Actual params: [-1.4962, -1.2125]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.540
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -1.245
iter 27 loss: 0.466
Actual params: [-1.5397, -1.245 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.583
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -1.277
iter 28 loss: 0.466
Actual params: [-1.5833, -1.2775]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.627
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -1.310
iter 29 loss: 0.466
Actual params: [-1.6268, -1.31  ]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.670
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -1.343
iter 30 loss: 0.466
Actual params: [-1.6704, -1.3425]
Target params: [1.3344, 1.5708]
Actual params: [-0.6634, -0.2295]
-Original Grad: 0.000, -lr * Pred Grad: 0.647, New P: -0.017
-Original Grad: 0.000, -lr * Pred Grad: 0.634, New P: 0.405
iter 0 loss: 0.228
Actual params: [-0.0168,  0.4045]
-Original Grad: 0.235, -lr * Pred Grad: 1.944, New P: 1.927
-Original Grad: 0.160, -lr * Pred Grad: 1.051, New P: 1.456
iter 1 loss: 0.199
Actual params: [1.9268, 1.456 ]
-Original Grad: -0.045, -lr * Pred Grad: -0.462, New P: 1.465
-Original Grad: -0.093, -lr * Pred Grad: -0.539, New P: 0.917
iter 2 loss: 0.039
Actual params: [1.4646, 0.9172]
-Original Grad: 0.130, -lr * Pred Grad: 0.862, New P: 2.327
-Original Grad: 0.214, -lr * Pred Grad: 0.503, New P: 1.420
iter 3 loss: 0.060
Actual params: [2.3271, 1.4204]
-Original Grad: -0.027, -lr * Pred Grad: -0.053, New P: 2.274
-Original Grad: -0.128, -lr * Pred Grad: -0.269, New P: 1.151
iter 4 loss: 0.051
Actual params: [2.2738, 1.1514]
-Original Grad: -0.021, -lr * Pred Grad: 0.079, New P: 2.352
-Original Grad: -0.089, -lr * Pred Grad: -0.182, New P: 0.970
iter 5 loss: 0.035
Actual params: [2.3523, 0.9699]
-Original Grad: -0.017, -lr * Pred Grad: 0.002, New P: 2.354
-Original Grad: -0.042, -lr * Pred Grad: -0.228, New P: 0.742
iter 6 loss: 0.030
Actual params: [2.3538, 0.7422]
-Original Grad: -0.006, -lr * Pred Grad: -0.002, New P: 2.352
-Original Grad: 0.033, -lr * Pred Grad: -0.052, New P: 0.690
iter 7 loss: 0.029
Actual params: [2.352 , 0.6898]
-Original Grad: -0.002, -lr * Pred Grad: -0.009, New P: 2.343
-Original Grad: 0.060, -lr * Pred Grad: 0.106, New P: 0.795
iter 8 loss: 0.031
Actual params: [2.3433, 0.7955]
-Original Grad: -0.008, -lr * Pred Grad: -0.035, New P: 2.309
-Original Grad: 0.016, -lr * Pred Grad: 0.057, New P: 0.853
iter 9 loss: 0.029
Actual params: [2.3087, 0.8528]
-Original Grad: -0.010, -lr * Pred Grad: -0.057, New P: 2.252
-Original Grad: 0.001, -lr * Pred Grad: 0.007, New P: 0.860
iter 10 loss: 0.028
Actual params: [2.2521, 0.86  ]
-Original Grad: -0.009, -lr * Pred Grad: -0.071, New P: 2.181
-Original Grad: 0.004, -lr * Pred Grad: -0.003, New P: 0.857
iter 11 loss: 0.028
Actual params: [2.1806, 0.8567]
-Original Grad: -0.007, -lr * Pred Grad: -0.078, New P: 2.102
-Original Grad: 0.013, -lr * Pred Grad: 0.013, New P: 0.869
iter 12 loss: 0.027
Actual params: [2.1023, 0.8693]
-Original Grad: -0.007, -lr * Pred Grad: -0.085, New P: 2.017
-Original Grad: 0.020, -lr * Pred Grad: 0.039, New P: 0.908
iter 13 loss: 0.026
Actual params: [2.0168, 0.9084]
-Original Grad: -0.009, -lr * Pred Grad: -0.095, New P: 1.921
-Original Grad: 0.019, -lr * Pred Grad: 0.047, New P: 0.955
iter 14 loss: 0.025
Actual params: [1.9215, 0.9553]
-Original Grad: -0.002, -lr * Pred Grad: -0.085, New P: 1.836
-Original Grad: 0.027, -lr * Pred Grad: 0.073, New P: 1.028
iter 15 loss: 0.024
Actual params: [1.8363, 1.028 ]
-Original Grad: -0.008, -lr * Pred Grad: -0.096, New P: 1.740
-Original Grad: 0.013, -lr * Pred Grad: 0.041, New P: 1.069
iter 16 loss: 0.023
Actual params: [1.7399, 1.0689]
-Original Grad: 0.007, -lr * Pred Grad: -0.060, New P: 1.679
-Original Grad: 0.055, -lr * Pred Grad: 0.156, New P: 1.225
iter 17 loss: 0.022
Actual params: [1.6795, 1.2253]
-Original Grad: -0.033, -lr * Pred Grad: -0.154, New P: 1.525
-Original Grad: -0.031, -lr * Pred Grad: -0.056, New P: 1.170
iter 18 loss: 0.021
Actual params: [1.5254, 1.1697]
-Original Grad: 0.042, -lr * Pred Grad: 0.006, New P: 1.532
-Original Grad: 0.192, -lr * Pred Grad: 0.578, New P: 1.747
iter 19 loss: 0.022
Actual params: [1.5317, 1.7474]
-Original Grad: -0.040, -lr * Pred Grad: -0.143, New P: 1.388
-Original Grad: -0.196, -lr * Pred Grad: -0.388, New P: 1.359
iter 20 loss: 0.040
Actual params: [1.3884, 1.3594]
-Original Grad: -0.015, -lr * Pred Grad: -0.145, New P: 1.244
-Original Grad: -0.028, -lr * Pred Grad: -0.133, New P: 1.226
iter 21 loss: 0.015
Actual params: [1.2435, 1.2259]
-Original Grad: 0.009, -lr * Pred Grad: -0.085, New P: 1.159
-Original Grad: 0.267, -lr * Pred Grad: 0.629, New P: 1.855
iter 22 loss: 0.024
Actual params: [1.1589, 1.8548]
-Original Grad: -0.018, -lr * Pred Grad: -0.133, New P: 1.026
-Original Grad: -0.271, -lr * Pred Grad: -0.450, New P: 1.405
iter 23 loss: 0.037
Actual params: [1.0264, 1.4049]
-Original Grad: 0.028, -lr * Pred Grad: -0.024, New P: 1.003
-Original Grad: 0.053, -lr * Pred Grad: -0.062, New P: 1.343
iter 24 loss: 0.014
Actual params: [1.0025, 1.3434]
-Original Grad: 0.038, -lr * Pred Grad: 0.062, New P: 1.064
-Original Grad: 0.117, -lr * Pred Grad: 0.204, New P: 1.547
iter 25 loss: 0.017
Actual params: [1.0644, 1.5472]
-Original Grad: 0.008, -lr * Pred Grad: 0.037, New P: 1.101
-Original Grad: -0.044, -lr * Pred Grad: -0.065, New P: 1.482
iter 26 loss: 0.013
Actual params: [1.101 , 1.4822]
-Original Grad: 0.003, -lr * Pred Grad: 0.022, New P: 1.123
-Original Grad: -0.017, -lr * Pred Grad: -0.062, New P: 1.420
iter 27 loss: 0.012
Actual params: [1.1234, 1.4202]
-Original Grad: 0.002, -lr * Pred Grad: 0.010, New P: 1.134
-Original Grad: 0.015, -lr * Pred Grad: -0.018, New P: 1.402
iter 28 loss: 0.012
Actual params: [1.1337, 1.4021]
-Original Grad: 0.004, -lr * Pred Grad: 0.006, New P: 1.140
-Original Grad: 0.032, -lr * Pred Grad: 0.056, New P: 1.458
iter 29 loss: 0.012
Actual params: [1.1397, 1.4584]
-Original Grad: -0.005, -lr * Pred Grad: -0.023, New P: 1.117
-Original Grad: -0.012, -lr * Pred Grad: -0.029, New P: 1.429
iter 30 loss: 0.012
Actual params: [1.1166, 1.429 ]
Target params: [1.3344, 1.5708]
Actual params: [-0.8962,  0.1733]
-Original Grad: 0.049, -lr * Pred Grad: 1.115, New P: 0.219
-Original Grad: -0.037, -lr * Pred Grad: 0.459, New P: 0.632
iter 0 loss: 0.152
Actual params: [0.2186, 0.6322]
-Original Grad: -0.325, -lr * Pred Grad: -1.688, New P: -1.469
-Original Grad: 0.034, -lr * Pred Grad: 0.299, New P: 0.932
iter 1 loss: 0.044
Actual params: [-1.4693,  0.9317]
-Original Grad: 0.000, -lr * Pred Grad: -0.740, New P: -2.209
-Original Grad: -0.000, -lr * Pred Grad: -0.050, New P: 0.882
iter 2 loss: 0.168
Actual params: [-2.2091,  0.8816]
-Original Grad: 0.000, -lr * Pred Grad: -0.579, New P: -2.788
-Original Grad: -0.000, -lr * Pred Grad: -0.021, New P: 0.861
iter 3 loss: 0.168
Actual params: [-2.7878,  0.8608]
-Original Grad: 0.000, -lr * Pred Grad: -0.490, New P: -3.277
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: 0.814
iter 4 loss: 0.168
Actual params: [-3.2774,  0.8144]
-Original Grad: 0.000, -lr * Pred Grad: -0.418, New P: -3.696
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: 0.772
iter 5 loss: 0.168
Actual params: [-3.6958,  0.7715]
-Original Grad: 0.000, -lr * Pred Grad: -0.351, New P: -4.047
-Original Grad: -0.000, -lr * Pred Grad: -0.040, New P: 0.732
iter 6 loss: 0.168
Actual params: [-4.0471,  0.7317]
-Original Grad: 0.000, -lr * Pred Grad: -0.292, New P: -4.339
-Original Grad: -0.000, -lr * Pred Grad: -0.035, New P: 0.696
iter 7 loss: 0.168
Actual params: [-4.3394,  0.6965]
-Original Grad: 0.000, -lr * Pred Grad: -0.242, New P: -4.581
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.664
iter 8 loss: 0.168
Actual params: [-4.5813,  0.6637]
-Original Grad: 0.000, -lr * Pred Grad: -0.200, New P: -4.781
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.632
iter 9 loss: 0.168
Actual params: [-4.7808,  0.6322]
-Original Grad: 0.000, -lr * Pred Grad: -0.165, New P: -4.946
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.601
iter 10 loss: 0.168
Actual params: [-4.9457,  0.6013]
-Original Grad: 0.000, -lr * Pred Grad: -0.137, New P: -5.083
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.570
iter 11 loss: 0.168
Actual params: [-5.0829,  0.5705]
-Original Grad: 0.000, -lr * Pred Grad: -0.115, New P: -5.198
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.539
iter 12 loss: 0.168
Actual params: [-5.1981,  0.5395]
-Original Grad: 0.000, -lr * Pred Grad: -0.098, New P: -5.296
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.508
iter 13 loss: 0.168
Actual params: [-5.2964,  0.5083]
-Original Grad: 0.000, -lr * Pred Grad: -0.085, New P: -5.382
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.477
iter 14 loss: 0.168
Actual params: [-5.3816,  0.4768]
-Original Grad: 0.000, -lr * Pred Grad: -0.075, New P: -5.457
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.445
iter 15 loss: 0.168
Actual params: [-5.4567,  0.4451]
-Original Grad: 0.000, -lr * Pred Grad: -0.067, New P: -5.524
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.413
iter 16 loss: 0.168
Actual params: [-5.5242,  0.4132]
-Original Grad: 0.000, -lr * Pred Grad: -0.062, New P: -5.586
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.381
iter 17 loss: 0.168
Actual params: [-5.5857,  0.3811]
-Original Grad: 0.000, -lr * Pred Grad: -0.057, New P: -5.643
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.349
iter 18 loss: 0.168
Actual params: [-5.6429,  0.3489]
-Original Grad: 0.000, -lr * Pred Grad: -0.054, New P: -5.697
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.317
iter 19 loss: 0.168
Actual params: [-5.6966,  0.3167]
-Original Grad: 0.000, -lr * Pred Grad: -0.051, New P: -5.748
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.284
iter 20 loss: 0.168
Actual params: [-5.7478,  0.2843]
-Original Grad: 0.000, -lr * Pred Grad: -0.049, New P: -5.797
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.252
iter 21 loss: 0.168
Actual params: [-5.797 ,  0.2519]
-Original Grad: 0.000, -lr * Pred Grad: -0.048, New P: -5.845
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.219
iter 22 loss: 0.168
Actual params: [-5.8448,  0.2194]
-Original Grad: 0.000, -lr * Pred Grad: -0.047, New P: -5.891
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.187
iter 23 loss: 0.168
Actual params: [-5.8914,  0.1869]
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: -5.937
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.154
iter 24 loss: 0.168
Actual params: [-5.9372,  0.1544]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -5.982
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.122
iter 25 loss: 0.168
Actual params: [-5.9824,  0.1218]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -6.027
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.089
iter 26 loss: 0.168
Actual params: [-6.0271,  0.0893]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -6.071
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.057
iter 27 loss: 0.168
Actual params: [-6.0715,  0.0568]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -6.116
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.024
iter 28 loss: 0.168
Actual params: [-6.1156,  0.0242]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -6.160
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.008
iter 29 loss: 0.168
Actual params: [-6.1595, -0.0083]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -6.203
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.041
iter 30 loss: 0.168
Actual params: [-6.2033, -0.0409]
Target params: [1.3344, 1.5708]
Actual params: [1.5544, 0.3381]
-Original Grad: 0.270, -lr * Pred Grad: 2.158, New P: 3.713
-Original Grad: 1.784, -lr * Pred Grad: 2.726, New P: 3.064
iter 0 loss: 0.258
Actual params: [3.7129, 3.0644]
-Original Grad: 0.038, -lr * Pred Grad: 0.014, New P: 3.727
-Original Grad: -0.112, -lr * Pred Grad: -0.673, New P: 2.391
iter 1 loss: 0.047
Actual params: [3.7272, 2.3914]
-Original Grad: 0.011, -lr * Pred Grad: 0.680, New P: 4.407
-Original Grad: -0.054, -lr * Pred Grad: -0.037, New P: 2.355
iter 2 loss: 0.021
Actual params: [4.407 , 2.3546]
-Original Grad: 0.005, -lr * Pred Grad: 0.195, New P: 4.602
-Original Grad: -0.026, -lr * Pred Grad: -0.113, New P: 2.242
iter 3 loss: 0.015
Actual params: [4.6025, 2.2418]
-Original Grad: 0.003, -lr * Pred Grad: 0.186, New P: 4.788
-Original Grad: -0.020, -lr * Pred Grad: -0.074, New P: 2.167
iter 4 loss: 0.013
Actual params: [4.7884, 2.1673]
-Original Grad: 0.002, -lr * Pred Grad: 0.108, New P: 4.897
-Original Grad: -0.015, -lr * Pred Grad: -0.078, New P: 2.089
iter 5 loss: 0.012
Actual params: [4.8966, 2.0889]
-Original Grad: 0.001, -lr * Pred Grad: 0.074, New P: 4.971
-Original Grad: -0.013, -lr * Pred Grad: -0.070, New P: 2.019
iter 6 loss: 0.011
Actual params: [4.971 , 2.0188]
-Original Grad: 0.001, -lr * Pred Grad: 0.048, New P: 5.019
-Original Grad: -0.012, -lr * Pred Grad: -0.069, New P: 1.950
iter 7 loss: 0.010
Actual params: [5.0187, 1.9495]
-Original Grad: 0.000, -lr * Pred Grad: 0.028, New P: 5.047
-Original Grad: -0.012, -lr * Pred Grad: -0.069, New P: 1.881
iter 8 loss: 0.010
Actual params: [5.0467, 1.8807]
-Original Grad: -0.000, -lr * Pred Grad: 0.012, New P: 5.059
-Original Grad: -0.011, -lr * Pred Grad: -0.069, New P: 1.811
iter 9 loss: 0.010
Actual params: [5.0589, 1.8114]
-Original Grad: -0.000, -lr * Pred Grad: -0.001, New P: 5.058
-Original Grad: -0.010, -lr * Pred Grad: -0.069, New P: 1.742
iter 10 loss: 0.009
Actual params: [5.0581, 1.7421]
-Original Grad: -0.001, -lr * Pred Grad: -0.012, New P: 5.046
-Original Grad: -0.010, -lr * Pred Grad: -0.069, New P: 1.673
iter 11 loss: 0.009
Actual params: [5.0465, 1.6731]
-Original Grad: -0.001, -lr * Pred Grad: -0.021, New P: 5.026
-Original Grad: -0.009, -lr * Pred Grad: -0.068, New P: 1.605
iter 12 loss: 0.009
Actual params: [5.0257, 1.605 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.029, New P: 4.997
-Original Grad: -0.008, -lr * Pred Grad: -0.067, New P: 1.538
iter 13 loss: 0.008
Actual params: [4.9971, 1.5381]
-Original Grad: -0.001, -lr * Pred Grad: -0.035, New P: 4.962
-Original Grad: -0.008, -lr * Pred Grad: -0.065, New P: 1.473
iter 14 loss: 0.008
Actual params: [4.962 , 1.4727]
-Original Grad: -0.002, -lr * Pred Grad: -0.041, New P: 4.921
-Original Grad: -0.007, -lr * Pred Grad: -0.063, New P: 1.409
iter 15 loss: 0.008
Actual params: [4.9213, 1.4093]
-Original Grad: -0.002, -lr * Pred Grad: -0.046, New P: 4.876
-Original Grad: -0.006, -lr * Pred Grad: -0.061, New P: 1.348
iter 16 loss: 0.007
Actual params: [4.8757, 1.3479]
-Original Grad: -0.002, -lr * Pred Grad: -0.050, New P: 4.826
-Original Grad: -0.006, -lr * Pred Grad: -0.059, New P: 1.289
iter 17 loss: 0.007
Actual params: [4.8261, 1.2888]
-Original Grad: -0.002, -lr * Pred Grad: -0.053, New P: 4.773
-Original Grad: -0.005, -lr * Pred Grad: -0.057, New P: 1.232
iter 18 loss: 0.007
Actual params: [4.7728, 1.2321]
-Original Grad: -0.002, -lr * Pred Grad: -0.056, New P: 4.717
-Original Grad: -0.004, -lr * Pred Grad: -0.054, New P: 1.178
iter 19 loss: 0.007
Actual params: [4.7165, 1.1778]
-Original Grad: -0.002, -lr * Pred Grad: -0.059, New P: 4.658
-Original Grad: -0.004, -lr * Pred Grad: -0.052, New P: 1.126
iter 20 loss: 0.006
Actual params: [4.6575, 1.1262]
-Original Grad: -0.003, -lr * Pred Grad: -0.061, New P: 4.596
-Original Grad: -0.003, -lr * Pred Grad: -0.049, New P: 1.077
iter 21 loss: 0.006
Actual params: [4.5962, 1.0773]
-Original Grad: -0.003, -lr * Pred Grad: -0.063, New P: 4.533
-Original Grad: -0.002, -lr * Pred Grad: -0.046, New P: 1.031
iter 22 loss: 0.006
Actual params: [4.533 , 1.0311]
-Original Grad: -0.003, -lr * Pred Grad: -0.065, New P: 4.468
-Original Grad: -0.002, -lr * Pred Grad: -0.044, New P: 0.987
iter 23 loss: 0.006
Actual params: [4.4682, 0.9868]
-Original Grad: -0.003, -lr * Pred Grad: -0.066, New P: 4.402
-Original Grad: -0.002, -lr * Pred Grad: -0.043, New P: 0.944
iter 24 loss: 0.005
Actual params: [4.4022, 0.9441]
-Original Grad: -0.003, -lr * Pred Grad: -0.067, New P: 4.335
-Original Grad: -0.002, -lr * Pred Grad: -0.043, New P: 0.901
iter 25 loss: 0.005
Actual params: [4.3352, 0.9008]
-Original Grad: -0.003, -lr * Pred Grad: -0.067, New P: 4.268
-Original Grad: -0.003, -lr * Pred Grad: -0.045, New P: 0.856
iter 26 loss: 0.005
Actual params: [4.2678, 0.8555]
-Original Grad: -0.003, -lr * Pred Grad: -0.068, New P: 4.200
-Original Grad: -0.004, -lr * Pred Grad: -0.047, New P: 0.808
iter 27 loss: 0.005
Actual params: [4.2002, 0.8082]
-Original Grad: -0.002, -lr * Pred Grad: -0.068, New P: 4.133
-Original Grad: -0.003, -lr * Pred Grad: -0.047, New P: 0.761
iter 28 loss: 0.004
Actual params: [4.1326, 0.7614]
-Original Grad: -0.002, -lr * Pred Grad: -0.067, New P: 4.065
-Original Grad: -0.002, -lr * Pred Grad: -0.045, New P: 0.717
iter 29 loss: 0.004
Actual params: [4.0653, 0.7165]
-Original Grad: -0.002, -lr * Pred Grad: -0.067, New P: 3.999
-Original Grad: -0.002, -lr * Pred Grad: -0.042, New P: 0.674
iter 30 loss: 0.004
Actual params: [3.9987, 0.6743]
Target params: [1.3344, 1.5708]
Actual params: [-0.7899, -0.493 ]
-Original Grad: -0.003, -lr * Pred Grad: 0.616, New P: -0.173
-Original Grad: -0.004, -lr * Pred Grad: 0.617, New P: 0.124
iter 0 loss: 0.537
Actual params: [-0.1735,  0.1238]
-Original Grad: 0.010, -lr * Pred Grad: 0.437, New P: 0.263
-Original Grad: 0.124, -lr * Pred Grad: 0.847, New P: 0.970
iter 1 loss: 0.531
Actual params: [0.2631, 0.9704]
-Original Grad: 0.213, -lr * Pred Grad: 1.569, New P: 1.832
-Original Grad: 0.103, -lr * Pred Grad: 0.443, New P: 1.414
iter 2 loss: 0.415
Actual params: [1.8325, 1.4139]
-Original Grad: -0.178, -lr * Pred Grad: -0.890, New P: 0.942
-Original Grad: 0.309, -lr * Pred Grad: 1.456, New P: 2.870
iter 3 loss: 0.233
Actual params: [0.942 , 2.8698]
-Original Grad: 0.162, -lr * Pred Grad: 0.414, New P: 1.356
-Original Grad: -1.270, -lr * Pred Grad: -0.677, New P: 2.193
iter 4 loss: 0.379
Actual params: [1.3563, 2.1927]
-Original Grad: 0.031, -lr * Pred Grad: 0.104, New P: 1.460
-Original Grad: -0.003, -lr * Pred Grad: -0.308, New P: 1.884
iter 5 loss: 0.079
Actual params: [1.4603, 1.8843]
-Original Grad: -0.138, -lr * Pred Grad: -0.270, New P: 1.190
-Original Grad: 0.201, -lr * Pred Grad: -0.163, New P: 1.721
iter 6 loss: 0.088
Actual params: [1.1901, 1.7215]
-Original Grad: 0.101, -lr * Pred Grad: 0.160, New P: 1.350
-Original Grad: 0.109, -lr * Pred Grad: 0.127, New P: 1.849
iter 7 loss: 0.099
Actual params: [1.3503, 1.8485]
-Original Grad: 0.036, -lr * Pred Grad: 0.117, New P: 1.468
-Original Grad: 0.067, -lr * Pred Grad: 0.159, New P: 2.008
iter 8 loss: 0.083
Actual params: [1.4675, 2.0078]
-Original Grad: -0.055, -lr * Pred Grad: -0.073, New P: 1.394
-Original Grad: 0.088, -lr * Pred Grad: 0.301, New P: 2.308
iter 9 loss: 0.080
Actual params: [1.3941, 2.3085]
-Original Grad: 0.041, -lr * Pred Grad: 0.083, New P: 1.477
-Original Grad: 0.014, -lr * Pred Grad: 0.103, New P: 2.411
iter 10 loss: 0.078
Actual params: [1.4771, 2.411 ]
-Original Grad: 0.054, -lr * Pred Grad: 0.163, New P: 1.640
-Original Grad: -0.024, -lr * Pred Grad: -0.041, New P: 2.370
iter 11 loss: 0.073
Actual params: [1.6404, 2.3699]
-Original Grad: -0.171, -lr * Pred Grad: -0.377, New P: 1.263
-Original Grad: 0.159, -lr * Pred Grad: 0.462, New P: 2.832
iter 12 loss: 0.077
Actual params: [1.2633, 2.8317]
-Original Grad: 0.155, -lr * Pred Grad: 0.176, New P: 1.439
-Original Grad: -1.687, -lr * Pred Grad: -0.439, New P: 2.392
iter 13 loss: 0.264
Actual params: [1.4395, 2.3923]
-Original Grad: 0.056, -lr * Pred Grad: 0.145, New P: 1.585
-Original Grad: 0.001, -lr * Pred Grad: -0.467, New P: 1.925
iter 14 loss: 0.075
Actual params: [1.5849, 1.925 ]
-Original Grad: -0.189, -lr * Pred Grad: -0.400, New P: 1.184
-Original Grad: 0.228, -lr * Pred Grad: -0.203, New P: 1.722
iter 15 loss: 0.105
Actual params: [1.1845, 1.7219]
-Original Grad: 0.107, -lr * Pred Grad: 0.042, New P: 1.227
-Original Grad: 0.107, -lr * Pred Grad: 0.021, New P: 1.743
iter 16 loss: 0.099
Actual params: [1.2267, 1.743 ]
-Original Grad: 0.069, -lr * Pred Grad: 0.124, New P: 1.351
-Original Grad: 0.108, -lr * Pred Grad: 0.189, New P: 1.932
iter 17 loss: 0.094
Actual params: [1.3509, 1.932 ]
-Original Grad: 0.030, -lr * Pred Grad: 0.134, New P: 1.485
-Original Grad: 0.037, -lr * Pred Grad: 0.112, New P: 2.044
iter 18 loss: 0.081
Actual params: [1.4848, 2.0439]
-Original Grad: -0.061, -lr * Pred Grad: -0.097, New P: 1.388
-Original Grad: 0.070, -lr * Pred Grad: 0.228, New P: 2.272
iter 19 loss: 0.079
Actual params: [1.3876, 2.2723]
-Original Grad: 0.035, -lr * Pred Grad: 0.054, New P: 1.442
-Original Grad: 0.002, -lr * Pred Grad: 0.047, New P: 2.320
iter 20 loss: 0.079
Actual params: [1.4418, 2.3197]
-Original Grad: 0.042, -lr * Pred Grad: 0.118, New P: 1.559
-Original Grad: 0.042, -lr * Pred Grad: 0.130, New P: 2.449
iter 21 loss: 0.076
Actual params: [1.5594, 2.4494]
-Original Grad: 0.055, -lr * Pred Grad: 0.203, New P: 1.762
-Original Grad: -0.112, -lr * Pred Grad: -0.222, New P: 2.227
iter 22 loss: 0.070
Actual params: [1.7619, 2.227 ]
-Original Grad: -0.295, -lr * Pred Grad: -0.649, New P: 1.113
-Original Grad: 0.171, -lr * Pred Grad: 0.319, New P: 2.546
iter 23 loss: 0.119
Actual params: [1.1127, 2.5455]
-Original Grad: 0.326, -lr * Pred Grad: 0.324, New P: 1.437
-Original Grad: -0.556, -lr * Pred Grad: -0.418, New P: 2.127
iter 24 loss: 0.138
Actual params: [1.4369, 2.1274]
-Original Grad: 0.021, -lr * Pred Grad: 0.043, New P: 1.480
-Original Grad: 0.004, -lr * Pred Grad: -0.353, New P: 1.774
iter 25 loss: 0.078
Actual params: [1.4795, 1.7744]
-Original Grad: -0.177, -lr * Pred Grad: -0.359, New P: 1.121
-Original Grad: 0.265, -lr * Pred Grad: 0.288, New P: 2.063
iter 26 loss: 0.104
Actual params: [1.1209, 2.0627]
-Original Grad: 0.193, -lr * Pred Grad: 0.241, New P: 1.362
-Original Grad: -0.003, -lr * Pred Grad: 0.011, New P: 2.073
iter 27 loss: 0.101
Actual params: [1.3619, 2.0735]
-Original Grad: 0.023, -lr * Pred Grad: 0.088, New P: 1.450
-Original Grad: 0.007, -lr * Pred Grad: 0.014, New P: 2.088
iter 28 loss: 0.079
Actual params: [1.45  , 2.0878]
-Original Grad: 0.012, -lr * Pred Grad: 0.107, New P: 1.557
-Original Grad: 0.012, -lr * Pred Grad: 0.008, New P: 2.096
iter 29 loss: 0.078
Actual params: [1.5567, 2.0957]
-Original Grad: -0.160, -lr * Pred Grad: -0.361, New P: 1.196
-Original Grad: 0.148, -lr * Pred Grad: 0.450, New P: 2.546
iter 30 loss: 0.084
Actual params: [1.1957, 2.546 ]
Target params: [1.3344, 1.5708]
Actual params: [0.3685, 0.155 ]
-Original Grad: 0.280, -lr * Pred Grad: 2.178, New P: 2.546
-Original Grad: 0.177, -lr * Pred Grad: 1.401, New P: 1.556
iter 0 loss: 0.457
Actual params: [2.5461, 1.5559]
-Original Grad: -0.058, -lr * Pred Grad: -0.704, New P: 1.842
-Original Grad: -0.510, -lr * Pred Grad: -0.887, New P: 0.668
iter 1 loss: 0.316
Actual params: [1.8418, 0.6685]
-Original Grad: -0.082, -lr * Pred Grad: -0.194, New P: 1.648
-Original Grad: -0.032, -lr * Pred Grad: -0.147, New P: 0.521
iter 2 loss: 0.106
Actual params: [1.6479, 0.5211]
-Original Grad: 0.011, -lr * Pred Grad: 0.031, New P: 1.679
-Original Grad: 0.068, -lr * Pred Grad: -0.183, New P: 0.338
iter 3 loss: 0.089
Actual params: [1.6792, 0.3377]
-Original Grad: 0.153, -lr * Pred Grad: 0.467, New P: 2.147
-Original Grad: 0.315, -lr * Pred Grad: 0.845, New P: 1.183
iter 4 loss: 0.105
Actual params: [2.1466, 1.1825]
-Original Grad: -0.079, -lr * Pred Grad: -0.095, New P: 2.051
-Original Grad: -0.352, -lr * Pred Grad: -0.524, New P: 0.659
iter 5 loss: 0.177
Actual params: [2.0514, 0.6587]
-Original Grad: -0.087, -lr * Pred Grad: -0.249, New P: 1.802
-Original Grad: -0.028, -lr * Pred Grad: -0.193, New P: 0.465
iter 6 loss: 0.123
Actual params: [1.8021, 0.4653]
-Original Grad: -0.117, -lr * Pred Grad: -0.465, New P: 1.338
-Original Grad: -0.000, -lr * Pred Grad: -0.215, New P: 0.250
iter 7 loss: 0.101
Actual params: [1.3375, 0.2504]
-Original Grad: 0.531, -lr * Pred Grad: 0.673, New P: 2.011
-Original Grad: 0.687, -lr * Pred Grad: 1.724, New P: 1.974
iter 8 loss: 0.291
Actual params: [2.0108, 1.9741]
-Original Grad: -0.056, -lr * Pred Grad: -0.108, New P: 1.902
-Original Grad: -0.603, -lr * Pred Grad: -0.785, New P: 1.189
iter 9 loss: 0.404
Actual params: [1.9024, 1.1891]
-Original Grad: -0.081, -lr * Pred Grad: -0.118, New P: 1.784
-Original Grad: -0.353, -lr * Pred Grad: -0.296, New P: 0.893
iter 10 loss: 0.156
Actual params: [1.7841, 0.8927]
-Original Grad: -0.066, -lr * Pred Grad: -0.210, New P: 1.575
-Original Grad: -0.134, -lr * Pred Grad: -0.524, New P: 0.369
iter 11 loss: 0.109
Actual params: [1.5746, 0.3689]
-Original Grad: 0.541, -lr * Pred Grad: 0.763, New P: 2.338
-Original Grad: 0.686, -lr * Pred Grad: 1.044, New P: 1.413
iter 12 loss: 0.133
Actual params: [2.3377, 1.4126]
-Original Grad: 0.087, -lr * Pred Grad: 0.225, New P: 2.563
-Original Grad: -14.436, -lr * Pred Grad: -0.614, New P: 0.799
iter 13 loss: 0.264
Actual params: [2.5631, 0.7988]
-Original Grad: -0.099, -lr * Pred Grad: 0.026, New P: 2.589
-Original Grad: -0.047, -lr * Pred Grad: -0.441, New P: 0.358
iter 14 loss: 0.166
Actual params: [2.5889, 0.3583]
-Original Grad: -0.127, -lr * Pred Grad: -0.243, New P: 2.346
-Original Grad: 0.200, -lr * Pred Grad: -0.438, New P: -0.080
iter 15 loss: 0.177
Actual params: [ 2.3462, -0.0796]
-Original Grad: -0.143, -lr * Pred Grad: -0.467, New P: 1.879
-Original Grad: 0.756, -lr * Pred Grad: 1.056, New P: 0.977
iter 16 loss: 0.251
Actual params: [1.8794, 0.9766]
-Original Grad: -0.081, -lr * Pred Grad: -0.508, New P: 1.371
-Original Grad: -0.223, -lr * Pred Grad: -0.541, New P: 0.436
iter 17 loss: 0.124
Actual params: [1.3713, 0.4361]
-Original Grad: 0.283, -lr * Pred Grad: 0.228, New P: 1.599
-Original Grad: 0.746, -lr * Pred Grad: 1.240, New P: 1.676
iter 18 loss: 0.210
Actual params: [1.5992, 1.6762]
-Original Grad: -0.032, -lr * Pred Grad: -0.066, New P: 1.533
-Original Grad: -0.608, -lr * Pred Grad: -0.560, New P: 1.116
iter 19 loss: 0.289
Actual params: [1.5333, 1.1158]
-Original Grad: -0.010, -lr * Pred Grad: -0.048, New P: 1.485
-Original Grad: -0.359, -lr * Pred Grad: -0.357, New P: 0.759
iter 20 loss: 0.125
Actual params: [1.485 , 0.7587]
-Original Grad: 0.005, -lr * Pred Grad: -0.029, New P: 1.456
-Original Grad: -0.102, -lr * Pred Grad: -0.498, New P: 0.261
iter 21 loss: 0.091
Actual params: [1.4563, 0.2611]
-Original Grad: 0.521, -lr * Pred Grad: 0.895, New P: 2.351
-Original Grad: 0.424, -lr * Pred Grad: 0.487, New P: 0.748
iter 22 loss: 0.227
Actual params: [2.3512, 0.748 ]
-Original Grad: -0.074, -lr * Pred Grad: -0.133, New P: 2.218
-Original Grad: -0.061, -lr * Pred Grad: -0.167, New P: 0.581
iter 23 loss: 0.146
Actual params: [2.2182, 0.5805]
-Original Grad: -0.070, -lr * Pred Grad: -0.057, New P: 2.162
-Original Grad: -0.005, -lr * Pred Grad: -0.044, New P: 0.537
iter 24 loss: 0.134
Actual params: [2.1616, 0.5366]
-Original Grad: -0.065, -lr * Pred Grad: -0.167, New P: 1.995
-Original Grad: 0.023, -lr * Pred Grad: -0.020, New P: 0.516
iter 25 loss: 0.130
Actual params: [1.9949, 0.5163]
-Original Grad: -0.080, -lr * Pred Grad: -0.288, New P: 1.707
-Original Grad: 0.011, -lr * Pred Grad: 0.005, New P: 0.521
iter 26 loss: 0.117
Actual params: [1.7067, 0.5211]
-Original Grad: -0.080, -lr * Pred Grad: -0.391, New P: 1.316
-Original Grad: -0.033, -lr * Pred Grad: -0.094, New P: 0.427
iter 27 loss: 0.091
Actual params: [1.3161, 0.427 ]
-Original Grad: 0.286, -lr * Pred Grad: 0.331, New P: 1.647
-Original Grad: 0.855, -lr * Pred Grad: 2.097, New P: 2.524
iter 28 loss: 0.229
Actual params: [1.6472, 2.5237]
-Original Grad: -0.019, -lr * Pred Grad: 0.005, New P: 1.652
-Original Grad: -0.645, -lr * Pred Grad: -0.893, New P: 1.631
iter 29 loss: 0.553
Actual params: [1.6519, 1.6311]
-Original Grad: -0.037, -lr * Pred Grad: -0.050, New P: 1.602
-Original Grad: -0.601, -lr * Pred Grad: -0.302, New P: 1.329
iter 30 loss: 0.277
Actual params: [1.6024, 1.3288]
