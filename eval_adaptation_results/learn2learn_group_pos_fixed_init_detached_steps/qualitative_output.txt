Target params: [1.1812, 0.2779]
iter 0 loss: 0.739
Actual params: [0.5941, 0.5941]
-Original Grad: 0.299, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.562, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.718
Actual params: [0.6617, 0.5315]
-Original Grad: 0.365, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.411, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.670
Actual params: [0.7467, 0.451 ]
-Original Grad: 0.303, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.450, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.577
Actual params: [0.8345, 0.3673]
-Original Grad: 0.258, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.028, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.424
Actual params: [0.9228, 0.2831]
-Original Grad: 0.380, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.144, -lr * Pred Grad: -0.081, New P: 0.202
iter 5 loss: 0.306
Actual params: [1.0111, 0.2021]
-Original Grad: 0.035, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 0.130, -lr * Pred Grad: -0.072, New P: 0.130
iter 6 loss: 0.327
Actual params: [1.0994, 0.1299]
-Original Grad: 0.032, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.116, -lr * Pred Grad: -0.067, New P: 0.063
iter 7 loss: 0.393
Actual params: [1.1877, 0.0629]
-Original Grad: -0.150, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.210, -lr * Pred Grad: -0.052, New P: 0.011
iter 8 loss: 0.436
Actual params: [1.276 , 0.0106]
-Original Grad: -0.435, -lr * Pred Grad: 0.083, New P: 1.359
-Original Grad: 0.737, -lr * Pred Grad: -0.033, New P: -0.023
iter 9 loss: 0.475
Actual params: [ 1.3594, -0.0227]
-Original Grad: -0.376, -lr * Pred Grad: -0.031, New P: 1.329
-Original Grad: 0.822, -lr * Pred Grad: -0.006, New P: -0.029
iter 10 loss: 0.461
Actual params: [ 1.3289, -0.0289]
-Original Grad: -0.404, -lr * Pred Grad: -0.054, New P: 1.275
-Original Grad: 1.072, -lr * Pred Grad: 0.042, New P: 0.013
iter 11 loss: 0.436
Actual params: [1.2753, 0.0132]
-Original Grad: -0.379, -lr * Pred Grad: -0.065, New P: 1.210
-Original Grad: 0.538, -lr * Pred Grad: 0.076, New P: 0.090
iter 12 loss: 0.407
Actual params: [1.2102, 0.0896]
-Original Grad: -0.143, -lr * Pred Grad: -0.071, New P: 1.139
-Original Grad: 0.195, -lr * Pred Grad: 0.086, New P: 0.176
iter 13 loss: 0.389
Actual params: [1.1389, 0.1758]
-Original Grad: 0.048, -lr * Pred Grad: -0.066, New P: 1.073
-Original Grad: 0.103, -lr * Pred Grad: 0.088, New P: 0.264
iter 14 loss: 0.408
Actual params: [1.0728, 0.2639]
-Original Grad: -0.204, -lr * Pred Grad: -0.068, New P: 1.005
-Original Grad: 0.081, -lr * Pred Grad: 0.088, New P: 0.352
iter 15 loss: 0.490
Actual params: [1.0049, 0.3522]
-Original Grad: 0.089, -lr * Pred Grad: -0.062, New P: 0.943
-Original Grad: 0.006, -lr * Pred Grad: 0.088, New P: 0.441
iter 16 loss: 0.611
Actual params: [0.9426, 0.4405]
-Original Grad: 0.198, -lr * Pred Grad: -0.040, New P: 0.903
-Original Grad: -0.370, -lr * Pred Grad: 0.088, New P: 0.529
iter 17 loss: 0.667
Actual params: [0.9026, 0.5288]
-Original Grad: 0.262, -lr * Pred Grad: -0.024, New P: 0.878
-Original Grad: -0.819, -lr * Pred Grad: 0.088, New P: 0.617
iter 18 loss: 0.687
Actual params: [0.8783, 0.6167]
-Original Grad: 0.127, -lr * Pred Grad: -0.017, New P: 0.862
-Original Grad: -0.807, -lr * Pred Grad: -0.023, New P: 0.594
iter 19 loss: 0.675
Actual params: [0.8616, 0.5938]
-Original Grad: 0.168, -lr * Pred Grad: -0.006, New P: 0.855
-Original Grad: -0.754, -lr * Pred Grad: -0.052, New P: 0.542
iter 20 loss: 0.664
Actual params: [0.8552, 0.5419]
-Original Grad: 0.230, -lr * Pred Grad: 0.022, New P: 0.878
-Original Grad: -0.901, -lr * Pred Grad: -0.060, New P: 0.482
Target params: [1.1812, 0.2779]
iter 0 loss: 0.312
Actual params: [0.5941, 0.5941]
-Original Grad: 0.033, -lr * Pred Grad: 0.054, New P: 0.648
-Original Grad: -0.131, -lr * Pred Grad: -0.056, New P: 0.538
iter 1 loss: 0.262
Actual params: [0.6476, 0.5377]
-Original Grad: 0.010, -lr * Pred Grad: 0.010, New P: 0.658
-Original Grad: -0.047, -lr * Pred Grad: -0.075, New P: 0.462
iter 2 loss: 0.253
Actual params: [0.6578, 0.4624]
-Original Grad: -0.009, -lr * Pred Grad: -0.042, New P: 0.616
-Original Grad: 0.001, -lr * Pred Grad: -0.071, New P: 0.391
iter 3 loss: 0.252
Actual params: [0.6162, 0.3911]
-Original Grad: -0.019, -lr * Pred Grad: -0.047, New P: 0.569
-Original Grad: 0.007, -lr * Pred Grad: -0.068, New P: 0.323
iter 4 loss: 0.251
Actual params: [0.5689, 0.3233]
-Original Grad: -0.033, -lr * Pred Grad: -0.036, New P: 0.533
-Original Grad: 0.017, -lr * Pred Grad: -0.058, New P: 0.266
iter 5 loss: 0.250
Actual params: [0.5332, 0.2656]
-Original Grad: -0.031, -lr * Pred Grad: -0.029, New P: 0.504
-Original Grad: 0.009, -lr * Pred Grad: -0.036, New P: 0.230
iter 6 loss: 0.250
Actual params: [0.5038, 0.2298]
-Original Grad: -0.036, -lr * Pred Grad: -0.035, New P: 0.469
-Original Grad: 0.009, -lr * Pred Grad: -0.009, New P: 0.221
iter 7 loss: 0.252
Actual params: [0.4692, 0.221 ]
-Original Grad: -0.033, -lr * Pred Grad: -0.037, New P: 0.432
-Original Grad: 0.004, -lr * Pred Grad: 0.005, New P: 0.226
iter 8 loss: 0.261
Actual params: [0.432 , 0.2259]
-Original Grad: -0.046, -lr * Pred Grad: -0.042, New P: 0.390
-Original Grad: 0.005, -lr * Pred Grad: -0.017, New P: 0.209
iter 9 loss: 0.273
Actual params: [0.3904, 0.2092]
-Original Grad: 0.004, -lr * Pred Grad: -0.032, New P: 0.359
-Original Grad: -0.003, -lr * Pred Grad: -0.017, New P: 0.192
iter 10 loss: 0.282
Actual params: [0.3586, 0.1919]
-Original Grad: 0.010, -lr * Pred Grad: -0.013, New P: 0.346
-Original Grad: -0.000, -lr * Pred Grad: -0.021, New P: 0.171
iter 11 loss: 0.285
Actual params: [0.3455, 0.1712]
-Original Grad: 0.011, -lr * Pred Grad: -0.000, New P: 0.345
-Original Grad: -0.000, -lr * Pred Grad: -0.021, New P: 0.150
iter 12 loss: 0.284
Actual params: [0.3454, 0.1504]
-Original Grad: 0.023, -lr * Pred Grad: 0.006, New P: 0.352
-Original Grad: 0.000, -lr * Pred Grad: -0.021, New P: 0.130
iter 13 loss: 0.283
Actual params: [0.3518, 0.1298]
-Original Grad: 0.012, -lr * Pred Grad: 0.002, New P: 0.354
-Original Grad: -0.001, -lr * Pred Grad: -0.021, New P: 0.109
iter 14 loss: 0.282
Actual params: [0.3536, 0.1086]
-Original Grad: 0.010, -lr * Pred Grad: -0.004, New P: 0.349
-Original Grad: -0.001, -lr * Pred Grad: -0.022, New P: 0.087
iter 15 loss: 0.284
Actual params: [0.3495, 0.0869]
-Original Grad: 0.007, -lr * Pred Grad: -0.008, New P: 0.342
-Original Grad: 0.002, -lr * Pred Grad: -0.021, New P: 0.066
iter 16 loss: 0.287
Actual params: [0.3415, 0.0659]
-Original Grad: 0.012, -lr * Pred Grad: -0.006, New P: 0.335
-Original Grad: 0.004, -lr * Pred Grad: -0.019, New P: 0.046
iter 17 loss: 0.290
Actual params: [0.3351, 0.0464]
-Original Grad: 0.019, -lr * Pred Grad: 0.000, New P: 0.335
-Original Grad: 0.003, -lr * Pred Grad: -0.019, New P: 0.028
iter 18 loss: 0.290
Actual params: [0.3353, 0.0275]
-Original Grad: 0.020, -lr * Pred Grad: 0.005, New P: 0.341
-Original Grad: 0.005, -lr * Pred Grad: -0.019, New P: 0.009
iter 19 loss: 0.289
Actual params: [0.3406, 0.0087]
-Original Grad: 0.017, -lr * Pred Grad: 0.005, New P: 0.345
-Original Grad: 0.007, -lr * Pred Grad: -0.018, New P: -0.009
iter 20 loss: 0.289
Actual params: [ 0.3454, -0.0093]
-Original Grad: 0.008, -lr * Pred Grad: -0.001, New P: 0.344
-Original Grad: 0.006, -lr * Pred Grad: -0.017, New P: -0.026
Target params: [1.1812, 0.2779]
iter 0 loss: 0.445
Actual params: [0.5941, 0.5941]
-Original Grad: -0.082, -lr * Pred Grad: -0.039, New P: 0.555
-Original Grad: -0.721, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.323
Actual params: [0.5546, 0.532 ]
-Original Grad: -0.136, -lr * Pred Grad: -0.075, New P: 0.480
-Original Grad: -0.624, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.299
Actual params: [0.4795, 0.4514]
-Original Grad: -0.109, -lr * Pred Grad: -0.081, New P: 0.399
-Original Grad: -0.157, -lr * Pred Grad: -0.084, New P: 0.368
iter 3 loss: 0.325
Actual params: [0.3987, 0.3678]
-Original Grad: 0.088, -lr * Pred Grad: -0.073, New P: 0.326
-Original Grad: -0.003, -lr * Pred Grad: -0.084, New P: 0.284
iter 4 loss: 0.346
Actual params: [0.326 , 0.2837]
-Original Grad: 0.165, -lr * Pred Grad: -0.063, New P: 0.263
-Original Grad: 0.000, -lr * Pred Grad: -0.083, New P: 0.201
iter 5 loss: 0.355
Actual params: [0.2631, 0.2012]
-Original Grad: 0.192, -lr * Pred Grad: -0.035, New P: 0.228
-Original Grad: -0.031, -lr * Pred Grad: -0.076, New P: 0.125
iter 6 loss: 0.358
Actual params: [0.228 , 0.1254]
-Original Grad: 0.194, -lr * Pred Grad: 0.013, New P: 0.242
-Original Grad: -0.015, -lr * Pred Grad: -0.070, New P: 0.055
iter 7 loss: 0.350
Actual params: [0.2415, 0.055 ]
-Original Grad: 0.150, -lr * Pred Grad: 0.065, New P: 0.306
-Original Grad: 0.011, -lr * Pred Grad: -0.068, New P: -0.012
iter 8 loss: 0.336
Actual params: [ 0.3063, -0.0125]
-Original Grad: 0.132, -lr * Pred Grad: 0.083, New P: 0.390
-Original Grad: 0.034, -lr * Pred Grad: -0.055, New P: -0.067
iter 9 loss: 0.323
Actual params: [ 0.3896, -0.0674]
-Original Grad: 0.126, -lr * Pred Grad: 0.086, New P: 0.475
-Original Grad: 0.055, -lr * Pred Grad: -0.037, New P: -0.104
iter 10 loss: 0.316
Actual params: [ 0.4751, -0.104 ]
-Original Grad: 0.038, -lr * Pred Grad: 0.080, New P: 0.555
-Original Grad: 0.040, -lr * Pred Grad: -0.027, New P: -0.131
iter 11 loss: 0.308
Actual params: [ 0.5551, -0.1312]
-Original Grad: -0.016, -lr * Pred Grad: 0.005, New P: 0.560
-Original Grad: 0.024, -lr * Pred Grad: -0.024, New P: -0.155
iter 12 loss: 0.308
Actual params: [ 0.5602, -0.1554]
-Original Grad: 0.004, -lr * Pred Grad: 0.030, New P: 0.590
-Original Grad: 0.055, -lr * Pred Grad: -0.018, New P: -0.174
iter 13 loss: 0.309
Actual params: [ 0.5903, -0.1738]
-Original Grad: -0.019, -lr * Pred Grad: -0.028, New P: 0.562
-Original Grad: 0.035, -lr * Pred Grad: -0.002, New P: -0.176
iter 14 loss: 0.308
Actual params: [ 0.5623, -0.1757]
-Original Grad: -0.009, -lr * Pred Grad: -0.014, New P: 0.548
-Original Grad: 0.025, -lr * Pred Grad: 0.026, New P: -0.150
iter 15 loss: 0.308
Actual params: [ 0.5485, -0.15  ]
-Original Grad: -0.016, -lr * Pred Grad: -0.030, New P: 0.519
-Original Grad: 0.036, -lr * Pred Grad: 0.009, New P: -0.141
iter 16 loss: 0.311
Actual params: [ 0.5188, -0.1407]
-Original Grad: 0.007, -lr * Pred Grad: -0.015, New P: 0.504
-Original Grad: 0.034, -lr * Pred Grad: 0.019, New P: -0.122
iter 17 loss: 0.313
Actual params: [ 0.5041, -0.1218]
-Original Grad: 0.040, -lr * Pred Grad: 0.012, New P: 0.516
-Original Grad: 0.039, -lr * Pred Grad: 0.018, New P: -0.104
iter 18 loss: 0.312
Actual params: [ 0.5159, -0.1041]
-Original Grad: 0.039, -lr * Pred Grad: 0.024, New P: 0.540
-Original Grad: 0.040, -lr * Pred Grad: 0.024, New P: -0.080
iter 19 loss: 0.308
Actual params: [ 0.5397, -0.0803]
-Original Grad: 0.000, -lr * Pred Grad: -0.001, New P: 0.538
-Original Grad: 0.033, -lr * Pred Grad: 0.017, New P: -0.063
iter 20 loss: 0.308
Actual params: [ 0.5384, -0.0635]
-Original Grad: -0.007, -lr * Pred Grad: -0.015, New P: 0.523
-Original Grad: 0.017, -lr * Pred Grad: 0.005, New P: -0.058
Target params: [1.1812, 0.2779]
iter 0 loss: 1.269
Actual params: [0.5941, 0.5941]
-Original Grad: 0.295, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.385, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 1.224
Actual params: [0.6617, 0.531 ]
-Original Grad: 0.478, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.515, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 1.130
Actual params: [0.7467, 0.4506]
-Original Grad: 0.606, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.289, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.880
Actual params: [0.8346, 0.367 ]
-Original Grad: 0.588, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.151, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.608
Actual params: [0.9229, 0.2828]
-Original Grad: 0.268, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.017, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.455
Actual params: [1.0112, 0.1986]
-Original Grad: 0.141, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: -0.000, -lr * Pred Grad: -0.082, New P: 0.117
iter 6 loss: 0.411
Actual params: [1.0995, 0.1167]
-Original Grad: 0.056, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.016, -lr * Pred Grad: -0.073, New P: 0.043
iter 7 loss: 0.425
Actual params: [1.1879, 0.0434]
-Original Grad: 0.021, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.021, -lr * Pred Grad: -0.069, New P: -0.026
iter 8 loss: 0.482
Actual params: [ 1.2762, -0.026 ]
-Original Grad: -0.083, -lr * Pred Grad: 0.088, New P: 1.364
-Original Grad: 0.043, -lr * Pred Grad: -0.062, New P: -0.088
iter 9 loss: 0.542
Actual params: [ 1.3645, -0.0881]
-Original Grad: -0.121, -lr * Pred Grad: 0.088, New P: 1.453
-Original Grad: 0.034, -lr * Pred Grad: -0.043, New P: -0.131
iter 10 loss: 0.599
Actual params: [ 1.4527, -0.1308]
-Original Grad: -0.125, -lr * Pred Grad: 0.087, New P: 1.540
-Original Grad: 0.017, -lr * Pred Grad: -0.031, New P: -0.162
iter 11 loss: 0.648
Actual params: [ 1.5398, -0.1616]
-Original Grad: -0.129, -lr * Pred Grad: 0.072, New P: 1.612
-Original Grad: 0.048, -lr * Pred Grad: -0.026, New P: -0.187
iter 12 loss: 0.695
Actual params: [ 1.6118, -0.1872]
-Original Grad: -0.150, -lr * Pred Grad: -0.012, New P: 1.600
-Original Grad: 0.089, -lr * Pred Grad: -0.020, New P: -0.207
iter 13 loss: 0.692
Actual params: [ 1.5999, -0.2071]
-Original Grad: -0.132, -lr * Pred Grad: -0.022, New P: 1.578
-Original Grad: 0.097, -lr * Pred Grad: -0.007, New P: -0.214
iter 14 loss: 0.678
Actual params: [ 1.5777, -0.2145]
-Original Grad: -0.145, -lr * Pred Grad: -0.049, New P: 1.529
-Original Grad: 0.083, -lr * Pred Grad: 0.027, New P: -0.187
iter 15 loss: 0.647
Actual params: [ 1.5286, -0.187 ]
-Original Grad: -0.133, -lr * Pred Grad: -0.053, New P: 1.475
-Original Grad: 0.085, -lr * Pred Grad: 0.066, New P: -0.121
iter 16 loss: 0.611
Actual params: [ 1.4752, -0.1207]
-Original Grad: -0.112, -lr * Pred Grad: -0.056, New P: 1.419
-Original Grad: 0.034, -lr * Pred Grad: 0.048, New P: -0.073
iter 17 loss: 0.568
Actual params: [ 1.4188, -0.0728]
-Original Grad: -0.108, -lr * Pred Grad: -0.058, New P: 1.361
-Original Grad: 0.011, -lr * Pred Grad: 0.012, New P: -0.061
iter 18 loss: 0.539
Actual params: [ 1.3608, -0.0609]
-Original Grad: -0.090, -lr * Pred Grad: -0.058, New P: 1.303
-Original Grad: 0.050, -lr * Pred Grad: 0.037, New P: -0.024
iter 19 loss: 0.499
Actual params: [ 1.3026, -0.0244]
-Original Grad: -0.095, -lr * Pred Grad: -0.058, New P: 1.244
-Original Grad: 0.020, -lr * Pred Grad: 0.000, New P: -0.024
iter 20 loss: 0.459
Actual params: [ 1.2445, -0.0239]
-Original Grad: -0.036, -lr * Pred Grad: -0.055, New P: 1.189
-Original Grad: 0.015, -lr * Pred Grad: 0.014, New P: -0.010
Target params: [1.1812, 0.2779]
iter 0 loss: 0.863
Actual params: [0.5941, 0.5941]
-Original Grad: 0.376, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.763, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.818
Actual params: [0.6617, 0.5321]
-Original Grad: 0.452, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.485, -lr * Pred Grad: -0.081, New P: 0.452
iter 2 loss: 0.742
Actual params: [0.7468, 0.4516]
-Original Grad: 0.351, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.314, -lr * Pred Grad: -0.084, New P: 0.368
iter 3 loss: 0.635
Actual params: [0.8347, 0.3679]
-Original Grad: 0.228, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.114, -lr * Pred Grad: -0.084, New P: 0.284
iter 4 loss: 0.554
Actual params: [0.9229, 0.2837]
-Original Grad: 0.078, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.025, -lr * Pred Grad: -0.084, New P: 0.200
iter 5 loss: 0.483
Actual params: [1.0112, 0.1996]
-Original Grad: 0.041, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: -0.023, -lr * Pred Grad: -0.083, New P: 0.117
iter 6 loss: 0.471
Actual params: [1.0996, 0.117 ]
-Original Grad: -0.031, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: -0.019, -lr * Pred Grad: -0.075, New P: 0.042
iter 7 loss: 0.487
Actual params: [1.1879, 0.0418]
-Original Grad: -0.037, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: -0.002, -lr * Pred Grad: -0.070, New P: -0.028
iter 8 loss: 0.516
Actual params: [ 1.276 , -0.0283]
-Original Grad: -0.041, -lr * Pred Grad: 0.087, New P: 1.363
-Original Grad: -0.021, -lr * Pred Grad: -0.068, New P: -0.096
iter 9 loss: 0.546
Actual params: [ 1.3629, -0.0958]
-Original Grad: -0.041, -lr * Pred Grad: 0.074, New P: 1.437
-Original Grad: 0.019, -lr * Pred Grad: -0.057, New P: -0.153
iter 10 loss: 0.574
Actual params: [ 1.4366, -0.1532]
-Original Grad: -0.024, -lr * Pred Grad: 0.024, New P: 1.461
-Original Grad: 0.007, -lr * Pred Grad: -0.041, New P: -0.194
iter 11 loss: 0.579
Actual params: [ 1.4606, -0.1941]
-Original Grad: -0.030, -lr * Pred Grad: 0.022, New P: 1.483
-Original Grad: 0.033, -lr * Pred Grad: -0.030, New P: -0.225
iter 12 loss: 0.575
Actual params: [ 1.4831, -0.2246]
-Original Grad: -0.021, -lr * Pred Grad: 0.052, New P: 1.535
-Original Grad: 0.025, -lr * Pred Grad: -0.027, New P: -0.251
iter 13 loss: 0.589
Actual params: [ 1.5346, -0.2513]
-Original Grad: -0.020, -lr * Pred Grad: 0.057, New P: 1.592
-Original Grad: 0.024, -lr * Pred Grad: -0.024, New P: -0.276
iter 14 loss: 0.602
Actual params: [ 1.5918, -0.2757]
-Original Grad: -0.015, -lr * Pred Grad: 0.046, New P: 1.638
-Original Grad: 0.023, -lr * Pred Grad: -0.022, New P: -0.298
iter 15 loss: 0.610
Actual params: [ 1.638 , -0.2979]
-Original Grad: -0.013, -lr * Pred Grad: -0.002, New P: 1.636
-Original Grad: 0.028, -lr * Pred Grad: -0.009, New P: -0.307
iter 16 loss: 0.610
Actual params: [ 1.6356, -0.3073]
-Original Grad: -0.015, -lr * Pred Grad: -0.010, New P: 1.625
-Original Grad: 0.031, -lr * Pred Grad: 0.015, New P: -0.292
iter 17 loss: 0.608
Actual params: [ 1.6254, -0.2919]
-Original Grad: -0.017, -lr * Pred Grad: -0.026, New P: 1.599
-Original Grad: 0.031, -lr * Pred Grad: 0.012, New P: -0.280
iter 18 loss: 0.604
Actual params: [ 1.5993, -0.28  ]
-Original Grad: -0.017, -lr * Pred Grad: -0.024, New P: 1.576
-Original Grad: 0.026, -lr * Pred Grad: 0.005, New P: -0.275
iter 19 loss: 0.599
Actual params: [ 1.5758, -0.2749]
-Original Grad: -0.012, -lr * Pred Grad: -0.022, New P: 1.553
-Original Grad: 0.016, -lr * Pred Grad: -0.002, New P: -0.277
iter 20 loss: 0.594
Actual params: [ 1.5534, -0.2774]
-Original Grad: -0.019, -lr * Pred Grad: -0.024, New P: 1.529
-Original Grad: 0.025, -lr * Pred Grad: -0.002, New P: -0.280
Target params: [1.1812, 0.2779]
iter 0 loss: 0.885
Actual params: [0.5941, 0.5941]
-Original Grad: 0.225, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.307, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.831
Actual params: [0.6616, 0.5309]
-Original Grad: 0.321, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.540, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.730
Actual params: [0.7464, 0.4506]
-Original Grad: 0.411, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.250, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.589
Actual params: [0.8342, 0.3671]
-Original Grad: 0.337, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: -0.062, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.475
Actual params: [0.9225, 0.2829]
-Original Grad: 0.123, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.019, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.405
Actual params: [1.0108, 0.1991]
-Original Grad: 0.039, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 0.019, -lr * Pred Grad: -0.078, New P: 0.121
iter 6 loss: 0.382
Actual params: [1.0991, 0.1208]
-Original Grad: -0.002, -lr * Pred Grad: 0.088, New P: 1.187
-Original Grad: 0.059, -lr * Pred Grad: -0.071, New P: 0.050
iter 7 loss: 0.395
Actual params: [1.1874, 0.05  ]
-Original Grad: -0.018, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.039, -lr * Pred Grad: -0.067, New P: -0.017
iter 8 loss: 0.406
Actual params: [ 1.2757, -0.0168]
-Original Grad: -0.052, -lr * Pred Grad: 0.088, New P: 1.363
-Original Grad: 0.170, -lr * Pred Grad: -0.051, New P: -0.068
iter 9 loss: 0.441
Actual params: [ 1.3634, -0.0678]
-Original Grad: -0.060, -lr * Pred Grad: 0.080, New P: 1.443
-Original Grad: 0.078, -lr * Pred Grad: -0.035, New P: -0.103
iter 10 loss: 0.468
Actual params: [ 1.4432, -0.1028]
-Original Grad: -0.064, -lr * Pred Grad: -0.005, New P: 1.438
-Original Grad: 0.061, -lr * Pred Grad: -0.026, New P: -0.128
iter 11 loss: 0.468
Actual params: [ 1.4378, -0.1285]
-Original Grad: -0.057, -lr * Pred Grad: 0.003, New P: 1.441
-Original Grad: 0.097, -lr * Pred Grad: -0.017, New P: -0.146
iter 12 loss: 0.470
Actual params: [ 1.4413, -0.1456]
-Original Grad: -0.055, -lr * Pred Grad: 0.031, New P: 1.473
-Original Grad: 0.030, -lr * Pred Grad: 0.008, New P: -0.138
iter 13 loss: 0.481
Actual params: [ 1.4726, -0.138 ]
-Original Grad: -0.053, -lr * Pred Grad: 0.005, New P: 1.478
-Original Grad: 0.030, -lr * Pred Grad: 0.043, New P: -0.096
iter 14 loss: 0.479
Actual params: [ 1.4779, -0.0955]
-Original Grad: -0.062, -lr * Pred Grad: -0.027, New P: 1.451
-Original Grad: 0.027, -lr * Pred Grad: 0.002, New P: -0.094
iter 15 loss: 0.470
Actual params: [ 1.4505, -0.0935]
-Original Grad: -0.059, -lr * Pred Grad: -0.045, New P: 1.405
-Original Grad: 0.038, -lr * Pred Grad: 0.034, New P: -0.060
iter 16 loss: 0.451
Actual params: [ 1.4053, -0.0595]
-Original Grad: -0.062, -lr * Pred Grad: -0.049, New P: 1.356
-Original Grad: 0.133, -lr * Pred Grad: 0.067, New P: 0.007
iter 17 loss: 0.436
Actual params: [1.356 , 0.0075]
-Original Grad: -0.080, -lr * Pred Grad: -0.052, New P: 1.304
-Original Grad: 0.036, -lr * Pred Grad: 0.053, New P: 0.061
iter 18 loss: 0.416
Actual params: [1.3043, 0.0607]
-Original Grad: -0.075, -lr * Pred Grad: -0.053, New P: 1.252
-Original Grad: 0.108, -lr * Pred Grad: 0.072, New P: 0.133
iter 19 loss: 0.405
Actual params: [1.2516, 0.1329]
-Original Grad: -0.041, -lr * Pred Grad: -0.048, New P: 1.203
-Original Grad: 0.043, -lr * Pred Grad: 0.043, New P: 0.176
iter 20 loss: 0.400
Actual params: [1.2032, 0.1757]
-Original Grad: -0.026, -lr * Pred Grad: -0.038, New P: 1.166
-Original Grad: 0.049, -lr * Pred Grad: 0.056, New P: 0.231
Target params: [1.1812, 0.2779]
iter 0 loss: 0.338
Actual params: [0.5941, 0.5941]
-Original Grad: 0.090, -lr * Pred Grad: 0.064, New P: 0.658
-Original Grad: -0.219, -lr * Pred Grad: -0.063, New P: 0.532
iter 1 loss: 0.311
Actual params: [0.6584, 0.5315]
-Original Grad: 0.033, -lr * Pred Grad: 0.076, New P: 0.735
-Original Grad: -0.184, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.283
Actual params: [0.7349, 0.4511]
-Original Grad: 0.026, -lr * Pred Grad: 0.006, New P: 0.741
-Original Grad: -0.088, -lr * Pred Grad: -0.083, New P: 0.368
iter 3 loss: 0.279
Actual params: [0.7408, 0.3677]
-Original Grad: 0.028, -lr * Pred Grad: 0.027, New P: 0.767
-Original Grad: -0.037, -lr * Pred Grad: -0.082, New P: 0.286
iter 4 loss: 0.274
Actual params: [0.7674, 0.2857]
-Original Grad: 0.018, -lr * Pred Grad: 0.001, New P: 0.768
-Original Grad: -0.045, -lr * Pred Grad: -0.076, New P: 0.210
iter 5 loss: 0.278
Actual params: [0.7681, 0.2101]
-Original Grad: 0.033, -lr * Pred Grad: 0.039, New P: 0.807
-Original Grad: -0.003, -lr * Pred Grad: -0.070, New P: 0.140
iter 6 loss: 0.275
Actual params: [0.8071, 0.14  ]
-Original Grad: 0.019, -lr * Pred Grad: -0.004, New P: 0.803
-Original Grad: 0.025, -lr * Pred Grad: -0.066, New P: 0.073
iter 7 loss: 0.281
Actual params: [0.803 , 0.0735]
-Original Grad: 0.027, -lr * Pred Grad: 0.026, New P: 0.829
-Original Grad: 0.051, -lr * Pred Grad: -0.050, New P: 0.023
iter 8 loss: 0.281
Actual params: [0.8289, 0.0234]
-Original Grad: 0.014, -lr * Pred Grad: -0.012, New P: 0.817
-Original Grad: 0.014, -lr * Pred Grad: -0.034, New P: -0.011
iter 9 loss: 0.284
Actual params: [ 0.8173, -0.0106]
-Original Grad: 0.041, -lr * Pred Grad: 0.028, New P: 0.845
-Original Grad: -0.002, -lr * Pred Grad: -0.028, New P: -0.039
iter 10 loss: 0.282
Actual params: [ 0.8451, -0.039 ]
-Original Grad: 0.012, -lr * Pred Grad: -0.004, New P: 0.841
-Original Grad: 0.029, -lr * Pred Grad: -0.022, New P: -0.061
iter 11 loss: 0.282
Actual params: [ 0.8411, -0.0606]
-Original Grad: 0.009, -lr * Pred Grad: 0.004, New P: 0.845
-Original Grad: 0.027, -lr * Pred Grad: -0.000, New P: -0.061
iter 12 loss: 0.282
Actual params: [ 0.8453, -0.061 ]
-Original Grad: 0.020, -lr * Pred Grad: -0.002, New P: 0.843
-Original Grad: 0.028, -lr * Pred Grad: 0.023, New P: -0.038
iter 13 loss: 0.282
Actual params: [ 0.8435, -0.0381]
-Original Grad: 0.006, -lr * Pred Grad: -0.003, New P: 0.840
-Original Grad: 0.033, -lr * Pred Grad: 0.007, New P: -0.031
iter 14 loss: 0.282
Actual params: [ 0.8403, -0.0312]
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 0.833
-Original Grad: 0.015, -lr * Pred Grad: 0.007, New P: -0.025
iter 15 loss: 0.282
Actual params: [ 0.8334, -0.0245]
-Original Grad: -0.001, -lr * Pred Grad: -0.011, New P: 0.822
-Original Grad: 0.014, -lr * Pred Grad: -0.006, New P: -0.031
iter 16 loss: 0.284
Actual params: [ 0.822 , -0.0309]
-Original Grad: 0.015, -lr * Pred Grad: -0.005, New P: 0.817
-Original Grad: 0.013, -lr * Pred Grad: -0.007, New P: -0.037
iter 17 loss: 0.284
Actual params: [ 0.8171, -0.0374]
-Original Grad: 0.023, -lr * Pred Grad: 0.005, New P: 0.823
-Original Grad: 0.004, -lr * Pred Grad: -0.014, New P: -0.051
iter 18 loss: 0.284
Actual params: [ 0.8226, -0.0511]
-Original Grad: 0.014, -lr * Pred Grad: 0.005, New P: 0.828
-Original Grad: 0.029, -lr * Pred Grad: -0.004, New P: -0.055
iter 19 loss: 0.284
Actual params: [ 0.8277, -0.0546]
-Original Grad: 0.006, -lr * Pred Grad: -0.002, New P: 0.826
-Original Grad: 0.018, -lr * Pred Grad: 0.001, New P: -0.054
iter 20 loss: 0.284
Actual params: [ 0.8258, -0.0537]
-Original Grad: 0.020, -lr * Pred Grad: 0.001, New P: 0.826
-Original Grad: 0.016, -lr * Pred Grad: -0.001, New P: -0.055
Target params: [1.1812, 0.2779]
iter 0 loss: 0.605
Actual params: [0.5941, 0.5941]
-Original Grad: 0.332, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: 0.000, -lr * Pred Grad: 0.035, New P: 0.629
iter 1 loss: 0.585
Actual params: [0.6617, 0.6292]
-Original Grad: 0.235, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.104, -lr * Pred Grad: -0.050, New P: 0.579
iter 2 loss: 0.517
Actual params: [0.7467, 0.5787]
-Original Grad: 0.257, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: 0.030, -lr * Pred Grad: -0.067, New P: 0.512
iter 3 loss: 0.430
Actual params: [0.8345, 0.5121]
-Original Grad: 0.160, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.028, -lr * Pred Grad: -0.064, New P: 0.448
iter 4 loss: 0.332
Actual params: [0.9227, 0.4481]
-Original Grad: 0.118, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.025, -lr * Pred Grad: -0.055, New P: 0.393
iter 5 loss: 0.271
Actual params: [1.011 , 0.3934]
-Original Grad: 0.024, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 0.017, -lr * Pred Grad: -0.027, New P: 0.366
iter 6 loss: 0.259
Actual params: [1.0993, 0.3663]
-Original Grad: -0.022, -lr * Pred Grad: 0.088, New P: 1.187
-Original Grad: 0.005, -lr * Pred Grad: 0.001, New P: 0.367
iter 7 loss: 0.263
Actual params: [1.1874, 0.3673]
-Original Grad: -0.040, -lr * Pred Grad: 0.087, New P: 1.275
-Original Grad: 0.047, -lr * Pred Grad: 0.021, New P: 0.388
iter 8 loss: 0.284
Actual params: [1.2746, 0.3882]
-Original Grad: -0.052, -lr * Pred Grad: 0.075, New P: 1.350
-Original Grad: 0.022, -lr * Pred Grad: 0.018, New P: 0.406
iter 9 loss: 0.306
Actual params: [1.3499, 0.4061]
-Original Grad: -0.060, -lr * Pred Grad: -0.016, New P: 1.334
-Original Grad: 0.018, -lr * Pred Grad: 0.010, New P: 0.416
iter 10 loss: 0.296
Actual params: [1.334 , 0.4159]
-Original Grad: -0.035, -lr * Pred Grad: 0.015, New P: 1.349
-Original Grad: 0.012, -lr * Pred Grad: -0.001, New P: 0.415
iter 11 loss: 0.302
Actual params: [1.349, 0.415]
-Original Grad: -0.032, -lr * Pred Grad: 0.047, New P: 1.396
-Original Grad: 0.011, -lr * Pred Grad: -0.004, New P: 0.411
iter 12 loss: 0.315
Actual params: [1.3962, 0.4108]
-Original Grad: -0.054, -lr * Pred Grad: -0.023, New P: 1.373
-Original Grad: 0.035, -lr * Pred Grad: 0.010, New P: 0.421
iter 13 loss: 0.312
Actual params: [1.3734, 0.421 ]
-Original Grad: -0.037, -lr * Pred Grad: -0.027, New P: 1.346
-Original Grad: 0.026, -lr * Pred Grad: 0.015, New P: 0.436
iter 14 loss: 0.294
Actual params: [1.3464, 0.4362]
-Original Grad: -0.035, -lr * Pred Grad: -0.039, New P: 1.308
-Original Grad: 0.002, -lr * Pred Grad: -0.002, New P: 0.434
iter 15 loss: 0.278
Actual params: [1.3078, 0.434 ]
-Original Grad: -0.040, -lr * Pred Grad: -0.038, New P: 1.270
-Original Grad: -0.004, -lr * Pred Grad: -0.014, New P: 0.420
iter 16 loss: 0.271
Actual params: [1.2697, 0.42  ]
-Original Grad: -0.013, -lr * Pred Grad: -0.027, New P: 1.242
-Original Grad: 0.002, -lr * Pred Grad: -0.015, New P: 0.405
iter 17 loss: 0.266
Actual params: [1.2424, 0.4055]
-Original Grad: -0.048, -lr * Pred Grad: -0.033, New P: 1.210
-Original Grad: 0.002, -lr * Pred Grad: -0.011, New P: 0.394
iter 18 loss: 0.266
Actual params: [1.2099, 0.3943]
-Original Grad: -0.030, -lr * Pred Grad: -0.034, New P: 1.176
-Original Grad: 0.023, -lr * Pred Grad: 0.001, New P: 0.395
iter 19 loss: 0.260
Actual params: [1.1764, 0.3952]
-Original Grad: -0.012, -lr * Pred Grad: -0.026, New P: 1.150
-Original Grad: -0.005, -lr * Pred Grad: -0.004, New P: 0.391
iter 20 loss: 0.256
Actual params: [1.1502, 0.391 ]
-Original Grad: -0.040, -lr * Pred Grad: -0.028, New P: 1.122
-Original Grad: 0.006, -lr * Pred Grad: -0.008, New P: 0.383
Target params: [1.1812, 0.2779]
iter 0 loss: 0.789
Actual params: [0.5941, 0.5941]
-Original Grad: 0.738, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.067, -lr * Pred Grad: -0.029, New P: 0.565
iter 1 loss: 0.748
Actual params: [0.6617, 0.5652]
-Original Grad: 0.671, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.092, -lr * Pred Grad: -0.071, New P: 0.494
iter 2 loss: 0.676
Actual params: [0.7469, 0.4943]
-Original Grad: 0.525, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.058, -lr * Pred Grad: -0.075, New P: 0.420
iter 3 loss: 0.549
Actual params: [0.8348, 0.4197]
-Original Grad: 0.422, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.125, -lr * Pred Grad: -0.066, New P: 0.353
iter 4 loss: 0.254
Actual params: [0.9231, 0.3534]
-Original Grad: -0.164, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.884, -lr * Pred Grad: -0.045, New P: 0.309
iter 5 loss: 1.250
Actual params: [1.0114, 0.3087]
-Original Grad: -1.537, -lr * Pred Grad: 0.086, New P: 1.098
-Original Grad: 3.170, -lr * Pred Grad: 0.010, New P: 0.319
iter 6 loss: 1.307
Actual params: [1.0976, 0.3189]
-Original Grad: 0.280, -lr * Pred Grad: 0.088, New P: 1.186
-Original Grad: 1.823, -lr * Pred Grad: 0.064, New P: 0.383
iter 7 loss: 1.320
Actual params: [1.1856, 0.3827]
-Original Grad: 0.099, -lr * Pred Grad: 0.088, New P: 1.274
-Original Grad: 0.690, -lr * Pred Grad: 0.084, New P: 0.466
iter 8 loss: 0.352
Actual params: [1.2736, 0.4664]
-Original Grad: 0.059, -lr * Pred Grad: 0.088, New P: 1.362
-Original Grad: -0.002, -lr * Pred Grad: 0.088, New P: 0.554
iter 9 loss: 0.504
Actual params: [1.3617, 0.5541]
-Original Grad: 0.035, -lr * Pred Grad: 0.088, New P: 1.450
-Original Grad: -0.066, -lr * Pred Grad: 0.088, New P: 0.642
iter 10 loss: 0.559
Actual params: [1.4495, 0.6423]
-Original Grad: -0.007, -lr * Pred Grad: 0.087, New P: 1.536
-Original Grad: -0.089, -lr * Pred Grad: 0.088, New P: 0.731
iter 11 loss: 0.593
Actual params: [1.5361, 0.7306]
-Original Grad: -0.043, -lr * Pred Grad: 0.080, New P: 1.616
-Original Grad: -0.075, -lr * Pred Grad: 0.088, New P: 0.819
iter 12 loss: 0.620
Actual params: [1.6162, 0.819 ]
-Original Grad: -0.025, -lr * Pred Grad: 0.045, New P: 1.662
-Original Grad: -0.051, -lr * Pred Grad: 0.088, New P: 0.907
iter 13 loss: 0.646
Actual params: [1.6616, 0.9073]
-Original Grad: -0.030, -lr * Pred Grad: 0.048, New P: 1.710
-Original Grad: -0.003, -lr * Pred Grad: 0.088, New P: 0.996
iter 14 loss: 0.667
Actual params: [1.7096, 0.9956]
-Original Grad: -0.024, -lr * Pred Grad: 0.054, New P: 1.763
-Original Grad: -0.082, -lr * Pred Grad: 0.088, New P: 1.084
iter 15 loss: 0.691
Actual params: [1.7632, 1.0839]
-Original Grad: -0.049, -lr * Pred Grad: 0.019, New P: 1.782
-Original Grad: -0.081, -lr * Pred Grad: 0.088, New P: 1.172
iter 16 loss: 0.719
Actual params: [1.7817, 1.1722]
-Original Grad: -0.057, -lr * Pred Grad: -0.005, New P: 1.776
-Original Grad: -0.104, -lr * Pred Grad: 0.088, New P: 1.260
iter 17 loss: 0.744
Actual params: [1.7764, 1.2602]
-Original Grad: -0.067, -lr * Pred Grad: -0.042, New P: 1.735
-Original Grad: -0.127, -lr * Pred Grad: 0.084, New P: 1.344
iter 18 loss: 1.111
Actual params: [1.7346, 1.3442]
-Original Grad: -0.050, -lr * Pred Grad: -0.048, New P: 1.687
-Original Grad: -0.172, -lr * Pred Grad: 0.011, New P: 1.355
iter 19 loss: 1.163
Actual params: [1.6868, 1.3548]
-Original Grad: -0.037, -lr * Pred Grad: -0.043, New P: 1.643
-Original Grad: -0.141, -lr * Pred Grad: -0.015, New P: 1.340
iter 20 loss: 1.212
Actual params: [1.6434, 1.3399]
-Original Grad: -0.071, -lr * Pred Grad: -0.046, New P: 1.597
-Original Grad: -0.146, -lr * Pred Grad: -0.043, New P: 1.297
Target params: [1.1812, 0.2779]
iter 0 loss: 0.743
Actual params: [0.5941, 0.5941]
-Original Grad: 0.068, -lr * Pred Grad: 0.062, New P: 0.656
-Original Grad: -0.601, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.511
Actual params: [0.656 , 0.5316]
-Original Grad: 0.052, -lr * Pred Grad: 0.074, New P: 0.730
-Original Grad: -0.452, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.322
Actual params: [0.7298, 0.4511]
-Original Grad: 0.045, -lr * Pred Grad: 0.017, New P: 0.747
-Original Grad: -0.118, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.261
Actual params: [0.747 , 0.3674]
-Original Grad: 0.061, -lr * Pred Grad: 0.052, New P: 0.799
-Original Grad: -0.041, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.233
Actual params: [0.7988, 0.2834]
-Original Grad: 0.028, -lr * Pred Grad: 0.013, New P: 0.811
-Original Grad: 0.000, -lr * Pred Grad: -0.083, New P: 0.201
iter 5 loss: 0.218
Actual params: [0.8115, 0.2007]
-Original Grad: 0.025, -lr * Pred Grad: 0.044, New P: 0.856
-Original Grad: 0.013, -lr * Pred Grad: -0.075, New P: 0.126
iter 6 loss: 0.212
Actual params: [0.8557, 0.126 ]
-Original Grad: 0.028, -lr * Pred Grad: -0.004, New P: 0.852
-Original Grad: 0.019, -lr * Pred Grad: -0.070, New P: 0.056
iter 7 loss: 0.209
Actual params: [0.8515, 0.0562]
-Original Grad: 0.034, -lr * Pred Grad: 0.036, New P: 0.888
-Original Grad: 0.035, -lr * Pred Grad: -0.064, New P: -0.008
iter 8 loss: 0.210
Actual params: [ 0.8875, -0.0081]
-Original Grad: 0.030, -lr * Pred Grad: -0.002, New P: 0.886
-Original Grad: 0.037, -lr * Pred Grad: -0.046, New P: -0.054
iter 9 loss: 0.212
Actual params: [ 0.8856, -0.0536]
-Original Grad: 0.038, -lr * Pred Grad: 0.036, New P: 0.921
-Original Grad: 0.056, -lr * Pred Grad: -0.031, New P: -0.085
iter 10 loss: 0.213
Actual params: [ 0.9214, -0.0849]
-Original Grad: 0.043, -lr * Pred Grad: 0.014, New P: 0.935
-Original Grad: 0.061, -lr * Pred Grad: -0.024, New P: -0.109
iter 11 loss: 0.215
Actual params: [ 0.9352, -0.109 ]
-Original Grad: 0.042, -lr * Pred Grad: 0.040, New P: 0.975
-Original Grad: 0.077, -lr * Pred Grad: -0.016, New P: -0.125
iter 12 loss: 0.212
Actual params: [ 0.9749, -0.1252]
-Original Grad: 0.043, -lr * Pred Grad: 0.019, New P: 0.994
-Original Grad: 0.094, -lr * Pred Grad: 0.008, New P: -0.118
iter 13 loss: 0.208
Actual params: [ 0.9935, -0.1176]
-Original Grad: 0.033, -lr * Pred Grad: 0.032, New P: 1.025
-Original Grad: 0.075, -lr * Pred Grad: 0.050, New P: -0.068
iter 14 loss: 0.202
Actual params: [ 1.0252, -0.068 ]
-Original Grad: 0.023, -lr * Pred Grad: 0.004, New P: 1.029
-Original Grad: 0.043, -lr * Pred Grad: 0.061, New P: -0.007
iter 15 loss: 0.204
Actual params: [ 1.0288, -0.0072]
-Original Grad: 0.027, -lr * Pred Grad: 0.017, New P: 1.046
-Original Grad: 0.043, -lr * Pred Grad: 0.023, New P: 0.016
iter 16 loss: 0.205
Actual params: [1.0462, 0.0155]
-Original Grad: 0.016, -lr * Pred Grad: 0.000, New P: 1.046
-Original Grad: 0.028, -lr * Pred Grad: 0.042, New P: 0.057
iter 17 loss: 0.209
Actual params: [1.0463, 0.0573]
-Original Grad: 0.016, -lr * Pred Grad: 0.003, New P: 1.050
-Original Grad: 0.023, -lr * Pred Grad: -0.008, New P: 0.049
iter 18 loss: 0.208
Actual params: [1.0496, 0.0493]
-Original Grad: 0.015, -lr * Pred Grad: 0.000, New P: 1.050
-Original Grad: 0.019, -lr * Pred Grad: 0.023, New P: 0.072
iter 19 loss: 0.213
Actual params: [1.0501, 0.0721]
-Original Grad: 0.012, -lr * Pred Grad: -0.001, New P: 1.050
-Original Grad: 0.023, -lr * Pred Grad: -0.008, New P: 0.064
iter 20 loss: 0.211
Actual params: [1.0495, 0.0644]
-Original Grad: 0.013, -lr * Pred Grad: -0.001, New P: 1.049
-Original Grad: 0.023, -lr * Pred Grad: 0.016, New P: 0.080
Target params: [1.1812, 0.2779]
iter 0 loss: 0.596
Actual params: [0.5941, 0.5941]
-Original Grad: 0.511, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: 0.145, -lr * Pred Grad: 0.067, New P: 0.661
iter 1 loss: 0.464
Actual params: [0.6617, 0.6609]
-Original Grad: 0.387, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: 0.009, -lr * Pred Grad: 0.082, New P: 0.743
iter 2 loss: 0.317
Actual params: [0.7469, 0.7426]
-Original Grad: -0.095, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.100, -lr * Pred Grad: -0.025, New P: 0.717
iter 3 loss: 0.370
Actual params: [0.8347, 0.7172]
-Original Grad: -0.188, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.231, -lr * Pred Grad: -0.062, New P: 0.655
iter 4 loss: 0.368
Actual params: [0.9226, 0.6548]
-Original Grad: -0.222, -lr * Pred Grad: 0.077, New P: 1.000
-Original Grad: -0.291, -lr * Pred Grad: -0.075, New P: 0.579
iter 5 loss: 0.305
Actual params: [0.9999, 0.5794]
-Original Grad: -0.073, -lr * Pred Grad: -0.027, New P: 0.972
-Original Grad: -0.237, -lr * Pred Grad: -0.081, New P: 0.499
iter 6 loss: 0.259
Actual params: [0.9725, 0.4987]
-Original Grad: -0.052, -lr * Pred Grad: -0.033, New P: 0.939
-Original Grad: -0.147, -lr * Pred Grad: -0.082, New P: 0.417
iter 7 loss: 0.283
Actual params: [0.939 , 0.4169]
-Original Grad: -0.023, -lr * Pred Grad: 0.021, New P: 0.960
-Original Grad: -0.068, -lr * Pred Grad: -0.079, New P: 0.338
iter 8 loss: 0.314
Actual params: [0.9599, 0.3381]
-Original Grad: 0.035, -lr * Pred Grad: 0.051, New P: 1.011
-Original Grad: 0.029, -lr * Pred Grad: -0.071, New P: 0.267
iter 9 loss: 0.333
Actual params: [1.0111, 0.2668]
-Original Grad: 0.036, -lr * Pred Grad: 0.017, New P: 1.028
-Original Grad: 0.100, -lr * Pred Grad: -0.065, New P: 0.201
iter 10 loss: 0.401
Actual params: [1.0283, 0.2013]
-Original Grad: 0.042, -lr * Pred Grad: 0.043, New P: 1.072
-Original Grad: 0.201, -lr * Pred Grad: -0.048, New P: 0.154
iter 11 loss: 0.502
Actual params: [1.0716, 0.1536]
-Original Grad: 0.028, -lr * Pred Grad: -0.002, New P: 1.070
-Original Grad: 0.268, -lr * Pred Grad: -0.033, New P: 0.120
iter 12 loss: 1.138
Actual params: [1.0698, 0.1204]
-Original Grad: 0.014, -lr * Pred Grad: 0.022, New P: 1.092
-Original Grad: 0.302, -lr * Pred Grad: -0.024, New P: 0.096
iter 13 loss: 1.191
Actual params: [1.092 , 0.0965]
-Original Grad: -0.010, -lr * Pred Grad: -0.024, New P: 1.068
-Original Grad: 0.383, -lr * Pred Grad: -0.011, New P: 0.085
iter 14 loss: 1.207
Actual params: [1.0677, 0.085 ]
-Original Grad: 0.020, -lr * Pred Grad: 0.003, New P: 1.071
-Original Grad: 0.389, -lr * Pred Grad: 0.015, New P: 0.100
iter 15 loss: 1.184
Actual params: [1.0709, 0.1001]
-Original Grad: 0.060, -lr * Pred Grad: 0.031, New P: 1.102
-Original Grad: 0.450, -lr * Pred Grad: 0.060, New P: 0.160
iter 16 loss: 0.439
Actual params: [1.1022, 0.1601]
-Original Grad: 0.003, -lr * Pred Grad: 0.003, New P: 1.105
-Original Grad: 0.269, -lr * Pred Grad: 0.083, New P: 0.243
iter 17 loss: 0.323
Actual params: [1.105 , 0.2431]
-Original Grad: 0.003, -lr * Pred Grad: -0.005, New P: 1.100
-Original Grad: 0.164, -lr * Pred Grad: 0.088, New P: 0.331
iter 18 loss: 0.266
Actual params: [1.1003, 0.3306]
-Original Grad: 0.000, -lr * Pred Grad: -0.015, New P: 1.086
-Original Grad: 0.009, -lr * Pred Grad: 0.088, New P: 0.419
iter 19 loss: 0.247
Actual params: [1.0858, 0.4188]
-Original Grad: 0.006, -lr * Pred Grad: -0.010, New P: 1.076
-Original Grad: -0.086, -lr * Pred Grad: 0.088, New P: 0.507
iter 20 loss: 0.261
Actual params: [1.0762, 0.5069]
-Original Grad: -0.041, -lr * Pred Grad: -0.028, New P: 1.049
-Original Grad: -0.217, -lr * Pred Grad: 0.020, New P: 0.527
Target params: [1.1812, 0.2779]
iter 0 loss: 0.722
Actual params: [0.5941, 0.5941]
-Original Grad: 0.229, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.406, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.685
Actual params: [0.6616, 0.531 ]
-Original Grad: 0.116, -lr * Pred Grad: 0.084, New P: 0.746
-Original Grad: -0.380, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.602
Actual params: [0.746 , 0.4506]
-Original Grad: 0.154, -lr * Pred Grad: 0.087, New P: 0.833
-Original Grad: -0.361, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.433
Actual params: [0.8333, 0.3669]
-Original Grad: 0.284, -lr * Pred Grad: 0.088, New P: 0.921
-Original Grad: -0.377, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.255
Actual params: [0.9213, 0.2827]
-Original Grad: 0.091, -lr * Pred Grad: 0.088, New P: 1.009
-Original Grad: -0.079, -lr * Pred Grad: -0.084, New P: 0.198
iter 5 loss: 0.165
Actual params: [1.0094, 0.1984]
-Original Grad: 0.035, -lr * Pred Grad: 0.087, New P: 1.097
-Original Grad: 0.023, -lr * Pred Grad: -0.084, New P: 0.115
iter 6 loss: 0.178
Actual params: [1.0966, 0.1145]
-Original Grad: 0.058, -lr * Pred Grad: 0.070, New P: 1.167
-Original Grad: 0.045, -lr * Pred Grad: -0.078, New P: 0.037
iter 7 loss: 0.182
Actual params: [1.1666, 0.037 ]
-Original Grad: 0.112, -lr * Pred Grad: 0.072, New P: 1.238
-Original Grad: 0.044, -lr * Pred Grad: -0.071, New P: -0.034
iter 8 loss: 0.172
Actual params: [ 1.2382, -0.0336]
-Original Grad: 0.069, -lr * Pred Grad: 0.075, New P: 1.313
-Original Grad: 0.019, -lr * Pred Grad: -0.067, New P: -0.100
iter 9 loss: 0.164
Actual params: [ 1.3132, -0.1005]
-Original Grad: 0.050, -lr * Pred Grad: 0.065, New P: 1.379
-Original Grad: 0.008, -lr * Pred Grad: -0.052, New P: -0.152
iter 10 loss: 0.167
Actual params: [ 1.3787, -0.1523]
-Original Grad: 0.034, -lr * Pred Grad: 0.047, New P: 1.425
-Original Grad: -0.003, -lr * Pred Grad: -0.039, New P: -0.191
iter 11 loss: 0.171
Actual params: [ 1.4254, -0.1909]
-Original Grad: 0.019, -lr * Pred Grad: 0.024, New P: 1.449
-Original Grad: -0.009, -lr * Pred Grad: -0.036, New P: -0.227
iter 12 loss: 0.168
Actual params: [ 1.4492, -0.2269]
-Original Grad: 0.026, -lr * Pred Grad: 0.017, New P: 1.466
-Original Grad: -0.004, -lr * Pred Grad: -0.036, New P: -0.263
iter 13 loss: 0.168
Actual params: [ 1.4658, -0.2628]
-Original Grad: 0.008, -lr * Pred Grad: -0.006, New P: 1.460
-Original Grad: 0.005, -lr * Pred Grad: -0.031, New P: -0.293
iter 14 loss: 0.167
Actual params: [ 1.4598, -0.2933]
-Original Grad: 0.010, -lr * Pred Grad: -0.005, New P: 1.455
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: -0.313
iter 15 loss: 0.163
Actual params: [ 1.4547, -0.3131]
-Original Grad: 0.019, -lr * Pred Grad: -0.006, New P: 1.449
-Original Grad: 0.015, -lr * Pred Grad: -0.003, New P: -0.316
iter 16 loss: 0.162
Actual params: [ 1.4491, -0.316 ]
-Original Grad: 0.019, -lr * Pred Grad: 0.000, New P: 1.449
-Original Grad: 0.021, -lr * Pred Grad: -0.001, New P: -0.317
iter 17 loss: 0.162
Actual params: [ 1.4495, -0.3173]
-Original Grad: 0.027, -lr * Pred Grad: 0.006, New P: 1.456
-Original Grad: 0.001, -lr * Pred Grad: -0.012, New P: -0.329
iter 18 loss: 0.160
Actual params: [ 1.4558, -0.3295]
-Original Grad: 0.016, -lr * Pred Grad: 0.003, New P: 1.459
-Original Grad: 0.005, -lr * Pred Grad: -0.017, New P: -0.347
iter 19 loss: 0.157
Actual params: [ 1.459 , -0.3468]
-Original Grad: 0.018, -lr * Pred Grad: 0.002, New P: 1.461
-Original Grad: 0.011, -lr * Pred Grad: -0.016, New P: -0.363
iter 20 loss: 0.154
Actual params: [ 1.4613, -0.3631]
-Original Grad: 0.020, -lr * Pred Grad: 0.004, New P: 1.465
-Original Grad: 0.025, -lr * Pred Grad: -0.007, New P: -0.370
Target params: [1.1812, 0.2779]
iter 0 loss: 0.519
Actual params: [0.5941, 0.5941]
-Original Grad: 0.098, -lr * Pred Grad: 0.065, New P: 0.659
-Original Grad: -0.172, -lr * Pred Grad: -0.061, New P: 0.533
iter 1 loss: 0.509
Actual params: [0.6591, 0.5333]
-Original Grad: 0.183, -lr * Pred Grad: 0.084, New P: 0.743
-Original Grad: -0.089, -lr * Pred Grad: -0.080, New P: 0.454
iter 2 loss: 0.504
Actual params: [0.7426, 0.4536]
-Original Grad: 0.149, -lr * Pred Grad: 0.087, New P: 0.829
-Original Grad: 0.038, -lr * Pred Grad: -0.076, New P: 0.378
iter 3 loss: 0.494
Actual params: [0.8291, 0.3775]
-Original Grad: 0.013, -lr * Pred Grad: 0.083, New P: 0.912
-Original Grad: 0.011, -lr * Pred Grad: -0.070, New P: 0.308
iter 4 loss: 0.483
Actual params: [0.9122, 0.3076]
-Original Grad: -0.014, -lr * Pred Grad: 0.001, New P: 0.913
-Original Grad: 0.013, -lr * Pred Grad: -0.064, New P: 0.243
iter 5 loss: 0.463
Actual params: [0.9132, 0.2433]
-Original Grad: 0.027, -lr * Pred Grad: 0.040, New P: 0.954
-Original Grad: 0.052, -lr * Pred Grad: -0.044, New P: 0.200
iter 6 loss: 0.455
Actual params: [0.9537, 0.1998]
-Original Grad: 0.019, -lr * Pred Grad: 0.009, New P: 0.963
-Original Grad: 0.051, -lr * Pred Grad: -0.017, New P: 0.182
iter 7 loss: 0.451
Actual params: [0.9625, 0.1823]
-Original Grad: -0.006, -lr * Pred Grad: 0.006, New P: 0.969
-Original Grad: 0.069, -lr * Pred Grad: 0.022, New P: 0.204
iter 8 loss: 0.456
Actual params: [0.9688, 0.2039]
-Original Grad: 0.027, -lr * Pred Grad: -0.004, New P: 0.964
-Original Grad: 0.084, -lr * Pred Grad: 0.060, New P: 0.264
iter 9 loss: 0.473
Actual params: [0.9644, 0.2638]
-Original Grad: -0.012, -lr * Pred Grad: -0.017, New P: 0.947
-Original Grad: 0.004, -lr * Pred Grad: 0.019, New P: 0.283
iter 10 loss: 0.477
Actual params: [0.947 , 0.2832]
-Original Grad: 0.001, -lr * Pred Grad: -0.017, New P: 0.930
-Original Grad: 0.035, -lr * Pred Grad: 0.033, New P: 0.316
iter 11 loss: 0.486
Actual params: [0.9296, 0.3158]
-Original Grad: -0.022, -lr * Pred Grad: -0.025, New P: 0.904
-Original Grad: 0.004, -lr * Pred Grad: -0.016, New P: 0.300
iter 12 loss: 0.479
Actual params: [0.9044, 0.2996]
-Original Grad: -0.029, -lr * Pred Grad: -0.031, New P: 0.874
-Original Grad: 0.033, -lr * Pred Grad: 0.023, New P: 0.323
iter 13 loss: 0.483
Actual params: [0.8737, 0.323 ]
-Original Grad: 0.002, -lr * Pred Grad: -0.022, New P: 0.852
-Original Grad: 0.019, -lr * Pred Grad: -0.001, New P: 0.322
iter 14 loss: 0.485
Actual params: [0.8522, 0.3217]
-Original Grad: 0.052, -lr * Pred Grad: 0.008, New P: 0.860
-Original Grad: 0.046, -lr * Pred Grad: 0.032, New P: 0.354
iter 15 loss: 0.489
Actual params: [0.8601, 0.354 ]
-Original Grad: 0.001, -lr * Pred Grad: 0.010, New P: 0.870
-Original Grad: 0.001, -lr * Pred Grad: -0.010, New P: 0.344
iter 16 loss: 0.488
Actual params: [0.8698, 0.3437]
-Original Grad: -0.003, -lr * Pred Grad: -0.009, New P: 0.861
-Original Grad: 0.009, -lr * Pred Grad: 0.002, New P: 0.345
iter 17 loss: 0.487
Actual params: [0.8609, 0.3452]
-Original Grad: 0.003, -lr * Pred Grad: -0.014, New P: 0.846
-Original Grad: 0.015, -lr * Pred Grad: -0.006, New P: 0.339
iter 18 loss: 0.488
Actual params: [0.8465, 0.3389]
-Original Grad: 0.024, -lr * Pred Grad: -0.001, New P: 0.845
-Original Grad: 0.023, -lr * Pred Grad: 0.008, New P: 0.347
iter 19 loss: 0.489
Actual params: [0.8451, 0.3465]
-Original Grad: 0.013, -lr * Pred Grad: 0.004, New P: 0.850
-Original Grad: 0.014, -lr * Pred Grad: 0.002, New P: 0.348
iter 20 loss: 0.489
Actual params: [0.8495, 0.3485]
-Original Grad: 0.007, -lr * Pred Grad: -0.002, New P: 0.848
-Original Grad: 0.013, -lr * Pred Grad: -0.000, New P: 0.348
Target params: [1.1812, 0.2779]
iter 0 loss: 0.729
Actual params: [0.5941, 0.5941]
-Original Grad: -0.093, -lr * Pred Grad: -0.045, New P: 0.549
-Original Grad: -0.481, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.648
Actual params: [0.5491, 0.5312]
-Original Grad: -0.082, -lr * Pred Grad: -0.074, New P: 0.475
-Original Grad: -0.333, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.541
Actual params: [0.4751, 0.4507]
-Original Grad: -0.044, -lr * Pred Grad: -0.074, New P: 0.402
-Original Grad: -0.225, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.457
Actual params: [0.4015, 0.3671]
-Original Grad: -0.008, -lr * Pred Grad: -0.069, New P: 0.332
-Original Grad: -0.212, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.394
Actual params: [0.3324, 0.2829]
-Original Grad: 0.024, -lr * Pred Grad: -0.063, New P: 0.270
-Original Grad: -0.104, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.372
Actual params: [0.2696, 0.1985]
-Original Grad: 0.075, -lr * Pred Grad: -0.040, New P: 0.230
-Original Grad: -0.042, -lr * Pred Grad: -0.084, New P: 0.115
iter 6 loss: 0.353
Actual params: [0.2297, 0.1145]
-Original Grad: 0.080, -lr * Pred Grad: -0.012, New P: 0.218
-Original Grad: -0.040, -lr * Pred Grad: -0.082, New P: 0.033
iter 7 loss: 0.348
Actual params: [0.2178, 0.033 ]
-Original Grad: 0.056, -lr * Pred Grad: 0.030, New P: 0.247
-Original Grad: -0.037, -lr * Pred Grad: -0.074, New P: -0.041
iter 8 loss: 0.340
Actual params: [ 0.2475, -0.0411]
-Original Grad: 0.073, -lr * Pred Grad: 0.063, New P: 0.311
-Original Grad: -0.057, -lr * Pred Grad: -0.070, New P: -0.111
iter 9 loss: 0.337
Actual params: [ 0.3107, -0.1112]
-Original Grad: 0.008, -lr * Pred Grad: 0.007, New P: 0.317
-Original Grad: -0.038, -lr * Pred Grad: -0.068, New P: -0.179
iter 10 loss: 0.326
Actual params: [ 0.3174, -0.1792]
-Original Grad: 0.024, -lr * Pred Grad: 0.034, New P: 0.351
-Original Grad: -0.029, -lr * Pred Grad: -0.064, New P: -0.244
iter 11 loss: 0.320
Actual params: [ 0.3511, -0.2435]
-Original Grad: 0.014, -lr * Pred Grad: -0.016, New P: 0.335
-Original Grad: -0.030, -lr * Pred Grad: -0.058, New P: -0.301
iter 12 loss: 0.311
Actual params: [ 0.335 , -0.3014]
-Original Grad: 0.033, -lr * Pred Grad: 0.027, New P: 0.362
-Original Grad: -0.012, -lr * Pred Grad: -0.048, New P: -0.349
iter 13 loss: 0.304
Actual params: [ 0.3624, -0.3494]
-Original Grad: 0.040, -lr * Pred Grad: 0.014, New P: 0.377
-Original Grad: -0.006, -lr * Pred Grad: -0.041, New P: -0.390
iter 14 loss: 0.300
Actual params: [ 0.3769, -0.3901]
-Original Grad: 0.057, -lr * Pred Grad: 0.045, New P: 0.422
-Original Grad: 0.013, -lr * Pred Grad: -0.033, New P: -0.423
iter 15 loss: 0.297
Actual params: [ 0.4216, -0.4229]
-Original Grad: 0.021, -lr * Pred Grad: 0.005, New P: 0.426
-Original Grad: 0.003, -lr * Pred Grad: -0.031, New P: -0.454
iter 16 loss: 0.290
Actual params: [ 0.4263, -0.4542]
-Original Grad: 0.021, -lr * Pred Grad: 0.022, New P: 0.448
-Original Grad: 0.050, -lr * Pred Grad: -0.024, New P: -0.478
iter 17 loss: 0.285
Actual params: [ 0.4478, -0.4783]
-Original Grad: 0.021, -lr * Pred Grad: -0.006, New P: 0.442
-Original Grad: 0.063, -lr * Pred Grad: -0.021, New P: -0.499
iter 18 loss: 0.278
Actual params: [ 0.442 , -0.4989]
-Original Grad: 0.023, -lr * Pred Grad: 0.015, New P: 0.457
-Original Grad: 0.033, -lr * Pred Grad: -0.017, New P: -0.516
iter 19 loss: 0.277
Actual params: [ 0.4574, -0.516 ]
-Original Grad: 0.023, -lr * Pred Grad: 0.004, New P: 0.461
-Original Grad: 0.064, -lr * Pred Grad: -0.002, New P: -0.518
iter 20 loss: 0.277
Actual params: [ 0.4615, -0.5181]
-Original Grad: 0.038, -lr * Pred Grad: 0.020, New P: 0.481
-Original Grad: 0.053, -lr * Pred Grad: 0.024, New P: -0.494
Target params: [1.1812, 0.2779]
iter 0 loss: 0.763
Actual params: [0.5941, 0.5941]
-Original Grad: 0.320, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.271, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.680
Actual params: [0.6617, 0.531 ]
-Original Grad: 0.447, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.121, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.579
Actual params: [0.7468, 0.4504]
-Original Grad: 0.309, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: 0.017, -lr * Pred Grad: -0.082, New P: 0.369
iter 3 loss: 0.475
Actual params: [0.8346, 0.3686]
-Original Grad: 0.080, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.082, -lr * Pred Grad: -0.073, New P: 0.295
iter 4 loss: 0.387
Actual params: [0.9229, 0.2953]
-Original Grad: 0.046, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.084, -lr * Pred Grad: -0.067, New P: 0.228
iter 5 loss: 0.346
Actual params: [1.0112, 0.228 ]
-Original Grad: -0.033, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 0.103, -lr * Pred Grad: -0.048, New P: 0.180
iter 6 loss: 0.358
Actual params: [1.0994, 0.18  ]
-Original Grad: -0.045, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.092, -lr * Pred Grad: -0.018, New P: 0.162
iter 7 loss: 0.385
Actual params: [1.1876, 0.1621]
-Original Grad: -0.041, -lr * Pred Grad: 0.087, New P: 1.274
-Original Grad: 0.084, -lr * Pred Grad: 0.029, New P: 0.191
iter 8 loss: 0.477
Actual params: [1.2742, 0.1914]
-Original Grad: -0.130, -lr * Pred Grad: 0.054, New P: 1.328
-Original Grad: 0.109, -lr * Pred Grad: 0.070, New P: 0.261
iter 9 loss: 0.540
Actual params: [1.3281, 0.2612]
-Original Grad: -0.230, -lr * Pred Grad: -0.029, New P: 1.299
-Original Grad: 0.119, -lr * Pred Grad: 0.081, New P: 0.342
iter 10 loss: 0.470
Actual params: [1.2991, 0.3422]
-Original Grad: -0.163, -lr * Pred Grad: -0.053, New P: 1.246
-Original Grad: 0.119, -lr * Pred Grad: 0.083, New P: 0.425
iter 11 loss: 0.349
Actual params: [1.2456, 0.425 ]
-Original Grad: -0.044, -lr * Pred Grad: -0.048, New P: 1.197
-Original Grad: 0.097, -lr * Pred Grad: 0.082, New P: 0.507
iter 12 loss: 0.273
Actual params: [1.1974, 0.5073]
-Original Grad: 0.008, -lr * Pred Grad: -0.026, New P: 1.172
-Original Grad: -0.004, -lr * Pred Grad: 0.035, New P: 0.542
iter 13 loss: 0.244
Actual params: [1.1717, 0.5423]
-Original Grad: 0.036, -lr * Pred Grad: 0.014, New P: 1.186
-Original Grad: -0.048, -lr * Pred Grad: -0.021, New P: 0.521
iter 14 loss: 0.260
Actual params: [1.186 , 0.5215]
-Original Grad: 0.026, -lr * Pred Grad: 0.028, New P: 1.214
-Original Grad: -0.096, -lr * Pred Grad: -0.049, New P: 0.472
iter 15 loss: 0.306
Actual params: [1.2141, 0.4725]
-Original Grad: 0.012, -lr * Pred Grad: 0.004, New P: 1.218
-Original Grad: 0.054, -lr * Pred Grad: -0.023, New P: 0.450
iter 16 loss: 0.322
Actual params: [1.2181, 0.4499]
-Original Grad: 0.009, -lr * Pred Grad: 0.007, New P: 1.225
-Original Grad: 0.070, -lr * Pred Grad: 0.033, New P: 0.483
iter 17 loss: 0.307
Actual params: [1.2253, 0.4828]
-Original Grad: -0.006, -lr * Pred Grad: -0.016, New P: 1.209
-Original Grad: 0.058, -lr * Pred Grad: 0.056, New P: 0.539
iter 18 loss: 0.259
Actual params: [1.209 , 0.5392]
-Original Grad: 0.056, -lr * Pred Grad: 0.021, New P: 1.230
-Original Grad: -0.111, -lr * Pred Grad: -0.035, New P: 0.504
iter 19 loss: 0.294
Actual params: [1.23  , 0.5043]
-Original Grad: -0.001, -lr * Pred Grad: 0.002, New P: 1.232
-Original Grad: 0.023, -lr * Pred Grad: -0.033, New P: 0.471
iter 20 loss: 0.317
Actual params: [1.2316, 0.4714]
-Original Grad: -0.005, -lr * Pred Grad: -0.013, New P: 1.219
-Original Grad: 0.049, -lr * Pred Grad: 0.018, New P: 0.489
Target params: [1.1812, 0.2779]
iter 0 loss: 0.333
Actual params: [0.5941, 0.5941]
-Original Grad: 0.068, -lr * Pred Grad: 0.062, New P: 0.656
-Original Grad: 0.006, -lr * Pred Grad: 0.040, New P: 0.634
iter 1 loss: 0.348
Actual params: [0.656 , 0.6337]
-Original Grad: 0.061, -lr * Pred Grad: 0.076, New P: 0.732
-Original Grad: 0.015, -lr * Pred Grad: -0.013, New P: 0.621
iter 2 loss: 0.390
Actual params: [0.732, 0.621]
-Original Grad: 0.014, -lr * Pred Grad: 0.001, New P: 0.733
-Original Grad: 0.010, -lr * Pred Grad: -0.043, New P: 0.578
iter 3 loss: 0.368
Actual params: [0.7329, 0.5775]
-Original Grad: 0.051, -lr * Pred Grad: 0.037, New P: 0.770
-Original Grad: 0.009, -lr * Pred Grad: -0.028, New P: 0.550
iter 4 loss: 0.386
Actual params: [0.7698, 0.5497]
-Original Grad: -0.012, -lr * Pred Grad: -0.018, New P: 0.752
-Original Grad: 0.005, -lr * Pred Grad: 0.002, New P: 0.552
iter 5 loss: 0.370
Actual params: [0.752 , 0.5518]
-Original Grad: 0.012, -lr * Pred Grad: 0.017, New P: 0.769
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: 0.552
iter 6 loss: 0.386
Actual params: [0.7685, 0.552 ]
-Original Grad: -0.032, -lr * Pred Grad: -0.032, New P: 0.736
-Original Grad: -0.003, -lr * Pred Grad: -0.008, New P: 0.544
iter 7 loss: 0.356
Actual params: [0.7363, 0.5436]
-Original Grad: 0.009, -lr * Pred Grad: -0.012, New P: 0.724
-Original Grad: -0.010, -lr * Pred Grad: -0.017, New P: 0.526
iter 8 loss: 0.345
Actual params: [0.7244, 0.5263]
-Original Grad: 0.043, -lr * Pred Grad: 0.019, New P: 0.744
-Original Grad: 0.005, -lr * Pred Grad: -0.012, New P: 0.514
iter 9 loss: 0.350
Actual params: [0.7437, 0.5144]
-Original Grad: 0.027, -lr * Pred Grad: 0.016, New P: 0.760
-Original Grad: -0.006, -lr * Pred Grad: -0.013, New P: 0.502
iter 10 loss: 0.355
Actual params: [0.7601, 0.5016]
-Original Grad: 0.001, -lr * Pred Grad: -0.003, New P: 0.758
-Original Grad: 0.012, -lr * Pred Grad: -0.007, New P: 0.495
iter 11 loss: 0.351
Actual params: [0.7575, 0.4948]
-Original Grad: 0.015, -lr * Pred Grad: -0.002, New P: 0.756
-Original Grad: 0.016, -lr * Pred Grad: -0.000, New P: 0.495
iter 12 loss: 0.350
Actual params: [0.7557, 0.4948]
-Original Grad: 0.034, -lr * Pred Grad: 0.011, New P: 0.767
-Original Grad: 0.000, -lr * Pred Grad: -0.006, New P: 0.489
iter 13 loss: 0.355
Actual params: [0.7665, 0.489 ]
-Original Grad: 0.015, -lr * Pred Grad: 0.006, New P: 0.772
-Original Grad: 0.013, -lr * Pred Grad: -0.005, New P: 0.484
iter 14 loss: 0.356
Actual params: [0.7722, 0.4836]
-Original Grad: -0.020, -lr * Pred Grad: -0.017, New P: 0.755
-Original Grad: -0.010, -lr * Pred Grad: -0.013, New P: 0.471
iter 15 loss: 0.343
Actual params: [0.7549, 0.4708]
-Original Grad: 0.028, -lr * Pred Grad: -0.006, New P: 0.749
-Original Grad: 0.007, -lr * Pred Grad: -0.012, New P: 0.459
iter 16 loss: 0.337
Actual params: [0.7493, 0.4586]
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: 0.750
-Original Grad: -0.007, -lr * Pred Grad: -0.013, New P: 0.445
iter 17 loss: 0.333
Actual params: [0.7498, 0.4453]
-Original Grad: 0.020, -lr * Pred Grad: 0.004, New P: 0.754
-Original Grad: 0.002, -lr * Pred Grad: -0.012, New P: 0.433
iter 18 loss: 0.331
Actual params: [0.7538, 0.4333]
-Original Grad: 0.018, -lr * Pred Grad: 0.004, New P: 0.758
-Original Grad: -0.011, -lr * Pred Grad: -0.014, New P: 0.419
iter 19 loss: 0.329
Actual params: [0.7583, 0.4189]
-Original Grad: 0.008, -lr * Pred Grad: -0.001, New P: 0.758
-Original Grad: -0.012, -lr * Pred Grad: -0.017, New P: 0.402
iter 20 loss: 0.325
Actual params: [0.7577, 0.4016]
-Original Grad: 0.013, -lr * Pred Grad: -0.002, New P: 0.756
-Original Grad: -0.014, -lr * Pred Grad: -0.019, New P: 0.383
Target params: [1.1812, 0.2779]
iter 0 loss: 0.324
Actual params: [0.5941, 0.5941]
-Original Grad: 0.286, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.113, -lr * Pred Grad: -0.052, New P: 0.542
iter 1 loss: 0.269
Actual params: [0.6617, 0.5418]
-Original Grad: 0.204, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.023, -lr * Pred Grad: -0.071, New P: 0.471
iter 2 loss: 0.245
Actual params: [0.7465, 0.4709]
-Original Grad: -0.042, -lr * Pred Grad: 0.087, New P: 0.834
-Original Grad: -0.018, -lr * Pred Grad: -0.069, New P: 0.402
iter 3 loss: 0.260
Actual params: [0.8339, 0.4016]
-Original Grad: -0.051, -lr * Pred Grad: 0.085, New P: 0.919
-Original Grad: -0.026, -lr * Pred Grad: -0.067, New P: 0.335
iter 4 loss: 0.239
Actual params: [0.9187, 0.3347]
-Original Grad: 0.022, -lr * Pred Grad: 0.015, New P: 0.934
-Original Grad: -0.011, -lr * Pred Grad: -0.061, New P: 0.274
iter 5 loss: 0.229
Actual params: [0.9339, 0.2738]
-Original Grad: 0.019, -lr * Pred Grad: 0.051, New P: 0.985
-Original Grad: 0.002, -lr * Pred Grad: -0.046, New P: 0.228
iter 6 loss: 0.220
Actual params: [0.9848, 0.2282]
-Original Grad: 0.022, -lr * Pred Grad: 0.033, New P: 1.018
-Original Grad: 0.003, -lr * Pred Grad: -0.026, New P: 0.203
iter 7 loss: 0.219
Actual params: [1.0178, 0.2026]
-Original Grad: 0.027, -lr * Pred Grad: 0.053, New P: 1.071
-Original Grad: -0.013, -lr * Pred Grad: -0.011, New P: 0.192
iter 8 loss: 0.215
Actual params: [1.0709, 0.1917]
-Original Grad: 0.003, -lr * Pred Grad: -0.013, New P: 1.058
-Original Grad: -0.005, -lr * Pred Grad: -0.020, New P: 0.172
iter 9 loss: 0.215
Actual params: [1.0581, 0.1718]
-Original Grad: 0.015, -lr * Pred Grad: 0.016, New P: 1.074
-Original Grad: -0.015, -lr * Pred Grad: -0.028, New P: 0.144
iter 10 loss: 0.216
Actual params: [1.0742, 0.1441]
-Original Grad: 0.005, -lr * Pred Grad: -0.022, New P: 1.053
-Original Grad: -0.010, -lr * Pred Grad: -0.029, New P: 0.115
iter 11 loss: 0.216
Actual params: [1.0526, 0.1152]
-Original Grad: 0.013, -lr * Pred Grad: 0.003, New P: 1.056
-Original Grad: -0.006, -lr * Pred Grad: -0.027, New P: 0.088
iter 12 loss: 0.217
Actual params: [1.056 , 0.0882]
-Original Grad: 0.006, -lr * Pred Grad: -0.011, New P: 1.045
-Original Grad: 0.005, -lr * Pred Grad: -0.021, New P: 0.067
iter 13 loss: 0.217
Actual params: [1.0447, 0.0668]
-Original Grad: 0.011, -lr * Pred Grad: -0.002, New P: 1.042
-Original Grad: -0.001, -lr * Pred Grad: -0.018, New P: 0.049
iter 14 loss: 0.218
Actual params: [1.0425, 0.0487]
-Original Grad: 0.009, -lr * Pred Grad: -0.007, New P: 1.036
-Original Grad: -0.004, -lr * Pred Grad: -0.020, New P: 0.029
iter 15 loss: 0.218
Actual params: [1.0358, 0.0287]
-Original Grad: 0.009, -lr * Pred Grad: -0.005, New P: 1.031
-Original Grad: -0.003, -lr * Pred Grad: -0.023, New P: 0.006
iter 16 loss: 0.219
Actual params: [1.0308, 0.0062]
-Original Grad: 0.002, -lr * Pred Grad: -0.009, New P: 1.022
-Original Grad: -0.005, -lr * Pred Grad: -0.024, New P: -0.018
iter 17 loss: 0.220
Actual params: [ 1.0217, -0.0183]
-Original Grad: 0.005, -lr * Pred Grad: -0.009, New P: 1.012
-Original Grad: 0.003, -lr * Pred Grad: -0.023, New P: -0.041
iter 18 loss: 0.220
Actual params: [ 1.0123, -0.0408]
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: 1.005
-Original Grad: 0.007, -lr * Pred Grad: -0.018, New P: -0.059
iter 19 loss: 0.220
Actual params: [ 1.0052, -0.0587]
-Original Grad: 0.013, -lr * Pred Grad: -0.002, New P: 1.003
-Original Grad: 0.000, -lr * Pred Grad: -0.018, New P: -0.076
iter 20 loss: 0.221
Actual params: [ 1.0029, -0.0763]
-Original Grad: 0.002, -lr * Pred Grad: -0.005, New P: 0.998
-Original Grad: 0.001, -lr * Pred Grad: -0.019, New P: -0.096
Target params: [1.1812, 0.2779]
iter 0 loss: 0.541
Actual params: [0.5941, 0.5941]
-Original Grad: 0.513, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.324, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.494
Actual params: [0.6617, 0.5309]
-Original Grad: 0.670, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.233, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.434
Actual params: [0.7469, 0.4503]
-Original Grad: 0.580, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.111, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.359
Actual params: [0.8348, 0.3668]
-Original Grad: 0.314, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.030, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.271
Actual params: [0.923 , 0.2831]
-Original Grad: 0.068, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.024, -lr * Pred Grad: -0.078, New P: 0.205
iter 5 loss: 0.222
Actual params: [1.0114, 0.205 ]
-Original Grad: 0.012, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: 0.031, -lr * Pred Grad: -0.071, New P: 0.134
iter 6 loss: 0.201
Actual params: [1.0997, 0.1342]
-Original Grad: 0.037, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.032, -lr * Pred Grad: -0.067, New P: 0.067
iter 7 loss: 0.185
Actual params: [1.188 , 0.0673]
-Original Grad: 0.008, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.061, -lr * Pred Grad: -0.050, New P: 0.017
iter 8 loss: 0.176
Actual params: [1.2763, 0.0173]
-Original Grad: -0.016, -lr * Pred Grad: 0.088, New P: 1.365
-Original Grad: 0.067, -lr * Pred Grad: -0.033, New P: -0.016
iter 9 loss: 0.168
Actual params: [ 1.3645, -0.0157]
-Original Grad: -0.093, -lr * Pred Grad: 0.087, New P: 1.452
-Original Grad: 0.092, -lr * Pred Grad: -0.023, New P: -0.039
iter 10 loss: 0.171
Actual params: [ 1.4519, -0.0386]
-Original Grad: -0.064, -lr * Pred Grad: 0.079, New P: 1.531
-Original Grad: 0.109, -lr * Pred Grad: -0.007, New P: -0.046
iter 11 loss: 0.174
Actual params: [ 1.5305, -0.0458]
-Original Grad: -0.081, -lr * Pred Grad: 0.053, New P: 1.583
-Original Grad: 0.140, -lr * Pred Grad: 0.031, New P: -0.015
iter 12 loss: 0.174
Actual params: [ 1.5831, -0.0153]
-Original Grad: -0.083, -lr * Pred Grad: -0.018, New P: 1.565
-Original Grad: 0.128, -lr * Pred Grad: 0.069, New P: 0.054
iter 13 loss: 0.167
Actual params: [1.5652, 0.0539]
-Original Grad: -0.052, -lr * Pred Grad: -0.000, New P: 1.565
-Original Grad: 0.095, -lr * Pred Grad: 0.082, New P: 0.136
iter 14 loss: 0.162
Actual params: [1.5652, 0.1362]
-Original Grad: -0.076, -lr * Pred Grad: 0.011, New P: 1.577
-Original Grad: 0.103, -lr * Pred Grad: 0.082, New P: 0.218
iter 15 loss: 0.160
Actual params: [1.5765, 0.218 ]
-Original Grad: -0.046, -lr * Pred Grad: -0.003, New P: 1.574
-Original Grad: 0.085, -lr * Pred Grad: 0.078, New P: 0.296
iter 16 loss: 0.158
Actual params: [1.5738, 0.2961]
-Original Grad: -0.079, -lr * Pred Grad: -0.035, New P: 1.539
-Original Grad: 0.045, -lr * Pred Grad: 0.058, New P: 0.355
iter 17 loss: 0.152
Actual params: [1.5386, 0.3546]
-Original Grad: -0.049, -lr * Pred Grad: -0.044, New P: 1.494
-Original Grad: 0.021, -lr * Pred Grad: 0.033, New P: 0.388
iter 18 loss: 0.147
Actual params: [1.4944, 0.388 ]
-Original Grad: -0.028, -lr * Pred Grad: -0.037, New P: 1.457
-Original Grad: 0.030, -lr * Pred Grad: 0.026, New P: 0.414
iter 19 loss: 0.142
Actual params: [1.4571, 0.4137]
-Original Grad: -0.021, -lr * Pred Grad: -0.026, New P: 1.431
-Original Grad: 0.023, -lr * Pred Grad: 0.007, New P: 0.421
iter 20 loss: 0.140
Actual params: [1.4312, 0.4211]
-Original Grad: -0.027, -lr * Pred Grad: -0.024, New P: 1.407
-Original Grad: 0.025, -lr * Pred Grad: 0.011, New P: 0.432
Target params: [1.1812, 0.2779]
iter 0 loss: 0.743
Actual params: [0.5941, 0.5941]
-Original Grad: 0.068, -lr * Pred Grad: 0.062, New P: 0.656
-Original Grad: -0.564, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.510
Actual params: [0.656 , 0.5315]
-Original Grad: 0.044, -lr * Pred Grad: 0.071, New P: 0.727
-Original Grad: -0.392, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.323
Actual params: [0.7273, 0.4509]
-Original Grad: 0.037, -lr * Pred Grad: 0.004, New P: 0.732
-Original Grad: -0.100, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.263
Actual params: [0.7316, 0.3673]
-Original Grad: 0.051, -lr * Pred Grad: 0.041, New P: 0.773
-Original Grad: -0.024, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.233
Actual params: [0.7727, 0.2833]
-Original Grad: 0.035, -lr * Pred Grad: 0.020, New P: 0.792
-Original Grad: 0.009, -lr * Pred Grad: -0.081, New P: 0.202
iter 5 loss: 0.219
Actual params: [0.7923, 0.202 ]
-Original Grad: 0.030, -lr * Pred Grad: 0.043, New P: 0.835
-Original Grad: 0.017, -lr * Pred Grad: -0.073, New P: 0.129
iter 6 loss: 0.212
Actual params: [0.8352, 0.1291]
-Original Grad: 0.035, -lr * Pred Grad: 0.007, New P: 0.843
-Original Grad: 0.033, -lr * Pred Grad: -0.069, New P: 0.060
iter 7 loss: 0.209
Actual params: [0.8425, 0.0601]
-Original Grad: 0.035, -lr * Pred Grad: 0.037, New P: 0.880
-Original Grad: 0.042, -lr * Pred Grad: -0.060, New P: 0.001
iter 8 loss: 0.210
Actual params: [8.7969e-01, 5.1467e-04]
-Original Grad: 0.034, -lr * Pred Grad: 0.003, New P: 0.883
-Original Grad: 0.034, -lr * Pred Grad: -0.040, New P: -0.039
iter 9 loss: 0.211
Actual params: [ 0.8828, -0.0395]
-Original Grad: 0.035, -lr * Pred Grad: 0.034, New P: 0.917
-Original Grad: 0.057, -lr * Pred Grad: -0.028, New P: -0.067
iter 10 loss: 0.211
Actual params: [ 0.9171, -0.0674]
-Original Grad: 0.039, -lr * Pred Grad: 0.011, New P: 0.928
-Original Grad: 0.063, -lr * Pred Grad: -0.021, New P: -0.088
iter 11 loss: 0.212
Actual params: [ 0.9282, -0.0882]
-Original Grad: 0.042, -lr * Pred Grad: 0.037, New P: 0.965
-Original Grad: 0.058, -lr * Pred Grad: -0.004, New P: -0.092
iter 12 loss: 0.209
Actual params: [ 0.965 , -0.0925]
-Original Grad: 0.038, -lr * Pred Grad: 0.016, New P: 0.981
-Original Grad: 0.061, -lr * Pred Grad: 0.033, New P: -0.059
iter 13 loss: 0.206
Actual params: [ 0.9807, -0.0592]
-Original Grad: 0.034, -lr * Pred Grad: 0.028, New P: 1.009
-Original Grad: 0.042, -lr * Pred Grad: 0.049, New P: -0.010
iter 14 loss: 0.204
Actual params: [ 1.0092, -0.0099]
-Original Grad: 0.029, -lr * Pred Grad: 0.010, New P: 1.019
-Original Grad: 0.049, -lr * Pred Grad: 0.025, New P: 0.015
iter 15 loss: 0.206
Actual params: [1.0189, 0.015 ]
-Original Grad: 0.018, -lr * Pred Grad: 0.010, New P: 1.029
-Original Grad: 0.022, -lr * Pred Grad: 0.030, New P: 0.045
iter 16 loss: 0.207
Actual params: [1.0288, 0.0452]
-Original Grad: 0.013, -lr * Pred Grad: -0.001, New P: 1.028
-Original Grad: 0.021, -lr * Pred Grad: -0.005, New P: 0.040
iter 17 loss: 0.206
Actual params: [1.0277, 0.0402]
-Original Grad: 0.015, -lr * Pred Grad: 0.000, New P: 1.028
-Original Grad: 0.022, -lr * Pred Grad: 0.015, New P: 0.055
iter 18 loss: 0.208
Actual params: [1.0278, 0.0554]
-Original Grad: 0.018, -lr * Pred Grad: 0.002, New P: 1.030
-Original Grad: 0.020, -lr * Pred Grad: -0.002, New P: 0.053
iter 19 loss: 0.208
Actual params: [1.03  , 0.0532]
-Original Grad: 0.020, -lr * Pred Grad: 0.005, New P: 1.035
-Original Grad: 0.027, -lr * Pred Grad: 0.014, New P: 0.067
iter 20 loss: 0.210
Actual params: [1.0351, 0.0668]
-Original Grad: 0.016, -lr * Pred Grad: 0.004, New P: 1.039
-Original Grad: 0.026, -lr * Pred Grad: 0.008, New P: 0.075
Target params: [1.1812, 0.2779]
iter 0 loss: 0.288
Actual params: [0.5941, 0.5941]
-Original Grad: 0.341, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.199, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.258
Actual params: [0.6617, 0.532 ]
-Original Grad: 0.114, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.063, -lr * Pred Grad: -0.080, New P: 0.452
iter 2 loss: 0.244
Actual params: [0.7465, 0.4522]
-Original Grad: 0.009, -lr * Pred Grad: 0.087, New P: 0.834
-Original Grad: -0.007, -lr * Pred Grad: -0.078, New P: 0.375
iter 3 loss: 0.240
Actual params: [0.8339, 0.3745]
-Original Grad: -0.000, -lr * Pred Grad: 0.087, New P: 0.921
-Original Grad: 0.027, -lr * Pred Grad: -0.071, New P: 0.304
iter 4 loss: 0.240
Actual params: [0.9205, 0.3039]
-Original Grad: 0.015, -lr * Pred Grad: 0.066, New P: 0.987
-Original Grad: 0.013, -lr * Pred Grad: -0.066, New P: 0.238
iter 5 loss: 0.239
Actual params: [0.9867, 0.2378]
-Original Grad: 0.029, -lr * Pred Grad: 0.020, New P: 1.007
-Original Grad: 0.028, -lr * Pred Grad: -0.050, New P: 0.188
iter 6 loss: 0.239
Actual params: [1.0069, 0.1882]
-Original Grad: 0.027, -lr * Pred Grad: 0.060, New P: 1.067
-Original Grad: 0.019, -lr * Pred Grad: -0.028, New P: 0.160
iter 7 loss: 0.239
Actual params: [1.0674, 0.1601]
-Original Grad: 0.019, -lr * Pred Grad: 0.028, New P: 1.095
-Original Grad: 0.018, -lr * Pred Grad: -0.000, New P: 0.160
iter 8 loss: 0.239
Actual params: [1.0951, 0.16  ]
-Original Grad: 0.020, -lr * Pred Grad: 0.044, New P: 1.139
-Original Grad: 0.024, -lr * Pred Grad: 0.020, New P: 0.180
iter 9 loss: 0.240
Actual params: [1.1388, 0.1804]
-Original Grad: 0.015, -lr * Pred Grad: -0.014, New P: 1.125
-Original Grad: 0.024, -lr * Pred Grad: 0.000, New P: 0.181
iter 10 loss: 0.240
Actual params: [1.1246, 0.1807]
-Original Grad: 0.017, -lr * Pred Grad: 0.017, New P: 1.142
-Original Grad: 0.027, -lr * Pred Grad: 0.015, New P: 0.196
iter 11 loss: 0.241
Actual params: [1.1417, 0.196 ]
-Original Grad: 0.012, -lr * Pred Grad: -0.016, New P: 1.126
-Original Grad: 0.019, -lr * Pred Grad: 0.002, New P: 0.198
iter 12 loss: 0.241
Actual params: [1.1256, 0.1981]
-Original Grad: 0.015, -lr * Pred Grad: 0.006, New P: 1.132
-Original Grad: 0.021, -lr * Pred Grad: 0.006, New P: 0.204
iter 13 loss: 0.241
Actual params: [1.1318, 0.2037]
-Original Grad: 0.016, -lr * Pred Grad: -0.006, New P: 1.126
-Original Grad: 0.019, -lr * Pred Grad: 0.002, New P: 0.205
iter 14 loss: 0.241
Actual params: [1.1261, 0.2052]
-Original Grad: 0.017, -lr * Pred Grad: 0.004, New P: 1.130
-Original Grad: 0.017, -lr * Pred Grad: 0.001, New P: 0.206
iter 15 loss: 0.241
Actual params: [1.1297, 0.2059]
-Original Grad: 0.016, -lr * Pred Grad: -0.001, New P: 1.129
-Original Grad: 0.024, -lr * Pred Grad: 0.004, New P: 0.210
iter 16 loss: 0.241
Actual params: [1.1292, 0.2102]
-Original Grad: 0.012, -lr * Pred Grad: -0.001, New P: 1.128
-Original Grad: 0.011, -lr * Pred Grad: 0.000, New P: 0.210
iter 17 loss: 0.241
Actual params: [1.1284, 0.2102]
-Original Grad: 0.014, -lr * Pred Grad: -0.001, New P: 1.127
-Original Grad: 0.019, -lr * Pred Grad: 0.001, New P: 0.211
iter 18 loss: 0.241
Actual params: [1.1273, 0.2111]
-Original Grad: 0.014, -lr * Pred Grad: -0.000, New P: 1.127
-Original Grad: 0.012, -lr * Pred Grad: -0.001, New P: 0.210
iter 19 loss: 0.241
Actual params: [1.1269, 0.2105]
-Original Grad: 0.016, -lr * Pred Grad: 0.001, New P: 1.128
-Original Grad: 0.023, -lr * Pred Grad: 0.003, New P: 0.214
iter 20 loss: 0.241
Actual params: [1.128 , 0.2137]
-Original Grad: 0.013, -lr * Pred Grad: 0.001, New P: 1.129
-Original Grad: 0.014, -lr * Pred Grad: 0.003, New P: 0.217
Target params: [1.1812, 0.2779]
iter 0 loss: 0.718
Actual params: [0.5941, 0.5941]
-Original Grad: 0.247, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.271, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.590
Actual params: [0.6616, 0.531 ]
-Original Grad: 0.165, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.220, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.459
Actual params: [0.7463, 0.4504]
-Original Grad: 0.129, -lr * Pred Grad: 0.087, New P: 0.834
-Original Grad: -0.080, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.394
Actual params: [0.8338, 0.367 ]
-Original Grad: 0.002, -lr * Pred Grad: 0.087, New P: 0.921
-Original Grad: 0.038, -lr * Pred Grad: -0.081, New P: 0.286
iter 4 loss: 0.368
Actual params: [0.9212, 0.2859]
-Original Grad: 0.028, -lr * Pred Grad: 0.082, New P: 1.003
-Original Grad: 0.065, -lr * Pred Grad: -0.072, New P: 0.214
iter 5 loss: 0.364
Actual params: [1.0031, 0.2136]
-Original Grad: 0.033, -lr * Pred Grad: 0.018, New P: 1.021
-Original Grad: 0.084, -lr * Pred Grad: -0.067, New P: 0.146
iter 6 loss: 0.353
Actual params: [1.0209, 0.1462]
-Original Grad: 0.039, -lr * Pred Grad: 0.060, New P: 1.081
-Original Grad: 0.038, -lr * Pred Grad: -0.050, New P: 0.097
iter 7 loss: 0.352
Actual params: [1.0807, 0.0967]
-Original Grad: 0.033, -lr * Pred Grad: 0.042, New P: 1.122
-Original Grad: 0.011, -lr * Pred Grad: -0.030, New P: 0.067
iter 8 loss: 0.350
Actual params: [1.1224, 0.0667]
-Original Grad: 0.027, -lr * Pred Grad: 0.048, New P: 1.171
-Original Grad: 0.013, -lr * Pred Grad: -0.012, New P: 0.055
iter 9 loss: 0.352
Actual params: [1.1708, 0.0548]
-Original Grad: 0.022, -lr * Pred Grad: 0.000, New P: 1.171
-Original Grad: 0.015, -lr * Pred Grad: 0.011, New P: 0.066
iter 10 loss: 0.353
Actual params: [1.1709, 0.0663]
-Original Grad: 0.025, -lr * Pred Grad: 0.027, New P: 1.198
-Original Grad: 0.026, -lr * Pred Grad: 0.001, New P: 0.068
iter 11 loss: 0.355
Actual params: [1.198 , 0.0676]
-Original Grad: 0.025, -lr * Pred Grad: -0.009, New P: 1.189
-Original Grad: 0.033, -lr * Pred Grad: 0.019, New P: 0.086
iter 12 loss: 0.356
Actual params: [1.1888, 0.0862]
-Original Grad: 0.022, -lr * Pred Grad: 0.018, New P: 1.206
-Original Grad: 0.010, -lr * Pred Grad: -0.002, New P: 0.084
iter 13 loss: 0.358
Actual params: [1.2064, 0.0838]
-Original Grad: 0.021, -lr * Pred Grad: -0.005, New P: 1.201
-Original Grad: 0.013, -lr * Pred Grad: -0.001, New P: 0.083
iter 14 loss: 0.357
Actual params: [1.2014, 0.0827]
-Original Grad: 0.017, -lr * Pred Grad: 0.007, New P: 1.209
-Original Grad: 0.011, -lr * Pred Grad: -0.007, New P: 0.076
iter 15 loss: 0.357
Actual params: [1.2088, 0.0761]
-Original Grad: 0.020, -lr * Pred Grad: -0.000, New P: 1.209
-Original Grad: 0.015, -lr * Pred Grad: -0.002, New P: 0.074
iter 16 loss: 0.356
Actual params: [1.2086, 0.0737]
-Original Grad: 0.021, -lr * Pred Grad: 0.006, New P: 1.215
-Original Grad: 0.015, -lr * Pred Grad: -0.000, New P: 0.074
iter 17 loss: 0.357
Actual params: [1.2148, 0.0736]
-Original Grad: 0.024, -lr * Pred Grad: 0.007, New P: 1.222
-Original Grad: 0.020, -lr * Pred Grad: 0.003, New P: 0.076
iter 18 loss: 0.358
Actual params: [1.2216, 0.0765]
-Original Grad: 0.023, -lr * Pred Grad: 0.008, New P: 1.230
-Original Grad: 0.009, -lr * Pred Grad: -0.001, New P: 0.075
iter 19 loss: 0.359
Actual params: [1.2295, 0.0755]
-Original Grad: 0.025, -lr * Pred Grad: 0.009, New P: 1.238
-Original Grad: 0.024, -lr * Pred Grad: 0.003, New P: 0.078
iter 20 loss: 0.360
Actual params: [1.2382, 0.0781]
-Original Grad: 0.019, -lr * Pred Grad: 0.006, New P: 1.244
-Original Grad: 0.011, -lr * Pred Grad: 0.001, New P: 0.079
Target params: [1.1812, 0.2779]
iter 0 loss: 0.727
Actual params: [0.5941, 0.5941]
-Original Grad: 0.609, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.022, -lr * Pred Grad: 0.015, New P: 0.609
iter 1 loss: 0.716
Actual params: [0.6617, 0.6094]
-Original Grad: 0.644, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.089, -lr * Pred Grad: -0.057, New P: 0.552
iter 2 loss: 0.686
Actual params: [0.7469, 0.5525]
-Original Grad: 0.347, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.083, -lr * Pred Grad: -0.075, New P: 0.478
iter 3 loss: 0.653
Actual params: [0.8348, 0.4777]
-Original Grad: 0.429, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.035, -lr * Pred Grad: -0.070, New P: 0.408
iter 4 loss: 0.599
Actual params: [0.923 , 0.4081]
-Original Grad: 0.243, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.173, -lr * Pred Grad: -0.051, New P: 0.357
iter 5 loss: 0.560
Actual params: [1.0114, 0.357 ]
-Original Grad: 0.187, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: 0.222, -lr * Pred Grad: -0.005, New P: 0.352
iter 6 loss: 0.500
Actual params: [1.0997, 0.3522]
-Original Grad: -0.067, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.437, -lr * Pred Grad: 0.054, New P: 0.406
iter 7 loss: 0.414
Actual params: [1.188 , 0.4064]
-Original Grad: -0.134, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.221, -lr * Pred Grad: 0.082, New P: 0.488
iter 8 loss: 0.393
Actual params: [1.2763, 0.4882]
-Original Grad: 0.031, -lr * Pred Grad: 0.088, New P: 1.365
-Original Grad: 0.016, -lr * Pred Grad: 0.087, New P: 0.575
iter 9 loss: 0.510
Actual params: [1.3646, 0.575 ]
-Original Grad: 0.073, -lr * Pred Grad: 0.088, New P: 1.453
-Original Grad: -0.240, -lr * Pred Grad: -0.003, New P: 0.572
iter 10 loss: 0.521
Actual params: [1.4526, 0.5721]
-Original Grad: 0.046, -lr * Pred Grad: 0.087, New P: 1.539
-Original Grad: -0.159, -lr * Pred Grad: -0.051, New P: 0.521
iter 11 loss: 0.498
Actual params: [1.5394, 0.5214]
-Original Grad: 0.008, -lr * Pred Grad: 0.080, New P: 1.620
-Original Grad: -0.054, -lr * Pred Grad: -0.054, New P: 0.467
iter 12 loss: 0.485
Actual params: [1.6199, 0.4672]
-Original Grad: 0.018, -lr * Pred Grad: 0.068, New P: 1.687
-Original Grad: 0.009, -lr * Pred Grad: -0.036, New P: 0.431
iter 13 loss: 0.477
Actual params: [1.6874, 0.4312]
-Original Grad: 0.020, -lr * Pred Grad: 0.056, New P: 1.743
-Original Grad: 0.044, -lr * Pred Grad: 0.005, New P: 0.436
iter 14 loss: 0.484
Actual params: [1.743 , 0.4359]
-Original Grad: 0.026, -lr * Pred Grad: 0.031, New P: 1.774
-Original Grad: 0.023, -lr * Pred Grad: 0.029, New P: 0.465
iter 15 loss: 0.500
Actual params: [1.7744, 0.4652]
-Original Grad: 0.021, -lr * Pred Grad: 0.071, New P: 1.845
-Original Grad: -0.056, -lr * Pred Grad: -0.033, New P: 0.433
iter 16 loss: 0.491
Actual params: [1.8451, 0.4325]
-Original Grad: 0.035, -lr * Pred Grad: 0.072, New P: 1.917
-Original Grad: 0.037, -lr * Pred Grad: -0.001, New P: 0.431
iter 17 loss: 0.500
Actual params: [1.9169, 0.4311]
-Original Grad: 0.006, -lr * Pred Grad: 0.079, New P: 1.996
-Original Grad: -0.002, -lr * Pred Grad: -0.001, New P: 0.430
iter 18 loss: 0.509
Actual params: [1.9961, 0.4298]
-Original Grad: 0.027, -lr * Pred Grad: 0.072, New P: 2.068
-Original Grad: 0.039, -lr * Pred Grad: 0.020, New P: 0.449
iter 19 loss: 0.528
Actual params: [2.0681, 0.4495]
-Original Grad: 0.009, -lr * Pred Grad: 0.041, New P: 2.109
-Original Grad: -0.010, -lr * Pred Grad: -0.010, New P: 0.439
iter 20 loss: 0.527
Actual params: [2.1089, 0.4395]
-Original Grad: 0.002, -lr * Pred Grad: 0.034, New P: 2.143
-Original Grad: -0.016, -lr * Pred Grad: -0.020, New P: 0.419
Target params: [1.1812, 0.2779]
iter 0 loss: 0.636
Actual params: [0.5941, 0.5941]
-Original Grad: 0.298, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.011, -lr * Pred Grad: 0.026, New P: 0.620
iter 1 loss: 0.555
Actual params: [0.6617, 0.6199]
-Original Grad: 0.197, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.163, -lr * Pred Grad: -0.055, New P: 0.564
iter 2 loss: 0.518
Actual params: [0.7465, 0.5644]
-Original Grad: 0.089, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.266, -lr * Pred Grad: -0.078, New P: 0.487
iter 3 loss: 0.473
Actual params: [0.8342, 0.4867]
-Original Grad: 0.406, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: -0.122, -lr * Pred Grad: -0.082, New P: 0.405
iter 4 loss: 0.418
Actual params: [0.9224, 0.405 ]
-Original Grad: 0.355, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.096, -lr * Pred Grad: -0.074, New P: 0.331
iter 5 loss: 0.388
Actual params: [1.0107, 0.331 ]
-Original Grad: 0.119, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 0.109, -lr * Pred Grad: -0.066, New P: 0.265
iter 6 loss: 0.351
Actual params: [1.099 , 0.2648]
-Original Grad: 0.092, -lr * Pred Grad: 0.088, New P: 1.187
-Original Grad: 0.394, -lr * Pred Grad: -0.047, New P: 0.217
iter 7 loss: 0.332
Actual params: [1.1873, 0.2174]
-Original Grad: 0.022, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.679, -lr * Pred Grad: -0.013, New P: 0.204
iter 8 loss: 0.337
Actual params: [1.2756, 0.2043]
-Original Grad: -0.030, -lr * Pred Grad: 0.088, New P: 1.364
-Original Grad: 0.463, -lr * Pred Grad: 0.041, New P: 0.246
iter 9 loss: 0.361
Actual params: [1.3638, 0.2455]
-Original Grad: -0.328, -lr * Pred Grad: 0.079, New P: 1.443
-Original Grad: 0.340, -lr * Pred Grad: 0.077, New P: 0.322
iter 10 loss: 0.389
Actual params: [1.4428, 0.3222]
-Original Grad: -0.139, -lr * Pred Grad: -0.029, New P: 1.414
-Original Grad: 0.144, -lr * Pred Grad: 0.086, New P: 0.408
iter 11 loss: 0.386
Actual params: [1.4142, 0.4084]
-Original Grad: -0.126, -lr * Pred Grad: -0.051, New P: 1.363
-Original Grad: 0.050, -lr * Pred Grad: 0.088, New P: 0.496
iter 12 loss: 0.363
Actual params: [1.363 , 0.4964]
-Original Grad: -0.134, -lr * Pred Grad: -0.051, New P: 1.312
-Original Grad: -0.059, -lr * Pred Grad: 0.088, New P: 0.585
iter 13 loss: 0.348
Actual params: [1.3119, 0.5846]
-Original Grad: 0.068, -lr * Pred Grad: -0.021, New P: 1.291
-Original Grad: -0.351, -lr * Pred Grad: 0.087, New P: 0.672
iter 14 loss: 0.341
Actual params: [1.2913, 0.6716]
-Original Grad: 0.247, -lr * Pred Grad: 0.027, New P: 1.318
-Original Grad: -0.996, -lr * Pred Grad: -0.032, New P: 0.640
iter 15 loss: 0.344
Actual params: [1.3184, 0.6401]
-Original Grad: 0.082, -lr * Pred Grad: 0.069, New P: 1.387
-Original Grad: -0.411, -lr * Pred Grad: -0.054, New P: 0.586
iter 16 loss: 0.377
Actual params: [1.3871, 0.5861]
-Original Grad: -0.121, -lr * Pred Grad: -0.009, New P: 1.378
-Original Grad: -0.501, -lr * Pred Grad: -0.064, New P: 0.522
iter 17 loss: 0.373
Actual params: [1.3783, 0.5225]
-Original Grad: -0.230, -lr * Pred Grad: -0.051, New P: 1.328
-Original Grad: -0.123, -lr * Pred Grad: -0.073, New P: 0.449
iter 18 loss: 0.358
Actual params: [1.3278, 0.449 ]
-Original Grad: -0.041, -lr * Pred Grad: -0.057, New P: 1.271
-Original Grad: -0.047, -lr * Pred Grad: -0.078, New P: 0.371
iter 19 loss: 0.343
Actual params: [1.2706, 0.3713]
-Original Grad: -0.022, -lr * Pred Grad: -0.052, New P: 1.219
-Original Grad: 0.031, -lr * Pred Grad: -0.072, New P: 0.299
iter 20 loss: 0.341
Actual params: [1.2188, 0.2991]
-Original Grad: -0.005, -lr * Pred Grad: -0.036, New P: 1.183
-Original Grad: 0.173, -lr * Pred Grad: -0.063, New P: 0.236
Target params: [1.1812, 0.2779]
iter 0 loss: 0.473
Actual params: [0.5941, 0.5941]
-Original Grad: 0.151, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.326, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.429
Actual params: [0.661 , 0.5309]
-Original Grad: 0.255, -lr * Pred Grad: 0.084, New P: 0.745
-Original Grad: -0.261, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.371
Actual params: [0.7454, 0.4504]
-Original Grad: 0.248, -lr * Pred Grad: 0.088, New P: 0.833
-Original Grad: -0.218, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.323
Actual params: [0.833 , 0.3668]
-Original Grad: 0.048, -lr * Pred Grad: 0.088, New P: 0.921
-Original Grad: -0.048, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.302
Actual params: [0.921 , 0.2827]
-Original Grad: -0.009, -lr * Pred Grad: 0.086, New P: 1.007
-Original Grad: 0.105, -lr * Pred Grad: -0.080, New P: 0.203
iter 5 loss: 0.295
Actual params: [1.0075, 0.2031]
-Original Grad: 0.107, -lr * Pred Grad: 0.075, New P: 1.082
-Original Grad: 0.100, -lr * Pred Grad: -0.071, New P: 0.132
iter 6 loss: 0.295
Actual params: [1.0822, 0.1318]
-Original Grad: 0.052, -lr * Pred Grad: 0.057, New P: 1.139
-Original Grad: 0.051, -lr * Pred Grad: -0.066, New P: 0.066
iter 7 loss: 0.292
Actual params: [1.1394, 0.0663]
-Original Grad: 0.046, -lr * Pred Grad: 0.067, New P: 1.207
-Original Grad: 0.054, -lr * Pred Grad: -0.046, New P: 0.021
iter 8 loss: 0.290
Actual params: [1.2068, 0.0207]
-Original Grad: 0.025, -lr * Pred Grad: 0.031, New P: 1.238
-Original Grad: 0.035, -lr * Pred Grad: -0.028, New P: -0.007
iter 9 loss: 0.289
Actual params: [ 1.2381, -0.0068]
-Original Grad: 0.025, -lr * Pred Grad: 0.048, New P: 1.286
-Original Grad: 0.022, -lr * Pred Grad: -0.008, New P: -0.015
iter 10 loss: 0.289
Actual params: [ 1.2865, -0.0151]
-Original Grad: 0.026, -lr * Pred Grad: -0.005, New P: 1.282
-Original Grad: 0.041, -lr * Pred Grad: 0.026, New P: 0.011
iter 11 loss: 0.290
Actual params: [1.2819, 0.0105]
-Original Grad: 0.027, -lr * Pred Grad: 0.033, New P: 1.315
-Original Grad: 0.048, -lr * Pred Grad: 0.035, New P: 0.046
iter 12 loss: 0.297
Actual params: [1.3152, 0.0459]
-Original Grad: 0.024, -lr * Pred Grad: -0.012, New P: 1.304
-Original Grad: 0.043, -lr * Pred Grad: 0.029, New P: 0.074
iter 13 loss: 0.300
Actual params: [1.3036, 0.0745]
-Original Grad: 0.025, -lr * Pred Grad: 0.021, New P: 1.325
-Original Grad: 0.041, -lr * Pred Grad: 0.034, New P: 0.108
iter 14 loss: 0.305
Actual params: [1.3247, 0.1085]
-Original Grad: 0.023, -lr * Pred Grad: -0.006, New P: 1.319
-Original Grad: 0.064, -lr * Pred Grad: 0.040, New P: 0.149
iter 15 loss: 0.311
Actual params: [1.3189, 0.1489]
-Original Grad: 0.017, -lr * Pred Grad: 0.011, New P: 1.330
-Original Grad: 0.042, -lr * Pred Grad: 0.034, New P: 0.183
iter 16 loss: 0.317
Actual params: [1.3296, 0.1831]
-Original Grad: 0.015, -lr * Pred Grad: -0.005, New P: 1.324
-Original Grad: 0.024, -lr * Pred Grad: 0.015, New P: 0.199
iter 17 loss: 0.317
Actual params: [1.3241, 0.1985]
-Original Grad: 0.014, -lr * Pred Grad: 0.002, New P: 1.326
-Original Grad: 0.030, -lr * Pred Grad: 0.016, New P: 0.214
iter 18 loss: 0.316
Actual params: [1.3259, 0.2144]
-Original Grad: 0.009, -lr * Pred Grad: -0.005, New P: 1.321
-Original Grad: 0.021, -lr * Pred Grad: 0.005, New P: 0.220
iter 19 loss: 0.316
Actual params: [1.3207, 0.2198]
-Original Grad: 0.010, -lr * Pred Grad: -0.004, New P: 1.317
-Original Grad: 0.019, -lr * Pred Grad: 0.006, New P: 0.226
iter 20 loss: 0.317
Actual params: [1.3167, 0.2258]
-Original Grad: 0.009, -lr * Pred Grad: -0.004, New P: 1.312
-Original Grad: 0.018, -lr * Pred Grad: 0.003, New P: 0.229
Target params: [1.1812, 0.2779]
iter 0 loss: 0.459
Actual params: [0.5941, 0.5941]
-Original Grad: 0.328, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.375, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.349
Actual params: [0.6617, 0.5309]
-Original Grad: 0.303, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.251, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.271
Actual params: [0.7467, 0.4504]
-Original Grad: 0.120, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.178, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.209
Actual params: [0.8345, 0.3668]
-Original Grad: 0.001, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.001, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.178
Actual params: [0.9227, 0.283 ]
-Original Grad: -0.023, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.016, -lr * Pred Grad: -0.080, New P: 0.203
iter 5 loss: 0.172
Actual params: [1.0107, 0.2031]
-Original Grad: 0.043, -lr * Pred Grad: 0.087, New P: 1.098
-Original Grad: 0.014, -lr * Pred Grad: -0.072, New P: 0.131
iter 6 loss: 0.167
Actual params: [1.0981, 0.1314]
-Original Grad: 0.024, -lr * Pred Grad: 0.084, New P: 1.182
-Original Grad: 0.017, -lr * Pred Grad: -0.069, New P: 0.063
iter 7 loss: 0.169
Actual params: [1.1818, 0.0628]
-Original Grad: 0.017, -lr * Pred Grad: 0.027, New P: 1.208
-Original Grad: 0.033, -lr * Pred Grad: -0.058, New P: 0.005
iter 8 loss: 0.170
Actual params: [1.2085, 0.005 ]
-Original Grad: 0.014, -lr * Pred Grad: 0.061, New P: 1.270
-Original Grad: 0.010, -lr * Pred Grad: -0.040, New P: -0.035
iter 9 loss: 0.181
Actual params: [ 1.2698, -0.0347]
-Original Grad: -0.005, -lr * Pred Grad: 0.020, New P: 1.290
-Original Grad: 0.012, -lr * Pred Grad: -0.030, New P: -0.065
iter 10 loss: 0.183
Actual params: [ 1.2902, -0.065 ]
-Original Grad: -0.003, -lr * Pred Grad: 0.049, New P: 1.339
-Original Grad: 0.042, -lr * Pred Grad: -0.023, New P: -0.088
iter 11 loss: 0.186
Actual params: [ 1.339 , -0.0876]
-Original Grad: 0.005, -lr * Pred Grad: -0.016, New P: 1.323
-Original Grad: 0.054, -lr * Pred Grad: -0.005, New P: -0.093
iter 12 loss: 0.185
Actual params: [ 1.323 , -0.0927]
-Original Grad: 0.002, -lr * Pred Grad: 0.011, New P: 1.334
-Original Grad: 0.049, -lr * Pred Grad: 0.030, New P: -0.063
iter 13 loss: 0.188
Actual params: [ 1.334 , -0.0629]
-Original Grad: -0.001, -lr * Pred Grad: -0.025, New P: 1.309
-Original Grad: 0.014, -lr * Pred Grad: 0.020, New P: -0.043
iter 14 loss: 0.187
Actual params: [ 1.3089, -0.0429]
-Original Grad: 0.006, -lr * Pred Grad: -0.006, New P: 1.303
-Original Grad: 0.028, -lr * Pred Grad: 0.002, New P: -0.041
iter 15 loss: 0.187
Actual params: [ 1.3032, -0.0413]
-Original Grad: -0.004, -lr * Pred Grad: -0.017, New P: 1.287
-Original Grad: 0.027, -lr * Pred Grad: 0.012, New P: -0.029
iter 16 loss: 0.185
Actual params: [ 1.2866, -0.0288]
-Original Grad: -0.010, -lr * Pred Grad: -0.018, New P: 1.268
-Original Grad: 0.030, -lr * Pred Grad: 0.010, New P: -0.019
iter 17 loss: 0.183
Actual params: [ 1.2685, -0.019 ]
-Original Grad: -0.009, -lr * Pred Grad: -0.020, New P: 1.248
-Original Grad: 0.019, -lr * Pred Grad: 0.006, New P: -0.013
iter 18 loss: 0.179
Actual params: [ 1.2484, -0.0126]
-Original Grad: 0.000, -lr * Pred Grad: -0.015, New P: 1.233
-Original Grad: 0.024, -lr * Pred Grad: 0.005, New P: -0.008
iter 19 loss: 0.175
Actual params: [ 1.2332, -0.0081]
-Original Grad: 0.013, -lr * Pred Grad: -0.006, New P: 1.228
-Original Grad: 0.026, -lr * Pred Grad: 0.007, New P: -0.001
iter 20 loss: 0.174
Actual params: [ 1.2276e+00, -8.9085e-04]
-Original Grad: -0.003, -lr * Pred Grad: -0.009, New P: 1.219
-Original Grad: 0.014, -lr * Pred Grad: 0.002, New P: 0.001
Target params: [1.1812, 0.2779]
iter 0 loss: 0.606
Actual params: [0.5941, 0.5941]
-Original Grad: 0.241, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.317, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.477
Actual params: [0.6616, 0.5309]
-Original Grad: 0.277, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.298, -lr * Pred Grad: -0.080, New P: 0.450
iter 2 loss: 0.370
Actual params: [0.7465, 0.4504]
-Original Grad: 0.250, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.195, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.351
Actual params: [0.8342, 0.3668]
-Original Grad: 0.090, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: -0.078, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.334
Actual params: [0.9224, 0.2827]
-Original Grad: 0.033, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.049, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.325
Actual params: [1.0106, 0.199 ]
-Original Grad: 0.053, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: -0.026, -lr * Pred Grad: -0.080, New P: 0.119
iter 6 loss: 0.325
Actual params: [1.0987, 0.1191]
-Original Grad: 0.061, -lr * Pred Grad: 0.087, New P: 1.186
-Original Grad: -0.013, -lr * Pred Grad: -0.072, New P: 0.047
iter 7 loss: 0.331
Actual params: [1.186 , 0.0471]
-Original Grad: 0.027, -lr * Pred Grad: 0.078, New P: 1.264
-Original Grad: -0.018, -lr * Pred Grad: -0.069, New P: -0.022
iter 8 loss: 0.341
Actual params: [ 1.2638, -0.022 ]
-Original Grad: -0.017, -lr * Pred Grad: -0.003, New P: 1.261
-Original Grad: 0.014, -lr * Pred Grad: -0.064, New P: -0.086
iter 9 loss: 0.341
Actual params: [ 1.2606, -0.0857]
-Original Grad: -0.051, -lr * Pred Grad: 0.014, New P: 1.274
-Original Grad: 0.034, -lr * Pred Grad: -0.046, New P: -0.132
iter 10 loss: 0.344
Actual params: [ 1.2743, -0.1318]
-Original Grad: -0.051, -lr * Pred Grad: -0.023, New P: 1.251
-Original Grad: 0.025, -lr * Pred Grad: -0.033, New P: -0.164
iter 11 loss: 0.342
Actual params: [ 1.2513, -0.1644]
-Original Grad: -0.050, -lr * Pred Grad: -0.031, New P: 1.220
-Original Grad: 0.035, -lr * Pred Grad: -0.026, New P: -0.191
iter 12 loss: 0.338
Actual params: [ 1.2202, -0.1909]
-Original Grad: -0.036, -lr * Pred Grad: -0.038, New P: 1.182
-Original Grad: 0.039, -lr * Pred Grad: -0.023, New P: -0.214
iter 13 loss: 0.330
Actual params: [ 1.1817, -0.2144]
-Original Grad: -0.026, -lr * Pred Grad: -0.035, New P: 1.146
-Original Grad: 0.035, -lr * Pred Grad: -0.019, New P: -0.234
iter 14 loss: 0.324
Actual params: [ 1.1465, -0.2336]
-Original Grad: -0.003, -lr * Pred Grad: -0.021, New P: 1.125
-Original Grad: 0.019, -lr * Pred Grad: -0.008, New P: -0.241
iter 15 loss: 0.320
Actual params: [ 1.1252, -0.2413]
-Original Grad: 0.006, -lr * Pred Grad: -0.008, New P: 1.117
-Original Grad: 0.011, -lr * Pred Grad: 0.007, New P: -0.234
iter 16 loss: 0.324
Actual params: [ 1.1173, -0.2339]
-Original Grad: 0.007, -lr * Pred Grad: -0.003, New P: 1.114
-Original Grad: 0.011, -lr * Pred Grad: -0.015, New P: -0.249
iter 17 loss: 0.320
Actual params: [ 1.1139, -0.2489]
-Original Grad: 0.003, -lr * Pred Grad: -0.006, New P: 1.108
-Original Grad: 0.016, -lr * Pred Grad: -0.008, New P: -0.257
iter 18 loss: 0.324
Actual params: [ 1.1082, -0.2569]
-Original Grad: -0.012, -lr * Pred Grad: -0.015, New P: 1.093
-Original Grad: 0.017, -lr * Pred Grad: -0.008, New P: -0.265
iter 19 loss: 0.322
Actual params: [ 1.0927, -0.2649]
-Original Grad: -0.002, -lr * Pred Grad: -0.017, New P: 1.075
-Original Grad: 0.013, -lr * Pred Grad: -0.010, New P: -0.274
iter 20 loss: 0.320
Actual params: [ 1.0752, -0.2744]
-Original Grad: 0.022, -lr * Pred Grad: -0.005, New P: 1.070
-Original Grad: 0.001, -lr * Pred Grad: -0.018, New P: -0.292
Target params: [1.1812, 0.2779]
iter 0 loss: 0.419
Actual params: [0.5941, 0.5941]
-Original Grad: 0.058, -lr * Pred Grad: 0.060, New P: 0.654
-Original Grad: -0.193, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.357
Actual params: [0.6542, 0.5322]
-Original Grad: 0.060, -lr * Pred Grad: 0.072, New P: 0.726
-Original Grad: -0.072, -lr * Pred Grad: -0.080, New P: 0.452
iter 2 loss: 0.325
Actual params: [0.7265, 0.4524]
-Original Grad: 0.026, -lr * Pred Grad: -0.000, New P: 0.726
-Original Grad: -0.014, -lr * Pred Grad: -0.078, New P: 0.374
iter 3 loss: 0.316
Actual params: [0.7263, 0.374 ]
-Original Grad: 0.026, -lr * Pred Grad: 0.020, New P: 0.747
-Original Grad: -0.027, -lr * Pred Grad: -0.071, New P: 0.303
iter 4 loss: 0.306
Actual params: [0.7465, 0.3026]
-Original Grad: 0.020, -lr * Pred Grad: 0.004, New P: 0.750
-Original Grad: 0.000, -lr * Pred Grad: -0.068, New P: 0.234
iter 5 loss: 0.296
Actual params: [0.7504, 0.2344]
-Original Grad: 0.014, -lr * Pred Grad: 0.022, New P: 0.773
-Original Grad: 0.014, -lr * Pred Grad: -0.060, New P: 0.175
iter 6 loss: 0.287
Actual params: [0.7726, 0.1748]
-Original Grad: 0.008, -lr * Pred Grad: -0.011, New P: 0.761
-Original Grad: 0.009, -lr * Pred Grad: -0.042, New P: 0.133
iter 7 loss: 0.285
Actual params: [0.7614, 0.1329]
-Original Grad: 0.005, -lr * Pred Grad: -0.001, New P: 0.760
-Original Grad: 0.008, -lr * Pred Grad: -0.028, New P: 0.105
iter 8 loss: 0.283
Actual params: [0.7601, 0.105 ]
-Original Grad: 0.012, -lr * Pred Grad: -0.009, New P: 0.751
-Original Grad: 0.007, -lr * Pred Grad: -0.010, New P: 0.095
iter 9 loss: 0.284
Actual params: [0.7514, 0.0946]
-Original Grad: 0.009, -lr * Pred Grad: -0.003, New P: 0.748
-Original Grad: 0.015, -lr * Pred Grad: 0.006, New P: 0.101
iter 10 loss: 0.285
Actual params: [0.748 , 0.1006]
-Original Grad: 0.020, -lr * Pred Grad: 0.001, New P: 0.749
-Original Grad: 0.003, -lr * Pred Grad: -0.015, New P: 0.086
iter 11 loss: 0.283
Actual params: [0.7488, 0.086 ]
-Original Grad: 0.020, -lr * Pred Grad: 0.005, New P: 0.753
-Original Grad: 0.004, -lr * Pred Grad: -0.015, New P: 0.071
iter 12 loss: 0.281
Actual params: [0.7533, 0.0712]
-Original Grad: 0.013, -lr * Pred Grad: 0.001, New P: 0.754
-Original Grad: 0.009, -lr * Pred Grad: -0.015, New P: 0.056
iter 13 loss: 0.280
Actual params: [0.7541, 0.0561]
-Original Grad: 0.015, -lr * Pred Grad: -0.001, New P: 0.754
-Original Grad: 0.014, -lr * Pred Grad: -0.011, New P: 0.045
iter 14 loss: 0.279
Actual params: [0.7536, 0.0452]
-Original Grad: 0.008, -lr * Pred Grad: -0.004, New P: 0.750
-Original Grad: 0.009, -lr * Pred Grad: -0.011, New P: 0.034
iter 15 loss: 0.279
Actual params: [0.7498, 0.0344]
-Original Grad: 0.015, -lr * Pred Grad: -0.001, New P: 0.748
-Original Grad: 0.003, -lr * Pred Grad: -0.016, New P: 0.018
iter 16 loss: 0.277
Actual params: [0.7485, 0.0184]
-Original Grad: 0.007, -lr * Pred Grad: -0.003, New P: 0.746
-Original Grad: 0.024, -lr * Pred Grad: -0.010, New P: 0.008
iter 17 loss: 0.279
Actual params: [0.7459, 0.0085]
-Original Grad: 0.008, -lr * Pred Grad: -0.004, New P: 0.742
-Original Grad: 0.017, -lr * Pred Grad: -0.005, New P: 0.004
iter 18 loss: 0.280
Actual params: [0.7423, 0.0038]
-Original Grad: 0.015, -lr * Pred Grad: 0.000, New P: 0.742
-Original Grad: 0.033, -lr * Pred Grad: 0.002, New P: 0.006
iter 19 loss: 0.279
Actual params: [0.7424, 0.006 ]
-Original Grad: 0.012, -lr * Pred Grad: 0.002, New P: 0.744
-Original Grad: 0.010, -lr * Pred Grad: -0.005, New P: 0.001
iter 20 loss: 0.280
Actual params: [0.7441, 0.0012]
-Original Grad: 0.006, -lr * Pred Grad: -0.001, New P: 0.743
-Original Grad: 0.017, -lr * Pred Grad: -0.010, New P: -0.009
Target params: [1.1812, 0.2779]
iter 0 loss: 0.388
Actual params: [0.5941, 0.5941]
-Original Grad: 0.043, -lr * Pred Grad: 0.057, New P: 0.651
-Original Grad: -0.065, -lr * Pred Grad: -0.028, New P: 0.566
iter 1 loss: 0.371
Actual params: [0.6508, 0.5665]
-Original Grad: 0.024, -lr * Pred Grad: 0.037, New P: 0.688
-Original Grad: -0.060, -lr * Pred Grad: -0.068, New P: 0.499
iter 2 loss: 0.332
Actual params: [0.6877, 0.4987]
-Original Grad: 0.024, -lr * Pred Grad: -0.022, New P: 0.665
-Original Grad: -0.048, -lr * Pred Grad: -0.071, New P: 0.428
iter 3 loss: 0.298
Actual params: [0.6653, 0.428 ]
-Original Grad: 0.030, -lr * Pred Grad: -0.001, New P: 0.665
-Original Grad: -0.052, -lr * Pred Grad: -0.068, New P: 0.360
iter 4 loss: 0.244
Actual params: [0.6646, 0.3597]
-Original Grad: 0.022, -lr * Pred Grad: 0.022, New P: 0.686
-Original Grad: -0.106, -lr * Pred Grad: -0.067, New P: 0.293
iter 5 loss: 0.179
Actual params: [0.6863, 0.2929]
-Original Grad: 0.044, -lr * Pred Grad: 0.031, New P: 0.717
-Original Grad: -0.051, -lr * Pred Grad: -0.065, New P: 0.227
iter 6 loss: 0.158
Actual params: [0.7172, 0.2275]
-Original Grad: -0.008, -lr * Pred Grad: -0.008, New P: 0.709
-Original Grad: 0.051, -lr * Pred Grad: -0.053, New P: 0.174
iter 7 loss: 0.189
Actual params: [0.7092, 0.1743]
-Original Grad: -0.028, -lr * Pred Grad: -0.025, New P: 0.684
-Original Grad: 0.101, -lr * Pred Grad: -0.032, New P: 0.142
iter 8 loss: 0.202
Actual params: [0.6844, 0.142 ]
-Original Grad: -0.055, -lr * Pred Grad: -0.041, New P: 0.643
-Original Grad: 0.151, -lr * Pred Grad: -0.019, New P: 0.123
iter 9 loss: 0.197
Actual params: [0.6433, 0.123 ]
-Original Grad: -0.031, -lr * Pred Grad: -0.039, New P: 0.604
-Original Grad: 0.089, -lr * Pred Grad: 0.003, New P: 0.126
iter 10 loss: 0.175
Actual params: [0.604 , 0.1258]
-Original Grad: -0.018, -lr * Pred Grad: -0.029, New P: 0.575
-Original Grad: 0.063, -lr * Pred Grad: 0.041, New P: 0.167
iter 11 loss: 0.155
Actual params: [0.5751, 0.1672]
-Original Grad: 0.025, -lr * Pred Grad: -0.007, New P: 0.568
-Original Grad: 0.021, -lr * Pred Grad: 0.055, New P: 0.222
iter 12 loss: 0.161
Actual params: [0.5685, 0.2223]
-Original Grad: 0.030, -lr * Pred Grad: 0.014, New P: 0.583
-Original Grad: -0.018, -lr * Pred Grad: -0.022, New P: 0.200
iter 13 loss: 0.152
Actual params: [0.5827, 0.2001]
-Original Grad: 0.020, -lr * Pred Grad: 0.015, New P: 0.598
-Original Grad: 0.010, -lr * Pred Grad: 0.005, New P: 0.205
iter 14 loss: 0.152
Actual params: [0.5976, 0.205 ]
-Original Grad: 0.034, -lr * Pred Grad: 0.016, New P: 0.613
-Original Grad: 0.007, -lr * Pred Grad: -0.012, New P: 0.193
iter 15 loss: 0.154
Actual params: [0.6135, 0.1926]
-Original Grad: 0.011, -lr * Pred Grad: 0.005, New P: 0.619
-Original Grad: 0.014, -lr * Pred Grad: -0.001, New P: 0.192
iter 16 loss: 0.155
Actual params: [0.6186, 0.1919]
-Original Grad: 0.011, -lr * Pred Grad: -0.003, New P: 0.616
-Original Grad: 0.026, -lr * Pred Grad: 0.002, New P: 0.194
iter 17 loss: 0.154
Actual params: [0.6157, 0.1943]
-Original Grad: 0.014, -lr * Pred Grad: -0.003, New P: 0.613
-Original Grad: 0.017, -lr * Pred Grad: 0.003, New P: 0.197
iter 18 loss: 0.153
Actual params: [0.6128, 0.1974]
-Original Grad: 0.019, -lr * Pred Grad: 0.002, New P: 0.615
-Original Grad: 0.029, -lr * Pred Grad: 0.009, New P: 0.206
iter 19 loss: 0.151
Actual params: [0.6146, 0.2064]
-Original Grad: 0.011, -lr * Pred Grad: 0.001, New P: 0.616
-Original Grad: 0.034, -lr * Pred Grad: 0.016, New P: 0.222
iter 20 loss: 0.153
Actual params: [0.6156, 0.2222]
-Original Grad: 0.021, -lr * Pred Grad: 0.003, New P: 0.619
-Original Grad: 0.004, -lr * Pred Grad: -0.001, New P: 0.221
Target params: [1.1812, 0.2779]
iter 0 loss: 1.328
Actual params: [0.5941, 0.5941]
-Original Grad: 0.505, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.007, -lr * Pred Grad: 0.029, New P: 0.623
iter 1 loss: 1.295
Actual params: [0.6617, 0.623 ]
-Original Grad: 0.480, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: 0.003, -lr * Pred Grad: -0.032, New P: 0.591
iter 2 loss: 1.229
Actual params: [0.7469, 0.5912]
-Original Grad: 0.554, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: 0.011, -lr * Pred Grad: -0.054, New P: 0.538
iter 3 loss: 1.128
Actual params: [0.8347, 0.5376]
-Original Grad: 0.665, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.073, -lr * Pred Grad: -0.010, New P: 0.527
iter 4 loss: 0.988
Actual params: [0.923 , 0.5273]
-Original Grad: 0.787, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.143, -lr * Pred Grad: 0.053, New P: 0.581
iter 5 loss: 0.803
Actual params: [1.0113, 0.5806]
-Original Grad: 0.691, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: 0.216, -lr * Pred Grad: 0.080, New P: 0.661
iter 6 loss: 0.446
Actual params: [1.0997, 0.6608]
-Original Grad: 0.057, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.005, -lr * Pred Grad: 0.083, New P: 0.744
iter 7 loss: 0.280
Actual params: [1.188 , 0.7435]
-Original Grad: -0.042, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: -0.091, -lr * Pred Grad: -0.022, New P: 0.721
iter 8 loss: 0.241
Actual params: [1.2763, 0.7215]
-Original Grad: -0.022, -lr * Pred Grad: 0.088, New P: 1.365
-Original Grad: -0.095, -lr * Pred Grad: -0.050, New P: 0.671
iter 9 loss: 0.189
Actual params: [1.3647, 0.6712]
-Original Grad: 0.006, -lr * Pred Grad: 0.088, New P: 1.453
-Original Grad: -0.058, -lr * Pred Grad: -0.053, New P: 0.618
iter 10 loss: 0.280
Actual params: [1.453 , 0.6179]
-Original Grad: -0.001, -lr * Pred Grad: 0.088, New P: 1.541
-Original Grad: -0.014, -lr * Pred Grad: -0.043, New P: 0.575
iter 11 loss: 0.409
Actual params: [1.5413, 0.5751]
-Original Grad: -0.030, -lr * Pred Grad: 0.088, New P: 1.630
-Original Grad: 0.051, -lr * Pred Grad: -0.006, New P: 0.569
iter 12 loss: 0.474
Actual params: [1.6296, 0.5694]
-Original Grad: -0.050, -lr * Pred Grad: 0.088, New P: 1.718
-Original Grad: 0.053, -lr * Pred Grad: 0.036, New P: 0.606
iter 13 loss: 0.485
Actual params: [1.7177, 0.6057]
-Original Grad: -0.064, -lr * Pred Grad: 0.087, New P: 1.804
-Original Grad: 0.048, -lr * Pred Grad: 0.048, New P: 0.654
iter 14 loss: 0.467
Actual params: [1.8044, 0.6538]
-Original Grad: -0.043, -lr * Pred Grad: 0.079, New P: 1.883
-Original Grad: -0.002, -lr * Pred Grad: -0.004, New P: 0.650
iter 15 loss: 0.495
Actual params: [1.883 , 0.6498]
-Original Grad: -0.046, -lr * Pred Grad: 0.063, New P: 1.946
-Original Grad: -0.027, -lr * Pred Grad: -0.020, New P: 0.630
iter 16 loss: 0.533
Actual params: [1.9457, 0.6298]
-Original Grad: -0.065, -lr * Pred Grad: 0.012, New P: 1.958
-Original Grad: 0.016, -lr * Pred Grad: -0.013, New P: 0.617
iter 17 loss: 0.549
Actual params: [1.958 , 0.6165]
-Original Grad: -0.053, -lr * Pred Grad: 0.022, New P: 1.980
-Original Grad: 0.034, -lr * Pred Grad: 0.013, New P: 0.630
iter 18 loss: 0.542
Actual params: [1.9801, 0.6296]
-Original Grad: -0.063, -lr * Pred Grad: 0.000, New P: 1.980
-Original Grad: 0.025, -lr * Pred Grad: 0.015, New P: 0.645
iter 19 loss: 0.529
Actual params: [1.9803, 0.6449]
-Original Grad: -0.060, -lr * Pred Grad: 0.026, New P: 2.006
-Original Grad: -0.044, -lr * Pred Grad: -0.024, New P: 0.621
iter 20 loss: 0.558
Actual params: [2.006 , 0.6207]
-Original Grad: -0.058, -lr * Pred Grad: -0.006, New P: 2.000
-Original Grad: -0.008, -lr * Pred Grad: -0.030, New P: 0.591
Target params: [1.1812, 0.2779]
iter 0 loss: 0.321
Actual params: [0.5941, 0.5941]
-Original Grad: 0.025, -lr * Pred Grad: 0.050, New P: 0.644
-Original Grad: -0.088, -lr * Pred Grad: -0.043, New P: 0.551
iter 1 loss: 0.327
Actual params: [0.6443, 0.5514]
-Original Grad: 0.010, -lr * Pred Grad: 0.001, New P: 0.645
-Original Grad: -0.039, -lr * Pred Grad: -0.069, New P: 0.482
iter 2 loss: 0.329
Actual params: [0.645 , 0.4825]
-Original Grad: 0.004, -lr * Pred Grad: -0.040, New P: 0.605
-Original Grad: -0.004, -lr * Pred Grad: -0.069, New P: 0.414
iter 3 loss: 0.329
Actual params: [0.6049, 0.4138]
-Original Grad: -0.004, -lr * Pred Grad: -0.035, New P: 0.569
-Original Grad: 0.016, -lr * Pred Grad: -0.064, New P: 0.349
iter 4 loss: 0.328
Actual params: [0.5695, 0.3495]
-Original Grad: -0.010, -lr * Pred Grad: -0.013, New P: 0.557
-Original Grad: 0.022, -lr * Pred Grad: -0.045, New P: 0.304
iter 5 loss: 0.328
Actual params: [0.5569, 0.3041]
-Original Grad: -0.008, -lr * Pred Grad: -0.010, New P: 0.547
-Original Grad: 0.027, -lr * Pred Grad: -0.010, New P: 0.294
iter 6 loss: 0.328
Actual params: [0.5465, 0.2938]
-Original Grad: -0.012, -lr * Pred Grad: -0.018, New P: 0.529
-Original Grad: 0.024, -lr * Pred Grad: 0.025, New P: 0.318
iter 7 loss: 0.328
Actual params: [0.5288, 0.3185]
-Original Grad: -0.007, -lr * Pred Grad: -0.017, New P: 0.511
-Original Grad: 0.011, -lr * Pred Grad: -0.003, New P: 0.315
iter 8 loss: 0.327
Actual params: [0.5114, 0.3152]
-Original Grad: -0.014, -lr * Pred Grad: -0.020, New P: 0.491
-Original Grad: 0.014, -lr * Pred Grad: 0.002, New P: 0.317
iter 9 loss: 0.326
Actual params: [0.491 , 0.3168]
-Original Grad: -0.016, -lr * Pred Grad: -0.023, New P: 0.468
-Original Grad: 0.011, -lr * Pred Grad: -0.007, New P: 0.309
iter 10 loss: 0.325
Actual params: [0.4682, 0.3094]
-Original Grad: -0.026, -lr * Pred Grad: -0.027, New P: 0.441
-Original Grad: 0.017, -lr * Pred Grad: -0.002, New P: 0.308
iter 11 loss: 0.324
Actual params: [0.4412, 0.3075]
-Original Grad: -0.016, -lr * Pred Grad: -0.027, New P: 0.415
-Original Grad: 0.009, -lr * Pred Grad: -0.006, New P: 0.302
iter 12 loss: 0.322
Actual params: [0.4146, 0.3018]
-Original Grad: -0.021, -lr * Pred Grad: -0.026, New P: 0.389
-Original Grad: 0.007, -lr * Pred Grad: -0.010, New P: 0.292
iter 13 loss: 0.321
Actual params: [0.3886, 0.2916]
-Original Grad: -0.014, -lr * Pred Grad: -0.024, New P: 0.364
-Original Grad: 0.003, -lr * Pred Grad: -0.015, New P: 0.276
iter 14 loss: 0.320
Actual params: [0.3645, 0.2764]
-Original Grad: -0.017, -lr * Pred Grad: -0.023, New P: 0.341
-Original Grad: 0.006, -lr * Pred Grad: -0.016, New P: 0.260
iter 15 loss: 0.320
Actual params: [0.3412, 0.2605]
-Original Grad: -0.015, -lr * Pred Grad: -0.023, New P: 0.318
-Original Grad: 0.006, -lr * Pred Grad: -0.015, New P: 0.246
iter 16 loss: 0.318
Actual params: [0.3182, 0.2456]
-Original Grad: -0.011, -lr * Pred Grad: -0.021, New P: 0.297
-Original Grad: 0.005, -lr * Pred Grad: -0.014, New P: 0.232
iter 17 loss: 0.318
Actual params: [0.2969, 0.2317]
-Original Grad: -0.016, -lr * Pred Grad: -0.021, New P: 0.276
-Original Grad: 0.007, -lr * Pred Grad: -0.013, New P: 0.219
iter 18 loss: 0.317
Actual params: [0.2758, 0.2191]
-Original Grad: -0.012, -lr * Pred Grad: -0.020, New P: 0.255
-Original Grad: 0.003, -lr * Pred Grad: -0.012, New P: 0.207
iter 19 loss: 0.317
Actual params: [0.2554, 0.2068]
-Original Grad: -0.009, -lr * Pred Grad: -0.019, New P: 0.237
-Original Grad: 0.008, -lr * Pred Grad: -0.010, New P: 0.197
iter 20 loss: 0.317
Actual params: [0.2368, 0.197 ]
-Original Grad: -0.007, -lr * Pred Grad: -0.017, New P: 0.220
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 0.190
