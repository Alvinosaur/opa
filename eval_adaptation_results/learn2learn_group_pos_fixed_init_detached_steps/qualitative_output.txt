Target params: [1.1812, 0.2779]
iter 0 loss: 0.739
Actual params: [0.5941, 0.5941]
-Original Grad: 0.376, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.428, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.718
Actual params: [0.6617, 0.5311]
-Original Grad: 0.336, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.422, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.670
Actual params: [0.7468, 0.4506]
-Original Grad: 0.395, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.351, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.577
Actual params: [0.8346, 0.367 ]
-Original Grad: 0.384, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.001, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.423
Actual params: [0.9229, 0.2829]
-Original Grad: 0.314, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.115, -lr * Pred Grad: -0.080, New P: 0.202
iter 5 loss: 0.307
Actual params: [1.0112, 0.2024]
-Original Grad: 0.146, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: 0.164, -lr * Pred Grad: -0.072, New P: 0.131
iter 6 loss: 0.327
Actual params: [1.0995, 0.1306]
-Original Grad: 0.191, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.126, -lr * Pred Grad: -0.066, New P: 0.065
iter 7 loss: 0.393
Actual params: [1.1879, 0.0647]
-Original Grad: -0.157, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.226, -lr * Pred Grad: -0.049, New P: 0.015
iter 8 loss: 0.436
Actual params: [1.2762, 0.0152]
-Original Grad: -0.454, -lr * Pred Grad: 0.088, New P: 1.364
-Original Grad: 0.634, -lr * Pred Grad: -0.028, New P: -0.013
iter 9 loss: 0.486
Actual params: [ 1.3641, -0.0126]
-Original Grad: -0.303, -lr * Pred Grad: 0.071, New P: 1.435
-Original Grad: 0.720, -lr * Pred Grad: 0.009, New P: -0.004
iter 10 loss: 0.517
Actual params: [ 1.4351, -0.0039]
-Original Grad: -0.377, -lr * Pred Grad: -0.030, New P: 1.405
-Original Grad: 0.703, -lr * Pred Grad: 0.059, New P: 0.055
iter 11 loss: 0.505
Actual params: [1.4046, 0.0546]
-Original Grad: -0.322, -lr * Pred Grad: -0.053, New P: 1.351
-Original Grad: 0.437, -lr * Pred Grad: 0.082, New P: 0.136
iter 12 loss: 0.494
Actual params: [1.3512, 0.1365]
-Original Grad: -0.326, -lr * Pred Grad: -0.063, New P: 1.288
-Original Grad: 0.318, -lr * Pred Grad: 0.087, New P: 0.224
iter 13 loss: 0.513
Actual params: [1.2882, 0.2238]
-Original Grad: -0.129, -lr * Pred Grad: -0.069, New P: 1.219
-Original Grad: 0.048, -lr * Pred Grad: 0.088, New P: 0.312
iter 14 loss: 0.575
Actual params: [1.2194, 0.312 ]
-Original Grad: -0.059, -lr * Pred Grad: -0.068, New P: 1.151
-Original Grad: -0.063, -lr * Pred Grad: 0.088, New P: 0.400
iter 15 loss: 0.644
Actual params: [1.1511, 0.4003]
-Original Grad: -0.120, -lr * Pred Grad: -0.068, New P: 1.083
-Original Grad: -0.290, -lr * Pred Grad: 0.088, New P: 0.489
iter 16 loss: 0.686
Actual params: [1.0834, 0.4886]
-Original Grad: -0.294, -lr * Pred Grad: -0.074, New P: 1.010
-Original Grad: -0.562, -lr * Pred Grad: 0.088, New P: 0.577
iter 17 loss: 0.707
Actual params: [1.0097, 0.5767]
-Original Grad: 0.093, -lr * Pred Grad: -0.067, New P: 0.942
-Original Grad: -1.069, -lr * Pred Grad: -0.031, New P: 0.546
iter 18 loss: 0.683
Actual params: [0.9425, 0.5462]
-Original Grad: 0.250, -lr * Pred Grad: -0.050, New P: 0.893
-Original Grad: -0.874, -lr * Pred Grad: -0.053, New P: 0.494
iter 19 loss: 0.647
Actual params: [0.8929, 0.4936]
-Original Grad: 0.430, -lr * Pred Grad: -0.030, New P: 0.863
-Original Grad: -0.790, -lr * Pred Grad: -0.064, New P: 0.430
iter 20 loss: 0.607
Actual params: [0.8631, 0.4299]
-Original Grad: 0.387, -lr * Pred Grad: -0.019, New P: 0.844
-Original Grad: -0.231, -lr * Pred Grad: -0.073, New P: 0.357
Target params: [1.1812, 0.2779]
iter 0 loss: 0.312
Actual params: [0.5941, 0.5941]
-Original Grad: 0.031, -lr * Pred Grad: 0.052, New P: 0.647
-Original Grad: -0.115, -lr * Pred Grad: -0.053, New P: 0.541
iter 1 loss: 0.264
Actual params: [0.6465, 0.5411]
-Original Grad: 0.011, -lr * Pred Grad: 0.007, New P: 0.654
-Original Grad: -0.044, -lr * Pred Grad: -0.073, New P: 0.468
iter 2 loss: 0.253
Actual params: [0.6538, 0.4679]
-Original Grad: -0.002, -lr * Pred Grad: -0.040, New P: 0.614
-Original Grad: -0.016, -lr * Pred Grad: -0.071, New P: 0.397
iter 3 loss: 0.252
Actual params: [0.6139, 0.3973]
-Original Grad: -0.025, -lr * Pred Grad: -0.046, New P: 0.568
-Original Grad: 0.010, -lr * Pred Grad: -0.067, New P: 0.330
iter 4 loss: 0.251
Actual params: [0.5679, 0.33  ]
-Original Grad: -0.025, -lr * Pred Grad: -0.033, New P: 0.535
-Original Grad: 0.012, -lr * Pred Grad: -0.057, New P: 0.273
iter 5 loss: 0.251
Actual params: [0.5352, 0.2734]
-Original Grad: -0.042, -lr * Pred Grad: -0.032, New P: 0.503
-Original Grad: 0.011, -lr * Pred Grad: -0.034, New P: 0.239
iter 6 loss: 0.250
Actual params: [0.5029, 0.2393]
-Original Grad: -0.028, -lr * Pred Grad: -0.034, New P: 0.469
-Original Grad: 0.008, -lr * Pred Grad: -0.006, New P: 0.233
iter 7 loss: 0.252
Actual params: [0.4688, 0.2332]
-Original Grad: -0.052, -lr * Pred Grad: -0.042, New P: 0.427
-Original Grad: 0.009, -lr * Pred Grad: 0.006, New P: 0.239
iter 8 loss: 0.263
Actual params: [0.4269, 0.2394]
-Original Grad: -0.012, -lr * Pred Grad: -0.037, New P: 0.390
-Original Grad: -0.001, -lr * Pred Grad: -0.018, New P: 0.221
iter 9 loss: 0.273
Actual params: [0.3901, 0.2212]
-Original Grad: -0.004, -lr * Pred Grad: -0.024, New P: 0.366
-Original Grad: -0.001, -lr * Pred Grad: -0.018, New P: 0.204
iter 10 loss: 0.280
Actual params: [0.3658, 0.2036]
-Original Grad: 0.005, -lr * Pred Grad: -0.012, New P: 0.354
-Original Grad: -0.000, -lr * Pred Grad: -0.020, New P: 0.183
iter 11 loss: 0.283
Actual params: [0.3542, 0.1833]
-Original Grad: 0.013, -lr * Pred Grad: -0.003, New P: 0.351
-Original Grad: -0.001, -lr * Pred Grad: -0.021, New P: 0.162
iter 12 loss: 0.283
Actual params: [0.3515, 0.1624]
-Original Grad: 0.019, -lr * Pred Grad: 0.003, New P: 0.354
-Original Grad: -0.002, -lr * Pred Grad: -0.022, New P: 0.141
iter 13 loss: 0.282
Actual params: [0.3542, 0.1406]
-Original Grad: 0.007, -lr * Pred Grad: -0.002, New P: 0.352
-Original Grad: -0.002, -lr * Pred Grad: -0.023, New P: 0.118
iter 14 loss: 0.283
Actual params: [0.3519, 0.1181]
-Original Grad: 0.002, -lr * Pred Grad: -0.010, New P: 0.342
-Original Grad: 0.000, -lr * Pred Grad: -0.022, New P: 0.096
iter 15 loss: 0.286
Actual params: [0.3421, 0.0962]
-Original Grad: 0.017, -lr * Pred Grad: -0.006, New P: 0.336
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: 0.076
iter 16 loss: 0.289
Actual params: [0.336 , 0.0758]
-Original Grad: 0.009, -lr * Pred Grad: -0.004, New P: 0.332
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: 0.056
iter 17 loss: 0.290
Actual params: [0.3324, 0.0562]
-Original Grad: 0.030, -lr * Pred Grad: 0.006, New P: 0.338
-Original Grad: 0.004, -lr * Pred Grad: -0.019, New P: 0.037
iter 18 loss: 0.289
Actual params: [0.3383, 0.0373]
-Original Grad: 0.013, -lr * Pred Grad: 0.005, New P: 0.343
-Original Grad: 0.004, -lr * Pred Grad: -0.019, New P: 0.018
iter 19 loss: 0.288
Actual params: [0.3434, 0.0185]
-Original Grad: 0.013, -lr * Pred Grad: 0.000, New P: 0.344
-Original Grad: 0.005, -lr * Pred Grad: -0.019, New P: -0.000
iter 20 loss: 0.289
Actual params: [ 3.4361e-01, -1.8735e-04]
-Original Grad: 0.006, -lr * Pred Grad: -0.004, New P: 0.340
-Original Grad: 0.004, -lr * Pred Grad: -0.019, New P: -0.019
Target params: [1.1812, 0.2779]
iter 0 loss: 0.445
Actual params: [0.5941, 0.5941]
-Original Grad: -0.073, -lr * Pred Grad: -0.033, New P: 0.561
-Original Grad: -0.740, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.326
Actual params: [0.5607, 0.5321]
-Original Grad: -0.140, -lr * Pred Grad: -0.074, New P: 0.487
-Original Grad: -0.839, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.299
Actual params: [0.4871, 0.4515]
-Original Grad: -0.049, -lr * Pred Grad: -0.078, New P: 0.409
-Original Grad: -0.095, -lr * Pred Grad: -0.084, New P: 0.368
iter 3 loss: 0.321
Actual params: [0.4089, 0.3678]
-Original Grad: 0.079, -lr * Pred Grad: -0.071, New P: 0.338
-Original Grad: 0.007, -lr * Pred Grad: -0.084, New P: 0.284
iter 4 loss: 0.343
Actual params: [0.3383, 0.2839]
-Original Grad: 0.188, -lr * Pred Grad: -0.058, New P: 0.280
-Original Grad: 0.003, -lr * Pred Grad: -0.082, New P: 0.202
iter 5 loss: 0.352
Actual params: [0.2804, 0.2022]
-Original Grad: 0.186, -lr * Pred Grad: -0.023, New P: 0.258
-Original Grad: -0.015, -lr * Pred Grad: -0.074, New P: 0.128
iter 6 loss: 0.353
Actual params: [0.2577, 0.1285]
-Original Grad: 0.172, -lr * Pred Grad: 0.033, New P: 0.291
-Original Grad: -0.001, -lr * Pred Grad: -0.070, New P: 0.059
iter 7 loss: 0.342
Actual params: [0.2911, 0.0589]
-Original Grad: 0.139, -lr * Pred Grad: 0.075, New P: 0.366
-Original Grad: 0.008, -lr * Pred Grad: -0.066, New P: -0.007
iter 8 loss: 0.326
Actual params: [ 0.3657, -0.0067]
-Original Grad: 0.136, -lr * Pred Grad: 0.085, New P: 0.450
-Original Grad: 0.033, -lr * Pred Grad: -0.050, New P: -0.056
iter 9 loss: 0.319
Actual params: [ 0.4504, -0.0563]
-Original Grad: 0.058, -lr * Pred Grad: 0.084, New P: 0.534
-Original Grad: 0.029, -lr * Pred Grad: -0.034, New P: -0.090
iter 10 loss: 0.308
Actual params: [ 0.5339, -0.0905]
-Original Grad: -0.014, -lr * Pred Grad: 0.020, New P: 0.554
-Original Grad: 0.023, -lr * Pred Grad: -0.027, New P: -0.117
iter 11 loss: 0.308
Actual params: [ 0.5539, -0.1174]
-Original Grad: -0.012, -lr * Pred Grad: 0.018, New P: 0.572
-Original Grad: 0.022, -lr * Pred Grad: -0.024, New P: -0.141
iter 12 loss: 0.308
Actual params: [ 0.5716, -0.1413]
-Original Grad: -0.001, -lr * Pred Grad: -0.020, New P: 0.552
-Original Grad: 0.052, -lr * Pred Grad: -0.012, New P: -0.154
iter 13 loss: 0.308
Actual params: [ 0.5515, -0.1537]
-Original Grad: -0.005, -lr * Pred Grad: -0.004, New P: 0.548
-Original Grad: 0.027, -lr * Pred Grad: 0.014, New P: -0.140
iter 14 loss: 0.308
Actual params: [ 0.5479, -0.1397]
-Original Grad: -0.018, -lr * Pred Grad: -0.030, New P: 0.517
-Original Grad: 0.030, -lr * Pred Grad: 0.023, New P: -0.116
iter 15 loss: 0.311
Actual params: [ 0.5174, -0.1163]
-Original Grad: 0.005, -lr * Pred Grad: -0.018, New P: 0.499
-Original Grad: 0.028, -lr * Pred Grad: 0.001, New P: -0.115
iter 16 loss: 0.313
Actual params: [ 0.4993, -0.1151]
-Original Grad: 0.042, -lr * Pred Grad: 0.010, New P: 0.509
-Original Grad: 0.038, -lr * Pred Grad: 0.022, New P: -0.093
iter 17 loss: 0.313
Actual params: [ 0.509 , -0.0933]
-Original Grad: 0.040, -lr * Pred Grad: 0.024, New P: 0.533
-Original Grad: 0.038, -lr * Pred Grad: 0.017, New P: -0.076
iter 18 loss: 0.308
Actual params: [ 0.5331, -0.0759]
-Original Grad: 0.005, -lr * Pred Grad: 0.002, New P: 0.535
-Original Grad: 0.031, -lr * Pred Grad: 0.019, New P: -0.057
iter 19 loss: 0.308
Actual params: [ 0.5347, -0.0572]
-Original Grad: -0.005, -lr * Pred Grad: -0.014, New P: 0.521
-Original Grad: 0.019, -lr * Pred Grad: 0.004, New P: -0.054
iter 20 loss: 0.310
Actual params: [ 0.521 , -0.0535]
-Original Grad: 0.001, -lr * Pred Grad: -0.016, New P: 0.505
-Original Grad: 0.016, -lr * Pred Grad: -0.000, New P: -0.054
Target params: [1.1812, 0.2779]
iter 0 loss: 1.269
Actual params: [0.5941, 0.5941]
-Original Grad: 0.276, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.379, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 1.224
Actual params: [0.6617, 0.5309]
-Original Grad: 0.390, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.411, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 1.130
Actual params: [0.7467, 0.4505]
-Original Grad: 0.748, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.378, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.880
Actual params: [0.8345, 0.3669]
-Original Grad: 0.538, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.176, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.608
Actual params: [0.9228, 0.2827]
-Original Grad: 0.269, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.025, -lr * Pred Grad: -0.084, New P: 0.198
iter 5 loss: 0.455
Actual params: [1.0111, 0.1985]
-Original Grad: 0.113, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 0.007, -lr * Pred Grad: -0.083, New P: 0.116
iter 6 loss: 0.411
Actual params: [1.0994, 0.116 ]
-Original Grad: 0.077, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.029, -lr * Pred Grad: -0.074, New P: 0.042
iter 7 loss: 0.425
Actual params: [1.1878, 0.0419]
-Original Grad: 0.024, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.025, -lr * Pred Grad: -0.070, New P: -0.028
iter 8 loss: 0.482
Actual params: [ 1.2761, -0.0277]
-Original Grad: -0.090, -lr * Pred Grad: 0.088, New P: 1.364
-Original Grad: 0.025, -lr * Pred Grad: -0.063, New P: -0.091
iter 9 loss: 0.542
Actual params: [ 1.3644, -0.0911]
-Original Grad: -0.105, -lr * Pred Grad: 0.088, New P: 1.453
-Original Grad: 0.001, -lr * Pred Grad: -0.046, New P: -0.137
iter 10 loss: 0.600
Actual params: [ 1.4526, -0.1371]
-Original Grad: -0.112, -lr * Pred Grad: 0.087, New P: 1.540
-Original Grad: 0.031, -lr * Pred Grad: -0.033, New P: -0.171
iter 11 loss: 0.650
Actual params: [ 1.5399, -0.1705]
-Original Grad: -0.135, -lr * Pred Grad: 0.073, New P: 1.613
-Original Grad: 0.071, -lr * Pred Grad: -0.025, New P: -0.196
iter 12 loss: 0.697
Actual params: [ 1.6128, -0.196 ]
-Original Grad: -0.139, -lr * Pred Grad: -0.010, New P: 1.603
-Original Grad: 0.092, -lr * Pred Grad: -0.020, New P: -0.216
iter 13 loss: 0.695
Actual params: [ 1.6028, -0.2163]
-Original Grad: -0.127, -lr * Pred Grad: -0.020, New P: 1.583
-Original Grad: 0.084, -lr * Pred Grad: -0.009, New P: -0.225
iter 14 loss: 0.684
Actual params: [ 1.5832, -0.2254]
-Original Grad: -0.143, -lr * Pred Grad: -0.048, New P: 1.535
-Original Grad: 0.089, -lr * Pred Grad: 0.022, New P: -0.203
iter 15 loss: 0.653
Actual params: [ 1.5349, -0.2035]
-Original Grad: -0.117, -lr * Pred Grad: -0.051, New P: 1.484
-Original Grad: 0.072, -lr * Pred Grad: 0.060, New P: -0.143
iter 16 loss: 0.612
Actual params: [ 1.4838, -0.143 ]
-Original Grad: -0.119, -lr * Pred Grad: -0.055, New P: 1.428
-Original Grad: 0.020, -lr * Pred Grad: 0.034, New P: -0.109
iter 17 loss: 0.577
Actual params: [ 1.4283, -0.1086]
-Original Grad: -0.089, -lr * Pred Grad: -0.057, New P: 1.371
-Original Grad: 0.052, -lr * Pred Grad: 0.045, New P: -0.064
iter 18 loss: 0.546
Actual params: [ 1.3713, -0.0637]
-Original Grad: -0.110, -lr * Pred Grad: -0.058, New P: 1.313
-Original Grad: 0.031, -lr * Pred Grad: 0.011, New P: -0.052
iter 19 loss: 0.510
Actual params: [ 1.3133, -0.0523]
-Original Grad: -0.077, -lr * Pred Grad: -0.058, New P: 1.256
-Original Grad: 0.012, -lr * Pred Grad: 0.017, New P: -0.035
iter 20 loss: 0.463
Actual params: [ 1.2556, -0.0352]
-Original Grad: -0.076, -lr * Pred Grad: -0.057, New P: 1.199
-Original Grad: 0.035, -lr * Pred Grad: 0.003, New P: -0.033
Target params: [1.1812, 0.2779]
iter 0 loss: 0.863
Actual params: [0.5941, 0.5941]
-Original Grad: 0.395, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.745, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.818
Actual params: [0.6617, 0.5321]
-Original Grad: 0.394, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.480, -lr * Pred Grad: -0.081, New P: 0.452
iter 2 loss: 0.742
Actual params: [0.7468, 0.4515]
-Original Grad: 0.363, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.394, -lr * Pred Grad: -0.084, New P: 0.368
iter 3 loss: 0.635
Actual params: [0.8347, 0.3679]
-Original Grad: 0.195, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.064, -lr * Pred Grad: -0.084, New P: 0.284
iter 4 loss: 0.554
Actual params: [0.9229, 0.2837]
-Original Grad: 0.082, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.041, -lr * Pred Grad: -0.084, New P: 0.200
iter 5 loss: 0.483
Actual params: [1.0112, 0.1995]
-Original Grad: 0.041, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: -0.017, -lr * Pred Grad: -0.082, New P: 0.117
iter 6 loss: 0.471
Actual params: [1.0996, 0.117 ]
-Original Grad: -0.022, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: -0.011, -lr * Pred Grad: -0.075, New P: 0.042
iter 7 loss: 0.487
Actual params: [1.1878, 0.0422]
-Original Grad: -0.040, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: -0.013, -lr * Pred Grad: -0.070, New P: -0.028
iter 8 loss: 0.516
Actual params: [ 1.2759, -0.0278]
-Original Grad: -0.046, -lr * Pred Grad: 0.086, New P: 1.362
-Original Grad: -0.004, -lr * Pred Grad: -0.067, New P: -0.095
iter 9 loss: 0.546
Actual params: [ 1.3622, -0.0949]
-Original Grad: -0.035, -lr * Pred Grad: 0.070, New P: 1.432
-Original Grad: 0.005, -lr * Pred Grad: -0.057, New P: -0.151
iter 10 loss: 0.573
Actual params: [ 1.4323, -0.1515]
-Original Grad: -0.031, -lr * Pred Grad: 0.003, New P: 1.435
-Original Grad: 0.026, -lr * Pred Grad: -0.040, New P: -0.191
iter 11 loss: 0.573
Actual params: [ 1.4351, -0.1911]
-Original Grad: -0.032, -lr * Pred Grad: 0.039, New P: 1.474
-Original Grad: 0.026, -lr * Pred Grad: -0.030, New P: -0.221
iter 12 loss: 0.574
Actual params: [ 1.4743, -0.2207]
-Original Grad: -0.025, -lr * Pred Grad: 0.043, New P: 1.517
-Original Grad: 0.022, -lr * Pred Grad: -0.027, New P: -0.247
iter 13 loss: 0.584
Actual params: [ 1.517 , -0.2474]
-Original Grad: -0.017, -lr * Pred Grad: 0.062, New P: 1.579
-Original Grad: 0.020, -lr * Pred Grad: -0.025, New P: -0.272
iter 14 loss: 0.599
Actual params: [ 1.5786, -0.2723]
-Original Grad: -0.018, -lr * Pred Grad: 0.022, New P: 1.601
-Original Grad: 0.028, -lr * Pred Grad: -0.021, New P: -0.293
iter 15 loss: 0.604
Actual params: [ 1.6009, -0.2935]
-Original Grad: -0.016, -lr * Pred Grad: 0.004, New P: 1.604
-Original Grad: 0.029, -lr * Pred Grad: -0.005, New P: -0.299
iter 16 loss: 0.605
Actual params: [ 1.6044, -0.2988]
-Original Grad: -0.020, -lr * Pred Grad: -0.029, New P: 1.575
-Original Grad: 0.030, -lr * Pred Grad: 0.018, New P: -0.281
iter 17 loss: 0.599
Actual params: [ 1.5754, -0.2805]
-Original Grad: -0.018, -lr * Pred Grad: -0.024, New P: 1.552
-Original Grad: 0.028, -lr * Pred Grad: 0.006, New P: -0.274
iter 18 loss: 0.593
Actual params: [ 1.5516, -0.2742]
-Original Grad: -0.015, -lr * Pred Grad: -0.023, New P: 1.528
-Original Grad: 0.019, -lr * Pred Grad: 0.001, New P: -0.273
iter 19 loss: 0.588
Actual params: [ 1.5284, -0.2727]
-Original Grad: -0.020, -lr * Pred Grad: -0.025, New P: 1.503
-Original Grad: 0.017, -lr * Pred Grad: -0.006, New P: -0.278
iter 20 loss: 0.581
Actual params: [ 1.5034, -0.2784]
-Original Grad: -0.020, -lr * Pred Grad: -0.026, New P: 1.477
-Original Grad: 0.023, -lr * Pred Grad: -0.004, New P: -0.282
Target params: [1.1812, 0.2779]
iter 0 loss: 0.885
Actual params: [0.5941, 0.5941]
-Original Grad: 0.129, -lr * Pred Grad: 0.066, New P: 0.660
-Original Grad: -0.674, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.832
Actual params: [0.6605, 0.5318]
-Original Grad: 0.330, -lr * Pred Grad: 0.085, New P: 0.745
-Original Grad: -0.474, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.732
Actual params: [0.745 , 0.4513]
-Original Grad: 0.372, -lr * Pred Grad: 0.088, New P: 0.833
-Original Grad: -0.179, -lr * Pred Grad: -0.084, New P: 0.368
iter 3 loss: 0.591
Actual params: [0.8327, 0.3677]
-Original Grad: 0.311, -lr * Pred Grad: 0.088, New P: 0.921
-Original Grad: -0.052, -lr * Pred Grad: -0.084, New P: 0.284
iter 4 loss: 0.477
Actual params: [0.9209, 0.2835]
-Original Grad: 0.111, -lr * Pred Grad: 0.088, New P: 1.009
-Original Grad: -0.012, -lr * Pred Grad: -0.084, New P: 0.200
iter 5 loss: 0.406
Actual params: [1.0092, 0.2   ]
-Original Grad: 0.046, -lr * Pred Grad: 0.088, New P: 1.097
-Original Grad: 0.021, -lr * Pred Grad: -0.077, New P: 0.123
iter 6 loss: 0.382
Actual params: [1.0975, 0.1228]
-Original Grad: -0.006, -lr * Pred Grad: 0.088, New P: 1.186
-Original Grad: 0.027, -lr * Pred Grad: -0.070, New P: 0.052
iter 7 loss: 0.394
Actual params: [1.1857, 0.0524]
-Original Grad: -0.027, -lr * Pred Grad: 0.087, New P: 1.273
-Original Grad: 0.178, -lr * Pred Grad: -0.066, New P: -0.013
iter 8 loss: 0.406
Actual params: [ 1.273 , -0.0132]
-Original Grad: -0.039, -lr * Pred Grad: 0.025, New P: 1.298
-Original Grad: 0.089, -lr * Pred Grad: -0.049, New P: -0.062
iter 9 loss: 0.412
Actual params: [ 1.2975, -0.0624]
-Original Grad: -0.068, -lr * Pred Grad: -0.033, New P: 1.265
-Original Grad: 0.085, -lr * Pred Grad: -0.033, New P: -0.095
iter 10 loss: 0.403
Actual params: [ 1.2649, -0.0951]
-Original Grad: -0.054, -lr * Pred Grad: -0.010, New P: 1.255
-Original Grad: 0.103, -lr * Pred Grad: -0.024, New P: -0.120
iter 11 loss: 0.401
Actual params: [ 1.2547, -0.1196]
-Original Grad: -0.034, -lr * Pred Grad: 0.001, New P: 1.256
-Original Grad: 0.023, -lr * Pred Grad: -0.016, New P: -0.135
iter 12 loss: 0.401
Actual params: [ 1.2557, -0.1353]
-Original Grad: -0.031, -lr * Pred Grad: -0.029, New P: 1.227
-Original Grad: 0.028, -lr * Pred Grad: 0.012, New P: -0.123
iter 13 loss: 0.399
Actual params: [ 1.2271, -0.1235]
-Original Grad: -0.035, -lr * Pred Grad: -0.035, New P: 1.192
-Original Grad: 0.025, -lr * Pred Grad: 0.034, New P: -0.090
iter 14 loss: 0.392
Actual params: [ 1.1918, -0.0896]
-Original Grad: -0.026, -lr * Pred Grad: -0.034, New P: 1.158
-Original Grad: 0.031, -lr * Pred Grad: -0.003, New P: -0.092
iter 15 loss: 0.385
Actual params: [ 1.158 , -0.0923]
-Original Grad: -0.026, -lr * Pred Grad: -0.030, New P: 1.128
-Original Grad: 0.098, -lr * Pred Grad: 0.056, New P: -0.037
iter 16 loss: 0.383
Actual params: [ 1.1285, -0.0366]
-Original Grad: -0.010, -lr * Pred Grad: -0.022, New P: 1.107
-Original Grad: 0.024, -lr * Pred Grad: 0.026, New P: -0.011
iter 17 loss: 0.383
Actual params: [ 1.1067, -0.0109]
-Original Grad: -0.015, -lr * Pred Grad: -0.019, New P: 1.088
-Original Grad: 0.097, -lr * Pred Grad: 0.064, New P: 0.053
iter 18 loss: 0.383
Actual params: [1.0876, 0.0534]
-Original Grad: -0.006, -lr * Pred Grad: -0.017, New P: 1.071
-Original Grad: 0.111, -lr * Pred Grad: 0.073, New P: 0.126
iter 19 loss: 0.383
Actual params: [1.071 , 0.1261]
-Original Grad: 0.013, -lr * Pred Grad: -0.008, New P: 1.063
-Original Grad: 0.097, -lr * Pred Grad: 0.078, New P: 0.204
iter 20 loss: 0.387
Actual params: [1.0634, 0.2037]
-Original Grad: 0.008, -lr * Pred Grad: -0.003, New P: 1.061
-Original Grad: -0.002, -lr * Pred Grad: 0.022, New P: 0.225
Target params: [1.1812, 0.2779]
iter 0 loss: 0.338
Actual params: [0.5941, 0.5941]
-Original Grad: 0.040, -lr * Pred Grad: 0.056, New P: 0.650
-Original Grad: -0.200, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.312
Actual params: [0.6498, 0.532 ]
-Original Grad: 0.051, -lr * Pred Grad: 0.057, New P: 0.707
-Original Grad: -0.170, -lr * Pred Grad: -0.080, New P: 0.452
iter 2 loss: 0.288
Actual params: [0.7069, 0.4516]
-Original Grad: 0.068, -lr * Pred Grad: 0.021, New P: 0.728
-Original Grad: -0.078, -lr * Pred Grad: -0.083, New P: 0.368
iter 3 loss: 0.282
Actual params: [0.7276, 0.3685]
-Original Grad: 0.033, -lr * Pred Grad: 0.029, New P: 0.757
-Original Grad: -0.078, -lr * Pred Grad: -0.082, New P: 0.286
iter 4 loss: 0.276
Actual params: [0.757 , 0.2863]
-Original Grad: 0.031, -lr * Pred Grad: 0.019, New P: 0.776
-Original Grad: -0.051, -lr * Pred Grad: -0.076, New P: 0.210
iter 5 loss: 0.276
Actual params: [0.7758, 0.2099]
-Original Grad: 0.025, -lr * Pred Grad: 0.030, New P: 0.806
-Original Grad: 0.011, -lr * Pred Grad: -0.070, New P: 0.140
iter 6 loss: 0.275
Actual params: [0.8055, 0.1395]
-Original Grad: 0.025, -lr * Pred Grad: 0.006, New P: 0.811
-Original Grad: 0.034, -lr * Pred Grad: -0.066, New P: 0.073
iter 7 loss: 0.280
Actual params: [0.8115, 0.0731]
-Original Grad: 0.017, -lr * Pred Grad: 0.012, New P: 0.824
-Original Grad: 0.006, -lr * Pred Grad: -0.052, New P: 0.021
iter 8 loss: 0.282
Actual params: [0.8236, 0.0209]
-Original Grad: 0.034, -lr * Pred Grad: 0.009, New P: 0.833
-Original Grad: 0.013, -lr * Pred Grad: -0.037, New P: -0.016
iter 9 loss: 0.282
Actual params: [ 0.8326, -0.0161]
-Original Grad: 0.014, -lr * Pred Grad: 0.005, New P: 0.838
-Original Grad: 0.021, -lr * Pred Grad: -0.028, New P: -0.044
iter 10 loss: 0.282
Actual params: [ 0.8379, -0.044 ]
-Original Grad: 0.012, -lr * Pred Grad: -0.002, New P: 0.836
-Original Grad: 0.030, -lr * Pred Grad: -0.019, New P: -0.063
iter 11 loss: 0.283
Actual params: [ 0.8355, -0.0629]
-Original Grad: 0.018, -lr * Pred Grad: 0.000, New P: 0.836
-Original Grad: 0.023, -lr * Pred Grad: 0.002, New P: -0.061
iter 12 loss: 0.283
Actual params: [ 0.8357, -0.0612]
-Original Grad: 0.015, -lr * Pred Grad: 0.000, New P: 0.836
-Original Grad: 0.035, -lr * Pred Grad: 0.025, New P: -0.036
iter 13 loss: 0.282
Actual params: [ 0.8359, -0.0363]
-Original Grad: 0.000, -lr * Pred Grad: -0.008, New P: 0.828
-Original Grad: 0.011, -lr * Pred Grad: -0.006, New P: -0.042
iter 14 loss: 0.283
Actual params: [ 0.8282, -0.0424]
-Original Grad: 0.012, -lr * Pred Grad: -0.006, New P: 0.822
-Original Grad: 0.011, -lr * Pred Grad: -0.003, New P: -0.045
iter 15 loss: 0.284
Actual params: [ 0.8221, -0.0452]
-Original Grad: 0.010, -lr * Pred Grad: -0.003, New P: 0.819
-Original Grad: 0.012, -lr * Pred Grad: -0.011, New P: -0.056
iter 16 loss: 0.285
Actual params: [ 0.8189, -0.0564]
-Original Grad: 0.019, -lr * Pred Grad: 0.003, New P: 0.821
-Original Grad: 0.007, -lr * Pred Grad: -0.013, New P: -0.069
iter 17 loss: 0.284
Actual params: [ 0.8214, -0.069 ]
-Original Grad: 0.016, -lr * Pred Grad: 0.004, New P: 0.826
-Original Grad: 0.024, -lr * Pred Grad: -0.006, New P: -0.075
iter 18 loss: 0.284
Actual params: [ 0.8256, -0.0746]
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 0.825
-Original Grad: 0.019, -lr * Pred Grad: -0.002, New P: -0.076
iter 19 loss: 0.284
Actual params: [ 0.8252, -0.0765]
-Original Grad: 0.018, -lr * Pred Grad: 0.001, New P: 0.826
-Original Grad: 0.013, -lr * Pred Grad: -0.006, New P: -0.083
iter 20 loss: 0.284
Actual params: [ 0.8258, -0.0825]
-Original Grad: 0.016, -lr * Pred Grad: 0.003, New P: 0.829
-Original Grad: 0.025, -lr * Pred Grad: -0.005, New P: -0.087
Target params: [1.1812, 0.2779]
iter 0 loss: 0.605
Actual params: [0.5941, 0.5941]
-Original Grad: 0.317, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.037, -lr * Pred Grad: -0.001, New P: 0.593
iter 1 loss: 0.577
Actual params: [0.6617, 0.593 ]
-Original Grad: 0.263, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: 0.041, -lr * Pred Grad: -0.025, New P: 0.568
iter 2 loss: 0.515
Actual params: [0.7467, 0.5681]
-Original Grad: 0.211, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.017, -lr * Pred Grad: -0.052, New P: 0.516
iter 3 loss: 0.429
Actual params: [0.8344, 0.5158]
-Original Grad: 0.152, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.024, -lr * Pred Grad: -0.054, New P: 0.462
iter 4 loss: 0.327
Actual params: [0.9227, 0.4616]
-Original Grad: 0.090, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.027, -lr * Pred Grad: -0.045, New P: 0.417
iter 5 loss: 0.273
Actual params: [1.011 , 0.4171]
-Original Grad: 0.020, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: -0.009, -lr * Pred Grad: -0.028, New P: 0.390
iter 6 loss: 0.251
Actual params: [1.0992, 0.3895]
-Original Grad: -0.013, -lr * Pred Grad: 0.088, New P: 1.187
-Original Grad: 0.017, -lr * Pred Grad: -0.009, New P: 0.381
iter 7 loss: 0.261
Actual params: [1.1873, 0.3807]
-Original Grad: -0.032, -lr * Pred Grad: 0.086, New P: 1.274
-Original Grad: 0.025, -lr * Pred Grad: 0.005, New P: 0.386
iter 8 loss: 0.284
Actual params: [1.2736, 0.3862]
-Original Grad: -0.051, -lr * Pred Grad: 0.036, New P: 1.310
-Original Grad: 0.018, -lr * Pred Grad: 0.006, New P: 0.392
iter 9 loss: 0.295
Actual params: [1.3096, 0.3921]
-Original Grad: -0.027, -lr * Pred Grad: -0.002, New P: 1.308
-Original Grad: 0.022, -lr * Pred Grad: 0.005, New P: 0.397
iter 10 loss: 0.293
Actual params: [1.308 , 0.3968]
-Original Grad: -0.043, -lr * Pred Grad: 0.036, New P: 1.344
-Original Grad: 0.023, -lr * Pred Grad: 0.004, New P: 0.401
iter 11 loss: 0.306
Actual params: [1.3443, 0.4005]
-Original Grad: -0.066, -lr * Pred Grad: -0.016, New P: 1.328
-Original Grad: 0.034, -lr * Pred Grad: 0.009, New P: 0.409
iter 12 loss: 0.296
Actual params: [1.3278, 0.4093]
-Original Grad: -0.060, -lr * Pred Grad: -0.030, New P: 1.297
-Original Grad: 0.025, -lr * Pred Grad: 0.008, New P: 0.418
iter 13 loss: 0.280
Actual params: [1.2975, 0.4177]
-Original Grad: -0.025, -lr * Pred Grad: -0.036, New P: 1.261
-Original Grad: 0.001, -lr * Pred Grad: -0.007, New P: 0.411
iter 14 loss: 0.269
Actual params: [1.2611, 0.4112]
-Original Grad: -0.026, -lr * Pred Grad: -0.032, New P: 1.229
-Original Grad: -0.004, -lr * Pred Grad: -0.019, New P: 0.392
iter 15 loss: 0.265
Actual params: [1.2295, 0.3921]
-Original Grad: -0.019, -lr * Pred Grad: -0.026, New P: 1.203
-Original Grad: 0.004, -lr * Pred Grad: -0.021, New P: 0.371
iter 16 loss: 0.266
Actual params: [1.2032, 0.3709]
-Original Grad: -0.040, -lr * Pred Grad: -0.031, New P: 1.172
-Original Grad: 0.022, -lr * Pred Grad: -0.012, New P: 0.359
iter 17 loss: 0.262
Actual params: [1.1717, 0.3588]
-Original Grad: -0.022, -lr * Pred Grad: -0.030, New P: 1.141
-Original Grad: 0.036, -lr * Pred Grad: 0.004, New P: 0.363
iter 18 loss: 0.258
Actual params: [1.1413, 0.3629]
-Original Grad: -0.017, -lr * Pred Grad: -0.026, New P: 1.116
-Original Grad: -0.001, -lr * Pred Grad: -0.002, New P: 0.361
iter 19 loss: 0.260
Actual params: [1.1157, 0.3612]
-Original Grad: -0.036, -lr * Pred Grad: -0.028, New P: 1.088
-Original Grad: 0.016, -lr * Pred Grad: -0.009, New P: 0.352
iter 20 loss: 0.257
Actual params: [1.0876, 0.352 ]
-Original Grad: -0.002, -lr * Pred Grad: -0.023, New P: 1.065
-Original Grad: 0.026, -lr * Pred Grad: -0.005, New P: 0.347
Target params: [1.1812, 0.2779]
iter 0 loss: 0.789
Actual params: [0.5941, 0.5941]
-Original Grad: 0.562, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.115, -lr * Pred Grad: -0.053, New P: 0.541
iter 1 loss: 0.746
Actual params: [0.6617, 0.5412]
-Original Grad: 0.731, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.073, -lr * Pred Grad: -0.076, New P: 0.466
iter 2 loss: 0.673
Actual params: [0.7469, 0.4655]
-Original Grad: 0.508, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.012, -lr * Pred Grad: -0.073, New P: 0.393
iter 3 loss: 0.539
Actual params: [0.8348, 0.3929]
-Original Grad: 0.400, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.359, -lr * Pred Grad: -0.059, New P: 0.334
iter 4 loss: 1.072
Actual params: [0.923 , 0.3343]
-Original Grad: -0.216, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 1.041, -lr * Pred Grad: -0.019, New P: 0.315
iter 5 loss: 1.247
Actual params: [1.0114, 0.315 ]
-Original Grad: -0.751, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 1.792, -lr * Pred Grad: 0.046, New P: 0.361
iter 6 loss: 1.297
Actual params: [1.0995, 0.3609]
-Original Grad: 0.132, -lr * Pred Grad: 0.088, New P: 1.187
-Original Grad: 1.106, -lr * Pred Grad: 0.080, New P: 0.441
iter 7 loss: 1.305
Actual params: [1.1875, 0.4412]
-Original Grad: 0.054, -lr * Pred Grad: 0.088, New P: 1.275
-Original Grad: 0.059, -lr * Pred Grad: 0.087, New P: 0.528
iter 8 loss: 0.449
Actual params: [1.2751, 0.5284]
-Original Grad: 0.065, -lr * Pred Grad: 0.087, New P: 1.362
-Original Grad: -0.018, -lr * Pred Grad: 0.088, New P: 0.617
iter 9 loss: 0.530
Actual params: [1.3616, 0.6166]
-Original Grad: 0.053, -lr * Pred Grad: 0.084, New P: 1.446
-Original Grad: -0.108, -lr * Pred Grad: 0.088, New P: 0.705
iter 10 loss: 0.564
Actual params: [1.4458, 0.7049]
-Original Grad: -0.018, -lr * Pred Grad: 0.077, New P: 1.523
-Original Grad: -0.085, -lr * Pred Grad: 0.088, New P: 0.793
iter 11 loss: 0.593
Actual params: [1.5226, 0.7932]
-Original Grad: -0.011, -lr * Pred Grad: 0.048, New P: 1.571
-Original Grad: -0.035, -lr * Pred Grad: 0.088, New P: 0.882
iter 12 loss: 0.616
Actual params: [1.5705, 0.8815]
-Original Grad: -0.017, -lr * Pred Grad: 0.062, New P: 1.632
-Original Grad: -0.007, -lr * Pred Grad: 0.088, New P: 0.970
iter 13 loss: 0.646
Actual params: [1.6325, 0.9698]
-Original Grad: -0.024, -lr * Pred Grad: 0.043, New P: 1.676
-Original Grad: -0.073, -lr * Pred Grad: 0.088, New P: 1.058
iter 14 loss: 0.676
Actual params: [1.6757, 1.0581]
-Original Grad: -0.045, -lr * Pred Grad: 0.034, New P: 1.710
-Original Grad: -0.077, -lr * Pred Grad: 0.088, New P: 1.146
iter 15 loss: 0.700
Actual params: [1.7099, 1.1463]
-Original Grad: -0.033, -lr * Pred Grad: -0.010, New P: 1.699
-Original Grad: -0.104, -lr * Pred Grad: 0.087, New P: 1.233
iter 16 loss: 0.740
Actual params: [1.6995, 1.2333]
-Original Grad: -0.039, -lr * Pred Grad: -0.023, New P: 1.676
-Original Grad: -0.113, -lr * Pred Grad: 0.070, New P: 1.303
iter 17 loss: 1.144
Actual params: [1.676, 1.303]
-Original Grad: -0.045, -lr * Pred Grad: -0.040, New P: 1.636
-Original Grad: -0.172, -lr * Pred Grad: -0.024, New P: 1.279
iter 18 loss: 1.173
Actual params: [1.6357, 1.2791]
-Original Grad: -0.048, -lr * Pred Grad: -0.044, New P: 1.592
-Original Grad: -0.143, -lr * Pred Grad: -0.047, New P: 1.232
iter 19 loss: 1.196
Actual params: [1.5916, 1.2323]
-Original Grad: -0.045, -lr * Pred Grad: -0.043, New P: 1.548
-Original Grad: -0.113, -lr * Pred Grad: -0.035, New P: 1.197
iter 20 loss: 1.221
Actual params: [1.5484, 1.1974]
-Original Grad: -0.018, -lr * Pred Grad: -0.034, New P: 1.515
-Original Grad: -0.135, -lr * Pred Grad: -0.049, New P: 1.148
Target params: [1.1812, 0.2779]
iter 0 loss: 0.743
Actual params: [0.5941, 0.5941]
-Original Grad: 0.084, -lr * Pred Grad: 0.064, New P: 0.658
-Original Grad: -0.708, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.512
Actual params: [0.6579, 0.532 ]
-Original Grad: 0.094, -lr * Pred Grad: 0.081, New P: 0.739
-Original Grad: -0.444, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.321
Actual params: [0.7393, 0.4514]
-Original Grad: 0.071, -lr * Pred Grad: 0.076, New P: 0.816
-Original Grad: -0.150, -lr * Pred Grad: -0.084, New P: 0.368
iter 3 loss: 0.260
Actual params: [0.8156, 0.3678]
-Original Grad: 0.018, -lr * Pred Grad: 0.012, New P: 0.827
-Original Grad: -0.032, -lr * Pred Grad: -0.084, New P: 0.284
iter 4 loss: 0.234
Actual params: [0.8273, 0.2837]
-Original Grad: 0.019, -lr * Pred Grad: 0.043, New P: 0.871
-Original Grad: 0.003, -lr * Pred Grad: -0.083, New P: 0.201
iter 5 loss: 0.220
Actual params: [0.8707, 0.2009]
-Original Grad: 0.022, -lr * Pred Grad: -0.003, New P: 0.868
-Original Grad: 0.010, -lr * Pred Grad: -0.075, New P: 0.126
iter 6 loss: 0.212
Actual params: [0.8681, 0.126 ]
-Original Grad: 0.027, -lr * Pred Grad: 0.038, New P: 0.906
-Original Grad: 0.020, -lr * Pred Grad: -0.070, New P: 0.056
iter 7 loss: 0.208
Actual params: [0.9061, 0.0561]
-Original Grad: 0.023, -lr * Pred Grad: -0.009, New P: 0.897
-Original Grad: 0.024, -lr * Pred Grad: -0.065, New P: -0.009
iter 8 loss: 0.209
Actual params: [ 0.8972, -0.009 ]
-Original Grad: 0.028, -lr * Pred Grad: 0.027, New P: 0.924
-Original Grad: 0.040, -lr * Pred Grad: -0.047, New P: -0.056
iter 9 loss: 0.210
Actual params: [ 0.9242, -0.056 ]
-Original Grad: 0.040, -lr * Pred Grad: 0.007, New P: 0.932
-Original Grad: 0.062, -lr * Pred Grad: -0.032, New P: -0.088
iter 10 loss: 0.212
Actual params: [ 0.9317, -0.0881]
-Original Grad: 0.041, -lr * Pred Grad: 0.036, New P: 0.967
-Original Grad: 0.070, -lr * Pred Grad: -0.025, New P: -0.113
iter 11 loss: 0.211
Actual params: [ 0.9673, -0.1127]
-Original Grad: 0.043, -lr * Pred Grad: 0.018, New P: 0.986
-Original Grad: 0.088, -lr * Pred Grad: -0.018, New P: -0.130
iter 12 loss: 0.211
Actual params: [ 0.9856, -0.1303]
-Original Grad: 0.041, -lr * Pred Grad: 0.035, New P: 1.021
-Original Grad: 0.101, -lr * Pred Grad: 0.003, New P: -0.128
iter 13 loss: 0.206
Actual params: [ 1.0208, -0.1277]
-Original Grad: 0.026, -lr * Pred Grad: 0.007, New P: 1.028
-Original Grad: 0.060, -lr * Pred Grad: 0.043, New P: -0.085
iter 14 loss: 0.203
Actual params: [ 1.0278, -0.0848]
-Original Grad: 0.028, -lr * Pred Grad: 0.020, New P: 1.048
-Original Grad: 0.065, -lr * Pred Grad: 0.066, New P: -0.019
iter 15 loss: 0.204
Actual params: [ 1.0481, -0.0186]
-Original Grad: 0.023, -lr * Pred Grad: 0.004, New P: 1.052
-Original Grad: 0.042, -lr * Pred Grad: 0.030, New P: 0.011
iter 16 loss: 0.204
Actual params: [1.0524, 0.0115]
-Original Grad: 0.015, -lr * Pred Grad: 0.005, New P: 1.058
-Original Grad: 0.028, -lr * Pred Grad: 0.044, New P: 0.056
iter 17 loss: 0.210
Actual params: [1.0577, 0.0555]
-Original Grad: 0.017, -lr * Pred Grad: 0.001, New P: 1.059
-Original Grad: 0.022, -lr * Pred Grad: -0.009, New P: 0.046
iter 18 loss: 0.208
Actual params: [1.0586, 0.0463]
-Original Grad: 0.012, -lr * Pred Grad: -0.001, New P: 1.058
-Original Grad: 0.027, -lr * Pred Grad: 0.028, New P: 0.075
iter 19 loss: 0.214
Actual params: [1.0577, 0.0746]
-Original Grad: 0.010, -lr * Pred Grad: -0.002, New P: 1.055
-Original Grad: 0.018, -lr * Pred Grad: -0.010, New P: 0.064
iter 20 loss: 0.212
Actual params: [1.0553, 0.0641]
-Original Grad: 0.018, -lr * Pred Grad: 0.001, New P: 1.057
-Original Grad: 0.028, -lr * Pred Grad: 0.021, New P: 0.085
Target params: [1.1812, 0.2779]
iter 0 loss: 0.596
Actual params: [0.5941, 0.5941]
-Original Grad: 0.547, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: 0.142, -lr * Pred Grad: 0.067, New P: 0.661
iter 1 loss: 0.464
Actual params: [0.6617, 0.6608]
-Original Grad: 0.469, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: 0.058, -lr * Pred Grad: 0.083, New P: 0.744
iter 2 loss: 0.317
Actual params: [0.7469, 0.7438]
-Original Grad: -0.094, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.110, -lr * Pred Grad: -0.009, New P: 0.734
iter 3 loss: 0.400
Actual params: [0.8347, 0.7345]
-Original Grad: -0.308, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.254, -lr * Pred Grad: -0.059, New P: 0.675
iter 4 loss: 0.407
Actual params: [0.9226, 0.6753]
-Original Grad: -0.188, -lr * Pred Grad: 0.076, New P: 0.999
-Original Grad: -0.291, -lr * Pred Grad: -0.073, New P: 0.602
iter 5 loss: 0.336
Actual params: [0.999 , 0.6019]
-Original Grad: -0.079, -lr * Pred Grad: -0.026, New P: 0.973
-Original Grad: -0.248, -lr * Pred Grad: -0.080, New P: 0.522
iter 6 loss: 0.265
Actual params: [0.9726, 0.5221]
-Original Grad: -0.110, -lr * Pred Grad: -0.047, New P: 0.926
-Original Grad: -0.196, -lr * Pred Grad: -0.082, New P: 0.440
iter 7 loss: 0.284
Actual params: [0.9261, 0.4396]
-Original Grad: -0.034, -lr * Pred Grad: -0.011, New P: 0.915
-Original Grad: -0.097, -lr * Pred Grad: -0.082, New P: 0.358
iter 8 loss: 0.359
Actual params: [0.9153, 0.3575]
-Original Grad: 0.086, -lr * Pred Grad: 0.044, New P: 0.959
-Original Grad: 0.046, -lr * Pred Grad: -0.074, New P: 0.284
iter 9 loss: 0.371
Actual params: [0.9592, 0.2836]
-Original Grad: 0.057, -lr * Pred Grad: 0.067, New P: 1.026
-Original Grad: 0.092, -lr * Pred Grad: -0.069, New P: 0.215
iter 10 loss: 0.383
Actual params: [1.0263, 0.2149]
-Original Grad: 0.055, -lr * Pred Grad: 0.036, New P: 1.063
-Original Grad: 0.182, -lr * Pred Grad: -0.056, New P: 0.159
iter 11 loss: 0.489
Actual params: [1.0627, 0.1585]
-Original Grad: 0.016, -lr * Pred Grad: 0.031, New P: 1.094
-Original Grad: 0.235, -lr * Pred Grad: -0.039, New P: 0.120
iter 12 loss: 1.134
Actual params: [1.0939, 0.1199]
-Original Grad: -0.004, -lr * Pred Grad: -0.021, New P: 1.073
-Original Grad: 0.327, -lr * Pred Grad: -0.027, New P: 0.093
iter 13 loss: 1.195
Actual params: [1.0729, 0.0933]
-Original Grad: 0.014, -lr * Pred Grad: 0.007, New P: 1.080
-Original Grad: 0.391, -lr * Pred Grad: -0.014, New P: 0.079
iter 14 loss: 1.214
Actual params: [1.0798, 0.0795]
-Original Grad: 0.020, -lr * Pred Grad: -0.001, New P: 1.079
-Original Grad: 0.427, -lr * Pred Grad: 0.010, New P: 0.089
iter 15 loss: 1.201
Actual params: [1.0791, 0.0895]
-Original Grad: 0.046, -lr * Pred Grad: 0.028, New P: 1.107
-Original Grad: 0.399, -lr * Pred Grad: 0.054, New P: 0.144
iter 16 loss: 0.527
Actual params: [1.1075, 0.1436]
-Original Grad: -0.020, -lr * Pred Grad: -0.018, New P: 1.089
-Original Grad: 0.340, -lr * Pred Grad: 0.081, New P: 0.225
iter 17 loss: 0.343
Actual params: [1.0893, 0.2248]
-Original Grad: 0.017, -lr * Pred Grad: -0.003, New P: 1.087
-Original Grad: 0.165, -lr * Pred Grad: 0.087, New P: 0.312
iter 18 loss: 0.277
Actual params: [1.0865, 0.312 ]
-Original Grad: 0.016, -lr * Pred Grad: -0.002, New P: 1.085
-Original Grad: 0.064, -lr * Pred Grad: 0.088, New P: 0.400
iter 19 loss: 0.249
Actual params: [1.0846, 0.4002]
-Original Grad: 0.001, -lr * Pred Grad: -0.006, New P: 1.079
-Original Grad: -0.087, -lr * Pred Grad: 0.088, New P: 0.488
iter 20 loss: 0.255
Actual params: [1.0785, 0.4884]
-Original Grad: -0.035, -lr * Pred Grad: -0.027, New P: 1.051
-Original Grad: -0.189, -lr * Pred Grad: 0.086, New P: 0.575
Target params: [1.1812, 0.2779]
iter 0 loss: 0.722
Actual params: [0.5941, 0.5941]
-Original Grad: 0.186, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.407, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.685
Actual params: [0.6614, 0.531 ]
-Original Grad: 0.149, -lr * Pred Grad: 0.084, New P: 0.746
-Original Grad: -0.356, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.602
Actual params: [0.7457, 0.4505]
-Original Grad: 0.232, -lr * Pred Grad: 0.087, New P: 0.833
-Original Grad: -0.332, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.433
Actual params: [0.8331, 0.3669]
-Original Grad: 0.209, -lr * Pred Grad: 0.088, New P: 0.921
-Original Grad: -0.265, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.255
Actual params: [0.9211, 0.2827]
-Original Grad: 0.075, -lr * Pred Grad: 0.088, New P: 1.009
-Original Grad: -0.049, -lr * Pred Grad: -0.084, New P: 0.198
iter 5 loss: 0.165
Actual params: [1.0091, 0.1984]
-Original Grad: 0.043, -lr * Pred Grad: 0.086, New P: 1.096
-Original Grad: 0.031, -lr * Pred Grad: -0.083, New P: 0.115
iter 6 loss: 0.178
Actual params: [1.0956, 0.1154]
-Original Grad: 0.044, -lr * Pred Grad: 0.047, New P: 1.142
-Original Grad: 0.041, -lr * Pred Grad: -0.075, New P: 0.041
iter 7 loss: 0.188
Actual params: [1.1424, 0.0407]
-Original Grad: 0.095, -lr * Pred Grad: 0.073, New P: 1.215
-Original Grad: 0.043, -lr * Pred Grad: -0.070, New P: -0.029
iter 8 loss: 0.177
Actual params: [ 1.2155, -0.0291]
-Original Grad: 0.092, -lr * Pred Grad: 0.076, New P: 1.291
-Original Grad: 0.024, -lr * Pred Grad: -0.063, New P: -0.093
iter 9 loss: 0.169
Actual params: [ 1.291 , -0.0925]
-Original Grad: 0.064, -lr * Pred Grad: 0.075, New P: 1.366
-Original Grad: 0.010, -lr * Pred Grad: -0.045, New P: -0.138
iter 10 loss: 0.166
Actual params: [ 1.3658, -0.1376]
-Original Grad: 0.028, -lr * Pred Grad: 0.039, New P: 1.405
-Original Grad: 0.000, -lr * Pred Grad: -0.035, New P: -0.173
iter 11 loss: 0.170
Actual params: [ 1.4051, -0.1726]
-Original Grad: 0.037, -lr * Pred Grad: 0.047, New P: 1.452
-Original Grad: -0.004, -lr * Pred Grad: -0.034, New P: -0.206
iter 12 loss: 0.170
Actual params: [ 1.4521, -0.2061]
-Original Grad: 0.018, -lr * Pred Grad: -0.007, New P: 1.445
-Original Grad: -0.003, -lr * Pred Grad: -0.033, New P: -0.239
iter 13 loss: 0.166
Actual params: [ 1.4451, -0.2395]
-Original Grad: 0.003, -lr * Pred Grad: 0.015, New P: 1.460
-Original Grad: -0.005, -lr * Pred Grad: -0.028, New P: -0.267
iter 14 loss: 0.166
Actual params: [ 1.4597, -0.2675]
-Original Grad: 0.020, -lr * Pred Grad: -0.016, New P: 1.444
-Original Grad: 0.005, -lr * Pred Grad: -0.012, New P: -0.280
iter 15 loss: 0.168
Actual params: [ 1.4441, -0.28  ]
-Original Grad: 0.018, -lr * Pred Grad: 0.008, New P: 1.452
-Original Grad: 0.020, -lr * Pred Grad: -0.002, New P: -0.282
iter 16 loss: 0.168
Actual params: [ 1.4523, -0.2822]
-Original Grad: 0.023, -lr * Pred Grad: -0.002, New P: 1.451
-Original Grad: 0.003, -lr * Pred Grad: -0.012, New P: -0.294
iter 17 loss: 0.166
Actual params: [ 1.4505, -0.2941]
-Original Grad: 0.011, -lr * Pred Grad: 0.000, New P: 1.451
-Original Grad: -0.004, -lr * Pred Grad: -0.021, New P: -0.315
iter 18 loss: 0.162
Actual params: [ 1.4505, -0.3146]
-Original Grad: 0.027, -lr * Pred Grad: 0.005, New P: 1.455
-Original Grad: 0.009, -lr * Pred Grad: -0.020, New P: -0.335
iter 19 loss: 0.159
Actual params: [ 1.4553, -0.3347]
-Original Grad: 0.022, -lr * Pred Grad: 0.007, New P: 1.462
-Original Grad: 0.014, -lr * Pred Grad: -0.014, New P: -0.349
iter 20 loss: 0.157
Actual params: [ 1.4621, -0.3486]
-Original Grad: 0.009, -lr * Pred Grad: -0.001, New P: 1.461
-Original Grad: 0.004, -lr * Pred Grad: -0.014, New P: -0.363
Target params: [1.1812, 0.2779]
iter 0 loss: 0.519
Actual params: [0.5941, 0.5941]
-Original Grad: 0.118, -lr * Pred Grad: 0.066, New P: 0.660
-Original Grad: -0.197, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.509
Actual params: [0.6601, 0.5321]
-Original Grad: 0.162, -lr * Pred Grad: 0.084, New P: 0.744
-Original Grad: -0.070, -lr * Pred Grad: -0.080, New P: 0.452
iter 2 loss: 0.503
Actual params: [0.7438, 0.4522]
-Original Grad: 0.126, -lr * Pred Grad: 0.086, New P: 0.830
-Original Grad: 0.021, -lr * Pred Grad: -0.077, New P: 0.375
iter 3 loss: 0.493
Actual params: [0.8302, 0.3753]
-Original Grad: 0.007, -lr * Pred Grad: 0.080, New P: 0.910
-Original Grad: 0.007, -lr * Pred Grad: -0.070, New P: 0.305
iter 4 loss: 0.482
Actual params: [0.91  , 0.3049]
-Original Grad: -0.006, -lr * Pred Grad: -0.003, New P: 0.907
-Original Grad: 0.014, -lr * Pred Grad: -0.066, New P: 0.239
iter 5 loss: 0.462
Actual params: [0.9071, 0.2392]
-Original Grad: 0.029, -lr * Pred Grad: 0.042, New P: 0.949
-Original Grad: 0.046, -lr * Pred Grad: -0.048, New P: 0.192
iter 6 loss: 0.453
Actual params: [0.9493, 0.1915]
-Original Grad: -0.008, -lr * Pred Grad: -0.010, New P: 0.939
-Original Grad: 0.056, -lr * Pred Grad: -0.025, New P: 0.167
iter 7 loss: 0.448
Actual params: [0.9389, 0.1669]
-Original Grad: 0.031, -lr * Pred Grad: 0.029, New P: 0.968
-Original Grad: 0.092, -lr * Pred Grad: 0.008, New P: 0.175
iter 8 loss: 0.449
Actual params: [0.9679, 0.1751]
-Original Grad: 0.001, -lr * Pred Grad: -0.021, New P: 0.947
-Original Grad: 0.051, -lr * Pred Grad: 0.050, New P: 0.225
iter 9 loss: 0.461
Actual params: [0.9471, 0.2251]
-Original Grad: 0.006, -lr * Pred Grad: -0.001, New P: 0.946
-Original Grad: 0.042, -lr * Pred Grad: 0.043, New P: 0.269
iter 10 loss: 0.473
Actual params: [0.9462, 0.2685]
-Original Grad: -0.006, -lr * Pred Grad: -0.022, New P: 0.925
-Original Grad: 0.039, -lr * Pred Grad: 0.029, New P: 0.298
iter 11 loss: 0.481
Actual params: [0.9246, 0.2976]
-Original Grad: -0.030, -lr * Pred Grad: -0.028, New P: 0.897
-Original Grad: 0.030, -lr * Pred Grad: 0.025, New P: 0.322
iter 12 loss: 0.486
Actual params: [0.897 , 0.3223]
-Original Grad: -0.026, -lr * Pred Grad: -0.031, New P: 0.866
-Original Grad: -0.005, -lr * Pred Grad: -0.013, New P: 0.310
iter 13 loss: 0.482
Actual params: [0.8657, 0.3096]
-Original Grad: 0.048, -lr * Pred Grad: 0.000, New P: 0.866
-Original Grad: 0.053, -lr * Pred Grad: 0.026, New P: 0.336
iter 14 loss: 0.485
Actual params: [0.8658, 0.3359]
-Original Grad: -0.004, -lr * Pred Grad: 0.004, New P: 0.869
-Original Grad: 0.009, -lr * Pred Grad: -0.001, New P: 0.335
iter 15 loss: 0.485
Actual params: [0.8695, 0.3349]
-Original Grad: 0.008, -lr * Pred Grad: -0.005, New P: 0.864
-Original Grad: 0.015, -lr * Pred Grad: 0.005, New P: 0.340
iter 16 loss: 0.486
Actual params: [0.8641, 0.3402]
-Original Grad: -0.007, -lr * Pred Grad: -0.015, New P: 0.850
-Original Grad: 0.009, -lr * Pred Grad: -0.009, New P: 0.331
iter 17 loss: 0.487
Actual params: [0.8495, 0.3315]
-Original Grad: 0.047, -lr * Pred Grad: 0.007, New P: 0.857
-Original Grad: 0.044, -lr * Pred Grad: 0.018, New P: 0.349
iter 18 loss: 0.487
Actual params: [0.8565, 0.3492]
-Original Grad: -0.008, -lr * Pred Grad: -0.001, New P: 0.856
-Original Grad: 0.003, -lr * Pred Grad: 0.001, New P: 0.350
iter 19 loss: 0.487
Actual params: [0.8556, 0.3497]
-Original Grad: -0.004, -lr * Pred Grad: -0.014, New P: 0.841
-Original Grad: 0.009, -lr * Pred Grad: -0.005, New P: 0.345
iter 20 loss: 0.489
Actual params: [0.8414, 0.3448]
-Original Grad: 0.014, -lr * Pred Grad: -0.009, New P: 0.832
-Original Grad: 0.019, -lr * Pred Grad: -0.001, New P: 0.344
Target params: [1.1812, 0.2779]
iter 0 loss: 0.729
Actual params: [0.5941, 0.5941]
-Original Grad: -0.086, -lr * Pred Grad: -0.041, New P: 0.553
-Original Grad: -0.364, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.649
Actual params: [0.5527, 0.5309]
-Original Grad: -0.095, -lr * Pred Grad: -0.074, New P: 0.479
-Original Grad: -0.391, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.543
Actual params: [0.4787, 0.4505]
-Original Grad: -0.043, -lr * Pred Grad: -0.074, New P: 0.404
-Original Grad: -0.261, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.458
Actual params: [0.4042, 0.3669]
-Original Grad: 0.001, -lr * Pred Grad: -0.069, New P: 0.335
-Original Grad: -0.180, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.393
Actual params: [0.3347, 0.2827]
-Original Grad: 0.034, -lr * Pred Grad: -0.062, New P: 0.272
-Original Grad: -0.112, -lr * Pred Grad: -0.084, New P: 0.198
iter 5 loss: 0.367
Actual params: [0.2724, 0.1984]
-Original Grad: 0.094, -lr * Pred Grad: -0.038, New P: 0.234
-Original Grad: -0.050, -lr * Pred Grad: -0.084, New P: 0.114
iter 6 loss: 0.353
Actual params: [0.2344, 0.1143]
-Original Grad: 0.054, -lr * Pred Grad: -0.006, New P: 0.228
-Original Grad: -0.035, -lr * Pred Grad: -0.082, New P: 0.033
iter 7 loss: 0.349
Actual params: [0.228 , 0.0327]
-Original Grad: 0.080, -lr * Pred Grad: 0.039, New P: 0.266
-Original Grad: -0.040, -lr * Pred Grad: -0.074, New P: -0.042
iter 8 loss: 0.343
Actual params: [ 0.2665, -0.0416]
-Original Grad: 0.027, -lr * Pred Grad: 0.053, New P: 0.320
-Original Grad: -0.055, -lr * Pred Grad: -0.070, New P: -0.112
iter 9 loss: 0.338
Actual params: [ 0.3198, -0.1118]
-Original Grad: 0.011, -lr * Pred Grad: -0.009, New P: 0.311
-Original Grad: -0.033, -lr * Pred Grad: -0.068, New P: -0.180
iter 10 loss: 0.325
Actual params: [ 0.3106, -0.1799]
-Original Grad: 0.033, -lr * Pred Grad: 0.033, New P: 0.343
-Original Grad: -0.038, -lr * Pred Grad: -0.065, New P: -0.245
iter 11 loss: 0.319
Actual params: [ 0.3432, -0.2446]
-Original Grad: 0.014, -lr * Pred Grad: -0.010, New P: 0.334
-Original Grad: -0.022, -lr * Pred Grad: -0.058, New P: -0.302
iter 12 loss: 0.311
Actual params: [ 0.3337, -0.3024]
-Original Grad: 0.051, -lr * Pred Grad: 0.037, New P: 0.371
-Original Grad: -0.015, -lr * Pred Grad: -0.048, New P: -0.350
iter 13 loss: 0.305
Actual params: [ 0.3707, -0.3504]
-Original Grad: 0.052, -lr * Pred Grad: 0.029, New P: 0.400
-Original Grad: 0.006, -lr * Pred Grad: -0.039, New P: -0.389
iter 14 loss: 0.299
Actual params: [ 0.4002, -0.389 ]
-Original Grad: 0.032, -lr * Pred Grad: 0.031, New P: 0.431
-Original Grad: -0.007, -lr * Pred Grad: -0.035, New P: -0.424
iter 15 loss: 0.299
Actual params: [ 0.4308, -0.4236]
-Original Grad: 0.002, -lr * Pred Grad: -0.011, New P: 0.420
-Original Grad: 0.023, -lr * Pred Grad: -0.030, New P: -0.454
iter 16 loss: 0.289
Actual params: [ 0.4202, -0.4536]
-Original Grad: 0.032, -lr * Pred Grad: 0.017, New P: 0.437
-Original Grad: 0.043, -lr * Pred Grad: -0.024, New P: -0.478
iter 17 loss: 0.284
Actual params: [ 0.437 , -0.4775]
-Original Grad: 0.030, -lr * Pred Grad: 0.009, New P: 0.446
-Original Grad: 0.043, -lr * Pred Grad: -0.022, New P: -0.499
iter 18 loss: 0.278
Actual params: [ 0.4461, -0.499 ]
-Original Grad: 0.021, -lr * Pred Grad: 0.013, New P: 0.459
-Original Grad: 0.050, -lr * Pred Grad: -0.017, New P: -0.516
iter 19 loss: 0.277
Actual params: [ 0.4588, -0.5163]
-Original Grad: 0.030, -lr * Pred Grad: 0.011, New P: 0.470
-Original Grad: 0.049, -lr * Pred Grad: -0.003, New P: -0.519
iter 20 loss: 0.278
Actual params: [ 0.4695, -0.519 ]
-Original Grad: 0.036, -lr * Pred Grad: 0.017, New P: 0.486
-Original Grad: 0.040, -lr * Pred Grad: 0.020, New P: -0.499
Target params: [1.1812, 0.2779]
iter 0 loss: 0.763
Actual params: [0.5941, 0.5941]
-Original Grad: 0.355, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.214, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.680
Actual params: [0.6617, 0.5317]
-Original Grad: 0.428, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.155, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.578
Actual params: [0.7468, 0.4512]
-Original Grad: 0.178, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: 0.011, -lr * Pred Grad: -0.082, New P: 0.370
iter 3 loss: 0.475
Actual params: [0.8346, 0.3695]
-Original Grad: 0.159, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.082, -lr * Pred Grad: -0.073, New P: 0.296
iter 4 loss: 0.387
Actual params: [0.9229, 0.2963]
-Original Grad: 0.033, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.091, -lr * Pred Grad: -0.067, New P: 0.229
iter 5 loss: 0.346
Actual params: [1.0112, 0.2295]
-Original Grad: -0.034, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 0.101, -lr * Pred Grad: -0.046, New P: 0.183
iter 6 loss: 0.358
Actual params: [1.0994, 0.1831]
-Original Grad: -0.046, -lr * Pred Grad: 0.088, New P: 1.187
-Original Grad: 0.080, -lr * Pred Grad: -0.015, New P: 0.168
iter 7 loss: 0.383
Actual params: [1.1875, 0.1684]
-Original Grad: -0.051, -lr * Pred Grad: 0.085, New P: 1.273
-Original Grad: 0.100, -lr * Pred Grad: 0.034, New P: 0.202
iter 8 loss: 0.471
Actual params: [1.2728, 0.2025]
-Original Grad: -0.119, -lr * Pred Grad: 0.018, New P: 1.291
-Original Grad: 0.102, -lr * Pred Grad: 0.072, New P: 0.275
iter 9 loss: 0.480
Actual params: [1.2912, 0.2746]
-Original Grad: -0.163, -lr * Pred Grad: -0.033, New P: 1.258
-Original Grad: 0.109, -lr * Pred Grad: 0.080, New P: 0.355
iter 10 loss: 0.396
Actual params: [1.2583, 0.3549]
-Original Grad: -0.092, -lr * Pred Grad: -0.045, New P: 1.214
-Original Grad: 0.092, -lr * Pred Grad: 0.080, New P: 0.434
iter 11 loss: 0.327
Actual params: [1.2135, 0.4345]
-Original Grad: -0.013, -lr * Pred Grad: -0.005, New P: 1.208
-Original Grad: 0.087, -lr * Pred Grad: 0.077, New P: 0.512
iter 12 loss: 0.274
Actual params: [1.2081, 0.5116]
-Original Grad: 0.013, -lr * Pred Grad: 0.028, New P: 1.236
-Original Grad: 0.011, -lr * Pred Grad: 0.033, New P: 0.545
iter 13 loss: 0.265
Actual params: [1.2357, 0.5451]
-Original Grad: 0.049, -lr * Pred Grad: 0.024, New P: 1.259
-Original Grad: -0.121, -lr * Pred Grad: -0.039, New P: 0.506
iter 14 loss: 0.315
Actual params: [1.2593, 0.5058]
-Original Grad: 0.002, -lr * Pred Grad: 0.004, New P: 1.264
-Original Grad: 0.011, -lr * Pred Grad: -0.039, New P: 0.467
iter 15 loss: 0.351
Actual params: [1.2636, 0.4666]
-Original Grad: -0.034, -lr * Pred Grad: -0.028, New P: 1.236
-Original Grad: 0.078, -lr * Pred Grad: 0.016, New P: 0.483
iter 16 loss: 0.312
Actual params: [1.2357, 0.4827]
-Original Grad: -0.014, -lr * Pred Grad: -0.028, New P: 1.207
-Original Grad: 0.070, -lr * Pred Grad: 0.060, New P: 0.542
iter 17 loss: 0.255
Actual params: [1.2073, 0.5425]
-Original Grad: 0.041, -lr * Pred Grad: 0.005, New P: 1.212
-Original Grad: -0.108, -lr * Pred Grad: -0.034, New P: 0.508
iter 18 loss: 0.279
Actual params: [1.212 , 0.5084]
-Original Grad: 0.030, -lr * Pred Grad: 0.024, New P: 1.236
-Original Grad: -0.006, -lr * Pred Grad: -0.043, New P: 0.466
iter 19 loss: 0.320
Actual params: [1.2358, 0.4655]
-Original Grad: -0.007, -lr * Pred Grad: -0.007, New P: 1.228
-Original Grad: 0.054, -lr * Pred Grad: 0.003, New P: 0.469
iter 20 loss: 0.317
Actual params: [1.2285, 0.4688]
-Original Grad: -0.027, -lr * Pred Grad: -0.026, New P: 1.203
-Original Grad: 0.094, -lr * Pred Grad: 0.055, New P: 0.524
Target params: [1.1812, 0.2779]
iter 0 loss: 0.333
Actual params: [0.5941, 0.5941]
-Original Grad: 0.084, -lr * Pred Grad: 0.064, New P: 0.658
-Original Grad: 0.017, -lr * Pred Grad: 0.046, New P: 0.640
iter 1 loss: 0.350
Actual params: [0.658 , 0.6402]
-Original Grad: 0.041, -lr * Pred Grad: 0.077, New P: 0.734
-Original Grad: 0.019, -lr * Pred Grad: 0.001, New P: 0.641
iter 2 loss: 0.405
Actual params: [0.7345, 0.6409]
-Original Grad: 0.042, -lr * Pred Grad: 0.020, New P: 0.754
-Original Grad: -0.007, -lr * Pred Grad: -0.044, New P: 0.597
iter 3 loss: 0.397
Actual params: [0.7542, 0.5973]
-Original Grad: -0.003, -lr * Pred Grad: 0.002, New P: 0.757
-Original Grad: 0.010, -lr * Pred Grad: -0.035, New P: 0.563
iter 4 loss: 0.380
Actual params: [0.7566, 0.5625]
-Original Grad: 0.018, -lr * Pred Grad: 0.001, New P: 0.758
-Original Grad: 0.005, -lr * Pred Grad: -0.000, New P: 0.562
iter 5 loss: 0.382
Actual params: [0.7581, 0.5625]
-Original Grad: -0.021, -lr * Pred Grad: -0.011, New P: 0.747
-Original Grad: 0.011, -lr * Pred Grad: 0.006, New P: 0.569
iter 6 loss: 0.375
Actual params: [0.747 , 0.5685]
-Original Grad: 0.019, -lr * Pred Grad: -0.001, New P: 0.746
-Original Grad: -0.000, -lr * Pred Grad: -0.008, New P: 0.561
iter 7 loss: 0.370
Actual params: [0.7457, 0.5608]
-Original Grad: 0.010, -lr * Pred Grad: -0.006, New P: 0.740
-Original Grad: -0.005, -lr * Pred Grad: -0.012, New P: 0.548
iter 8 loss: 0.360
Actual params: [0.7399, 0.5484]
-Original Grad: 0.025, -lr * Pred Grad: 0.005, New P: 0.745
-Original Grad: 0.000, -lr * Pred Grad: -0.013, New P: 0.535
iter 9 loss: 0.358
Actual params: [0.7449, 0.5351]
-Original Grad: 0.038, -lr * Pred Grad: 0.016, New P: 0.761
-Original Grad: 0.015, -lr * Pred Grad: -0.003, New P: 0.532
iter 10 loss: 0.368
Actual params: [0.7608, 0.5317]
-Original Grad: -0.018, -lr * Pred Grad: -0.013, New P: 0.748
-Original Grad: 0.011, -lr * Pred Grad: -0.001, New P: 0.531
iter 11 loss: 0.358
Actual params: [0.7476, 0.5308]
-Original Grad: 0.031, -lr * Pred Grad: -0.001, New P: 0.746
-Original Grad: -0.002, -lr * Pred Grad: -0.009, New P: 0.522
iter 12 loss: 0.354
Actual params: [0.7464, 0.5221]
-Original Grad: 0.029, -lr * Pred Grad: 0.012, New P: 0.758
-Original Grad: 0.030, -lr * Pred Grad: 0.002, New P: 0.524
iter 13 loss: 0.362
Actual params: [0.758 , 0.5237]
-Original Grad: -0.001, -lr * Pred Grad: -0.003, New P: 0.755
-Original Grad: -0.000, -lr * Pred Grad: -0.003, New P: 0.521
iter 14 loss: 0.359
Actual params: [0.7548, 0.5205]
-Original Grad: 0.021, -lr * Pred Grad: -0.001, New P: 0.754
-Original Grad: 0.003, -lr * Pred Grad: -0.009, New P: 0.511
iter 15 loss: 0.355
Actual params: [0.7541, 0.5111]
-Original Grad: -0.011, -lr * Pred Grad: -0.012, New P: 0.743
-Original Grad: 0.004, -lr * Pred Grad: -0.010, New P: 0.501
iter 16 loss: 0.346
Actual params: [0.7425, 0.5008]
-Original Grad: 0.023, -lr * Pred Grad: -0.003, New P: 0.740
-Original Grad: -0.003, -lr * Pred Grad: -0.012, New P: 0.489
iter 17 loss: 0.341
Actual params: [0.7396, 0.4893]
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: 0.740
-Original Grad: 0.005, -lr * Pred Grad: -0.009, New P: 0.480
iter 18 loss: 0.339
Actual params: [0.7404, 0.4802]
-Original Grad: 0.017, -lr * Pred Grad: 0.003, New P: 0.744
-Original Grad: 0.017, -lr * Pred Grad: -0.000, New P: 0.480
iter 19 loss: 0.340
Actual params: [0.7436, 0.4799]
-Original Grad: 0.040, -lr * Pred Grad: 0.015, New P: 0.758
-Original Grad: -0.003, -lr * Pred Grad: -0.004, New P: 0.476
iter 20 loss: 0.346
Actual params: [0.7581, 0.4761]
-Original Grad: 0.026, -lr * Pred Grad: 0.017, New P: 0.775
-Original Grad: 0.007, -lr * Pred Grad: -0.006, New P: 0.471
Target params: [1.1812, 0.2779]
iter 0 loss: 0.324
Actual params: [0.5941, 0.5941]
-Original Grad: 0.338, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.142, -lr * Pred Grad: -0.058, New P: 0.536
iter 1 loss: 0.268
Actual params: [0.6617, 0.5361]
-Original Grad: 0.151, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.012, -lr * Pred Grad: -0.073, New P: 0.463
iter 2 loss: 0.245
Actual params: [0.7466, 0.4627]
-Original Grad: -0.062, -lr * Pred Grad: 0.087, New P: 0.834
-Original Grad: -0.023, -lr * Pred Grad: -0.070, New P: 0.392
iter 3 loss: 0.257
Actual params: [0.8339, 0.3924]
-Original Grad: -0.069, -lr * Pred Grad: 0.083, New P: 0.917
-Original Grad: -0.012, -lr * Pred Grad: -0.068, New P: 0.325
iter 4 loss: 0.238
Actual params: [0.9174, 0.3249]
-Original Grad: 0.021, -lr * Pred Grad: 0.004, New P: 0.922
-Original Grad: -0.006, -lr * Pred Grad: -0.061, New P: 0.264
iter 5 loss: 0.229
Actual params: [0.9215, 0.2637]
-Original Grad: 0.024, -lr * Pred Grad: 0.050, New P: 0.972
-Original Grad: -0.005, -lr * Pred Grad: -0.047, New P: 0.217
iter 6 loss: 0.220
Actual params: [0.9717, 0.2168]
-Original Grad: 0.030, -lr * Pred Grad: 0.053, New P: 1.024
-Original Grad: -0.001, -lr * Pred Grad: -0.030, New P: 0.187
iter 7 loss: 0.218
Actual params: [1.0245, 0.1871]
-Original Grad: 0.015, -lr * Pred Grad: 0.034, New P: 1.058
-Original Grad: -0.005, -lr * Pred Grad: -0.011, New P: 0.176
iter 8 loss: 0.214
Actual params: [1.0585, 0.176 ]
-Original Grad: 0.016, -lr * Pred Grad: 0.013, New P: 1.072
-Original Grad: -0.020, -lr * Pred Grad: -0.024, New P: 0.152
iter 9 loss: 0.216
Actual params: [1.0718, 0.1522]
-Original Grad: 0.003, -lr * Pred Grad: -0.013, New P: 1.059
-Original Grad: -0.004, -lr * Pred Grad: -0.027, New P: 0.125
iter 10 loss: 0.216
Actual params: [1.0593, 0.1254]
-Original Grad: 0.012, -lr * Pred Grad: -0.005, New P: 1.055
-Original Grad: -0.007, -lr * Pred Grad: -0.025, New P: 0.100
iter 11 loss: 0.217
Actual params: [1.0547, 0.1001]
-Original Grad: 0.008, -lr * Pred Grad: -0.009, New P: 1.046
-Original Grad: 0.002, -lr * Pred Grad: -0.022, New P: 0.079
iter 12 loss: 0.217
Actual params: [1.0456, 0.0786]
-Original Grad: 0.010, -lr * Pred Grad: -0.006, New P: 1.040
-Original Grad: 0.001, -lr * Pred Grad: -0.019, New P: 0.060
iter 13 loss: 0.217
Actual params: [1.0395, 0.0598]
-Original Grad: 0.010, -lr * Pred Grad: -0.007, New P: 1.033
-Original Grad: -0.008, -lr * Pred Grad: -0.022, New P: 0.038
iter 14 loss: 0.218
Actual params: [1.033 , 0.0383]
-Original Grad: 0.008, -lr * Pred Grad: -0.006, New P: 1.026
-Original Grad: -0.001, -lr * Pred Grad: -0.023, New P: 0.015
iter 15 loss: 0.218
Actual params: [1.0265, 0.0154]
-Original Grad: 0.006, -lr * Pred Grad: -0.008, New P: 1.019
-Original Grad: -0.006, -lr * Pred Grad: -0.024, New P: -0.008
iter 16 loss: 0.219
Actual params: [ 1.0188, -0.0085]
-Original Grad: 0.003, -lr * Pred Grad: -0.009, New P: 1.010
-Original Grad: 0.002, -lr * Pred Grad: -0.022, New P: -0.031
iter 17 loss: 0.219
Actual params: [ 1.0095, -0.0307]
-Original Grad: 0.005, -lr * Pred Grad: -0.009, New P: 1.001
-Original Grad: -0.002, -lr * Pred Grad: -0.021, New P: -0.052
iter 18 loss: 0.220
Actual params: [ 1.001 , -0.0518]
-Original Grad: 0.014, -lr * Pred Grad: -0.002, New P: 0.999
-Original Grad: 0.009, -lr * Pred Grad: -0.017, New P: -0.069
iter 19 loss: 0.220
Actual params: [ 0.9988, -0.0692]
-Original Grad: 0.007, -lr * Pred Grad: -0.002, New P: 0.997
-Original Grad: -0.002, -lr * Pred Grad: -0.018, New P: -0.087
iter 20 loss: 0.221
Actual params: [ 0.997 , -0.0872]
-Original Grad: -0.000, -lr * Pred Grad: -0.007, New P: 0.990
-Original Grad: 0.003, -lr * Pred Grad: -0.019, New P: -0.107
Target params: [1.1812, 0.2779]
iter 0 loss: 0.541
Actual params: [0.5941, 0.5941]
-Original Grad: 0.479, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.330, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.494
Actual params: [0.6617, 0.5309]
-Original Grad: 0.737, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.257, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.434
Actual params: [0.7469, 0.4504]
-Original Grad: 0.598, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.135, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.359
Actual params: [0.8348, 0.3668]
-Original Grad: 0.276, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.020, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.271
Actual params: [0.923, 0.283]
-Original Grad: 0.070, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.025, -lr * Pred Grad: -0.079, New P: 0.204
iter 5 loss: 0.222
Actual params: [1.0114, 0.2041]
-Original Grad: 0.018, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: 0.024, -lr * Pred Grad: -0.071, New P: 0.133
iter 6 loss: 0.201
Actual params: [1.0997, 0.133 ]
-Original Grad: 0.030, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.047, -lr * Pred Grad: -0.067, New P: 0.066
iter 7 loss: 0.185
Actual params: [1.188 , 0.0656]
-Original Grad: -0.006, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.057, -lr * Pred Grad: -0.051, New P: 0.014
iter 8 loss: 0.176
Actual params: [1.2763, 0.0143]
-Original Grad: -0.018, -lr * Pred Grad: 0.088, New P: 1.365
-Original Grad: 0.076, -lr * Pred Grad: -0.034, New P: -0.019
iter 9 loss: 0.168
Actual params: [ 1.3645, -0.0194]
-Original Grad: -0.039, -lr * Pred Grad: 0.088, New P: 1.452
-Original Grad: 0.070, -lr * Pred Grad: -0.024, New P: -0.043
iter 10 loss: 0.172
Actual params: [ 1.4521, -0.043 ]
-Original Grad: -0.077, -lr * Pred Grad: 0.080, New P: 1.532
-Original Grad: 0.138, -lr * Pred Grad: -0.010, New P: -0.053
iter 11 loss: 0.175
Actual params: [ 1.5321, -0.0527]
-Original Grad: -0.077, -lr * Pred Grad: 0.055, New P: 1.587
-Original Grad: 0.145, -lr * Pred Grad: 0.027, New P: -0.026
iter 12 loss: 0.175
Actual params: [ 1.5871, -0.0261]
-Original Grad: -0.094, -lr * Pred Grad: -0.019, New P: 1.568
-Original Grad: 0.120, -lr * Pred Grad: 0.067, New P: 0.041
iter 13 loss: 0.170
Actual params: [1.5682, 0.0411]
-Original Grad: -0.066, -lr * Pred Grad: -0.010, New P: 1.559
-Original Grad: 0.109, -lr * Pred Grad: 0.082, New P: 0.123
iter 14 loss: 0.162
Actual params: [1.5585, 0.1234]
-Original Grad: -0.061, -lr * Pred Grad: 0.014, New P: 1.572
-Original Grad: 0.093, -lr * Pred Grad: 0.082, New P: 0.205
iter 15 loss: 0.160
Actual params: [1.5725, 0.2053]
-Original Grad: -0.060, -lr * Pred Grad: -0.010, New P: 1.563
-Original Grad: 0.085, -lr * Pred Grad: 0.077, New P: 0.283
iter 16 loss: 0.157
Actual params: [1.5626, 0.2827]
-Original Grad: -0.063, -lr * Pred Grad: -0.032, New P: 1.531
-Original Grad: 0.045, -lr * Pred Grad: 0.058, New P: 0.340
iter 17 loss: 0.152
Actual params: [1.5307, 0.3403]
-Original Grad: -0.041, -lr * Pred Grad: -0.039, New P: 1.492
-Original Grad: 0.029, -lr * Pred Grad: 0.041, New P: 0.381
iter 18 loss: 0.147
Actual params: [1.4919, 0.3815]
-Original Grad: -0.032, -lr * Pred Grad: -0.035, New P: 1.457
-Original Grad: 0.029, -lr * Pred Grad: 0.022, New P: 0.403
iter 19 loss: 0.143
Actual params: [1.4569, 0.403 ]
-Original Grad: -0.032, -lr * Pred Grad: -0.031, New P: 1.426
-Original Grad: 0.028, -lr * Pred Grad: 0.020, New P: 0.423
iter 20 loss: 0.140
Actual params: [1.426 , 0.4231]
-Original Grad: -0.024, -lr * Pred Grad: -0.028, New P: 1.398
-Original Grad: 0.016, -lr * Pred Grad: -0.004, New P: 0.419
Target params: [1.1812, 0.2779]
iter 0 loss: 0.743
Actual params: [0.5941, 0.5941]
-Original Grad: 0.068, -lr * Pred Grad: 0.062, New P: 0.656
-Original Grad: -0.622, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.511
Actual params: [0.656 , 0.5317]
-Original Grad: 0.065, -lr * Pred Grad: 0.077, New P: 0.733
-Original Grad: -0.379, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.322
Actual params: [0.7328, 0.4511]
-Original Grad: 0.060, -lr * Pred Grad: 0.042, New P: 0.775
-Original Grad: -0.101, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.259
Actual params: [0.7747, 0.3675]
-Original Grad: 0.039, -lr * Pred Grad: 0.038, New P: 0.813
-Original Grad: -0.017, -lr * Pred Grad: -0.084, New P: 0.284
iter 4 loss: 0.233
Actual params: [0.8131, 0.2835]
-Original Grad: 0.022, -lr * Pred Grad: 0.013, New P: 0.826
-Original Grad: 0.007, -lr * Pred Grad: -0.081, New P: 0.202
iter 5 loss: 0.218
Actual params: [0.8264, 0.2023]
-Original Grad: 0.034, -lr * Pred Grad: 0.041, New P: 0.868
-Original Grad: 0.019, -lr * Pred Grad: -0.073, New P: 0.130
iter 6 loss: 0.213
Actual params: [0.8677, 0.1297]
-Original Grad: 0.029, -lr * Pred Grad: 0.005, New P: 0.872
-Original Grad: 0.024, -lr * Pred Grad: -0.069, New P: 0.061
iter 7 loss: 0.208
Actual params: [0.8724, 0.0606]
-Original Grad: 0.030, -lr * Pred Grad: 0.031, New P: 0.903
-Original Grad: 0.025, -lr * Pred Grad: -0.061, New P: 0.000
iter 8 loss: 0.209
Actual params: [9.0350e-01, 7.6149e-05]
-Original Grad: 0.027, -lr * Pred Grad: -0.003, New P: 0.900
-Original Grad: 0.042, -lr * Pred Grad: -0.041, New P: -0.041
iter 9 loss: 0.210
Actual params: [ 0.9002, -0.041 ]
-Original Grad: 0.038, -lr * Pred Grad: 0.031, New P: 0.931
-Original Grad: 0.065, -lr * Pred Grad: -0.029, New P: -0.070
iter 10 loss: 0.210
Actual params: [ 0.931 , -0.0696]
-Original Grad: 0.038, -lr * Pred Grad: 0.013, New P: 0.944
-Original Grad: 0.060, -lr * Pred Grad: -0.022, New P: -0.091
iter 11 loss: 0.211
Actual params: [ 0.9435, -0.0911]
-Original Grad: 0.039, -lr * Pred Grad: 0.032, New P: 0.976
-Original Grad: 0.064, -lr * Pred Grad: -0.007, New P: -0.098
iter 12 loss: 0.208
Actual params: [ 0.9758, -0.0985]
-Original Grad: 0.038, -lr * Pred Grad: 0.017, New P: 0.993
-Original Grad: 0.057, -lr * Pred Grad: 0.027, New P: -0.071
iter 13 loss: 0.206
Actual params: [ 0.9931, -0.0715]
-Original Grad: 0.034, -lr * Pred Grad: 0.025, New P: 1.018
-Original Grad: 0.057, -lr * Pred Grad: 0.055, New P: -0.017
iter 14 loss: 0.204
Actual params: [ 1.0182, -0.0167]
-Original Grad: 0.019, -lr * Pred Grad: 0.005, New P: 1.024
-Original Grad: 0.028, -lr * Pred Grad: 0.014, New P: -0.003
iter 15 loss: 0.205
Actual params: [ 1.0237, -0.0026]
-Original Grad: 0.018, -lr * Pred Grad: 0.006, New P: 1.029
-Original Grad: 0.028, -lr * Pred Grad: 0.034, New P: 0.031
iter 16 loss: 0.206
Actual params: [1.0292, 0.0312]
-Original Grad: 0.016, -lr * Pred Grad: 0.001, New P: 1.031
-Original Grad: 0.027, -lr * Pred Grad: -0.005, New P: 0.027
iter 17 loss: 0.205
Actual params: [1.0305, 0.0266]
-Original Grad: 0.020, -lr * Pred Grad: 0.004, New P: 1.034
-Original Grad: 0.027, -lr * Pred Grad: 0.025, New P: 0.051
iter 18 loss: 0.208
Actual params: [1.0342, 0.0511]
-Original Grad: 0.018, -lr * Pred Grad: 0.004, New P: 1.039
-Original Grad: 0.024, -lr * Pred Grad: -0.001, New P: 0.050
iter 19 loss: 0.208
Actual params: [1.0387, 0.0503]
-Original Grad: 0.018, -lr * Pred Grad: 0.004, New P: 1.043
-Original Grad: 0.032, -lr * Pred Grad: 0.022, New P: 0.073
iter 20 loss: 0.212
Actual params: [1.0431, 0.0726]
-Original Grad: 0.013, -lr * Pred Grad: 0.002, New P: 1.045
-Original Grad: 0.020, -lr * Pred Grad: 0.001, New P: 0.073
Target params: [1.1812, 0.2779]
iter 0 loss: 0.288
Actual params: [0.5941, 0.5941]
-Original Grad: 0.381, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.223, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.258
Actual params: [0.6617, 0.5315]
-Original Grad: 0.153, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.112, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.244
Actual params: [0.7467, 0.451 ]
-Original Grad: 0.007, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.000, -lr * Pred Grad: -0.081, New P: 0.370
iter 3 loss: 0.240
Actual params: [0.8343, 0.3699]
-Original Grad: 0.005, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: 0.016, -lr * Pred Grad: -0.073, New P: 0.297
iter 4 loss: 0.240
Actual params: [0.922 , 0.2969]
-Original Grad: 0.021, -lr * Pred Grad: 0.086, New P: 1.008
-Original Grad: 0.038, -lr * Pred Grad: -0.069, New P: 0.228
iter 5 loss: 0.239
Actual params: [1.0079, 0.2283]
-Original Grad: 0.018, -lr * Pred Grad: 0.052, New P: 1.060
-Original Grad: 0.012, -lr * Pred Grad: -0.058, New P: 0.170
iter 6 loss: 0.239
Actual params: [1.0597, 0.1702]
-Original Grad: 0.021, -lr * Pred Grad: 0.041, New P: 1.100
-Original Grad: 0.019, -lr * Pred Grad: -0.038, New P: 0.132
iter 7 loss: 0.238
Actual params: [1.1005, 0.1324]
-Original Grad: 0.022, -lr * Pred Grad: 0.057, New P: 1.158
-Original Grad: 0.035, -lr * Pred Grad: -0.016, New P: 0.117
iter 8 loss: 0.239
Actual params: [1.1576, 0.1166]
-Original Grad: 0.014, -lr * Pred Grad: 0.031, New P: 1.189
-Original Grad: 0.037, -lr * Pred Grad: 0.018, New P: 0.135
iter 9 loss: 0.240
Actual params: [1.189 , 0.1346]
-Original Grad: 0.007, -lr * Pred Grad: 0.029, New P: 1.218
-Original Grad: 0.004, -lr * Pred Grad: 0.010, New P: 0.145
iter 10 loss: 0.241
Actual params: [1.2178, 0.1445]
-Original Grad: 0.006, -lr * Pred Grad: -0.016, New P: 1.202
-Original Grad: 0.013, -lr * Pred Grad: -0.013, New P: 0.132
iter 11 loss: 0.240
Actual params: [1.2019, 0.1315]
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 1.201
-Original Grad: 0.021, -lr * Pred Grad: 0.001, New P: 0.132
iter 12 loss: 0.240
Actual params: [1.201 , 0.1322]
-Original Grad: 0.011, -lr * Pred Grad: -0.012, New P: 1.189
-Original Grad: 0.021, -lr * Pred Grad: 0.002, New P: 0.134
iter 13 loss: 0.240
Actual params: [1.189 , 0.1341]
-Original Grad: 0.013, -lr * Pred Grad: -0.002, New P: 1.187
-Original Grad: 0.031, -lr * Pred Grad: 0.011, New P: 0.145
iter 14 loss: 0.240
Actual params: [1.1868, 0.1454]
-Original Grad: 0.011, -lr * Pred Grad: -0.006, New P: 1.181
-Original Grad: 0.014, -lr * Pred Grad: 0.003, New P: 0.148
iter 15 loss: 0.240
Actual params: [1.1813, 0.1482]
-Original Grad: 0.010, -lr * Pred Grad: -0.005, New P: 1.176
-Original Grad: 0.015, -lr * Pred Grad: -0.003, New P: 0.145
iter 16 loss: 0.240
Actual params: [1.1761, 0.145 ]
-Original Grad: 0.008, -lr * Pred Grad: -0.006, New P: 1.170
-Original Grad: 0.007, -lr * Pred Grad: -0.009, New P: 0.136
iter 17 loss: 0.240
Actual params: [1.1698, 0.1359]
-Original Grad: 0.011, -lr * Pred Grad: -0.004, New P: 1.165
-Original Grad: 0.014, -lr * Pred Grad: -0.007, New P: 0.129
iter 18 loss: 0.239
Actual params: [1.1654, 0.1292]
-Original Grad: 0.014, -lr * Pred Grad: -0.001, New P: 1.164
-Original Grad: 0.020, -lr * Pred Grad: 0.000, New P: 0.130
iter 19 loss: 0.239
Actual params: [1.1642, 0.1296]
-Original Grad: 0.012, -lr * Pred Grad: -0.001, New P: 1.164
-Original Grad: 0.024, -lr * Pred Grad: 0.007, New P: 0.136
iter 20 loss: 0.239
Actual params: [1.1636, 0.1364]
-Original Grad: 0.015, -lr * Pred Grad: 0.001, New P: 1.165
-Original Grad: 0.044, -lr * Pred Grad: 0.018, New P: 0.154
Target params: [1.1812, 0.2779]
iter 0 loss: 0.718
Actual params: [0.5941, 0.5941]
-Original Grad: 0.276, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.340, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.590
Actual params: [0.6617, 0.5309]
-Original Grad: 0.253, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.311, -lr * Pred Grad: -0.080, New P: 0.450
iter 2 loss: 0.459
Actual params: [0.7466, 0.4504]
-Original Grad: 0.081, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.047, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.394
Actual params: [0.8342, 0.3669]
-Original Grad: 0.008, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: 0.026, -lr * Pred Grad: -0.082, New P: 0.284
iter 4 loss: 0.368
Actual params: [0.9222, 0.2844]
-Original Grad: 0.030, -lr * Pred Grad: 0.087, New P: 1.009
-Original Grad: 0.075, -lr * Pred Grad: -0.074, New P: 0.211
iter 5 loss: 0.363
Actual params: [1.0094, 0.2106]
-Original Grad: 0.028, -lr * Pred Grad: 0.076, New P: 1.086
-Original Grad: 0.031, -lr * Pred Grad: -0.069, New P: 0.141
iter 6 loss: 0.357
Actual params: [1.0858, 0.1414]
-Original Grad: 0.036, -lr * Pred Grad: 0.020, New P: 1.106
-Original Grad: 0.063, -lr * Pred Grad: -0.058, New P: 0.083
iter 7 loss: 0.352
Actual params: [1.106 , 0.0833]
-Original Grad: 0.028, -lr * Pred Grad: 0.061, New P: 1.167
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: 0.045
iter 8 loss: 0.350
Actual params: [1.1673, 0.0452]
-Original Grad: 0.023, -lr * Pred Grad: 0.038, New P: 1.205
-Original Grad: 0.015, -lr * Pred Grad: -0.025, New P: 0.020
iter 9 loss: 0.348
Actual params: [1.2052, 0.0203]
-Original Grad: 0.024, -lr * Pred Grad: 0.050, New P: 1.255
-Original Grad: 0.029, -lr * Pred Grad: -0.004, New P: 0.016
iter 10 loss: 0.351
Actual params: [1.2547, 0.016 ]
-Original Grad: 0.023, -lr * Pred Grad: -0.002, New P: 1.253
-Original Grad: 0.030, -lr * Pred Grad: 0.024, New P: 0.040
iter 11 loss: 0.354
Actual params: [1.2531, 0.0397]
-Original Grad: 0.018, -lr * Pred Grad: 0.025, New P: 1.278
-Original Grad: 0.021, -lr * Pred Grad: 0.002, New P: 0.042
iter 12 loss: 0.357
Actual params: [1.2781, 0.0418]
-Original Grad: 0.017, -lr * Pred Grad: -0.016, New P: 1.262
-Original Grad: 0.019, -lr * Pred Grad: 0.008, New P: 0.050
iter 13 loss: 0.357
Actual params: [1.2624, 0.0499]
-Original Grad: 0.015, -lr * Pred Grad: 0.010, New P: 1.272
-Original Grad: 0.017, -lr * Pred Grad: -0.002, New P: 0.048
iter 14 loss: 0.358
Actual params: [1.2721, 0.0477]
-Original Grad: 0.016, -lr * Pred Grad: -0.008, New P: 1.264
-Original Grad: 0.022, -lr * Pred Grad: 0.004, New P: 0.052
iter 15 loss: 0.358
Actual params: [1.2638, 0.052 ]
-Original Grad: 0.017, -lr * Pred Grad: 0.005, New P: 1.268
-Original Grad: 0.020, -lr * Pred Grad: 0.003, New P: 0.055
iter 16 loss: 0.359
Actual params: [1.2684, 0.0552]
-Original Grad: 0.018, -lr * Pred Grad: -0.001, New P: 1.268
-Original Grad: 0.021, -lr * Pred Grad: 0.004, New P: 0.059
iter 17 loss: 0.360
Actual params: [1.2678, 0.0592]
-Original Grad: 0.020, -lr * Pred Grad: 0.005, New P: 1.273
-Original Grad: 0.020, -lr * Pred Grad: 0.004, New P: 0.063
iter 18 loss: 0.361
Actual params: [1.2726, 0.0627]
-Original Grad: 0.016, -lr * Pred Grad: 0.002, New P: 1.275
-Original Grad: 0.019, -lr * Pred Grad: 0.003, New P: 0.066
iter 19 loss: 0.362
Actual params: [1.2745, 0.0657]
-Original Grad: 0.017, -lr * Pred Grad: 0.002, New P: 1.277
-Original Grad: 0.019, -lr * Pred Grad: 0.003, New P: 0.069
iter 20 loss: 0.363
Actual params: [1.2769, 0.0686]
-Original Grad: 0.023, -lr * Pred Grad: 0.006, New P: 1.283
-Original Grad: 0.025, -lr * Pred Grad: 0.006, New P: 0.075
Target params: [1.1812, 0.2779]
iter 0 loss: 0.727
Actual params: [0.5941, 0.5941]
-Original Grad: 0.648, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.038, -lr * Pred Grad: -0.002, New P: 0.592
iter 1 loss: 0.717
Actual params: [0.6617, 0.5923]
-Original Grad: 0.531, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.106, -lr * Pred Grad: -0.064, New P: 0.529
iter 2 loss: 0.687
Actual params: [0.7469, 0.5286]
-Original Grad: 0.357, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.007, -lr * Pred Grad: -0.072, New P: 0.457
iter 3 loss: 0.654
Actual params: [0.8348, 0.4565]
-Original Grad: 0.445, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.086, -lr * Pred Grad: -0.062, New P: 0.394
iter 4 loss: 0.600
Actual params: [0.923 , 0.3941]
-Original Grad: 0.297, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.173, -lr * Pred Grad: -0.024, New P: 0.370
iter 5 loss: 0.557
Actual params: [1.0114, 0.3701]
-Original Grad: 0.089, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: 0.249, -lr * Pred Grad: 0.038, New P: 0.408
iter 6 loss: 0.497
Actual params: [1.0997, 0.4084]
-Original Grad: -0.041, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.145, -lr * Pred Grad: 0.076, New P: 0.484
iter 7 loss: 0.433
Actual params: [1.188 , 0.4841]
-Original Grad: -0.124, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.005, -lr * Pred Grad: 0.083, New P: 0.567
iter 8 loss: 0.498
Actual params: [1.2763, 0.5672]
-Original Grad: 0.009, -lr * Pred Grad: 0.088, New P: 1.365
-Original Grad: -0.300, -lr * Pred Grad: -0.035, New P: 0.532
iter 9 loss: 0.465
Actual params: [1.3645, 0.5321]
-Original Grad: 0.089, -lr * Pred Grad: 0.088, New P: 1.452
-Original Grad: -0.055, -lr * Pred Grad: -0.058, New P: 0.474
iter 10 loss: 0.449
Actual params: [1.4524, 0.4742]
-Original Grad: 0.038, -lr * Pred Grad: 0.086, New P: 1.539
-Original Grad: 0.026, -lr * Pred Grad: -0.045, New P: 0.429
iter 11 loss: 0.441
Actual params: [1.5387, 0.4292]
-Original Grad: 0.021, -lr * Pred Grad: 0.079, New P: 1.618
-Original Grad: 0.074, -lr * Pred Grad: 0.003, New P: 0.433
iter 12 loss: 0.464
Actual params: [1.6177, 0.4325]
-Original Grad: 0.023, -lr * Pred Grad: 0.067, New P: 1.684
-Original Grad: 0.042, -lr * Pred Grad: 0.050, New P: 0.483
iter 13 loss: 0.498
Actual params: [1.6843, 0.4828]
-Original Grad: 0.020, -lr * Pred Grad: 0.054, New P: 1.738
-Original Grad: -0.012, -lr * Pred Grad: -0.015, New P: 0.468
iter 14 loss: 0.496
Actual params: [1.7379, 0.4681]
-Original Grad: 0.017, -lr * Pred Grad: 0.029, New P: 1.767
-Original Grad: -0.058, -lr * Pred Grad: -0.037, New P: 0.431
iter 15 loss: 0.481
Actual params: [1.7667, 0.4314]
-Original Grad: 0.038, -lr * Pred Grad: 0.074, New P: 1.840
-Original Grad: 0.056, -lr * Pred Grad: 0.003, New P: 0.434
iter 16 loss: 0.492
Actual params: [1.8403, 0.4341]
-Original Grad: 0.017, -lr * Pred Grad: 0.072, New P: 1.913
-Original Grad: 0.018, -lr * Pred Grad: 0.024, New P: 0.458
iter 17 loss: 0.515
Actual params: [1.9127, 0.458 ]
-Original Grad: 0.009, -lr * Pred Grad: 0.078, New P: 1.991
-Original Grad: -0.036, -lr * Pred Grad: -0.026, New P: 0.432
iter 18 loss: 0.510
Actual params: [1.9909, 0.4321]
-Original Grad: 0.025, -lr * Pred Grad: 0.068, New P: 2.059
-Original Grad: 0.046, -lr * Pred Grad: 0.012, New P: 0.444
iter 19 loss: 0.524
Actual params: [2.0586, 0.444 ]
-Original Grad: 0.002, -lr * Pred Grad: 0.034, New P: 2.093
-Original Grad: -0.016, -lr * Pred Grad: -0.014, New P: 0.430
iter 20 loss: 0.521
Actual params: [2.0928, 0.4303]
-Original Grad: 0.020, -lr * Pred Grad: 0.045, New P: 2.138
-Original Grad: -0.002, -lr * Pred Grad: -0.011, New P: 0.419
Target params: [1.1812, 0.2779]
iter 0 loss: 0.636
Actual params: [0.5941, 0.5941]
-Original Grad: 0.282, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.059, -lr * Pred Grad: -0.023, New P: 0.571
iter 1 loss: 0.589
Actual params: [0.6617, 0.5711]
-Original Grad: 0.224, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.082, -lr * Pred Grad: -0.069, New P: 0.502
iter 2 loss: 0.562
Actual params: [0.7465, 0.5023]
-Original Grad: 0.429, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.037, -lr * Pred Grad: -0.072, New P: 0.430
iter 3 loss: 0.487
Actual params: [0.8343, 0.43  ]
-Original Grad: 0.430, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.088, -lr * Pred Grad: -0.065, New P: 0.365
iter 4 loss: 0.423
Actual params: [0.9226, 0.365 ]
-Original Grad: 0.283, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.101, -lr * Pred Grad: -0.035, New P: 0.330
iter 5 loss: 0.388
Actual params: [1.0109, 0.3297]
-Original Grad: 0.134, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 0.168, -lr * Pred Grad: 0.019, New P: 0.349
iter 6 loss: 0.348
Actual params: [1.0992, 0.349 ]
-Original Grad: 0.071, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.123, -lr * Pred Grad: 0.067, New P: 0.416
iter 7 loss: 0.341
Actual params: [1.1876, 0.4155]
-Original Grad: 0.025, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: -0.057, -lr * Pred Grad: 0.045, New P: 0.460
iter 8 loss: 0.344
Actual params: [1.2759, 0.4601]
-Original Grad: 0.008, -lr * Pred Grad: 0.088, New P: 1.364
-Original Grad: -0.095, -lr * Pred Grad: -0.040, New P: 0.420
iter 9 loss: 0.362
Actual params: [1.3642, 0.4197]
-Original Grad: -0.110, -lr * Pred Grad: 0.088, New P: 1.452
-Original Grad: 0.022, -lr * Pred Grad: -0.034, New P: 0.386
iter 10 loss: 0.395
Actual params: [1.4522, 0.3856]
-Original Grad: -0.150, -lr * Pred Grad: 0.082, New P: 1.534
-Original Grad: 0.102, -lr * Pred Grad: 0.030, New P: 0.416
iter 11 loss: 0.418
Actual params: [1.5339, 0.4159]
-Original Grad: -0.161, -lr * Pred Grad: -0.002, New P: 1.531
-Original Grad: 0.099, -lr * Pred Grad: 0.071, New P: 0.487
iter 12 loss: 0.412
Actual params: [1.5314, 0.4867]
-Original Grad: -0.124, -lr * Pred Grad: -0.022, New P: 1.509
-Original Grad: -0.012, -lr * Pred Grad: 0.012, New P: 0.498
iter 13 loss: 0.411
Actual params: [1.5089, 0.4982]
-Original Grad: -0.131, -lr * Pred Grad: -0.048, New P: 1.461
-Original Grad: -0.053, -lr * Pred Grad: -0.031, New P: 0.467
iter 14 loss: 0.397
Actual params: [1.4606, 0.4667]
-Original Grad: -0.146, -lr * Pred Grad: -0.053, New P: 1.407
-Original Grad: -0.038, -lr * Pred Grad: -0.045, New P: 0.422
iter 15 loss: 0.384
Actual params: [1.4073, 0.422 ]
-Original Grad: -0.228, -lr * Pred Grad: -0.058, New P: 1.349
-Original Grad: 0.053, -lr * Pred Grad: -0.003, New P: 0.419
iter 16 loss: 0.362
Actual params: [1.3492, 0.419 ]
-Original Grad: -0.186, -lr * Pred Grad: -0.062, New P: 1.287
-Original Grad: 0.059, -lr * Pred Grad: 0.044, New P: 0.463
iter 17 loss: 0.348
Actual params: [1.2872, 0.4635]
-Original Grad: 0.008, -lr * Pred Grad: -0.060, New P: 1.227
-Original Grad: -0.105, -lr * Pred Grad: -0.035, New P: 0.429
iter 18 loss: 0.343
Actual params: [1.2271, 0.4289]
-Original Grad: 0.005, -lr * Pred Grad: -0.051, New P: 1.176
-Original Grad: -0.076, -lr * Pred Grad: -0.053, New P: 0.376
iter 19 loss: 0.341
Actual params: [1.1764, 0.3763]
-Original Grad: 0.014, -lr * Pred Grad: -0.034, New P: 1.142
-Original Grad: 0.035, -lr * Pred Grad: -0.031, New P: 0.345
iter 20 loss: 0.343
Actual params: [1.1425, 0.3449]
-Original Grad: 0.018, -lr * Pred Grad: -0.019, New P: 1.124
-Original Grad: 0.107, -lr * Pred Grad: 0.024, New P: 0.369
Target params: [1.1812, 0.2779]
iter 0 loss: 0.473
Actual params: [0.5941, 0.5941]
-Original Grad: 0.255, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.309, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.428
Actual params: [0.6616, 0.5309]
-Original Grad: 0.224, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.300, -lr * Pred Grad: -0.080, New P: 0.450
iter 2 loss: 0.370
Actual params: [0.7465, 0.4504]
-Original Grad: 0.294, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.317, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.323
Actual params: [0.8342, 0.3668]
-Original Grad: 0.051, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: -0.060, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.301
Actual params: [0.9224, 0.2827]
-Original Grad: 0.007, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.070, -lr * Pred Grad: -0.082, New P: 0.201
iter 5 loss: 0.295
Actual params: [1.0105, 0.2007]
-Original Grad: 0.080, -lr * Pred Grad: 0.088, New P: 1.098
-Original Grad: 0.090, -lr * Pred Grad: -0.073, New P: 0.128
iter 6 loss: 0.297
Actual params: [1.0984, 0.1277]
-Original Grad: 0.047, -lr * Pred Grad: 0.086, New P: 1.185
-Original Grad: 0.075, -lr * Pred Grad: -0.068, New P: 0.059
iter 7 loss: 0.293
Actual params: [1.1847, 0.0592]
-Original Grad: 0.026, -lr * Pred Grad: 0.046, New P: 1.230
-Original Grad: 0.028, -lr * Pred Grad: -0.054, New P: 0.005
iter 8 loss: 0.289
Actual params: [1.2305, 0.0054]
-Original Grad: 0.026, -lr * Pred Grad: 0.050, New P: 1.281
-Original Grad: 0.023, -lr * Pred Grad: -0.035, New P: -0.030
iter 9 loss: 0.288
Actual params: [ 1.2809, -0.0295]
-Original Grad: 0.029, -lr * Pred Grad: 0.054, New P: 1.335
-Original Grad: 0.067, -lr * Pred Grad: -0.022, New P: -0.052
iter 10 loss: 0.287
Actual params: [ 1.3346, -0.0516]
-Original Grad: 0.031, -lr * Pred Grad: 0.056, New P: 1.390
-Original Grad: 0.060, -lr * Pred Grad: -0.001, New P: -0.052
iter 11 loss: 0.288
Actual params: [ 1.3902, -0.0524]
-Original Grad: 0.026, -lr * Pred Grad: 0.032, New P: 1.422
-Original Grad: 0.031, -lr * Pred Grad: 0.038, New P: -0.014
iter 12 loss: 0.290
Actual params: [ 1.4223, -0.0143]
-Original Grad: 0.030, -lr * Pred Grad: 0.029, New P: 1.452
-Original Grad: 0.052, -lr * Pred Grad: 0.035, New P: 0.020
iter 13 loss: 0.299
Actual params: [1.4516, 0.0203]
-Original Grad: 0.025, -lr * Pred Grad: 0.003, New P: 1.454
-Original Grad: 0.048, -lr * Pred Grad: 0.037, New P: 0.057
iter 14 loss: 0.303
Actual params: [1.4542, 0.0574]
-Original Grad: 0.020, -lr * Pred Grad: 0.013, New P: 1.467
-Original Grad: 0.029, -lr * Pred Grad: 0.020, New P: 0.077
iter 15 loss: 0.307
Actual params: [1.4667, 0.0771]
-Original Grad: 0.020, -lr * Pred Grad: -0.004, New P: 1.463
-Original Grad: 0.019, -lr * Pred Grad: 0.011, New P: 0.088
iter 16 loss: 0.309
Actual params: [1.4628, 0.0878]
-Original Grad: 0.018, -lr * Pred Grad: 0.004, New P: 1.467
-Original Grad: 0.050, -lr * Pred Grad: 0.027, New P: 0.114
iter 17 loss: 0.313
Actual params: [1.4665, 0.1143]
-Original Grad: 0.020, -lr * Pred Grad: 0.000, New P: 1.467
-Original Grad: 0.037, -lr * Pred Grad: 0.023, New P: 0.138
iter 18 loss: 0.317
Actual params: [1.4668, 0.1378]
-Original Grad: 0.014, -lr * Pred Grad: -0.000, New P: 1.467
-Original Grad: 0.033, -lr * Pred Grad: 0.021, New P: 0.159
iter 19 loss: 0.314
Actual params: [1.4667, 0.1592]
-Original Grad: 0.012, -lr * Pred Grad: -0.003, New P: 1.464
-Original Grad: 0.017, -lr * Pred Grad: 0.005, New P: 0.164
iter 20 loss: 0.314
Actual params: [1.4638, 0.1638]
-Original Grad: 0.010, -lr * Pred Grad: -0.004, New P: 1.460
-Original Grad: 0.021, -lr * Pred Grad: 0.005, New P: 0.169
Target params: [1.1812, 0.2779]
iter 0 loss: 0.459
Actual params: [0.5941, 0.5941]
-Original Grad: 0.303, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.357, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.349
Actual params: [0.6617, 0.5309]
-Original Grad: 0.315, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.344, -lr * Pred Grad: -0.080, New P: 0.450
iter 2 loss: 0.271
Actual params: [0.7467, 0.4505]
-Original Grad: 0.086, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.098, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.209
Actual params: [0.8344, 0.3669]
-Original Grad: 0.010, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.010, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.178
Actual params: [0.9226, 0.2831]
-Original Grad: -0.015, -lr * Pred Grad: 0.088, New P: 1.010
-Original Grad: 0.008, -lr * Pred Grad: -0.079, New P: 0.204
iter 5 loss: 0.173
Actual params: [1.0105, 0.2037]
-Original Grad: 0.050, -lr * Pred Grad: 0.087, New P: 1.097
-Original Grad: 0.018, -lr * Pred Grad: -0.071, New P: 0.132
iter 6 loss: 0.167
Actual params: [1.0972, 0.1323]
-Original Grad: 0.024, -lr * Pred Grad: 0.074, New P: 1.171
-Original Grad: 0.011, -lr * Pred Grad: -0.068, New P: 0.064
iter 7 loss: 0.168
Actual params: [1.171 , 0.0639]
-Original Grad: 0.019, -lr * Pred Grad: 0.019, New P: 1.190
-Original Grad: 0.037, -lr * Pred Grad: -0.057, New P: 0.007
iter 8 loss: 0.168
Actual params: [1.19  , 0.0068]
-Original Grad: 0.020, -lr * Pred Grad: 0.060, New P: 1.250
-Original Grad: 0.015, -lr * Pred Grad: -0.039, New P: -0.032
iter 9 loss: 0.177
Actual params: [ 1.2503, -0.0321]
-Original Grad: -0.004, -lr * Pred Grad: 0.020, New P: 1.270
-Original Grad: 0.036, -lr * Pred Grad: -0.028, New P: -0.060
iter 10 loss: 0.180
Actual params: [ 1.2705, -0.0601]
-Original Grad: -0.001, -lr * Pred Grad: 0.035, New P: 1.305
-Original Grad: 0.045, -lr * Pred Grad: -0.021, New P: -0.081
iter 11 loss: 0.184
Actual params: [ 1.3052, -0.0807]
-Original Grad: 0.002, -lr * Pred Grad: -0.021, New P: 1.284
-Original Grad: 0.050, -lr * Pred Grad: -0.003, New P: -0.083
iter 12 loss: 0.181
Actual params: [ 1.2843, -0.0834]
-Original Grad: -0.012, -lr * Pred Grad: -0.011, New P: 1.274
-Original Grad: 0.024, -lr * Pred Grad: 0.027, New P: -0.056
iter 13 loss: 0.180
Actual params: [ 1.2737, -0.0559]
-Original Grad: 0.006, -lr * Pred Grad: -0.018, New P: 1.256
-Original Grad: 0.030, -lr * Pred Grad: 0.010, New P: -0.046
iter 14 loss: 0.178
Actual params: [ 1.2558, -0.0457]
-Original Grad: 0.002, -lr * Pred Grad: -0.010, New P: 1.246
-Original Grad: 0.034, -lr * Pred Grad: 0.017, New P: -0.029
iter 15 loss: 0.177
Actual params: [ 1.2456, -0.0287]
-Original Grad: -0.014, -lr * Pred Grad: -0.021, New P: 1.225
-Original Grad: 0.022, -lr * Pred Grad: 0.006, New P: -0.023
iter 16 loss: 0.173
Actual params: [ 1.225 , -0.0226]
-Original Grad: 0.005, -lr * Pred Grad: -0.014, New P: 1.211
-Original Grad: 0.032, -lr * Pred Grad: 0.013, New P: -0.009
iter 17 loss: 0.170
Actual params: [ 1.2105, -0.0091]
-Original Grad: 0.010, -lr * Pred Grad: -0.007, New P: 1.204
-Original Grad: 0.027, -lr * Pred Grad: 0.009, New P: 0.000
iter 18 loss: 0.170
Actual params: [1.2039e+00, 2.3478e-04]
-Original Grad: 0.023, -lr * Pred Grad: 0.004, New P: 1.207
-Original Grad: 0.022, -lr * Pred Grad: 0.007, New P: 0.007
iter 19 loss: 0.170
Actual params: [1.2074, 0.0074]
-Original Grad: 0.008, -lr * Pred Grad: -0.000, New P: 1.207
-Original Grad: 0.020, -lr * Pred Grad: 0.003, New P: 0.010
iter 20 loss: 0.170
Actual params: [1.2071, 0.0103]
-Original Grad: 0.014, -lr * Pred Grad: -0.002, New P: 1.205
-Original Grad: 0.015, -lr * Pred Grad: -0.000, New P: 0.010
Target params: [1.1812, 0.2779]
iter 0 loss: 0.606
Actual params: [0.5941, 0.5941]
-Original Grad: 0.299, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.456, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.477
Actual params: [0.6617, 0.5311]
-Original Grad: 0.259, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.344, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.370
Actual params: [0.7466, 0.4506]
-Original Grad: 0.147, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.111, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.351
Actual params: [0.8344, 0.367 ]
-Original Grad: 0.074, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.083, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.334
Actual params: [0.9225, 0.2829]
-Original Grad: 0.025, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.058, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.325
Actual params: [1.0107, 0.1991]
-Original Grad: 0.060, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: -0.041, -lr * Pred Grad: -0.081, New P: 0.118
iter 6 loss: 0.325
Actual params: [1.0986, 0.1184]
-Original Grad: 0.083, -lr * Pred Grad: 0.087, New P: 1.186
-Original Grad: -0.018, -lr * Pred Grad: -0.073, New P: 0.046
iter 7 loss: 0.331
Actual params: [1.1856, 0.0456]
-Original Grad: 0.019, -lr * Pred Grad: 0.075, New P: 1.261
-Original Grad: 0.011, -lr * Pred Grad: -0.069, New P: -0.024
iter 8 loss: 0.341
Actual params: [ 1.2607, -0.0237]
-Original Grad: -0.040, -lr * Pred Grad: -0.007, New P: 1.253
-Original Grad: 0.008, -lr * Pred Grad: -0.064, New P: -0.088
iter 9 loss: 0.340
Actual params: [ 1.2533, -0.0876]
-Original Grad: -0.036, -lr * Pred Grad: 0.016, New P: 1.269
-Original Grad: 0.006, -lr * Pred Grad: -0.048, New P: -0.136
iter 10 loss: 0.344
Actual params: [ 1.2689, -0.1361]
-Original Grad: -0.052, -lr * Pred Grad: -0.022, New P: 1.247
-Original Grad: 0.041, -lr * Pred Grad: -0.034, New P: -0.170
iter 11 loss: 0.342
Actual params: [ 1.2468, -0.1701]
-Original Grad: -0.049, -lr * Pred Grad: -0.032, New P: 1.215
-Original Grad: 0.033, -lr * Pred Grad: -0.027, New P: -0.197
iter 12 loss: 0.337
Actual params: [ 1.2145, -0.1967]
-Original Grad: -0.040, -lr * Pred Grad: -0.041, New P: 1.174
-Original Grad: 0.037, -lr * Pred Grad: -0.024, New P: -0.220
iter 13 loss: 0.329
Actual params: [ 1.174 , -0.2203]
-Original Grad: -0.010, -lr * Pred Grad: -0.030, New P: 1.144
-Original Grad: 0.041, -lr * Pred Grad: -0.019, New P: -0.239
iter 14 loss: 0.324
Actual params: [ 1.1436, -0.2388]
-Original Grad: -0.003, -lr * Pred Grad: -0.016, New P: 1.128
-Original Grad: 0.019, -lr * Pred Grad: -0.005, New P: -0.244
iter 15 loss: 0.321
Actual params: [ 1.128 , -0.2442]
-Original Grad: -0.005, -lr * Pred Grad: -0.012, New P: 1.116
-Original Grad: 0.019, -lr * Pred Grad: 0.012, New P: -0.232
iter 16 loss: 0.323
Actual params: [ 1.116 , -0.2318]
-Original Grad: 0.008, -lr * Pred Grad: -0.007, New P: 1.109
-Original Grad: 0.011, -lr * Pred Grad: -0.012, New P: -0.244
iter 17 loss: 0.323
Actual params: [ 1.1088, -0.2442]
-Original Grad: -0.004, -lr * Pred Grad: -0.010, New P: 1.098
-Original Grad: 0.012, -lr * Pred Grad: -0.009, New P: -0.254
iter 18 loss: 0.322
Actual params: [ 1.0985, -0.2536]
-Original Grad: -0.001, -lr * Pred Grad: -0.013, New P: 1.086
-Original Grad: 0.015, -lr * Pred Grad: -0.011, New P: -0.265
iter 19 loss: 0.321
Actual params: [ 1.0858, -0.2647]
-Original Grad: 0.020, -lr * Pred Grad: -0.004, New P: 1.082
-Original Grad: -0.003, -lr * Pred Grad: -0.019, New P: -0.284
iter 20 loss: 0.321
Actual params: [ 1.0817, -0.2839]
-Original Grad: 0.020, -lr * Pred Grad: 0.004, New P: 1.085
-Original Grad: 0.007, -lr * Pred Grad: -0.021, New P: -0.305
Target params: [1.1812, 0.2779]
iter 0 loss: 0.419
Actual params: [0.5941, 0.5941]
-Original Grad: 0.073, -lr * Pred Grad: 0.063, New P: 0.657
-Original Grad: -0.170, -lr * Pred Grad: -0.061, New P: 0.533
iter 1 loss: 0.356
Actual params: [0.6567, 0.5334]
-Original Grad: 0.053, -lr * Pred Grad: 0.076, New P: 0.732
-Original Grad: -0.044, -lr * Pred Grad: -0.078, New P: 0.455
iter 2 loss: 0.324
Actual params: [0.7325, 0.4552]
-Original Grad: 0.026, -lr * Pred Grad: 0.007, New P: 0.740
-Original Grad: -0.049, -lr * Pred Grad: -0.076, New P: 0.379
iter 3 loss: 0.315
Actual params: [0.7398, 0.3792]
-Original Grad: 0.024, -lr * Pred Grad: 0.022, New P: 0.762
-Original Grad: -0.007, -lr * Pred Grad: -0.070, New P: 0.309
iter 4 loss: 0.304
Actual params: [0.7621, 0.3089]
-Original Grad: 0.013, -lr * Pred Grad: -0.004, New P: 0.758
-Original Grad: 0.002, -lr * Pred Grad: -0.067, New P: 0.242
iter 5 loss: 0.296
Actual params: [0.7579, 0.2422]
-Original Grad: 0.010, -lr * Pred Grad: 0.021, New P: 0.778
-Original Grad: 0.009, -lr * Pred Grad: -0.055, New P: 0.187
iter 6 loss: 0.288
Actual params: [0.7784, 0.1867]
-Original Grad: 0.006, -lr * Pred Grad: -0.014, New P: 0.764
-Original Grad: 0.004, -lr * Pred Grad: -0.038, New P: 0.148
iter 7 loss: 0.286
Actual params: [0.764 , 0.1484]
-Original Grad: 0.014, -lr * Pred Grad: 0.005, New P: 0.769
-Original Grad: 0.004, -lr * Pred Grad: -0.022, New P: 0.126
iter 8 loss: 0.284
Actual params: [0.7694, 0.1259]
-Original Grad: 0.010, -lr * Pred Grad: -0.009, New P: 0.760
-Original Grad: 0.012, -lr * Pred Grad: -0.000, New P: 0.126
iter 9 loss: 0.285
Actual params: [0.7604, 0.1258]
-Original Grad: 0.015, -lr * Pred Grad: 0.001, New P: 0.762
-Original Grad: 0.001, -lr * Pred Grad: -0.008, New P: 0.117
iter 10 loss: 0.284
Actual params: [0.7618, 0.1174]
-Original Grad: 0.022, -lr * Pred Grad: 0.003, New P: 0.765
-Original Grad: 0.002, -lr * Pred Grad: -0.019, New P: 0.099
iter 11 loss: 0.282
Actual params: [0.765 , 0.0986]
-Original Grad: 0.013, -lr * Pred Grad: 0.001, New P: 0.766
-Original Grad: 0.010, -lr * Pred Grad: -0.014, New P: 0.084
iter 12 loss: 0.281
Actual params: [0.7661, 0.0841]
-Original Grad: 0.013, -lr * Pred Grad: -0.002, New P: 0.764
-Original Grad: 0.010, -lr * Pred Grad: -0.012, New P: 0.072
iter 13 loss: 0.280
Actual params: [0.7643, 0.0725]
-Original Grad: 0.010, -lr * Pred Grad: -0.004, New P: 0.761
-Original Grad: 0.005, -lr * Pred Grad: -0.013, New P: 0.059
iter 14 loss: 0.279
Actual params: [0.7606, 0.0592]
-Original Grad: 0.015, -lr * Pred Grad: -0.001, New P: 0.759
-Original Grad: 0.007, -lr * Pred Grad: -0.015, New P: 0.044
iter 15 loss: 0.279
Actual params: [0.7592, 0.0439]
-Original Grad: 0.006, -lr * Pred Grad: -0.004, New P: 0.756
-Original Grad: 0.020, -lr * Pred Grad: -0.010, New P: 0.034
iter 16 loss: 0.278
Actual params: [0.7557, 0.0338]
-Original Grad: 0.012, -lr * Pred Grad: -0.003, New P: 0.753
-Original Grad: 0.009, -lr * Pred Grad: -0.009, New P: 0.025
iter 17 loss: 0.277
Actual params: [0.7532, 0.0246]
-Original Grad: 0.011, -lr * Pred Grad: -0.001, New P: 0.752
-Original Grad: 0.033, -lr * Pred Grad: -0.001, New P: 0.024
iter 18 loss: 0.277
Actual params: [0.7522, 0.0238]
-Original Grad: 0.013, -lr * Pred Grad: 0.001, New P: 0.753
-Original Grad: 0.013, -lr * Pred Grad: -0.003, New P: 0.020
iter 19 loss: 0.277
Actual params: [0.7528, 0.0203]
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 0.751
-Original Grad: 0.010, -lr * Pred Grad: -0.011, New P: 0.009
iter 20 loss: 0.279
Actual params: [0.7513, 0.0088]
-Original Grad: 0.007, -lr * Pred Grad: -0.004, New P: 0.748
-Original Grad: 0.018, -lr * Pred Grad: -0.012, New P: -0.003
Target params: [1.1812, 0.2779]
iter 0 loss: 0.388
Actual params: [0.5941, 0.5941]
-Original Grad: 0.044, -lr * Pred Grad: 0.057, New P: 0.651
-Original Grad: -0.037, -lr * Pred Grad: -0.001, New P: 0.593
iter 1 loss: 0.379
Actual params: [0.6511, 0.5929]
-Original Grad: 0.033, -lr * Pred Grad: 0.047, New P: 0.698
-Original Grad: -0.015, -lr * Pred Grad: -0.053, New P: 0.540
iter 2 loss: 0.350
Actual params: [0.6977, 0.5401]
-Original Grad: 0.025, -lr * Pred Grad: -0.020, New P: 0.678
-Original Grad: -0.076, -lr * Pred Grad: -0.067, New P: 0.473
iter 3 loss: 0.321
Actual params: [0.6775, 0.4732]
-Original Grad: 0.025, -lr * Pred Grad: -0.004, New P: 0.674
-Original Grad: -0.076, -lr * Pred Grad: -0.067, New P: 0.406
iter 4 loss: 0.280
Actual params: [0.674, 0.406]
-Original Grad: 0.040, -lr * Pred Grad: 0.032, New P: 0.706
-Original Grad: -0.068, -lr * Pred Grad: -0.066, New P: 0.340
iter 5 loss: 0.214
Actual params: [0.7057, 0.3401]
-Original Grad: 0.029, -lr * Pred Grad: 0.021, New P: 0.727
-Original Grad: -0.115, -lr * Pred Grad: -0.066, New P: 0.274
iter 6 loss: 0.157
Actual params: [0.727 , 0.2745]
-Original Grad: 0.020, -lr * Pred Grad: 0.016, New P: 0.743
-Original Grad: -0.041, -lr * Pred Grad: -0.064, New P: 0.210
iter 7 loss: 0.173
Actual params: [0.7434, 0.2102]
-Original Grad: -0.012, -lr * Pred Grad: -0.018, New P: 0.725
-Original Grad: 0.079, -lr * Pred Grad: -0.047, New P: 0.164
iter 8 loss: 0.206
Actual params: [0.7251, 0.1636]
-Original Grad: -0.043, -lr * Pred Grad: -0.034, New P: 0.691
-Original Grad: 0.113, -lr * Pred Grad: -0.028, New P: 0.135
iter 9 loss: 0.211
Actual params: [0.6911, 0.1352]
-Original Grad: -0.049, -lr * Pred Grad: -0.041, New P: 0.650
-Original Grad: 0.118, -lr * Pred Grad: -0.016, New P: 0.119
iter 10 loss: 0.202
Actual params: [0.6499, 0.1194]
-Original Grad: -0.033, -lr * Pred Grad: -0.038, New P: 0.612
-Original Grad: 0.097, -lr * Pred Grad: 0.006, New P: 0.125
iter 11 loss: 0.179
Actual params: [0.6116, 0.1254]
-Original Grad: -0.027, -lr * Pred Grad: -0.032, New P: 0.579
-Original Grad: 0.092, -lr * Pred Grad: 0.044, New P: 0.169
iter 12 loss: 0.155
Actual params: [0.5795, 0.1694]
-Original Grad: 0.020, -lr * Pred Grad: -0.014, New P: 0.566
-Original Grad: 0.011, -lr * Pred Grad: 0.062, New P: 0.232
iter 13 loss: 0.167
Actual params: [0.5655, 0.2316]
-Original Grad: 0.049, -lr * Pred Grad: 0.014, New P: 0.580
-Original Grad: -0.026, -lr * Pred Grad: -0.025, New P: 0.207
iter 14 loss: 0.154
Actual params: [0.5795, 0.207 ]
-Original Grad: 0.035, -lr * Pred Grad: 0.032, New P: 0.611
-Original Grad: -0.014, -lr * Pred Grad: -0.016, New P: 0.191
iter 15 loss: 0.154
Actual params: [0.6111, 0.1914]
-Original Grad: 0.013, -lr * Pred Grad: 0.012, New P: 0.623
-Original Grad: 0.036, -lr * Pred Grad: 0.006, New P: 0.197
iter 16 loss: 0.154
Actual params: [0.6231, 0.1975]
-Original Grad: 0.015, -lr * Pred Grad: 0.000, New P: 0.624
-Original Grad: 0.008, -lr * Pred Grad: -0.002, New P: 0.195
iter 17 loss: 0.155
Actual params: [0.6236, 0.195 ]
-Original Grad: 0.006, -lr * Pred Grad: -0.006, New P: 0.617
-Original Grad: 0.064, -lr * Pred Grad: 0.031, New P: 0.226
iter 18 loss: 0.154
Actual params: [0.6173, 0.2262]
-Original Grad: 0.027, -lr * Pred Grad: 0.002, New P: 0.620
-Original Grad: -0.012, -lr * Pred Grad: -0.010, New P: 0.216
iter 19 loss: 0.152
Actual params: [0.6197, 0.2158]
-Original Grad: 0.022, -lr * Pred Grad: 0.008, New P: 0.628
-Original Grad: -0.001, -lr * Pred Grad: -0.013, New P: 0.203
iter 20 loss: 0.153
Actual params: [0.6278, 0.2032]
-Original Grad: 0.014, -lr * Pred Grad: 0.004, New P: 0.631
-Original Grad: 0.028, -lr * Pred Grad: -0.000, New P: 0.203
Target params: [1.1812, 0.2779]
iter 0 loss: 1.328
Actual params: [0.5941, 0.5941]
-Original Grad: 0.583, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.004, -lr * Pred Grad: 0.032, New P: 0.626
iter 1 loss: 1.295
Actual params: [0.6617, 0.626 ]
-Original Grad: 0.545, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.011, -lr * Pred Grad: -0.036, New P: 0.590
iter 2 loss: 1.229
Actual params: [0.7469, 0.5899]
-Original Grad: 0.512, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: 0.016, -lr * Pred Grad: -0.056, New P: 0.534
iter 3 loss: 1.129
Actual params: [0.8348, 0.5337]
-Original Grad: 0.631, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.070, -lr * Pred Grad: -0.016, New P: 0.518
iter 4 loss: 0.992
Actual params: [0.923 , 0.5178]
-Original Grad: 0.782, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.149, -lr * Pred Grad: 0.049, New P: 0.567
iter 5 loss: 0.815
Actual params: [1.0114, 0.5672]
-Original Grad: 0.564, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: 0.147, -lr * Pred Grad: 0.079, New P: 0.646
iter 6 loss: 0.475
Actual params: [1.0997, 0.646 ]
-Original Grad: 0.066, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.044, -lr * Pred Grad: 0.081, New P: 0.727
iter 7 loss: 0.260
Actual params: [1.188 , 0.7267]
-Original Grad: -0.046, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: -0.085, -lr * Pred Grad: -0.022, New P: 0.705
iter 8 loss: 0.220
Actual params: [1.2764, 0.7046]
-Original Grad: -0.010, -lr * Pred Grad: 0.088, New P: 1.365
-Original Grad: -0.072, -lr * Pred Grad: -0.047, New P: 0.658
iter 9 loss: 0.189
Actual params: [1.3647, 0.6577]
-Original Grad: 0.008, -lr * Pred Grad: 0.088, New P: 1.453
-Original Grad: -0.054, -lr * Pred Grad: -0.050, New P: 0.608
iter 10 loss: 0.296
Actual params: [1.453 , 0.6078]
-Original Grad: -0.002, -lr * Pred Grad: 0.088, New P: 1.541
-Original Grad: 0.006, -lr * Pred Grad: -0.031, New P: 0.577
iter 11 loss: 0.407
Actual params: [1.5413, 0.5768]
-Original Grad: -0.034, -lr * Pred Grad: 0.088, New P: 1.630
-Original Grad: 0.046, -lr * Pred Grad: 0.011, New P: 0.588
iter 12 loss: 0.456
Actual params: [1.6296, 0.5878]
-Original Grad: -0.042, -lr * Pred Grad: 0.088, New P: 1.718
-Original Grad: 0.035, -lr * Pred Grad: 0.034, New P: 0.622
iter 13 loss: 0.469
Actual params: [1.7175, 0.6215]
-Original Grad: -0.048, -lr * Pred Grad: 0.086, New P: 1.803
-Original Grad: 0.022, -lr * Pred Grad: 0.012, New P: 0.634
iter 14 loss: 0.490
Actual params: [1.8034, 0.6336]
-Original Grad: -0.036, -lr * Pred Grad: 0.076, New P: 1.879
-Original Grad: -0.015, -lr * Pred Grad: -0.011, New P: 0.622
iter 15 loss: 0.522
Actual params: [1.8792, 0.6223]
-Original Grad: -0.050, -lr * Pred Grad: 0.060, New P: 1.939
-Original Grad: 0.027, -lr * Pred Grad: -0.001, New P: 0.621
iter 16 loss: 0.540
Actual params: [1.939 , 0.6212]
-Original Grad: -0.050, -lr * Pred Grad: 0.006, New P: 1.945
-Original Grad: 0.016, -lr * Pred Grad: 0.002, New P: 0.624
iter 17 loss: 0.539
Actual params: [1.9445, 0.6237]
-Original Grad: -0.054, -lr * Pred Grad: 0.029, New P: 1.973
-Original Grad: 0.039, -lr * Pred Grad: 0.016, New P: 0.640
iter 18 loss: 0.531
Actual params: [1.9732, 0.6402]
-Original Grad: -0.061, -lr * Pred Grad: -0.002, New P: 1.971
-Original Grad: -0.023, -lr * Pred Grad: -0.013, New P: 0.627
iter 19 loss: 0.542
Actual params: [1.9712, 0.6268]
-Original Grad: -0.055, -lr * Pred Grad: 0.029, New P: 2.000
-Original Grad: -0.022, -lr * Pred Grad: -0.028, New P: 0.599
iter 20 loss: 0.574
Actual params: [2.0005, 0.5986]
-Original Grad: -0.053, -lr * Pred Grad: -0.007, New P: 1.994
-Original Grad: 0.020, -lr * Pred Grad: -0.014, New P: 0.584
Target params: [1.1812, 0.2779]
iter 0 loss: 0.321
Actual params: [0.5941, 0.5941]
-Original Grad: 0.013, -lr * Pred Grad: 0.044, New P: 0.638
-Original Grad: -0.071, -lr * Pred Grad: -0.032, New P: 0.562
iter 1 loss: 0.327
Actual params: [0.6379, 0.562 ]
-Original Grad: 0.013, -lr * Pred Grad: -0.009, New P: 0.629
-Original Grad: -0.052, -lr * Pred Grad: -0.068, New P: 0.494
iter 2 loss: 0.329
Actual params: [0.6293, 0.4942]
-Original Grad: 0.004, -lr * Pred Grad: -0.044, New P: 0.586
-Original Grad: -0.011, -lr * Pred Grad: -0.069, New P: 0.425
iter 3 loss: 0.328
Actual params: [0.5857, 0.4251]
-Original Grad: -0.003, -lr * Pred Grad: -0.037, New P: 0.549
-Original Grad: 0.011, -lr * Pred Grad: -0.065, New P: 0.360
iter 4 loss: 0.328
Actual params: [0.5488, 0.36  ]
-Original Grad: -0.004, -lr * Pred Grad: -0.010, New P: 0.538
-Original Grad: 0.019, -lr * Pred Grad: -0.048, New P: 0.312
iter 5 loss: 0.328
Actual params: [0.5384, 0.312 ]
-Original Grad: -0.014, -lr * Pred Grad: -0.014, New P: 0.525
-Original Grad: 0.021, -lr * Pred Grad: -0.014, New P: 0.298
iter 6 loss: 0.328
Actual params: [0.5248, 0.2977]
-Original Grad: -0.008, -lr * Pred Grad: -0.017, New P: 0.508
-Original Grad: 0.015, -lr * Pred Grad: 0.018, New P: 0.315
iter 7 loss: 0.327
Actual params: [0.5075, 0.3155]
-Original Grad: -0.013, -lr * Pred Grad: -0.021, New P: 0.487
-Original Grad: 0.011, -lr * Pred Grad: -0.006, New P: 0.309
iter 8 loss: 0.326
Actual params: [0.4867, 0.3091]
-Original Grad: -0.020, -lr * Pred Grad: -0.025, New P: 0.462
-Original Grad: 0.015, -lr * Pred Grad: 0.001, New P: 0.310
iter 9 loss: 0.325
Actual params: [0.4617, 0.3096]
-Original Grad: -0.021, -lr * Pred Grad: -0.027, New P: 0.434
-Original Grad: 0.015, -lr * Pred Grad: -0.004, New P: 0.305
iter 10 loss: 0.323
Actual params: [0.4342, 0.3053]
-Original Grad: -0.017, -lr * Pred Grad: -0.027, New P: 0.407
-Original Grad: 0.007, -lr * Pred Grad: -0.008, New P: 0.298
iter 11 loss: 0.322
Actual params: [0.4074, 0.2977]
-Original Grad: -0.025, -lr * Pred Grad: -0.028, New P: 0.379
-Original Grad: 0.008, -lr * Pred Grad: -0.011, New P: 0.287
iter 12 loss: 0.321
Actual params: [0.3791, 0.2867]
-Original Grad: -0.013, -lr * Pred Grad: -0.027, New P: 0.352
-Original Grad: 0.002, -lr * Pred Grad: -0.015, New P: 0.271
iter 13 loss: 0.320
Actual params: [0.3523, 0.2715]
-Original Grad: -0.017, -lr * Pred Grad: -0.025, New P: 0.327
-Original Grad: 0.005, -lr * Pred Grad: -0.017, New P: 0.255
iter 14 loss: 0.319
Actual params: [0.327 , 0.2546]
-Original Grad: -0.014, -lr * Pred Grad: -0.024, New P: 0.303
-Original Grad: 0.005, -lr * Pred Grad: -0.017, New P: 0.238
iter 15 loss: 0.318
Actual params: [0.3026, 0.238 ]
-Original Grad: -0.010, -lr * Pred Grad: -0.023, New P: 0.280
-Original Grad: 0.004, -lr * Pred Grad: -0.017, New P: 0.221
iter 16 loss: 0.318
Actual params: [0.2798, 0.2211]
-Original Grad: -0.013, -lr * Pred Grad: -0.022, New P: 0.258
-Original Grad: 0.008, -lr * Pred Grad: -0.016, New P: 0.205
iter 17 loss: 0.317
Actual params: [0.2577, 0.205 ]
-Original Grad: -0.013, -lr * Pred Grad: -0.022, New P: 0.236
-Original Grad: 0.005, -lr * Pred Grad: -0.016, New P: 0.189
iter 18 loss: 0.317
Actual params: [0.2355, 0.1892]
-Original Grad: -0.009, -lr * Pred Grad: -0.021, New P: 0.214
-Original Grad: 0.009, -lr * Pred Grad: -0.014, New P: 0.175
iter 19 loss: 0.317
Actual params: [0.2144, 0.1752]
-Original Grad: -0.010, -lr * Pred Grad: -0.020, New P: 0.195
-Original Grad: 0.005, -lr * Pred Grad: -0.014, New P: 0.162
iter 20 loss: 0.317
Actual params: [0.1947, 0.1616]
-Original Grad: -0.011, -lr * Pred Grad: -0.019, New P: 0.176
-Original Grad: 0.008, -lr * Pred Grad: -0.012, New P: 0.149
Target params: [1.1812, 0.2779]
iter 0 loss: 0.383
Actual params: [0.5941, 0.5941]
-Original Grad: 0.331, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.292, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.339
Actual params: [0.6617, 0.5309]
-Original Grad: 0.028, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.055, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.325
Actual params: [0.7463, 0.4503]
-Original Grad: 0.000, -lr * Pred Grad: 0.087, New P: 0.833
-Original Grad: -0.022, -lr * Pred Grad: -0.082, New P: 0.369
iter 3 loss: 0.323
Actual params: [0.8328, 0.3687]
-Original Grad: -0.006, -lr * Pred Grad: 0.076, New P: 0.909
-Original Grad: -0.007, -lr * Pred Grad: -0.074, New P: 0.294
iter 4 loss: 0.327
Actual params: [0.909 , 0.2943]
-Original Grad: -0.002, -lr * Pred Grad: -0.007, New P: 0.902
-Original Grad: -0.000, -lr * Pred Grad: -0.070, New P: 0.225
iter 5 loss: 0.326
Actual params: [0.9016, 0.2247]
-Original Grad: -0.009, -lr * Pred Grad: 0.033, New P: 0.935
-Original Grad: 0.007, -lr * Pred Grad: -0.065, New P: 0.160
iter 6 loss: 0.329
Actual params: [0.9347, 0.1598]
-Original Grad: 0.002, -lr * Pred Grad: 0.003, New P: 0.938
-Original Grad: 0.005, -lr * Pred Grad: -0.051, New P: 0.109
iter 7 loss: 0.330
Actual params: [0.9376, 0.1092]
-Original Grad: -0.004, -lr * Pred Grad: 0.005, New P: 0.942
-Original Grad: 0.006, -lr * Pred Grad: -0.036, New P: 0.073
iter 8 loss: 0.330
Actual params: [0.9421, 0.0729]
-Original Grad: -0.005, -lr * Pred Grad: -0.024, New P: 0.919
-Original Grad: 0.009, -lr * Pred Grad: -0.024, New P: 0.049
iter 9 loss: 0.328
Actual params: [0.9186, 0.0491]
-Original Grad: -0.003, -lr * Pred Grad: -0.017, New P: 0.901
-Original Grad: 0.016, -lr * Pred Grad: -0.003, New P: 0.046
iter 10 loss: 0.327
Actual params: [0.9012, 0.0457]
-Original Grad: -0.001, -lr * Pred Grad: -0.016, New P: 0.885
-Original Grad: 0.021, -lr * Pred Grad: 0.011, New P: 0.056
iter 11 loss: 0.325
Actual params: [0.8849, 0.0565]
-Original Grad: -0.003, -lr * Pred Grad: -0.015, New P: 0.870
-Original Grad: 0.025, -lr * Pred Grad: 0.002, New P: 0.058
iter 12 loss: 0.324
Actual params: [0.8702, 0.058 ]
-Original Grad: 0.002, -lr * Pred Grad: -0.012, New P: 0.858
-Original Grad: 0.024, -lr * Pred Grad: 0.008, New P: 0.066
iter 13 loss: 0.324
Actual params: [0.8581, 0.0662]
-Original Grad: 0.000, -lr * Pred Grad: -0.012, New P: 0.847
-Original Grad: 0.017, -lr * Pred Grad: 0.000, New P: 0.066
iter 14 loss: 0.323
Actual params: [0.8466, 0.0663]
-Original Grad: 0.004, -lr * Pred Grad: -0.010, New P: 0.836
-Original Grad: 0.021, -lr * Pred Grad: 0.000, New P: 0.067
iter 15 loss: 0.323
Actual params: [0.8362, 0.0666]
-Original Grad: -0.002, -lr * Pred Grad: -0.012, New P: 0.824
-Original Grad: 0.014, -lr * Pred Grad: -0.005, New P: 0.062
iter 16 loss: 0.322
Actual params: [0.824 , 0.0621]
-Original Grad: 0.006, -lr * Pred Grad: -0.010, New P: 0.814
-Original Grad: 0.035, -lr * Pred Grad: 0.004, New P: 0.066
iter 17 loss: 0.322
Actual params: [0.8138, 0.0665]
-Original Grad: -0.000, -lr * Pred Grad: -0.010, New P: 0.804
-Original Grad: 0.014, -lr * Pred Grad: 0.001, New P: 0.068
iter 18 loss: 0.322
Actual params: [0.8039, 0.0676]
-Original Grad: -0.004, -lr * Pred Grad: -0.012, New P: 0.792
-Original Grad: 0.011, -lr * Pred Grad: -0.007, New P: 0.060
iter 19 loss: 0.322
Actual params: [0.7917, 0.0601]
-Original Grad: -0.001, -lr * Pred Grad: -0.012, New P: 0.779
-Original Grad: 0.011, -lr * Pred Grad: -0.011, New P: 0.049
iter 20 loss: 0.322
Actual params: [0.7795, 0.0489]
-Original Grad: -0.000, -lr * Pred Grad: -0.010, New P: 0.769
-Original Grad: 0.017, -lr * Pred Grad: -0.008, New P: 0.041
Target params: [1.1812, 0.2779]
iter 0 loss: 0.327
Actual params: [0.5941, 0.5941]
-Original Grad: -0.091, -lr * Pred Grad: -0.044, New P: 0.550
-Original Grad: -0.210, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.307
Actual params: [0.5498, 0.5317]
-Original Grad: -0.056, -lr * Pred Grad: -0.071, New P: 0.478
-Original Grad: -0.284, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.299
Actual params: [0.4785, 0.4514]
-Original Grad: 0.133, -lr * Pred Grad: -0.064, New P: 0.415
-Original Grad: -0.064, -lr * Pred Grad: -0.083, New P: 0.368
iter 3 loss: 0.316
Actual params: [0.4146, 0.368 ]
-Original Grad: 0.223, -lr * Pred Grad: -0.027, New P: 0.388
-Original Grad: 0.047, -lr * Pred Grad: -0.080, New P: 0.288
iter 4 loss: 0.333
Actual params: [0.3878, 0.2876]
-Original Grad: 0.192, -lr * Pred Grad: 0.038, New P: 0.426
-Original Grad: 0.149, -lr * Pred Grad: -0.072, New P: 0.216
iter 5 loss: 0.330
Actual params: [0.4262, 0.216 ]
-Original Grad: 0.137, -lr * Pred Grad: 0.077, New P: 0.503
-Original Grad: 0.110, -lr * Pred Grad: -0.064, New P: 0.152
iter 6 loss: 0.313
Actual params: [0.503 , 0.1518]
-Original Grad: 0.188, -lr * Pred Grad: 0.085, New P: 0.588
-Original Grad: 0.107, -lr * Pred Grad: -0.042, New P: 0.110
iter 7 loss: 0.290
Actual params: [0.5885, 0.1098]
-Original Grad: 0.078, -lr * Pred Grad: 0.086, New P: 0.675
-Original Grad: 0.097, -lr * Pred Grad: -0.010, New P: 0.100
iter 8 loss: 0.278
Actual params: [0.6745, 0.0997]
-Original Grad: 0.011, -lr * Pred Grad: 0.066, New P: 0.741
-Original Grad: 0.066, -lr * Pred Grad: 0.038, New P: 0.138
iter 9 loss: 0.281
Actual params: [0.7407, 0.1379]
-Original Grad: -0.005, -lr * Pred Grad: 0.000, New P: 0.741
-Original Grad: 0.088, -lr * Pred Grad: 0.072, New P: 0.210
iter 10 loss: 0.287
Actual params: [0.7409, 0.21  ]
-Original Grad: -0.033, -lr * Pred Grad: 0.001, New P: 0.742
-Original Grad: 0.003, -lr * Pred Grad: 0.017, New P: 0.227
iter 11 loss: 0.289
Actual params: [0.742 , 0.2268]
-Original Grad: -0.019, -lr * Pred Grad: -0.031, New P: 0.710
-Original Grad: 0.035, -lr * Pred Grad: 0.046, New P: 0.273
iter 12 loss: 0.292
Actual params: [0.7105, 0.2728]
-Original Grad: -0.067, -lr * Pred Grad: -0.045, New P: 0.666
-Original Grad: 0.002, -lr * Pred Grad: -0.021, New P: 0.251
iter 13 loss: 0.288
Actual params: [0.6659, 0.2514]
-Original Grad: 0.005, -lr * Pred Grad: -0.033, New P: 0.632
-Original Grad: 0.072, -lr * Pred Grad: 0.041, New P: 0.293
iter 14 loss: 0.291
Actual params: [0.6325, 0.2925]
-Original Grad: -0.040, -lr * Pred Grad: -0.031, New P: 0.601
-Original Grad: 0.026, -lr * Pred Grad: 0.017, New P: 0.310
iter 15 loss: 0.293
Actual params: [0.6013, 0.3099]
-Original Grad: -0.015, -lr * Pred Grad: -0.027, New P: 0.574
-Original Grad: 0.073, -lr * Pred Grad: 0.053, New P: 0.363
iter 16 loss: 0.297
Actual params: [0.5739, 0.3629]
-Original Grad: -0.006, -lr * Pred Grad: -0.019, New P: 0.555
-Original Grad: 0.029, -lr * Pred Grad: 0.008, New P: 0.371
iter 17 loss: 0.296
Actual params: [0.5549, 0.3712]
-Original Grad: 0.025, -lr * Pred Grad: -0.001, New P: 0.554
-Original Grad: 0.036, -lr * Pred Grad: 0.041, New P: 0.412
iter 18 loss: 0.299
Actual params: [0.5541, 0.412 ]
-Original Grad: 0.021, -lr * Pred Grad: 0.009, New P: 0.563
-Original Grad: 0.006, -lr * Pred Grad: -0.019, New P: 0.393
iter 19 loss: 0.298
Actual params: [0.5634, 0.3935]
-Original Grad: 0.005, -lr * Pred Grad: 0.002, New P: 0.565
-Original Grad: 0.006, -lr * Pred Grad: 0.008, New P: 0.402
iter 20 loss: 0.299
Actual params: [0.565 , 0.4016]
-Original Grad: 0.005, -lr * Pred Grad: -0.008, New P: 0.557
-Original Grad: -0.020, -lr * Pred Grad: -0.029, New P: 0.372
Target params: [1.1812, 0.2779]
iter 0 loss: 0.340
Actual params: [0.5941, 0.5941]
-Original Grad: -0.028, -lr * Pred Grad: 0.009, New P: 0.603
-Original Grad: -0.177, -lr * Pred Grad: -0.061, New P: 0.533
iter 1 loss: 0.343
Actual params: [0.603, 0.533]
-Original Grad: -0.015, -lr * Pred Grad: -0.049, New P: 0.554
-Original Grad: -0.068, -lr * Pred Grad: -0.079, New P: 0.454
iter 2 loss: 0.342
Actual params: [0.5538, 0.4537]
-Original Grad: 0.005, -lr * Pred Grad: -0.063, New P: 0.491
-Original Grad: -0.044, -lr * Pred Grad: -0.078, New P: 0.375
iter 3 loss: 0.342
Actual params: [0.4911, 0.3752]
-Original Grad: 0.011, -lr * Pred Grad: -0.053, New P: 0.439
-Original Grad: -0.020, -lr * Pred Grad: -0.072, New P: 0.304
iter 4 loss: 0.341
Actual params: [0.4386, 0.3037]
-Original Grad: -0.004, -lr * Pred Grad: -0.024, New P: 0.415
-Original Grad: -0.004, -lr * Pred Grad: -0.068, New P: 0.235
iter 5 loss: 0.339
Actual params: [0.4149, 0.2354]
-Original Grad: -0.017, -lr * Pred Grad: -0.016, New P: 0.399
-Original Grad: 0.001, -lr * Pred Grad: -0.061, New P: 0.174
iter 6 loss: 0.336
Actual params: [0.3992, 0.1742]
-Original Grad: -0.002, -lr * Pred Grad: -0.021, New P: 0.378
-Original Grad: 0.007, -lr * Pred Grad: -0.045, New P: 0.129
iter 7 loss: 0.334
Actual params: [0.3783, 0.129 ]
-Original Grad: 0.010, -lr * Pred Grad: -0.013, New P: 0.366
-Original Grad: 0.007, -lr * Pred Grad: -0.031, New P: 0.098
iter 8 loss: 0.330
Actual params: [0.3656, 0.0975]
-Original Grad: 0.001, -lr * Pred Grad: -0.012, New P: 0.353
-Original Grad: 0.015, -lr * Pred Grad: -0.014, New P: 0.084
iter 9 loss: 0.329
Actual params: [0.3532, 0.084 ]
-Original Grad: -0.002, -lr * Pred Grad: -0.016, New P: 0.337
-Original Grad: 0.014, -lr * Pred Grad: 0.007, New P: 0.091
iter 10 loss: 0.331
Actual params: [0.3369, 0.0912]
-Original Grad: -0.000, -lr * Pred Grad: -0.019, New P: 0.318
-Original Grad: 0.014, -lr * Pred Grad: -0.004, New P: 0.087
iter 11 loss: 0.331
Actual params: [0.3181, 0.0874]
-Original Grad: -0.003, -lr * Pred Grad: -0.021, New P: 0.297
-Original Grad: 0.014, -lr * Pred Grad: -0.006, New P: 0.082
iter 12 loss: 0.331
Actual params: [0.2974, 0.0815]
-Original Grad: -0.001, -lr * Pred Grad: -0.021, New P: 0.276
-Original Grad: 0.008, -lr * Pred Grad: -0.011, New P: 0.071
iter 13 loss: 0.330
Actual params: [0.2763, 0.0708]
-Original Grad: -0.004, -lr * Pred Grad: -0.022, New P: 0.255
-Original Grad: 0.015, -lr * Pred Grad: -0.010, New P: 0.061
iter 14 loss: 0.329
Actual params: [0.2547, 0.0608]
-Original Grad: -0.002, -lr * Pred Grad: -0.022, New P: 0.233
-Original Grad: 0.026, -lr * Pred Grad: -0.002, New P: 0.059
iter 15 loss: 0.330
Actual params: [0.233 , 0.0586]
-Original Grad: -0.002, -lr * Pred Grad: -0.021, New P: 0.212
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 0.051
iter 16 loss: 0.329
Actual params: [0.2116, 0.0513]
-Original Grad: -0.003, -lr * Pred Grad: -0.022, New P: 0.190
-Original Grad: 0.016, -lr * Pred Grad: -0.011, New P: 0.041
iter 17 loss: 0.327
Actual params: [0.1901, 0.0407]
-Original Grad: -0.004, -lr * Pred Grad: -0.022, New P: 0.168
-Original Grad: 0.010, -lr * Pred Grad: -0.013, New P: 0.028
iter 18 loss: 0.325
Actual params: [0.1678, 0.0279]
-Original Grad: -0.002, -lr * Pred Grad: -0.023, New P: 0.145
-Original Grad: 0.020, -lr * Pred Grad: -0.010, New P: 0.018
iter 19 loss: 0.324
Actual params: [0.145 , 0.0179]
-Original Grad: -0.005, -lr * Pred Grad: -0.024, New P: 0.121
-Original Grad: 0.013, -lr * Pred Grad: -0.010, New P: 0.008
iter 20 loss: 0.323
Actual params: [0.1213, 0.0083]
-Original Grad: -0.006, -lr * Pred Grad: -0.025, New P: 0.096
-Original Grad: 0.043, -lr * Pred Grad: 0.002, New P: 0.010
Target params: [1.1812, 0.2779]
iter 0 loss: 0.813
Actual params: [0.5941, 0.5941]
-Original Grad: 0.206, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.656, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.718
Actual params: [0.6615, 0.5318]
-Original Grad: 0.223, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.436, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.572
Actual params: [0.7462, 0.4512]
-Original Grad: 0.177, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.192, -lr * Pred Grad: -0.084, New P: 0.368
iter 3 loss: 0.445
Actual params: [0.8338, 0.3676]
-Original Grad: 0.039, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: -0.027, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.378
Actual params: [0.9216, 0.2835]
-Original Grad: 0.018, -lr * Pred Grad: 0.086, New P: 1.008
-Original Grad: -0.007, -lr * Pred Grad: -0.083, New P: 0.200
iter 5 loss: 0.368
Actual params: [1.0081, 0.2003]
-Original Grad: 0.006, -lr * Pred Grad: 0.028, New P: 1.036
-Original Grad: 0.012, -lr * Pred Grad: -0.076, New P: 0.124
iter 6 loss: 0.379
Actual params: [1.0357, 0.1243]
-Original Grad: 0.002, -lr * Pred Grad: 0.028, New P: 1.064
-Original Grad: 0.018, -lr * Pred Grad: -0.070, New P: 0.054
iter 7 loss: 0.389
Actual params: [1.0637, 0.0541]
-Original Grad: -0.005, -lr * Pred Grad: 0.008, New P: 1.072
-Original Grad: 0.009, -lr * Pred Grad: -0.067, New P: -0.013
iter 8 loss: 0.393
Actual params: [ 1.0722, -0.0125]
-Original Grad: -0.007, -lr * Pred Grad: 0.013, New P: 1.085
-Original Grad: 0.004, -lr * Pred Grad: -0.053, New P: -0.066
iter 9 loss: 0.397
Actual params: [ 1.0847, -0.066 ]
-Original Grad: -0.012, -lr * Pred Grad: -0.023, New P: 1.062
-Original Grad: 0.005, -lr * Pred Grad: -0.039, New P: -0.105
iter 10 loss: 0.394
Actual params: [ 1.0616, -0.105 ]
-Original Grad: -0.012, -lr * Pred Grad: -0.024, New P: 1.038
-Original Grad: 0.010, -lr * Pred Grad: -0.032, New P: -0.137
iter 11 loss: 0.394
Actual params: [ 1.0375, -0.1366]
-Original Grad: 0.004, -lr * Pred Grad: -0.018, New P: 1.020
-Original Grad: 0.012, -lr * Pred Grad: -0.028, New P: -0.164
iter 12 loss: 0.394
Actual params: [ 1.0198, -0.1644]
-Original Grad: 0.003, -lr * Pred Grad: -0.011, New P: 1.009
-Original Grad: 0.010, -lr * Pred Grad: -0.021, New P: -0.185
iter 13 loss: 0.395
Actual params: [ 1.0086, -0.1853]
-Original Grad: 0.010, -lr * Pred Grad: -0.007, New P: 1.002
-Original Grad: 0.014, -lr * Pred Grad: -0.004, New P: -0.189
iter 14 loss: 0.395
Actual params: [ 1.002 , -0.1891]
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 0.995
-Original Grad: 0.013, -lr * Pred Grad: -0.001, New P: -0.190
iter 15 loss: 0.395
Actual params: [ 0.9955, -0.1901]
-Original Grad: 0.005, -lr * Pred Grad: -0.008, New P: 0.987
-Original Grad: 0.015, -lr * Pred Grad: -0.009, New P: -0.199
iter 16 loss: 0.396
Actual params: [ 0.9872, -0.1994]
-Original Grad: 0.007, -lr * Pred Grad: -0.008, New P: 0.980
-Original Grad: 0.013, -lr * Pred Grad: -0.008, New P: -0.208
iter 17 loss: 0.396
Actual params: [ 0.9796, -0.2076]
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 0.973
-Original Grad: 0.012, -lr * Pred Grad: -0.011, New P: -0.218
iter 18 loss: 0.397
Actual params: [ 0.9729, -0.2183]
-Original Grad: 0.012, -lr * Pred Grad: -0.003, New P: 0.970
-Original Grad: 0.010, -lr * Pred Grad: -0.013, New P: -0.231
iter 19 loss: 0.397
Actual params: [ 0.9695, -0.231 ]
-Original Grad: 0.007, -lr * Pred Grad: -0.003, New P: 0.966
-Original Grad: 0.011, -lr * Pred Grad: -0.014, New P: -0.245
iter 20 loss: 0.398
Actual params: [ 0.9661, -0.2448]
-Original Grad: 0.008, -lr * Pred Grad: -0.004, New P: 0.962
-Original Grad: 0.013, -lr * Pred Grad: -0.013, New P: -0.258
Target params: [1.1812, 0.2779]
iter 0 loss: 0.595
Actual params: [0.5941, 0.5941]
-Original Grad: 0.196, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.088, -lr * Pred Grad: -0.043, New P: 0.551
iter 1 loss: 0.582
Actual params: [0.6614, 0.5514]
-Original Grad: 0.204, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.043, -lr * Pred Grad: -0.069, New P: 0.482
iter 2 loss: 0.568
Actual params: [0.746 , 0.4821]
-Original Grad: 0.274, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.031, -lr * Pred Grad: -0.069, New P: 0.413
iter 3 loss: 0.549
Actual params: [0.8337, 0.4126]
-Original Grad: 0.056, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: -0.015, -lr * Pred Grad: -0.067, New P: 0.346
iter 4 loss: 0.534
Actual params: [0.9217, 0.3457]
-Original Grad: 0.024, -lr * Pred Grad: 0.088, New P: 1.009
-Original Grad: 0.022, -lr * Pred Grad: -0.057, New P: 0.289
iter 5 loss: 0.530
Actual params: [1.0093, 0.2888]
-Original Grad: 0.015, -lr * Pred Grad: 0.077, New P: 1.086
-Original Grad: 0.054, -lr * Pred Grad: -0.031, New P: 0.258
iter 6 loss: 0.532
Actual params: [1.0859, 0.2582]
-Original Grad: -0.022, -lr * Pred Grad: -0.012, New P: 1.074
-Original Grad: 0.037, -lr * Pred Grad: 0.004, New P: 0.263
iter 7 loss: 0.532
Actual params: [1.0738, 0.2627]
-Original Grad: -0.004, -lr * Pred Grad: 0.025, New P: 1.099
-Original Grad: 0.061, -lr * Pred Grad: 0.044, New P: 0.306
iter 8 loss: 0.526
Actual params: [1.0986, 0.3065]
-Original Grad: -0.022, -lr * Pred Grad: -0.008, New P: 1.091
-Original Grad: 0.017, -lr * Pred Grad: 0.017, New P: 0.323
iter 9 loss: 0.520
Actual params: [1.0908, 0.3234]
-Original Grad: -0.015, -lr * Pred Grad: -0.012, New P: 1.079
-Original Grad: 0.061, -lr * Pred Grad: 0.044, New P: 0.368
iter 10 loss: 0.504
Actual params: [1.0789, 0.3676]
-Original Grad: -0.003, -lr * Pred Grad: -0.021, New P: 1.058
-Original Grad: 0.053, -lr * Pred Grad: 0.034, New P: 0.402
iter 11 loss: 0.492
Actual params: [1.0577, 0.4021]
-Original Grad: -0.002, -lr * Pred Grad: -0.017, New P: 1.041
-Original Grad: -0.050, -lr * Pred Grad: -0.024, New P: 0.378
iter 12 loss: 0.505
Actual params: [1.041 , 0.3776]
-Original Grad: 0.015, -lr * Pred Grad: -0.006, New P: 1.035
-Original Grad: 0.014, -lr * Pred Grad: -0.014, New P: 0.364
iter 13 loss: 0.511
Actual params: [1.0352, 0.3637]
-Original Grad: 0.014, -lr * Pred Grad: -0.001, New P: 1.034
-Original Grad: -0.019, -lr * Pred Grad: -0.022, New P: 0.341
iter 14 loss: 0.519
Actual params: [1.0344, 0.3414]
-Original Grad: -0.004, -lr * Pred Grad: -0.010, New P: 1.024
-Original Grad: -0.030, -lr * Pred Grad: -0.030, New P: 0.312
iter 15 loss: 0.523
Actual params: [1.0244, 0.3119]
-Original Grad: 0.031, -lr * Pred Grad: 0.002, New P: 1.026
-Original Grad: 0.055, -lr * Pred Grad: 0.006, New P: 0.318
iter 16 loss: 0.529
Actual params: [1.0262, 0.3181]
-Original Grad: 0.016, -lr * Pred Grad: 0.005, New P: 1.031
-Original Grad: 0.021, -lr * Pred Grad: 0.021, New P: 0.339
iter 17 loss: 0.520
Actual params: [1.0312, 0.3395]
-Original Grad: 0.012, -lr * Pred Grad: 0.000, New P: 1.031
-Original Grad: -0.001, -lr * Pred Grad: -0.004, New P: 0.336
iter 18 loss: 0.522
Actual params: [1.0314, 0.3356]
-Original Grad: 0.007, -lr * Pred Grad: -0.005, New P: 1.026
-Original Grad: 0.012, -lr * Pred Grad: -0.005, New P: 0.330
iter 19 loss: 0.524
Actual params: [1.0264, 0.3303]
-Original Grad: 0.015, -lr * Pred Grad: -0.002, New P: 1.025
-Original Grad: 0.021, -lr * Pred Grad: 0.001, New P: 0.331
iter 20 loss: 0.524
Actual params: [1.0246, 0.3311]
-Original Grad: 0.036, -lr * Pred Grad: 0.012, New P: 1.037
-Original Grad: 0.042, -lr * Pred Grad: 0.018, New P: 0.349
Target params: [1.1812, 0.2779]
iter 0 loss: 0.451
Actual params: [0.5941, 0.5941]
-Original Grad: 0.240, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: 0.023, -lr * Pred Grad: 0.049, New P: 0.643
iter 1 loss: 0.390
Actual params: [0.6616, 0.6433]
-Original Grad: 0.432, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: 0.043, -lr * Pred Grad: 0.033, New P: 0.676
iter 2 loss: 0.293
Actual params: [0.7466, 0.6764]
-Original Grad: 0.161, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: 0.022, -lr * Pred Grad: -0.024, New P: 0.653
iter 3 loss: 0.219
Actual params: [0.8344, 0.6526]
-Original Grad: -0.040, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.011, -lr * Pred Grad: -0.030, New P: 0.623
iter 4 loss: 0.190
Actual params: [0.9226, 0.6228]
-Original Grad: -0.072, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.042, -lr * Pred Grad: -0.032, New P: 0.591
iter 5 loss: 0.241
Actual params: [1.0105, 0.5911]
-Original Grad: -0.126, -lr * Pred Grad: 0.079, New P: 1.089
-Original Grad: -0.074, -lr * Pred Grad: -0.046, New P: 0.545
iter 6 loss: 0.256
Actual params: [1.0894, 0.5454]
-Original Grad: 0.025, -lr * Pred Grad: -0.011, New P: 1.078
-Original Grad: -0.081, -lr * Pred Grad: -0.054, New P: 0.491
iter 7 loss: 0.225
Actual params: [1.0784, 0.4909]
-Original Grad: -0.047, -lr * Pred Grad: 0.023, New P: 1.101
-Original Grad: -0.074, -lr * Pred Grad: -0.059, New P: 0.432
iter 8 loss: 0.201
Actual params: [1.101, 0.432]
-Original Grad: -0.029, -lr * Pred Grad: 0.017, New P: 1.118
-Original Grad: -0.072, -lr * Pred Grad: -0.061, New P: 0.371
iter 9 loss: 0.184
Actual params: [1.1176, 0.3712]
-Original Grad: 0.058, -lr * Pred Grad: 0.052, New P: 1.169
-Original Grad: -0.051, -lr * Pred Grad: -0.060, New P: 0.312
iter 10 loss: 0.182
Actual params: [1.1694, 0.3117]
-Original Grad: 0.022, -lr * Pred Grad: 0.014, New P: 1.183
-Original Grad: -0.041, -lr * Pred Grad: -0.055, New P: 0.257
iter 11 loss: 0.193
Actual params: [1.1835, 0.2567]
-Original Grad: 0.017, -lr * Pred Grad: 0.019, New P: 1.203
-Original Grad: -0.038, -lr * Pred Grad: -0.048, New P: 0.209
iter 12 loss: 0.213
Actual params: [1.2028, 0.2087]
-Original Grad: -0.068, -lr * Pred Grad: -0.038, New P: 1.164
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.176
iter 13 loss: 0.204
Actual params: [1.1644, 0.1756]
-Original Grad: -0.017, -lr * Pred Grad: -0.041, New P: 1.124
-Original Grad: -0.002, -lr * Pred Grad: -0.021, New P: 0.155
iter 14 loss: 0.197
Actual params: [1.1236, 0.1545]
-Original Grad: 0.028, -lr * Pred Grad: -0.006, New P: 1.117
-Original Grad: -0.006, -lr * Pred Grad: -0.018, New P: 0.136
iter 15 loss: 0.199
Actual params: [1.1173, 0.1365]
-Original Grad: 0.020, -lr * Pred Grad: 0.017, New P: 1.134
-Original Grad: -0.009, -lr * Pred Grad: -0.021, New P: 0.115
iter 16 loss: 0.206
Actual params: [1.1342, 0.1152]
-Original Grad: -0.015, -lr * Pred Grad: -0.016, New P: 1.118
-Original Grad: 0.016, -lr * Pred Grad: -0.017, New P: 0.098
iter 17 loss: 0.208
Actual params: [1.1184, 0.0983]
-Original Grad: -0.006, -lr * Pred Grad: -0.016, New P: 1.102
-Original Grad: 0.002, -lr * Pred Grad: -0.015, New P: 0.084
iter 18 loss: 0.211
Actual params: [1.1021, 0.0838]
-Original Grad: 0.000, -lr * Pred Grad: -0.015, New P: 1.087
-Original Grad: 0.004, -lr * Pred Grad: -0.016, New P: 0.068
iter 19 loss: 0.217
Actual params: [1.0872, 0.0678]
-Original Grad: 0.005, -lr * Pred Grad: -0.009, New P: 1.078
-Original Grad: 0.010, -lr * Pred Grad: -0.016, New P: 0.052
iter 20 loss: 0.223
Actual params: [1.0782, 0.0517]
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 1.071
-Original Grad: 0.014, -lr * Pred Grad: -0.013, New P: 0.038
Target params: [1.1812, 0.2779]
iter 0 loss: 0.334
Actual params: [0.5941, 0.5941]
-Original Grad: 0.015, -lr * Pred Grad: 0.045, New P: 0.639
-Original Grad: -0.217, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.305
Actual params: [0.6391, 0.5316]
-Original Grad: 0.014, -lr * Pred Grad: -0.006, New P: 0.633
-Original Grad: -0.098, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.294
Actual params: [0.6333, 0.4512]
-Original Grad: 0.035, -lr * Pred Grad: -0.029, New P: 0.605
-Original Grad: -0.085, -lr * Pred Grad: -0.083, New P: 0.368
iter 3 loss: 0.288
Actual params: [0.6046, 0.3684]
-Original Grad: 0.041, -lr * Pred Grad: 0.011, New P: 0.615
-Original Grad: -0.048, -lr * Pred Grad: -0.079, New P: 0.289
iter 4 loss: 0.277
Actual params: [0.6155, 0.2895]
-Original Grad: 0.036, -lr * Pred Grad: 0.032, New P: 0.648
-Original Grad: -0.018, -lr * Pred Grad: -0.072, New P: 0.218
iter 5 loss: 0.263
Actual params: [0.6478, 0.2178]
-Original Grad: 0.019, -lr * Pred Grad: 0.012, New P: 0.660
-Original Grad: -0.013, -lr * Pred Grad: -0.069, New P: 0.149
iter 6 loss: 0.253
Actual params: [0.6601, 0.1491]
-Original Grad: 0.011, -lr * Pred Grad: 0.009, New P: 0.669
-Original Grad: 0.001, -lr * Pred Grad: -0.063, New P: 0.086
iter 7 loss: 0.247
Actual params: [0.6693, 0.0864]
-Original Grad: 0.004, -lr * Pred Grad: -0.011, New P: 0.658
-Original Grad: 0.007, -lr * Pred Grad: -0.048, New P: 0.039
iter 8 loss: 0.243
Actual params: [0.6584, 0.0388]
-Original Grad: 0.008, -lr * Pred Grad: -0.004, New P: 0.655
-Original Grad: 0.005, -lr * Pred Grad: -0.036, New P: 0.003
iter 9 loss: 0.240
Actual params: [0.6546, 0.0031]
-Original Grad: 0.003, -lr * Pred Grad: -0.008, New P: 0.646
-Original Grad: 0.006, -lr * Pred Grad: -0.029, New P: -0.025
iter 10 loss: 0.239
Actual params: [ 0.6463, -0.0255]
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: 0.639
-Original Grad: 0.027, -lr * Pred Grad: -0.016, New P: -0.041
iter 11 loss: 0.238
Actual params: [ 0.6394, -0.041 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.011, New P: 0.629
-Original Grad: 0.058, -lr * Pred Grad: 0.015, New P: -0.026
iter 12 loss: 0.239
Actual params: [ 0.6287, -0.0261]
-Original Grad: 0.008, -lr * Pred Grad: -0.009, New P: 0.620
-Original Grad: 0.038, -lr * Pred Grad: 0.037, New P: 0.011
iter 13 loss: 0.242
Actual params: [0.6196, 0.0109]
-Original Grad: 0.015, -lr * Pred Grad: -0.003, New P: 0.617
-Original Grad: 0.004, -lr * Pred Grad: -0.005, New P: 0.006
iter 14 loss: 0.241
Actual params: [0.6165, 0.0059]
-Original Grad: 0.020, -lr * Pred Grad: 0.004, New P: 0.620
-Original Grad: 0.010, -lr * Pred Grad: -0.004, New P: 0.001
iter 15 loss: 0.241
Actual params: [0.6203, 0.0014]
-Original Grad: 0.017, -lr * Pred Grad: 0.005, New P: 0.625
-Original Grad: 0.001, -lr * Pred Grad: -0.018, New P: -0.016
iter 16 loss: 0.240
Actual params: [ 0.625 , -0.0163]
-Original Grad: 0.016, -lr * Pred Grad: 0.003, New P: 0.628
-Original Grad: 0.003, -lr * Pred Grad: -0.018, New P: -0.034
iter 17 loss: 0.239
Actual params: [ 0.6277, -0.0341]
-Original Grad: 0.005, -lr * Pred Grad: -0.003, New P: 0.624
-Original Grad: 0.053, -lr * Pred Grad: 0.009, New P: -0.025
iter 18 loss: 0.239
Actual params: [ 0.6243, -0.0254]
-Original Grad: 0.022, -lr * Pred Grad: 0.001, New P: 0.625
-Original Grad: 0.025, -lr * Pred Grad: 0.019, New P: -0.007
iter 19 loss: 0.240
Actual params: [ 0.6254, -0.0068]
-Original Grad: 0.019, -lr * Pred Grad: 0.006, New P: 0.631
-Original Grad: 0.012, -lr * Pred Grad: 0.002, New P: -0.005
iter 20 loss: 0.240
Actual params: [ 0.6313, -0.005 ]
-Original Grad: 0.007, -lr * Pred Grad: 0.002, New P: 0.633
-Original Grad: 0.011, -lr * Pred Grad: -0.010, New P: -0.015
Target params: [1.1812, 0.2779]
iter 0 loss: 0.244
Actual params: [0.5941, 0.5941]
-Original Grad: 0.046, -lr * Pred Grad: 0.058, New P: 0.652
-Original Grad: -0.272, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.230
Actual params: [0.6516, 0.531 ]
-Original Grad: 0.018, -lr * Pred Grad: 0.035, New P: 0.686
-Original Grad: -0.118, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.226
Actual params: [0.6865, 0.4504]
-Original Grad: 0.014, -lr * Pred Grad: -0.029, New P: 0.658
-Original Grad: -0.066, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.225
Actual params: [0.6577, 0.3672]
-Original Grad: -0.002, -lr * Pred Grad: -0.031, New P: 0.627
-Original Grad: -0.020, -lr * Pred Grad: -0.080, New P: 0.287
iter 4 loss: 0.226
Actual params: [0.627 , 0.2872]
-Original Grad: -0.004, -lr * Pred Grad: -0.006, New P: 0.621
-Original Grad: -0.012, -lr * Pred Grad: -0.072, New P: 0.215
iter 5 loss: 0.227
Actual params: [0.6211, 0.2151]
-Original Grad: -0.005, -lr * Pred Grad: -0.008, New P: 0.613
-Original Grad: -0.001, -lr * Pred Grad: -0.069, New P: 0.146
iter 6 loss: 0.227
Actual params: [0.613 , 0.1462]
-Original Grad: -0.003, -lr * Pred Grad: -0.012, New P: 0.601
-Original Grad: -0.002, -lr * Pred Grad: -0.063, New P: 0.083
iter 7 loss: 0.226
Actual params: [0.6012, 0.0833]
-Original Grad: -0.007, -lr * Pred Grad: -0.016, New P: 0.585
-Original Grad: 0.007, -lr * Pred Grad: -0.048, New P: 0.036
iter 8 loss: 0.227
Actual params: [0.5851, 0.0356]
-Original Grad: -0.005, -lr * Pred Grad: -0.017, New P: 0.568
-Original Grad: 0.016, -lr * Pred Grad: -0.035, New P: 0.001
iter 9 loss: 0.228
Actual params: [0.5685, 0.001 ]
-Original Grad: -0.003, -lr * Pred Grad: -0.015, New P: 0.553
-Original Grad: 0.035, -lr * Pred Grad: -0.024, New P: -0.023
iter 10 loss: 0.231
Actual params: [ 0.5534, -0.0229]
-Original Grad: 0.027, -lr * Pred Grad: -0.000, New P: 0.553
-Original Grad: 0.014, -lr * Pred Grad: -0.011, New P: -0.034
iter 11 loss: 0.231
Actual params: [ 0.5532, -0.0339]
-Original Grad: 0.026, -lr * Pred Grad: 0.010, New P: 0.563
-Original Grad: 0.019, -lr * Pred Grad: 0.009, New P: -0.025
iter 12 loss: 0.229
Actual params: [ 0.5632, -0.0253]
-Original Grad: 0.014, -lr * Pred Grad: 0.005, New P: 0.568
-Original Grad: 0.015, -lr * Pred Grad: -0.003, New P: -0.029
iter 13 loss: 0.228
Actual params: [ 0.5681, -0.0285]
-Original Grad: 0.004, -lr * Pred Grad: -0.006, New P: 0.562
-Original Grad: 0.010, -lr * Pred Grad: -0.010, New P: -0.039
iter 14 loss: 0.229
Actual params: [ 0.562 , -0.0386]
-Original Grad: 0.014, -lr * Pred Grad: -0.006, New P: 0.556
-Original Grad: 0.020, -lr * Pred Grad: -0.006, New P: -0.045
iter 15 loss: 0.230
Actual params: [ 0.5563, -0.0451]
-Original Grad: 0.030, -lr * Pred Grad: 0.007, New P: 0.563
-Original Grad: 0.019, -lr * Pred Grad: -0.004, New P: -0.050
iter 16 loss: 0.229
Actual params: [ 0.5633, -0.0496]
-Original Grad: 0.026, -lr * Pred Grad: 0.013, New P: 0.576
-Original Grad: 0.009, -lr * Pred Grad: -0.009, New P: -0.059
iter 17 loss: 0.227
Actual params: [ 0.5759, -0.0589]
-Original Grad: -0.005, -lr * Pred Grad: -0.004, New P: 0.572
-Original Grad: 0.025, -lr * Pred Grad: -0.006, New P: -0.065
iter 18 loss: 0.228
Actual params: [ 0.5715, -0.0648]
-Original Grad: 0.007, -lr * Pred Grad: -0.010, New P: 0.562
-Original Grad: 0.023, -lr * Pred Grad: -0.002, New P: -0.067
iter 19 loss: 0.229
Actual params: [ 0.5618, -0.0667]
-Original Grad: 0.018, -lr * Pred Grad: -0.002, New P: 0.560
-Original Grad: 0.022, -lr * Pred Grad: -0.002, New P: -0.069
iter 20 loss: 0.229
Actual params: [ 0.5601, -0.0686]
-Original Grad: 0.006, -lr * Pred Grad: 0.000, New P: 0.561
-Original Grad: 0.018, -lr * Pred Grad: -0.005, New P: -0.074
Target params: [1.1812, 0.2779]
iter 0 loss: 0.336
Actual params: [0.5941, 0.5941]
-Original Grad: 0.267, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.214, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.263
Actual params: [0.6616, 0.5316]
-Original Grad: 0.099, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.075, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.181
Actual params: [0.7462, 0.4514]
-Original Grad: -0.007, -lr * Pred Grad: 0.087, New P: 0.833
-Original Grad: 0.004, -lr * Pred Grad: -0.079, New P: 0.372
iter 3 loss: 0.147
Actual params: [0.8328, 0.3725]
-Original Grad: 0.013, -lr * Pred Grad: 0.078, New P: 0.911
-Original Grad: 0.011, -lr * Pred Grad: -0.071, New P: 0.301
iter 4 loss: 0.140
Actual params: [0.9107, 0.3011]
-Original Grad: 0.022, -lr * Pred Grad: 0.008, New P: 0.918
-Original Grad: -0.002, -lr * Pred Grad: -0.068, New P: 0.233
iter 5 loss: 0.138
Actual params: [0.9183, 0.2333]
-Original Grad: 0.021, -lr * Pred Grad: 0.052, New P: 0.970
-Original Grad: 0.010, -lr * Pred Grad: -0.058, New P: 0.176
iter 6 loss: 0.132
Actual params: [0.9704, 0.1756]
-Original Grad: 0.041, -lr * Pred Grad: 0.040, New P: 1.011
-Original Grad: -0.004, -lr * Pred Grad: -0.041, New P: 0.135
iter 7 loss: 0.127
Actual params: [1.0108, 0.1346]
-Original Grad: 0.061, -lr * Pred Grad: 0.062, New P: 1.073
-Original Grad: -0.001, -lr * Pred Grad: -0.027, New P: 0.107
iter 8 loss: 0.117
Actual params: [1.0728, 0.1075]
-Original Grad: 0.042, -lr * Pred Grad: 0.024, New P: 1.096
-Original Grad: 0.005, -lr * Pred Grad: -0.007, New P: 0.101
iter 9 loss: 0.114
Actual params: [1.0965, 0.1005]
-Original Grad: 0.044, -lr * Pred Grad: 0.049, New P: 1.146
-Original Grad: -0.001, -lr * Pred Grad: -0.010, New P: 0.091
iter 10 loss: 0.113
Actual params: [1.1457, 0.0907]
-Original Grad: 0.012, -lr * Pred Grad: -0.016, New P: 1.130
-Original Grad: 0.012, -lr * Pred Grad: -0.013, New P: 0.078
iter 11 loss: 0.112
Actual params: [1.1299, 0.0777]
-Original Grad: 0.024, -lr * Pred Grad: 0.024, New P: 1.154
-Original Grad: 0.017, -lr * Pred Grad: -0.005, New P: 0.072
iter 12 loss: 0.112
Actual params: [1.1543, 0.0724]
-Original Grad: 0.014, -lr * Pred Grad: -0.014, New P: 1.140
-Original Grad: 0.010, -lr * Pred Grad: -0.007, New P: 0.066
iter 13 loss: 0.112
Actual params: [1.1401, 0.0656]
-Original Grad: 0.017, -lr * Pred Grad: 0.012, New P: 1.153
-Original Grad: 0.008, -lr * Pred Grad: -0.011, New P: 0.054
iter 14 loss: 0.111
Actual params: [1.1526, 0.0544]
-Original Grad: 0.006, -lr * Pred Grad: -0.014, New P: 1.139
-Original Grad: 0.005, -lr * Pred Grad: -0.016, New P: 0.039
iter 15 loss: 0.111
Actual params: [1.1386, 0.0387]
-Original Grad: 0.016, -lr * Pred Grad: 0.004, New P: 1.143
-Original Grad: 0.001, -lr * Pred Grad: -0.020, New P: 0.019
iter 16 loss: 0.110
Actual params: [1.1426, 0.0188]
-Original Grad: 0.023, -lr * Pred Grad: 0.003, New P: 1.145
-Original Grad: 0.030, -lr * Pred Grad: -0.009, New P: 0.010
iter 17 loss: 0.109
Actual params: [1.1452, 0.01  ]
-Original Grad: 0.012, -lr * Pred Grad: 0.002, New P: 1.147
-Original Grad: 0.002, -lr * Pred Grad: -0.009, New P: 0.001
iter 18 loss: 0.109
Actual params: [1.1474e+00, 8.7716e-04]
-Original Grad: 0.016, -lr * Pred Grad: 0.000, New P: 1.148
-Original Grad: 0.002, -lr * Pred Grad: -0.018, New P: -0.017
iter 19 loss: 0.109
Actual params: [ 1.1479, -0.0173]
-Original Grad: 0.008, -lr * Pred Grad: -0.004, New P: 1.144
-Original Grad: 0.019, -lr * Pred Grad: -0.015, New P: -0.033
iter 20 loss: 0.108
Actual params: [ 1.1442, -0.0325]
-Original Grad: 0.011, -lr * Pred Grad: -0.004, New P: 1.140
-Original Grad: 0.012, -lr * Pred Grad: -0.011, New P: -0.044
Target params: [1.1812, 0.2779]
iter 0 loss: 0.252
Actual params: [0.5941, 0.5941]
-Original Grad: 0.101, -lr * Pred Grad: 0.065, New P: 0.659
-Original Grad: -0.123, -lr * Pred Grad: -0.055, New P: 0.539
iter 1 loss: 0.205
Actual params: [0.6592, 0.5394]
-Original Grad: 0.115, -lr * Pred Grad: 0.083, New P: 0.742
-Original Grad: -0.154, -lr * Pred Grad: -0.079, New P: 0.461
iter 2 loss: 0.179
Actual params: [0.7421, 0.4606]
-Original Grad: -0.004, -lr * Pred Grad: 0.067, New P: 0.809
-Original Grad: -0.055, -lr * Pred Grad: -0.081, New P: 0.379
iter 3 loss: 0.165
Actual params: [0.8086, 0.3794]
-Original Grad: -0.029, -lr * Pred Grad: -0.028, New P: 0.780
-Original Grad: -0.031, -lr * Pred Grad: -0.074, New P: 0.305
iter 4 loss: 0.144
Actual params: [0.7802, 0.3049]
-Original Grad: -0.012, -lr * Pred Grad: -0.014, New P: 0.766
-Original Grad: -0.018, -lr * Pred Grad: -0.070, New P: 0.235
iter 5 loss: 0.131
Actual params: [0.7657, 0.2352]
-Original Grad: -0.021, -lr * Pred Grad: -0.014, New P: 0.752
-Original Grad: -0.010, -lr * Pred Grad: -0.066, New P: 0.169
iter 6 loss: 0.123
Actual params: [0.752, 0.169]
-Original Grad: 0.011, -lr * Pred Grad: -0.005, New P: 0.747
-Original Grad: -0.003, -lr * Pred Grad: -0.056, New P: 0.113
iter 7 loss: 0.119
Actual params: [0.7474, 0.1127]
-Original Grad: -0.047, -lr * Pred Grad: -0.033, New P: 0.714
-Original Grad: 0.008, -lr * Pred Grad: -0.041, New P: 0.072
iter 8 loss: 0.110
Actual params: [0.7141, 0.0719]
-Original Grad: 0.003, -lr * Pred Grad: -0.024, New P: 0.690
-Original Grad: 0.010, -lr * Pred Grad: -0.029, New P: 0.043
iter 9 loss: 0.103
Actual params: [0.69 , 0.043]
-Original Grad: 0.013, -lr * Pred Grad: -0.005, New P: 0.685
-Original Grad: 0.014, -lr * Pred Grad: -0.014, New P: 0.029
iter 10 loss: 0.101
Actual params: [0.6849, 0.0291]
-Original Grad: 0.012, -lr * Pred Grad: 0.001, New P: 0.686
-Original Grad: 0.013, -lr * Pred Grad: 0.004, New P: 0.033
iter 11 loss: 0.101
Actual params: [0.6858, 0.0329]
-Original Grad: -0.006, -lr * Pred Grad: -0.010, New P: 0.676
-Original Grad: 0.017, -lr * Pred Grad: -0.002, New P: 0.031
iter 12 loss: 0.099
Actual params: [0.676 , 0.0307]
-Original Grad: 0.015, -lr * Pred Grad: -0.007, New P: 0.669
-Original Grad: 0.012, -lr * Pred Grad: -0.007, New P: 0.023
iter 13 loss: 0.097
Actual params: [0.6693, 0.0233]
-Original Grad: 0.020, -lr * Pred Grad: 0.001, New P: 0.670
-Original Grad: 0.016, -lr * Pred Grad: -0.007, New P: 0.016
iter 14 loss: 0.097
Actual params: [0.6703, 0.0162]
-Original Grad: 0.005, -lr * Pred Grad: -0.003, New P: 0.667
-Original Grad: 0.016, -lr * Pred Grad: -0.007, New P: 0.009
iter 15 loss: 0.096
Actual params: [0.6674, 0.0094]
-Original Grad: 0.023, -lr * Pred Grad: 0.001, New P: 0.669
-Original Grad: 0.023, -lr * Pred Grad: -0.003, New P: 0.007
iter 16 loss: 0.097
Actual params: [0.6687, 0.0065]
-Original Grad: 0.022, -lr * Pred Grad: 0.006, New P: 0.675
-Original Grad: 0.015, -lr * Pred Grad: -0.005, New P: 0.002
iter 17 loss: 0.098
Actual params: [0.6752, 0.0019]
-Original Grad: 0.013, -lr * Pred Grad: 0.002, New P: 0.678
-Original Grad: 0.014, -lr * Pred Grad: -0.009, New P: -0.007
iter 18 loss: 0.099
Actual params: [ 0.6776, -0.0072]
-Original Grad: 0.012, -lr * Pred Grad: -0.001, New P: 0.677
-Original Grad: 0.017, -lr * Pred Grad: -0.010, New P: -0.018
iter 19 loss: 0.099
Actual params: [ 0.6767, -0.0175]
-Original Grad: 0.028, -lr * Pred Grad: 0.007, New P: 0.684
-Original Grad: 0.020, -lr * Pred Grad: -0.008, New P: -0.026
iter 20 loss: 0.101
Actual params: [ 0.6839, -0.0257]
-Original Grad: 0.005, -lr * Pred Grad: 0.002, New P: 0.686
-Original Grad: 0.018, -lr * Pred Grad: -0.007, New P: -0.033
Target params: [1.1812, 0.2779]
iter 0 loss: 1.004
Actual params: [0.5941, 0.5941]
-Original Grad: 0.311, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.474, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.935
Actual params: [0.6617, 0.5312]
-Original Grad: 0.398, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.468, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.834
Actual params: [0.7467, 0.4507]
-Original Grad: 0.337, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.245, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.708
Actual params: [0.8346, 0.3671]
-Original Grad: 0.293, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.187, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.581
Actual params: [0.9228, 0.2829]
-Original Grad: 0.092, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.075, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.504
Actual params: [1.0111, 0.1986]
-Original Grad: 0.027, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: -0.025, -lr * Pred Grad: -0.084, New P: 0.115
iter 6 loss: 0.481
Actual params: [1.0995, 0.1149]
-Original Grad: -0.020, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: -0.007, -lr * Pred Grad: -0.078, New P: 0.036
iter 7 loss: 0.511
Actual params: [1.1878, 0.0364]
-Original Grad: -0.051, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.008, -lr * Pred Grad: -0.071, New P: -0.035
iter 8 loss: 0.568
Actual params: [ 1.2759, -0.0346]
-Original Grad: -0.052, -lr * Pred Grad: 0.087, New P: 1.363
-Original Grad: 0.011, -lr * Pred Grad: -0.068, New P: -0.103
iter 9 loss: 0.612
Actual params: [ 1.3627, -0.1029]
-Original Grad: -0.055, -lr * Pred Grad: 0.071, New P: 1.434
-Original Grad: 0.016, -lr * Pred Grad: -0.058, New P: -0.161
iter 10 loss: 0.630
Actual params: [ 1.434 , -0.1612]
-Original Grad: -0.031, -lr * Pred Grad: -0.004, New P: 1.429
-Original Grad: 0.018, -lr * Pred Grad: -0.041, New P: -0.202
iter 11 loss: 0.625
Actual params: [ 1.4295, -0.2018]
-Original Grad: -0.027, -lr * Pred Grad: 0.040, New P: 1.470
-Original Grad: 0.011, -lr * Pred Grad: -0.031, New P: -0.233
iter 12 loss: 0.642
Actual params: [ 1.4695, -0.2332]
-Original Grad: -0.034, -lr * Pred Grad: 0.042, New P: 1.512
-Original Grad: 0.015, -lr * Pred Grad: -0.029, New P: -0.262
iter 13 loss: 0.659
Actual params: [ 1.5117, -0.2622]
-Original Grad: -0.034, -lr * Pred Grad: 0.047, New P: 1.559
-Original Grad: 0.031, -lr * Pred Grad: -0.025, New P: -0.287
iter 14 loss: 0.673
Actual params: [ 1.559 , -0.2875]
-Original Grad: -0.022, -lr * Pred Grad: 0.006, New P: 1.565
-Original Grad: 0.009, -lr * Pred Grad: -0.023, New P: -0.311
iter 15 loss: 0.664
Actual params: [ 1.5654, -0.3108]
-Original Grad: -0.035, -lr * Pred Grad: -0.021, New P: 1.544
-Original Grad: 0.025, -lr * Pred Grad: -0.009, New P: -0.320
iter 16 loss: 0.669
Actual params: [ 1.5443, -0.3202]
-Original Grad: -0.032, -lr * Pred Grad: -0.035, New P: 1.509
-Original Grad: 0.018, -lr * Pred Grad: 0.010, New P: -0.310
iter 17 loss: 0.657
Actual params: [ 1.5089, -0.3105]
-Original Grad: -0.033, -lr * Pred Grad: -0.035, New P: 1.474
-Original Grad: 0.027, -lr * Pred Grad: 0.001, New P: -0.310
iter 18 loss: 0.643
Actual params: [ 1.4739, -0.3098]
-Original Grad: -0.033, -lr * Pred Grad: -0.034, New P: 1.440
-Original Grad: 0.025, -lr * Pred Grad: 0.005, New P: -0.305
iter 19 loss: 0.628
Actual params: [ 1.4401, -0.3047]
-Original Grad: -0.041, -lr * Pred Grad: -0.036, New P: 1.404
-Original Grad: 0.031, -lr * Pred Grad: 0.007, New P: -0.298
iter 20 loss: 0.623
Actual params: [ 1.4045, -0.2981]
-Original Grad: -0.030, -lr * Pred Grad: -0.034, New P: 1.370
-Original Grad: 0.015, -lr * Pred Grad: -0.001, New P: -0.299
Target params: [1.1812, 0.2779]
iter 0 loss: 0.604
Actual params: [0.5941, 0.5941]
-Original Grad: 0.082, -lr * Pred Grad: 0.064, New P: 0.658
-Original Grad: -0.444, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.494
Actual params: [0.6577, 0.5311]
-Original Grad: 0.062, -lr * Pred Grad: 0.079, New P: 0.737
-Original Grad: -0.405, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.401
Actual params: [0.7369, 0.4506]
-Original Grad: 0.051, -lr * Pred Grad: 0.045, New P: 0.781
-Original Grad: -0.294, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.339
Actual params: [0.7815, 0.367 ]
-Original Grad: 0.021, -lr * Pred Grad: 0.018, New P: 0.799
-Original Grad: -0.162, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.295
Actual params: [0.7994, 0.2828]
-Original Grad: -0.004, -lr * Pred Grad: -0.002, New P: 0.797
-Original Grad: 0.001, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.271
Actual params: [0.7972, 0.1987]
-Original Grad: -0.012, -lr * Pred Grad: -0.008, New P: 0.789
-Original Grad: 0.038, -lr * Pred Grad: -0.080, New P: 0.119
iter 6 loss: 0.260
Actual params: [0.789 , 0.1187]
-Original Grad: -0.022, -lr * Pred Grad: -0.026, New P: 0.763
-Original Grad: 0.027, -lr * Pred Grad: -0.072, New P: 0.047
iter 7 loss: 0.253
Actual params: [0.7628, 0.047 ]
-Original Grad: -0.028, -lr * Pred Grad: -0.032, New P: 0.731
-Original Grad: 0.020, -lr * Pred Grad: -0.069, New P: -0.022
iter 8 loss: 0.247
Actual params: [ 0.7311, -0.0216]
-Original Grad: -0.032, -lr * Pred Grad: -0.034, New P: 0.697
-Original Grad: 0.017, -lr * Pred Grad: -0.058, New P: -0.080
iter 9 loss: 0.243
Actual params: [ 0.697 , -0.0796]
-Original Grad: -0.029, -lr * Pred Grad: -0.033, New P: 0.664
-Original Grad: 0.018, -lr * Pred Grad: -0.040, New P: -0.120
iter 10 loss: 0.242
Actual params: [ 0.664 , -0.1197]
-Original Grad: -0.020, -lr * Pred Grad: -0.028, New P: 0.636
-Original Grad: 0.015, -lr * Pred Grad: -0.031, New P: -0.150
iter 11 loss: 0.241
Actual params: [ 0.6355, -0.1503]
-Original Grad: -0.019, -lr * Pred Grad: -0.025, New P: 0.611
-Original Grad: 0.027, -lr * Pred Grad: -0.026, New P: -0.176
iter 12 loss: 0.238
Actual params: [ 0.6109, -0.1763]
-Original Grad: -0.014, -lr * Pred Grad: -0.022, New P: 0.589
-Original Grad: 0.046, -lr * Pred Grad: -0.017, New P: -0.194
iter 13 loss: 0.236
Actual params: [ 0.5893, -0.1938]
-Original Grad: -0.006, -lr * Pred Grad: -0.017, New P: 0.572
-Original Grad: 0.048, -lr * Pred Grad: 0.005, New P: -0.189
iter 14 loss: 0.241
Actual params: [ 0.5719, -0.1889]
-Original Grad: -0.002, -lr * Pred Grad: -0.013, New P: 0.558
-Original Grad: 0.048, -lr * Pred Grad: 0.038, New P: -0.151
iter 15 loss: 0.251
Actual params: [ 0.5584, -0.1506]
-Original Grad: -0.008, -lr * Pred Grad: -0.014, New P: 0.545
-Original Grad: 0.027, -lr * Pred Grad: 0.017, New P: -0.134
iter 16 loss: 0.255
Actual params: [ 0.5446, -0.134 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.013, New P: 0.532
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: -0.133
iter 17 loss: 0.257
Actual params: [ 0.5319, -0.1333]
-Original Grad: 0.009, -lr * Pred Grad: -0.008, New P: 0.524
-Original Grad: 0.025, -lr * Pred Grad: 0.001, New P: -0.132
iter 18 loss: 0.259
Actual params: [ 0.5242, -0.1321]
-Original Grad: 0.002, -lr * Pred Grad: -0.007, New P: 0.518
-Original Grad: 0.034, -lr * Pred Grad: 0.013, New P: -0.119
iter 19 loss: 0.262
Actual params: [ 0.5176, -0.1194]
-Original Grad: -0.008, -lr * Pred Grad: -0.013, New P: 0.505
-Original Grad: 0.025, -lr * Pred Grad: 0.011, New P: -0.108
iter 20 loss: 0.266
Actual params: [ 0.5047, -0.1085]
-Original Grad: 0.016, -lr * Pred Grad: -0.008, New P: 0.497
-Original Grad: 0.022, -lr * Pred Grad: 0.006, New P: -0.102
Target params: [1.1812, 0.2779]
iter 0 loss: 0.267
Actual params: [0.5941, 0.5941]
-Original Grad: 0.167, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: 0.010, -lr * Pred Grad: 0.042, New P: 0.636
iter 1 loss: 0.235
Actual params: [0.6612, 0.636 ]
-Original Grad: 0.154, -lr * Pred Grad: 0.084, New P: 0.745
-Original Grad: 0.013, -lr * Pred Grad: -0.012, New P: 0.624
iter 2 loss: 0.235
Actual params: [0.7454, 0.624 ]
-Original Grad: 0.030, -lr * Pred Grad: 0.086, New P: 0.832
-Original Grad: -0.019, -lr * Pred Grad: -0.051, New P: 0.573
iter 3 loss: 0.243
Actual params: [0.8315, 0.5731]
-Original Grad: -0.162, -lr * Pred Grad: -0.020, New P: 0.812
-Original Grad: -0.033, -lr * Pred Grad: -0.057, New P: 0.516
iter 4 loss: 0.221
Actual params: [0.8115, 0.5165]
-Original Grad: -0.052, -lr * Pred Grad: -0.049, New P: 0.763
-Original Grad: -0.017, -lr * Pred Grad: -0.044, New P: 0.472
iter 5 loss: 0.219
Actual params: [0.763 , 0.4721]
-Original Grad: 0.094, -lr * Pred Grad: 0.003, New P: 0.766
-Original Grad: -0.011, -lr * Pred Grad: -0.022, New P: 0.450
iter 6 loss: 0.220
Actual params: [0.7655, 0.4498]
-Original Grad: 0.056, -lr * Pred Grad: 0.056, New P: 0.822
-Original Grad: 0.005, -lr * Pred Grad: -0.008, New P: 0.441
iter 7 loss: 0.213
Actual params: [0.8217, 0.4415]
-Original Grad: -0.060, -lr * Pred Grad: -0.026, New P: 0.796
-Original Grad: -0.028, -lr * Pred Grad: -0.026, New P: 0.416
iter 8 loss: 0.216
Actual params: [0.7959, 0.4159]
-Original Grad: -0.028, -lr * Pred Grad: -0.039, New P: 0.757
-Original Grad: -0.029, -lr * Pred Grad: -0.035, New P: 0.381
iter 9 loss: 0.228
Actual params: [0.7571, 0.3806]
-Original Grad: 0.121, -lr * Pred Grad: 0.021, New P: 0.779
-Original Grad: -0.028, -lr * Pred Grad: -0.039, New P: 0.342
iter 10 loss: 0.225
Actual params: [0.7785, 0.3416]
-Original Grad: 0.086, -lr * Pred Grad: 0.067, New P: 0.846
-Original Grad: -0.029, -lr * Pred Grad: -0.040, New P: 0.302
iter 11 loss: 0.210
Actual params: [0.8458, 0.3017]
-Original Grad: -0.093, -lr * Pred Grad: -0.027, New P: 0.819
-Original Grad: -0.019, -lr * Pred Grad: -0.037, New P: 0.264
iter 12 loss: 0.218
Actual params: [0.8189, 0.2642]
-Original Grad: -0.056, -lr * Pred Grad: -0.047, New P: 0.771
-Original Grad: -0.022, -lr * Pred Grad: -0.036, New P: 0.228
iter 13 loss: 0.239
Actual params: [0.7714, 0.2284]
-Original Grad: 0.077, -lr * Pred Grad: -0.006, New P: 0.766
-Original Grad: -0.014, -lr * Pred Grad: -0.033, New P: 0.195
iter 14 loss: 0.243
Actual params: [0.7658, 0.1951]
-Original Grad: 0.083, -lr * Pred Grad: 0.050, New P: 0.816
-Original Grad: -0.010, -lr * Pred Grad: -0.030, New P: 0.165
iter 15 loss: 0.223
Actual params: [0.8161, 0.1653]
-Original Grad: -0.071, -lr * Pred Grad: -0.022, New P: 0.794
-Original Grad: -0.004, -lr * Pred Grad: -0.026, New P: 0.139
iter 16 loss: 0.233
Actual params: [0.7936, 0.1392]
-Original Grad: -0.016, -lr * Pred Grad: -0.032, New P: 0.762
-Original Grad: -0.012, -lr * Pred Grad: -0.027, New P: 0.112
iter 17 loss: 0.246
Actual params: [0.7621, 0.1121]
-Original Grad: 0.025, -lr * Pred Grad: -0.003, New P: 0.759
-Original Grad: -0.023, -lr * Pred Grad: -0.033, New P: 0.079
iter 18 loss: 0.248
Actual params: [0.7593, 0.0791]
-Original Grad: 0.036, -lr * Pred Grad: 0.024, New P: 0.784
-Original Grad: -0.016, -lr * Pred Grad: -0.035, New P: 0.044
iter 19 loss: 0.238
Actual params: [0.7835, 0.0439]
-Original Grad: 0.036, -lr * Pred Grad: 0.024, New P: 0.808
-Original Grad: -0.009, -lr * Pred Grad: -0.033, New P: 0.011
iter 20 loss: 0.228
Actual params: [0.8079, 0.0114]
-Original Grad: -0.054, -lr * Pred Grad: -0.027, New P: 0.780
-Original Grad: -0.007, -lr * Pred Grad: -0.029, New P: -0.017
Target params: [1.1812, 0.2779]
iter 0 loss: 0.722
Actual params: [0.5941, 0.5941]
-Original Grad: 0.120, -lr * Pred Grad: 0.066, New P: 0.660
-Original Grad: -0.540, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.685
Actual params: [0.6602, 0.5314]
-Original Grad: 0.193, -lr * Pred Grad: 0.084, New P: 0.744
-Original Grad: -0.335, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.603
Actual params: [0.7441, 0.4509]
-Original Grad: 0.140, -lr * Pred Grad: 0.087, New P: 0.831
-Original Grad: -0.383, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.436
Actual params: [0.831 , 0.3672]
-Original Grad: 0.288, -lr * Pred Grad: 0.088, New P: 0.919
-Original Grad: -0.288, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.256
Actual params: [0.9188, 0.283 ]
-Original Grad: 0.073, -lr * Pred Grad: 0.088, New P: 1.006
-Original Grad: -0.049, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.166
Actual params: [1.0065, 0.1987]
-Original Grad: 0.057, -lr * Pred Grad: 0.086, New P: 1.092
-Original Grad: 0.030, -lr * Pred Grad: -0.083, New P: 0.115
iter 6 loss: 0.178
Actual params: [1.0923, 0.1154]
-Original Grad: 0.050, -lr * Pred Grad: 0.052, New P: 1.144
-Original Grad: 0.040, -lr * Pred Grad: -0.076, New P: 0.040
iter 7 loss: 0.187
Actual params: [1.1438, 0.0399]
-Original Grad: 0.105, -lr * Pred Grad: 0.076, New P: 1.219
-Original Grad: 0.038, -lr * Pred Grad: -0.070, New P: -0.030
iter 8 loss: 0.177
Actual params: [ 1.2194, -0.0301]
-Original Grad: 0.094, -lr * Pred Grad: 0.076, New P: 1.296
-Original Grad: 0.026, -lr * Pred Grad: -0.065, New P: -0.095
iter 9 loss: 0.169
Actual params: [ 1.2959, -0.095 ]
-Original Grad: 0.054, -lr * Pred Grad: 0.070, New P: 1.366
-Original Grad: 0.001, -lr * Pred Grad: -0.048, New P: -0.143
iter 10 loss: 0.166
Actual params: [ 1.3656, -0.1427]
-Original Grad: 0.031, -lr * Pred Grad: 0.035, New P: 1.401
-Original Grad: -0.011, -lr * Pred Grad: -0.038, New P: -0.181
iter 11 loss: 0.169
Actual params: [ 1.4009, -0.1811]
-Original Grad: 0.026, -lr * Pred Grad: 0.035, New P: 1.436
-Original Grad: -0.008, -lr * Pred Grad: -0.037, New P: -0.218
iter 12 loss: 0.168
Actual params: [ 1.4359, -0.218 ]
-Original Grad: 0.024, -lr * Pred Grad: -0.002, New P: 1.434
-Original Grad: -0.006, -lr * Pred Grad: -0.036, New P: -0.254
iter 13 loss: 0.169
Actual params: [ 1.4336, -0.254 ]
-Original Grad: 0.019, -lr * Pred Grad: 0.019, New P: 1.453
-Original Grad: -0.007, -lr * Pred Grad: -0.032, New P: -0.286
iter 14 loss: 0.168
Actual params: [ 1.4528, -0.2859]
-Original Grad: 0.006, -lr * Pred Grad: -0.019, New P: 1.433
-Original Grad: 0.010, -lr * Pred Grad: -0.016, New P: -0.301
iter 15 loss: 0.163
Actual params: [ 1.4335, -0.3015]
-Original Grad: 0.017, -lr * Pred Grad: 0.002, New P: 1.435
-Original Grad: -0.007, -lr * Pred Grad: -0.013, New P: -0.314
iter 16 loss: 0.161
Actual params: [ 1.4353, -0.3142]
-Original Grad: 0.026, -lr * Pred Grad: 0.001, New P: 1.436
-Original Grad: -0.004, -lr * Pred Grad: -0.024, New P: -0.338
iter 17 loss: 0.157
Actual params: [ 1.4364, -0.3385]
-Original Grad: 0.020, -lr * Pred Grad: 0.007, New P: 1.443
-Original Grad: 0.015, -lr * Pred Grad: -0.017, New P: -0.355
iter 18 loss: 0.154
Actual params: [ 1.443 , -0.3553]
-Original Grad: 0.027, -lr * Pred Grad: 0.008, New P: 1.451
-Original Grad: 0.016, -lr * Pred Grad: -0.008, New P: -0.364
iter 19 loss: 0.153
Actual params: [ 1.451 , -0.3637]
-Original Grad: 0.019, -lr * Pred Grad: 0.005, New P: 1.456
-Original Grad: 0.036, -lr * Pred Grad: 0.006, New P: -0.358
iter 20 loss: 0.155
Actual params: [ 1.4561, -0.3581]
-Original Grad: 0.026, -lr * Pred Grad: 0.008, New P: 1.464
-Original Grad: 0.038, -lr * Pred Grad: 0.016, New P: -0.342
Target params: [1.1812, 0.2779]
iter 0 loss: 0.260
Actual params: [0.5941, 0.5941]
-Original Grad: 0.049, -lr * Pred Grad: 0.058, New P: 0.652
-Original Grad: 0.066, -lr * Pred Grad: 0.062, New P: 0.656
iter 1 loss: 0.203
Actual params: [0.6523, 0.6557]
-Original Grad: 0.045, -lr * Pred Grad: 0.060, New P: 0.712
-Original Grad: 0.082, -lr * Pred Grad: 0.079, New P: 0.734
iter 2 loss: 0.219
Actual params: [0.7123, 0.7343]
-Original Grad: -0.056, -lr * Pred Grad: -0.040, New P: 0.672
-Original Grad: -0.006, -lr * Pred Grad: 0.002, New P: 0.736
iter 3 loss: 0.189
Actual params: [0.6723, 0.7363]
-Original Grad: -0.025, -lr * Pred Grad: -0.056, New P: 0.617
-Original Grad: 0.022, -lr * Pred Grad: 0.011, New P: 0.748
iter 4 loss: 0.186
Actual params: [0.6165, 0.7476]
-Original Grad: -0.007, -lr * Pred Grad: -0.038, New P: 0.578
-Original Grad: 0.053, -lr * Pred Grad: 0.033, New P: 0.780
iter 5 loss: 0.186
Actual params: [0.5781, 0.7802]
-Original Grad: 0.003, -lr * Pred Grad: -0.003, New P: 0.575
-Original Grad: 0.069, -lr * Pred Grad: 0.060, New P: 0.840
iter 6 loss: 0.167
Actual params: [0.5748, 0.8397]
-Original Grad: -0.041, -lr * Pred Grad: -0.027, New P: 0.548
-Original Grad: 0.027, -lr * Pred Grad: 0.022, New P: 0.862
iter 7 loss: 0.173
Actual params: [0.5477, 0.862 ]
-Original Grad: 0.014, -lr * Pred Grad: -0.011, New P: 0.537
-Original Grad: 0.024, -lr * Pred Grad: 0.029, New P: 0.891
iter 8 loss: 0.170
Actual params: [0.5369, 0.8911]
-Original Grad: -0.029, -lr * Pred Grad: -0.023, New P: 0.514
-Original Grad: 0.006, -lr * Pred Grad: -0.018, New P: 0.873
iter 9 loss: 0.195
Actual params: [0.5142, 0.8727]
-Original Grad: 0.017, -lr * Pred Grad: -0.007, New P: 0.507
-Original Grad: 0.019, -lr * Pred Grad: 0.012, New P: 0.885
iter 10 loss: 0.196
Actual params: [0.5068, 0.885 ]
-Original Grad: 0.030, -lr * Pred Grad: 0.011, New P: 0.518
-Original Grad: 0.010, -lr * Pred Grad: -0.009, New P: 0.876
iter 11 loss: 0.191
Actual params: [0.518 , 0.8764]
-Original Grad: 0.012, -lr * Pred Grad: 0.005, New P: 0.523
-Original Grad: 0.017, -lr * Pred Grad: 0.006, New P: 0.882
iter 12 loss: 0.185
Actual params: [0.523 , 0.8824]
-Original Grad: 0.047, -lr * Pred Grad: 0.022, New P: 0.545
-Original Grad: 0.002, -lr * Pred Grad: -0.010, New P: 0.872
iter 13 loss: 0.172
Actual params: [0.5447, 0.872 ]
-Original Grad: -0.023, -lr * Pred Grad: -0.012, New P: 0.533
-Original Grad: 0.014, -lr * Pred Grad: -0.003, New P: 0.869
iter 14 loss: 0.179
Actual params: [0.5328, 0.8692]
-Original Grad: 0.080, -lr * Pred Grad: 0.024, New P: 0.557
-Original Grad: 0.005, -lr * Pred Grad: -0.007, New P: 0.863
iter 15 loss: 0.169
Actual params: [0.5566, 0.8626]
-Original Grad: -0.051, -lr * Pred Grad: -0.017, New P: 0.540
-Original Grad: 0.021, -lr * Pred Grad: 0.001, New P: 0.863
iter 16 loss: 0.175
Actual params: [0.5401, 0.8633]
-Original Grad: -0.086, -lr * Pred Grad: -0.041, New P: 0.499
-Original Grad: 0.024, -lr * Pred Grad: 0.007, New P: 0.871
iter 17 loss: 0.210
Actual params: [0.4989, 0.8705]
-Original Grad: 0.066, -lr * Pred Grad: -0.022, New P: 0.477
-Original Grad: 0.015, -lr * Pred Grad: 0.004, New P: 0.875
iter 18 loss: 0.230
Actual params: [0.4769, 0.8746]
-Original Grad: 0.055, -lr * Pred Grad: 0.020, New P: 0.497
-Original Grad: 0.031, -lr * Pred Grad: 0.009, New P: 0.884
iter 19 loss: 0.209
Actual params: [0.4968, 0.884 ]
-Original Grad: 0.020, -lr * Pred Grad: 0.033, New P: 0.530
-Original Grad: 0.005, -lr * Pred Grad: 0.001, New P: 0.885
iter 20 loss: 0.177
Actual params: [0.53  , 0.8853]
-Original Grad: 0.018, -lr * Pred Grad: 0.005, New P: 0.535
-Original Grad: 0.003, -lr * Pred Grad: -0.007, New P: 0.879
Target params: [1.1812, 0.2779]
iter 0 loss: 0.358
Actual params: [0.5941, 0.5941]
-Original Grad: 0.335, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: 0.002, -lr * Pred Grad: 0.036, New P: 0.631
iter 1 loss: 0.287
Actual params: [0.6617, 0.6305]
-Original Grad: 0.225, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: 0.016, -lr * Pred Grad: -0.016, New P: 0.614
iter 2 loss: 0.231
Actual params: [0.7467, 0.6142]
-Original Grad: -0.032, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.033, -lr * Pred Grad: -0.055, New P: 0.559
iter 3 loss: 0.214
Actual params: [0.8342, 0.5594]
-Original Grad: -0.065, -lr * Pred Grad: 0.087, New P: 0.921
-Original Grad: -0.063, -lr * Pred Grad: -0.062, New P: 0.497
iter 4 loss: 0.210
Actual params: [0.9214, 0.497 ]
-Original Grad: 0.011, -lr * Pred Grad: 0.076, New P: 0.997
-Original Grad: -0.064, -lr * Pred Grad: -0.062, New P: 0.435
iter 5 loss: 0.216
Actual params: [0.9975, 0.4346]
-Original Grad: 0.013, -lr * Pred Grad: 0.004, New P: 1.002
-Original Grad: -0.030, -lr * Pred Grad: -0.058, New P: 0.377
iter 6 loss: 0.220
Actual params: [1.0019, 0.377 ]
-Original Grad: 0.009, -lr * Pred Grad: 0.051, New P: 1.053
-Original Grad: -0.016, -lr * Pred Grad: -0.045, New P: 0.331
iter 7 loss: 0.224
Actual params: [1.0525, 0.3315]
-Original Grad: 0.006, -lr * Pred Grad: 0.029, New P: 1.081
-Original Grad: -0.003, -lr * Pred Grad: -0.025, New P: 0.306
iter 8 loss: 0.230
Actual params: [1.0815, 0.3061]
-Original Grad: 0.004, -lr * Pred Grad: 0.024, New P: 1.105
-Original Grad: -0.002, -lr * Pred Grad: -0.013, New P: 0.293
iter 9 loss: 0.235
Actual params: [1.105 , 0.2934]
-Original Grad: 0.000, -lr * Pred Grad: -0.016, New P: 1.089
-Original Grad: -0.001, -lr * Pred Grad: -0.014, New P: 0.279
iter 10 loss: 0.233
Actual params: [1.0891, 0.2794]
-Original Grad: -0.006, -lr * Pred Grad: -0.014, New P: 1.075
-Original Grad: 0.001, -lr * Pred Grad: -0.016, New P: 0.263
iter 11 loss: 0.231
Actual params: [1.0749, 0.2632]
-Original Grad: 0.008, -lr * Pred Grad: -0.014, New P: 1.061
-Original Grad: -0.005, -lr * Pred Grad: -0.020, New P: 0.243
iter 12 loss: 0.231
Actual params: [1.061 , 0.2429]
-Original Grad: 0.002, -lr * Pred Grad: -0.011, New P: 1.050
-Original Grad: -0.008, -lr * Pred Grad: -0.024, New P: 0.219
iter 13 loss: 0.231
Actual params: [1.0503, 0.2185]
-Original Grad: 0.000, -lr * Pred Grad: -0.013, New P: 1.038
-Original Grad: 0.000, -lr * Pred Grad: -0.024, New P: 0.194
iter 14 loss: 0.231
Actual params: [1.0376, 0.1944]
-Original Grad: 0.005, -lr * Pred Grad: -0.011, New P: 1.027
-Original Grad: -0.001, -lr * Pred Grad: -0.022, New P: 0.172
iter 15 loss: 0.232
Actual params: [1.0268, 0.1721]
-Original Grad: 0.006, -lr * Pred Grad: -0.009, New P: 1.018
-Original Grad: -0.000, -lr * Pred Grad: -0.021, New P: 0.151
iter 16 loss: 0.232
Actual params: [1.0179, 0.151 ]
-Original Grad: 0.002, -lr * Pred Grad: -0.009, New P: 1.008
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: 0.131
iter 17 loss: 0.233
Actual params: [1.0084, 0.1308]
-Original Grad: 0.006, -lr * Pred Grad: -0.008, New P: 1.000
-Original Grad: -0.003, -lr * Pred Grad: -0.022, New P: 0.109
iter 18 loss: 0.234
Actual params: [1.0002, 0.1092]
-Original Grad: 0.004, -lr * Pred Grad: -0.008, New P: 0.992
-Original Grad: -0.000, -lr * Pred Grad: -0.022, New P: 0.087
iter 19 loss: 0.235
Actual params: [0.9923, 0.0867]
-Original Grad: 0.001, -lr * Pred Grad: -0.009, New P: 0.983
-Original Grad: 0.000, -lr * Pred Grad: -0.022, New P: 0.065
iter 20 loss: 0.236
Actual params: [0.9829, 0.0645]
-Original Grad: 0.004, -lr * Pred Grad: -0.008, New P: 0.975
-Original Grad: -0.004, -lr * Pred Grad: -0.024, New P: 0.041
Target params: [1.1812, 0.2779]
iter 0 loss: 0.654
Actual params: [0.5941, 0.5941]
-Original Grad: 0.164, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.282, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.488
Actual params: [0.6612, 0.5309]
-Original Grad: 0.241, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.164, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.466
Actual params: [0.7457, 0.4503]
-Original Grad: 0.190, -lr * Pred Grad: 0.088, New P: 0.833
-Original Grad: -0.051, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.446
Actual params: [0.8332, 0.367 ]
-Original Grad: 0.050, -lr * Pred Grad: 0.088, New P: 0.921
-Original Grad: 0.027, -lr * Pred Grad: -0.079, New P: 0.288
iter 4 loss: 0.423
Actual params: [0.921 , 0.2877]
-Original Grad: 0.014, -lr * Pred Grad: 0.086, New P: 1.007
-Original Grad: 0.038, -lr * Pred Grad: -0.071, New P: 0.216
iter 5 loss: 0.414
Actual params: [1.0066, 0.2165]
-Original Grad: -0.016, -lr * Pred Grad: 0.004, New P: 1.011
-Original Grad: 0.026, -lr * Pred Grad: -0.067, New P: 0.149
iter 6 loss: 0.422
Actual params: [1.0109, 0.1492]
-Original Grad: -0.022, -lr * Pred Grad: 0.009, New P: 1.020
-Original Grad: 0.034, -lr * Pred Grad: -0.052, New P: 0.097
iter 7 loss: 0.427
Actual params: [1.0203, 0.0975]
-Original Grad: -0.011, -lr * Pred Grad: -0.005, New P: 1.015
-Original Grad: 0.035, -lr * Pred Grad: -0.033, New P: 0.064
iter 8 loss: 0.432
Actual params: [1.0148, 0.0644]
-Original Grad: -0.020, -lr * Pred Grad: -0.011, New P: 1.004
-Original Grad: 0.079, -lr * Pred Grad: -0.016, New P: 0.049
iter 9 loss: 0.434
Actual params: [1.0039, 0.0486]
-Original Grad: -0.015, -lr * Pred Grad: -0.027, New P: 0.977
-Original Grad: 0.077, -lr * Pred Grad: 0.016, New P: 0.065
iter 10 loss: 0.439
Actual params: [0.9768, 0.0647]
-Original Grad: -0.012, -lr * Pred Grad: -0.026, New P: 0.950
-Original Grad: 0.063, -lr * Pred Grad: 0.056, New P: 0.120
iter 11 loss: 0.440
Actual params: [0.9503, 0.1203]
-Original Grad: -0.006, -lr * Pred Grad: -0.021, New P: 0.930
-Original Grad: 0.049, -lr * Pred Grad: 0.048, New P: 0.168
iter 12 loss: 0.438
Actual params: [0.9295, 0.1681]
-Original Grad: 0.002, -lr * Pred Grad: -0.013, New P: 0.916
-Original Grad: 0.060, -lr * Pred Grad: 0.052, New P: 0.220
iter 13 loss: 0.435
Actual params: [0.9162, 0.2199]
-Original Grad: -0.006, -lr * Pred Grad: -0.014, New P: 0.902
-Original Grad: 0.067, -lr * Pred Grad: 0.053, New P: 0.273
iter 14 loss: 0.433
Actual params: [0.9023, 0.273 ]
-Original Grad: 0.007, -lr * Pred Grad: -0.010, New P: 0.892
-Original Grad: 0.054, -lr * Pred Grad: 0.049, New P: 0.322
iter 15 loss: 0.426
Actual params: [0.8919, 0.3225]
-Original Grad: 0.038, -lr * Pred Grad: 0.008, New P: 0.899
-Original Grad: 0.034, -lr * Pred Grad: 0.027, New P: 0.350
iter 16 loss: 0.417
Actual params: [0.8994, 0.3498]
-Original Grad: -0.009, -lr * Pred Grad: -0.003, New P: 0.896
-Original Grad: 0.043, -lr * Pred Grad: 0.035, New P: 0.385
iter 17 loss: 0.406
Actual params: [0.896 , 0.3847]
-Original Grad: 0.001, -lr * Pred Grad: -0.014, New P: 0.882
-Original Grad: 0.024, -lr * Pred Grad: 0.004, New P: 0.388
iter 18 loss: 0.411
Actual params: [0.8824, 0.3884]
-Original Grad: 0.044, -lr * Pred Grad: 0.006, New P: 0.889
-Original Grad: 0.028, -lr * Pred Grad: 0.022, New P: 0.410
iter 19 loss: 0.400
Actual params: [0.8888, 0.4105]
-Original Grad: 0.022, -lr * Pred Grad: 0.015, New P: 0.904
-Original Grad: 0.003, -lr * Pred Grad: -0.015, New P: 0.395
iter 20 loss: 0.400
Actual params: [0.9038, 0.3951]
-Original Grad: -0.011, -lr * Pred Grad: -0.008, New P: 0.896
-Original Grad: 0.007, -lr * Pred Grad: -0.004, New P: 0.391
Target params: [1.1812, 0.2779]
iter 0 loss: 0.710
Actual params: [0.5941, 0.5941]
-Original Grad: 0.432, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.270, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.636
Actual params: [0.6617, 0.531 ]
-Original Grad: 0.172, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.116, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.575
Actual params: [0.7467, 0.4504]
-Original Grad: 0.014, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.045, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.531
Actual params: [0.8344, 0.3674]
-Original Grad: 0.012, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: -0.013, -lr * Pred Grad: -0.078, New P: 0.289
iter 4 loss: 0.515
Actual params: [0.9224, 0.289 ]
-Original Grad: -0.000, -lr * Pred Grad: 0.087, New P: 1.010
-Original Grad: 0.000, -lr * Pred Grad: -0.071, New P: 0.218
iter 5 loss: 0.504
Actual params: [1.0099, 0.218 ]
-Original Grad: 0.002, -lr * Pred Grad: 0.083, New P: 1.093
-Original Grad: 0.031, -lr * Pred Grad: -0.068, New P: 0.150
iter 6 loss: 0.503
Actual params: [1.0929, 0.1504]
-Original Grad: 0.007, -lr * Pred Grad: 0.013, New P: 1.106
-Original Grad: 0.043, -lr * Pred Grad: -0.053, New P: 0.098
iter 7 loss: 0.501
Actual params: [1.106 , 0.0976]
-Original Grad: 0.011, -lr * Pred Grad: 0.055, New P: 1.161
-Original Grad: 0.046, -lr * Pred Grad: -0.034, New P: 0.063
iter 8 loss: 0.503
Actual params: [1.1608, 0.0634]
-Original Grad: 0.022, -lr * Pred Grad: 0.049, New P: 1.210
-Original Grad: 0.065, -lr * Pred Grad: -0.021, New P: 0.043
iter 9 loss: 0.505
Actual params: [1.21  , 0.0427]
-Original Grad: 0.003, -lr * Pred Grad: 0.041, New P: 1.251
-Original Grad: 0.054, -lr * Pred Grad: 0.002, New P: 0.045
iter 10 loss: 0.507
Actual params: [1.2513, 0.0446]
-Original Grad: 0.001, -lr * Pred Grad: -0.003, New P: 1.248
-Original Grad: 0.074, -lr * Pred Grad: 0.043, New P: 0.087
iter 11 loss: 0.511
Actual params: [1.2484, 0.0875]
-Original Grad: 0.001, -lr * Pred Grad: -0.003, New P: 1.246
-Original Grad: 0.045, -lr * Pred Grad: 0.053, New P: 0.141
iter 12 loss: 0.516
Actual params: [1.2459, 0.1407]
-Original Grad: -0.008, -lr * Pred Grad: -0.024, New P: 1.222
-Original Grad: 0.026, -lr * Pred Grad: 0.011, New P: 0.152
iter 13 loss: 0.515
Actual params: [1.2218, 0.1517]
-Original Grad: 0.012, -lr * Pred Grad: -0.008, New P: 1.214
-Original Grad: 0.044, -lr * Pred Grad: 0.043, New P: 0.195
iter 14 loss: 0.520
Actual params: [1.2141, 0.1946]
-Original Grad: 0.008, -lr * Pred Grad: -0.007, New P: 1.207
-Original Grad: 0.037, -lr * Pred Grad: 0.007, New P: 0.202
iter 15 loss: 0.520
Actual params: [1.2068, 0.2019]
-Original Grad: -0.001, -lr * Pred Grad: -0.011, New P: 1.195
-Original Grad: 0.018, -lr * Pred Grad: 0.025, New P: 0.227
iter 16 loss: 0.518
Actual params: [1.1954, 0.2269]
-Original Grad: 0.011, -lr * Pred Grad: -0.008, New P: 1.187
-Original Grad: 0.026, -lr * Pred Grad: -0.004, New P: 0.223
iter 17 loss: 0.522
Actual params: [1.1874, 0.223 ]
-Original Grad: 0.008, -lr * Pred Grad: -0.006, New P: 1.181
-Original Grad: 0.023, -lr * Pred Grad: 0.017, New P: 0.240
iter 18 loss: 0.518
Actual params: [1.1814, 0.2398]
-Original Grad: -0.000, -lr * Pred Grad: -0.010, New P: 1.172
-Original Grad: -0.005, -lr * Pred Grad: -0.017, New P: 0.222
iter 19 loss: 0.521
Actual params: [1.1716, 0.2224]
-Original Grad: 0.004, -lr * Pred Grad: -0.010, New P: 1.162
-Original Grad: 0.015, -lr * Pred Grad: -0.001, New P: 0.221
iter 20 loss: 0.520
Actual params: [1.1615, 0.2215]
-Original Grad: -0.001, -lr * Pred Grad: -0.011, New P: 1.151
-Original Grad: 0.010, -lr * Pred Grad: -0.005, New P: 0.216
Target params: [1.1812, 0.2779]
iter 0 loss: 0.383
Actual params: [0.5941, 0.5941]
-Original Grad: 0.301, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.263, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.339
Actual params: [0.6617, 0.531 ]
-Original Grad: 0.028, -lr * Pred Grad: 0.084, New P: 0.746
-Original Grad: -0.097, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.325
Actual params: [0.7461, 0.4504]
-Original Grad: -0.007, -lr * Pred Grad: 0.086, New P: 0.832
-Original Grad: -0.054, -lr * Pred Grad: -0.083, New P: 0.368
iter 3 loss: 0.323
Actual params: [0.8321, 0.3676]
-Original Grad: -0.012, -lr * Pred Grad: 0.048, New P: 0.880
-Original Grad: -0.007, -lr * Pred Grad: -0.077, New P: 0.290
iter 4 loss: 0.324
Actual params: [0.8798, 0.2901]
-Original Grad: 0.002, -lr * Pred Grad: 0.001, New P: 0.881
-Original Grad: -0.001, -lr * Pred Grad: -0.071, New P: 0.219
iter 5 loss: 0.324
Actual params: [0.8806, 0.2195]
-Original Grad: -0.002, -lr * Pred Grad: 0.034, New P: 0.915
-Original Grad: 0.010, -lr * Pred Grad: -0.067, New P: 0.152
iter 6 loss: 0.327
Actual params: [0.915 , 0.1521]
-Original Grad: 0.001, -lr * Pred Grad: -0.005, New P: 0.910
-Original Grad: 0.010, -lr * Pred Grad: -0.056, New P: 0.096
iter 7 loss: 0.327
Actual params: [0.9105, 0.0963]
-Original Grad: -0.002, -lr * Pred Grad: 0.004, New P: 0.914
-Original Grad: 0.009, -lr * Pred Grad: -0.039, New P: 0.057
iter 8 loss: 0.328
Actual params: [0.9141, 0.0573]
-Original Grad: -0.006, -lr * Pred Grad: -0.026, New P: 0.888
-Original Grad: 0.022, -lr * Pred Grad: -0.027, New P: 0.030
iter 9 loss: 0.326
Actual params: [0.8883, 0.0304]
-Original Grad: 0.002, -lr * Pred Grad: -0.015, New P: 0.874
-Original Grad: 0.017, -lr * Pred Grad: -0.012, New P: 0.019
iter 10 loss: 0.325
Actual params: [0.8735, 0.0185]
-Original Grad: -0.006, -lr * Pred Grad: -0.017, New P: 0.857
-Original Grad: 0.012, -lr * Pred Grad: 0.007, New P: 0.026
iter 11 loss: 0.323
Actual params: [0.8566, 0.0259]
-Original Grad: -0.003, -lr * Pred Grad: -0.016, New P: 0.841
-Original Grad: 0.016, -lr * Pred Grad: -0.005, New P: 0.021
iter 12 loss: 0.323
Actual params: [0.8409, 0.0207]
-Original Grad: 0.002, -lr * Pred Grad: -0.013, New P: 0.828
-Original Grad: 0.021, -lr * Pred Grad: -0.001, New P: 0.020
iter 13 loss: 0.322
Actual params: [0.8283, 0.0198]
-Original Grad: -0.005, -lr * Pred Grad: -0.014, New P: 0.814
-Original Grad: 0.011, -lr * Pred Grad: -0.006, New P: 0.013
iter 14 loss: 0.322
Actual params: [0.8141, 0.0134]
-Original Grad: -0.001, -lr * Pred Grad: -0.014, New P: 0.800
-Original Grad: 0.008, -lr * Pred Grad: -0.012, New P: 0.002
iter 15 loss: 0.322
Actual params: [0.8002, 0.0016]
-Original Grad: -0.004, -lr * Pred Grad: -0.014, New P: 0.786
-Original Grad: 0.028, -lr * Pred Grad: -0.005, New P: -0.003
iter 16 loss: 0.322
Actual params: [ 0.7862, -0.0029]
-Original Grad: 0.002, -lr * Pred Grad: -0.012, New P: 0.774
-Original Grad: 0.021, -lr * Pred Grad: 0.000, New P: -0.003
iter 17 loss: 0.322
Actual params: [ 0.7744, -0.0027]
-Original Grad: 0.002, -lr * Pred Grad: -0.009, New P: 0.765
-Original Grad: 0.045, -lr * Pred Grad: 0.013, New P: 0.010
iter 18 loss: 0.323
Actual params: [0.765 , 0.0099]
-Original Grad: 0.004, -lr * Pred Grad: -0.008, New P: 0.757
-Original Grad: 0.009, -lr * Pred Grad: 0.002, New P: 0.012
iter 19 loss: 0.323
Actual params: [0.7574, 0.0118]
-Original Grad: 0.005, -lr * Pred Grad: -0.006, New P: 0.751
-Original Grad: 0.019, -lr * Pred Grad: -0.008, New P: 0.004
iter 20 loss: 0.324
Actual params: [0.751 , 0.0043]
-Original Grad: 0.011, -lr * Pred Grad: -0.003, New P: 0.748
-Original Grad: 0.024, -lr * Pred Grad: -0.007, New P: -0.003
Target params: [1.1812, 0.2779]
iter 0 loss: 0.494
Actual params: [0.5941, 0.5941]
-Original Grad: 0.133, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.530, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.394
Actual params: [0.6606, 0.5314]
-Original Grad: 0.084, -lr * Pred Grad: 0.083, New P: 0.744
-Original Grad: -0.281, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.253
Actual params: [0.7438, 0.4508]
-Original Grad: 0.076, -lr * Pred Grad: 0.084, New P: 0.827
-Original Grad: -0.142, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.167
Actual params: [0.8274, 0.3672]
-Original Grad: -0.013, -lr * Pred Grad: 0.012, New P: 0.840
-Original Grad: -0.007, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.167
Actual params: [0.8397, 0.2833]
-Original Grad: -0.031, -lr * Pred Grad: -0.002, New P: 0.837
-Original Grad: 0.021, -lr * Pred Grad: -0.080, New P: 0.203
iter 5 loss: 0.174
Actual params: [0.8375, 0.203 ]
-Original Grad: -0.046, -lr * Pred Grad: -0.036, New P: 0.801
-Original Grad: 0.030, -lr * Pred Grad: -0.072, New P: 0.131
iter 6 loss: 0.170
Actual params: [0.8011, 0.1312]
-Original Grad: -0.057, -lr * Pred Grad: -0.043, New P: 0.759
-Original Grad: 0.025, -lr * Pred Grad: -0.068, New P: 0.063
iter 7 loss: 0.168
Actual params: [0.7586, 0.0627]
-Original Grad: -0.048, -lr * Pred Grad: -0.045, New P: 0.713
-Original Grad: 0.023, -lr * Pred Grad: -0.057, New P: 0.005
iter 8 loss: 0.165
Actual params: [0.7131, 0.0054]
-Original Grad: -0.023, -lr * Pred Grad: -0.038, New P: 0.675
-Original Grad: 0.017, -lr * Pred Grad: -0.039, New P: -0.034
iter 9 loss: 0.166
Actual params: [ 0.6751, -0.0339]
-Original Grad: -0.026, -lr * Pred Grad: -0.029, New P: 0.646
-Original Grad: 0.019, -lr * Pred Grad: -0.029, New P: -0.063
iter 10 loss: 0.162
Actual params: [ 0.6456, -0.0628]
-Original Grad: -0.026, -lr * Pred Grad: -0.027, New P: 0.619
-Original Grad: 0.018, -lr * Pred Grad: -0.022, New P: -0.085
iter 11 loss: 0.160
Actual params: [ 0.6189, -0.0853]
-Original Grad: 0.000, -lr * Pred Grad: -0.017, New P: 0.602
-Original Grad: 0.016, -lr * Pred Grad: -0.007, New P: -0.092
iter 12 loss: 0.159
Actual params: [ 0.6018, -0.0924]
-Original Grad: -0.005, -lr * Pred Grad: -0.012, New P: 0.590
-Original Grad: 0.018, -lr * Pred Grad: 0.010, New P: -0.083
iter 13 loss: 0.159
Actual params: [ 0.5901, -0.0827]
-Original Grad: -0.002, -lr * Pred Grad: -0.011, New P: 0.579
-Original Grad: 0.020, -lr * Pred Grad: -0.005, New P: -0.088
iter 14 loss: 0.159
Actual params: [ 0.579 , -0.0879]
-Original Grad: -0.000, -lr * Pred Grad: -0.011, New P: 0.568
-Original Grad: 0.019, -lr * Pred Grad: 0.001, New P: -0.087
iter 15 loss: 0.159
Actual params: [ 0.5677, -0.0867]
-Original Grad: -0.001, -lr * Pred Grad: -0.012, New P: 0.556
-Original Grad: 0.016, -lr * Pred Grad: -0.005, New P: -0.092
iter 16 loss: 0.159
Actual params: [ 0.5558, -0.0917]
-Original Grad: 0.002, -lr * Pred Grad: -0.011, New P: 0.545
-Original Grad: 0.017, -lr * Pred Grad: -0.005, New P: -0.097
iter 17 loss: 0.159
Actual params: [ 0.5447, -0.0971]
-Original Grad: -0.013, -lr * Pred Grad: -0.016, New P: 0.528
-Original Grad: 0.014, -lr * Pred Grad: -0.008, New P: -0.105
iter 18 loss: 0.160
Actual params: [ 0.5284, -0.1051]
-Original Grad: 0.006, -lr * Pred Grad: -0.014, New P: 0.515
-Original Grad: 0.015, -lr * Pred Grad: -0.009, New P: -0.114
iter 19 loss: 0.159
Actual params: [ 0.5147, -0.1142]
-Original Grad: -0.005, -lr * Pred Grad: -0.012, New P: 0.502
-Original Grad: 0.018, -lr * Pred Grad: -0.008, New P: -0.123
iter 20 loss: 0.156
Actual params: [ 0.5022, -0.1226]
-Original Grad: 0.002, -lr * Pred Grad: -0.011, New P: 0.491
-Original Grad: 0.020, -lr * Pred Grad: -0.006, New P: -0.129
Target params: [1.1812, 0.2779]
iter 0 loss: 0.552
Actual params: [0.5941, 0.5941]
-Original Grad: 0.501, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.178, -lr * Pred Grad: -0.061, New P: 0.533
iter 1 loss: 0.488
Actual params: [0.6617, 0.5329]
-Original Grad: 0.323, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.099, -lr * Pred Grad: -0.080, New P: 0.453
iter 2 loss: 0.387
Actual params: [0.7468, 0.453 ]
-Original Grad: 0.173, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.053, -lr * Pred Grad: -0.081, New P: 0.372
iter 3 loss: 0.275
Actual params: [0.8347, 0.372 ]
-Original Grad: 0.325, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.118, -lr * Pred Grad: -0.072, New P: 0.300
iter 4 loss: 0.180
Actual params: [0.9229, 0.2997]
-Original Grad: 0.285, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.154, -lr * Pred Grad: -0.064, New P: 0.236
iter 5 loss: 0.158
Actual params: [1.0113, 0.2357]
-Original Grad: 0.221, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: 0.114, -lr * Pred Grad: -0.039, New P: 0.197
iter 6 loss: 0.156
Actual params: [1.0996, 0.1971]
-Original Grad: 0.492, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.309, -lr * Pred Grad: 0.004, New P: 0.201
iter 7 loss: 0.152
Actual params: [1.1879, 0.2008]
-Original Grad: 0.016, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.025, -lr * Pred Grad: 0.057, New P: 0.258
iter 8 loss: 0.166
Actual params: [1.2762, 0.2578]
-Original Grad: -0.037, -lr * Pred Grad: 0.088, New P: 1.365
-Original Grad: 0.006, -lr * Pred Grad: 0.075, New P: 0.333
iter 9 loss: 0.209
Actual params: [1.3645, 0.3332]
-Original Grad: -0.067, -lr * Pred Grad: 0.088, New P: 1.453
-Original Grad: -0.036, -lr * Pred Grad: -0.025, New P: 0.309
iter 10 loss: 0.234
Actual params: [1.4527, 0.3086]
-Original Grad: -0.105, -lr * Pred Grad: 0.087, New P: 1.540
-Original Grad: 0.055, -lr * Pred Grad: 0.033, New P: 0.342
iter 11 loss: 0.285
Actual params: [1.5396, 0.3415]
-Original Grad: -0.118, -lr * Pred Grad: 0.072, New P: 1.612
-Original Grad: 0.005, -lr * Pred Grad: 0.010, New P: 0.351
iter 12 loss: 0.324
Actual params: [1.6119, 0.3513]
-Original Grad: -0.085, -lr * Pred Grad: 0.031, New P: 1.643
-Original Grad: -0.034, -lr * Pred Grad: -0.030, New P: 0.322
iter 13 loss: 0.323
Actual params: [1.6431, 0.3216]
-Original Grad: -0.077, -lr * Pred Grad: -0.009, New P: 1.634
-Original Grad: 0.050, -lr * Pred Grad: 0.007, New P: 0.329
iter 14 loss: 0.323
Actual params: [1.6342, 0.329 ]
-Original Grad: -0.078, -lr * Pred Grad: -0.025, New P: 1.610
-Original Grad: -0.039, -lr * Pred Grad: -0.028, New P: 0.301
iter 15 loss: 0.299
Actual params: [1.6096, 0.301 ]
-Original Grad: -0.074, -lr * Pred Grad: -0.005, New P: 1.605
-Original Grad: 0.034, -lr * Pred Grad: 0.003, New P: 0.304
iter 16 loss: 0.299
Actual params: [1.605, 0.304]
-Original Grad: -0.078, -lr * Pred Grad: -0.030, New P: 1.575
-Original Grad: 0.015, -lr * Pred Grad: 0.005, New P: 0.309
iter 17 loss: 0.291
Actual params: [1.5747, 0.3086]
-Original Grad: -0.114, -lr * Pred Grad: -0.050, New P: 1.524
-Original Grad: 0.035, -lr * Pred Grad: 0.018, New P: 0.326
iter 18 loss: 0.272
Actual params: [1.5243, 0.3262]
-Original Grad: -0.091, -lr * Pred Grad: -0.055, New P: 1.469
-Original Grad: -0.078, -lr * Pred Grad: -0.033, New P: 0.294
iter 19 loss: 0.234
Actual params: [1.4688, 0.2935]
-Original Grad: -0.120, -lr * Pred Grad: -0.057, New P: 1.411
-Original Grad: 0.080, -lr * Pred Grad: 0.004, New P: 0.298
iter 20 loss: 0.215
Actual params: [1.4113, 0.2977]
-Original Grad: -0.096, -lr * Pred Grad: -0.058, New P: 1.353
-Original Grad: 0.009, -lr * Pred Grad: 0.019, New P: 0.317
Target params: [1.1812, 0.2779]
iter 0 loss: 0.339
Actual params: [0.5941, 0.5941]
-Original Grad: 0.083, -lr * Pred Grad: 0.064, New P: 0.658
-Original Grad: -0.196, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.248
Actual params: [0.6578, 0.5321]
-Original Grad: 0.089, -lr * Pred Grad: 0.081, New P: 0.739
-Original Grad: -0.146, -lr * Pred Grad: -0.080, New P: 0.452
iter 2 loss: 0.161
Actual params: [0.7389, 0.4518]
-Original Grad: 0.028, -lr * Pred Grad: 0.052, New P: 0.791
-Original Grad: -0.022, -lr * Pred Grad: -0.082, New P: 0.370
iter 3 loss: 0.105
Actual params: [0.7905, 0.3697]
-Original Grad: 0.031, -lr * Pred Grad: 0.015, New P: 0.805
-Original Grad: -0.005, -lr * Pred Grad: -0.075, New P: 0.295
iter 4 loss: 0.099
Actual params: [0.8051, 0.2947]
-Original Grad: 0.011, -lr * Pred Grad: 0.023, New P: 0.828
-Original Grad: 0.017, -lr * Pred Grad: -0.070, New P: 0.225
iter 5 loss: 0.107
Actual params: [0.8281, 0.225 ]
-Original Grad: 0.010, -lr * Pred Grad: 0.001, New P: 0.829
-Original Grad: 0.028, -lr * Pred Grad: -0.064, New P: 0.161
iter 6 loss: 0.119
Actual params: [0.829 , 0.1615]
-Original Grad: 0.007, -lr * Pred Grad: 0.004, New P: 0.833
-Original Grad: 0.057, -lr * Pred Grad: -0.043, New P: 0.118
iter 7 loss: 0.127
Actual params: [0.8334, 0.1182]
-Original Grad: 0.009, -lr * Pred Grad: -0.012, New P: 0.821
-Original Grad: 0.047, -lr * Pred Grad: -0.026, New P: 0.092
iter 8 loss: 0.129
Actual params: [0.8214, 0.0923]
-Original Grad: 0.020, -lr * Pred Grad: 0.002, New P: 0.823
-Original Grad: 0.065, -lr * Pred Grad: -0.004, New P: 0.088
iter 9 loss: 0.130
Actual params: [0.8233, 0.0881]
-Original Grad: -0.003, -lr * Pred Grad: -0.013, New P: 0.811
-Original Grad: 0.037, -lr * Pred Grad: 0.034, New P: 0.122
iter 10 loss: 0.122
Actual params: [0.8106, 0.1219]
-Original Grad: 0.008, -lr * Pred Grad: -0.010, New P: 0.801
-Original Grad: 0.040, -lr * Pred Grad: 0.034, New P: 0.156
iter 11 loss: 0.116
Actual params: [0.8011, 0.1555]
-Original Grad: 0.014, -lr * Pred Grad: -0.004, New P: 0.797
-Original Grad: 0.072, -lr * Pred Grad: 0.048, New P: 0.203
iter 12 loss: 0.108
Actual params: [0.7968, 0.2034]
-Original Grad: 0.015, -lr * Pred Grad: -0.000, New P: 0.796
-Original Grad: 0.033, -lr * Pred Grad: 0.031, New P: 0.234
iter 13 loss: 0.104
Actual params: [0.7965, 0.2339]
-Original Grad: 0.018, -lr * Pred Grad: 0.002, New P: 0.799
-Original Grad: 0.019, -lr * Pred Grad: 0.016, New P: 0.249
iter 14 loss: 0.103
Actual params: [0.7988, 0.2495]
-Original Grad: 0.019, -lr * Pred Grad: 0.004, New P: 0.803
-Original Grad: 0.015, -lr * Pred Grad: -0.002, New P: 0.248
iter 15 loss: 0.103
Actual params: [0.8031, 0.2475]
-Original Grad: 0.009, -lr * Pred Grad: -0.001, New P: 0.802
-Original Grad: 0.021, -lr * Pred Grad: 0.006, New P: 0.253
iter 16 loss: 0.102
Actual params: [0.8024, 0.2532]
-Original Grad: 0.017, -lr * Pred Grad: 0.001, New P: 0.803
-Original Grad: 0.020, -lr * Pred Grad: 0.003, New P: 0.256
iter 17 loss: 0.102
Actual params: [0.803, 0.256]
-Original Grad: 0.025, -lr * Pred Grad: 0.008, New P: 0.810
-Original Grad: 0.013, -lr * Pred Grad: 0.001, New P: 0.257
iter 18 loss: 0.102
Actual params: [0.8105, 0.2565]
-Original Grad: 0.004, -lr * Pred Grad: 0.001, New P: 0.811
-Original Grad: 0.021, -lr * Pred Grad: 0.002, New P: 0.259
iter 19 loss: 0.102
Actual params: [0.8113, 0.2588]
-Original Grad: 0.016, -lr * Pred Grad: -0.000, New P: 0.811
-Original Grad: 0.019, -lr * Pred Grad: 0.004, New P: 0.263
iter 20 loss: 0.102
Actual params: [0.811 , 0.2626]
-Original Grad: 0.016, -lr * Pred Grad: 0.002, New P: 0.813
-Original Grad: 0.022, -lr * Pred Grad: 0.006, New P: 0.268
Target params: [1.1812, 0.2779]
iter 0 loss: 0.445
Actual params: [0.5941, 0.5941]
-Original Grad: -0.099, -lr * Pred Grad: -0.048, New P: 0.546
-Original Grad: -1.031, -lr * Pred Grad: -0.061, New P: 0.533
iter 1 loss: 0.319
Actual params: [0.5464, 0.5327]
-Original Grad: -0.187, -lr * Pred Grad: -0.077, New P: 0.469
-Original Grad: -0.820, -lr * Pred Grad: -0.081, New P: 0.452
iter 2 loss: 0.300
Actual params: [0.469 , 0.4521]
-Original Grad: -0.090, -lr * Pred Grad: -0.082, New P: 0.387
-Original Grad: -0.131, -lr * Pred Grad: -0.084, New P: 0.368
iter 3 loss: 0.330
Actual params: [0.3869, 0.3684]
-Original Grad: 0.132, -lr * Pred Grad: -0.074, New P: 0.313
-Original Grad: 0.016, -lr * Pred Grad: -0.084, New P: 0.284
iter 4 loss: 0.349
Actual params: [0.3126, 0.2845]
-Original Grad: 0.174, -lr * Pred Grad: -0.065, New P: 0.248
-Original Grad: -0.003, -lr * Pred Grad: -0.082, New P: 0.202
iter 5 loss: 0.357
Actual params: [0.2478, 0.2025]
-Original Grad: 0.189, -lr * Pred Grad: -0.039, New P: 0.209
-Original Grad: -0.019, -lr * Pred Grad: -0.074, New P: 0.128
iter 6 loss: 0.362
Actual params: [0.2087, 0.128 ]
-Original Grad: 0.166, -lr * Pred Grad: 0.011, New P: 0.219
-Original Grad: -0.022, -lr * Pred Grad: -0.070, New P: 0.058
iter 7 loss: 0.354
Actual params: [0.2193, 0.0581]
-Original Grad: 0.161, -lr * Pred Grad: 0.063, New P: 0.283
-Original Grad: 0.015, -lr * Pred Grad: -0.067, New P: -0.009
iter 8 loss: 0.340
Actual params: [ 0.2826, -0.0085]
-Original Grad: 0.166, -lr * Pred Grad: 0.083, New P: 0.366
-Original Grad: 0.036, -lr * Pred Grad: -0.052, New P: -0.061
iter 9 loss: 0.325
Actual params: [ 0.3658, -0.0607]
-Original Grad: 0.110, -lr * Pred Grad: 0.086, New P: 0.452
-Original Grad: 0.038, -lr * Pred Grad: -0.035, New P: -0.096
iter 10 loss: 0.319
Actual params: [ 0.4516, -0.096 ]
-Original Grad: 0.073, -lr * Pred Grad: 0.084, New P: 0.535
-Original Grad: 0.039, -lr * Pred Grad: -0.027, New P: -0.123
iter 11 loss: 0.309
Actual params: [ 0.5352, -0.1227]
-Original Grad: -0.008, -lr * Pred Grad: 0.025, New P: 0.560
-Original Grad: 0.017, -lr * Pred Grad: -0.024, New P: -0.147
iter 12 loss: 0.308
Actual params: [ 0.5599, -0.1469]
-Original Grad: -0.000, -lr * Pred Grad: 0.029, New P: 0.589
-Original Grad: 0.025, -lr * Pred Grad: -0.020, New P: -0.167
iter 13 loss: 0.309
Actual params: [ 0.5885, -0.1667]
-Original Grad: -0.003, -lr * Pred Grad: -0.019, New P: 0.569
-Original Grad: 0.049, -lr * Pred Grad: 0.002, New P: -0.165
iter 14 loss: 0.308
Actual params: [ 0.5691, -0.1649]
-Original Grad: 0.004, -lr * Pred Grad: 0.005, New P: 0.574
-Original Grad: 0.043, -lr * Pred Grad: 0.035, New P: -0.130
iter 15 loss: 0.309
Actual params: [ 0.5741, -0.1297]
-Original Grad: -0.010, -lr * Pred Grad: -0.028, New P: 0.546
-Original Grad: 0.023, -lr * Pred Grad: 0.009, New P: -0.120
iter 16 loss: 0.308
Actual params: [ 0.5464, -0.1203]
-Original Grad: -0.005, -lr * Pred Grad: -0.020, New P: 0.526
-Original Grad: 0.028, -lr * Pred Grad: 0.014, New P: -0.106
iter 17 loss: 0.310
Actual params: [ 0.5261, -0.1064]
-Original Grad: 0.011, -lr * Pred Grad: -0.011, New P: 0.515
-Original Grad: 0.027, -lr * Pred Grad: 0.005, New P: -0.102
iter 18 loss: 0.312
Actual params: [ 0.5152, -0.1016]
-Original Grad: 0.021, -lr * Pred Grad: 0.002, New P: 0.517
-Original Grad: 0.036, -lr * Pred Grad: 0.017, New P: -0.084
iter 19 loss: 0.311
Actual params: [ 0.5172, -0.0845]
-Original Grad: 0.032, -lr * Pred Grad: 0.012, New P: 0.529
-Original Grad: 0.040, -lr * Pred Grad: 0.021, New P: -0.064
iter 20 loss: 0.308
Actual params: [ 0.5295, -0.0636]
-Original Grad: 0.002, -lr * Pred Grad: -0.003, New P: 0.527
-Original Grad: 0.021, -lr * Pred Grad: 0.010, New P: -0.053
Target params: [1.1812, 0.2779]
iter 0 loss: 0.667
Actual params: [0.5941, 0.5941]
-Original Grad: 0.712, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.117, -lr * Pred Grad: -0.053, New P: 0.541
iter 1 loss: 0.605
Actual params: [0.6617, 0.5408]
-Original Grad: 0.562, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.021, -lr * Pred Grad: -0.071, New P: 0.470
iter 2 loss: 0.495
Actual params: [0.7469, 0.4696]
-Original Grad: 0.241, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: 0.061, -lr * Pred Grad: -0.068, New P: 0.402
iter 3 loss: 0.313
Actual params: [0.8348, 0.4017]
-Original Grad: 0.207, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.113, -lr * Pred Grad: -0.047, New P: 0.355
iter 4 loss: 0.256
Actual params: [0.923, 0.355]
-Original Grad: 0.089, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.042, -lr * Pred Grad: 0.004, New P: 0.359
iter 5 loss: 0.240
Actual params: [1.0114, 0.3592]
-Original Grad: 0.049, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: 0.018, -lr * Pred Grad: 0.047, New P: 0.406
iter 6 loss: 0.229
Actual params: [1.0997, 0.4057]
-Original Grad: 0.011, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: -0.012, -lr * Pred Grad: -0.025, New P: 0.380
iter 7 loss: 0.246
Actual params: [1.188 , 0.3803]
-Original Grad: -0.088, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.044, -lr * Pred Grad: 0.022, New P: 0.403
iter 8 loss: 0.262
Actual params: [1.2761, 0.4026]
-Original Grad: -0.034, -lr * Pred Grad: 0.087, New P: 1.363
-Original Grad: 0.052, -lr * Pred Grad: 0.036, New P: 0.438
iter 9 loss: 0.281
Actual params: [1.3628, 0.4381]
-Original Grad: -0.012, -lr * Pred Grad: 0.077, New P: 1.439
-Original Grad: 0.088, -lr * Pred Grad: 0.062, New P: 0.500
iter 10 loss: 0.281
Actual params: [1.4394, 0.4998]
-Original Grad: -0.025, -lr * Pred Grad: 0.056, New P: 1.496
-Original Grad: 0.066, -lr * Pred Grad: 0.059, New P: 0.559
iter 11 loss: 0.260
Actual params: [1.4958, 0.559 ]
-Original Grad: -0.015, -lr * Pred Grad: -0.001, New P: 1.494
-Original Grad: -0.026, -lr * Pred Grad: -0.012, New P: 0.547
iter 12 loss: 0.268
Actual params: [1.4944, 0.5469]
-Original Grad: -0.015, -lr * Pred Grad: 0.052, New P: 1.547
-Original Grad: -0.017, -lr * Pred Grad: -0.016, New P: 0.531
iter 13 loss: 0.276
Actual params: [1.5467, 0.5308]
-Original Grad: -0.007, -lr * Pred Grad: 0.055, New P: 1.601
-Original Grad: 0.006, -lr * Pred Grad: -0.016, New P: 0.515
iter 14 loss: 0.288
Actual params: [1.6013, 0.5145]
-Original Grad: 0.014, -lr * Pred Grad: 0.074, New P: 1.676
-Original Grad: 0.018, -lr * Pred Grad: 0.005, New P: 0.519
iter 15 loss: 0.288
Actual params: [1.6758, 0.5193]
-Original Grad: 0.007, -lr * Pred Grad: 0.058, New P: 1.734
-Original Grad: 0.027, -lr * Pred Grad: 0.010, New P: 0.530
iter 16 loss: 0.279
Actual params: [1.7335, 0.5296]
-Original Grad: 0.007, -lr * Pred Grad: 0.026, New P: 1.760
-Original Grad: -0.054, -lr * Pred Grad: -0.029, New P: 0.501
iter 17 loss: 0.290
Actual params: [1.7595, 0.5006]
-Original Grad: 0.007, -lr * Pred Grad: 0.013, New P: 1.773
-Original Grad: 0.036, -lr * Pred Grad: -0.010, New P: 0.490
iter 18 loss: 0.293
Actual params: [1.7727, 0.4902]
-Original Grad: 0.013, -lr * Pred Grad: -0.001, New P: 1.772
-Original Grad: 0.066, -lr * Pred Grad: 0.030, New P: 0.520
iter 19 loss: 0.280
Actual params: [1.7719, 0.5204]
-Original Grad: 0.006, -lr * Pred Grad: -0.001, New P: 1.771
-Original Grad: -0.019, -lr * Pred Grad: 0.001, New P: 0.521
iter 20 loss: 0.280
Actual params: [1.7707, 0.5211]
-Original Grad: 0.009, -lr * Pred Grad: -0.009, New P: 1.762
-Original Grad: -0.026, -lr * Pred Grad: -0.026, New P: 0.495
Target params: [1.1812, 0.2779]
iter 0 loss: 0.250
Actual params: [0.5941, 0.5941]
-Original Grad: 0.009, -lr * Pred Grad: 0.041, New P: 0.635
-Original Grad: -0.101, -lr * Pred Grad: -0.048, New P: 0.546
iter 1 loss: 0.231
Actual params: [0.6353, 0.5456]
-Original Grad: 0.017, -lr * Pred Grad: -0.010, New P: 0.626
-Original Grad: -0.067, -lr * Pred Grad: -0.074, New P: 0.472
iter 2 loss: 0.221
Actual params: [0.6256, 0.4718]
-Original Grad: 0.005, -lr * Pred Grad: -0.044, New P: 0.582
-Original Grad: -0.005, -lr * Pred Grad: -0.071, New P: 0.401
iter 3 loss: 0.220
Actual params: [0.5818, 0.4006]
-Original Grad: 0.006, -lr * Pred Grad: -0.032, New P: 0.550
-Original Grad: 0.006, -lr * Pred Grad: -0.068, New P: 0.333
iter 4 loss: 0.220
Actual params: [0.5503, 0.3329]
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: 0.552
-Original Grad: 0.021, -lr * Pred Grad: -0.057, New P: 0.276
iter 5 loss: 0.216
Actual params: [0.552 , 0.2762]
-Original Grad: 0.005, -lr * Pred Grad: 0.000, New P: 0.552
-Original Grad: 0.020, -lr * Pred Grad: -0.032, New P: 0.244
iter 6 loss: 0.212
Actual params: [0.5523, 0.2443]
-Original Grad: 0.003, -lr * Pred Grad: -0.005, New P: 0.547
-Original Grad: 0.017, -lr * Pred Grad: 0.001, New P: 0.245
iter 7 loss: 0.213
Actual params: [0.5473, 0.2449]
-Original Grad: 0.000, -lr * Pred Grad: -0.010, New P: 0.537
-Original Grad: 0.016, -lr * Pred Grad: 0.017, New P: 0.262
iter 8 loss: 0.216
Actual params: [0.5372, 0.2615]
-Original Grad: 0.005, -lr * Pred Grad: -0.009, New P: 0.529
-Original Grad: 0.015, -lr * Pred Grad: -0.008, New P: 0.253
iter 9 loss: 0.216
Actual params: [0.5286, 0.2532]
-Original Grad: 0.004, -lr * Pred Grad: -0.008, New P: 0.521
-Original Grad: 0.015, -lr * Pred Grad: 0.002, New P: 0.255
iter 10 loss: 0.217
Actual params: [0.5209, 0.255 ]
-Original Grad: 0.007, -lr * Pred Grad: -0.006, New P: 0.514
-Original Grad: 0.013, -lr * Pred Grad: -0.005, New P: 0.249
iter 11 loss: 0.218
Actual params: [0.5145, 0.2495]
-Original Grad: 0.008, -lr * Pred Grad: -0.006, New P: 0.509
-Original Grad: 0.014, -lr * Pred Grad: -0.004, New P: 0.245
iter 12 loss: 0.218
Actual params: [0.5086, 0.2451]
-Original Grad: 0.005, -lr * Pred Grad: -0.007, New P: 0.501
-Original Grad: 0.017, -lr * Pred Grad: -0.004, New P: 0.241
iter 13 loss: 0.219
Actual params: [0.5013, 0.2412]
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: 0.494
-Original Grad: 0.014, -lr * Pred Grad: -0.005, New P: 0.237
iter 14 loss: 0.219
Actual params: [0.494 , 0.2367]
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: 0.487
-Original Grad: 0.015, -lr * Pred Grad: -0.005, New P: 0.231
iter 15 loss: 0.220
Actual params: [0.487 , 0.2312]
-Original Grad: 0.009, -lr * Pred Grad: -0.006, New P: 0.481
-Original Grad: 0.013, -lr * Pred Grad: -0.007, New P: 0.224
iter 16 loss: 0.220
Actual params: [0.4814, 0.2243]
-Original Grad: 0.007, -lr * Pred Grad: -0.005, New P: 0.476
-Original Grad: 0.016, -lr * Pred Grad: -0.006, New P: 0.218
iter 17 loss: 0.220
Actual params: [0.4765, 0.218 ]
-Original Grad: 0.012, -lr * Pred Grad: -0.002, New P: 0.474
-Original Grad: 0.014, -lr * Pred Grad: -0.005, New P: 0.213
iter 18 loss: 0.219
Actual params: [0.4741, 0.2128]
-Original Grad: 0.002, -lr * Pred Grad: -0.004, New P: 0.470
-Original Grad: 0.020, -lr * Pred Grad: -0.001, New P: 0.212
iter 19 loss: 0.220
Actual params: [0.4696, 0.2118]
-Original Grad: 0.003, -lr * Pred Grad: -0.006, New P: 0.463
-Original Grad: 0.017, -lr * Pred Grad: 0.002, New P: 0.213
iter 20 loss: 0.221
Actual params: [0.4632, 0.2135]
-Original Grad: 0.005, -lr * Pred Grad: -0.006, New P: 0.458
-Original Grad: 0.017, -lr * Pred Grad: 0.002, New P: 0.216
Target params: [1.1812, 0.2779]
iter 0 loss: 0.502
Actual params: [0.5941, 0.5941]
-Original Grad: 0.327, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.295, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.477
Actual params: [0.6617, 0.5309]
-Original Grad: 0.151, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.240, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.439
Actual params: [0.7466, 0.4504]
-Original Grad: 0.158, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.119, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.391
Actual params: [0.8342, 0.3668]
-Original Grad: 0.124, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: 0.018, -lr * Pred Grad: -0.083, New P: 0.284
iter 4 loss: 0.349
Actual params: [0.9224, 0.2839]
-Original Grad: -0.074, -lr * Pred Grad: 0.088, New P: 1.010
-Original Grad: 0.013, -lr * Pred Grad: -0.076, New P: 0.208
iter 5 loss: 0.321
Actual params: [1.0102, 0.2083]
-Original Grad: -0.317, -lr * Pred Grad: -0.021, New P: 0.989
-Original Grad: 0.409, -lr * Pred Grad: -0.069, New P: 0.139
iter 6 loss: 0.289
Actual params: [0.9891, 0.1392]
-Original Grad: -0.388, -lr * Pred Grad: -0.053, New P: 0.936
-Original Grad: 0.153, -lr * Pred Grad: -0.060, New P: 0.079
iter 7 loss: 0.279
Actual params: [0.9359, 0.0791]
-Original Grad: -0.362, -lr * Pred Grad: -0.068, New P: 0.868
-Original Grad: 0.066, -lr * Pred Grad: -0.037, New P: 0.042
iter 8 loss: 0.294
Actual params: [0.868 , 0.0424]
-Original Grad: 0.045, -lr * Pred Grad: -0.066, New P: 0.802
-Original Grad: -0.010, -lr * Pred Grad: -0.008, New P: 0.034
iter 9 loss: 0.338
Actual params: [0.8023, 0.0342]
-Original Grad: 0.132, -lr * Pred Grad: -0.052, New P: 0.750
-Original Grad: -0.084, -lr * Pred Grad: -0.016, New P: 0.019
iter 10 loss: 0.390
Actual params: [0.7503, 0.0185]
-Original Grad: 0.158, -lr * Pred Grad: -0.023, New P: 0.727
-Original Grad: -0.004, -lr * Pred Grad: -0.040, New P: -0.022
iter 11 loss: 0.405
Actual params: [ 0.7273, -0.0216]
-Original Grad: 0.261, -lr * Pred Grad: 0.011, New P: 0.738
-Original Grad: 0.045, -lr * Pred Grad: 0.006, New P: -0.015
iter 12 loss: 0.396
Actual params: [ 0.7383, -0.0153]
-Original Grad: 0.274, -lr * Pred Grad: 0.057, New P: 0.796
-Original Grad: -0.018, -lr * Pred Grad: -0.010, New P: -0.025
iter 13 loss: 0.334
Actual params: [ 0.7956, -0.0249]
-Original Grad: 0.031, -lr * Pred Grad: 0.081, New P: 0.877
-Original Grad: 0.001, -lr * Pred Grad: -0.019, New P: -0.044
iter 14 loss: 0.284
Actual params: [ 0.8771, -0.0442]
-Original Grad: 0.013, -lr * Pred Grad: 0.080, New P: 0.957
-Original Grad: 0.030, -lr * Pred Grad: -0.001, New P: -0.045
iter 15 loss: 0.263
Actual params: [ 0.9566, -0.0447]
-Original Grad: -0.510, -lr * Pred Grad: -0.035, New P: 0.921
-Original Grad: -0.288, -lr * Pred Grad: -0.047, New P: -0.092
iter 16 loss: 0.275
Actual params: [ 0.9215, -0.0921]
-Original Grad: -0.122, -lr * Pred Grad: -0.056, New P: 0.865
-Original Grad: 0.008, -lr * Pred Grad: -0.061, New P: -0.153
iter 17 loss: 0.293
Actual params: [ 0.8654, -0.1527]
-Original Grad: -0.004, -lr * Pred Grad: -0.057, New P: 0.809
-Original Grad: 0.023, -lr * Pred Grad: -0.045, New P: -0.198
iter 18 loss: 0.322
Actual params: [ 0.8086, -0.1979]
-Original Grad: 0.152, -lr * Pred Grad: -0.025, New P: 0.783
-Original Grad: 0.040, -lr * Pred Grad: -0.004, New P: -0.202
iter 19 loss: 0.350
Actual params: [ 0.7832, -0.202 ]
-Original Grad: 0.051, -lr * Pred Grad: 0.015, New P: 0.798
-Original Grad: 0.061, -lr * Pred Grad: 0.040, New P: -0.162
iter 20 loss: 0.331
Actual params: [ 0.7985, -0.1621]
-Original Grad: 0.062, -lr * Pred Grad: 0.060, New P: 0.859
-Original Grad: 0.071, -lr * Pred Grad: 0.062, New P: -0.100
Target params: [1.1812, 0.2779]
iter 0 loss: 0.450
Actual params: [0.5941, 0.5941]
-Original Grad: 0.341, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.191, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.390
Actual params: [0.6617, 0.5323]
-Original Grad: 0.306, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.144, -lr * Pred Grad: -0.080, New P: 0.452
iter 2 loss: 0.311
Actual params: [0.7467, 0.452 ]
-Original Grad: 0.267, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.045, -lr * Pred Grad: -0.082, New P: 0.370
iter 3 loss: 0.232
Actual params: [0.8345, 0.3696]
-Original Grad: 0.032, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.014, -lr * Pred Grad: -0.075, New P: 0.294
iter 4 loss: 0.209
Actual params: [0.9228, 0.2942]
-Original Grad: 0.059, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.029, -lr * Pred Grad: -0.070, New P: 0.224
iter 5 loss: 0.206
Actual params: [1.0111, 0.2244]
-Original Grad: -0.028, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 0.044, -lr * Pred Grad: -0.062, New P: 0.162
iter 6 loss: 0.228
Actual params: [1.0993, 0.1622]
-Original Grad: -0.044, -lr * Pred Grad: 0.088, New P: 1.187
-Original Grad: 0.071, -lr * Pred Grad: -0.041, New P: 0.122
iter 7 loss: 0.261
Actual params: [1.1869, 0.1215]
-Original Grad: -0.059, -lr * Pred Grad: 0.080, New P: 1.267
-Original Grad: 0.103, -lr * Pred Grad: -0.022, New P: 0.099
iter 8 loss: 0.286
Actual params: [1.2672, 0.0994]
-Original Grad: -0.063, -lr * Pred Grad: -0.014, New P: 1.254
-Original Grad: 0.081, -lr * Pred Grad: 0.007, New P: 0.107
iter 9 loss: 0.282
Actual params: [1.2536, 0.1068]
-Original Grad: -0.061, -lr * Pred Grad: -0.007, New P: 1.246
-Original Grad: 0.118, -lr * Pred Grad: 0.052, New P: 0.159
iter 10 loss: 0.277
Actual params: [1.2463, 0.1587]
-Original Grad: -0.061, -lr * Pred Grad: 0.022, New P: 1.268
-Original Grad: 0.076, -lr * Pred Grad: 0.076, New P: 0.235
iter 11 loss: 0.285
Actual params: [1.268, 0.235]
-Original Grad: -0.048, -lr * Pred Grad: -0.022, New P: 1.246
-Original Grad: 0.062, -lr * Pred Grad: 0.064, New P: 0.299
iter 12 loss: 0.277
Actual params: [1.2464, 0.2989]
-Original Grad: -0.055, -lr * Pred Grad: -0.037, New P: 1.209
-Original Grad: 0.080, -lr * Pred Grad: 0.070, New P: 0.369
iter 13 loss: 0.267
Actual params: [1.2093, 0.3691]
-Original Grad: -0.040, -lr * Pred Grad: -0.043, New P: 1.167
-Original Grad: 0.050, -lr * Pred Grad: 0.050, New P: 0.419
iter 14 loss: 0.252
Actual params: [1.1666, 0.4195]
-Original Grad: -0.022, -lr * Pred Grad: -0.034, New P: 1.132
-Original Grad: 0.008, -lr * Pred Grad: 0.020, New P: 0.439
iter 15 loss: 0.238
Actual params: [1.1322, 0.4391]
-Original Grad: -0.018, -lr * Pred Grad: -0.024, New P: 1.108
-Original Grad: -0.001, -lr * Pred Grad: -0.009, New P: 0.430
iter 16 loss: 0.221
Actual params: [1.1079, 0.4299]
-Original Grad: -0.011, -lr * Pred Grad: -0.019, New P: 1.089
-Original Grad: 0.001, -lr * Pred Grad: -0.012, New P: 0.418
iter 17 loss: 0.212
Actual params: [1.0892, 0.4182]
-Original Grad: -0.012, -lr * Pred Grad: -0.018, New P: 1.072
-Original Grad: -0.003, -lr * Pred Grad: -0.018, New P: 0.400
iter 18 loss: 0.204
Actual params: [1.0716, 0.4004]
-Original Grad: -0.010, -lr * Pred Grad: -0.018, New P: 1.054
-Original Grad: 0.007, -lr * Pred Grad: -0.011, New P: 0.389
iter 19 loss: 0.188
Actual params: [1.0537, 0.3895]
-Original Grad: -0.017, -lr * Pred Grad: -0.021, New P: 1.033
-Original Grad: 0.024, -lr * Pred Grad: 0.001, New P: 0.390
iter 20 loss: 0.179
Actual params: [1.0329, 0.39  ]
-Original Grad: -0.020, -lr * Pred Grad: -0.024, New P: 1.009
-Original Grad: 0.024, -lr * Pred Grad: 0.007, New P: 0.397
Target params: [1.1812, 0.2779]
iter 0 loss: 0.617
Actual params: [0.5941, 0.5941]
-Original Grad: 0.220, -lr * Pred Grad: 0.067, New P: 0.662
-Original Grad: -0.394, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.571
Actual params: [0.6615, 0.531 ]
-Original Grad: 0.094, -lr * Pred Grad: 0.084, New P: 0.746
-Original Grad: -0.143, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.509
Actual params: [0.7459, 0.4503]
-Original Grad: 0.041, -lr * Pred Grad: 0.086, New P: 0.832
-Original Grad: -0.041, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.468
Actual params: [0.8321, 0.3669]
-Original Grad: 0.028, -lr * Pred Grad: 0.076, New P: 0.909
-Original Grad: -0.011, -lr * Pred Grad: -0.082, New P: 0.285
iter 4 loss: 0.457
Actual params: [0.9086, 0.2852]
-Original Grad: -0.000, -lr * Pred Grad: 0.001, New P: 0.910
-Original Grad: -0.000, -lr * Pred Grad: -0.074, New P: 0.212
iter 5 loss: 0.445
Actual params: [0.9097, 0.2116]
-Original Grad: 0.000, -lr * Pred Grad: 0.038, New P: 0.948
-Original Grad: 0.022, -lr * Pred Grad: -0.069, New P: 0.142
iter 6 loss: 0.441
Actual params: [0.9476, 0.1422]
-Original Grad: 0.002, -lr * Pred Grad: -0.011, New P: 0.937
-Original Grad: 0.020, -lr * Pred Grad: -0.063, New P: 0.079
iter 7 loss: 0.438
Actual params: [0.937 , 0.0793]
-Original Grad: 0.004, -lr * Pred Grad: 0.014, New P: 0.951
-Original Grad: 0.020, -lr * Pred Grad: -0.045, New P: 0.035
iter 8 loss: 0.439
Actual params: [0.9507, 0.0346]
-Original Grad: 0.005, -lr * Pred Grad: -0.021, New P: 0.930
-Original Grad: 0.019, -lr * Pred Grad: -0.032, New P: 0.003
iter 9 loss: 0.439
Actual params: [0.9296, 0.0031]
-Original Grad: 0.006, -lr * Pred Grad: -0.005, New P: 0.925
-Original Grad: 0.007, -lr * Pred Grad: -0.024, New P: -0.021
iter 10 loss: 0.439
Actual params: [ 0.9249, -0.0208]
-Original Grad: 0.007, -lr * Pred Grad: -0.012, New P: 0.913
-Original Grad: 0.002, -lr * Pred Grad: -0.013, New P: -0.034
iter 11 loss: 0.439
Actual params: [ 0.913 , -0.0342]
-Original Grad: 0.003, -lr * Pred Grad: -0.009, New P: 0.904
-Original Grad: 0.004, -lr * Pred Grad: -0.006, New P: -0.040
iter 12 loss: 0.438
Actual params: [ 0.9044, -0.04  ]
-Original Grad: 0.001, -lr * Pred Grad: -0.012, New P: 0.893
-Original Grad: 0.006, -lr * Pred Grad: -0.018, New P: -0.058
iter 13 loss: 0.438
Actual params: [ 0.8926, -0.0578]
-Original Grad: -0.000, -lr * Pred Grad: -0.013, New P: 0.880
-Original Grad: 0.006, -lr * Pred Grad: -0.015, New P: -0.073
iter 14 loss: 0.438
Actual params: [ 0.88  , -0.0725]
-Original Grad: 0.003, -lr * Pred Grad: -0.011, New P: 0.869
-Original Grad: 0.003, -lr * Pred Grad: -0.018, New P: -0.090
iter 15 loss: 0.438
Actual params: [ 0.8686, -0.0901]
-Original Grad: 0.009, -lr * Pred Grad: -0.007, New P: 0.861
-Original Grad: -0.000, -lr * Pred Grad: -0.021, New P: -0.111
iter 16 loss: 0.438
Actual params: [ 0.8615, -0.1107]
-Original Grad: 0.013, -lr * Pred Grad: -0.002, New P: 0.859
-Original Grad: 0.016, -lr * Pred Grad: -0.015, New P: -0.126
iter 17 loss: 0.439
Actual params: [ 0.8593, -0.1261]
-Original Grad: 0.011, -lr * Pred Grad: -0.001, New P: 0.858
-Original Grad: 0.011, -lr * Pred Grad: -0.011, New P: -0.137
iter 18 loss: 0.439
Actual params: [ 0.858 , -0.1373]
-Original Grad: 0.016, -lr * Pred Grad: 0.001, New P: 0.859
-Original Grad: 0.015, -lr * Pred Grad: -0.009, New P: -0.146
iter 19 loss: 0.439
Actual params: [ 0.8588, -0.1461]
-Original Grad: 0.016, -lr * Pred Grad: 0.002, New P: 0.861
-Original Grad: 0.006, -lr * Pred Grad: -0.013, New P: -0.159
iter 20 loss: 0.440
Actual params: [ 0.8612, -0.159 ]
-Original Grad: 0.014, -lr * Pred Grad: 0.002, New P: 0.864
-Original Grad: 0.008, -lr * Pred Grad: -0.017, New P: -0.176
Target params: [1.1812, 0.2779]
iter 0 loss: 0.947
Actual params: [0.5941, 0.5941]
-Original Grad: -0.022, -lr * Pred Grad: 0.015, New P: 0.609
-Original Grad: -0.233, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.912
Actual params: [0.6087, 0.5313]
-Original Grad: -0.012, -lr * Pred Grad: -0.046, New P: 0.563
-Original Grad: -0.273, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.834
Actual params: [0.5626, 0.4509]
-Original Grad: -0.028, -lr * Pred Grad: -0.064, New P: 0.499
-Original Grad: -0.563, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.759
Actual params: [0.4987, 0.3674]
-Original Grad: -0.186, -lr * Pred Grad: -0.068, New P: 0.431
-Original Grad: -0.684, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.687
Actual params: [0.4308, 0.2832]
-Original Grad: -0.058, -lr * Pred Grad: -0.067, New P: 0.364
-Original Grad: -0.519, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.639
Actual params: [0.3638, 0.1988]
-Original Grad: 0.102, -lr * Pred Grad: -0.053, New P: 0.311
-Original Grad: -0.206, -lr * Pred Grad: -0.084, New P: 0.114
iter 6 loss: 0.627
Actual params: [0.3107, 0.1143]
-Original Grad: 0.160, -lr * Pred Grad: -0.028, New P: 0.283
-Original Grad: -0.037, -lr * Pred Grad: -0.084, New P: 0.030
iter 7 loss: 0.619
Actual params: [0.2829, 0.0299]
-Original Grad: 0.181, -lr * Pred Grad: 0.002, New P: 0.285
-Original Grad: -0.031, -lr * Pred Grad: -0.084, New P: -0.054
iter 8 loss: 0.611
Actual params: [ 0.2854, -0.0541]
-Original Grad: 0.165, -lr * Pred Grad: 0.050, New P: 0.335
-Original Grad: -0.014, -lr * Pred Grad: -0.080, New P: -0.134
iter 9 loss: 0.601
Actual params: [ 0.335 , -0.1339]
-Original Grad: 0.232, -lr * Pred Grad: 0.079, New P: 0.414
-Original Grad: 0.005, -lr * Pred Grad: -0.072, New P: -0.206
iter 10 loss: 0.578
Actual params: [ 0.4143, -0.2057]
-Original Grad: 0.114, -lr * Pred Grad: 0.086, New P: 0.500
-Original Grad: 0.000, -lr * Pred Grad: -0.069, New P: -0.275
iter 11 loss: 0.546
Actual params: [ 0.5003, -0.2747]
-Original Grad: 0.070, -lr * Pred Grad: 0.086, New P: 0.586
-Original Grad: 0.020, -lr * Pred Grad: -0.062, New P: -0.337
iter 12 loss: 0.522
Actual params: [ 0.5858, -0.3365]
-Original Grad: 0.014, -lr * Pred Grad: 0.053, New P: 0.639
-Original Grad: 0.031, -lr * Pred Grad: -0.044, New P: -0.380
iter 13 loss: 0.512
Actual params: [ 0.6388, -0.3801]
-Original Grad: -0.017, -lr * Pred Grad: 0.001, New P: 0.640
-Original Grad: 0.040, -lr * Pred Grad: -0.031, New P: -0.412
iter 14 loss: 0.507
Actual params: [ 0.6399, -0.4116]
-Original Grad: -0.006, -lr * Pred Grad: 0.009, New P: 0.649
-Original Grad: 0.049, -lr * Pred Grad: -0.026, New P: -0.438
iter 15 loss: 0.507
Actual params: [ 0.6492, -0.4377]
-Original Grad: -0.023, -lr * Pred Grad: -0.028, New P: 0.621
-Original Grad: 0.062, -lr * Pred Grad: -0.024, New P: -0.462
iter 16 loss: 0.507
Actual params: [ 0.6213, -0.4619]
-Original Grad: -0.014, -lr * Pred Grad: -0.025, New P: 0.596
-Original Grad: 0.062, -lr * Pred Grad: -0.021, New P: -0.483
iter 17 loss: 0.512
Actual params: [ 0.5964, -0.4833]
-Original Grad: -0.013, -lr * Pred Grad: -0.028, New P: 0.569
-Original Grad: 0.079, -lr * Pred Grad: -0.020, New P: -0.503
iter 18 loss: 0.517
Actual params: [ 0.5685, -0.5029]
-Original Grad: 0.030, -lr * Pred Grad: -0.004, New P: 0.565
-Original Grad: 0.070, -lr * Pred Grad: -0.016, New P: -0.519
iter 19 loss: 0.517
Actual params: [ 0.5646, -0.5189]
-Original Grad: 0.025, -lr * Pred Grad: 0.009, New P: 0.574
-Original Grad: 0.101, -lr * Pred Grad: 0.001, New P: -0.518
iter 20 loss: 0.515
Actual params: [ 0.5741, -0.5177]
-Original Grad: 0.004, -lr * Pred Grad: -0.003, New P: 0.572
-Original Grad: 0.088, -lr * Pred Grad: 0.039, New P: -0.479
Target params: [1.1812, 0.2779]
iter 0 loss: 0.477
Actual params: [0.5941, 0.5941]
-Original Grad: -0.220, -lr * Pred Grad: -0.063, New P: 0.532
-Original Grad: -0.233, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.414
Actual params: [0.5315, 0.5313]
-Original Grad: -0.186, -lr * Pred Grad: -0.080, New P: 0.451
-Original Grad: -0.115, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.366
Actual params: [0.451 , 0.4508]
-Original Grad: -0.082, -lr * Pred Grad: -0.083, New P: 0.368
-Original Grad: 0.020, -lr * Pred Grad: -0.081, New P: 0.370
iter 3 loss: 0.339
Actual params: [0.3677, 0.3701]
-Original Grad: -0.049, -lr * Pred Grad: -0.082, New P: 0.285
-Original Grad: 0.029, -lr * Pred Grad: -0.072, New P: 0.298
iter 4 loss: 0.317
Actual params: [0.2854, 0.2976]
-Original Grad: -0.044, -lr * Pred Grad: -0.076, New P: 0.209
-Original Grad: 0.013, -lr * Pred Grad: -0.068, New P: 0.229
iter 5 loss: 0.301
Actual params: [0.2093, 0.2292]
-Original Grad: -0.032, -lr * Pred Grad: -0.071, New P: 0.139
-Original Grad: 0.011, -lr * Pred Grad: -0.058, New P: 0.171
iter 6 loss: 0.290
Actual params: [0.1387, 0.1708]
-Original Grad: -0.036, -lr * Pred Grad: -0.068, New P: 0.070
-Original Grad: 0.013, -lr * Pred Grad: -0.039, New P: 0.132
iter 7 loss: 0.280
Actual params: [0.0705, 0.1316]
-Original Grad: -0.026, -lr * Pred Grad: -0.064, New P: 0.006
-Original Grad: 0.022, -lr * Pred Grad: -0.019, New P: 0.112
iter 8 loss: 0.272
Actual params: [0.006 , 0.1122]
-Original Grad: -0.027, -lr * Pred Grad: -0.057, New P: -0.051
-Original Grad: 0.019, -lr * Pred Grad: 0.008, New P: 0.120
iter 9 loss: 0.267
Actual params: [-0.0515,  0.1205]
-Original Grad: -0.023, -lr * Pred Grad: -0.049, New P: -0.101
-Original Grad: 0.014, -lr * Pred Grad: 0.006, New P: 0.127
iter 10 loss: 0.264
Actual params: [-0.1008,  0.127 ]
-Original Grad: -0.026, -lr * Pred Grad: -0.046, New P: -0.147
-Original Grad: 0.021, -lr * Pred Grad: -0.004, New P: 0.122
iter 11 loss: 0.261
Actual params: [-0.1468,  0.1225]
-Original Grad: -0.024, -lr * Pred Grad: -0.044, New P: -0.191
-Original Grad: 0.019, -lr * Pred Grad: 0.002, New P: 0.125
iter 12 loss: 0.258
Actual params: [-0.191 ,  0.1245]
-Original Grad: -0.018, -lr * Pred Grad: -0.042, New P: -0.233
-Original Grad: 0.026, -lr * Pred Grad: 0.004, New P: 0.129
iter 13 loss: 0.256
Actual params: [-0.2326,  0.129 ]
-Original Grad: -0.018, -lr * Pred Grad: -0.037, New P: -0.269
-Original Grad: 0.014, -lr * Pred Grad: 0.000, New P: 0.129
iter 14 loss: 0.254
Actual params: [-0.2695,  0.1292]
-Original Grad: -0.014, -lr * Pred Grad: -0.030, New P: -0.300
-Original Grad: 0.018, -lr * Pred Grad: -0.003, New P: 0.126
iter 15 loss: 0.252
Actual params: [-0.2999,  0.1264]
-Original Grad: -0.011, -lr * Pred Grad: -0.028, New P: -0.328
-Original Grad: 0.015, -lr * Pred Grad: -0.006, New P: 0.121
iter 16 loss: 0.251
Actual params: [-0.3276,  0.1208]
-Original Grad: -0.013, -lr * Pred Grad: -0.029, New P: -0.356
-Original Grad: 0.018, -lr * Pred Grad: -0.005, New P: 0.116
iter 17 loss: 0.249
Actual params: [-0.3563,  0.1158]
-Original Grad: -0.012, -lr * Pred Grad: -0.030, New P: -0.386
-Original Grad: 0.016, -lr * Pred Grad: -0.004, New P: 0.111
iter 18 loss: 0.248
Actual params: [-0.386 ,  0.1115]
-Original Grad: -0.009, -lr * Pred Grad: -0.029, New P: -0.415
-Original Grad: 0.015, -lr * Pred Grad: -0.004, New P: 0.107
iter 19 loss: 0.247
Actual params: [-0.4153,  0.1075]
-Original Grad: -0.012, -lr * Pred Grad: -0.029, New P: -0.445
-Original Grad: 0.019, -lr * Pred Grad: -0.001, New P: 0.106
iter 20 loss: 0.246
Actual params: [-0.4447,  0.1063]
-Original Grad: -0.011, -lr * Pred Grad: -0.030, New P: -0.474
-Original Grad: 0.014, -lr * Pred Grad: -0.000, New P: 0.106
Target params: [1.1812, 0.2779]
iter 0 loss: 0.393
Actual params: [0.5941, 0.5941]
-Original Grad: -0.023, -lr * Pred Grad: 0.014, New P: 0.608
-Original Grad: -0.170, -lr * Pred Grad: -0.061, New P: 0.533
iter 1 loss: 0.356
Actual params: [0.6082, 0.5334]
-Original Grad: -0.018, -lr * Pred Grad: -0.048, New P: 0.560
-Original Grad: -0.153, -lr * Pred Grad: -0.080, New P: 0.453
iter 2 loss: 0.327
Actual params: [0.5602, 0.4533]
-Original Grad: -0.055, -lr * Pred Grad: -0.066, New P: 0.494
-Original Grad: -0.106, -lr * Pred Grad: -0.083, New P: 0.370
iter 3 loss: 0.292
Actual params: [0.4941, 0.3703]
-Original Grad: -0.033, -lr * Pred Grad: -0.066, New P: 0.428
-Original Grad: -0.051, -lr * Pred Grad: -0.081, New P: 0.290
iter 4 loss: 0.263
Actual params: [0.4281, 0.2895]
-Original Grad: -0.062, -lr * Pred Grad: -0.064, New P: 0.365
-Original Grad: -0.026, -lr * Pred Grad: -0.073, New P: 0.216
iter 5 loss: 0.246
Actual params: [0.3646, 0.2163]
-Original Grad: -0.045, -lr * Pred Grad: -0.060, New P: 0.305
-Original Grad: -0.023, -lr * Pred Grad: -0.069, New P: 0.147
iter 6 loss: 0.237
Actual params: [0.3046, 0.1469]
-Original Grad: -0.038, -lr * Pred Grad: -0.055, New P: 0.250
-Original Grad: -0.009, -lr * Pred Grad: -0.066, New P: 0.081
iter 7 loss: 0.233
Actual params: [0.2499, 0.081 ]
-Original Grad: -0.031, -lr * Pred Grad: -0.046, New P: 0.203
-Original Grad: 0.004, -lr * Pred Grad: -0.055, New P: 0.026
iter 8 loss: 0.232
Actual params: [0.2035, 0.026 ]
-Original Grad: -0.034, -lr * Pred Grad: -0.040, New P: 0.164
-Original Grad: 0.011, -lr * Pred Grad: -0.039, New P: -0.014
iter 9 loss: 0.231
Actual params: [ 0.1638, -0.0135]
-Original Grad: -0.027, -lr * Pred Grad: -0.037, New P: 0.127
-Original Grad: 0.011, -lr * Pred Grad: -0.030, New P: -0.044
iter 10 loss: 0.229
Actual params: [ 0.1271, -0.0439]
-Original Grad: -0.029, -lr * Pred Grad: -0.037, New P: 0.090
-Original Grad: 0.016, -lr * Pred Grad: -0.024, New P: -0.068
iter 11 loss: 0.226
Actual params: [ 0.0904, -0.0684]
-Original Grad: -0.025, -lr * Pred Grad: -0.036, New P: 0.054
-Original Grad: 0.014, -lr * Pred Grad: -0.013, New P: -0.081
iter 12 loss: 0.221
Actual params: [ 0.054 , -0.0813]
-Original Grad: -0.022, -lr * Pred Grad: -0.036, New P: 0.018
-Original Grad: 0.014, -lr * Pred Grad: 0.003, New P: -0.078
iter 13 loss: 0.216
Actual params: [ 0.0183, -0.0783]
-Original Grad: -0.021, -lr * Pred Grad: -0.035, New P: -0.017
-Original Grad: 0.013, -lr * Pred Grad: -0.007, New P: -0.086
iter 14 loss: 0.211
Actual params: [-0.0167, -0.0857]
-Original Grad: -0.024, -lr * Pred Grad: -0.035, New P: -0.052
-Original Grad: 0.013, -lr * Pred Grad: -0.009, New P: -0.095
iter 15 loss: 0.211
Actual params: [-0.052 , -0.0945]
-Original Grad: -0.017, -lr * Pred Grad: -0.035, New P: -0.087
-Original Grad: 0.010, -lr * Pred Grad: -0.012, New P: -0.106
iter 16 loss: 0.208
Actual params: [-0.0867, -0.1061]
-Original Grad: -0.020, -lr * Pred Grad: -0.034, New P: -0.121
-Original Grad: 0.016, -lr * Pred Grad: -0.010, New P: -0.116
iter 17 loss: 0.204
Actual params: [-0.1208, -0.1161]
-Original Grad: -0.011, -lr * Pred Grad: -0.032, New P: -0.153
-Original Grad: 0.014, -lr * Pred Grad: -0.010, New P: -0.126
iter 18 loss: 0.200
Actual params: [-0.1528, -0.1256]
-Original Grad: -0.007, -lr * Pred Grad: -0.029, New P: -0.181
-Original Grad: 0.015, -lr * Pred Grad: -0.009, New P: -0.135
iter 19 loss: 0.197
Actual params: [-0.1815, -0.1351]
-Original Grad: -0.002, -lr * Pred Grad: -0.024, New P: -0.206
-Original Grad: 0.019, -lr * Pred Grad: -0.008, New P: -0.143
iter 20 loss: 0.196
Actual params: [-0.206 , -0.1432]
-Original Grad: -0.001, -lr * Pred Grad: -0.021, New P: -0.227
-Original Grad: 0.017, -lr * Pred Grad: -0.008, New P: -0.151
Target params: [1.1812, 0.2779]
iter 0 loss: 0.990
Actual params: [0.5941, 0.5941]
-Original Grad: 0.829, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.138, -lr * Pred Grad: -0.057, New P: 0.537
iter 1 loss: 0.920
Actual params: [0.6618, 0.5366]
-Original Grad: 0.917, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.103, -lr * Pred Grad: -0.079, New P: 0.458
iter 2 loss: 0.782
Actual params: [0.7469, 0.458 ]
-Original Grad: 0.880, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: 0.029, -lr * Pred Grad: -0.075, New P: 0.383
iter 3 loss: 0.620
Actual params: [0.8348, 0.3828]
-Original Grad: 0.557, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.017, -lr * Pred Grad: -0.070, New P: 0.313
iter 4 loss: 0.549
Actual params: [0.9231, 0.313 ]
-Original Grad: 0.106, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.038, -lr * Pred Grad: -0.067, New P: 0.246
iter 5 loss: 0.559
Actual params: [1.0114, 0.2463]
-Original Grad: 0.041, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: -0.014, -lr * Pred Grad: -0.060, New P: 0.186
iter 6 loss: 0.588
Actual params: [1.0998, 0.1864]
-Original Grad: 0.029, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: -0.048, -lr * Pred Grad: -0.054, New P: 0.133
iter 7 loss: 0.639
Actual params: [1.1881, 0.1326]
-Original Grad: -0.003, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: -0.029, -lr * Pred Grad: -0.049, New P: 0.084
iter 8 loss: 0.688
Actual params: [1.2764, 0.0839]
-Original Grad: -0.031, -lr * Pred Grad: 0.088, New P: 1.365
-Original Grad: 0.001, -lr * Pred Grad: -0.033, New P: 0.051
iter 9 loss: 0.733
Actual params: [1.3647, 0.0506]
-Original Grad: -0.016, -lr * Pred Grad: 0.088, New P: 1.453
-Original Grad: -0.013, -lr * Pred Grad: -0.021, New P: 0.030
iter 10 loss: 0.788
Actual params: [1.453, 0.03 ]
-Original Grad: -0.057, -lr * Pred Grad: 0.088, New P: 1.541
-Original Grad: -0.001, -lr * Pred Grad: -0.018, New P: 0.012
iter 11 loss: 0.838
Actual params: [1.541 , 0.0125]
-Original Grad: -0.040, -lr * Pred Grad: 0.086, New P: 1.627
-Original Grad: -0.013, -lr * Pred Grad: -0.024, New P: -0.012
iter 12 loss: 0.884
Actual params: [ 1.6272, -0.0116]
-Original Grad: -0.034, -lr * Pred Grad: 0.076, New P: 1.703
-Original Grad: -0.012, -lr * Pred Grad: -0.028, New P: -0.040
iter 13 loss: 0.924
Actual params: [ 1.7031, -0.04  ]
-Original Grad: -0.047, -lr * Pred Grad: 0.058, New P: 1.761
-Original Grad: -0.007, -lr * Pred Grad: -0.028, New P: -0.068
iter 14 loss: 0.951
Actual params: [ 1.7614, -0.0684]
-Original Grad: -0.064, -lr * Pred Grad: -0.009, New P: 1.753
-Original Grad: -0.011, -lr * Pred Grad: -0.028, New P: -0.097
iter 15 loss: 0.950
Actual params: [ 1.7528, -0.0967]
-Original Grad: -0.105, -lr * Pred Grad: -0.001, New P: 1.752
-Original Grad: -0.005, -lr * Pred Grad: -0.027, New P: -0.124
iter 16 loss: 0.952
Actual params: [ 1.7517, -0.1235]
-Original Grad: -0.052, -lr * Pred Grad: 0.008, New P: 1.760
-Original Grad: -0.008, -lr * Pred Grad: -0.026, New P: -0.149
iter 17 loss: 0.957
Actual params: [ 1.7602, -0.1493]
-Original Grad: -0.078, -lr * Pred Grad: -0.008, New P: 1.753
-Original Grad: -0.003, -lr * Pred Grad: -0.024, New P: -0.174
iter 18 loss: 0.951
Actual params: [ 1.7526, -0.1737]
-Original Grad: -0.053, -lr * Pred Grad: -0.023, New P: 1.729
-Original Grad: -0.009, -lr * Pred Grad: -0.025, New P: -0.199
iter 19 loss: 0.941
Actual params: [ 1.7293, -0.199 ]
-Original Grad: -0.078, -lr * Pred Grad: -0.044, New P: 1.685
-Original Grad: -0.002, -lr * Pred Grad: -0.025, New P: -0.224
iter 20 loss: 0.924
Actual params: [ 1.685 , -0.2241]
-Original Grad: -0.106, -lr * Pred Grad: -0.054, New P: 1.631
-Original Grad: -0.005, -lr * Pred Grad: -0.025, New P: -0.249
Target params: [1.1812, 0.2779]
iter 0 loss: 0.319
Actual params: [0.5941, 0.5941]
-Original Grad: -0.054, -lr * Pred Grad: -0.018, New P: 0.576
-Original Grad: -0.014, -lr * Pred Grad: 0.023, New P: 0.617
iter 1 loss: 0.315
Actual params: [0.5758, 0.6168]
-Original Grad: -0.045, -lr * Pred Grad: -0.064, New P: 0.512
-Original Grad: -0.011, -lr * Pred Grad: -0.042, New P: 0.575
iter 2 loss: 0.299
Actual params: [0.5121, 0.575 ]
-Original Grad: 0.028, -lr * Pred Grad: -0.067, New P: 0.445
-Original Grad: 0.003, -lr * Pred Grad: -0.060, New P: 0.515
iter 3 loss: 0.305
Actual params: [0.4451, 0.5146]
-Original Grad: 0.037, -lr * Pred Grad: -0.054, New P: 0.391
-Original Grad: 0.002, -lr * Pred Grad: -0.052, New P: 0.462
iter 4 loss: 0.316
Actual params: [0.3912, 0.4622]
-Original Grad: 0.072, -lr * Pred Grad: -0.008, New P: 0.383
-Original Grad: -0.036, -lr * Pred Grad: -0.038, New P: 0.424
iter 5 loss: 0.316
Actual params: [0.3831, 0.4245]
-Original Grad: 0.049, -lr * Pred Grad: 0.045, New P: 0.428
-Original Grad: -0.022, -lr * Pred Grad: -0.031, New P: 0.394
iter 6 loss: 0.308
Actual params: [0.428 , 0.3936]
-Original Grad: 0.017, -lr * Pred Grad: 0.020, New P: 0.448
-Original Grad: -0.015, -lr * Pred Grad: -0.030, New P: 0.364
iter 7 loss: 0.307
Actual params: [0.448 , 0.3637]
-Original Grad: 0.013, -lr * Pred Grad: 0.009, New P: 0.457
-Original Grad: -0.006, -lr * Pred Grad: -0.025, New P: 0.338
iter 8 loss: 0.307
Actual params: [0.4568, 0.3384]
-Original Grad: -0.025, -lr * Pred Grad: -0.028, New P: 0.429
-Original Grad: 0.004, -lr * Pred Grad: -0.019, New P: 0.320
iter 9 loss: 0.309
Actual params: [0.4291, 0.3198]
-Original Grad: 0.033, -lr * Pred Grad: 0.004, New P: 0.433
-Original Grad: -0.005, -lr * Pred Grad: -0.018, New P: 0.302
iter 10 loss: 0.309
Actual params: [0.4333, 0.3021]
-Original Grad: -0.008, -lr * Pred Grad: -0.009, New P: 0.424
-Original Grad: 0.002, -lr * Pred Grad: -0.018, New P: 0.284
iter 11 loss: 0.309
Actual params: [0.424 , 0.2844]
-Original Grad: -0.011, -lr * Pred Grad: -0.017, New P: 0.407
-Original Grad: -0.000, -lr * Pred Grad: -0.018, New P: 0.266
iter 12 loss: 0.310
Actual params: [0.4065, 0.2663]
-Original Grad: 0.008, -lr * Pred Grad: -0.012, New P: 0.395
-Original Grad: -0.002, -lr * Pred Grad: -0.020, New P: 0.247
iter 13 loss: 0.311
Actual params: [0.3947, 0.2467]
-Original Grad: -0.003, -lr * Pred Grad: -0.011, New P: 0.383
-Original Grad: -0.004, -lr * Pred Grad: -0.022, New P: 0.225
iter 14 loss: 0.312
Actual params: [0.3833, 0.2252]
-Original Grad: 0.007, -lr * Pred Grad: -0.008, New P: 0.375
-Original Grad: -0.007, -lr * Pred Grad: -0.024, New P: 0.201
iter 15 loss: 0.312
Actual params: [0.3752, 0.2013]
-Original Grad: 0.017, -lr * Pred Grad: -0.001, New P: 0.374
-Original Grad: -0.006, -lr * Pred Grad: -0.025, New P: 0.176
iter 16 loss: 0.311
Actual params: [0.3744, 0.1761]
-Original Grad: -0.016, -lr * Pred Grad: -0.013, New P: 0.361
-Original Grad: 0.001, -lr * Pred Grad: -0.023, New P: 0.153
iter 17 loss: 0.311
Actual params: [0.3614, 0.153 ]
-Original Grad: 0.008, -lr * Pred Grad: -0.013, New P: 0.348
-Original Grad: 0.005, -lr * Pred Grad: -0.019, New P: 0.134
iter 18 loss: 0.312
Actual params: [0.3481, 0.1339]
-Original Grad: -0.004, -lr * Pred Grad: -0.012, New P: 0.336
-Original Grad: 0.005, -lr * Pred Grad: -0.016, New P: 0.118
iter 19 loss: 0.313
Actual params: [0.3357, 0.1181]
-Original Grad: 0.012, -lr * Pred Grad: -0.006, New P: 0.330
-Original Grad: 0.001, -lr * Pred Grad: -0.016, New P: 0.102
iter 20 loss: 0.313
Actual params: [0.3301, 0.102 ]
-Original Grad: 0.016, -lr * Pred Grad: 0.003, New P: 0.333
-Original Grad: -0.003, -lr * Pred Grad: -0.019, New P: 0.083
Target params: [1.1812, 0.2779]
iter 0 loss: 0.445
Actual params: [0.5941, 0.5941]
-Original Grad: -0.072, -lr * Pred Grad: -0.033, New P: 0.561
-Original Grad: -0.907, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.326
Actual params: [0.561 , 0.5325]
-Original Grad: -0.163, -lr * Pred Grad: -0.074, New P: 0.487
-Original Grad: -0.865, -lr * Pred Grad: -0.081, New P: 0.452
iter 2 loss: 0.299
Actual params: [0.4872, 0.4519]
-Original Grad: -0.089, -lr * Pred Grad: -0.081, New P: 0.406
-Original Grad: -0.159, -lr * Pred Grad: -0.084, New P: 0.368
iter 3 loss: 0.322
Actual params: [0.4063, 0.3683]
-Original Grad: 0.055, -lr * Pred Grad: -0.073, New P: 0.333
-Original Grad: -0.001, -lr * Pred Grad: -0.084, New P: 0.284
iter 4 loss: 0.344
Actual params: [0.3331, 0.2842]
-Original Grad: 0.154, -lr * Pred Grad: -0.064, New P: 0.269
-Original Grad: -0.004, -lr * Pred Grad: -0.083, New P: 0.201
iter 5 loss: 0.354
Actual params: [0.2692, 0.2013]
-Original Grad: 0.209, -lr * Pred Grad: -0.037, New P: 0.232
-Original Grad: -0.014, -lr * Pred Grad: -0.076, New P: 0.125
iter 6 loss: 0.357
Actual params: [0.2317, 0.1254]
-Original Grad: 0.174, -lr * Pred Grad: 0.009, New P: 0.241
-Original Grad: -0.009, -lr * Pred Grad: -0.070, New P: 0.055
iter 7 loss: 0.350
Actual params: [0.241, 0.055]
-Original Grad: 0.142, -lr * Pred Grad: 0.062, New P: 0.303
-Original Grad: 0.006, -lr * Pred Grad: -0.067, New P: -0.012
iter 8 loss: 0.336
Actual params: [ 0.3028, -0.0124]
-Original Grad: 0.176, -lr * Pred Grad: 0.083, New P: 0.385
-Original Grad: 0.027, -lr * Pred Grad: -0.055, New P: -0.068
iter 9 loss: 0.323
Actual params: [ 0.3855, -0.0678]
-Original Grad: 0.138, -lr * Pred Grad: 0.086, New P: 0.472
-Original Grad: 0.055, -lr * Pred Grad: -0.037, New P: -0.105
iter 10 loss: 0.317
Actual params: [ 0.4716, -0.1048]
-Original Grad: 0.038, -lr * Pred Grad: 0.084, New P: 0.555
-Original Grad: 0.021, -lr * Pred Grad: -0.028, New P: -0.132
iter 11 loss: 0.308
Actual params: [ 0.5551, -0.1324]
-Original Grad: -0.007, -lr * Pred Grad: 0.016, New P: 0.571
-Original Grad: 0.036, -lr * Pred Grad: -0.024, New P: -0.157
iter 12 loss: 0.308
Actual params: [ 0.5706, -0.1568]
-Original Grad: -0.007, -lr * Pred Grad: 0.025, New P: 0.596
-Original Grad: 0.052, -lr * Pred Grad: -0.018, New P: -0.175
iter 13 loss: 0.309
Actual params: [ 0.5958, -0.1749]
-Original Grad: -0.013, -lr * Pred Grad: -0.025, New P: 0.570
-Original Grad: 0.055, -lr * Pred Grad: 0.001, New P: -0.174
iter 14 loss: 0.308
Actual params: [ 0.5704, -0.1742]
-Original Grad: -0.010, -lr * Pred Grad: -0.010, New P: 0.560
-Original Grad: 0.042, -lr * Pred Grad: 0.036, New P: -0.138
iter 15 loss: 0.308
Actual params: [ 0.56  , -0.1385]
-Original Grad: -0.009, -lr * Pred Grad: -0.027, New P: 0.533
-Original Grad: 0.047, -lr * Pred Grad: 0.033, New P: -0.105
iter 16 loss: 0.309
Actual params: [ 0.5332, -0.1051]
-Original Grad: 0.009, -lr * Pred Grad: -0.012, New P: 0.521
-Original Grad: 0.044, -lr * Pred Grad: 0.030, New P: -0.076
iter 17 loss: 0.311
Actual params: [ 0.5208, -0.0756]
-Original Grad: 0.004, -lr * Pred Grad: -0.011, New P: 0.510
-Original Grad: 0.032, -lr * Pred Grad: 0.022, New P: -0.053
iter 18 loss: 0.312
Actual params: [ 0.51  , -0.0534]
-Original Grad: 0.018, -lr * Pred Grad: -0.002, New P: 0.508
-Original Grad: 0.020, -lr * Pred Grad: 0.006, New P: -0.047
iter 19 loss: 0.312
Actual params: [ 0.5083, -0.0473]
-Original Grad: 0.005, -lr * Pred Grad: -0.006, New P: 0.503
-Original Grad: 0.021, -lr * Pred Grad: 0.004, New P: -0.043
iter 20 loss: 0.313
Actual params: [ 0.5027, -0.0429]
-Original Grad: 0.026, -lr * Pred Grad: 0.003, New P: 0.506
-Original Grad: 0.017, -lr * Pred Grad: -0.000, New P: -0.043
Target params: [1.1812, 0.2779]
iter 0 loss: 0.492
Actual params: [0.5941, 0.5941]
-Original Grad: 0.255, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.169, -lr * Pred Grad: -0.061, New P: 0.533
iter 1 loss: 0.414
Actual params: [0.6616, 0.5335]
-Original Grad: 0.346, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.154, -lr * Pred Grad: -0.080, New P: 0.453
iter 2 loss: 0.333
Actual params: [0.7466, 0.4534]
-Original Grad: 0.263, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.082, -lr * Pred Grad: -0.083, New P: 0.371
iter 3 loss: 0.269
Actual params: [0.8344, 0.3706]
-Original Grad: 0.115, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.050, -lr * Pred Grad: -0.080, New P: 0.291
iter 4 loss: 0.208
Actual params: [0.9226, 0.2911]
-Original Grad: 0.064, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.020, -lr * Pred Grad: -0.072, New P: 0.219
iter 5 loss: 0.192
Actual params: [1.0109, 0.2191]
-Original Grad: 0.014, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: -0.002, -lr * Pred Grad: -0.069, New P: 0.150
iter 6 loss: 0.196
Actual params: [1.0992, 0.1503]
-Original Grad: 0.006, -lr * Pred Grad: 0.088, New P: 1.187
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: 0.088
iter 7 loss: 0.205
Actual params: [1.1872, 0.0879]
-Original Grad: -0.003, -lr * Pred Grad: 0.087, New P: 1.274
-Original Grad: 0.015, -lr * Pred Grad: -0.046, New P: 0.042
iter 8 loss: 0.223
Actual params: [1.2741, 0.0421]
-Original Grad: -0.003, -lr * Pred Grad: 0.075, New P: 1.349
-Original Grad: 0.015, -lr * Pred Grad: -0.033, New P: 0.009
iter 9 loss: 0.245
Actual params: [1.3487, 0.0091]
-Original Grad: -0.032, -lr * Pred Grad: -0.006, New P: 1.343
-Original Grad: 0.023, -lr * Pred Grad: -0.024, New P: -0.015
iter 10 loss: 0.243
Actual params: [ 1.3426, -0.0147]
-Original Grad: -0.026, -lr * Pred Grad: 0.035, New P: 1.377
-Original Grad: -0.004, -lr * Pred Grad: -0.015, New P: -0.030
iter 11 loss: 0.247
Actual params: [ 1.3771, -0.0295]
-Original Grad: -0.030, -lr * Pred Grad: 0.026, New P: 1.403
-Original Grad: 0.005, -lr * Pred Grad: -0.006, New P: -0.035
iter 12 loss: 0.254
Actual params: [ 1.4029, -0.0352]
-Original Grad: -0.022, -lr * Pred Grad: -0.003, New P: 1.400
-Original Grad: -0.001, -lr * Pred Grad: -0.021, New P: -0.056
iter 13 loss: 0.253
Actual params: [ 1.4   , -0.0559]
-Original Grad: -0.021, -lr * Pred Grad: -0.021, New P: 1.379
-Original Grad: -0.000, -lr * Pred Grad: -0.021, New P: -0.077
iter 14 loss: 0.248
Actual params: [ 1.3793, -0.0767]
-Original Grad: -0.025, -lr * Pred Grad: -0.032, New P: 1.347
-Original Grad: 0.007, -lr * Pred Grad: -0.018, New P: -0.095
iter 15 loss: 0.244
Actual params: [ 1.3471, -0.095 ]
-Original Grad: -0.016, -lr * Pred Grad: -0.028, New P: 1.319
-Original Grad: -0.003, -lr * Pred Grad: -0.020, New P: -0.115
iter 16 loss: 0.235
Actual params: [ 1.3193, -0.1153]
-Original Grad: -0.011, -lr * Pred Grad: -0.022, New P: 1.298
-Original Grad: -0.006, -lr * Pred Grad: -0.025, New P: -0.140
iter 17 loss: 0.229
Actual params: [ 1.2977, -0.1399]
-Original Grad: -0.023, -lr * Pred Grad: -0.024, New P: 1.273
-Original Grad: 0.006, -lr * Pred Grad: -0.023, New P: -0.163
iter 18 loss: 0.223
Actual params: [ 1.2733, -0.1627]
-Original Grad: -0.011, -lr * Pred Grad: -0.023, New P: 1.251
-Original Grad: -0.003, -lr * Pred Grad: -0.022, New P: -0.185
iter 19 loss: 0.219
Actual params: [ 1.2505, -0.1848]
-Original Grad: -0.006, -lr * Pred Grad: -0.018, New P: 1.232
-Original Grad: 0.020, -lr * Pred Grad: -0.014, New P: -0.199
iter 20 loss: 0.216
Actual params: [ 1.2323, -0.1989]
-Original Grad: -0.003, -lr * Pred Grad: -0.014, New P: 1.218
-Original Grad: 0.007, -lr * Pred Grad: -0.011, New P: -0.210
Target params: [1.1812, 0.2779]
iter 0 loss: 0.773
Actual params: [0.5941, 0.5941]
-Original Grad: 0.220, -lr * Pred Grad: 0.067, New P: 0.662
-Original Grad: -0.200, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.626
Actual params: [0.6615, 0.532 ]
-Original Grad: 0.146, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.105, -lr * Pred Grad: -0.080, New P: 0.452
iter 2 loss: 0.500
Actual params: [0.7461, 0.4517]
-Original Grad: 0.057, -lr * Pred Grad: 0.087, New P: 0.833
-Original Grad: -0.067, -lr * Pred Grad: -0.082, New P: 0.369
iter 3 loss: 0.451
Actual params: [0.8331, 0.3694]
-Original Grad: 0.037, -lr * Pred Grad: 0.085, New P: 0.918
-Original Grad: -0.028, -lr * Pred Grad: -0.076, New P: 0.293
iter 4 loss: 0.428
Actual params: [0.9184, 0.293 ]
-Original Grad: 0.019, -lr * Pred Grad: 0.037, New P: 0.956
-Original Grad: -0.020, -lr * Pred Grad: -0.070, New P: 0.222
iter 5 loss: 0.411
Actual params: [0.9557, 0.2225]
-Original Grad: -0.004, -lr * Pred Grad: 0.028, New P: 0.984
-Original Grad: -0.014, -lr * Pred Grad: -0.068, New P: 0.155
iter 6 loss: 0.398
Actual params: [0.9836, 0.1548]
-Original Grad: 0.012, -lr * Pred Grad: 0.017, New P: 1.000
-Original Grad: -0.011, -lr * Pred Grad: -0.061, New P: 0.094
iter 7 loss: 0.396
Actual params: [1.0003, 0.094 ]
-Original Grad: 0.013, -lr * Pred Grad: 0.022, New P: 1.022
-Original Grad: 0.004, -lr * Pred Grad: -0.046, New P: 0.048
iter 8 loss: 0.392
Actual params: [1.0219, 0.0478]
-Original Grad: 0.008, -lr * Pred Grad: -0.009, New P: 1.013
-Original Grad: 0.010, -lr * Pred Grad: -0.035, New P: 0.013
iter 9 loss: 0.387
Actual params: [1.013 , 0.0132]
-Original Grad: 0.011, -lr * Pred Grad: -0.004, New P: 1.009
-Original Grad: 0.022, -lr * Pred Grad: -0.024, New P: -0.010
iter 10 loss: 0.383
Actual params: [ 1.0087, -0.0105]
-Original Grad: 0.016, -lr * Pred Grad: -0.008, New P: 1.001
-Original Grad: 0.029, -lr * Pred Grad: -0.006, New P: -0.017
iter 11 loss: 0.382
Actual params: [ 1.0007, -0.0166]
-Original Grad: 0.012, -lr * Pred Grad: -0.004, New P: 0.997
-Original Grad: 0.036, -lr * Pred Grad: 0.019, New P: 0.003
iter 12 loss: 0.385
Actual params: [0.9971, 0.0028]
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: 0.990
-Original Grad: 0.023, -lr * Pred Grad: 0.012, New P: 0.014
iter 13 loss: 0.387
Actual params: [0.9896, 0.0144]
-Original Grad: 0.023, -lr * Pred Grad: 0.001, New P: 0.991
-Original Grad: 0.015, -lr * Pred Grad: -0.002, New P: 0.012
iter 14 loss: 0.387
Actual params: [0.9907, 0.0123]
-Original Grad: 0.020, -lr * Pred Grad: 0.005, New P: 0.995
-Original Grad: 0.038, -lr * Pred Grad: 0.010, New P: 0.022
iter 15 loss: 0.388
Actual params: [0.9953, 0.0223]
-Original Grad: 0.007, -lr * Pred Grad: -0.002, New P: 0.993
-Original Grad: 0.013, -lr * Pred Grad: 0.001, New P: 0.023
iter 16 loss: 0.389
Actual params: [0.9932, 0.0232]
-Original Grad: 0.016, -lr * Pred Grad: -0.001, New P: 0.992
-Original Grad: 0.007, -lr * Pred Grad: -0.010, New P: 0.013
iter 17 loss: 0.387
Actual params: [0.9925, 0.0133]
-Original Grad: 0.013, -lr * Pred Grad: 0.000, New P: 0.993
-Original Grad: 0.024, -lr * Pred Grad: -0.007, New P: 0.006
iter 18 loss: 0.386
Actual params: [0.9929, 0.0063]
-Original Grad: 0.014, -lr * Pred Grad: 0.001, New P: 0.994
-Original Grad: 0.021, -lr * Pred Grad: -0.003, New P: 0.004
iter 19 loss: 0.385
Actual params: [0.9941, 0.0037]
-Original Grad: 0.013, -lr * Pred Grad: 0.001, New P: 0.996
-Original Grad: 0.017, -lr * Pred Grad: -0.004, New P: 0.000
iter 20 loss: 0.385
Actual params: [9.9553e-01, 2.3199e-04]
-Original Grad: 0.004, -lr * Pred Grad: -0.003, New P: 0.992
-Original Grad: 0.019, -lr * Pred Grad: -0.005, New P: -0.004
Target params: [1.1812, 0.2779]
iter 0 loss: 0.860
Actual params: [0.5941, 0.5941]
-Original Grad: 0.593, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: 0.253, -lr * Pred Grad: 0.068, New P: 0.662
iter 1 loss: 0.687
Actual params: [0.6617, 0.6616]
-Original Grad: 0.954, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: 0.256, -lr * Pred Grad: 0.085, New P: 0.746
iter 2 loss: 0.365
Actual params: [0.7469, 0.7465]
-Original Grad: 0.221, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.004, -lr * Pred Grad: 0.087, New P: 0.834
iter 3 loss: 0.413
Actual params: [0.8348, 0.834 ]
-Original Grad: -0.242, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.187, -lr * Pred Grad: 0.080, New P: 0.914
iter 4 loss: 0.830
Actual params: [0.923 , 0.9139]
-Original Grad: -0.325, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.104, -lr * Pred Grad: -0.033, New P: 0.881
iter 5 loss: 0.956
Actual params: [1.0112, 0.881 ]
-Original Grad: -0.158, -lr * Pred Grad: 0.086, New P: 1.097
-Original Grad: -0.156, -lr * Pred Grad: -0.055, New P: 0.826
iter 6 loss: 0.931
Actual params: [1.0969, 0.8264]
-Original Grad: -0.165, -lr * Pred Grad: -0.002, New P: 1.095
-Original Grad: -0.282, -lr * Pred Grad: -0.063, New P: 0.764
iter 7 loss: 0.809
Actual params: [1.0948, 0.7638]
-Original Grad: -0.100, -lr * Pred Grad: -0.036, New P: 1.059
-Original Grad: -0.300, -lr * Pred Grad: -0.073, New P: 0.691
iter 8 loss: 0.627
Actual params: [1.0589, 0.6906]
-Original Grad: -0.134, -lr * Pred Grad: -0.043, New P: 1.016
-Original Grad: -0.295, -lr * Pred Grad: -0.080, New P: 0.611
iter 9 loss: 0.371
Actual params: [1.016 , 0.6106]
-Original Grad: -0.117, -lr * Pred Grad: -0.047, New P: 0.969
-Original Grad: -0.282, -lr * Pred Grad: -0.083, New P: 0.528
iter 10 loss: 0.177
Actual params: [0.9694, 0.5277]
-Original Grad: -0.054, -lr * Pred Grad: -0.048, New P: 0.922
-Original Grad: -0.175, -lr * Pred Grad: -0.084, New P: 0.444
iter 11 loss: 0.251
Actual params: [0.9218, 0.4441]
-Original Grad: 0.147, -lr * Pred Grad: -0.004, New P: 0.918
-Original Grad: 0.019, -lr * Pred Grad: -0.081, New P: 0.363
iter 12 loss: 0.405
Actual params: [0.9177, 0.3629]
-Original Grad: 0.284, -lr * Pred Grad: 0.039, New P: 0.957
-Original Grad: 0.181, -lr * Pred Grad: -0.072, New P: 0.291
iter 13 loss: 0.436
Actual params: [0.957 , 0.2907]
-Original Grad: 0.198, -lr * Pred Grad: 0.072, New P: 1.029
-Original Grad: 0.143, -lr * Pred Grad: -0.066, New P: 0.225
iter 14 loss: 0.397
Actual params: [1.0286, 0.2247]
-Original Grad: 0.180, -lr * Pred Grad: 0.085, New P: 1.113
-Original Grad: 0.187, -lr * Pred Grad: -0.050, New P: 0.175
iter 15 loss: 0.377
Actual params: [1.1132, 0.1751]
-Original Grad: 0.026, -lr * Pred Grad: 0.086, New P: 1.199
-Original Grad: 0.161, -lr * Pred Grad: -0.033, New P: 0.142
iter 16 loss: 0.420
Actual params: [1.1993, 0.1423]
-Original Grad: -0.138, -lr * Pred Grad: -0.012, New P: 1.187
-Original Grad: 0.183, -lr * Pred Grad: -0.022, New P: 0.120
iter 17 loss: 0.443
Actual params: [1.1869, 0.1199]
-Original Grad: -0.115, -lr * Pred Grad: -0.050, New P: 1.137
-Original Grad: 0.175, -lr * Pred Grad: -0.015, New P: 0.105
iter 18 loss: 0.455
Actual params: [1.1369, 0.1051]
-Original Grad: -0.032, -lr * Pred Grad: -0.048, New P: 1.089
-Original Grad: 0.183, -lr * Pred Grad: -0.003, New P: 0.102
iter 19 loss: 0.475
Actual params: [1.0887, 0.1017]
-Original Grad: 0.078, -lr * Pred Grad: -0.007, New P: 1.082
-Original Grad: 0.157, -lr * Pred Grad: 0.030, New P: 0.132
iter 20 loss: 0.443
Actual params: [1.0816, 0.1315]
-Original Grad: 0.117, -lr * Pred Grad: 0.045, New P: 1.127
-Original Grad: 0.184, -lr * Pred Grad: 0.069, New P: 0.201
Target params: [1.1812, 0.2779]
iter 0 loss: 0.817
Actual params: [0.5941, 0.5941]
-Original Grad: 0.028, -lr * Pred Grad: 0.051, New P: 0.645
-Original Grad: -0.429, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.778
Actual params: [0.6453, 0.5311]
-Original Grad: -0.004, -lr * Pred Grad: -0.009, New P: 0.636
-Original Grad: -0.354, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.744
Actual params: [0.6363, 0.4506]
-Original Grad: 0.021, -lr * Pred Grad: -0.038, New P: 0.599
-Original Grad: -0.286, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.717
Actual params: [0.5987, 0.367 ]
-Original Grad: 0.008, -lr * Pred Grad: -0.021, New P: 0.578
-Original Grad: -0.239, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.691
Actual params: [0.5779, 0.2828]
-Original Grad: 0.051, -lr * Pred Grad: 0.028, New P: 0.606
-Original Grad: -0.128, -lr * Pred Grad: -0.084, New P: 0.198
iter 5 loss: 0.642
Actual params: [0.606 , 0.1984]
-Original Grad: 0.045, -lr * Pred Grad: 0.042, New P: 0.648
-Original Grad: -0.101, -lr * Pred Grad: -0.084, New P: 0.114
iter 6 loss: 0.602
Actual params: [0.6476, 0.1141]
-Original Grad: 0.049, -lr * Pred Grad: 0.038, New P: 0.685
-Original Grad: -0.052, -lr * Pred Grad: -0.084, New P: 0.030
iter 7 loss: 0.560
Actual params: [0.6854, 0.0303]
-Original Grad: 0.006, -lr * Pred Grad: 0.004, New P: 0.690
-Original Grad: -0.031, -lr * Pred Grad: -0.080, New P: -0.049
iter 8 loss: 0.525
Actual params: [ 0.6895, -0.0493]
-Original Grad: 0.022, -lr * Pred Grad: 0.011, New P: 0.700
-Original Grad: 0.001, -lr * Pred Grad: -0.072, New P: -0.121
iter 9 loss: 0.493
Actual params: [ 0.7004, -0.121 ]
-Original Grad: 0.001, -lr * Pred Grad: -0.011, New P: 0.690
-Original Grad: -0.017, -lr * Pred Grad: -0.069, New P: -0.190
iter 10 loss: 0.462
Actual params: [ 0.6896, -0.19  ]
-Original Grad: -0.000, -lr * Pred Grad: -0.009, New P: 0.681
-Original Grad: 0.009, -lr * Pred Grad: -0.063, New P: -0.253
iter 11 loss: 0.438
Actual params: [ 0.681 , -0.2532]
-Original Grad: -0.031, -lr * Pred Grad: -0.026, New P: 0.655
-Original Grad: 0.009, -lr * Pred Grad: -0.048, New P: -0.301
iter 12 loss: 0.423
Actual params: [ 0.6547, -0.3008]
-Original Grad: -0.105, -lr * Pred Grad: -0.045, New P: 0.609
-Original Grad: 0.053, -lr * Pred Grad: -0.033, New P: -0.334
iter 13 loss: 0.411
Actual params: [ 0.6094, -0.3341]
-Original Grad: -0.033, -lr * Pred Grad: -0.048, New P: 0.561
-Original Grad: 0.046, -lr * Pred Grad: -0.026, New P: -0.360
iter 14 loss: 0.400
Actual params: [ 0.5613, -0.3604]
-Original Grad: -0.042, -lr * Pred Grad: -0.043, New P: 0.519
-Original Grad: 0.045, -lr * Pred Grad: -0.025, New P: -0.385
iter 15 loss: 0.389
Actual params: [ 0.5188, -0.3849]
-Original Grad: -0.027, -lr * Pred Grad: -0.034, New P: 0.485
-Original Grad: 0.037, -lr * Pred Grad: -0.022, New P: -0.407
iter 16 loss: 0.374
Actual params: [ 0.4849, -0.4071]
-Original Grad: 0.000, -lr * Pred Grad: -0.021, New P: 0.464
-Original Grad: 0.054, -lr * Pred Grad: -0.020, New P: -0.428
iter 17 loss: 0.358
Actual params: [ 0.4639, -0.4275]
-Original Grad: -0.015, -lr * Pred Grad: -0.016, New P: 0.448
-Original Grad: 0.053, -lr * Pred Grad: -0.014, New P: -0.441
iter 18 loss: 0.346
Actual params: [ 0.4477, -0.4412]
-Original Grad: -0.014, -lr * Pred Grad: -0.018, New P: 0.430
-Original Grad: 0.049, -lr * Pred Grad: 0.011, New P: -0.431
iter 19 loss: 0.358
Actual params: [ 0.4298, -0.4306]
-Original Grad: -0.006, -lr * Pred Grad: -0.018, New P: 0.412
-Original Grad: 0.062, -lr * Pred Grad: 0.044, New P: -0.386
iter 20 loss: 0.398
Actual params: [ 0.4119, -0.3864]
-Original Grad: 0.001, -lr * Pred Grad: -0.014, New P: 0.398
-Original Grad: 0.029, -lr * Pred Grad: 0.021, New P: -0.365
Target params: [1.1812, 0.2779]
iter 0 loss: 0.220
Actual params: [0.5941, 0.5941]
-Original Grad: -0.108, -lr * Pred Grad: -0.051, New P: 0.543
-Original Grad: -0.041, -lr * Pred Grad: -0.005, New P: 0.589
iter 1 loss: 0.179
Actual params: [0.5433, 0.5891]
-Original Grad: -0.092, -lr * Pred Grad: -0.076, New P: 0.467
-Original Grad: -0.029, -lr * Pred Grad: -0.057, New P: 0.532
iter 2 loss: 0.169
Actual params: [0.4671, 0.5316]
-Original Grad: 0.027, -lr * Pred Grad: -0.073, New P: 0.394
-Original Grad: 0.013, -lr * Pred Grad: -0.065, New P: 0.466
iter 3 loss: 0.292
Actual params: [0.3944, 0.4663]
-Original Grad: 0.247, -lr * Pred Grad: -0.059, New P: 0.336
-Original Grad: 0.192, -lr * Pred Grad: -0.029, New P: 0.437
iter 4 loss: 0.348
Actual params: [0.3359, 0.4372]
-Original Grad: 0.223, -lr * Pred Grad: -0.017, New P: 0.319
-Original Grad: 0.219, -lr * Pred Grad: 0.037, New P: 0.475
iter 5 loss: 0.338
Actual params: [0.319 , 0.4745]
-Original Grad: 0.217, -lr * Pred Grad: 0.046, New P: 0.365
-Original Grad: 0.219, -lr * Pred Grad: 0.077, New P: 0.552
iter 6 loss: 0.265
Actual params: [0.3653, 0.5517]
-Original Grad: 0.162, -lr * Pred Grad: 0.080, New P: 0.445
-Original Grad: 0.103, -lr * Pred Grad: 0.086, New P: 0.637
iter 7 loss: 0.147
Actual params: [0.445 , 0.6372]
-Original Grad: 0.006, -lr * Pred Grad: 0.084, New P: 0.529
-Original Grad: 0.001, -lr * Pred Grad: 0.081, New P: 0.718
iter 8 loss: 0.196
Actual params: [0.5291, 0.7182]
-Original Grad: -0.110, -lr * Pred Grad: -0.019, New P: 0.510
-Original Grad: -0.015, -lr * Pred Grad: -0.003, New P: 0.715
iter 9 loss: 0.181
Actual params: [0.5098, 0.7155]
-Original Grad: -0.090, -lr * Pred Grad: -0.048, New P: 0.462
-Original Grad: -0.012, -lr * Pred Grad: 0.018, New P: 0.734
iter 10 loss: 0.154
Actual params: [0.4621, 0.7338]
-Original Grad: -0.028, -lr * Pred Grad: -0.043, New P: 0.419
-Original Grad: -0.003, -lr * Pred Grad: -0.022, New P: 0.712
iter 11 loss: 0.160
Actual params: [0.4193, 0.7122]
-Original Grad: 0.016, -lr * Pred Grad: -0.014, New P: 0.406
-Original Grad: -0.003, -lr * Pred Grad: -0.004, New P: 0.709
iter 12 loss: 0.178
Actual params: [0.4055, 0.7085]
-Original Grad: 0.050, -lr * Pred Grad: 0.024, New P: 0.430
-Original Grad: -0.001, -lr * Pred Grad: -0.022, New P: 0.687
iter 13 loss: 0.153
Actual params: [0.4299, 0.687 ]
-Original Grad: -0.005, -lr * Pred Grad: -0.005, New P: 0.425
-Original Grad: -0.002, -lr * Pred Grad: -0.015, New P: 0.672
iter 14 loss: 0.161
Actual params: [0.4248, 0.6719]
-Original Grad: 0.055, -lr * Pred Grad: 0.028, New P: 0.453
-Original Grad: 0.002, -lr * Pred Grad: -0.014, New P: 0.658
iter 15 loss: 0.144
Actual params: [0.4529, 0.6576]
-Original Grad: 0.013, -lr * Pred Grad: 0.004, New P: 0.457
-Original Grad: -0.001, -lr * Pred Grad: -0.013, New P: 0.645
iter 16 loss: 0.144
Actual params: [0.4571, 0.6447]
-Original Grad: -0.006, -lr * Pred Grad: -0.008, New P: 0.449
-Original Grad: -0.001, -lr * Pred Grad: -0.013, New P: 0.631
iter 17 loss: 0.146
Actual params: [0.4492, 0.6314]
-Original Grad: 0.022, -lr * Pred Grad: -0.004, New P: 0.446
-Original Grad: 0.002, -lr * Pred Grad: -0.012, New P: 0.619
iter 18 loss: 0.150
Actual params: [0.4456, 0.6192]
-Original Grad: -0.011, -lr * Pred Grad: -0.014, New P: 0.432
-Original Grad: 0.001, -lr * Pred Grad: -0.011, New P: 0.608
iter 19 loss: 0.169
Actual params: [0.4318, 0.6078]
-Original Grad: 0.061, -lr * Pred Grad: 0.017, New P: 0.448
-Original Grad: 0.010, -lr * Pred Grad: -0.007, New P: 0.601
iter 20 loss: 0.153
Actual params: [0.4484, 0.6005]
-Original Grad: 0.034, -lr * Pred Grad: 0.027, New P: 0.476
-Original Grad: 0.002, -lr * Pred Grad: -0.008, New P: 0.593
Target params: [1.1812, 0.2779]
iter 0 loss: 0.338
Actual params: [0.5941, 0.5941]
-Original Grad: 0.113, -lr * Pred Grad: 0.066, New P: 0.660
-Original Grad: -0.197, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.310
Actual params: [0.6598, 0.5321]
-Original Grad: 0.056, -lr * Pred Grad: 0.082, New P: 0.742
-Original Grad: -0.236, -lr * Pred Grad: -0.080, New P: 0.452
iter 2 loss: 0.281
Actual params: [0.7416, 0.4518]
-Original Grad: 0.035, -lr * Pred Grad: 0.056, New P: 0.798
-Original Grad: -0.086, -lr * Pred Grad: -0.083, New P: 0.368
iter 3 loss: 0.266
Actual params: [0.798 , 0.3684]
-Original Grad: 0.021, -lr * Pred Grad: 0.005, New P: 0.803
-Original Grad: -0.033, -lr * Pred Grad: -0.082, New P: 0.286
iter 4 loss: 0.266
Actual params: [0.8026, 0.286 ]
-Original Grad: 0.005, -lr * Pred Grad: 0.025, New P: 0.827
-Original Grad: -0.035, -lr * Pred Grad: -0.076, New P: 0.210
iter 5 loss: 0.267
Actual params: [0.8272, 0.2098]
-Original Grad: -0.001, -lr * Pred Grad: -0.013, New P: 0.814
-Original Grad: -0.006, -lr * Pred Grad: -0.070, New P: 0.140
iter 6 loss: 0.274
Actual params: [0.8144, 0.1395]
-Original Grad: 0.006, -lr * Pred Grad: 0.006, New P: 0.820
-Original Grad: 0.013, -lr * Pred Grad: -0.067, New P: 0.072
iter 7 loss: 0.278
Actual params: [0.8204, 0.0723]
-Original Grad: 0.010, -lr * Pred Grad: -0.013, New P: 0.807
-Original Grad: 0.020, -lr * Pred Grad: -0.055, New P: 0.018
iter 8 loss: 0.284
Actual params: [0.8071, 0.0176]
-Original Grad: 0.005, -lr * Pred Grad: -0.005, New P: 0.802
-Original Grad: 0.014, -lr * Pred Grad: -0.038, New P: -0.020
iter 9 loss: 0.286
Actual params: [ 0.8019, -0.0204]
-Original Grad: 0.013, -lr * Pred Grad: -0.006, New P: 0.796
-Original Grad: 0.013, -lr * Pred Grad: -0.029, New P: -0.050
iter 10 loss: 0.288
Actual params: [ 0.7958, -0.0497]
-Original Grad: 0.027, -lr * Pred Grad: 0.006, New P: 0.802
-Original Grad: 0.023, -lr * Pred Grad: -0.023, New P: -0.073
iter 11 loss: 0.287
Actual params: [ 0.8022, -0.073 ]
-Original Grad: 0.035, -lr * Pred Grad: 0.016, New P: 0.818
-Original Grad: 0.015, -lr * Pred Grad: -0.010, New P: -0.083
iter 12 loss: 0.285
Actual params: [ 0.8184, -0.0833]
-Original Grad: 0.024, -lr * Pred Grad: 0.013, New P: 0.831
-Original Grad: 0.003, -lr * Pred Grad: 0.001, New P: -0.082
iter 13 loss: 0.284
Actual params: [ 0.8313, -0.0818]
-Original Grad: 0.030, -lr * Pred Grad: 0.013, New P: 0.844
-Original Grad: 0.025, -lr * Pred Grad: -0.007, New P: -0.088
iter 14 loss: 0.283
Actual params: [ 0.844 , -0.0883]
-Original Grad: 0.006, -lr * Pred Grad: -0.001, New P: 0.843
-Original Grad: 0.015, -lr * Pred Grad: -0.003, New P: -0.091
iter 15 loss: 0.283
Actual params: [ 0.8429, -0.0912]
-Original Grad: 0.015, -lr * Pred Grad: -0.002, New P: 0.841
-Original Grad: 0.030, -lr * Pred Grad: 0.003, New P: -0.089
iter 16 loss: 0.283
Actual params: [ 0.8406, -0.0886]
-Original Grad: 0.005, -lr * Pred Grad: -0.004, New P: 0.836
-Original Grad: 0.020, -lr * Pred Grad: 0.001, New P: -0.087
iter 17 loss: 0.283
Actual params: [ 0.8364, -0.0873]
-Original Grad: 0.008, -lr * Pred Grad: -0.004, New P: 0.832
-Original Grad: 0.030, -lr * Pred Grad: 0.004, New P: -0.083
iter 18 loss: 0.284
Actual params: [ 0.8322, -0.0835]
-Original Grad: 0.004, -lr * Pred Grad: -0.005, New P: 0.827
-Original Grad: 0.018, -lr * Pred Grad: -0.001, New P: -0.084
iter 19 loss: 0.284
Actual params: [ 0.8273, -0.0844]
-Original Grad: 0.002, -lr * Pred Grad: -0.006, New P: 0.821
-Original Grad: 0.021, -lr * Pred Grad: -0.005, New P: -0.089
iter 20 loss: 0.285
Actual params: [ 0.8209, -0.0892]
-Original Grad: 0.015, -lr * Pred Grad: -0.001, New P: 0.820
-Original Grad: 0.013, -lr * Pred Grad: -0.010, New P: -0.099
Target params: [1.1812, 0.2779]
iter 0 loss: 0.336
Actual params: [0.5941, 0.5941]
-Original Grad: 0.326, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.242, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.263
Actual params: [0.6617, 0.5312]
-Original Grad: 0.123, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.067, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.181
Actual params: [0.7465, 0.4508]
-Original Grad: -0.006, -lr * Pred Grad: 0.087, New P: 0.834
-Original Grad: 0.023, -lr * Pred Grad: -0.079, New P: 0.372
iter 3 loss: 0.146
Actual params: [0.8338, 0.3719]
-Original Grad: 0.009, -lr * Pred Grad: 0.086, New P: 0.920
-Original Grad: 0.012, -lr * Pred Grad: -0.071, New P: 0.301
iter 4 loss: 0.138
Actual params: [0.9201, 0.3006]
-Original Grad: 0.020, -lr * Pred Grad: 0.059, New P: 0.979
-Original Grad: 0.006, -lr * Pred Grad: -0.068, New P: 0.233
iter 5 loss: 0.129
Actual params: [0.9794, 0.233 ]
-Original Grad: 0.036, -lr * Pred Grad: 0.037, New P: 1.017
-Original Grad: 0.017, -lr * Pred Grad: -0.056, New P: 0.177
iter 6 loss: 0.125
Actual params: [1.0165, 0.1774]
-Original Grad: 0.056, -lr * Pred Grad: 0.069, New P: 1.085
-Original Grad: 0.015, -lr * Pred Grad: -0.036, New P: 0.141
iter 7 loss: 0.117
Actual params: [1.085 , 0.1412]
-Original Grad: 0.037, -lr * Pred Grad: 0.053, New P: 1.138
-Original Grad: -0.003, -lr * Pred Grad: -0.019, New P: 0.122
iter 8 loss: 0.114
Actual params: [1.1376, 0.1224]
-Original Grad: 0.017, -lr * Pred Grad: 0.037, New P: 1.174
-Original Grad: 0.012, -lr * Pred Grad: 0.002, New P: 0.125
iter 9 loss: 0.115
Actual params: [1.1742, 0.1247]
-Original Grad: 0.000, -lr * Pred Grad: -0.011, New P: 1.163
-Original Grad: 0.005, -lr * Pred Grad: -0.010, New P: 0.114
iter 10 loss: 0.114
Actual params: [1.1633, 0.1144]
-Original Grad: 0.017, -lr * Pred Grad: 0.007, New P: 1.170
-Original Grad: 0.005, -lr * Pred Grad: -0.014, New P: 0.100
iter 11 loss: 0.114
Actual params: [1.1704, 0.1003]
-Original Grad: -0.007, -lr * Pred Grad: -0.022, New P: 1.148
-Original Grad: 0.009, -lr * Pred Grad: -0.013, New P: 0.087
iter 12 loss: 0.113
Actual params: [1.1482, 0.0872]
-Original Grad: 0.017, -lr * Pred Grad: -0.003, New P: 1.146
-Original Grad: 0.021, -lr * Pred Grad: -0.006, New P: 0.081
iter 13 loss: 0.112
Actual params: [1.1457, 0.0814]
-Original Grad: 0.022, -lr * Pred Grad: 0.001, New P: 1.146
-Original Grad: 0.028, -lr * Pred Grad: 0.004, New P: 0.085
iter 14 loss: 0.112
Actual params: [1.1464, 0.085 ]
-Original Grad: 0.011, -lr * Pred Grad: -0.001, New P: 1.145
-Original Grad: -0.000, -lr * Pred Grad: -0.008, New P: 0.077
iter 15 loss: 0.112
Actual params: [1.1454, 0.077 ]
-Original Grad: 0.017, -lr * Pred Grad: -0.000, New P: 1.145
-Original Grad: 0.026, -lr * Pred Grad: -0.007, New P: 0.070
iter 16 loss: 0.112
Actual params: [1.145 , 0.0698]
-Original Grad: 0.010, -lr * Pred Grad: -0.003, New P: 1.142
-Original Grad: 0.009, -lr * Pred Grad: -0.010, New P: 0.060
iter 17 loss: 0.111
Actual params: [1.142 , 0.0603]
-Original Grad: 0.025, -lr * Pred Grad: 0.005, New P: 1.147
-Original Grad: 0.012, -lr * Pred Grad: -0.012, New P: 0.048
iter 18 loss: 0.111
Actual params: [1.1465, 0.0478]
-Original Grad: 0.010, -lr * Pred Grad: 0.001, New P: 1.147
-Original Grad: 0.004, -lr * Pred Grad: -0.017, New P: 0.031
iter 19 loss: 0.110
Actual params: [1.1472, 0.031 ]
-Original Grad: 0.013, -lr * Pred Grad: -0.001, New P: 1.146
-Original Grad: 0.005, -lr * Pred Grad: -0.019, New P: 0.012
iter 20 loss: 0.110
Actual params: [1.146 , 0.0123]
-Original Grad: 0.019, -lr * Pred Grad: 0.003, New P: 1.149
-Original Grad: 0.012, -lr * Pred Grad: -0.015, New P: -0.003
Target params: [1.1812, 0.2779]
iter 0 loss: 0.118
Actual params: [0.5941, 0.5941]
-Original Grad: -0.001, -lr * Pred Grad: 0.034, New P: 0.628
-Original Grad: -0.034, -lr * Pred Grad: 0.002, New P: 0.597
iter 1 loss: 0.113
Actual params: [0.6282, 0.5965]
-Original Grad: 0.008, -lr * Pred Grad: -0.024, New P: 0.604
-Original Grad: -0.037, -lr * Pred Grad: -0.057, New P: 0.540
iter 2 loss: 0.114
Actual params: [0.6041, 0.5398]
-Original Grad: -0.003, -lr * Pred Grad: -0.053, New P: 0.551
-Original Grad: -0.030, -lr * Pred Grad: -0.068, New P: 0.472
iter 3 loss: 0.118
Actual params: [0.5513, 0.4721]
-Original Grad: 0.006, -lr * Pred Grad: -0.043, New P: 0.508
-Original Grad: -0.006, -lr * Pred Grad: -0.065, New P: 0.407
iter 4 loss: 0.124
Actual params: [0.508 , 0.4071]
-Original Grad: 0.010, -lr * Pred Grad: -0.007, New P: 0.501
-Original Grad: 0.001, -lr * Pred Grad: -0.051, New P: 0.356
iter 5 loss: 0.125
Actual params: [0.5011, 0.3557]
-Original Grad: 0.007, -lr * Pred Grad: 0.007, New P: 0.508
-Original Grad: 0.008, -lr * Pred Grad: -0.021, New P: 0.335
iter 6 loss: 0.123
Actual params: [0.5079, 0.3347]
-Original Grad: -0.005, -lr * Pred Grad: -0.012, New P: 0.495
-Original Grad: 0.016, -lr * Pred Grad: 0.009, New P: 0.343
iter 7 loss: 0.126
Actual params: [0.4955, 0.3434]
-Original Grad: 0.003, -lr * Pred Grad: -0.008, New P: 0.487
-Original Grad: 0.009, -lr * Pred Grad: -0.004, New P: 0.339
iter 8 loss: 0.127
Actual params: [0.4871, 0.339 ]
-Original Grad: -0.003, -lr * Pred Grad: -0.013, New P: 0.474
-Original Grad: 0.007, -lr * Pred Grad: -0.006, New P: 0.333
iter 9 loss: 0.130
Actual params: [0.4737, 0.3333]
-Original Grad: 0.010, -lr * Pred Grad: -0.007, New P: 0.467
-Original Grad: 0.003, -lr * Pred Grad: -0.013, New P: 0.320
iter 10 loss: 0.131
Actual params: [0.4671, 0.3202]
-Original Grad: 0.005, -lr * Pred Grad: -0.005, New P: 0.462
-Original Grad: 0.005, -lr * Pred Grad: -0.015, New P: 0.305
iter 11 loss: 0.132
Actual params: [0.4616, 0.3054]
-Original Grad: -0.003, -lr * Pred Grad: -0.010, New P: 0.451
-Original Grad: 0.014, -lr * Pred Grad: -0.011, New P: 0.295
iter 12 loss: 0.134
Actual params: [0.4512, 0.2947]
-Original Grad: -0.006, -lr * Pred Grad: -0.015, New P: 0.436
-Original Grad: 0.020, -lr * Pred Grad: -0.004, New P: 0.291
iter 13 loss: 0.139
Actual params: [0.4359, 0.2912]
-Original Grad: 0.005, -lr * Pred Grad: -0.013, New P: 0.423
-Original Grad: 0.008, -lr * Pred Grad: -0.006, New P: 0.285
iter 14 loss: 0.145
Actual params: [0.4229, 0.2848]
-Original Grad: 0.011, -lr * Pred Grad: -0.007, New P: 0.416
-Original Grad: 0.002, -lr * Pred Grad: -0.015, New P: 0.269
iter 15 loss: 0.147
Actual params: [0.4161, 0.2693]
-Original Grad: 0.012, -lr * Pred Grad: -0.002, New P: 0.414
-Original Grad: 0.004, -lr * Pred Grad: -0.020, New P: 0.250
iter 16 loss: 0.146
Actual params: [0.414 , 0.2496]
-Original Grad: 0.005, -lr * Pred Grad: -0.004, New P: 0.410
-Original Grad: 0.018, -lr * Pred Grad: -0.013, New P: 0.237
iter 17 loss: 0.148
Actual params: [0.4099, 0.2366]
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 0.403
-Original Grad: 0.017, -lr * Pred Grad: -0.006, New P: 0.230
iter 18 loss: 0.151
Actual params: [0.4033, 0.2301]
-Original Grad: 0.009, -lr * Pred Grad: -0.005, New P: 0.398
-Original Grad: 0.010, -lr * Pred Grad: -0.008, New P: 0.222
iter 19 loss: 0.154
Actual params: [0.3981, 0.2217]
-Original Grad: 0.004, -lr * Pred Grad: -0.005, New P: 0.393
-Original Grad: 0.022, -lr * Pred Grad: -0.007, New P: 0.215
iter 20 loss: 0.156
Actual params: [0.3931, 0.2149]
-Original Grad: 0.002, -lr * Pred Grad: -0.006, New P: 0.387
-Original Grad: 0.011, -lr * Pred Grad: -0.008, New P: 0.207
Target params: [1.1812, 0.2779]
iter 0 loss: 0.551
Actual params: [0.5941, 0.5941]
-Original Grad: 0.277, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.040, -lr * Pred Grad: -0.004, New P: 0.590
iter 1 loss: 0.504
Actual params: [0.6617, 0.5896]
-Original Grad: 0.509, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: 0.026, -lr * Pred Grad: -0.037, New P: 0.552
iter 2 loss: 0.408
Actual params: [0.7467, 0.5522]
-Original Grad: 0.639, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: 0.074, -lr * Pred Grad: -0.020, New P: 0.532
iter 3 loss: 0.289
Actual params: [0.8346, 0.532 ]
-Original Grad: 0.093, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.031, -lr * Pred Grad: -0.032, New P: 0.500
iter 4 loss: 0.223
Actual params: [0.9228, 0.5002]
-Original Grad: 0.005, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.072, -lr * Pred Grad: -0.049, New P: 0.451
iter 5 loss: 0.213
Actual params: [1.0112, 0.451 ]
-Original Grad: 0.008, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: -0.008, -lr * Pred Grad: -0.044, New P: 0.407
iter 6 loss: 0.251
Actual params: [1.0995, 0.4073]
-Original Grad: -0.006, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.003, -lr * Pred Grad: -0.024, New P: 0.384
iter 7 loss: 0.306
Actual params: [1.1878, 0.3835]
-Original Grad: -0.030, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.071, -lr * Pred Grad: 0.016, New P: 0.399
iter 8 loss: 0.331
Actual params: [1.2759, 0.3992]
-Original Grad: -0.048, -lr * Pred Grad: 0.087, New P: 1.363
-Original Grad: 0.092, -lr * Pred Grad: 0.055, New P: 0.454
iter 9 loss: 0.300
Actual params: [1.3629, 0.454 ]
-Original Grad: -0.032, -lr * Pred Grad: 0.075, New P: 1.438
-Original Grad: 0.066, -lr * Pred Grad: 0.066, New P: 0.520
iter 10 loss: 0.264
Actual params: [1.438 , 0.5203]
-Original Grad: -0.016, -lr * Pred Grad: 0.022, New P: 1.460
-Original Grad: 0.041, -lr * Pred Grad: 0.042, New P: 0.563
iter 11 loss: 0.227
Actual params: [1.4599, 0.5628]
-Original Grad: -0.006, -lr * Pred Grad: 0.049, New P: 1.509
-Original Grad: -0.007, -lr * Pred Grad: 0.003, New P: 0.566
iter 12 loss: 0.251
Actual params: [1.5089, 0.5659]
-Original Grad: -0.017, -lr * Pred Grad: 0.056, New P: 1.564
-Original Grad: 0.033, -lr * Pred Grad: 0.016, New P: 0.582
iter 13 loss: 0.259
Actual params: [1.5644, 0.5823]
-Original Grad: -0.010, -lr * Pred Grad: 0.067, New P: 1.632
-Original Grad: 0.021, -lr * Pred Grad: 0.006, New P: 0.589
iter 14 loss: 0.714
Actual params: [1.6317, 0.5886]
-Original Grad: -0.022, -lr * Pred Grad: 0.032, New P: 1.664
-Original Grad: 0.092, -lr * Pred Grad: 0.051, New P: 0.640
iter 15 loss: 0.224
Actual params: [1.6641, 0.6398]
-Original Grad: 0.000, -lr * Pred Grad: 0.025, New P: 1.689
-Original Grad: -0.121, -lr * Pred Grad: -0.033, New P: 0.606
iter 16 loss: 0.761
Actual params: [1.6889, 0.6065]
-Original Grad: -0.023, -lr * Pred Grad: -0.026, New P: 1.662
-Original Grad: -0.049, -lr * Pred Grad: -0.051, New P: 0.555
iter 17 loss: 0.454
Actual params: [1.6625, 0.5553]
-Original Grad: -0.038, -lr * Pred Grad: -0.031, New P: 1.631
-Original Grad: 0.124, -lr * Pred Grad: -0.004, New P: 0.552
iter 18 loss: 0.378
Actual params: [1.6314, 0.5517]
-Original Grad: -0.029, -lr * Pred Grad: -0.035, New P: 1.596
-Original Grad: 0.085, -lr * Pred Grad: 0.052, New P: 0.604
iter 19 loss: 0.246
Actual params: [1.5962, 0.6037]
-Original Grad: -0.007, -lr * Pred Grad: -0.024, New P: 1.572
-Original Grad: -0.071, -lr * Pred Grad: -0.014, New P: 0.590
iter 20 loss: 0.255
Actual params: [1.572 , 0.5897]
-Original Grad: -0.015, -lr * Pred Grad: -0.020, New P: 1.552
-Original Grad: 0.029, -lr * Pred Grad: -0.005, New P: 0.585
Target params: [1.1812, 0.2779]
iter 0 loss: 0.378
Actual params: [0.5941, 0.5941]
-Original Grad: 0.208, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.073, -lr * Pred Grad: -0.033, New P: 0.561
iter 1 loss: 0.287
Actual params: [0.6615, 0.5606]
-Original Grad: 0.066, -lr * Pred Grad: 0.084, New P: 0.746
-Original Grad: 0.001, -lr * Pred Grad: -0.060, New P: 0.501
iter 2 loss: 0.214
Actual params: [0.7456, 0.5008]
-Original Grad: -0.036, -lr * Pred Grad: 0.082, New P: 0.828
-Original Grad: 0.018, -lr * Pred Grad: -0.063, New P: 0.438
iter 3 loss: 0.203
Actual params: [0.8275, 0.4382]
-Original Grad: -0.043, -lr * Pred Grad: -0.022, New P: 0.805
-Original Grad: 0.014, -lr * Pred Grad: -0.050, New P: 0.388
iter 4 loss: 0.206
Actual params: [0.8054, 0.3882]
-Original Grad: -0.014, -lr * Pred Grad: -0.011, New P: 0.794
-Original Grad: 0.046, -lr * Pred Grad: -0.009, New P: 0.379
iter 5 loss: 0.207
Actual params: [0.7941, 0.3794]
-Original Grad: -0.004, -lr * Pred Grad: 0.008, New P: 0.802
-Original Grad: 0.023, -lr * Pred Grad: 0.028, New P: 0.408
iter 6 loss: 0.205
Actual params: [0.8018, 0.4078]
-Original Grad: -0.034, -lr * Pred Grad: -0.029, New P: 0.773
-Original Grad: 0.014, -lr * Pred Grad: 0.002, New P: 0.409
iter 7 loss: 0.211
Actual params: [0.7731, 0.4093]
-Original Grad: -0.023, -lr * Pred Grad: -0.031, New P: 0.742
-Original Grad: 0.021, -lr * Pred Grad: 0.012, New P: 0.421
iter 8 loss: 0.222
Actual params: [0.742 , 0.4215]
-Original Grad: -0.013, -lr * Pred Grad: -0.027, New P: 0.715
-Original Grad: 0.021, -lr * Pred Grad: 0.002, New P: 0.424
iter 9 loss: 0.235
Actual params: [0.7149, 0.4237]
-Original Grad: -0.012, -lr * Pred Grad: -0.021, New P: 0.694
-Original Grad: 0.029, -lr * Pred Grad: 0.014, New P: 0.437
iter 10 loss: 0.241
Actual params: [0.694 , 0.4374]
-Original Grad: 0.003, -lr * Pred Grad: -0.012, New P: 0.682
-Original Grad: 0.024, -lr * Pred Grad: 0.011, New P: 0.448
iter 11 loss: 0.250
Actual params: [0.6817, 0.4481]
-Original Grad: 0.021, -lr * Pred Grad: 0.001, New P: 0.682
-Original Grad: 0.023, -lr * Pred Grad: 0.010, New P: 0.458
iter 12 loss: 0.249
Actual params: [0.6825, 0.4582]
-Original Grad: 0.027, -lr * Pred Grad: 0.010, New P: 0.693
-Original Grad: 0.023, -lr * Pred Grad: 0.008, New P: 0.466
iter 13 loss: 0.241
Actual params: [0.6929, 0.4665]
-Original Grad: -0.004, -lr * Pred Grad: -0.004, New P: 0.689
-Original Grad: 0.022, -lr * Pred Grad: 0.008, New P: 0.475
iter 14 loss: 0.245
Actual params: [0.6888, 0.4748]
-Original Grad: 0.021, -lr * Pred Grad: -0.003, New P: 0.685
-Original Grad: 0.022, -lr * Pred Grad: 0.008, New P: 0.483
iter 15 loss: 0.247
Actual params: [0.6854, 0.4827]
-Original Grad: 0.004, -lr * Pred Grad: -0.005, New P: 0.680
-Original Grad: 0.012, -lr * Pred Grad: 0.001, New P: 0.484
iter 16 loss: 0.252
Actual params: [0.68  , 0.4842]
-Original Grad: 0.032, -lr * Pred Grad: 0.006, New P: 0.686
-Original Grad: 0.013, -lr * Pred Grad: -0.001, New P: 0.483
iter 17 loss: 0.247
Actual params: [0.686, 0.483]
-Original Grad: 0.029, -lr * Pred Grad: 0.014, New P: 0.700
-Original Grad: 0.015, -lr * Pred Grad: 0.000, New P: 0.483
iter 18 loss: 0.236
Actual params: [0.6998, 0.4832]
-Original Grad: 0.023, -lr * Pred Grad: 0.011, New P: 0.710
-Original Grad: 0.020, -lr * Pred Grad: 0.004, New P: 0.488
iter 19 loss: 0.229
Actual params: [0.7103, 0.4875]
-Original Grad: -0.008, -lr * Pred Grad: -0.008, New P: 0.703
-Original Grad: 0.018, -lr * Pred Grad: 0.005, New P: 0.493
iter 20 loss: 0.233
Actual params: [0.7026, 0.4927]
-Original Grad: -0.015, -lr * Pred Grad: -0.020, New P: 0.682
-Original Grad: 0.017, -lr * Pred Grad: 0.004, New P: 0.497
Target params: [1.1812, 0.2779]
iter 0 loss: 0.346
Actual params: [0.5941, 0.5941]
-Original Grad: 0.088, -lr * Pred Grad: 0.064, New P: 0.658
-Original Grad: -0.562, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.329
Actual params: [0.6583, 0.5315]
-Original Grad: 0.054, -lr * Pred Grad: 0.079, New P: 0.737
-Original Grad: -0.161, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.346
Actual params: [0.7375, 0.4508]
-Original Grad: 0.003, -lr * Pred Grad: 0.006, New P: 0.743
-Original Grad: -0.017, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.355
Actual params: [0.7431, 0.3674]
-Original Grad: -0.009, -lr * Pred Grad: -0.010, New P: 0.733
-Original Grad: 0.019, -lr * Pred Grad: -0.081, New P: 0.286
iter 4 loss: 0.364
Actual params: [0.7327, 0.286 ]
-Original Grad: -0.022, -lr * Pred Grad: -0.026, New P: 0.707
-Original Grad: 0.029, -lr * Pred Grad: -0.073, New P: 0.213
iter 5 loss: 0.371
Actual params: [0.7068, 0.2131]
-Original Grad: -0.021, -lr * Pred Grad: -0.017, New P: 0.690
-Original Grad: 0.028, -lr * Pred Grad: -0.069, New P: 0.144
iter 6 loss: 0.372
Actual params: [0.6897, 0.1441]
-Original Grad: -0.041, -lr * Pred Grad: -0.034, New P: 0.656
-Original Grad: 0.047, -lr * Pred Grad: -0.059, New P: 0.086
iter 7 loss: 0.374
Actual params: [0.6558, 0.0856]
-Original Grad: -0.007, -lr * Pred Grad: -0.027, New P: 0.629
-Original Grad: 0.051, -lr * Pred Grad: -0.038, New P: 0.047
iter 8 loss: 0.374
Actual params: [0.6289, 0.0473]
-Original Grad: -0.026, -lr * Pred Grad: -0.025, New P: 0.604
-Original Grad: 0.068, -lr * Pred Grad: -0.024, New P: 0.023
iter 9 loss: 0.373
Actual params: [0.6035, 0.0229]
-Original Grad: -0.031, -lr * Pred Grad: -0.029, New P: 0.574
-Original Grad: 0.091, -lr * Pred Grad: -0.007, New P: 0.016
iter 10 loss: 0.370
Actual params: [0.5743, 0.0163]
-Original Grad: -0.005, -lr * Pred Grad: -0.023, New P: 0.551
-Original Grad: 0.063, -lr * Pred Grad: 0.032, New P: 0.048
iter 11 loss: 0.365
Actual params: [0.551 , 0.0482]
-Original Grad: -0.011, -lr * Pred Grad: -0.018, New P: 0.533
-Original Grad: 0.054, -lr * Pred Grad: 0.061, New P: 0.110
iter 12 loss: 0.357
Actual params: [0.533 , 0.1097]
-Original Grad: -0.029, -lr * Pred Grad: -0.024, New P: 0.509
-Original Grad: 0.053, -lr * Pred Grad: 0.034, New P: 0.144
iter 13 loss: 0.352
Actual params: [0.5094, 0.1439]
-Original Grad: -0.003, -lr * Pred Grad: -0.021, New P: 0.488
-Original Grad: 0.045, -lr * Pred Grad: 0.054, New P: 0.198
iter 14 loss: 0.344
Actual params: [0.4881, 0.1978]
-Original Grad: -0.001, -lr * Pred Grad: -0.014, New P: 0.474
-Original Grad: 0.035, -lr * Pred Grad: 0.007, New P: 0.205
iter 15 loss: 0.341
Actual params: [0.4737, 0.2052]
-Original Grad: -0.001, -lr * Pred Grad: -0.011, New P: 0.463
-Original Grad: 0.007, -lr * Pred Grad: 0.022, New P: 0.227
iter 16 loss: 0.336
Actual params: [0.4632, 0.2274]
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: 0.456
-Original Grad: 0.012, -lr * Pred Grad: -0.019, New P: 0.208
iter 17 loss: 0.337
Actual params: [0.4563, 0.2084]
-Original Grad: 0.013, -lr * Pred Grad: -0.002, New P: 0.454
-Original Grad: 0.005, -lr * Pred Grad: 0.001, New P: 0.209
iter 18 loss: 0.337
Actual params: [0.454 , 0.2089]
-Original Grad: 0.002, -lr * Pred Grad: -0.005, New P: 0.449
-Original Grad: 0.021, -lr * Pred Grad: -0.003, New P: 0.206
iter 19 loss: 0.336
Actual params: [0.4485, 0.2061]
-Original Grad: -0.021, -lr * Pred Grad: -0.019, New P: 0.430
-Original Grad: 0.022, -lr * Pred Grad: 0.008, New P: 0.214
iter 20 loss: 0.332
Actual params: [0.4296, 0.2144]
-Original Grad: 0.000, -lr * Pred Grad: -0.019, New P: 0.410
-Original Grad: 0.007, -lr * Pred Grad: -0.005, New P: 0.210
Target params: [1.1812, 0.2779]
iter 0 loss: 0.584
Actual params: [0.5941, 0.5941]
-Original Grad: 0.228, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.196, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.488
Actual params: [0.6616, 0.5321]
-Original Grad: 0.103, -lr * Pred Grad: 0.084, New P: 0.746
-Original Grad: -0.183, -lr * Pred Grad: -0.080, New P: 0.452
iter 2 loss: 0.397
Actual params: [0.746 , 0.4518]
-Original Grad: 0.030, -lr * Pred Grad: 0.086, New P: 0.832
-Original Grad: -0.140, -lr * Pred Grad: -0.083, New P: 0.368
iter 3 loss: 0.323
Actual params: [0.8324, 0.3684]
-Original Grad: 0.019, -lr * Pred Grad: 0.077, New P: 0.909
-Original Grad: -0.079, -lr * Pred Grad: -0.083, New P: 0.285
iter 4 loss: 0.275
Actual params: [0.9095, 0.285 ]
-Original Grad: -0.007, -lr * Pred Grad: -0.004, New P: 0.905
-Original Grad: -0.023, -lr * Pred Grad: -0.079, New P: 0.206
iter 5 loss: 0.269
Actual params: [0.9053, 0.2061]
-Original Grad: -0.007, -lr * Pred Grad: 0.031, New P: 0.936
-Original Grad: -0.020, -lr * Pred Grad: -0.072, New P: 0.134
iter 6 loss: 0.253
Actual params: [0.9363, 0.1345]
-Original Grad: -0.007, -lr * Pred Grad: -0.014, New P: 0.922
-Original Grad: -0.015, -lr * Pred Grad: -0.069, New P: 0.066
iter 7 loss: 0.249
Actual params: [0.9224, 0.0657]
-Original Grad: -0.001, -lr * Pred Grad: 0.005, New P: 0.927
-Original Grad: -0.011, -lr * Pred Grad: -0.064, New P: 0.002
iter 8 loss: 0.241
Actual params: [0.927 , 0.0017]
-Original Grad: -0.003, -lr * Pred Grad: -0.024, New P: 0.903
-Original Grad: -0.008, -lr * Pred Grad: -0.052, New P: -0.051
iter 9 loss: 0.246
Actual params: [ 0.9034, -0.0507]
-Original Grad: 0.000, -lr * Pred Grad: -0.013, New P: 0.891
-Original Grad: -0.004, -lr * Pred Grad: -0.041, New P: -0.092
iter 10 loss: 0.243
Actual params: [ 0.8906, -0.0918]
-Original Grad: 0.001, -lr * Pred Grad: -0.014, New P: 0.876
-Original Grad: -0.003, -lr * Pred Grad: -0.035, New P: -0.127
iter 11 loss: 0.242
Actual params: [ 0.8762, -0.1271]
-Original Grad: 0.004, -lr * Pred Grad: -0.010, New P: 0.866
-Original Grad: -0.002, -lr * Pred Grad: -0.033, New P: -0.160
iter 12 loss: 0.242
Actual params: [ 0.8662, -0.1599]
-Original Grad: 0.006, -lr * Pred Grad: -0.008, New P: 0.858
-Original Grad: -0.006, -lr * Pred Grad: -0.030, New P: -0.189
iter 13 loss: 0.241
Actual params: [ 0.8582, -0.1894]
-Original Grad: 0.008, -lr * Pred Grad: -0.006, New P: 0.852
-Original Grad: 0.002, -lr * Pred Grad: -0.018, New P: -0.207
iter 14 loss: 0.242
Actual params: [ 0.8518, -0.2075]
-Original Grad: 0.012, -lr * Pred Grad: -0.004, New P: 0.848
-Original Grad: -0.001, -lr * Pred Grad: -0.015, New P: -0.222
iter 15 loss: 0.242
Actual params: [ 0.8478, -0.2221]
-Original Grad: 0.012, -lr * Pred Grad: -0.003, New P: 0.845
-Original Grad: 0.004, -lr * Pred Grad: -0.018, New P: -0.241
iter 16 loss: 0.242
Actual params: [ 0.8451, -0.2405]
-Original Grad: 0.011, -lr * Pred Grad: -0.002, New P: 0.843
-Original Grad: 0.004, -lr * Pred Grad: -0.018, New P: -0.259
iter 17 loss: 0.242
Actual params: [ 0.8427, -0.2588]
-Original Grad: 0.014, -lr * Pred Grad: -0.001, New P: 0.842
-Original Grad: 0.004, -lr * Pred Grad: -0.018, New P: -0.277
iter 18 loss: 0.241
Actual params: [ 0.8415, -0.2767]
-Original Grad: 0.005, -lr * Pred Grad: -0.004, New P: 0.837
-Original Grad: 0.006, -lr * Pred Grad: -0.017, New P: -0.294
iter 19 loss: 0.242
Actual params: [ 0.8373, -0.2938]
-Original Grad: 0.007, -lr * Pred Grad: -0.005, New P: 0.832
-Original Grad: 0.008, -lr * Pred Grad: -0.016, New P: -0.309
iter 20 loss: 0.242
Actual params: [ 0.8321, -0.3095]
-Original Grad: 0.010, -lr * Pred Grad: -0.003, New P: 0.830
-Original Grad: 0.009, -lr * Pred Grad: -0.014, New P: -0.324
Target params: [1.1812, 0.2779]
iter 0 loss: 0.312
Actual params: [0.5941, 0.5941]
-Original Grad: 0.012, -lr * Pred Grad: 0.043, New P: 0.637
-Original Grad: -0.059, -lr * Pred Grad: -0.023, New P: 0.571
iter 1 loss: 0.281
Actual params: [0.6373, 0.5715]
-Original Grad: 0.009, -lr * Pred Grad: -0.013, New P: 0.624
-Original Grad: -0.018, -lr * Pred Grad: -0.060, New P: 0.512
iter 2 loss: 0.262
Actual params: [0.6239, 0.5116]
-Original Grad: 0.009, -lr * Pred Grad: -0.044, New P: 0.580
-Original Grad: -0.027, -lr * Pred Grad: -0.066, New P: 0.445
iter 3 loss: 0.258
Actual params: [0.5796, 0.4454]
-Original Grad: -0.022, -lr * Pred Grad: -0.044, New P: 0.535
-Original Grad: -0.016, -lr * Pred Grad: -0.064, New P: 0.381
iter 4 loss: 0.254
Actual params: [0.5354, 0.3815]
-Original Grad: -0.022, -lr * Pred Grad: -0.030, New P: 0.505
-Original Grad: -0.001, -lr * Pred Grad: -0.052, New P: 0.330
iter 5 loss: 0.253
Actual params: [0.5051, 0.3297]
-Original Grad: -0.028, -lr * Pred Grad: -0.028, New P: 0.477
-Original Grad: 0.006, -lr * Pred Grad: -0.025, New P: 0.304
iter 6 loss: 0.255
Actual params: [0.4774, 0.3045]
-Original Grad: -0.030, -lr * Pred Grad: -0.034, New P: 0.444
-Original Grad: 0.006, -lr * Pred Grad: -0.000, New P: 0.304
iter 7 loss: 0.265
Actual params: [0.4437, 0.3043]
-Original Grad: -0.016, -lr * Pred Grad: -0.032, New P: 0.412
-Original Grad: -0.002, -lr * Pred Grad: -0.012, New P: 0.292
iter 8 loss: 0.273
Actual params: [0.4118, 0.2921]
-Original Grad: -0.020, -lr * Pred Grad: -0.030, New P: 0.381
-Original Grad: -0.002, -lr * Pred Grad: -0.018, New P: 0.274
iter 9 loss: 0.279
Actual params: [0.3814, 0.2743]
-Original Grad: -0.014, -lr * Pred Grad: -0.029, New P: 0.352
-Original Grad: -0.001, -lr * Pred Grad: -0.020, New P: 0.254
iter 10 loss: 0.287
Actual params: [0.3524, 0.2543]
-Original Grad: 0.003, -lr * Pred Grad: -0.022, New P: 0.330
-Original Grad: -0.004, -lr * Pred Grad: -0.022, New P: 0.232
iter 11 loss: 0.293
Actual params: [0.3303, 0.2324]
-Original Grad: 0.015, -lr * Pred Grad: -0.011, New P: 0.320
-Original Grad: -0.001, -lr * Pred Grad: -0.022, New P: 0.211
iter 12 loss: 0.296
Actual params: [0.3197, 0.2107]
-Original Grad: 0.036, -lr * Pred Grad: 0.008, New P: 0.327
-Original Grad: -0.002, -lr * Pred Grad: -0.021, New P: 0.190
iter 13 loss: 0.292
Actual params: [0.3272, 0.1895]
-Original Grad: 0.013, -lr * Pred Grad: 0.008, New P: 0.335
-Original Grad: -0.002, -lr * Pred Grad: -0.021, New P: 0.168
iter 14 loss: 0.288
Actual params: [0.3352, 0.1682]
-Original Grad: 0.012, -lr * Pred Grad: -0.002, New P: 0.333
-Original Grad: 0.001, -lr * Pred Grad: -0.020, New P: 0.148
iter 15 loss: 0.289
Actual params: [0.3334, 0.1477]
-Original Grad: -0.002, -lr * Pred Grad: -0.012, New P: 0.321
-Original Grad: -0.000, -lr * Pred Grad: -0.020, New P: 0.128
iter 16 loss: 0.293
Actual params: [0.3212, 0.1277]
-Original Grad: 0.017, -lr * Pred Grad: -0.008, New P: 0.313
-Original Grad: -0.000, -lr * Pred Grad: -0.020, New P: 0.108
iter 17 loss: 0.296
Actual params: [0.3133, 0.1075]
-Original Grad: 0.030, -lr * Pred Grad: 0.007, New P: 0.320
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: 0.088
iter 18 loss: 0.294
Actual params: [0.32  , 0.0878]
-Original Grad: 0.040, -lr * Pred Grad: 0.020, New P: 0.340
-Original Grad: 0.000, -lr * Pred Grad: -0.020, New P: 0.068
iter 19 loss: 0.288
Actual params: [0.3402, 0.0679]
-Original Grad: 0.005, -lr * Pred Grad: 0.005, New P: 0.345
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: 0.048
iter 20 loss: 0.287
Actual params: [0.3453, 0.0482]
-Original Grad: 0.014, -lr * Pred Grad: -0.003, New P: 0.343
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: 0.028
Target params: [1.1812, 0.2779]
iter 0 loss: 0.235
Actual params: [0.5941, 0.5941]
-Original Grad: -0.130, -lr * Pred Grad: -0.056, New P: 0.538
-Original Grad: -0.413, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.236
Actual params: [0.5379, 0.531 ]
-Original Grad: 0.086, -lr * Pred Grad: -0.059, New P: 0.479
-Original Grad: -0.414, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.263
Actual params: [0.4789, 0.4506]
-Original Grad: 0.135, -lr * Pred Grad: -0.017, New P: 0.462
-Original Grad: -0.044, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.271
Actual params: [0.462, 0.367]
-Original Grad: 0.128, -lr * Pred Grad: 0.047, New P: 0.509
-Original Grad: -0.023, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.256
Actual params: [0.5087, 0.2833]
-Original Grad: 0.007, -lr * Pred Grad: 0.054, New P: 0.562
-Original Grad: -0.008, -lr * Pred Grad: -0.080, New P: 0.203
iter 5 loss: 0.247
Actual params: [0.5624, 0.203 ]
-Original Grad: -0.041, -lr * Pred Grad: -0.028, New P: 0.535
-Original Grad: -0.009, -lr * Pred Grad: -0.072, New P: 0.131
iter 6 loss: 0.248
Actual params: [0.5347, 0.1307]
-Original Grad: -0.011, -lr * Pred Grad: -0.024, New P: 0.511
-Original Grad: 0.005, -lr * Pred Grad: -0.069, New P: 0.062
iter 7 loss: 0.249
Actual params: [0.5111, 0.0617]
-Original Grad: 0.001, -lr * Pred Grad: -0.012, New P: 0.500
-Original Grad: 0.006, -lr * Pred Grad: -0.063, New P: -0.001
iter 8 loss: 0.250
Actual params: [ 0.4996, -0.0013]
-Original Grad: 0.062, -lr * Pred Grad: 0.030, New P: 0.530
-Original Grad: 0.016, -lr * Pred Grad: -0.046, New P: -0.048
iter 9 loss: 0.245
Actual params: [ 0.5296, -0.0476]
-Original Grad: -0.006, -lr * Pred Grad: -0.001, New P: 0.528
-Original Grad: 0.020, -lr * Pred Grad: -0.034, New P: -0.081
iter 10 loss: 0.245
Actual params: [ 0.5281, -0.0812]
-Original Grad: -0.007, -lr * Pred Grad: -0.010, New P: 0.518
-Original Grad: 0.025, -lr * Pred Grad: -0.027, New P: -0.108
iter 11 loss: 0.245
Actual params: [ 0.5176, -0.1078]
-Original Grad: 0.029, -lr * Pred Grad: 0.001, New P: 0.518
-Original Grad: 0.012, -lr * Pred Grad: -0.022, New P: -0.130
iter 12 loss: 0.244
Actual params: [ 0.5184, -0.1302]
-Original Grad: 0.038, -lr * Pred Grad: 0.019, New P: 0.538
-Original Grad: 0.023, -lr * Pred Grad: -0.009, New P: -0.139
iter 13 loss: 0.242
Actual params: [ 0.5377, -0.1392]
-Original Grad: -0.029, -lr * Pred Grad: -0.015, New P: 0.522
-Original Grad: 0.028, -lr * Pred Grad: 0.014, New P: -0.125
iter 14 loss: 0.244
Actual params: [ 0.5225, -0.1249]
-Original Grad: 0.010, -lr * Pred Grad: -0.014, New P: 0.509
-Original Grad: 0.033, -lr * Pred Grad: 0.012, New P: -0.113
iter 15 loss: 0.246
Actual params: [ 0.5089, -0.1132]
-Original Grad: 0.043, -lr * Pred Grad: 0.011, New P: 0.520
-Original Grad: 0.021, -lr * Pred Grad: 0.006, New P: -0.107
iter 16 loss: 0.245
Actual params: [ 0.5196, -0.1069]
-Original Grad: 0.027, -lr * Pred Grad: 0.020, New P: 0.539
-Original Grad: 0.025, -lr * Pred Grad: 0.004, New P: -0.103
iter 17 loss: 0.243
Actual params: [ 0.5392, -0.1031]
-Original Grad: -0.034, -lr * Pred Grad: -0.017, New P: 0.522
-Original Grad: 0.017, -lr * Pred Grad: -0.003, New P: -0.106
iter 18 loss: 0.245
Actual params: [ 0.5221, -0.1056]
-Original Grad: 0.023, -lr * Pred Grad: -0.013, New P: 0.509
-Original Grad: 0.008, -lr * Pred Grad: -0.011, New P: -0.117
iter 19 loss: 0.246
Actual params: [ 0.5094, -0.1169]
-Original Grad: 0.039, -lr * Pred Grad: 0.012, New P: 0.521
-Original Grad: 0.038, -lr * Pred Grad: 0.001, New P: -0.116
iter 20 loss: 0.244
Actual params: [ 0.5215, -0.1164]
-Original Grad: 0.008, -lr * Pred Grad: 0.008, New P: 0.529
-Original Grad: 0.024, -lr * Pred Grad: 0.006, New P: -0.111
Target params: [1.1812, 0.2779]
iter 0 loss: 0.731
Actual params: [0.5941, 0.5941]
-Original Grad: 0.190, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.517, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.683
Actual params: [0.6614, 0.5313]
-Original Grad: 0.135, -lr * Pred Grad: 0.084, New P: 0.746
-Original Grad: -0.382, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.591
Actual params: [0.7457, 0.4508]
-Original Grad: 0.096, -lr * Pred Grad: 0.087, New P: 0.833
-Original Grad: -0.357, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.473
Actual params: [0.8325, 0.3672]
-Original Grad: 0.047, -lr * Pred Grad: 0.085, New P: 0.918
-Original Grad: -0.160, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.389
Actual params: [0.9177, 0.283 ]
-Original Grad: 0.002, -lr * Pred Grad: 0.028, New P: 0.945
-Original Grad: -0.075, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.328
Actual params: [0.9452, 0.1987]
-Original Grad: -0.001, -lr * Pred Grad: 0.030, New P: 0.975
-Original Grad: -0.023, -lr * Pred Grad: -0.084, New P: 0.115
iter 6 loss: 0.300
Actual params: [0.9747, 0.1149]
-Original Grad: 0.000, -lr * Pred Grad: -0.005, New P: 0.970
-Original Grad: -0.017, -lr * Pred Grad: -0.079, New P: 0.036
iter 7 loss: 0.281
Actual params: [0.9698, 0.036 ]
-Original Grad: -0.005, -lr * Pred Grad: 0.010, New P: 0.979
-Original Grad: -0.000, -lr * Pred Grad: -0.071, New P: -0.035
iter 8 loss: 0.271
Actual params: [ 0.9793, -0.0353]
-Original Grad: 0.002, -lr * Pred Grad: -0.021, New P: 0.959
-Original Grad: 0.009, -lr * Pred Grad: -0.069, New P: -0.104
iter 9 loss: 0.263
Actual params: [ 0.9585, -0.1039]
-Original Grad: -0.006, -lr * Pred Grad: -0.017, New P: 0.942
-Original Grad: 0.016, -lr * Pred Grad: -0.060, New P: -0.164
iter 10 loss: 0.262
Actual params: [ 0.942 , -0.1638]
-Original Grad: -0.006, -lr * Pred Grad: -0.021, New P: 0.921
-Original Grad: 0.017, -lr * Pred Grad: -0.042, New P: -0.206
iter 11 loss: 0.258
Actual params: [ 0.921, -0.206]
-Original Grad: -0.003, -lr * Pred Grad: -0.017, New P: 0.904
-Original Grad: 0.016, -lr * Pred Grad: -0.032, New P: -0.238
iter 12 loss: 0.255
Actual params: [ 0.9038, -0.2379]
-Original Grad: -0.012, -lr * Pred Grad: -0.019, New P: 0.885
-Original Grad: 0.026, -lr * Pred Grad: -0.028, New P: -0.265
iter 13 loss: 0.252
Actual params: [ 0.8846, -0.2655]
-Original Grad: -0.004, -lr * Pred Grad: -0.018, New P: 0.867
-Original Grad: 0.026, -lr * Pred Grad: -0.025, New P: -0.291
iter 14 loss: 0.250
Actual params: [ 0.8671, -0.2906]
-Original Grad: -0.008, -lr * Pred Grad: -0.017, New P: 0.850
-Original Grad: 0.021, -lr * Pred Grad: -0.023, New P: -0.314
iter 15 loss: 0.248
Actual params: [ 0.85  , -0.3135]
-Original Grad: -0.022, -lr * Pred Grad: -0.023, New P: 0.827
-Original Grad: 0.034, -lr * Pred Grad: -0.011, New P: -0.325
iter 16 loss: 0.248
Actual params: [ 0.8272, -0.3249]
-Original Grad: -0.000, -lr * Pred Grad: -0.019, New P: 0.808
-Original Grad: 0.040, -lr * Pred Grad: 0.018, New P: -0.307
iter 17 loss: 0.253
Actual params: [ 0.8085, -0.3066]
-Original Grad: -0.004, -lr * Pred Grad: -0.014, New P: 0.795
-Original Grad: 0.030, -lr * Pred Grad: 0.021, New P: -0.285
iter 18 loss: 0.256
Actual params: [ 0.7949, -0.2852]
-Original Grad: 0.007, -lr * Pred Grad: -0.008, New P: 0.787
-Original Grad: 0.029, -lr * Pred Grad: 0.005, New P: -0.280
iter 19 loss: 0.257
Actual params: [ 0.7871, -0.2803]
-Original Grad: 0.002, -lr * Pred Grad: -0.007, New P: 0.780
-Original Grad: 0.023, -lr * Pred Grad: 0.008, New P: -0.272
iter 20 loss: 0.259
Actual params: [ 0.78  , -0.2724]
-Original Grad: 0.010, -lr * Pred Grad: -0.005, New P: 0.775
-Original Grad: 0.016, -lr * Pred Grad: -0.004, New P: -0.276
Target params: [1.1812, 0.2779]
iter 0 loss: 0.710
Actual params: [0.5941, 0.5941]
-Original Grad: 0.352, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.246, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.636
Actual params: [0.6617, 0.5312]
-Original Grad: 0.170, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.104, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.575
Actual params: [0.7466, 0.4506]
-Original Grad: 0.089, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.050, -lr * Pred Grad: -0.083, New P: 0.368
iter 3 loss: 0.531
Actual params: [0.8343, 0.3679]
-Original Grad: 0.022, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: -0.008, -lr * Pred Grad: -0.077, New P: 0.291
iter 4 loss: 0.516
Actual params: [0.9223, 0.2911]
-Original Grad: 0.030, -lr * Pred Grad: 0.088, New P: 1.010
-Original Grad: 0.025, -lr * Pred Grad: -0.070, New P: 0.221
iter 5 loss: 0.504
Actual params: [1.0099, 0.2207]
-Original Grad: 0.010, -lr * Pred Grad: 0.083, New P: 1.093
-Original Grad: 0.027, -lr * Pred Grad: -0.066, New P: 0.155
iter 6 loss: 0.503
Actual params: [1.0932, 0.1551]
-Original Grad: -0.004, -lr * Pred Grad: 0.004, New P: 1.097
-Original Grad: 0.028, -lr * Pred Grad: -0.048, New P: 0.107
iter 7 loss: 0.501
Actual params: [1.0968, 0.107 ]
-Original Grad: 0.017, -lr * Pred Grad: 0.047, New P: 1.144
-Original Grad: 0.046, -lr * Pred Grad: -0.030, New P: 0.077
iter 8 loss: 0.503
Actual params: [1.1441, 0.0765]
-Original Grad: 0.022, -lr * Pred Grad: 0.048, New P: 1.192
-Original Grad: 0.062, -lr * Pred Grad: -0.013, New P: 0.064
iter 9 loss: 0.505
Actual params: [1.192 , 0.0638]
-Original Grad: 0.011, -lr * Pred Grad: 0.031, New P: 1.223
-Original Grad: 0.038, -lr * Pred Grad: 0.021, New P: 0.084
iter 10 loss: 0.508
Actual params: [1.2235, 0.0844]
-Original Grad: 0.004, -lr * Pred Grad: 0.002, New P: 1.225
-Original Grad: 0.044, -lr * Pred Grad: 0.043, New P: 0.127
iter 11 loss: 0.513
Actual params: [1.225 , 0.1272]
-Original Grad: 0.010, -lr * Pred Grad: -0.006, New P: 1.219
-Original Grad: 0.044, -lr * Pred Grad: 0.020, New P: 0.147
iter 12 loss: 0.514
Actual params: [1.2194, 0.147 ]
-Original Grad: 0.004, -lr * Pred Grad: -0.012, New P: 1.207
-Original Grad: 0.035, -lr * Pred Grad: 0.038, New P: 0.185
iter 13 loss: 0.518
Actual params: [1.207 , 0.1845]
-Original Grad: 0.005, -lr * Pred Grad: -0.010, New P: 1.197
-Original Grad: 0.039, -lr * Pred Grad: 0.013, New P: 0.198
iter 14 loss: 0.519
Actual params: [1.1967, 0.198 ]
-Original Grad: 0.006, -lr * Pred Grad: -0.010, New P: 1.187
-Original Grad: 0.023, -lr * Pred Grad: 0.022, New P: 0.220
iter 15 loss: 0.522
Actual params: [1.1866, 0.2203]
-Original Grad: -0.000, -lr * Pred Grad: -0.013, New P: 1.174
-Original Grad: 0.007, -lr * Pred Grad: -0.011, New P: 0.210
iter 16 loss: 0.519
Actual params: [1.1741, 0.2097]
-Original Grad: 0.003, -lr * Pred Grad: -0.012, New P: 1.162
-Original Grad: 0.026, -lr * Pred Grad: 0.011, New P: 0.220
iter 17 loss: 0.520
Actual params: [1.1623, 0.2202]
-Original Grad: 0.002, -lr * Pred Grad: -0.011, New P: 1.151
-Original Grad: 0.007, -lr * Pred Grad: -0.006, New P: 0.214
iter 18 loss: 0.518
Actual params: [1.1514, 0.214 ]
-Original Grad: 0.008, -lr * Pred Grad: -0.007, New P: 1.144
-Original Grad: 0.021, -lr * Pred Grad: 0.004, New P: 0.218
iter 19 loss: 0.518
Actual params: [1.1442, 0.2182]
-Original Grad: 0.008, -lr * Pred Grad: -0.005, New P: 1.139
-Original Grad: 0.031, -lr * Pred Grad: 0.011, New P: 0.229
iter 20 loss: 0.519
Actual params: [1.1392, 0.2288]
-Original Grad: -0.001, -lr * Pred Grad: -0.009, New P: 1.130
-Original Grad: 0.012, -lr * Pred Grad: 0.004, New P: 0.232
Target params: [1.1812, 0.2779]
iter 0 loss: 0.120
Actual params: [0.5941, 0.5941]
-Original Grad: -0.004, -lr * Pred Grad: 0.032, New P: 0.626
-Original Grad: -0.126, -lr * Pred Grad: -0.055, New P: 0.539
iter 1 loss: 0.111
Actual params: [0.6261, 0.5387]
-Original Grad: -0.020, -lr * Pred Grad: -0.040, New P: 0.586
-Original Grad: -0.092, -lr * Pred Grad: -0.078, New P: 0.461
iter 2 loss: 0.123
Actual params: [0.5865, 0.461 ]
-Original Grad: 0.019, -lr * Pred Grad: -0.058, New P: 0.528
-Original Grad: -0.030, -lr * Pred Grad: -0.076, New P: 0.385
iter 3 loss: 0.184
Actual params: [0.5283, 0.385 ]
-Original Grad: 0.041, -lr * Pred Grad: -0.031, New P: 0.498
-Original Grad: 0.010, -lr * Pred Grad: -0.070, New P: 0.315
iter 4 loss: 0.204
Actual params: [0.4976, 0.315 ]
-Original Grad: 0.053, -lr * Pred Grad: 0.028, New P: 0.526
-Original Grad: 0.010, -lr * Pred Grad: -0.065, New P: 0.250
iter 5 loss: 0.188
Actual params: [0.5256, 0.2496]
-Original Grad: 0.048, -lr * Pred Grad: 0.050, New P: 0.576
-Original Grad: 0.006, -lr * Pred Grad: -0.051, New P: 0.199
iter 6 loss: 0.159
Actual params: [0.576 , 0.1991]
-Original Grad: 0.061, -lr * Pred Grad: 0.045, New P: 0.621
-Original Grad: 0.008, -lr * Pred Grad: -0.031, New P: 0.168
iter 7 loss: 0.131
Actual params: [0.6207, 0.1679]
-Original Grad: 0.049, -lr * Pred Grad: 0.048, New P: 0.669
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: 0.161
iter 8 loss: 0.101
Actual params: [0.669 , 0.1609]
-Original Grad: 0.012, -lr * Pred Grad: 0.002, New P: 0.671
-Original Grad: 0.002, -lr * Pred Grad: 0.000, New P: 0.161
iter 9 loss: 0.100
Actual params: [0.671 , 0.1611]
-Original Grad: 0.022, -lr * Pred Grad: 0.024, New P: 0.695
-Original Grad: 0.006, -lr * Pred Grad: -0.017, New P: 0.144
iter 10 loss: 0.090
Actual params: [0.6948, 0.1438]
-Original Grad: 0.015, -lr * Pred Grad: -0.009, New P: 0.686
-Original Grad: 0.003, -lr * Pred Grad: -0.015, New P: 0.129
iter 11 loss: 0.095
Actual params: [0.6858, 0.1292]
-Original Grad: 0.024, -lr * Pred Grad: 0.018, New P: 0.704
-Original Grad: 0.004, -lr * Pred Grad: -0.016, New P: 0.113
iter 12 loss: 0.088
Actual params: [0.7042, 0.1128]
-Original Grad: 0.015, -lr * Pred Grad: -0.003, New P: 0.701
-Original Grad: 0.006, -lr * Pred Grad: -0.016, New P: 0.097
iter 13 loss: 0.091
Actual params: [0.7009, 0.0969]
-Original Grad: 0.016, -lr * Pred Grad: 0.006, New P: 0.707
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: 0.080
iter 14 loss: 0.089
Actual params: [0.7071, 0.0802]
-Original Grad: 0.021, -lr * Pred Grad: 0.002, New P: 0.709
-Original Grad: 0.010, -lr * Pred Grad: -0.015, New P: 0.065
iter 15 loss: 0.089
Actual params: [0.7093, 0.0654]
-Original Grad: 0.015, -lr * Pred Grad: 0.003, New P: 0.712
-Original Grad: 0.007, -lr * Pred Grad: -0.014, New P: 0.051
iter 16 loss: 0.088
Actual params: [0.7119, 0.0511]
-Original Grad: 0.016, -lr * Pred Grad: 0.001, New P: 0.713
-Original Grad: 0.011, -lr * Pred Grad: -0.014, New P: 0.037
iter 17 loss: 0.088
Actual params: [0.7126, 0.0374]
-Original Grad: 0.018, -lr * Pred Grad: 0.002, New P: 0.714
-Original Grad: 0.009, -lr * Pred Grad: -0.014, New P: 0.023
iter 18 loss: 0.088
Actual params: [0.7145, 0.0232]
-Original Grad: 0.012, -lr * Pred Grad: 0.000, New P: 0.715
-Original Grad: 0.009, -lr * Pred Grad: -0.015, New P: 0.008
iter 19 loss: 0.088
Actual params: [0.7146, 0.0081]
-Original Grad: 0.018, -lr * Pred Grad: 0.002, New P: 0.716
-Original Grad: 0.016, -lr * Pred Grad: -0.012, New P: -0.004
iter 20 loss: 0.088
Actual params: [ 0.7164, -0.0041]
-Original Grad: 0.018, -lr * Pred Grad: 0.004, New P: 0.720
-Original Grad: 0.012, -lr * Pred Grad: -0.011, New P: -0.015
Target params: [1.1812, 0.2779]
iter 0 loss: 1.013
Actual params: [0.5941, 0.5941]
-Original Grad: 0.727, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.942, -lr * Pred Grad: -0.062, New P: 0.533
iter 1 loss: 0.849
Actual params: [0.6617, 0.5325]
-Original Grad: 0.922, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.883, -lr * Pred Grad: -0.081, New P: 0.452
iter 2 loss: 0.603
Actual params: [0.7469, 0.452 ]
-Original Grad: 0.548, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.338, -lr * Pred Grad: -0.084, New P: 0.368
iter 3 loss: 0.393
Actual params: [0.8348, 0.3683]
-Original Grad: 0.248, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.086, -lr * Pred Grad: -0.084, New P: 0.284
iter 4 loss: 0.208
Actual params: [0.9231, 0.2841]
-Original Grad: 0.021, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.014, -lr * Pred Grad: -0.084, New P: 0.200
iter 5 loss: 0.162
Actual params: [1.0114, 0.2   ]
-Original Grad: 0.005, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: -0.011, -lr * Pred Grad: -0.082, New P: 0.118
iter 6 loss: 0.162
Actual params: [1.0997, 0.1181]
-Original Grad: 0.005, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: -0.007, -lr * Pred Grad: -0.074, New P: 0.044
iter 7 loss: 0.169
Actual params: [1.1881, 0.0442]
-Original Grad: 0.004, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: -0.005, -lr * Pred Grad: -0.070, New P: -0.025
iter 8 loss: 0.180
Actual params: [ 1.2763, -0.0254]
-Original Grad: 0.001, -lr * Pred Grad: 0.088, New P: 1.365
-Original Grad: -0.001, -lr * Pred Grad: -0.066, New P: -0.091
iter 9 loss: 0.199
Actual params: [ 1.3645, -0.0915]
-Original Grad: -0.003, -lr * Pred Grad: 0.088, New P: 1.452
-Original Grad: 0.007, -lr * Pred Grad: -0.054, New P: -0.145
iter 10 loss: 0.229
Actual params: [ 1.4521, -0.145 ]
-Original Grad: -0.011, -lr * Pred Grad: 0.083, New P: 1.535
-Original Grad: 0.004, -lr * Pred Grad: -0.039, New P: -0.184
iter 11 loss: 0.262
Actual params: [ 1.5352, -0.1844]
-Original Grad: -0.010, -lr * Pred Grad: 0.068, New P: 1.604
-Original Grad: 0.007, -lr * Pred Grad: -0.032, New P: -0.217
iter 12 loss: 0.278
Actual params: [ 1.6035, -0.2169]
-Original Grad: -0.012, -lr * Pred Grad: 0.050, New P: 1.653
-Original Grad: 0.005, -lr * Pred Grad: -0.031, New P: -0.248
iter 13 loss: 0.290
Actual params: [ 1.6531, -0.248 ]
-Original Grad: -0.014, -lr * Pred Grad: 0.004, New P: 1.657
-Original Grad: 0.008, -lr * Pred Grad: -0.029, New P: -0.277
iter 14 loss: 0.285
Actual params: [ 1.6575, -0.2773]
-Original Grad: -0.018, -lr * Pred Grad: 0.056, New P: 1.713
-Original Grad: 0.004, -lr * Pred Grad: -0.026, New P: -0.304
iter 15 loss: 0.299
Actual params: [ 1.713 , -0.3035]
-Original Grad: -0.017, -lr * Pred Grad: 0.053, New P: 1.766
-Original Grad: 0.009, -lr * Pred Grad: -0.012, New P: -0.316
iter 16 loss: 0.318
Actual params: [ 1.7659, -0.3156]
-Original Grad: -0.012, -lr * Pred Grad: 0.067, New P: 1.833
-Original Grad: 0.018, -lr * Pred Grad: -0.000, New P: -0.316
iter 17 loss: 0.339
Actual params: [ 1.8329, -0.316 ]
-Original Grad: -0.018, -lr * Pred Grad: 0.018, New P: 1.851
-Original Grad: 0.012, -lr * Pred Grad: -0.009, New P: -0.325
iter 18 loss: 0.344
Actual params: [ 1.8507, -0.3251]
-Original Grad: -0.018, -lr * Pred Grad: 0.012, New P: 1.863
-Original Grad: 0.020, -lr * Pred Grad: -0.006, New P: -0.331
iter 19 loss: 0.348
Actual params: [ 1.8628, -0.3309]
-Original Grad: -0.024, -lr * Pred Grad: -0.032, New P: 1.831
-Original Grad: -0.001, -lr * Pred Grad: -0.015, New P: -0.346
iter 20 loss: 0.330
Actual params: [ 1.8313, -0.3457]
-Original Grad: -0.018, -lr * Pred Grad: -0.023, New P: 1.809
-Original Grad: 0.004, -lr * Pred Grad: -0.020, New P: -0.366
Target params: [1.1812, 0.2779]
iter 0 loss: 0.857
Actual params: [0.5941, 0.5941]
-Original Grad: 0.165, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.313, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.846
Actual params: [0.6612, 0.5309]
-Original Grad: 0.136, -lr * Pred Grad: 0.084, New P: 0.745
-Original Grad: -0.302, -lr * Pred Grad: -0.080, New P: 0.450
iter 2 loss: 0.822
Actual params: [0.7453, 0.4504]
-Original Grad: 0.149, -lr * Pred Grad: 0.087, New P: 0.832
-Original Grad: -0.421, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.786
Actual params: [0.8322, 0.3668]
-Original Grad: 0.077, -lr * Pred Grad: 0.087, New P: 0.919
-Original Grad: -0.451, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.734
Actual params: [0.9188, 0.2826]
-Original Grad: 0.038, -lr * Pred Grad: 0.076, New P: 0.995
-Original Grad: -0.267, -lr * Pred Grad: -0.084, New P: 0.198
iter 5 loss: 0.643
Actual params: [0.9951, 0.1982]
-Original Grad: 0.027, -lr * Pred Grad: 0.023, New P: 1.018
-Original Grad: -0.210, -lr * Pred Grad: -0.084, New P: 0.114
iter 6 loss: 0.591
Actual params: [1.0184, 0.1138]
-Original Grad: 0.006, -lr * Pred Grad: 0.047, New P: 1.066
-Original Grad: -0.138, -lr * Pred Grad: -0.084, New P: 0.029
iter 7 loss: 0.576
Actual params: [1.0658, 0.0294]
-Original Grad: 0.001, -lr * Pred Grad: -0.016, New P: 1.050
-Original Grad: -0.131, -lr * Pred Grad: -0.084, New P: -0.055
iter 8 loss: 0.575
Actual params: [ 1.05 , -0.055]
-Original Grad: -0.025, -lr * Pred Grad: -0.005, New P: 1.045
-Original Grad: -0.113, -lr * Pred Grad: -0.084, New P: -0.139
iter 9 loss: 0.577
Actual params: [ 1.0451, -0.1393]
-Original Grad: -0.033, -lr * Pred Grad: -0.039, New P: 1.006
-Original Grad: -0.063, -lr * Pred Grad: -0.083, New P: -0.223
iter 10 loss: 0.570
Actual params: [ 1.0061, -0.2227]
-Original Grad: -0.023, -lr * Pred Grad: -0.036, New P: 0.970
-Original Grad: -0.063, -lr * Pred Grad: -0.079, New P: -0.302
iter 11 loss: 0.558
Actual params: [ 0.9701, -0.3021]
-Original Grad: -0.015, -lr * Pred Grad: -0.026, New P: 0.944
-Original Grad: -0.048, -lr * Pred Grad: -0.073, New P: -0.375
iter 12 loss: 0.555
Actual params: [ 0.9438, -0.3747]
-Original Grad: -0.045, -lr * Pred Grad: -0.033, New P: 0.910
-Original Grad: -0.026, -lr * Pred Grad: -0.069, New P: -0.444
iter 13 loss: 0.551
Actual params: [ 0.9104, -0.4442]
-Original Grad: -0.014, -lr * Pred Grad: -0.029, New P: 0.881
-Original Grad: -0.046, -lr * Pred Grad: -0.067, New P: -0.511
iter 14 loss: 0.546
Actual params: [ 0.8812, -0.5114]
-Original Grad: -0.004, -lr * Pred Grad: -0.018, New P: 0.863
-Original Grad: 0.039, -lr * Pred Grad: -0.057, New P: -0.568
iter 15 loss: 0.537
Actual params: [ 0.8629, -0.5685]
-Original Grad: -0.006, -lr * Pred Grad: -0.013, New P: 0.850
-Original Grad: 0.095, -lr * Pred Grad: -0.038, New P: -0.607
iter 16 loss: 0.529
Actual params: [ 0.8499, -0.6065]
-Original Grad: -0.035, -lr * Pred Grad: -0.024, New P: 0.826
-Original Grad: 0.184, -lr * Pred Grad: -0.029, New P: -0.636
iter 17 loss: 0.523
Actual params: [ 0.8256, -0.6358]
-Original Grad: -0.019, -lr * Pred Grad: -0.028, New P: 0.798
-Original Grad: 0.208, -lr * Pred Grad: -0.025, New P: -0.661
iter 18 loss: 0.517
Actual params: [ 0.7976, -0.6608]
-Original Grad: 0.011, -lr * Pred Grad: -0.016, New P: 0.781
-Original Grad: 0.231, -lr * Pred Grad: -0.023, New P: -0.684
iter 19 loss: 0.513
Actual params: [ 0.7813, -0.6838]
-Original Grad: 0.034, -lr * Pred Grad: 0.004, New P: 0.786
-Original Grad: 0.207, -lr * Pred Grad: -0.022, New P: -0.706
iter 20 loss: 0.472
Actual params: [ 0.7856, -0.7061]
-Original Grad: 0.016, -lr * Pred Grad: 0.011, New P: 0.796
-Original Grad: 0.373, -lr * Pred Grad: -0.020, New P: -0.726
Target params: [1.1812, 0.2779]
iter 0 loss: 0.309
Actual params: [0.5941, 0.5941]
-Original Grad: 0.182, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.139, -lr * Pred Grad: -0.058, New P: 0.536
iter 1 loss: 0.287
Actual params: [0.6613, 0.5365]
-Original Grad: 0.108, -lr * Pred Grad: 0.084, New P: 0.745
-Original Grad: -0.107, -lr * Pred Grad: -0.079, New P: 0.458
iter 2 loss: 0.288
Actual params: [0.7454, 0.4576]
-Original Grad: -0.144, -lr * Pred Grad: 0.049, New P: 0.794
-Original Grad: -0.035, -lr * Pred Grad: -0.079, New P: 0.379
iter 3 loss: 0.293
Actual params: [0.7939, 0.3791]
-Original Grad: -0.171, -lr * Pred Grad: -0.045, New P: 0.749
-Original Grad: -0.029, -lr * Pred Grad: -0.072, New P: 0.307
iter 4 loss: 0.283
Actual params: [0.7489, 0.3074]
-Original Grad: -0.084, -lr * Pred Grad: -0.060, New P: 0.689
-Original Grad: -0.004, -lr * Pred Grad: -0.068, New P: 0.239
iter 5 loss: 0.291
Actual params: [0.689 , 0.2391]
-Original Grad: -0.012, -lr * Pred Grad: -0.056, New P: 0.633
-Original Grad: 0.008, -lr * Pred Grad: -0.061, New P: 0.178
iter 6 loss: 0.301
Actual params: [0.6334, 0.1782]
-Original Grad: 0.031, -lr * Pred Grad: -0.029, New P: 0.605
-Original Grad: 0.009, -lr * Pred Grad: -0.044, New P: 0.134
iter 7 loss: 0.305
Actual params: [0.6048, 0.1343]
-Original Grad: 0.052, -lr * Pred Grad: 0.019, New P: 0.624
-Original Grad: 0.014, -lr * Pred Grad: -0.029, New P: 0.105
iter 8 loss: 0.301
Actual params: [0.6242, 0.1054]
-Original Grad: 0.046, -lr * Pred Grad: 0.051, New P: 0.676
-Original Grad: 0.018, -lr * Pred Grad: -0.009, New P: 0.096
iter 9 loss: 0.292
Actual params: [0.6756, 0.0962]
-Original Grad: -0.001, -lr * Pred Grad: -0.006, New P: 0.669
-Original Grad: 0.013, -lr * Pred Grad: 0.010, New P: 0.106
iter 10 loss: 0.293
Actual params: [0.6694, 0.1061]
-Original Grad: 0.003, -lr * Pred Grad: 0.010, New P: 0.680
-Original Grad: 0.015, -lr * Pred Grad: -0.007, New P: 0.100
iter 11 loss: 0.291
Actual params: [0.6798, 0.0996]
-Original Grad: 0.003, -lr * Pred Grad: -0.019, New P: 0.660
-Original Grad: 0.015, -lr * Pred Grad: -0.004, New P: 0.095
iter 12 loss: 0.294
Actual params: [0.6605, 0.0954]
-Original Grad: 0.014, -lr * Pred Grad: 0.006, New P: 0.666
-Original Grad: 0.015, -lr * Pred Grad: -0.007, New P: 0.089
iter 13 loss: 0.293
Actual params: [0.6663, 0.0887]
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 0.659
-Original Grad: 0.013, -lr * Pred Grad: -0.008, New P: 0.081
iter 14 loss: 0.294
Actual params: [0.6592, 0.0807]
-Original Grad: 0.030, -lr * Pred Grad: 0.011, New P: 0.671
-Original Grad: 0.018, -lr * Pred Grad: -0.007, New P: 0.074
iter 15 loss: 0.292
Actual params: [0.6705, 0.0742]
-Original Grad: -0.008, -lr * Pred Grad: -0.011, New P: 0.660
-Original Grad: 0.010, -lr * Pred Grad: -0.009, New P: 0.065
iter 16 loss: 0.293
Actual params: [0.6595, 0.0649]
-Original Grad: 0.008, -lr * Pred Grad: -0.009, New P: 0.651
-Original Grad: 0.011, -lr * Pred Grad: -0.012, New P: 0.053
iter 17 loss: 0.294
Actual params: [0.6505, 0.0526]
-Original Grad: 0.014, -lr * Pred Grad: -0.004, New P: 0.647
-Original Grad: 0.009, -lr * Pred Grad: -0.015, New P: 0.038
iter 18 loss: 0.294
Actual params: [0.6467, 0.0378]
-Original Grad: 0.029, -lr * Pred Grad: 0.008, New P: 0.655
-Original Grad: 0.019, -lr * Pred Grad: -0.011, New P: 0.027
iter 19 loss: 0.293
Actual params: [0.6549, 0.0268]
-Original Grad: 0.026, -lr * Pred Grad: 0.012, New P: 0.667
-Original Grad: 0.012, -lr * Pred Grad: -0.010, New P: 0.017
iter 20 loss: 0.291
Actual params: [0.6672, 0.0167]
-Original Grad: 0.003, -lr * Pred Grad: -0.001, New P: 0.666
-Original Grad: 0.012, -lr * Pred Grad: -0.011, New P: 0.005
Target params: [1.1812, 0.2779]
iter 0 loss: 0.966
Actual params: [0.5941, 0.5941]
-Original Grad: 0.189, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.275, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.890
Actual params: [0.6614, 0.531 ]
-Original Grad: 0.340, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.297, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.715
Actual params: [0.7462, 0.4505]
-Original Grad: 0.418, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.148, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.524
Actual params: [0.834, 0.367]
-Original Grad: 0.368, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: -0.342, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.352
Actual params: [0.9222, 0.2828]
-Original Grad: 0.127, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.031, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.265
Actual params: [1.0106, 0.1986]
-Original Grad: 0.075, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 0.066, -lr * Pred Grad: -0.081, New P: 0.118
iter 6 loss: 0.270
Actual params: [1.0989, 0.1179]
-Original Grad: 0.058, -lr * Pred Grad: 0.088, New P: 1.187
-Original Grad: 0.038, -lr * Pred Grad: -0.072, New P: 0.046
iter 7 loss: 0.286
Actual params: [1.1872, 0.0457]
-Original Grad: 0.031, -lr * Pred Grad: 0.088, New P: 1.275
-Original Grad: 0.026, -lr * Pred Grad: -0.069, New P: -0.023
iter 8 loss: 0.299
Actual params: [ 1.2755, -0.023 ]
-Original Grad: 0.025, -lr * Pred Grad: 0.088, New P: 1.364
-Original Grad: 0.012, -lr * Pred Grad: -0.058, New P: -0.081
iter 9 loss: 0.310
Actual params: [ 1.3637, -0.0815]
-Original Grad: 0.017, -lr * Pred Grad: 0.088, New P: 1.451
-Original Grad: 0.009, -lr * Pred Grad: -0.041, New P: -0.122
iter 10 loss: 0.328
Actual params: [ 1.4514, -0.1224]
-Original Grad: 0.012, -lr * Pred Grad: 0.084, New P: 1.535
-Original Grad: 0.005, -lr * Pred Grad: -0.033, New P: -0.155
iter 11 loss: 0.335
Actual params: [ 1.5349, -0.1552]
-Original Grad: 0.004, -lr * Pred Grad: 0.067, New P: 1.602
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: -0.187
iter 12 loss: 0.340
Actual params: [ 1.6022, -0.1865]
-Original Grad: -0.004, -lr * Pred Grad: 0.009, New P: 1.611
-Original Grad: -0.005, -lr * Pred Grad: -0.030, New P: -0.217
iter 13 loss: 0.327
Actual params: [ 1.6115, -0.2167]
-Original Grad: -0.007, -lr * Pred Grad: 0.062, New P: 1.673
-Original Grad: -0.003, -lr * Pred Grad: -0.021, New P: -0.238
iter 14 loss: 0.333
Actual params: [ 1.6734, -0.2376]
-Original Grad: -0.015, -lr * Pred Grad: 0.060, New P: 1.733
-Original Grad: 0.012, -lr * Pred Grad: -0.007, New P: -0.245
iter 15 loss: 0.346
Actual params: [ 1.733 , -0.2448]
-Original Grad: -0.030, -lr * Pred Grad: 0.044, New P: 1.777
-Original Grad: 0.008, -lr * Pred Grad: -0.011, New P: -0.256
iter 16 loss: 0.353
Actual params: [ 1.7768, -0.2563]
-Original Grad: -0.046, -lr * Pred Grad: -0.013, New P: 1.764
-Original Grad: 0.008, -lr * Pred Grad: -0.013, New P: -0.269
iter 17 loss: 0.342
Actual params: [ 1.7641, -0.2691]
-Original Grad: -0.045, -lr * Pred Grad: -0.031, New P: 1.733
-Original Grad: 0.009, -lr * Pred Grad: -0.014, New P: -0.283
iter 18 loss: 0.325
Actual params: [ 1.733 , -0.2832]
-Original Grad: -0.026, -lr * Pred Grad: -0.034, New P: 1.699
-Original Grad: 0.020, -lr * Pred Grad: -0.009, New P: -0.293
iter 19 loss: 0.316
Actual params: [ 1.6991, -0.2925]
-Original Grad: -0.021, -lr * Pred Grad: -0.028, New P: 1.671
-Original Grad: 0.024, -lr * Pred Grad: -0.002, New P: -0.294
iter 20 loss: 0.309
Actual params: [ 1.671 , -0.2944]
-Original Grad: -0.024, -lr * Pred Grad: -0.028, New P: 1.643
-Original Grad: 0.011, -lr * Pred Grad: -0.005, New P: -0.300
Target params: [1.1812, 0.2779]
iter 0 loss: 0.386
Actual params: [0.5941, 0.5941]
-Original Grad: 0.090, -lr * Pred Grad: 0.064, New P: 0.658
-Original Grad: -0.242, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.340
Actual params: [0.6584, 0.5312]
-Original Grad: 0.032, -lr * Pred Grad: 0.076, New P: 0.734
-Original Grad: -0.187, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.304
Actual params: [0.7344, 0.4507]
-Original Grad: 0.016, -lr * Pred Grad: -0.002, New P: 0.732
-Original Grad: -0.125, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.290
Actual params: [0.7321, 0.3672]
-Original Grad: 0.022, -lr * Pred Grad: 0.016, New P: 0.748
-Original Grad: -0.042, -lr * Pred Grad: -0.083, New P: 0.284
iter 4 loss: 0.279
Actual params: [0.7482, 0.284 ]
-Original Grad: 0.010, -lr * Pred Grad: -0.003, New P: 0.745
-Original Grad: -0.021, -lr * Pred Grad: -0.078, New P: 0.206
iter 5 loss: 0.272
Actual params: [0.7449, 0.2061]
-Original Grad: 0.018, -lr * Pred Grad: 0.023, New P: 0.768
-Original Grad: -0.008, -lr * Pred Grad: -0.071, New P: 0.135
iter 6 loss: 0.265
Actual params: [0.7679, 0.1352]
-Original Grad: 0.011, -lr * Pred Grad: -0.009, New P: 0.759
-Original Grad: -0.004, -lr * Pred Grad: -0.068, New P: 0.067
iter 7 loss: 0.266
Actual params: [0.7588, 0.067 ]
-Original Grad: 0.011, -lr * Pred Grad: 0.005, New P: 0.764
-Original Grad: 0.007, -lr * Pred Grad: -0.060, New P: 0.007
iter 8 loss: 0.267
Actual params: [0.7635, 0.0069]
-Original Grad: 0.013, -lr * Pred Grad: -0.008, New P: 0.755
-Original Grad: 0.013, -lr * Pred Grad: -0.043, New P: -0.036
iter 9 loss: 0.271
Actual params: [ 0.7553, -0.0362]
-Original Grad: 0.020, -lr * Pred Grad: 0.005, New P: 0.760
-Original Grad: 0.019, -lr * Pred Grad: -0.032, New P: -0.068
iter 10 loss: 0.272
Actual params: [ 0.7603, -0.068 ]
-Original Grad: 0.015, -lr * Pred Grad: 0.000, New P: 0.761
-Original Grad: 0.027, -lr * Pred Grad: -0.025, New P: -0.094
iter 11 loss: 0.274
Actual params: [ 0.7607, -0.0935]
-Original Grad: 0.015, -lr * Pred Grad: 0.001, New P: 0.762
-Original Grad: 0.019, -lr * Pred Grad: -0.018, New P: -0.112
iter 12 loss: 0.276
Actual params: [ 0.7615, -0.1116]
-Original Grad: 0.018, -lr * Pred Grad: 0.001, New P: 0.763
-Original Grad: 0.023, -lr * Pred Grad: 0.001, New P: -0.111
iter 13 loss: 0.275
Actual params: [ 0.7628, -0.1111]
-Original Grad: 0.010, -lr * Pred Grad: -0.002, New P: 0.761
-Original Grad: 0.024, -lr * Pred Grad: 0.014, New P: -0.097
iter 14 loss: 0.274
Actual params: [ 0.7609, -0.0972]
-Original Grad: 0.015, -lr * Pred Grad: -0.001, New P: 0.760
-Original Grad: 0.022, -lr * Pred Grad: -0.003, New P: -0.100
iter 15 loss: 0.275
Actual params: [ 0.76, -0.1 ]
-Original Grad: 0.015, -lr * Pred Grad: 0.001, New P: 0.761
-Original Grad: 0.028, -lr * Pred Grad: 0.008, New P: -0.092
iter 16 loss: 0.274
Actual params: [ 0.7605, -0.0923]
-Original Grad: 0.013, -lr * Pred Grad: 0.000, New P: 0.761
-Original Grad: 0.022, -lr * Pred Grad: 0.002, New P: -0.090
iter 17 loss: 0.274
Actual params: [ 0.761 , -0.0904]
-Original Grad: 0.013, -lr * Pred Grad: 0.000, New P: 0.761
-Original Grad: 0.026, -lr * Pred Grad: 0.004, New P: -0.087
iter 18 loss: 0.274
Actual params: [ 0.7613, -0.0867]
-Original Grad: 0.015, -lr * Pred Grad: 0.002, New P: 0.763
-Original Grad: 0.017, -lr * Pred Grad: -0.002, New P: -0.089
iter 19 loss: 0.274
Actual params: [ 0.7629, -0.0891]
-Original Grad: 0.021, -lr * Pred Grad: 0.005, New P: 0.768
-Original Grad: 0.033, -lr * Pred Grad: 0.003, New P: -0.087
iter 20 loss: 0.273
Actual params: [ 0.7682, -0.0865]
-Original Grad: 0.012, -lr * Pred Grad: 0.004, New P: 0.772
-Original Grad: 0.026, -lr * Pred Grad: 0.004, New P: -0.083
Target params: [1.1812, 0.2779]
iter 0 loss: 0.519
Actual params: [0.5941, 0.5941]
-Original Grad: 0.119, -lr * Pred Grad: 0.066, New P: 0.660
-Original Grad: -0.227, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.509
Actual params: [0.6601, 0.5314]
-Original Grad: 0.131, -lr * Pred Grad: 0.083, New P: 0.744
-Original Grad: -0.087, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.503
Actual params: [0.7436, 0.451 ]
-Original Grad: 0.110, -lr * Pred Grad: 0.086, New P: 0.829
-Original Grad: 0.025, -lr * Pred Grad: -0.079, New P: 0.372
iter 3 loss: 0.493
Actual params: [0.8293, 0.3717]
-Original Grad: 0.016, -lr * Pred Grad: 0.070, New P: 0.899
-Original Grad: 0.011, -lr * Pred Grad: -0.071, New P: 0.300
iter 4 loss: 0.478
Actual params: [0.8994, 0.3003]
-Original Grad: -0.008, -lr * Pred Grad: -0.005, New P: 0.894
-Original Grad: 0.026, -lr * Pred Grad: -0.067, New P: 0.233
iter 5 loss: 0.461
Actual params: [0.8944, 0.2329]
-Original Grad: 0.024, -lr * Pred Grad: 0.041, New P: 0.935
-Original Grad: 0.051, -lr * Pred Grad: -0.051, New P: 0.182
iter 6 loss: 0.451
Actual params: [0.935 , 0.1815]
-Original Grad: 0.021, -lr * Pred Grad: 0.005, New P: 0.940
-Original Grad: 0.059, -lr * Pred Grad: -0.029, New P: 0.153
iter 7 loss: 0.445
Actual params: [0.9401, 0.1526]
-Original Grad: 0.003, -lr * Pred Grad: 0.012, New P: 0.952
-Original Grad: 0.063, -lr * Pred Grad: -0.000, New P: 0.152
iter 8 loss: 0.444
Actual params: [0.9517, 0.1521]
-Original Grad: 0.006, -lr * Pred Grad: -0.020, New P: 0.932
-Original Grad: 0.050, -lr * Pred Grad: 0.041, New P: 0.193
iter 9 loss: 0.454
Actual params: [0.9319, 0.1934]
-Original Grad: 0.016, -lr * Pred Grad: 0.001, New P: 0.932
-Original Grad: 0.060, -lr * Pred Grad: 0.053, New P: 0.246
iter 10 loss: 0.466
Actual params: [0.9325, 0.2459]
-Original Grad: -0.015, -lr * Pred Grad: -0.022, New P: 0.911
-Original Grad: 0.033, -lr * Pred Grad: 0.023, New P: 0.269
iter 11 loss: 0.469
Actual params: [0.9105, 0.2692]
-Original Grad: 0.009, -lr * Pred Grad: -0.011, New P: 0.900
-Original Grad: 0.051, -lr * Pred Grad: 0.048, New P: 0.318
iter 12 loss: 0.485
Actual params: [0.8999, 0.3176]
-Original Grad: -0.011, -lr * Pred Grad: -0.017, New P: 0.883
-Original Grad: 0.014, -lr * Pred Grad: -0.006, New P: 0.312
iter 13 loss: 0.481
Actual params: [0.8828, 0.3117]
-Original Grad: 0.018, -lr * Pred Grad: -0.006, New P: 0.877
-Original Grad: 0.038, -lr * Pred Grad: 0.035, New P: 0.347
iter 14 loss: 0.491
Actual params: [0.8769, 0.347 ]
-Original Grad: -0.024, -lr * Pred Grad: -0.018, New P: 0.859
-Original Grad: -0.012, -lr * Pred Grad: -0.024, New P: 0.323
iter 15 loss: 0.485
Actual params: [0.8586, 0.3234]
-Original Grad: 0.011, -lr * Pred Grad: -0.014, New P: 0.845
-Original Grad: 0.027, -lr * Pred Grad: 0.012, New P: 0.335
iter 16 loss: 0.487
Actual params: [0.8448, 0.3354]
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: 0.838
-Original Grad: 0.019, -lr * Pred Grad: 0.001, New P: 0.336
iter 17 loss: 0.487
Actual params: [0.838 , 0.3359]
-Original Grad: 0.032, -lr * Pred Grad: 0.008, New P: 0.846
-Original Grad: 0.031, -lr * Pred Grad: 0.020, New P: 0.356
iter 18 loss: 0.490
Actual params: [0.8456, 0.356 ]
-Original Grad: 0.023, -lr * Pred Grad: 0.012, New P: 0.858
-Original Grad: 0.020, -lr * Pred Grad: 0.003, New P: 0.359
iter 19 loss: 0.491
Actual params: [0.8581, 0.3593]
-Original Grad: -0.007, -lr * Pred Grad: -0.007, New P: 0.851
-Original Grad: 0.005, -lr * Pred Grad: -0.002, New P: 0.358
iter 20 loss: 0.489
Actual params: [0.8514, 0.3576]
-Original Grad: 0.014, -lr * Pred Grad: -0.008, New P: 0.843
-Original Grad: 0.014, -lr * Pred Grad: -0.005, New P: 0.353
Target params: [1.1812, 0.2779]
iter 0 loss: 0.594
Actual params: [0.5941, 0.5941]
-Original Grad: 0.371, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.147, -lr * Pred Grad: -0.059, New P: 0.535
iter 1 loss: 0.577
Actual params: [0.6617, 0.5354]
-Original Grad: 0.231, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.258, -lr * Pred Grad: -0.080, New P: 0.456
iter 2 loss: 0.525
Actual params: [0.7467, 0.4558]
-Original Grad: 0.244, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.180, -lr * Pred Grad: -0.083, New P: 0.372
iter 3 loss: 0.434
Actual params: [0.8345, 0.3725]
-Original Grad: 0.093, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.057, -lr * Pred Grad: -0.084, New P: 0.289
iter 4 loss: 0.381
Actual params: [0.9227, 0.2889]
-Original Grad: -0.011, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.026, -lr * Pred Grad: -0.080, New P: 0.209
iter 5 loss: 0.402
Actual params: [1.011 , 0.2089]
-Original Grad: -0.048, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 0.047, -lr * Pred Grad: -0.072, New P: 0.137
iter 6 loss: 0.449
Actual params: [1.0991, 0.1372]
-Original Grad: -0.083, -lr * Pred Grad: 0.086, New P: 1.185
-Original Grad: 0.065, -lr * Pred Grad: -0.068, New P: 0.070
iter 7 loss: 0.507
Actual params: [1.1849, 0.0696]
-Original Grad: -0.103, -lr * Pred Grad: -0.003, New P: 1.182
-Original Grad: 0.102, -lr * Pred Grad: -0.052, New P: 0.018
iter 8 loss: 0.517
Actual params: [1.1817, 0.0178]
-Original Grad: -0.121, -lr * Pred Grad: -0.043, New P: 1.139
-Original Grad: 0.140, -lr * Pred Grad: -0.035, New P: -0.017
iter 9 loss: 0.502
Actual params: [ 1.1389, -0.0168]
-Original Grad: -0.131, -lr * Pred Grad: -0.047, New P: 1.092
-Original Grad: 0.136, -lr * Pred Grad: -0.024, New P: -0.041
iter 10 loss: 0.480
Actual params: [ 1.0923, -0.0405]
-Original Grad: -0.100, -lr * Pred Grad: -0.050, New P: 1.042
-Original Grad: 0.133, -lr * Pred Grad: -0.006, New P: -0.047
iter 11 loss: 0.459
Actual params: [ 1.0425, -0.0465]
-Original Grad: -0.075, -lr * Pred Grad: -0.053, New P: 0.989
-Original Grad: 0.112, -lr * Pred Grad: 0.035, New P: -0.011
iter 12 loss: 0.432
Actual params: [ 0.9894, -0.0114]
-Original Grad: -0.060, -lr * Pred Grad: -0.053, New P: 0.936
-Original Grad: 0.077, -lr * Pred Grad: 0.072, New P: 0.061
iter 13 loss: 0.406
Actual params: [0.9364, 0.0607]
-Original Grad: -0.056, -lr * Pred Grad: -0.050, New P: 0.886
-Original Grad: 0.088, -lr * Pred Grad: 0.078, New P: 0.138
iter 14 loss: 0.413
Actual params: [0.8859, 0.1383]
-Original Grad: -0.004, -lr * Pred Grad: -0.036, New P: 0.850
-Original Grad: 0.065, -lr * Pred Grad: 0.067, New P: 0.205
iter 15 loss: 0.438
Actual params: [0.85  , 0.2048]
-Original Grad: 0.071, -lr * Pred Grad: -0.002, New P: 0.848
-Original Grad: 0.013, -lr * Pred Grad: 0.029, New P: 0.234
iter 16 loss: 0.439
Actual params: [0.8482, 0.234 ]
-Original Grad: 0.101, -lr * Pred Grad: 0.028, New P: 0.876
-Original Grad: 0.001, -lr * Pred Grad: 0.010, New P: 0.244
iter 17 loss: 0.408
Actual params: [0.8765, 0.2444]
-Original Grad: 0.032, -lr * Pred Grad: 0.051, New P: 0.927
-Original Grad: -0.001, -lr * Pred Grad: -0.019, New P: 0.226
iter 18 loss: 0.386
Actual params: [0.9274, 0.2255]
-Original Grad: -0.023, -lr * Pred Grad: 0.000, New P: 0.928
-Original Grad: 0.012, -lr * Pred Grad: -0.003, New P: 0.222
iter 19 loss: 0.386
Actual params: [0.9276, 0.2221]
-Original Grad: -0.012, -lr * Pred Grad: -0.022, New P: 0.905
-Original Grad: 0.026, -lr * Pred Grad: -0.001, New P: 0.221
iter 20 loss: 0.389
Actual params: [0.9053, 0.2215]
-Original Grad: -0.019, -lr * Pred Grad: -0.026, New P: 0.879
-Original Grad: 0.008, -lr * Pred Grad: -0.005, New P: 0.216
Target params: [1.1812, 0.2779]
iter 0 loss: 0.818
Actual params: [0.5941, 0.5941]
-Original Grad: 0.451, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.136, -lr * Pred Grad: -0.057, New P: 0.537
iter 1 loss: 0.743
Actual params: [0.6617, 0.5369]
-Original Grad: 0.515, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.083, -lr * Pred Grad: -0.078, New P: 0.459
iter 2 loss: 0.667
Actual params: [0.7468, 0.4589]
-Original Grad: 0.541, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.030, -lr * Pred Grad: -0.076, New P: 0.383
iter 3 loss: 0.538
Actual params: [0.8347, 0.3829]
-Original Grad: 0.579, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: 0.063, -lr * Pred Grad: -0.070, New P: 0.313
iter 4 loss: 0.412
Actual params: [0.923 , 0.3132]
-Original Grad: 0.188, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: 0.054, -lr * Pred Grad: -0.059, New P: 0.254
iter 5 loss: 0.347
Actual params: [1.0113, 0.254 ]
-Original Grad: -0.068, -lr * Pred Grad: 0.088, New P: 1.100
-Original Grad: 0.056, -lr * Pred Grad: -0.032, New P: 0.222
iter 6 loss: 0.358
Actual params: [1.0997, 0.2223]
-Original Grad: -0.068, -lr * Pred Grad: 0.088, New P: 1.188
-Original Grad: 0.060, -lr * Pred Grad: 0.007, New P: 0.229
iter 7 loss: 0.394
Actual params: [1.188 , 0.2294]
-Original Grad: -0.047, -lr * Pred Grad: 0.088, New P: 1.276
-Original Grad: 0.051, -lr * Pred Grad: 0.051, New P: 0.280
iter 8 loss: 0.424
Actual params: [1.2763, 0.2802]
-Original Grad: -0.061, -lr * Pred Grad: 0.088, New P: 1.364
-Original Grad: 0.096, -lr * Pred Grad: 0.066, New P: 0.346
iter 9 loss: 0.463
Actual params: [1.3645, 0.346 ]
-Original Grad: -0.074, -lr * Pred Grad: 0.087, New P: 1.452
-Original Grad: 0.074, -lr * Pred Grad: 0.067, New P: 0.413
iter 10 loss: 0.507
Actual params: [1.4516, 0.4133]
-Original Grad: -0.102, -lr * Pred Grad: 0.074, New P: 1.526
-Original Grad: 0.090, -lr * Pred Grad: 0.073, New P: 0.486
iter 11 loss: 0.562
Actual params: [1.5258, 0.4858]
-Original Grad: -0.112, -lr * Pred Grad: 0.008, New P: 1.534
-Original Grad: 0.091, -lr * Pred Grad: 0.074, New P: 0.560
iter 12 loss: 0.582
Actual params: [1.5342, 0.5596]
-Original Grad: -0.107, -lr * Pred Grad: -0.007, New P: 1.528
-Original Grad: 0.024, -lr * Pred Grad: 0.039, New P: 0.598
iter 13 loss: 0.589
Actual params: [1.5275, 0.5984]
-Original Grad: -0.121, -lr * Pred Grad: -0.043, New P: 1.485
-Original Grad: -0.002, -lr * Pred Grad: 0.009, New P: 0.607
iter 14 loss: 0.565
Actual params: [1.4846, 0.6074]
-Original Grad: -0.080, -lr * Pred Grad: -0.032, New P: 1.452
-Original Grad: 0.005, -lr * Pred Grad: -0.013, New P: 0.595
iter 15 loss: 0.537
Actual params: [1.4524, 0.5948]
-Original Grad: -0.074, -lr * Pred Grad: -0.036, New P: 1.416
-Original Grad: -0.010, -lr * Pred Grad: -0.015, New P: 0.580
iter 16 loss: 0.493
Actual params: [1.4163, 0.5795]
-Original Grad: -0.039, -lr * Pred Grad: -0.039, New P: 1.377
-Original Grad: -0.043, -lr * Pred Grad: -0.037, New P: 0.543
iter 17 loss: 0.428
Actual params: [1.3774, 0.5429]
-Original Grad: -0.043, -lr * Pred Grad: -0.038, New P: 1.340
-Original Grad: -0.024, -lr * Pred Grad: -0.036, New P: 0.507
iter 18 loss: 0.399
Actual params: [1.3397, 0.5069]
-Original Grad: -0.040, -lr * Pred Grad: -0.035, New P: 1.304
-Original Grad: 0.008, -lr * Pred Grad: -0.018, New P: 0.489
iter 19 loss: 0.378
Actual params: [1.3045, 0.4889]
-Original Grad: -0.038, -lr * Pred Grad: -0.034, New P: 1.271
-Original Grad: 0.019, -lr * Pred Grad: 0.001, New P: 0.490
iter 20 loss: 0.353
Actual params: [1.2709, 0.4898]
-Original Grad: -0.040, -lr * Pred Grad: -0.034, New P: 1.237
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.490
Target params: [1.1812, 0.2779]
iter 0 loss: 0.555
Actual params: [0.5941, 0.5941]
-Original Grad: 0.139, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.318, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.475
Actual params: [0.6607, 0.5309]
-Original Grad: 0.069, -lr * Pred Grad: 0.083, New P: 0.744
-Original Grad: -0.186, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.394
Actual params: [0.7438, 0.4503]
-Original Grad: 0.054, -lr * Pred Grad: 0.081, New P: 0.825
-Original Grad: -0.077, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.334
Actual params: [0.825 , 0.3668]
-Original Grad: 0.012, -lr * Pred Grad: 0.010, New P: 0.835
-Original Grad: -0.004, -lr * Pred Grad: -0.082, New P: 0.284
iter 4 loss: 0.307
Actual params: [0.8349, 0.2844]
-Original Grad: 0.002, -lr * Pred Grad: 0.034, New P: 0.869
-Original Grad: 0.046, -lr * Pred Grad: -0.074, New P: 0.211
iter 5 loss: 0.289
Actual params: [0.8691, 0.2105]
-Original Grad: -0.006, -lr * Pred Grad: -0.020, New P: 0.849
-Original Grad: 0.042, -lr * Pred Grad: -0.069, New P: 0.141
iter 6 loss: 0.285
Actual params: [0.8494, 0.1413]
-Original Grad: -0.009, -lr * Pred Grad: 0.001, New P: 0.851
-Original Grad: 0.014, -lr * Pred Grad: -0.060, New P: 0.081
iter 7 loss: 0.279
Actual params: [0.8509, 0.0808]
-Original Grad: -0.013, -lr * Pred Grad: -0.029, New P: 0.822
-Original Grad: 0.016, -lr * Pred Grad: -0.041, New P: 0.039
iter 8 loss: 0.279
Actual params: [0.8223, 0.0394]
-Original Grad: -0.010, -lr * Pred Grad: -0.021, New P: 0.802
-Original Grad: 0.011, -lr * Pred Grad: -0.029, New P: 0.011
iter 9 loss: 0.279
Actual params: [0.8017, 0.0106]
-Original Grad: -0.008, -lr * Pred Grad: -0.020, New P: 0.782
-Original Grad: 0.012, -lr * Pred Grad: -0.015, New P: -0.005
iter 10 loss: 0.280
Actual params: [ 0.7821, -0.0047]
-Original Grad: -0.000, -lr * Pred Grad: -0.014, New P: 0.768
-Original Grad: 0.008, -lr * Pred Grad: 0.003, New P: -0.002
iter 11 loss: 0.282
Actual params: [ 0.7684, -0.0019]
-Original Grad: -0.010, -lr * Pred Grad: -0.016, New P: 0.752
-Original Grad: 0.022, -lr * Pred Grad: -0.001, New P: -0.003
iter 12 loss: 0.283
Actual params: [ 0.7521, -0.0034]
-Original Grad: -0.006, -lr * Pred Grad: -0.017, New P: 0.735
-Original Grad: 0.018, -lr * Pred Grad: -0.000, New P: -0.003
iter 13 loss: 0.285
Actual params: [ 0.735 , -0.0035]
-Original Grad: -0.011, -lr * Pred Grad: -0.019, New P: 0.716
-Original Grad: 0.015, -lr * Pred Grad: -0.003, New P: -0.006
iter 14 loss: 0.287
Actual params: [ 0.7161, -0.0063]
-Original Grad: 0.004, -lr * Pred Grad: -0.014, New P: 0.702
-Original Grad: 0.002, -lr * Pred Grad: -0.013, New P: -0.020
iter 15 loss: 0.287
Actual params: [ 0.7021, -0.0195]
-Original Grad: 0.000, -lr * Pred Grad: -0.011, New P: 0.691
-Original Grad: 0.020, -lr * Pred Grad: -0.010, New P: -0.030
iter 16 loss: 0.287
Actual params: [ 0.6914, -0.0297]
-Original Grad: 0.009, -lr * Pred Grad: -0.007, New P: 0.684
-Original Grad: 0.013, -lr * Pred Grad: -0.008, New P: -0.038
iter 17 loss: 0.287
Actual params: [ 0.6844, -0.0379]
-Original Grad: 0.003, -lr * Pred Grad: -0.007, New P: 0.677
-Original Grad: 0.019, -lr * Pred Grad: -0.006, New P: -0.044
iter 18 loss: 0.287
Actual params: [ 0.6773, -0.0438]
-Original Grad: 0.001, -lr * Pred Grad: -0.010, New P: 0.667
-Original Grad: 0.013, -lr * Pred Grad: -0.008, New P: -0.052
iter 19 loss: 0.287
Actual params: [ 0.6674, -0.0517]
-Original Grad: 0.014, -lr * Pred Grad: -0.005, New P: 0.662
-Original Grad: 0.000, -lr * Pred Grad: -0.016, New P: -0.068
iter 20 loss: 0.286
Actual params: [ 0.6625, -0.0678]
-Original Grad: 0.012, -lr * Pred Grad: -0.000, New P: 0.662
-Original Grad: 0.014, -lr * Pred Grad: -0.016, New P: -0.084
Target params: [1.1812, 0.2779]
iter 0 loss: 0.660
Actual params: [0.5941, 0.5941]
-Original Grad: 0.602, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.171, -lr * Pred Grad: -0.061, New P: 0.533
iter 1 loss: 0.656
Actual params: [0.6617, 0.5333]
-Original Grad: 0.306, -lr * Pred Grad: 0.085, New P: 0.747
-Original Grad: -0.158, -lr * Pred Grad: -0.080, New P: 0.453
iter 2 loss: 0.633
Actual params: [0.7469, 0.4532]
-Original Grad: 0.142, -lr * Pred Grad: 0.088, New P: 0.835
-Original Grad: -0.059, -lr * Pred Grad: -0.083, New P: 0.371
iter 3 loss: 0.613
Actual params: [0.8347, 0.3706]
-Original Grad: 0.026, -lr * Pred Grad: 0.088, New P: 0.923
-Original Grad: -0.033, -lr * Pred Grad: -0.078, New P: 0.293
iter 4 loss: 0.598
Actual params: [0.923 , 0.2929]
-Original Grad: 0.006, -lr * Pred Grad: 0.088, New P: 1.011
-Original Grad: -0.011, -lr * Pred Grad: -0.071, New P: 0.222
iter 5 loss: 0.604
Actual params: [1.0112, 0.2219]
-Original Grad: -0.002, -lr * Pred Grad: 0.088, New P: 1.099
-Original Grad: 0.002, -lr * Pred Grad: -0.068, New P: 0.154
iter 6 loss: 0.608
Actual params: [1.0994, 0.1541]
-Original Grad: -0.008, -lr * Pred Grad: 0.087, New P: 1.187
-Original Grad: 0.018, -lr * Pred Grad: -0.058, New P: 0.096
iter 7 loss: 0.616
Actual params: [1.1869, 0.0963]
-Original Grad: -0.012, -lr * Pred Grad: 0.081, New P: 1.268
-Original Grad: 0.049, -lr * Pred Grad: -0.038, New P: 0.058
iter 8 loss: 0.624
Actual params: [1.2683, 0.0581]
-Original Grad: -0.017, -lr * Pred Grad: 0.052, New P: 1.320
-Original Grad: 0.017, -lr * Pred Grad: -0.026, New P: 0.032
iter 9 loss: 0.629
Actual params: [1.3203, 0.0321]
-Original Grad: -0.015, -lr * Pred Grad: 0.010, New P: 1.331
-Original Grad: 0.011, -lr * Pred Grad: -0.016, New P: 0.016
iter 10 loss: 0.629
Actual params: [1.3306, 0.0163]
-Original Grad: -0.013, -lr * Pred Grad: 0.056, New P: 1.387
-Original Grad: 0.008, -lr * Pred Grad: -0.000, New P: 0.016
iter 11 loss: 0.635
Actual params: [1.3868, 0.0162]
-Original Grad: -0.017, -lr * Pred Grad: 0.061, New P: 1.448
-Original Grad: 0.027, -lr * Pred Grad: 0.004, New P: 0.020
iter 12 loss: 0.643
Actual params: [1.448 , 0.0199]
-Original Grad: -0.017, -lr * Pred Grad: 0.033, New P: 1.481
-Original Grad: 0.012, -lr * Pred Grad: -0.006, New P: 0.014
iter 13 loss: 0.646
Actual params: [1.4814, 0.0142]
-Original Grad: -0.022, -lr * Pred Grad: -0.000, New P: 1.481
-Original Grad: 0.009, -lr * Pred Grad: -0.010, New P: 0.005
iter 14 loss: 0.646
Actual params: [1.4814, 0.0047]
-Original Grad: -0.019, -lr * Pred Grad: -0.025, New P: 1.456
-Original Grad: -0.005, -lr * Pred Grad: -0.021, New P: -0.016
iter 15 loss: 0.642
Actual params: [ 1.4562, -0.0165]
-Original Grad: -0.018, -lr * Pred Grad: -0.025, New P: 1.432
-Original Grad: 0.004, -lr * Pred Grad: -0.023, New P: -0.040
iter 16 loss: 0.638
Actual params: [ 1.4316, -0.0395]
-Original Grad: -0.020, -lr * Pred Grad: -0.026, New P: 1.405
-Original Grad: 0.018, -lr * Pred Grad: -0.015, New P: -0.054
iter 17 loss: 0.634
Actual params: [ 1.4052, -0.0544]
-Original Grad: -0.014, -lr * Pred Grad: -0.024, New P: 1.381
-Original Grad: 0.021, -lr * Pred Grad: -0.005, New P: -0.059
iter 18 loss: 0.631
Actual params: [ 1.3811, -0.0594]
-Original Grad: -0.015, -lr * Pred Grad: -0.023, New P: 1.358
-Original Grad: 0.009, -lr * Pred Grad: -0.007, New P: -0.066
iter 19 loss: 0.628
Actual params: [ 1.3583, -0.0661]
-Original Grad: -0.014, -lr * Pred Grad: -0.022, New P: 1.336
-Original Grad: 0.014, -lr * Pred Grad: -0.011, New P: -0.077
iter 20 loss: 0.626
Actual params: [ 1.3361, -0.0774]
-Original Grad: -0.015, -lr * Pred Grad: -0.022, New P: 1.314
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: -0.095
Target params: [1.1812, 0.2779]
iter 0 loss: 0.369
Actual params: [0.5941, 0.5941]
-Original Grad: -0.036, -lr * Pred Grad: 0.000, New P: 0.594
-Original Grad: -0.087, -lr * Pred Grad: -0.042, New P: 0.552
iter 1 loss: 0.358
Actual params: [0.5942, 0.5519]
-Original Grad: -0.033, -lr * Pred Grad: -0.057, New P: 0.537
-Original Grad: -0.083, -lr * Pred Grad: -0.073, New P: 0.479
iter 2 loss: 0.325
Actual params: [0.5374, 0.4786]
-Original Grad: -0.037, -lr * Pred Grad: -0.068, New P: 0.470
-Original Grad: -0.076, -lr * Pred Grad: -0.075, New P: 0.403
iter 3 loss: 0.298
Actual params: [0.4699, 0.4034]
-Original Grad: -0.026, -lr * Pred Grad: -0.066, New P: 0.404
-Original Grad: -0.061, -lr * Pred Grad: -0.071, New P: 0.333
iter 4 loss: 0.276
Actual params: [0.4038, 0.3327]
-Original Grad: -0.048, -lr * Pred Grad: -0.062, New P: 0.342
-Original Grad: -0.050, -lr * Pred Grad: -0.068, New P: 0.265
iter 5 loss: 0.261
Actual params: [0.3418, 0.2646]
-Original Grad: -0.031, -lr * Pred Grad: -0.055, New P: 0.287
-Original Grad: -0.052, -lr * Pred Grad: -0.066, New P: 0.199
iter 6 loss: 0.248
Actual params: [0.2867, 0.1988]
-Original Grad: -0.022, -lr * Pred Grad: -0.044, New P: 0.243
-Original Grad: -0.040, -lr * Pred Grad: -0.062, New P: 0.136
iter 7 loss: 0.239
Actual params: [0.243 , 0.1363]
-Original Grad: -0.023, -lr * Pred Grad: -0.032, New P: 0.211
-Original Grad: -0.028, -lr * Pred Grad: -0.057, New P: 0.080
iter 8 loss: 0.235
Actual params: [0.2106, 0.0797]
-Original Grad: -0.016, -lr * Pred Grad: -0.028, New P: 0.183
-Original Grad: -0.014, -lr * Pred Grad: -0.047, New P: 0.032
iter 9 loss: 0.232
Actual params: [0.1827, 0.0324]
-Original Grad: -0.014, -lr * Pred Grad: -0.027, New P: 0.155
-Original Grad: -0.016, -lr * Pred Grad: -0.040, New P: -0.008
iter 10 loss: 0.230
Actual params: [ 0.1553, -0.0077]
-Original Grad: -0.011, -lr * Pred Grad: -0.027, New P: 0.128
-Original Grad: -0.011, -lr * Pred Grad: -0.032, New P: -0.040
iter 11 loss: 0.230
Actual params: [ 0.1283, -0.0396]
-Original Grad: -0.005, -lr * Pred Grad: -0.025, New P: 0.103
-Original Grad: -0.012, -lr * Pred Grad: -0.025, New P: -0.065
iter 12 loss: 0.229
Actual params: [ 0.1034, -0.0647]
-Original Grad: -0.003, -lr * Pred Grad: -0.022, New P: 0.081
-Original Grad: -0.010, -lr * Pred Grad: -0.025, New P: -0.089
iter 13 loss: 0.229
Actual params: [ 0.0811, -0.0893]
-Original Grad: -0.002, -lr * Pred Grad: -0.021, New P: 0.060
-Original Grad: -0.009, -lr * Pred Grad: -0.026, New P: -0.116
iter 14 loss: 0.229
Actual params: [ 0.0605, -0.1157]
-Original Grad: -0.002, -lr * Pred Grad: -0.020, New P: 0.040
-Original Grad: -0.008, -lr * Pred Grad: -0.027, New P: -0.143
iter 15 loss: 0.229
Actual params: [ 0.0405, -0.1427]
-Original Grad: 0.001, -lr * Pred Grad: -0.019, New P: 0.021
-Original Grad: -0.008, -lr * Pred Grad: -0.027, New P: -0.170
iter 16 loss: 0.229
Actual params: [ 0.0213, -0.1697]
-Original Grad: -0.001, -lr * Pred Grad: -0.019, New P: 0.002
-Original Grad: -0.006, -lr * Pred Grad: -0.027, New P: -0.196
iter 17 loss: 0.230
Actual params: [ 0.0024, -0.1962]
-Original Grad: 0.002, -lr * Pred Grad: -0.018, New P: -0.016
-Original Grad: -0.006, -lr * Pred Grad: -0.026, New P: -0.222
iter 18 loss: 0.230
Actual params: [-0.0161, -0.2223]
-Original Grad: 0.002, -lr * Pred Grad: -0.018, New P: -0.034
-Original Grad: -0.005, -lr * Pred Grad: -0.025, New P: -0.248
iter 19 loss: 0.230
Actual params: [-0.0342, -0.2478]
-Original Grad: 0.003, -lr * Pred Grad: -0.018, New P: -0.052
-Original Grad: -0.002, -lr * Pred Grad: -0.024, New P: -0.272
iter 20 loss: 0.231
Actual params: [-0.0518, -0.2719]
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: -0.069
-Original Grad: -0.004, -lr * Pred Grad: -0.024, New P: -0.296
Target params: [1.1812, 0.2779]
iter 0 loss: 0.416
Actual params: [0.5941, 0.5941]
-Original Grad: 0.270, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.001, -lr * Pred Grad: 0.034, New P: 0.628
iter 1 loss: 0.362
Actual params: [0.6616, 0.6281]
-Original Grad: -0.006, -lr * Pred Grad: 0.084, New P: 0.746
-Original Grad: -0.003, -lr * Pred Grad: -0.031, New P: 0.597
iter 2 loss: 0.327
Actual params: [0.7457, 0.5971]
-Original Grad: -0.093, -lr * Pred Grad: 0.069, New P: 0.814
-Original Grad: -0.023, -lr * Pred Grad: -0.059, New P: 0.538
iter 3 loss: 0.312
Actual params: [0.8145, 0.5381]
-Original Grad: -0.023, -lr * Pred Grad: -0.032, New P: 0.783
-Original Grad: -0.042, -lr * Pred Grad: -0.062, New P: 0.476
iter 4 loss: 0.317
Actual params: [0.7827, 0.4762]
-Original Grad: -0.037, -lr * Pred Grad: -0.025, New P: 0.757
-Original Grad: -0.023, -lr * Pred Grad: -0.054, New P: 0.422
iter 5 loss: 0.328
Actual params: [0.7572, 0.4217]
-Original Grad: -0.026, -lr * Pred Grad: -0.010, New P: 0.748
-Original Grad: -0.015, -lr * Pred Grad: -0.037, New P: 0.385
iter 6 loss: 0.329
Actual params: [0.7476, 0.3849]
-Original Grad: -0.005, -lr * Pred Grad: -0.017, New P: 0.730
-Original Grad: -0.004, -lr * Pred Grad: -0.018, New P: 0.367
iter 7 loss: 0.338
Actual params: [0.7303, 0.3669]
-Original Grad: -0.029, -lr * Pred Grad: -0.028, New P: 0.702
-Original Grad: -0.010, -lr * Pred Grad: -0.019, New P: 0.348
iter 8 loss: 0.356
Actual params: [0.702 , 0.3475]
-Original Grad: -0.011, -lr * Pred Grad: -0.026, New P: 0.676
-Original Grad: -0.003, -lr * Pred Grad: -0.020, New P: 0.328
iter 9 loss: 0.368
Actual params: [0.6759, 0.3278]
-Original Grad: 0.025, -lr * Pred Grad: -0.004, New P: 0.672
-Original Grad: 0.004, -lr * Pred Grad: -0.017, New P: 0.311
iter 10 loss: 0.373
Actual params: [0.6718, 0.3111]
-Original Grad: 0.051, -lr * Pred Grad: 0.024, New P: 0.696
-Original Grad: 0.011, -lr * Pred Grad: -0.011, New P: 0.300
iter 11 loss: 0.361
Actual params: [0.6961, 0.2999]
-Original Grad: 0.021, -lr * Pred Grad: 0.021, New P: 0.717
-Original Grad: 0.004, -lr * Pred Grad: -0.012, New P: 0.288
iter 12 loss: 0.354
Actual params: [0.7173, 0.2883]
-Original Grad: -0.017, -lr * Pred Grad: -0.012, New P: 0.705
-Original Grad: -0.007, -lr * Pred Grad: -0.019, New P: 0.269
iter 13 loss: 0.359
Actual params: [0.7048, 0.2694]
-Original Grad: 0.017, -lr * Pred Grad: -0.011, New P: 0.694
-Original Grad: 0.005, -lr * Pred Grad: -0.021, New P: 0.249
iter 14 loss: 0.367
Actual params: [0.6938, 0.2488]
-Original Grad: 0.033, -lr * Pred Grad: 0.008, New P: 0.702
-Original Grad: 0.005, -lr * Pred Grad: -0.018, New P: 0.230
iter 15 loss: 0.364
Actual params: [0.7019, 0.2305]
-Original Grad: 0.027, -lr * Pred Grad: 0.015, New P: 0.717
-Original Grad: 0.007, -lr * Pred Grad: -0.015, New P: 0.215
iter 16 loss: 0.358
Actual params: [0.717 , 0.2151]
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: 0.719
-Original Grad: 0.006, -lr * Pred Grad: -0.014, New P: 0.201
iter 17 loss: 0.358
Actual params: [0.7188, 0.2007]
-Original Grad: 0.016, -lr * Pred Grad: -0.002, New P: 0.717
-Original Grad: 0.006, -lr * Pred Grad: -0.015, New P: 0.186
iter 18 loss: 0.360
Actual params: [0.7171, 0.1858]
-Original Grad: 0.016, -lr * Pred Grad: 0.001, New P: 0.718
-Original Grad: 0.009, -lr * Pred Grad: -0.014, New P: 0.171
iter 19 loss: 0.361
Actual params: [0.7181, 0.1714]
-Original Grad: 0.008, -lr * Pred Grad: -0.001, New P: 0.717
-Original Grad: 0.010, -lr * Pred Grad: -0.013, New P: 0.158
iter 20 loss: 0.362
Actual params: [0.7171, 0.1582]
-Original Grad: 0.021, -lr * Pred Grad: 0.003, New P: 0.720
-Original Grad: 0.017, -lr * Pred Grad: -0.010, New P: 0.148
Target params: [1.1812, 0.2779]
iter 0 loss: 0.321
Actual params: [0.5941, 0.5941]
-Original Grad: 0.029, -lr * Pred Grad: 0.052, New P: 0.646
-Original Grad: -0.088, -lr * Pred Grad: -0.042, New P: 0.552
iter 1 loss: 0.327
Actual params: [0.6459, 0.5516]
-Original Grad: 0.010, -lr * Pred Grad: 0.005, New P: 0.651
-Original Grad: -0.037, -lr * Pred Grad: -0.069, New P: 0.483
iter 2 loss: 0.329
Actual params: [0.6507, 0.483 ]
-Original Grad: 0.007, -lr * Pred Grad: -0.038, New P: 0.613
-Original Grad: -0.008, -lr * Pred Grad: -0.069, New P: 0.414
iter 3 loss: 0.329
Actual params: [0.6132, 0.4144]
-Original Grad: -0.003, -lr * Pred Grad: -0.033, New P: 0.581
-Original Grad: 0.015, -lr * Pred Grad: -0.064, New P: 0.350
iter 4 loss: 0.329
Actual params: [0.5805, 0.3501]
-Original Grad: -0.012, -lr * Pred Grad: -0.012, New P: 0.568
-Original Grad: 0.022, -lr * Pred Grad: -0.046, New P: 0.305
iter 5 loss: 0.328
Actual params: [0.5681, 0.3046]
-Original Grad: -0.012, -lr * Pred Grad: -0.014, New P: 0.554
-Original Grad: 0.027, -lr * Pred Grad: -0.011, New P: 0.294
iter 6 loss: 0.328
Actual params: [0.5538, 0.2937]
-Original Grad: -0.009, -lr * Pred Grad: -0.017, New P: 0.537
-Original Grad: 0.027, -lr * Pred Grad: 0.025, New P: 0.319
iter 7 loss: 0.328
Actual params: [0.5371, 0.3188]
-Original Grad: -0.010, -lr * Pred Grad: -0.018, New P: 0.519
-Original Grad: 0.018, -lr * Pred Grad: 0.003, New P: 0.322
iter 8 loss: 0.327
Actual params: [0.5186, 0.3223]
-Original Grad: -0.020, -lr * Pred Grad: -0.023, New P: 0.496
-Original Grad: 0.023, -lr * Pred Grad: 0.011, New P: 0.333
iter 9 loss: 0.326
Actual params: [0.4955, 0.3331]
-Original Grad: -0.017, -lr * Pred Grad: -0.025, New P: 0.471
-Original Grad: 0.012, -lr * Pred Grad: -0.003, New P: 0.330
iter 10 loss: 0.325
Actual params: [0.4708, 0.3303]
-Original Grad: -0.013, -lr * Pred Grad: -0.023, New P: 0.448
-Original Grad: 0.007, -lr * Pred Grad: -0.006, New P: 0.324
iter 11 loss: 0.324
Actual params: [0.4478, 0.3245]
-Original Grad: -0.017, -lr * Pred Grad: -0.023, New P: 0.425
-Original Grad: 0.007, -lr * Pred Grad: -0.011, New P: 0.314
iter 12 loss: 0.323
Actual params: [0.4249, 0.3139]
-Original Grad: -0.020, -lr * Pred Grad: -0.025, New P: 0.400
-Original Grad: 0.006, -lr * Pred Grad: -0.011, New P: 0.303
iter 13 loss: 0.322
Actual params: [0.4002, 0.3025]
-Original Grad: -0.022, -lr * Pred Grad: -0.026, New P: 0.374
-Original Grad: 0.009, -lr * Pred Grad: -0.010, New P: 0.292
iter 14 loss: 0.320
Actual params: [0.3738, 0.2924]
-Original Grad: -0.021, -lr * Pred Grad: -0.027, New P: 0.347
-Original Grad: 0.002, -lr * Pred Grad: -0.012, New P: 0.280
iter 15 loss: 0.319
Actual params: [0.347, 0.28 ]
-Original Grad: -0.007, -lr * Pred Grad: -0.022, New P: 0.325
-Original Grad: -0.002, -lr * Pred Grad: -0.017, New P: 0.263
iter 16 loss: 0.318
Actual params: [0.3247, 0.2634]
-Original Grad: -0.007, -lr * Pred Grad: -0.018, New P: 0.307
-Original Grad: -0.001, -lr * Pred Grad: -0.018, New P: 0.246
iter 17 loss: 0.318
Actual params: [0.307 , 0.2455]
-Original Grad: -0.011, -lr * Pred Grad: -0.017, New P: 0.290
-Original Grad: 0.004, -lr * Pred Grad: -0.014, New P: 0.231
iter 18 loss: 0.318
Actual params: [0.2902, 0.2312]
-Original Grad: -0.009, -lr * Pred Grad: -0.017, New P: 0.273
-Original Grad: 0.004, -lr * Pred Grad: -0.010, New P: 0.221
iter 19 loss: 0.317
Actual params: [0.273 , 0.2209]
-Original Grad: -0.014, -lr * Pred Grad: -0.019, New P: 0.254
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 0.214
iter 20 loss: 0.317
Actual params: [0.2541, 0.2142]
-Original Grad: -0.010, -lr * Pred Grad: -0.019, New P: 0.235
-Original Grad: -0.000, -lr * Pred Grad: -0.007, New P: 0.207
Target params: [1.1812, 0.2779]
iter 0 loss: 0.473
Actual params: [0.5941, 0.5941]
-Original Grad: 0.228, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.333, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.428
Actual params: [0.6616, 0.5309]
-Original Grad: 0.242, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.319, -lr * Pred Grad: -0.080, New P: 0.450
iter 2 loss: 0.370
Actual params: [0.7464, 0.4504]
-Original Grad: 0.256, -lr * Pred Grad: 0.088, New P: 0.834
-Original Grad: -0.200, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.323
Actual params: [0.8341, 0.3668]
-Original Grad: 0.059, -lr * Pred Grad: 0.088, New P: 0.922
-Original Grad: -0.043, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.302
Actual params: [0.9222, 0.2828]
-Original Grad: 0.023, -lr * Pred Grad: 0.088, New P: 1.010
-Original Grad: 0.070, -lr * Pred Grad: -0.081, New P: 0.202
iter 5 loss: 0.295
Actual params: [1.0103, 0.2022]
-Original Grad: 0.102, -lr * Pred Grad: 0.088, New P: 1.098
-Original Grad: 0.079, -lr * Pred Grad: -0.072, New P: 0.130
iter 6 loss: 0.297
Actual params: [1.0979, 0.1304]
-Original Grad: 0.050, -lr * Pred Grad: 0.084, New P: 1.182
-Original Grad: 0.053, -lr * Pred Grad: -0.067, New P: 0.063
iter 7 loss: 0.293
Actual params: [1.1822, 0.0629]
-Original Grad: 0.036, -lr * Pred Grad: 0.031, New P: 1.213
-Original Grad: 0.062, -lr * Pred Grad: -0.051, New P: 0.012
iter 8 loss: 0.290
Actual params: [1.2129, 0.0123]
-Original Grad: 0.031, -lr * Pred Grad: 0.063, New P: 1.276
-Original Grad: 0.043, -lr * Pred Grad: -0.032, New P: -0.020
iter 9 loss: 0.289
Actual params: [ 1.2757, -0.0202]
-Original Grad: 0.026, -lr * Pred Grad: 0.032, New P: 1.308
-Original Grad: 0.042, -lr * Pred Grad: -0.020, New P: -0.040
iter 10 loss: 0.288
Actual params: [ 1.3081, -0.0398]
-Original Grad: 0.022, -lr * Pred Grad: 0.057, New P: 1.365
-Original Grad: 0.020, -lr * Pred Grad: 0.003, New P: -0.037
iter 11 loss: 0.288
Actual params: [ 1.3649, -0.037 ]
-Original Grad: 0.028, -lr * Pred Grad: 0.002, New P: 1.366
-Original Grad: 0.056, -lr * Pred Grad: 0.037, New P: 0.000
iter 12 loss: 0.290
Actual params: [1.3665e+00, 8.9500e-05]
-Original Grad: 0.025, -lr * Pred Grad: 0.038, New P: 1.404
-Original Grad: 0.059, -lr * Pred Grad: 0.041, New P: 0.041
iter 13 loss: 0.299
Actual params: [1.4041, 0.0412]
-Original Grad: 0.023, -lr * Pred Grad: -0.014, New P: 1.390
-Original Grad: 0.035, -lr * Pred Grad: 0.032, New P: 0.073
iter 14 loss: 0.303
Actual params: [1.3899, 0.0733]
-Original Grad: 0.020, -lr * Pred Grad: 0.020, New P: 1.410
-Original Grad: 0.032, -lr * Pred Grad: 0.021, New P: 0.094
iter 15 loss: 0.307
Actual params: [1.4097, 0.0944]
-Original Grad: 0.024, -lr * Pred Grad: -0.006, New P: 1.403
-Original Grad: 0.050, -lr * Pred Grad: 0.031, New P: 0.126
iter 16 loss: 0.311
Actual params: [1.4035, 0.1258]
-Original Grad: 0.015, -lr * Pred Grad: 0.010, New P: 1.413
-Original Grad: 0.026, -lr * Pred Grad: 0.014, New P: 0.139
iter 17 loss: 0.314
Actual params: [1.4135, 0.1394]
-Original Grad: 0.018, -lr * Pred Grad: -0.004, New P: 1.410
-Original Grad: 0.046, -lr * Pred Grad: 0.032, New P: 0.172
iter 18 loss: 0.314
Actual params: [1.4095, 0.1716]
-Original Grad: 0.013, -lr * Pred Grad: 0.002, New P: 1.412
-Original Grad: 0.019, -lr * Pred Grad: 0.005, New P: 0.177
iter 19 loss: 0.315
Actual params: [1.4117, 0.1765]
-Original Grad: 0.010, -lr * Pred Grad: -0.005, New P: 1.407
-Original Grad: 0.020, -lr * Pred Grad: 0.010, New P: 0.187
iter 20 loss: 0.315
Actual params: [1.4072, 0.1867]
-Original Grad: 0.012, -lr * Pred Grad: -0.002, New P: 1.405
-Original Grad: 0.020, -lr * Pred Grad: 0.001, New P: 0.188
Target params: [1.1812, 0.2779]
iter 0 loss: 0.324
Actual params: [0.5941, 0.5941]
-Original Grad: 0.165, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.180, -lr * Pred Grad: -0.061, New P: 0.533
iter 1 loss: 0.207
Actual params: [0.6612, 0.5328]
-Original Grad: 0.016, -lr * Pred Grad: 0.083, New P: 0.744
-Original Grad: -0.058, -lr * Pred Grad: -0.079, New P: 0.454
iter 2 loss: 0.203
Actual params: [0.744 , 0.4536]
-Original Grad: -0.031, -lr * Pred Grad: 0.024, New P: 0.768
-Original Grad: -0.018, -lr * Pred Grad: -0.077, New P: 0.377
iter 3 loss: 0.193
Actual params: [0.7676, 0.3771]
-Original Grad: -0.017, -lr * Pred Grad: -0.025, New P: 0.742
-Original Grad: -0.002, -lr * Pred Grad: -0.070, New P: 0.307
iter 4 loss: 0.184
Actual params: [0.7423, 0.3067]
-Original Grad: 0.001, -lr * Pred Grad: -0.004, New P: 0.738
-Original Grad: 0.015, -lr * Pred Grad: -0.066, New P: 0.240
iter 5 loss: 0.183
Actual params: [0.7381, 0.2405]
-Original Grad: -0.004, -lr * Pred Grad: 0.002, New P: 0.740
-Original Grad: 0.019, -lr * Pred Grad: -0.052, New P: 0.189
iter 6 loss: 0.182
Actual params: [0.7397, 0.1889]
-Original Grad: -0.004, -lr * Pred Grad: -0.014, New P: 0.726
-Original Grad: 0.013, -lr * Pred Grad: -0.032, New P: 0.157
iter 7 loss: 0.181
Actual params: [0.7255, 0.1565]
-Original Grad: -0.002, -lr * Pred Grad: -0.015, New P: 0.710
-Original Grad: 0.028, -lr * Pred Grad: -0.008, New P: 0.148
iter 8 loss: 0.182
Actual params: [0.7102, 0.1485]
-Original Grad: -0.010, -lr * Pred Grad: -0.021, New P: 0.690
-Original Grad: 0.050, -lr * Pred Grad: 0.028, New P: 0.176
iter 9 loss: 0.185
Actual params: [0.6897, 0.1762]
-Original Grad: -0.011, -lr * Pred Grad: -0.021, New P: 0.669
-Original Grad: 0.031, -lr * Pred Grad: 0.028, New P: 0.204
iter 10 loss: 0.186
Actual params: [0.669 , 0.2044]
-Original Grad: -0.031, -lr * Pred Grad: -0.028, New P: 0.641
-Original Grad: 0.049, -lr * Pred Grad: 0.030, New P: 0.234
iter 11 loss: 0.193
Actual params: [0.6412, 0.2344]
-Original Grad: 0.017, -lr * Pred Grad: -0.015, New P: 0.626
-Original Grad: 0.021, -lr * Pred Grad: 0.015, New P: 0.250
iter 12 loss: 0.199
Actual params: [0.626 , 0.2498]
-Original Grad: 0.009, -lr * Pred Grad: -0.003, New P: 0.623
-Original Grad: 0.019, -lr * Pred Grad: 0.006, New P: 0.256
iter 13 loss: 0.200
Actual params: [0.623 , 0.2561]
-Original Grad: 0.010, -lr * Pred Grad: -0.001, New P: 0.622
-Original Grad: 0.019, -lr * Pred Grad: 0.002, New P: 0.258
iter 14 loss: 0.201
Actual params: [0.6219, 0.2585]
-Original Grad: -0.005, -lr * Pred Grad: -0.010, New P: 0.611
-Original Grad: 0.016, -lr * Pred Grad: 0.002, New P: 0.260
iter 15 loss: 0.204
Actual params: [0.6114, 0.2602]
-Original Grad: 0.010, -lr * Pred Grad: -0.010, New P: 0.602
-Original Grad: 0.015, -lr * Pred Grad: 0.000, New P: 0.260
iter 16 loss: 0.208
Actual params: [0.6016, 0.2603]
-Original Grad: 0.029, -lr * Pred Grad: 0.005, New P: 0.606
-Original Grad: 0.012, -lr * Pred Grad: -0.002, New P: 0.258
iter 17 loss: 0.206
Actual params: [0.6063, 0.2583]
-Original Grad: 0.034, -lr * Pred Grad: 0.017, New P: 0.623
-Original Grad: 0.011, -lr * Pred Grad: -0.004, New P: 0.255
iter 18 loss: 0.200
Actual params: [0.6234, 0.2547]
-Original Grad: 0.010, -lr * Pred Grad: 0.007, New P: 0.630
-Original Grad: 0.017, -lr * Pred Grad: -0.000, New P: 0.255
iter 19 loss: 0.198
Actual params: [0.6299, 0.2546]
-Original Grad: 0.002, -lr * Pred Grad: -0.007, New P: 0.623
-Original Grad: 0.019, -lr * Pred Grad: 0.004, New P: 0.259
iter 20 loss: 0.201
Actual params: [0.6225, 0.2586]
-Original Grad: 0.008, -lr * Pred Grad: -0.008, New P: 0.614
-Original Grad: 0.016, -lr * Pred Grad: 0.004, New P: 0.262
Target params: [1.1812, 0.2779]
iter 0 loss: 0.724
Actual params: [0.5941, 0.5941]
-Original Grad: 0.195, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.390, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.654
Actual params: [0.6614, 0.531 ]
-Original Grad: 0.193, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.341, -lr * Pred Grad: -0.080, New P: 0.450
iter 2 loss: 0.593
Actual params: [0.746 , 0.4505]
-Original Grad: 0.070, -lr * Pred Grad: 0.087, New P: 0.833
-Original Grad: -0.341, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.548
Actual params: [0.8332, 0.3669]
-Original Grad: -0.008, -lr * Pred Grad: 0.085, New P: 0.918
-Original Grad: -0.110, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.510
Actual params: [0.9183, 0.2827]
-Original Grad: -0.020, -lr * Pred Grad: 0.002, New P: 0.920
-Original Grad: -0.070, -lr * Pred Grad: -0.084, New P: 0.198
iter 5 loss: 0.479
Actual params: [0.9202, 0.1984]
-Original Grad: -0.018, -lr * Pred Grad: 0.012, New P: 0.932
-Original Grad: -0.027, -lr * Pred Grad: -0.083, New P: 0.115
iter 6 loss: 0.458
Actual params: [0.9321, 0.1152]
-Original Grad: -0.017, -lr * Pred Grad: -0.011, New P: 0.921
-Original Grad: -0.018, -lr * Pred Grad: -0.077, New P: 0.038
iter 7 loss: 0.453
Actual params: [0.9207, 0.0383]
-Original Grad: -0.018, -lr * Pred Grad: -0.010, New P: 0.911
-Original Grad: -0.011, -lr * Pred Grad: -0.071, New P: -0.032
iter 8 loss: 0.453
Actual params: [ 0.9108, -0.0324]
-Original Grad: -0.015, -lr * Pred Grad: -0.028, New P: 0.883
-Original Grad: -0.014, -lr * Pred Grad: -0.068, New P: -0.101
iter 9 loss: 0.451
Actual params: [ 0.8833, -0.1006]
-Original Grad: -0.011, -lr * Pred Grad: -0.025, New P: 0.858
-Original Grad: 0.013, -lr * Pred Grad: -0.060, New P: -0.161
iter 10 loss: 0.452
Actual params: [ 0.858 , -0.1606]
-Original Grad: -0.010, -lr * Pred Grad: -0.022, New P: 0.836
-Original Grad: 0.017, -lr * Pred Grad: -0.043, New P: -0.203
iter 11 loss: 0.454
Actual params: [ 0.8359, -0.2033]
-Original Grad: -0.004, -lr * Pred Grad: -0.017, New P: 0.819
-Original Grad: 0.011, -lr * Pred Grad: -0.033, New P: -0.236
iter 12 loss: 0.458
Actual params: [ 0.8187, -0.2359]
-Original Grad: 0.000, -lr * Pred Grad: -0.013, New P: 0.806
-Original Grad: 0.013, -lr * Pred Grad: -0.029, New P: -0.265
iter 13 loss: 0.463
Actual params: [ 0.8058, -0.2652]
-Original Grad: 0.000, -lr * Pred Grad: -0.011, New P: 0.794
-Original Grad: 0.017, -lr * Pred Grad: -0.028, New P: -0.293
iter 14 loss: 0.468
Actual params: [ 0.7944, -0.293 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.011, New P: 0.783
-Original Grad: 0.019, -lr * Pred Grad: -0.025, New P: -0.318
iter 15 loss: 0.473
Actual params: [ 0.7831, -0.3184]
-Original Grad: -0.003, -lr * Pred Grad: -0.013, New P: 0.770
-Original Grad: 0.026, -lr * Pred Grad: -0.017, New P: -0.336
iter 16 loss: 0.476
Actual params: [ 0.77  , -0.3357]
-Original Grad: 0.013, -lr * Pred Grad: -0.007, New P: 0.763
-Original Grad: 0.027, -lr * Pred Grad: 0.005, New P: -0.331
iter 17 loss: 0.475
Actual params: [ 0.7628, -0.3308]
-Original Grad: 0.017, -lr * Pred Grad: 0.001, New P: 0.763
-Original Grad: 0.022, -lr * Pred Grad: 0.011, New P: -0.320
iter 18 loss: 0.472
Actual params: [ 0.7633, -0.3197]
-Original Grad: 0.017, -lr * Pred Grad: 0.004, New P: 0.767
-Original Grad: 0.022, -lr * Pred Grad: -0.005, New P: -0.324
iter 19 loss: 0.473
Actual params: [ 0.7668, -0.3244]
-Original Grad: 0.014, -lr * Pred Grad: 0.001, New P: 0.768
-Original Grad: 0.014, -lr * Pred Grad: -0.004, New P: -0.329
iter 20 loss: 0.474
Actual params: [ 0.7683, -0.3287]
-Original Grad: 0.010, -lr * Pred Grad: -0.002, New P: 0.766
-Original Grad: 0.019, -lr * Pred Grad: -0.007, New P: -0.336
Target params: [1.1812, 0.2779]
iter 0 loss: 0.320
Actual params: [0.5941, 0.5941]
-Original Grad: 0.105, -lr * Pred Grad: 0.065, New P: 0.659
-Original Grad: -0.089, -lr * Pred Grad: -0.043, New P: 0.551
iter 1 loss: 0.274
Actual params: [0.6595, 0.551 ]
-Original Grad: 0.017, -lr * Pred Grad: 0.077, New P: 0.737
-Original Grad: -0.044, -lr * Pred Grad: -0.070, New P: 0.481
iter 2 loss: 0.239
Actual params: [0.7368, 0.4813]
-Original Grad: 0.012, -lr * Pred Grad: -0.003, New P: 0.733
-Original Grad: -0.024, -lr * Pred Grad: -0.069, New P: 0.412
iter 3 loss: 0.237
Actual params: [0.7335, 0.4119]
-Original Grad: 0.004, -lr * Pred Grad: 0.001, New P: 0.734
-Original Grad: -0.017, -lr * Pred Grad: -0.067, New P: 0.345
iter 4 loss: 0.230
Actual params: [0.7341, 0.345 ]
-Original Grad: 0.018, -lr * Pred Grad: 0.004, New P: 0.738
-Original Grad: 0.005, -lr * Pred Grad: -0.059, New P: 0.286
iter 5 loss: 0.227
Actual params: [0.7384, 0.2864]
-Original Grad: 0.020, -lr * Pred Grad: 0.021, New P: 0.759
-Original Grad: 0.007, -lr * Pred Grad: -0.039, New P: 0.248
iter 6 loss: 0.225
Actual params: [0.7589, 0.2475]
-Original Grad: 0.009, -lr * Pred Grad: -0.006, New P: 0.753
-Original Grad: 0.004, -lr * Pred Grad: -0.014, New P: 0.233
iter 7 loss: 0.226
Actual params: [0.7532, 0.2333]
-Original Grad: 0.016, -lr * Pred Grad: 0.004, New P: 0.757
-Original Grad: 0.009, -lr * Pred Grad: 0.004, New P: 0.238
iter 8 loss: 0.225
Actual params: [0.7568, 0.2376]
-Original Grad: 0.020, -lr * Pred Grad: -0.002, New P: 0.755
-Original Grad: 0.008, -lr * Pred Grad: -0.010, New P: 0.227
iter 9 loss: 0.226
Actual params: [0.7551, 0.2275]
-Original Grad: 0.011, -lr * Pred Grad: -0.001, New P: 0.754
-Original Grad: 0.008, -lr * Pred Grad: -0.010, New P: 0.218
iter 10 loss: 0.226
Actual params: [0.7544, 0.2176]
-Original Grad: 0.007, -lr * Pred Grad: -0.006, New P: 0.748
-Original Grad: 0.004, -lr * Pred Grad: -0.015, New P: 0.203
iter 11 loss: 0.228
Actual params: [0.7483, 0.203 ]
-Original Grad: 0.017, -lr * Pred Grad: -0.001, New P: 0.747
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: 0.186
iter 12 loss: 0.228
Actual params: [0.7468, 0.1859]
-Original Grad: 0.015, -lr * Pred Grad: 0.000, New P: 0.747
-Original Grad: 0.005, -lr * Pred Grad: -0.018, New P: 0.168
iter 13 loss: 0.229
Actual params: [0.7473, 0.1684]
-Original Grad: 0.021, -lr * Pred Grad: 0.004, New P: 0.751
-Original Grad: 0.012, -lr * Pred Grad: -0.014, New P: 0.155
iter 14 loss: 0.230
Actual params: [0.7512, 0.1547]
-Original Grad: 0.005, -lr * Pred Grad: -0.003, New P: 0.749
-Original Grad: 0.005, -lr * Pred Grad: -0.014, New P: 0.141
iter 15 loss: 0.231
Actual params: [0.7486, 0.1411]
-Original Grad: 0.016, -lr * Pred Grad: -0.002, New P: 0.747
-Original Grad: 0.009, -lr * Pred Grad: -0.014, New P: 0.127
iter 16 loss: 0.232
Actual params: [0.747 , 0.1268]
-Original Grad: 0.023, -lr * Pred Grad: 0.005, New P: 0.752
-Original Grad: 0.020, -lr * Pred Grad: -0.009, New P: 0.118
iter 17 loss: 0.232
Actual params: [0.7522, 0.1176]
-Original Grad: 0.018, -lr * Pred Grad: 0.007, New P: 0.759
-Original Grad: 0.020, -lr * Pred Grad: -0.004, New P: 0.113
iter 18 loss: 0.232
Actual params: [0.7591, 0.1133]
-Original Grad: 0.019, -lr * Pred Grad: 0.006, New P: 0.765
-Original Grad: 0.019, -lr * Pred Grad: -0.004, New P: 0.109
iter 19 loss: 0.231
Actual params: [0.7654, 0.1093]
-Original Grad: 0.013, -lr * Pred Grad: 0.003, New P: 0.769
-Original Grad: 0.015, -lr * Pred Grad: -0.007, New P: 0.102
iter 20 loss: 0.231
Actual params: [0.7686, 0.1019]
-Original Grad: 0.021, -lr * Pred Grad: 0.004, New P: 0.773
-Original Grad: 0.025, -lr * Pred Grad: -0.006, New P: 0.096
Target params: [1.1812, 0.2779]
iter 0 loss: 0.280
Actual params: [0.5941, 0.5941]
-Original Grad: 0.197, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.027, -lr * Pred Grad: 0.009, New P: 0.603
iter 1 loss: 0.221
Actual params: [0.6614, 0.6034]
-Original Grad: 0.065, -lr * Pred Grad: 0.084, New P: 0.745
-Original Grad: -0.060, -lr * Pred Grad: -0.057, New P: 0.546
iter 2 loss: 0.202
Actual params: [0.7454, 0.5461]
-Original Grad: -0.015, -lr * Pred Grad: 0.082, New P: 0.828
-Original Grad: -0.122, -lr * Pred Grad: -0.075, New P: 0.471
iter 3 loss: 0.195
Actual params: [0.8277, 0.4708]
-Original Grad: -0.020, -lr * Pred Grad: -0.013, New P: 0.815
-Original Grad: -0.093, -lr * Pred Grad: -0.072, New P: 0.398
iter 4 loss: 0.196
Actual params: [0.8145, 0.3983]
-Original Grad: 0.038, -lr * Pred Grad: 0.035, New P: 0.849
-Original Grad: -0.030, -lr * Pred Grad: -0.069, New P: 0.330
iter 5 loss: 0.204
Actual params: [0.8492, 0.3296]
-Original Grad: 0.055, -lr * Pred Grad: 0.057, New P: 0.907
-Original Grad: -0.002, -lr * Pred Grad: -0.064, New P: 0.266
iter 6 loss: 0.216
Actual params: [0.9066, 0.2655]
-Original Grad: 0.020, -lr * Pred Grad: 0.019, New P: 0.926
-Original Grad: 0.008, -lr * Pred Grad: -0.050, New P: 0.215
iter 7 loss: 0.235
Actual params: [0.9257, 0.2155]
-Original Grad: 0.016, -lr * Pred Grad: 0.023, New P: 0.948
-Original Grad: 0.015, -lr * Pred Grad: -0.032, New P: 0.184
iter 8 loss: 0.246
Actual params: [0.9482, 0.1839]
-Original Grad: 0.010, -lr * Pred Grad: -0.019, New P: 0.929
-Original Grad: 0.017, -lr * Pred Grad: -0.008, New P: 0.176
iter 9 loss: 0.257
Actual params: [0.9288, 0.1758]
-Original Grad: 0.022, -lr * Pred Grad: 0.012, New P: 0.941
-Original Grad: 0.030, -lr * Pred Grad: 0.019, New P: 0.195
iter 10 loss: 0.242
Actual params: [0.9408, 0.1948]
-Original Grad: 0.017, -lr * Pred Grad: -0.004, New P: 0.937
-Original Grad: 0.019, -lr * Pred Grad: 0.005, New P: 0.200
iter 11 loss: 0.240
Actual params: [0.9371, 0.2003]
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: 0.938
-Original Grad: 0.018, -lr * Pred Grad: 0.002, New P: 0.202
iter 12 loss: 0.238
Actual params: [0.9382, 0.202 ]
-Original Grad: 0.016, -lr * Pred Grad: -0.003, New P: 0.935
-Original Grad: 0.022, -lr * Pred Grad: 0.001, New P: 0.203
iter 13 loss: 0.239
Actual params: [0.9347, 0.203 ]
-Original Grad: 0.010, -lr * Pred Grad: -0.003, New P: 0.932
-Original Grad: 0.025, -lr * Pred Grad: 0.004, New P: 0.207
iter 14 loss: 0.237
Actual params: [0.9318, 0.2072]
-Original Grad: 0.018, -lr * Pred Grad: 0.001, New P: 0.932
-Original Grad: 0.029, -lr * Pred Grad: 0.009, New P: 0.216
iter 15 loss: 0.232
Actual params: [0.9324, 0.2157]
-Original Grad: 0.033, -lr * Pred Grad: 0.012, New P: 0.945
-Original Grad: 0.025, -lr * Pred Grad: 0.008, New P: 0.224
iter 16 loss: 0.224
Actual params: [0.9445, 0.2241]
-Original Grad: 0.009, -lr * Pred Grad: 0.003, New P: 0.947
-Original Grad: 0.015, -lr * Pred Grad: 0.000, New P: 0.225
iter 17 loss: 0.223
Actual params: [0.9472, 0.2245]
-Original Grad: 0.015, -lr * Pred Grad: -0.001, New P: 0.946
-Original Grad: 0.020, -lr * Pred Grad: -0.002, New P: 0.223
iter 18 loss: 0.224
Actual params: [0.9463, 0.2229]
-Original Grad: 0.003, -lr * Pred Grad: -0.006, New P: 0.940
-Original Grad: 0.011, -lr * Pred Grad: -0.004, New P: 0.219
iter 19 loss: 0.228
Actual params: [0.9405, 0.2189]
-Original Grad: 0.010, -lr * Pred Grad: -0.004, New P: 0.936
-Original Grad: 0.023, -lr * Pred Grad: 0.001, New P: 0.220
iter 20 loss: 0.229
Actual params: [0.936 , 0.2197]
-Original Grad: 0.010, -lr * Pred Grad: -0.002, New P: 0.935
-Original Grad: 0.009, -lr * Pred Grad: -0.001, New P: 0.219
Target params: [1.1812, 0.2779]
iter 0 loss: 1.080
Actual params: [0.5941, 0.5941]
-Original Grad: 0.140, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.277, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 1.081
Actual params: [0.6607, 0.531 ]
-Original Grad: 0.263, -lr * Pred Grad: 0.084, New P: 0.745
-Original Grad: -0.210, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 1.035
Actual params: [0.7452, 0.4504]
-Original Grad: 0.235, -lr * Pred Grad: 0.088, New P: 0.833
-Original Grad: -0.444, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.825
Actual params: [0.8327, 0.3669]
-Original Grad: 0.176, -lr * Pred Grad: 0.088, New P: 0.921
-Original Grad: -0.209, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.501
Actual params: [0.9208, 0.2827]
-Original Grad: 0.068, -lr * Pred Grad: 0.088, New P: 1.009
-Original Grad: -0.076, -lr * Pred Grad: -0.084, New P: 0.198
iter 5 loss: 0.336
Actual params: [1.0088, 0.1984]
-Original Grad: 0.030, -lr * Pred Grad: 0.087, New P: 1.095
-Original Grad: -0.014, -lr * Pred Grad: -0.084, New P: 0.115
iter 6 loss: 0.277
Actual params: [1.0953, 0.1148]
-Original Grad: 0.004, -lr * Pred Grad: 0.018, New P: 1.114
-Original Grad: 0.010, -lr * Pred Grad: -0.077, New P: 0.037
iter 7 loss: 0.256
Actual params: [1.1135, 0.0374]
-Original Grad: 0.008, -lr * Pred Grad: 0.033, New P: 1.146
-Original Grad: 0.015, -lr * Pred Grad: -0.071, New P: -0.033
iter 8 loss: 0.244
Actual params: [ 1.1462, -0.0332]
-Original Grad: 0.020, -lr * Pred Grad: 0.024, New P: 1.171
-Original Grad: 0.020, -lr * Pred Grad: -0.068, New P: -0.101
iter 9 loss: 0.228
Actual params: [ 1.1705, -0.1008]
-Original Grad: 0.033, -lr * Pred Grad: 0.047, New P: 1.217
-Original Grad: 0.039, -lr * Pred Grad: -0.053, New P: -0.154
iter 10 loss: 0.220
Actual params: [ 1.2174, -0.1541]
-Original Grad: 0.018, -lr * Pred Grad: -0.000, New P: 1.217
-Original Grad: 0.035, -lr * Pred Grad: -0.036, New P: -0.190
iter 11 loss: 0.212
Actual params: [ 1.2171, -0.1903]
-Original Grad: 0.011, -lr * Pred Grad: 0.011, New P: 1.228
-Original Grad: 0.024, -lr * Pred Grad: -0.028, New P: -0.218
iter 12 loss: 0.209
Actual params: [ 1.2281, -0.218 ]
-Original Grad: 0.009, -lr * Pred Grad: -0.018, New P: 1.210
-Original Grad: 0.029, -lr * Pred Grad: -0.026, New P: -0.244
iter 13 loss: 0.207
Actual params: [ 1.2098, -0.2436]
-Original Grad: 0.025, -lr * Pred Grad: 0.007, New P: 1.217
-Original Grad: 0.032, -lr * Pred Grad: -0.022, New P: -0.266
iter 14 loss: 0.205
Actual params: [ 1.217 , -0.2657]
-Original Grad: 0.013, -lr * Pred Grad: -0.003, New P: 1.214
-Original Grad: 0.029, -lr * Pred Grad: -0.013, New P: -0.279
iter 15 loss: 0.203
Actual params: [ 1.2144, -0.2788]
-Original Grad: 0.024, -lr * Pred Grad: 0.007, New P: 1.221
-Original Grad: 0.037, -lr * Pred Grad: 0.015, New P: -0.264
iter 16 loss: 0.206
Actual params: [ 1.2213, -0.2639]
-Original Grad: 0.022, -lr * Pred Grad: 0.005, New P: 1.226
-Original Grad: 0.042, -lr * Pred Grad: 0.031, New P: -0.233
iter 17 loss: 0.211
Actual params: [ 1.2264, -0.2332]
-Original Grad: 0.015, -lr * Pred Grad: 0.003, New P: 1.229
-Original Grad: 0.041, -lr * Pred Grad: 0.017, New P: -0.216
iter 18 loss: 0.210
Actual params: [ 1.229, -0.216]
-Original Grad: 0.010, -lr * Pred Grad: -0.002, New P: 1.227
-Original Grad: 0.033, -lr * Pred Grad: 0.026, New P: -0.190
iter 19 loss: 0.214
Actual params: [ 1.2266, -0.1898]
-Original Grad: 0.025, -lr * Pred Grad: 0.005, New P: 1.231
-Original Grad: 0.034, -lr * Pred Grad: 0.013, New P: -0.177
iter 20 loss: 0.218
Actual params: [ 1.2314, -0.1771]
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: 1.232
-Original Grad: 0.037, -lr * Pred Grad: 0.023, New P: -0.154
