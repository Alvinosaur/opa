Target params: [6.067]
Actual params: [1.084 , 0.5507]
-Original Grad: 0.037, -lr * Pred Grad: 1.008, New P: 2.092
-Original Grad: 0.588, -lr * Pred Grad: 2.282, New P: 2.833
iter 0 loss: 0.621
Actual params: [2.0916, 2.8329]
-Original Grad: 0.002, -lr * Pred Grad: 0.315, New P: 2.407
-Original Grad: 0.102, -lr * Pred Grad: 0.438, New P: 3.271
iter 1 loss: 0.025
Actual params: [2.4066, 3.2712]
-Original Grad: 0.003, -lr * Pred Grad: 0.322, New P: 2.728
-Original Grad: -0.106, -lr * Pred Grad: -0.352, New P: 2.919
iter 2 loss: 0.026
Actual params: [2.7282, 2.9191]
-Original Grad: 0.001, -lr * Pred Grad: 0.155, New P: 2.883
-Original Grad: 0.039, -lr * Pred Grad: 0.027, New P: 2.946
iter 3 loss: 0.021
Actual params: [2.8829, 2.9462]
-Original Grad: 0.001, -lr * Pred Grad: 0.102, New P: 2.985
-Original Grad: 0.025, -lr * Pred Grad: -0.003, New P: 2.943
iter 4 loss: 0.021
Actual params: [2.9854, 2.9432]
-Original Grad: 0.001, -lr * Pred Grad: 0.062, New P: 3.047
-Original Grad: 0.023, -lr * Pred Grad: 0.034, New P: 2.978
iter 5 loss: 0.021
Actual params: [3.0471, 2.9776]
-Original Grad: 0.000, -lr * Pred Grad: 0.036, New P: 3.083
-Original Grad: 0.013, -lr * Pred Grad: 0.027, New P: 3.005
iter 6 loss: 0.020
Actual params: [3.0829, 3.0045]
-Original Grad: 0.000, -lr * Pred Grad: 0.018, New P: 3.101
-Original Grad: 0.007, -lr * Pred Grad: 0.016, New P: 3.021
iter 7 loss: 0.020
Actual params: [3.1007, 3.0206]
-Original Grad: 0.000, -lr * Pred Grad: 0.005, New P: 3.105
-Original Grad: 0.003, -lr * Pred Grad: 0.002, New P: 3.023
iter 8 loss: 0.020
Actual params: [3.1053, 3.0229]
-Original Grad: 0.000, -lr * Pred Grad: -0.005, New P: 3.100
-Original Grad: 0.003, -lr * Pred Grad: -0.005, New P: 3.018
iter 9 loss: 0.020
Actual params: [3.0999, 3.0178]
-Original Grad: 0.000, -lr * Pred Grad: -0.013, New P: 3.087
-Original Grad: 0.004, -lr * Pred Grad: -0.007, New P: 3.011
iter 10 loss: 0.020
Actual params: [3.0868, 3.0111]
-Original Grad: 0.000, -lr * Pred Grad: -0.019, New P: 3.068
-Original Grad: 0.005, -lr * Pred Grad: -0.005, New P: 3.007
iter 11 loss: 0.020
Actual params: [3.0676, 3.0065]
-Original Grad: 0.000, -lr * Pred Grad: -0.024, New P: 3.044
-Original Grad: 0.006, -lr * Pred Grad: -0.002, New P: 3.005
iter 12 loss: 0.020
Actual params: [3.0436, 3.0048]
-Original Grad: 0.000, -lr * Pred Grad: -0.028, New P: 3.016
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 3.005
iter 13 loss: 0.020
Actual params: [3.0156, 3.005 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 2.985
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: 3.006
iter 14 loss: 0.020
Actual params: [2.9845, 3.0061]
-Original Grad: 0.000, -lr * Pred Grad: -0.034, New P: 2.951
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: 3.007
iter 15 loss: 0.020
Actual params: [2.9509, 3.0071]
-Original Grad: 0.000, -lr * Pred Grad: -0.036, New P: 2.915
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: 3.008
iter 16 loss: 0.020
Actual params: [2.9153, 3.008 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.037, New P: 2.878
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: 3.009
iter 17 loss: 0.020
Actual params: [2.878 , 3.0087]
-Original Grad: 0.000, -lr * Pred Grad: -0.039, New P: 2.840
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: 3.010
iter 18 loss: 0.020
Actual params: [2.8395, 3.0095]
-Original Grad: 0.000, -lr * Pred Grad: -0.040, New P: 2.800
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 3.010
iter 19 loss: 0.020
Actual params: [2.8   , 3.0105]
-Original Grad: 0.000, -lr * Pred Grad: -0.040, New P: 2.760
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 3.012
iter 20 loss: 0.020
Actual params: [2.7597, 3.0115]
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: 2.719
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 3.013
iter 21 loss: 0.020
Actual params: [2.7188, 3.0126]
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: 2.677
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 3.014
iter 22 loss: 0.020
Actual params: [2.6773, 3.0136]
-Original Grad: 0.000, -lr * Pred Grad: -0.042, New P: 2.635
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 3.015
iter 23 loss: 0.020
Actual params: [2.6354, 3.0146]
-Original Grad: 0.000, -lr * Pred Grad: -0.042, New P: 2.593
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 3.015
iter 24 loss: 0.020
Actual params: [2.5932, 3.0154]
-Original Grad: 0.000, -lr * Pred Grad: -0.042, New P: 2.551
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 3.016
iter 25 loss: 0.020
Actual params: [2.5508, 3.0162]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: 2.508
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 3.017
iter 26 loss: 0.020
Actual params: [2.5081, 3.0169]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: 2.465
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 3.018
iter 27 loss: 0.020
Actual params: [2.4653, 3.0175]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: 2.422
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 3.018
iter 28 loss: 0.020
Actual params: [2.4223, 3.0181]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: 2.379
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 3.019
iter 29 loss: 0.020
Actual params: [2.3792, 3.0186]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: 2.336
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 3.019
iter 30 loss: 0.020
Actual params: [2.3359, 3.0191]
Target params: [6.067]
Actual params: [-1.0585,  1.4018]
-Original Grad: -0.024, -lr * Pred Grad: 0.390, New P: -0.668
-Original Grad: 0.360, -lr * Pred Grad: 1.923, New P: 3.325
iter 0 loss: 0.096
Actual params: [-0.6683,  3.3253]
-Original Grad: -0.002, -lr * Pred Grad: 0.303, New P: -0.365
-Original Grad: -0.633, -lr * Pred Grad: -0.968, New P: 2.358
iter 1 loss: 0.158
Actual params: [-0.3649,  2.3577]
-Original Grad: -0.000, -lr * Pred Grad: 0.176, New P: -0.189
-Original Grad: 0.005, -lr * Pred Grad: -0.090, New P: 2.268
iter 2 loss: 0.007
Actual params: [-0.1889,  2.2679]
-Original Grad: -0.001, -lr * Pred Grad: 0.098, New P: -0.091
-Original Grad: 0.062, -lr * Pred Grad: -0.199, New P: 2.069
iter 3 loss: 0.009
Actual params: [-0.0907,  2.0694]
-Original Grad: -0.005, -lr * Pred Grad: 0.034, New P: -0.056
-Original Grad: 0.166, -lr * Pred Grad: 0.347, New P: 2.416
iter 4 loss: 0.021
Actual params: [-0.0563,  2.4163]
-Original Grad: -0.000, -lr * Pred Grad: 0.020, New P: -0.036
-Original Grad: -0.032, -lr * Pred Grad: -0.043, New P: 2.373
iter 5 loss: 0.008
Actual params: [-0.0363,  2.3733]
-Original Grad: -0.001, -lr * Pred Grad: 0.003, New P: -0.033
-Original Grad: -0.002, -lr * Pred Grad: -0.012, New P: 2.361
iter 6 loss: 0.008
Actual params: [-0.033 ,  2.3609]
-Original Grad: -0.001, -lr * Pred Grad: -0.008, New P: -0.041
-Original Grad: 0.006, -lr * Pred Grad: -0.022, New P: 2.339
iter 7 loss: 0.008
Actual params: [-0.0411,  2.3388]
-Original Grad: -0.001, -lr * Pred Grad: -0.017, New P: -0.058
-Original Grad: 0.020, -lr * Pred Grad: 0.024, New P: 2.363
iter 8 loss: 0.008
Actual params: [-0.0579,  2.3631]
-Original Grad: -0.001, -lr * Pred Grad: -0.023, New P: -0.081
-Original Grad: 0.004, -lr * Pred Grad: -0.001, New P: 2.362
iter 9 loss: 0.008
Actual params: [-0.0808,  2.3622]
-Original Grad: -0.001, -lr * Pred Grad: -0.028, New P: -0.109
-Original Grad: 0.005, -lr * Pred Grad: -0.006, New P: 2.357
iter 10 loss: 0.008
Actual params: [-0.1086,  2.3565]
-Original Grad: -0.001, -lr * Pred Grad: -0.032, New P: -0.141
-Original Grad: 0.008, -lr * Pred Grad: -0.001, New P: 2.356
iter 11 loss: 0.008
Actual params: [-0.1405,  2.3556]
-Original Grad: -0.000, -lr * Pred Grad: -0.035, New P: -0.175
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: 2.357
iter 12 loss: 0.007
Actual params: [-0.1754,  2.3572]
-Original Grad: -0.000, -lr * Pred Grad: -0.037, New P: -0.213
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.357
iter 13 loss: 0.007
Actual params: [-0.2126,  2.357 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.039, New P: -0.252
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.356
iter 14 loss: 0.007
Actual params: [-0.2518,  2.3559]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -0.292
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.355
iter 15 loss: 0.007
Actual params: [-0.2924,  2.3554]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -0.334
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.355
iter 16 loss: 0.007
Actual params: [-0.3339,  2.3549]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -0.376
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.354
iter 17 loss: 0.007
Actual params: [-0.3762,  2.3544]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.419
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.354
iter 18 loss: 0.007
Actual params: [-0.4191,  2.3542]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.462
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.354
iter 19 loss: 0.007
Actual params: [-0.4623,  2.3537]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.506
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.353
iter 20 loss: 0.007
Actual params: [-0.5058,  2.3529]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.549
-Original Grad: 0.008, -lr * Pred Grad: -0.000, New P: 2.353
iter 21 loss: 0.007
Actual params: [-0.5495,  2.3528]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.593
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.352
iter 22 loss: 0.007
Actual params: [-0.5933,  2.3523]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.637
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.352
iter 23 loss: 0.007
Actual params: [-0.6373,  2.3518]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.681
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.351
iter 24 loss: 0.007
Actual params: [-0.6813,  2.3513]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.725
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.351
iter 25 loss: 0.007
Actual params: [-0.7254,  2.3509]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.770
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.351
iter 26 loss: 0.007
Actual params: [-0.7695,  2.3506]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.814
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.350
iter 27 loss: 0.007
Actual params: [-0.8137,  2.3501]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.858
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.350
iter 28 loss: 0.007
Actual params: [-0.8579,  2.3499]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.902
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.350
iter 29 loss: 0.007
Actual params: [-0.9021,  2.3496]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.946
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.349
iter 30 loss: 0.007
Actual params: [-0.9464,  2.3492]
Target params: [6.067]
Actual params: [1.5477, 0.5327]
-Original Grad: 0.017, -lr * Pred Grad: 0.815, New P: 2.363
-Original Grad: -0.704, -lr * Pred Grad: -0.545, New P: -0.012
iter 0 loss: 0.290
Actual params: [ 2.3632, -0.0122]
-Original Grad: 0.014, -lr * Pred Grad: 0.470, New P: 2.833
-Original Grad: -0.464, -lr * Pred Grad: -0.553, New P: -0.566
iter 1 loss: 0.113
Actual params: [ 2.8327, -0.5655]
-Original Grad: 0.002, -lr * Pred Grad: 0.288, New P: 3.121
-Original Grad: -0.172, -lr * Pred Grad: -0.584, New P: -1.149
iter 2 loss: 0.022
Actual params: [ 3.1211, -1.1491]
-Original Grad: 0.003, -lr * Pred Grad: 0.179, New P: 3.300
-Original Grad: 0.088, -lr * Pred Grad: -0.312, New P: -1.461
iter 3 loss: 0.012
Actual params: [ 3.2996, -1.4611]
-Original Grad: 0.019, -lr * Pred Grad: 0.199, New P: 3.499
-Original Grad: 0.209, -lr * Pred Grad: 0.238, New P: -1.224
iter 4 loss: 0.033
Actual params: [ 3.499 , -1.2236]
-Original Grad: 0.005, -lr * Pred Grad: 0.112, New P: 3.611
-Original Grad: 0.090, -lr * Pred Grad: 0.238, New P: -0.986
iter 5 loss: 0.014
Actual params: [ 3.6115, -0.9857]
-Original Grad: -0.003, -lr * Pred Grad: 0.057, New P: 3.668
-Original Grad: 0.010, -lr * Pred Grad: 0.046, New P: -0.940
iter 6 loss: 0.008
Actual params: [ 3.6681, -0.9399]
-Original Grad: -0.004, -lr * Pred Grad: 0.022, New P: 3.690
-Original Grad: -0.004, -lr * Pred Grad: -0.011, New P: -0.950
iter 7 loss: 0.008
Actual params: [ 3.69  , -0.9504]
-Original Grad: -0.004, -lr * Pred Grad: -0.002, New P: 3.688
-Original Grad: -0.001, -lr * Pred Grad: -0.029, New P: -0.980
iter 8 loss: 0.008
Actual params: [ 3.6883, -0.9798]
-Original Grad: -0.004, -lr * Pred Grad: -0.018, New P: 3.670
-Original Grad: 0.007, -lr * Pred Grad: -0.016, New P: -0.996
iter 9 loss: 0.008
Actual params: [ 3.67  , -0.9962]
-Original Grad: -0.003, -lr * Pred Grad: -0.030, New P: 3.640
-Original Grad: 0.011, -lr * Pred Grad: -0.001, New P: -0.997
iter 10 loss: 0.008
Actual params: [ 3.6401, -0.9968]
-Original Grad: -0.003, -lr * Pred Grad: -0.038, New P: 3.602
-Original Grad: 0.013, -lr * Pred Grad: 0.011, New P: -0.986
iter 11 loss: 0.008
Actual params: [ 3.6018, -0.9855]
-Original Grad: -0.003, -lr * Pred Grad: -0.045, New P: 3.557
-Original Grad: 0.010, -lr * Pred Grad: 0.009, New P: -0.976
iter 12 loss: 0.008
Actual params: [ 3.5566, -0.9762]
-Original Grad: -0.002, -lr * Pred Grad: -0.049, New P: 3.508
-Original Grad: 0.008, -lr * Pred Grad: 0.004, New P: -0.973
iter 13 loss: 0.008
Actual params: [ 3.5079, -0.9727]
-Original Grad: -0.002, -lr * Pred Grad: -0.051, New P: 3.457
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: -0.973
iter 14 loss: 0.008
Actual params: [ 3.4572, -0.9726]
-Original Grad: -0.002, -lr * Pred Grad: -0.053, New P: 3.404
-Original Grad: 0.009, -lr * Pred Grad: 0.004, New P: -0.969
iter 15 loss: 0.008
Actual params: [ 3.4044, -0.969 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.054, New P: 3.351
-Original Grad: 0.008, -lr * Pred Grad: 0.003, New P: -0.966
iter 16 loss: 0.008
Actual params: [ 3.3509, -0.9662]
-Original Grad: -0.001, -lr * Pred Grad: -0.054, New P: 3.297
-Original Grad: 0.009, -lr * Pred Grad: 0.004, New P: -0.962
iter 17 loss: 0.008
Actual params: [ 3.2968, -0.9624]
-Original Grad: -0.001, -lr * Pred Grad: -0.055, New P: 3.242
-Original Grad: 0.009, -lr * Pred Grad: 0.005, New P: -0.958
iter 18 loss: 0.007
Actual params: [ 3.2418, -0.9578]
-Original Grad: -0.001, -lr * Pred Grad: -0.055, New P: 3.187
-Original Grad: 0.009, -lr * Pred Grad: 0.005, New P: -0.952
iter 19 loss: 0.007
Actual params: [ 3.1866, -0.9524]
-Original Grad: -0.001, -lr * Pred Grad: -0.056, New P: 3.131
-Original Grad: 0.008, -lr * Pred Grad: 0.004, New P: -0.949
iter 20 loss: 0.007
Actual params: [ 3.1309, -0.9487]
-Original Grad: -0.001, -lr * Pred Grad: -0.055, New P: 3.076
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: -0.948
iter 21 loss: 0.007
Actual params: [ 3.0761, -0.9477]
-Original Grad: -0.001, -lr * Pred Grad: -0.054, New P: 3.022
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: -0.947
iter 22 loss: 0.007
Actual params: [ 3.0224, -0.9473]
-Original Grad: -0.001, -lr * Pred Grad: -0.053, New P: 2.969
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -0.946
iter 23 loss: 0.007
Actual params: [ 2.9695, -0.9456]
-Original Grad: -0.000, -lr * Pred Grad: -0.051, New P: 2.918
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -0.943
iter 24 loss: 0.007
Actual params: [ 2.918 , -0.9432]
-Original Grad: -0.000, -lr * Pred Grad: -0.050, New P: 2.868
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -0.941
iter 25 loss: 0.007
Actual params: [ 2.8683, -0.9414]
-Original Grad: 0.000, -lr * Pred Grad: -0.048, New P: 2.820
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -0.940
iter 26 loss: 0.007
Actual params: [ 2.8203, -0.9401]
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: 2.774
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -0.939
iter 27 loss: 0.007
Actual params: [ 2.7741, -0.939 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: 2.730
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -0.937
iter 28 loss: 0.007
Actual params: [ 2.7296, -0.9374]
-Original Grad: 0.001, -lr * Pred Grad: -0.043, New P: 2.687
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -0.936
iter 29 loss: 0.007
Actual params: [ 2.6866, -0.9356]
-Original Grad: 0.001, -lr * Pred Grad: -0.042, New P: 2.645
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -0.934
iter 30 loss: 0.007
Actual params: [ 2.645 , -0.9336]
Target params: [6.067]
Actual params: [0.0029, 0.9353]
-Original Grad: -0.009, -lr * Pred Grad: 0.547, New P: 0.550
-Original Grad: -0.417, -lr * Pred Grad: -0.509, New P: 0.426
iter 0 loss: 0.086
Actual params: [0.5497, 0.4264]
-Original Grad: -0.000, -lr * Pred Grad: 0.331, New P: 0.881
-Original Grad: -0.153, -lr * Pred Grad: -0.382, New P: 0.044
iter 1 loss: 0.011
Actual params: [0.8806, 0.0441]
-Original Grad: 0.001, -lr * Pred Grad: 0.213, New P: 1.094
-Original Grad: 0.090, -lr * Pred Grad: -0.093, New P: -0.049
iter 2 loss: 0.005
Actual params: [ 1.0936, -0.0486]
-Original Grad: 0.001, -lr * Pred Grad: 0.127, New P: 1.220
-Original Grad: 0.146, -lr * Pred Grad: 0.332, New P: 0.283
iter 3 loss: 0.010
Actual params: [1.2204, 0.2834]
-Original Grad: 0.000, -lr * Pred Grad: 0.074, New P: 1.295
-Original Grad: -0.062, -lr * Pred Grad: -0.154, New P: 0.129
iter 4 loss: 0.003
Actual params: [1.2947, 0.129 ]
-Original Grad: 0.000, -lr * Pred Grad: 0.044, New P: 1.338
-Original Grad: 0.034, -lr * Pred Grad: 0.031, New P: 0.160
iter 5 loss: 0.002
Actual params: [1.3382, 0.1602]
-Original Grad: 0.000, -lr * Pred Grad: 0.023, New P: 1.361
-Original Grad: 0.015, -lr * Pred Grad: 0.006, New P: 0.166
iter 6 loss: 0.002
Actual params: [1.3608, 0.1657]
-Original Grad: 0.000, -lr * Pred Grad: 0.008, New P: 1.369
-Original Grad: 0.011, -lr * Pred Grad: 0.015, New P: 0.180
iter 7 loss: 0.002
Actual params: [1.3689, 0.1805]
-Original Grad: 0.000, -lr * Pred Grad: -0.002, New P: 1.367
-Original Grad: 0.002, -lr * Pred Grad: -0.010, New P: 0.170
iter 8 loss: 0.001
Actual params: [1.3665, 0.17  ]
-Original Grad: 0.000, -lr * Pred Grad: -0.010, New P: 1.356
-Original Grad: 0.009, -lr * Pred Grad: -0.001, New P: 0.169
iter 9 loss: 0.002
Actual params: [1.3562, 0.1694]
-Original Grad: 0.000, -lr * Pred Grad: -0.016, New P: 1.340
-Original Grad: 0.009, -lr * Pred Grad: 0.003, New P: 0.172
iter 10 loss: 0.002
Actual params: [1.3397, 0.1721]
-Original Grad: 0.000, -lr * Pred Grad: -0.021, New P: 1.318
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.173
iter 11 loss: 0.002
Actual params: [1.3183, 0.1733]
-Original Grad: 0.000, -lr * Pred Grad: -0.025, New P: 1.293
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 0.172
iter 12 loss: 0.002
Actual params: [1.2928, 0.1724]
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 1.264
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: 0.172
iter 13 loss: 0.002
Actual params: [1.2642, 0.1724]
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 1.233
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.173
iter 14 loss: 0.002
Actual params: [1.233, 0.173]
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.200
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.173
iter 15 loss: 0.002
Actual params: [1.1998, 0.1733]
-Original Grad: 0.000, -lr * Pred Grad: -0.035, New P: 1.165
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.173
iter 16 loss: 0.002
Actual params: [1.1648, 0.1735]
-Original Grad: 0.000, -lr * Pred Grad: -0.036, New P: 1.129
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: 0.174
iter 17 loss: 0.002
Actual params: [1.1286, 0.1737]
-Original Grad: 0.000, -lr * Pred Grad: -0.037, New P: 1.091
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: 0.174
iter 18 loss: 0.002
Actual params: [1.0913, 0.1741]
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: 1.053
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.174
iter 19 loss: 0.002
Actual params: [1.0531, 0.1742]
-Original Grad: 0.000, -lr * Pred Grad: -0.039, New P: 1.014
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.175
iter 20 loss: 0.002
Actual params: [1.0143, 0.1747]
-Original Grad: 0.000, -lr * Pred Grad: -0.039, New P: 0.975
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.175
iter 21 loss: 0.002
Actual params: [0.9749, 0.1754]
-Original Grad: 0.000, -lr * Pred Grad: -0.040, New P: 0.935
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.176
iter 22 loss: 0.002
Actual params: [0.9353, 0.1757]
-Original Grad: 0.000, -lr * Pred Grad: -0.040, New P: 0.895
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.176
iter 23 loss: 0.002
Actual params: [0.8953, 0.1757]
-Original Grad: 0.000, -lr * Pred Grad: -0.040, New P: 0.855
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: 0.176
iter 24 loss: 0.002
Actual params: [0.8551, 0.1761]
-Original Grad: 0.000, -lr * Pred Grad: -0.040, New P: 0.815
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.176
iter 25 loss: 0.002
Actual params: [0.8147, 0.1764]
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: 0.774
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.177
iter 26 loss: 0.002
Actual params: [0.7741, 0.1767]
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: 0.734
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.177
iter 27 loss: 0.002
Actual params: [0.7336, 0.1771]
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: 0.693
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.177
iter 28 loss: 0.002
Actual params: [0.6929, 0.1772]
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: 0.652
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: 0.177
iter 29 loss: 0.002
Actual params: [0.6521, 0.1775]
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: 0.611
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.178
iter 30 loss: 0.002
Actual params: [0.6113, 0.1778]
Target params: [6.067]
Actual params: [-0.6756, -1.5044]
-Original Grad: -0.004, -lr * Pred Grad: 0.606, New P: -0.069
-Original Grad: 0.504, -lr * Pred Grad: 2.177, New P: 0.673
iter 0 loss: 0.137
Actual params: [-0.0692,  0.6727]
-Original Grad: -0.003, -lr * Pred Grad: 0.307, New P: 0.238
-Original Grad: -0.603, -lr * Pred Grad: -1.031, New P: -0.359
iter 1 loss: 0.198
Actual params: [ 0.2383, -0.3586]
-Original Grad: -0.002, -lr * Pred Grad: 0.195, New P: 0.433
-Original Grad: -0.069, -lr * Pred Grad: -0.109, New P: -0.467
iter 2 loss: 0.018
Actual params: [ 0.4331, -0.4673]
-Original Grad: -0.003, -lr * Pred Grad: 0.094, New P: 0.527
-Original Grad: -0.005, -lr * Pred Grad: -0.333, New P: -0.800
iter 3 loss: 0.017
Actual params: [ 0.5269, -0.8003]
-Original Grad: -0.007, -lr * Pred Grad: 0.028, New P: 0.555
-Original Grad: 0.174, -lr * Pred Grad: 0.249, New P: -0.552
iter 4 loss: 0.031
Actual params: [ 0.5549, -0.5517]
-Original Grad: -0.004, -lr * Pred Grad: 0.004, New P: 0.559
-Original Grad: 0.044, -lr * Pred Grad: 0.163, New P: -0.389
iter 5 loss: 0.018
Actual params: [ 0.5586, -0.3886]
-Original Grad: -0.003, -lr * Pred Grad: -0.013, New P: 0.545
-Original Grad: -0.047, -lr * Pred Grad: -0.087, New P: -0.476
iter 6 loss: 0.018
Actual params: [ 0.5451, -0.4758]
-Original Grad: -0.004, -lr * Pred Grad: -0.029, New P: 0.516
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: -0.517
iter 7 loss: 0.017
Actual params: [ 0.5159, -0.5172]
-Original Grad: -0.004, -lr * Pred Grad: -0.038, New P: 0.478
-Original Grad: 0.024, -lr * Pred Grad: 0.014, New P: -0.503
iter 8 loss: 0.017
Actual params: [ 0.4776, -0.5028]
-Original Grad: -0.003, -lr * Pred Grad: -0.046, New P: 0.432
-Original Grad: 0.016, -lr * Pred Grad: 0.027, New P: -0.475
iter 9 loss: 0.017
Actual params: [ 0.4321, -0.4754]
-Original Grad: -0.003, -lr * Pred Grad: -0.051, New P: 0.381
-Original Grad: -0.000, -lr * Pred Grad: -0.009, New P: -0.484
iter 10 loss: 0.017
Actual params: [ 0.3814, -0.4841]
-Original Grad: -0.004, -lr * Pred Grad: -0.056, New P: 0.325
-Original Grad: 0.003, -lr * Pred Grad: -0.014, New P: -0.498
iter 11 loss: 0.016
Actual params: [ 0.3249, -0.4978]
-Original Grad: -0.004, -lr * Pred Grad: -0.061, New P: 0.264
-Original Grad: 0.009, -lr * Pred Grad: -0.003, New P: -0.500
iter 12 loss: 0.016
Actual params: [ 0.2636, -0.5005]
-Original Grad: -0.003, -lr * Pred Grad: -0.063, New P: 0.201
-Original Grad: 0.010, -lr * Pred Grad: 0.006, New P: -0.495
iter 13 loss: 0.016
Actual params: [ 0.2008, -0.4947]
-Original Grad: -0.003, -lr * Pred Grad: -0.065, New P: 0.136
-Original Grad: 0.006, -lr * Pred Grad: -0.002, New P: -0.497
iter 14 loss: 0.016
Actual params: [ 0.1359, -0.4967]
-Original Grad: -0.003, -lr * Pred Grad: -0.066, New P: 0.069
-Original Grad: 0.005, -lr * Pred Grad: -0.007, New P: -0.504
iter 15 loss: 0.016
Actual params: [ 0.0695, -0.5035]
-Original Grad: -0.003, -lr * Pred Grad: -0.068, New P: 0.002
-Original Grad: 0.007, -lr * Pred Grad: -0.003, New P: -0.506
iter 16 loss: 0.015
Actual params: [ 0.002 , -0.5061]
-Original Grad: -0.003, -lr * Pred Grad: -0.069, New P: -0.067
-Original Grad: 0.007, -lr * Pred Grad: -0.003, New P: -0.509
iter 17 loss: 0.015
Actual params: [-0.067 , -0.5092]
-Original Grad: -0.004, -lr * Pred Grad: -0.072, New P: -0.139
-Original Grad: 0.006, -lr * Pred Grad: -0.005, New P: -0.514
iter 18 loss: 0.015
Actual params: [-0.1386, -0.5145]
-Original Grad: -0.003, -lr * Pred Grad: -0.072, New P: -0.211
-Original Grad: 0.008, -lr * Pred Grad: -0.001, New P: -0.515
iter 19 loss: 0.015
Actual params: [-0.2106, -0.5155]
-Original Grad: -0.004, -lr * Pred Grad: -0.074, New P: -0.285
-Original Grad: 0.006, -lr * Pred Grad: -0.004, New P: -0.520
iter 20 loss: 0.014
Actual params: [-0.2851, -0.52  ]
-Original Grad: -0.004, -lr * Pred Grad: -0.076, New P: -0.361
-Original Grad: 0.007, -lr * Pred Grad: -0.003, New P: -0.523
iter 21 loss: 0.014
Actual params: [-0.3608, -0.5229]
-Original Grad: -0.004, -lr * Pred Grad: -0.079, New P: -0.440
-Original Grad: 0.005, -lr * Pred Grad: -0.007, New P: -0.530
iter 22 loss: 0.014
Actual params: [-0.4396, -0.5301]
-Original Grad: -0.003, -lr * Pred Grad: -0.078, New P: -0.518
-Original Grad: 0.012, -lr * Pred Grad: 0.008, New P: -0.522
iter 23 loss: 0.014
Actual params: [-0.5175, -0.5225]
-Original Grad: -0.003, -lr * Pred Grad: -0.077, New P: -0.595
-Original Grad: 0.006, -lr * Pred Grad: -0.001, New P: -0.523
iter 24 loss: 0.013
Actual params: [-0.5949, -0.5231]
-Original Grad: -0.004, -lr * Pred Grad: -0.080, New P: -0.675
-Original Grad: 0.001, -lr * Pred Grad: -0.015, New P: -0.538
iter 25 loss: 0.013
Actual params: [-0.6754, -0.5383]
-Original Grad: -0.004, -lr * Pred Grad: -0.080, New P: -0.756
-Original Grad: 0.008, -lr * Pred Grad: -0.003, New P: -0.542
iter 26 loss: 0.013
Actual params: [-0.7559, -0.5417]
-Original Grad: -0.004, -lr * Pred Grad: -0.080, New P: -0.836
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: -0.540
iter 27 loss: 0.012
Actual params: [-0.8362, -0.5402]
-Original Grad: -0.003, -lr * Pred Grad: -0.079, New P: -0.916
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -0.541
iter 28 loss: 0.012
Actual params: [-0.9157, -0.541 ]
-Original Grad: -0.003, -lr * Pred Grad: -0.079, New P: -0.994
-Original Grad: 0.006, -lr * Pred Grad: -0.004, New P: -0.545
iter 29 loss: 0.012
Actual params: [-0.9943, -0.5449]
-Original Grad: -0.003, -lr * Pred Grad: -0.076, New P: -1.071
-Original Grad: 0.009, -lr * Pred Grad: 0.002, New P: -0.543
iter 30 loss: 0.012
Actual params: [-1.0707, -0.5431]
Target params: [6.067]
Actual params: [-0.6634, -0.2295]
-Original Grad: -0.017, -lr * Pred Grad: 0.464, New P: -0.199
-Original Grad: 0.607, -lr * Pred Grad: 2.303, New P: 2.074
iter 0 loss: 0.794
Actual params: [-0.1993,  2.0738]
-Original Grad: -0.006, -lr * Pred Grad: 0.273, New P: 0.074
-Original Grad: 0.299, -lr * Pred Grad: 1.549, New P: 3.623
iter 1 loss: 0.042
Actual params: [0.0736, 3.623 ]
-Original Grad: -0.010, -lr * Pred Grad: 0.097, New P: 0.170
-Original Grad: -0.532, -lr * Pred Grad: -0.987, New P: 2.636
iter 2 loss: 0.138
Actual params: [0.1705, 2.6359]
-Original Grad: 0.001, -lr * Pred Grad: 0.091, New P: 0.262
-Original Grad: -0.015, -lr * Pred Grad: -0.089, New P: 2.547
iter 3 loss: 0.007
Actual params: [0.2618, 2.5466]
-Original Grad: 0.001, -lr * Pred Grad: 0.048, New P: 0.310
-Original Grad: 0.025, -lr * Pred Grad: -0.238, New P: 2.309
iter 4 loss: 0.007
Actual params: [0.3097, 2.3085]
-Original Grad: -0.003, -lr * Pred Grad: 0.013, New P: 0.322
-Original Grad: 0.161, -lr * Pred Grad: 0.295, New P: 2.604
iter 5 loss: 0.018
Actual params: [0.3224, 2.6039]
-Original Grad: 0.001, -lr * Pred Grad: 0.007, New P: 0.330
-Original Grad: 0.001, -lr * Pred Grad: 0.062, New P: 2.666
iter 6 loss: 0.007
Actual params: [0.3299, 2.6655]
-Original Grad: 0.001, -lr * Pred Grad: 0.000, New P: 0.330
-Original Grad: -0.027, -lr * Pred Grad: -0.040, New P: 2.625
iter 7 loss: 0.007
Actual params: [0.3301, 2.6254]
-Original Grad: 0.001, -lr * Pred Grad: -0.006, New P: 0.324
-Original Grad: -0.009, -lr * Pred Grad: -0.050, New P: 2.575
iter 8 loss: 0.007
Actual params: [0.3242, 2.575 ]
-Original Grad: 0.001, -lr * Pred Grad: -0.012, New P: 0.313
-Original Grad: 0.014, -lr * Pred Grad: -0.006, New P: 2.569
iter 9 loss: 0.007
Actual params: [0.3125, 2.569 ]
-Original Grad: 0.001, -lr * Pred Grad: -0.016, New P: 0.296
-Original Grad: 0.016, -lr * Pred Grad: 0.021, New P: 2.590
iter 10 loss: 0.007
Actual params: [0.2963, 2.5896]
-Original Grad: 0.001, -lr * Pred Grad: -0.020, New P: 0.277
-Original Grad: 0.007, -lr * Pred Grad: 0.008, New P: 2.597
iter 11 loss: 0.007
Actual params: [0.2768, 2.5975]
-Original Grad: 0.001, -lr * Pred Grad: -0.022, New P: 0.255
-Original Grad: 0.003, -lr * Pred Grad: -0.006, New P: 2.591
iter 12 loss: 0.007
Actual params: [0.2547, 2.5914]
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 0.230
-Original Grad: 0.005, -lr * Pred Grad: -0.007, New P: 2.585
iter 13 loss: 0.007
Actual params: [0.2304, 2.5847]
-Original Grad: 0.001, -lr * Pred Grad: -0.026, New P: 0.204
-Original Grad: 0.008, -lr * Pred Grad: -0.001, New P: 2.583
iter 14 loss: 0.007
Actual params: [0.2043, 2.5834]
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.177
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 2.584
iter 15 loss: 0.007
Actual params: [0.1766, 2.5842]
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.148
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.584
iter 16 loss: 0.007
Actual params: [0.1478, 2.5838]
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 0.118
-Original Grad: 0.007, -lr * Pred Grad: -0.002, New P: 2.582
iter 17 loss: 0.007
Actual params: [0.1181, 2.5822]
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 0.088
-Original Grad: 0.007, -lr * Pred Grad: -0.002, New P: 2.580
iter 18 loss: 0.007
Actual params: [0.0878, 2.5805]
-Original Grad: 0.001, -lr * Pred Grad: -0.031, New P: 0.057
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.579
iter 19 loss: 0.007
Actual params: [0.0568, 2.5795]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: 0.025
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.578
iter 20 loss: 0.007
Actual params: [0.0252, 2.5785]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: -0.007
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.577
iter 21 loss: 0.007
Actual params: [-0.0066,  2.5774]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: -0.039
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.576
iter 22 loss: 0.007
Actual params: [-0.0386,  2.5761]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: -0.071
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.575
iter 23 loss: 0.007
Actual params: [-0.0708,  2.5748]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: -0.103
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.573
iter 24 loss: 0.007
Actual params: [-0.1031,  2.5733]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: -0.135
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.572
iter 25 loss: 0.007
Actual params: [-0.1355,  2.5724]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: -0.168
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.571
iter 26 loss: 0.007
Actual params: [-0.168 ,  2.5713]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: -0.200
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.570
iter 27 loss: 0.007
Actual params: [-0.2004,  2.5702]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: -0.233
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.569
iter 28 loss: 0.007
Actual params: [-0.2329,  2.569 ]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: -0.265
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.568
iter 29 loss: 0.007
Actual params: [-0.2653,  2.5678]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: -0.298
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 2.567
iter 30 loss: 0.007
Actual params: [-0.2976,  2.5666]
Target params: [6.067]
Actual params: [-0.8962,  0.1733]
-Original Grad: -0.002, -lr * Pred Grad: 0.626, New P: -0.270
-Original Grad: 0.718, -lr * Pred Grad: 2.404, New P: 2.577
iter 0 loss: 0.369
Actual params: [-0.2702,  2.5773]
-Original Grad: -0.003, -lr * Pred Grad: 0.301, New P: 0.031
-Original Grad: -0.286, -lr * Pred Grad: -0.882, New P: 1.695
iter 1 loss: 0.043
Actual params: [0.031 , 1.6954]
-Original Grad: 0.001, -lr * Pred Grad: 0.227, New P: 0.258
-Original Grad: 0.193, -lr * Pred Grad: 0.348, New P: 2.043
iter 2 loss: 0.023
Actual params: [0.2576, 2.043 ]
-Original Grad: 0.001, -lr * Pred Grad: 0.125, New P: 0.383
-Original Grad: 0.008, -lr * Pred Grad: 0.053, New P: 2.096
iter 3 loss: 0.005
Actual params: [0.3826, 2.0958]
-Original Grad: 0.001, -lr * Pred Grad: 0.079, New P: 0.461
-Original Grad: -0.023, -lr * Pred Grad: -0.031, New P: 2.064
iter 4 loss: 0.006
Actual params: [0.4612, 2.0643]
-Original Grad: 0.001, -lr * Pred Grad: 0.047, New P: 0.508
-Original Grad: -0.005, -lr * Pred Grad: -0.045, New P: 2.019
iter 5 loss: 0.005
Actual params: [0.5083, 2.0188]
-Original Grad: 0.001, -lr * Pred Grad: 0.027, New P: 0.536
-Original Grad: 0.020, -lr * Pred Grad: 0.011, New P: 2.029
iter 6 loss: 0.005
Actual params: [0.5356, 2.0294]
-Original Grad: 0.001, -lr * Pred Grad: 0.013, New P: 0.549
-Original Grad: 0.014, -lr * Pred Grad: 0.024, New P: 2.054
iter 7 loss: 0.005
Actual params: [0.549 , 2.0537]
-Original Grad: 0.001, -lr * Pred Grad: 0.003, New P: 0.552
-Original Grad: 0.000, -lr * Pred Grad: -0.003, New P: 2.051
iter 8 loss: 0.005
Actual params: [0.552 , 2.0505]
-Original Grad: 0.001, -lr * Pred Grad: -0.005, New P: 0.547
-Original Grad: 0.002, -lr * Pred Grad: -0.010, New P: 2.041
iter 9 loss: 0.005
Actual params: [0.5472, 2.041 ]
-Original Grad: 0.001, -lr * Pred Grad: -0.011, New P: 0.536
-Original Grad: 0.008, -lr * Pred Grad: -0.001, New P: 2.040
iter 10 loss: 0.005
Actual params: [0.5365, 2.0402]
-Original Grad: 0.001, -lr * Pred Grad: -0.016, New P: 0.521
-Original Grad: 0.008, -lr * Pred Grad: 0.003, New P: 2.044
iter 11 loss: 0.005
Actual params: [0.5209, 2.0437]
-Original Grad: 0.001, -lr * Pred Grad: -0.019, New P: 0.501
-Original Grad: 0.006, -lr * Pred Grad: 0.000, New P: 2.044
iter 12 loss: 0.005
Actual params: [0.5014, 2.0438]
-Original Grad: 0.001, -lr * Pred Grad: -0.023, New P: 0.479
-Original Grad: 0.006, -lr * Pred Grad: -0.001, New P: 2.042
iter 13 loss: 0.005
Actual params: [0.4788, 2.0425]
-Original Grad: 0.001, -lr * Pred Grad: -0.025, New P: 0.454
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.042
iter 14 loss: 0.005
Actual params: [0.4536, 2.0423]
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 0.426
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 2.043
iter 15 loss: 0.005
Actual params: [0.4263, 2.0427]
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.397
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 2.043
iter 16 loss: 0.005
Actual params: [0.3972, 2.043 ]
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 0.367
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 2.043
iter 17 loss: 0.005
Actual params: [0.3668, 2.043 ]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: 0.335
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.043
iter 18 loss: 0.005
Actual params: [0.3352, 2.0427]
-Original Grad: 0.001, -lr * Pred Grad: -0.033, New P: 0.303
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: 2.043
iter 19 loss: 0.005
Actual params: [0.3026, 2.0429]
-Original Grad: 0.001, -lr * Pred Grad: -0.033, New P: 0.269
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 2.043
iter 20 loss: 0.005
Actual params: [0.2693, 2.0433]
-Original Grad: 0.001, -lr * Pred Grad: -0.034, New P: 0.235
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 2.044
iter 21 loss: 0.005
Actual params: [0.2353, 2.0435]
-Original Grad: 0.001, -lr * Pred Grad: -0.034, New P: 0.201
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 2.044
iter 22 loss: 0.006
Actual params: [0.2009, 2.0437]
-Original Grad: 0.001, -lr * Pred Grad: -0.035, New P: 0.166
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 2.044
iter 23 loss: 0.006
Actual params: [0.1661, 2.044 ]
-Original Grad: 0.001, -lr * Pred Grad: -0.035, New P: 0.131
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 2.044
iter 24 loss: 0.006
Actual params: [0.1309, 2.0443]
-Original Grad: 0.001, -lr * Pred Grad: -0.036, New P: 0.095
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 2.044
iter 25 loss: 0.006
Actual params: [0.0953, 2.0441]
-Original Grad: 0.001, -lr * Pred Grad: -0.036, New P: 0.059
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: 2.044
iter 26 loss: 0.006
Actual params: [0.0594, 2.0444]
-Original Grad: 0.001, -lr * Pred Grad: -0.036, New P: 0.023
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 2.045
iter 27 loss: 0.006
Actual params: [0.0231, 2.0449]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: -0.014
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 2.045
iter 28 loss: 0.006
Actual params: [-0.0135,  2.045 ]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: -0.050
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 2.045
iter 29 loss: 0.006
Actual params: [-0.0504,  2.0452]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: -0.088
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 2.045
iter 30 loss: 0.006
Actual params: [-0.0876,  2.0454]
Target params: [6.067]
Actual params: [1.5544, 0.3381]
-Original Grad: -0.001, -lr * Pred Grad: 0.630, New P: 2.184
-Original Grad: -0.183, -lr * Pred Grad: -0.122, New P: 0.216
iter 0 loss: 0.014
Actual params: [2.1842, 0.2158]
-Original Grad: -0.002, -lr * Pred Grad: 0.312, New P: 2.496
-Original Grad: -0.102, -lr * Pred Grad: -0.222, New P: -0.007
iter 1 loss: 0.007
Actual params: [ 2.4958, -0.0065]
-Original Grad: -0.001, -lr * Pred Grad: 0.211, New P: 2.707
-Original Grad: 0.030, -lr * Pred Grad: -0.047, New P: -0.054
iter 2 loss: 0.004
Actual params: [ 2.7069, -0.0539]
-Original Grad: -0.000, -lr * Pred Grad: 0.118, New P: 2.825
-Original Grad: 0.054, -lr * Pred Grad: 0.065, New P: 0.012
iter 3 loss: 0.005
Actual params: [2.8252, 0.0116]
-Original Grad: -0.001, -lr * Pred Grad: 0.066, New P: 2.892
-Original Grad: 0.015, -lr * Pred Grad: 0.020, New P: 0.031
iter 4 loss: 0.004
Actual params: [2.8915, 0.0312]
-Original Grad: -0.001, -lr * Pred Grad: 0.034, New P: 2.926
-Original Grad: 0.004, -lr * Pred Grad: -0.009, New P: 0.022
iter 5 loss: 0.004
Actual params: [2.9255, 0.0221]
-Original Grad: -0.001, -lr * Pred Grad: 0.013, New P: 2.939
-Original Grad: 0.009, -lr * Pred Grad: -0.007, New P: 0.015
iter 6 loss: 0.004
Actual params: [2.9388, 0.0155]
-Original Grad: -0.001, -lr * Pred Grad: -0.001, New P: 2.938
-Original Grad: 0.012, -lr * Pred Grad: 0.005, New P: 0.020
iter 7 loss: 0.004
Actual params: [2.9377, 0.0204]
-Original Grad: -0.001, -lr * Pred Grad: -0.012, New P: 2.926
-Original Grad: 0.009, -lr * Pred Grad: 0.005, New P: 0.025
iter 8 loss: 0.004
Actual params: [2.9259, 0.0255]
-Original Grad: -0.001, -lr * Pred Grad: -0.020, New P: 2.906
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.026
iter 9 loss: 0.004
Actual params: [2.9059, 0.0256]
-Original Grad: -0.001, -lr * Pred Grad: -0.026, New P: 2.879
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 0.025
iter 10 loss: 0.004
Actual params: [2.8795, 0.0252]
-Original Grad: -0.001, -lr * Pred Grad: -0.032, New P: 2.848
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.026
iter 11 loss: 0.004
Actual params: [2.8478, 0.0259]
-Original Grad: -0.001, -lr * Pred Grad: -0.036, New P: 2.812
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.027
iter 12 loss: 0.004
Actual params: [2.812, 0.027]
-Original Grad: -0.001, -lr * Pred Grad: -0.039, New P: 2.773
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.028
iter 13 loss: 0.004
Actual params: [2.7726, 0.0279]
-Original Grad: -0.001, -lr * Pred Grad: -0.041, New P: 2.731
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: 0.027
iter 14 loss: 0.003
Actual params: [2.7311, 0.0275]
-Original Grad: -0.001, -lr * Pred Grad: -0.043, New P: 2.688
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.028
iter 15 loss: 0.003
Actual params: [2.6877, 0.0283]
-Original Grad: -0.001, -lr * Pred Grad: -0.045, New P: 2.643
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.030
iter 16 loss: 0.003
Actual params: [2.6428, 0.0296]
-Original Grad: -0.001, -lr * Pred Grad: -0.046, New P: 2.596
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.031
iter 17 loss: 0.003
Actual params: [2.5964, 0.0307]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: 2.549
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: 0.031
iter 18 loss: 0.003
Actual params: [2.5487, 0.0313]
-Original Grad: -0.001, -lr * Pred Grad: -0.049, New P: 2.500
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.032
iter 19 loss: 0.003
Actual params: [2.5001, 0.032 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.049, New P: 2.451
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.033
iter 20 loss: 0.003
Actual params: [2.4507, 0.0331]
-Original Grad: -0.002, -lr * Pred Grad: -0.052, New P: 2.399
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 0.032
iter 21 loss: 0.003
Actual params: [2.399 , 0.0322]
-Original Grad: -0.001, -lr * Pred Grad: -0.053, New P: 2.346
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.033
iter 22 loss: 0.003
Actual params: [2.3464, 0.0331]
-Original Grad: -0.002, -lr * Pred Grad: -0.054, New P: 2.292
-Original Grad: 0.009, -lr * Pred Grad: 0.005, New P: 0.038
iter 23 loss: 0.003
Actual params: [2.2924, 0.0382]
-Original Grad: -0.002, -lr * Pred Grad: -0.056, New P: 2.236
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.039
iter 24 loss: 0.003
Actual params: [2.2363, 0.0387]
-Original Grad: -0.002, -lr * Pred Grad: -0.057, New P: 2.179
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.040
iter 25 loss: 0.003
Actual params: [2.179 , 0.0396]
-Original Grad: -0.002, -lr * Pred Grad: -0.059, New P: 2.120
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.040
iter 26 loss: 0.003
Actual params: [2.1197, 0.0398]
-Original Grad: -0.002, -lr * Pred Grad: -0.061, New P: 2.059
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.041
iter 27 loss: 0.003
Actual params: [2.0588, 0.0406]
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: 1.997
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 0.041
iter 28 loss: 0.002
Actual params: [1.997 , 0.0409]
-Original Grad: -0.002, -lr * Pred Grad: -0.063, New P: 1.934
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 0.042
iter 29 loss: 0.002
Actual params: [1.9341, 0.0419]
-Original Grad: -0.002, -lr * Pred Grad: -0.064, New P: 1.870
-Original Grad: 0.007, -lr * Pred Grad: -0.002, New P: 0.040
iter 30 loss: 0.002
Actual params: [1.8703, 0.04  ]
Target params: [6.067]
Actual params: [-0.7899, -0.493 ]
-Original Grad: -0.001, -lr * Pred Grad: 0.631, New P: -0.159
-Original Grad: 0.197, -lr * Pred Grad: 1.473, New P: 0.980
iter 0 loss: 0.022
Actual params: [-0.1586,  0.9797]
-Original Grad: -0.010, -lr * Pred Grad: 0.227, New P: 0.069
-Original Grad: -0.652, -lr * Pred Grad: -0.846, New P: 0.133
iter 1 loss: 0.188
Actual params: [0.0686, 0.1335]
-Original Grad: -0.002, -lr * Pred Grad: 0.186, New P: 0.254
-Original Grad: -0.156, -lr * Pred Grad: -0.273, New P: -0.139
iter 2 loss: 0.014
Actual params: [ 0.2543, -0.1395]
-Original Grad: -0.002, -lr * Pred Grad: 0.087, New P: 0.341
-Original Grad: 0.011, -lr * Pred Grad: -0.393, New P: -0.533
iter 3 loss: 0.005
Actual params: [ 0.3408, -0.5325]
-Original Grad: -0.005, -lr * Pred Grad: 0.034, New P: 0.374
-Original Grad: 0.232, -lr * Pred Grad: 0.303, New P: -0.229
iter 4 loss: 0.029
Actual params: [ 0.3743, -0.2291]
-Original Grad: -0.003, -lr * Pred Grad: 0.009, New P: 0.383
-Original Grad: 0.065, -lr * Pred Grad: 0.219, New P: -0.010
iter 5 loss: 0.007
Actual params: [ 0.3831, -0.0102]
-Original Grad: -0.003, -lr * Pred Grad: -0.010, New P: 0.373
-Original Grad: -0.067, -lr * Pred Grad: -0.137, New P: -0.148
iter 6 loss: 0.007
Actual params: [ 0.3732, -0.1477]
-Original Grad: -0.003, -lr * Pred Grad: -0.022, New P: 0.351
-Original Grad: 0.016, -lr * Pred Grad: -0.020, New P: -0.168
iter 7 loss: 0.005
Actual params: [ 0.3509, -0.1679]
-Original Grad: -0.003, -lr * Pred Grad: -0.032, New P: 0.319
-Original Grad: 0.029, -lr * Pred Grad: 0.029, New P: -0.139
iter 8 loss: 0.005
Actual params: [ 0.3191, -0.1394]
-Original Grad: -0.003, -lr * Pred Grad: -0.039, New P: 0.280
-Original Grad: 0.011, -lr * Pred Grad: 0.023, New P: -0.117
iter 9 loss: 0.005
Actual params: [ 0.2801, -0.1168]
-Original Grad: -0.002, -lr * Pred Grad: -0.044, New P: 0.236
-Original Grad: -0.002, -lr * Pred Grad: -0.016, New P: -0.133
iter 10 loss: 0.005
Actual params: [ 0.2362, -0.1326]
-Original Grad: -0.002, -lr * Pred Grad: -0.048, New P: 0.188
-Original Grad: 0.007, -lr * Pred Grad: -0.006, New P: -0.139
iter 11 loss: 0.005
Actual params: [ 0.1883, -0.1389]
-Original Grad: -0.002, -lr * Pred Grad: -0.051, New P: 0.137
-Original Grad: 0.011, -lr * Pred Grad: 0.004, New P: -0.134
iter 12 loss: 0.005
Actual params: [ 0.1374, -0.1344]
-Original Grad: -0.002, -lr * Pred Grad: -0.053, New P: 0.085
-Original Grad: 0.008, -lr * Pred Grad: 0.003, New P: -0.131
iter 13 loss: 0.004
Actual params: [ 0.0845, -0.1313]
-Original Grad: -0.002, -lr * Pred Grad: -0.054, New P: 0.030
-Original Grad: 0.006, -lr * Pred Grad: -0.002, New P: -0.133
iter 14 loss: 0.004
Actual params: [ 0.0301, -0.1331]
-Original Grad: -0.002, -lr * Pred Grad: -0.055, New P: -0.025
-Original Grad: 0.008, -lr * Pred Grad: -0.001, New P: -0.134
iter 15 loss: 0.004
Actual params: [-0.0249, -0.1337]
-Original Grad: -0.001, -lr * Pred Grad: -0.055, New P: -0.080
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: -0.133
iter 16 loss: 0.004
Actual params: [-0.0803, -0.1332]
-Original Grad: -0.001, -lr * Pred Grad: -0.055, New P: -0.136
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: -0.133
iter 17 loss: 0.004
Actual params: [-0.1358, -0.133 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.055, New P: -0.191
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -0.133
iter 18 loss: 0.004
Actual params: [-0.1911, -0.1331]
-Original Grad: -0.001, -lr * Pred Grad: -0.055, New P: -0.246
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -0.133
iter 19 loss: 0.004
Actual params: [-0.2463, -0.1333]
-Original Grad: -0.001, -lr * Pred Grad: -0.055, New P: -0.301
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: -0.133
iter 20 loss: 0.004
Actual params: [-0.3009, -0.1332]
-Original Grad: -0.001, -lr * Pred Grad: -0.054, New P: -0.355
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -0.133
iter 21 loss: 0.004
Actual params: [-0.3553, -0.1333]
-Original Grad: -0.001, -lr * Pred Grad: -0.054, New P: -0.409
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -0.133
iter 22 loss: 0.004
Actual params: [-0.4093, -0.1334]
-Original Grad: -0.001, -lr * Pred Grad: -0.053, New P: -0.463
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: -0.133
iter 23 loss: 0.004
Actual params: [-0.4627, -0.1333]
-Original Grad: -0.001, -lr * Pred Grad: -0.053, New P: -0.516
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -0.133
iter 24 loss: 0.004
Actual params: [-0.5156, -0.1334]
-Original Grad: -0.001, -lr * Pred Grad: -0.052, New P: -0.568
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -0.134
iter 25 loss: 0.004
Actual params: [-0.568 , -0.1335]
-Original Grad: -0.001, -lr * Pred Grad: -0.052, New P: -0.620
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -0.134
iter 26 loss: 0.004
Actual params: [-0.62  , -0.1337]
-Original Grad: -0.001, -lr * Pred Grad: -0.051, New P: -0.671
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -0.134
iter 27 loss: 0.004
Actual params: [-0.6713, -0.1337]
-Original Grad: -0.001, -lr * Pred Grad: -0.051, New P: -0.722
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -0.134
iter 28 loss: 0.003
Actual params: [-0.7221, -0.1338]
-Original Grad: -0.001, -lr * Pred Grad: -0.050, New P: -0.772
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -0.134
iter 29 loss: 0.003
Actual params: [-0.7724, -0.1339]
-Original Grad: -0.001, -lr * Pred Grad: -0.050, New P: -0.822
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -0.134
iter 30 loss: 0.003
Actual params: [-0.8224, -0.134 ]
Target params: [6.067]
Actual params: [0.3685, 0.155 ]
-Original Grad: 0.000, -lr * Pred Grad: 0.648, New P: 1.017
-Original Grad: -0.816, -lr * Pred Grad: -0.535, New P: -0.380
iter 0 loss: 0.347
Actual params: [ 1.0165, -0.3799]
-Original Grad: 0.008, -lr * Pred Grad: 0.420, New P: 1.437
-Original Grad: -0.584, -lr * Pred Grad: -0.563, New P: -0.943
iter 1 loss: 0.154
Actual params: [ 1.4367, -0.9431]
-Original Grad: 0.004, -lr * Pred Grad: 0.273, New P: 1.709
-Original Grad: -0.262, -lr * Pred Grad: -0.616, New P: -1.559
iter 2 loss: 0.032
Actual params: [ 1.7092, -1.5592]
-Original Grad: 0.000, -lr * Pred Grad: 0.147, New P: 1.856
-Original Grad: 0.089, -lr * Pred Grad: -0.408, New P: -1.967
iter 3 loss: 0.004
Actual params: [ 1.856 , -1.9673]
-Original Grad: 0.001, -lr * Pred Grad: 0.097, New P: 1.953
-Original Grad: 0.311, -lr * Pred Grad: 0.371, New P: -1.596
iter 4 loss: 0.046
Actual params: [ 1.9533, -1.5961]
-Original Grad: -0.000, -lr * Pred Grad: 0.054, New P: 2.007
-Original Grad: 0.110, -lr * Pred Grad: 0.315, New P: -1.281
iter 5 loss: 0.006
Actual params: [ 2.0075, -1.2811]
-Original Grad: 0.000, -lr * Pred Grad: 0.031, New P: 2.039
-Original Grad: -0.069, -lr * Pred Grad: -0.166, New P: -1.447
iter 6 loss: 0.003
Actual params: [ 2.0387, -1.4469]
-Original Grad: -0.000, -lr * Pred Grad: 0.013, New P: 2.052
-Original Grad: 0.025, -lr * Pred Grad: -0.001, New P: -1.448
iter 7 loss: 0.001
Actual params: [ 2.0518, -1.448 ]
-Original Grad: -0.000, -lr * Pred Grad: 0.001, New P: 2.052
-Original Grad: 0.026, -lr * Pred Grad: 0.023, New P: -1.425
iter 8 loss: 0.001
Actual params: [ 2.0524, -1.4253]
-Original Grad: -0.000, -lr * Pred Grad: -0.009, New P: 2.044
-Original Grad: 0.013, -lr * Pred Grad: 0.025, New P: -1.400
iter 9 loss: 0.001
Actual params: [ 2.0438, -1.3999]
-Original Grad: -0.000, -lr * Pred Grad: -0.016, New P: 2.028
-Original Grad: -0.002, -lr * Pred Grad: -0.015, New P: -1.415
iter 10 loss: 0.001
Actual params: [ 2.0279, -1.415 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.022, New P: 2.006
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: -1.422
iter 11 loss: 0.001
Actual params: [ 2.0063, -1.4221]
-Original Grad: -0.000, -lr * Pred Grad: -0.026, New P: 1.980
-Original Grad: 0.011, -lr * Pred Grad: 0.003, New P: -1.419
iter 12 loss: 0.001
Actual params: [ 1.9799, -1.4187]
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 1.950
-Original Grad: 0.009, -lr * Pred Grad: 0.004, New P: -1.415
iter 13 loss: 0.001
Actual params: [ 1.9501, -1.4151]
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 1.917
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.416
iter 14 loss: 0.001
Actual params: [ 1.9174, -1.4163]
-Original Grad: -0.000, -lr * Pred Grad: -0.035, New P: 1.882
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.418
iter 15 loss: 0.001
Actual params: [ 1.8824, -1.4177]
-Original Grad: 0.000, -lr * Pred Grad: -0.036, New P: 1.846
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: -1.417
iter 16 loss: 0.001
Actual params: [ 1.8464, -1.4173]
-Original Grad: 0.000, -lr * Pred Grad: -0.037, New P: 1.809
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: -1.417
iter 17 loss: 0.001
Actual params: [ 1.8094, -1.4171]
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: 1.772
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -1.417
iter 18 loss: 0.001
Actual params: [ 1.7716, -1.4175]
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: 1.733
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -1.418
iter 19 loss: 0.001
Actual params: [ 1.7334, -1.4179]
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: 1.695
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -1.418
iter 20 loss: 0.001
Actual params: [ 1.695 , -1.4183]
-Original Grad: 0.001, -lr * Pred Grad: -0.038, New P: 1.657
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -1.418
iter 21 loss: 0.001
Actual params: [ 1.6568, -1.4184]
-Original Grad: 0.001, -lr * Pred Grad: -0.038, New P: 1.619
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.419
iter 22 loss: 0.001
Actual params: [ 1.6187, -1.4189]
-Original Grad: 0.001, -lr * Pred Grad: -0.038, New P: 1.581
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.419
iter 23 loss: 0.001
Actual params: [ 1.5809, -1.4194]
-Original Grad: 0.001, -lr * Pred Grad: -0.038, New P: 1.543
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.420
iter 24 loss: 0.001
Actual params: [ 1.5433, -1.4201]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: 1.506
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.421
iter 25 loss: 0.001
Actual params: [ 1.506 , -1.4207]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: 1.469
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.422
iter 26 loss: 0.001
Actual params: [ 1.469 , -1.4221]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: 1.432
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.423
iter 27 loss: 0.001
Actual params: [ 1.432 , -1.4227]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: 1.395
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: -1.423
iter 28 loss: 0.001
Actual params: [ 1.3951, -1.4225]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: 1.358
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -1.423
iter 29 loss: 0.001
Actual params: [ 1.358 , -1.4229]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: 1.321
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.424
iter 30 loss: 0.001
Actual params: [ 1.3209, -1.4238]
