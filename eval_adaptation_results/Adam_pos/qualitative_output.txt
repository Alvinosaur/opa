Target params: [1.1812, 0.2779]
Actual params: [0.4128, 0.9469]
-Original Grad: 0.087, -lr * Pred Grad:  0.100, New P: 0.513
-Original Grad: -0.142, -lr * Pred Grad:  -0.100, New P: 0.847
iter 0 loss: 0.419
Actual params: [0.5128, 0.8469]
-Original Grad: 0.166, -lr * Pred Grad:  0.097, New P: 0.610
-Original Grad: -0.197, -lr * Pred Grad:  -0.100, New P: 0.747
iter 1 loss: 0.392
Actual params: [0.6098, 0.7474]
-Original Grad: 0.365, -lr * Pred Grad:  0.091, New P: 0.701
-Original Grad: -0.254, -lr * Pred Grad:  -0.099, New P: 0.648
iter 2 loss: 0.345
Actual params: [0.7008, 0.6481]
-Original Grad: -4.796, -lr * Pred Grad:  -0.052, New P: 0.649
-Original Grad: -0.993, -lr * Pred Grad:  -0.082, New P: 0.566
iter 3 loss: 0.283
Actual params: [0.6493, 0.5661]
-Original Grad: 0.883, -lr * Pred Grad:  -0.033, New P: 0.616
-Original Grad: 0.575, -lr * Pred Grad:  -0.035, New P: 0.532
iter 4 loss: 0.329
Actual params: [0.6163, 0.5315]
-Original Grad: 0.362, -lr * Pred Grad:  -0.025, New P: 0.592
-Original Grad: 0.014, -lr * Pred Grad:  -0.029, New P: 0.502
iter 5 loss: 0.343
Actual params: [0.5917, 0.5023]
-Original Grad: 1.390, -lr * Pred Grad:  -0.007, New P: 0.585
-Original Grad: 0.951, -lr * Pred Grad:  0.011, New P: 0.514
iter 6 loss: 0.360
Actual params: [0.585 , 0.5138]
-Original Grad: 0.419, -lr * Pred Grad:  -0.002, New P: 0.583
-Original Grad: 0.124, -lr * Pred Grad:  0.014, New P: 0.528
iter 7 loss: 0.357
Actual params: [0.5831, 0.5279]
-Original Grad: 0.329, -lr * Pred Grad:  0.001, New P: 0.585
-Original Grad: 0.039, -lr * Pred Grad:  0.014, New P: 0.542
iter 8 loss: 0.357
Actual params: [0.5845, 0.5416]
-Original Grad: 0.352, -lr * Pred Grad:  0.005, New P: 0.589
-Original Grad: 0.079, -lr * Pred Grad:  0.015, New P: 0.556
iter 9 loss: 0.355
Actual params: [0.5892, 0.5564]
-Original Grad: 0.337, -lr * Pred Grad:  0.007, New P: 0.596
-Original Grad: 0.007, -lr * Pred Grad:  0.013, New P: 0.570
iter 10 loss: 0.353
Actual params: [0.5965, 0.5698]
-Original Grad: 0.331, -lr * Pred Grad:  0.010, New P: 0.606
-Original Grad: -0.053, -lr * Pred Grad:  0.010, New P: 0.580
iter 11 loss: 0.350
Actual params: [0.6061, 0.5802]
-Original Grad: 0.384, -lr * Pred Grad:  0.012, New P: 0.618
-Original Grad: 0.007, -lr * Pred Grad:  0.010, New P: 0.590
iter 12 loss: 0.347
Actual params: [0.6184, 0.5898]
-Original Grad: 0.395, -lr * Pred Grad:  0.015, New P: 0.633
-Original Grad: -0.001, -lr * Pred Grad:  0.009, New P: 0.598
iter 13 loss: 0.341
Actual params: [0.6332, 0.5984]
-Original Grad: -0.219, -lr * Pred Grad:  0.011, New P: 0.644
-Original Grad: -0.859, -lr * Pred Grad:  -0.017, New P: 0.581
iter 14 loss: 0.331
Actual params: [0.6444, 0.5814]
-Original Grad: 0.621, -lr * Pred Grad:  0.016, New P: 0.660
-Original Grad: 0.254, -lr * Pred Grad:  -0.008, New P: 0.573
iter 15 loss: 0.329
Actual params: [0.6604, 0.5732]
-Original Grad: -1.208, -lr * Pred Grad:  0.003, New P: 0.663
-Original Grad: -1.430, -lr * Pred Grad:  -0.037, New P: 0.536
iter 16 loss: 0.321
Actual params: [0.6634, 0.5365]
-Original Grad: 0.330, -lr * Pred Grad:  0.006, New P: 0.669
-Original Grad: 0.043, -lr * Pred Grad:  -0.032, New P: 0.504
iter 17 loss: 0.325
Actual params: [0.6692, 0.5041]
-Original Grad: 0.443, -lr * Pred Grad:  0.009, New P: 0.679
-Original Grad: 0.090, -lr * Pred Grad:  -0.027, New P: 0.477
iter 18 loss: 0.325
Actual params: [0.6785, 0.4766]
-Original Grad: 0.352, -lr * Pred Grad:  0.012, New P: 0.690
-Original Grad: 0.107, -lr * Pred Grad:  -0.023, New P: 0.454
iter 19 loss: 0.323
Actual params: [0.6903, 0.454 ]
-Original Grad: 0.437, -lr * Pred Grad:  0.015, New P: 0.705
-Original Grad: 0.196, -lr * Pred Grad:  -0.016, New P: 0.438
iter 20 loss: 0.321
Actual params: [0.7052, 0.438 ]
Target params: [1.1812, 0.2779]
Actual params: [0.8379, 0.635 ]
-Original Grad: -0.102, -lr * Pred Grad:  -0.100, New P: 0.738
-Original Grad: -0.004, -lr * Pred Grad:  -0.100, New P: 0.535
iter 0 loss: 0.121
Actual params: [0.7379, 0.535 ]
-Original Grad: 0.041, -lr * Pred Grad:  -0.034, New P: 0.704
-Original Grad: -0.046, -lr * Pred Grad:  -0.080, New P: 0.455
iter 1 loss: 0.122
Actual params: [0.7035, 0.455 ]
-Original Grad: -0.135, -lr * Pred Grad:  -0.066, New P: 0.637
-Original Grad: 0.082, -lr * Pred Grad:  0.025, New P: 0.480
iter 2 loss: 0.116
Actual params: [0.6373, 0.4803]
-Original Grad: 0.066, -lr * Pred Grad:  -0.030, New P: 0.607
-Original Grad: -0.110, -lr * Pred Grad:  -0.031, New P: 0.450
iter 3 loss: 0.125
Actual params: [0.6073, 0.4496]
-Original Grad: 0.204, -lr * Pred Grad:  0.023, New P: 0.631
-Original Grad: -0.217, -lr * Pred Grad:  -0.060, New P: 0.390
iter 4 loss: 0.124
Actual params: [0.6306, 0.3898]
-Original Grad: -0.000, -lr * Pred Grad:  0.020, New P: 0.651
-Original Grad: -0.038, -lr * Pred Grad:  -0.058, New P: 0.331
iter 5 loss: 0.117
Actual params: [0.6506, 0.3313]
-Original Grad: -0.209, -lr * Pred Grad:  -0.017, New P: 0.634
-Original Grad: 0.052, -lr * Pred Grad:  -0.040, New P: 0.291
iter 6 loss: 0.115
Actual params: [0.6339, 0.291 ]
-Original Grad: -0.196, -lr * Pred Grad:  -0.037, New P: 0.597
-Original Grad: 0.047, -lr * Pred Grad:  -0.026, New P: 0.265
iter 7 loss: 0.114
Actual params: [0.5967, 0.2645]
-Original Grad: 0.188, -lr * Pred Grad:  -0.009, New P: 0.588
-Original Grad: -0.050, -lr * Pred Grad:  -0.032, New P: 0.233
iter 8 loss: 0.110
Actual params: [0.5877, 0.2326]
-Original Grad: 0.239, -lr * Pred Grad:  0.016, New P: 0.604
-Original Grad: -0.053, -lr * Pred Grad:  -0.037, New P: 0.196
iter 9 loss: 0.111
Actual params: [0.6038, 0.1956]
-Original Grad: -0.122, -lr * Pred Grad:  0.003, New P: 0.606
-Original Grad: 0.011, -lr * Pred Grad:  -0.031, New P: 0.164
iter 10 loss: 0.112
Actual params: [0.6064, 0.1644]
-Original Grad: -0.144, -lr * Pred Grad:  -0.011, New P: 0.596
-Original Grad: -0.002, -lr * Pred Grad:  -0.028, New P: 0.136
iter 11 loss: 0.112
Actual params: [0.5957, 0.1361]
-Original Grad: -0.174, -lr * Pred Grad:  -0.024, New P: 0.572
-Original Grad: -0.003, -lr * Pred Grad:  -0.026, New P: 0.110
iter 12 loss: 0.110
Actual params: [0.5716, 0.1101]
-Original Grad: -0.048, -lr * Pred Grad:  -0.026, New P: 0.546
-Original Grad: 0.010, -lr * Pred Grad:  -0.022, New P: 0.088
iter 13 loss: 0.113
Actual params: [0.5457, 0.0884]
-Original Grad: -0.032, -lr * Pred Grad:  -0.026, New P: 0.520
-Original Grad: 0.015, -lr * Pred Grad:  -0.017, New P: 0.071
iter 14 loss: 0.112
Actual params: [0.5197, 0.0713]
-Original Grad: -0.013, -lr * Pred Grad:  -0.025, New P: 0.495
-Original Grad: 0.014, -lr * Pred Grad:  -0.013, New P: 0.058
iter 15 loss: 0.113
Actual params: [0.4949, 0.0582]
-Original Grad: 0.083, -lr * Pred Grad:  -0.015, New P: 0.480
-Original Grad: 0.028, -lr * Pred Grad:  -0.007, New P: 0.051
iter 16 loss: 0.113
Actual params: [0.4798, 0.0513]
-Original Grad: 0.215, -lr * Pred Grad:  0.005, New P: 0.485
-Original Grad: 0.036, -lr * Pred Grad:  0.000, New P: 0.051
iter 17 loss: 0.115
Actual params: [0.4845, 0.0515]
-Original Grad: 0.242, -lr * Pred Grad:  0.023, New P: 0.507
-Original Grad: 0.039, -lr * Pred Grad:  0.007, New P: 0.058
iter 18 loss: 0.114
Actual params: [0.5071, 0.0583]
-Original Grad: 0.071, -lr * Pred Grad:  0.026, New P: 0.533
-Original Grad: 0.026, -lr * Pred Grad:  0.011, New P: 0.069
iter 19 loss: 0.112
Actual params: [0.5329, 0.0691]
-Original Grad: -0.012, -lr * Pred Grad:  0.023, New P: 0.555
-Original Grad: 0.015, -lr * Pred Grad:  0.012, New P: 0.082
iter 20 loss: 0.113
Actual params: [0.5554, 0.0816]
Target params: [1.1812, 0.2779]
Actual params: [0.5397, 0.7849]
-Original Grad: -0.985, -lr * Pred Grad:  -0.100, New P: 0.440
-Original Grad: -1.213, -lr * Pred Grad:  -0.100, New P: 0.685
iter 0 loss: 0.717
Actual params: [0.4397, 0.6849]
-Original Grad: -1.446, -lr * Pred Grad:  -0.099, New P: 0.340
-Original Grad: -2.801, -lr * Pred Grad:  -0.095, New P: 0.590
iter 1 loss: 0.432
Actual params: [0.3405, 0.59  ]
-Original Grad: 2.043, -lr * Pred Grad:  -0.001, New P: 0.339
-Original Grad: -0.732, -lr * Pred Grad:  -0.086, New P: 0.504
iter 2 loss: 0.200
Actual params: [0.3391, 0.5037]
-Original Grad: 0.411, -lr * Pred Grad:  0.008, New P: 0.347
-Original Grad: 0.140, -lr * Pred Grad:  -0.068, New P: 0.436
iter 3 loss: 0.187
Actual params: [0.3468, 0.4358]
-Original Grad: 0.163, -lr * Pred Grad:  0.010, New P: 0.357
-Original Grad: 0.115, -lr * Pred Grad:  -0.055, New P: 0.380
iter 4 loss: 0.191
Actual params: [0.3566, 0.3803]
-Original Grad: 0.339, -lr * Pred Grad:  0.015, New P: 0.371
-Original Grad: -0.003, -lr * Pred Grad:  -0.048, New P: 0.333
iter 5 loss: 0.188
Actual params: [0.3714, 0.3325]
-Original Grad: 0.197, -lr * Pred Grad:  0.017, New P: 0.388
-Original Grad: 0.041, -lr * Pred Grad:  -0.041, New P: 0.291
iter 6 loss: 0.186
Actual params: [0.3879, 0.2914]
-Original Grad: 0.139, -lr * Pred Grad:  0.017, New P: 0.405
-Original Grad: 0.040, -lr * Pred Grad:  -0.036, New P: 0.256
iter 7 loss: 0.186
Actual params: [0.405 , 0.2558]
-Original Grad: -0.019, -lr * Pred Grad:  0.015, New P: 0.420
-Original Grad: 0.060, -lr * Pred Grad:  -0.031, New P: 0.225
iter 8 loss: 0.188
Actual params: [0.4198, 0.2252]
-Original Grad: -0.016, -lr * Pred Grad:  0.013, New P: 0.433
-Original Grad: 0.065, -lr * Pred Grad:  -0.026, New P: 0.199
iter 9 loss: 0.189
Actual params: [0.4327, 0.1988]
-Original Grad: 0.117, -lr * Pred Grad:  0.014, New P: 0.446
-Original Grad: 0.036, -lr * Pred Grad:  -0.023, New P: 0.176
iter 10 loss: 0.191
Actual params: [0.4463, 0.1758]
-Original Grad: 0.011, -lr * Pred Grad:  0.012, New P: 0.459
-Original Grad: 0.051, -lr * Pred Grad:  -0.020, New P: 0.156
iter 11 loss: 0.191
Actual params: [0.4587, 0.1558]
-Original Grad: 0.033, -lr * Pred Grad:  0.012, New P: 0.471
-Original Grad: 0.041, -lr * Pred Grad:  -0.017, New P: 0.138
iter 12 loss: 0.192
Actual params: [0.4705, 0.1385]
-Original Grad: 0.030, -lr * Pred Grad:  0.011, New P: 0.482
-Original Grad: 0.032, -lr * Pred Grad:  -0.015, New P: 0.123
iter 13 loss: 0.192
Actual params: [0.4817, 0.1233]
-Original Grad: 0.087, -lr * Pred Grad:  0.012, New P: 0.493
-Original Grad: 0.016, -lr * Pred Grad:  -0.013, New P: 0.110
iter 14 loss: 0.192
Actual params: [0.4933, 0.1098]
-Original Grad: 0.078, -lr * Pred Grad:  0.012, New P: 0.505
-Original Grad: 0.008, -lr * Pred Grad:  -0.012, New P: 0.098
iter 15 loss: 0.191
Actual params: [0.5053, 0.0978]
-Original Grad: -0.009, -lr * Pred Grad:  0.011, New P: 0.516
-Original Grad: -0.007, -lr * Pred Grad:  -0.011, New P: 0.087
iter 16 loss: 0.190
Actual params: [0.5159, 0.0867]
-Original Grad: 0.131, -lr * Pred Grad:  0.012, New P: 0.528
-Original Grad: -0.012, -lr * Pred Grad:  -0.010, New P: 0.076
iter 17 loss: 0.190
Actual params: [0.528 , 0.0764]
-Original Grad: 0.277, -lr * Pred Grad:  0.016, New P: 0.544
-Original Grad: 0.009, -lr * Pred Grad:  -0.009, New P: 0.067
iter 18 loss: 0.187
Actual params: [0.544 , 0.0672]
-Original Grad: 0.257, -lr * Pred Grad:  0.019, New P: 0.563
-Original Grad: 0.024, -lr * Pred Grad:  -0.008, New P: 0.059
iter 19 loss: 0.183
Actual params: [0.5631, 0.0592]
-Original Grad: 0.255, -lr * Pred Grad:  0.022, New P: 0.585
-Original Grad: 0.041, -lr * Pred Grad:  -0.007, New P: 0.053
iter 20 loss: 0.177
Actual params: [0.5852, 0.0527]
Target params: [1.1812, 0.2779]
Actual params: [0.5515, 0.3482]
-Original Grad: 0.248, -lr * Pred Grad:  0.100, New P: 0.651
-Original Grad: -0.125, -lr * Pred Grad:  -0.100, New P: 0.248
iter 0 loss: 0.697
Actual params: [0.6515, 0.2482]
-Original Grad: 0.729, -lr * Pred Grad:  0.092, New P: 0.744
-Original Grad: -0.086, -lr * Pred Grad:  -0.097, New P: 0.151
iter 1 loss: 0.645
Actual params: [0.7435, 0.1508]
-Original Grad: 2.962, -lr * Pred Grad:  0.080, New P: 0.823
-Original Grad: -0.364, -lr * Pred Grad:  -0.088, New P: 0.063
iter 2 loss: 0.479
Actual params: [0.8232, 0.0629]
-Original Grad: 0.833, -lr * Pred Grad:  0.078, New P: 0.902
-Original Grad: -0.080, -lr * Pred Grad:  -0.082, New P: -0.019
iter 3 loss: 0.301
Actual params: [ 0.9015, -0.0193]
-Original Grad: 1.709, -lr * Pred Grad:  0.084, New P: 0.986
-Original Grad: -0.067, -lr * Pred Grad:  -0.077, New P: -0.097
iter 4 loss: 0.195
Actual params: [ 0.9857, -0.0968]
-Original Grad: 0.453, -lr * Pred Grad:  0.078, New P: 1.064
-Original Grad: 0.039, -lr * Pred Grad:  -0.062, New P: -0.158
iter 5 loss: 0.146
Actual params: [ 1.0641, -0.1583]
-Original Grad: 0.149, -lr * Pred Grad:  0.071, New P: 1.135
-Original Grad: 0.063, -lr * Pred Grad:  -0.045, New P: -0.204
iter 6 loss: 0.125
Actual params: [ 1.1347, -0.2037]
-Original Grad: -0.170, -lr * Pred Grad:  0.060, New P: 1.194
-Original Grad: 0.143, -lr * Pred Grad:  -0.022, New P: -0.225
iter 7 loss: 0.131
Actual params: [ 1.1945, -0.2254]
-Original Grad: -0.454, -lr * Pred Grad:  0.047, New P: 1.241
-Original Grad: 0.182, -lr * Pred Grad:  0.001, New P: -0.224
iter 8 loss: 0.149
Actual params: [ 1.2411, -0.2243]
-Original Grad: -0.613, -lr * Pred Grad:  0.033, New P: 1.274
-Original Grad: 0.198, -lr * Pred Grad:  0.020, New P: -0.205
iter 9 loss: 0.169
Actual params: [ 1.2741, -0.2047]
-Original Grad: -0.437, -lr * Pred Grad:  0.024, New P: 1.298
-Original Grad: 0.129, -lr * Pred Grad:  0.029, New P: -0.176
iter 10 loss: 0.181
Actual params: [ 1.2978, -0.176 ]
-Original Grad: -0.680, -lr * Pred Grad:  0.012, New P: 1.310
-Original Grad: 0.165, -lr * Pred Grad:  0.039, New P: -0.137
iter 11 loss: 0.191
Actual params: [ 1.3102, -0.137 ]
-Original Grad: -0.655, -lr * Pred Grad:  0.003, New P: 1.313
-Original Grad: 0.146, -lr * Pred Grad:  0.046, New P: -0.091
iter 12 loss: 0.193
Actual params: [ 1.313 , -0.0908]
-Original Grad: -0.635, -lr * Pred Grad:  -0.005, New P: 1.308
-Original Grad: 0.088, -lr * Pred Grad:  0.049, New P: -0.042
iter 13 loss: 0.189
Actual params: [ 1.3075, -0.0421]
-Original Grad: -0.706, -lr * Pred Grad:  -0.014, New P: 1.294
-Original Grad: 0.057, -lr * Pred Grad:  0.049, New P: 0.006
iter 14 loss: 0.183
Actual params: [1.294 , 0.0065]
-Original Grad: -0.417, -lr * Pred Grad:  -0.017, New P: 1.277
-Original Grad: 0.054, -lr * Pred Grad:  0.048, New P: 0.055
iter 15 loss: 0.171
Actual params: [1.2767, 0.0548]
-Original Grad: -0.559, -lr * Pred Grad:  -0.022, New P: 1.254
-Original Grad: 0.014, -lr * Pred Grad:  0.045, New P: 0.100
iter 16 loss: 0.161
Actual params: [1.2543, 0.0998]
-Original Grad: -0.726, -lr * Pred Grad:  -0.029, New P: 1.225
-Original Grad: -0.029, -lr * Pred Grad:  0.038, New P: 0.138
iter 17 loss: 0.147
Actual params: [1.2254, 0.1381]
-Original Grad: -0.382, -lr * Pred Grad:  -0.031, New P: 1.195
-Original Grad: -0.052, -lr * Pred Grad:  0.030, New P: 0.168
iter 18 loss: 0.136
Actual params: [1.1946, 0.1683]
-Original Grad: -0.404, -lr * Pred Grad:  -0.033, New P: 1.162
-Original Grad: -0.063, -lr * Pred Grad:  0.022, New P: 0.190
iter 19 loss: 0.126
Actual params: [1.1617, 0.1902]
-Original Grad: -0.093, -lr * Pred Grad:  -0.031, New P: 1.131
-Original Grad: -0.009, -lr * Pred Grad:  0.019, New P: 0.209
iter 20 loss: 0.120
Actual params: [1.1306, 0.2093]
Target params: [1.1812, 0.2779]
Actual params: [1.0909, 1.0332]
-Original Grad: -0.160, -lr * Pred Grad:  -0.100, New P: 0.991
-Original Grad: 0.334, -lr * Pred Grad:  0.100, New P: 1.133
iter 0 loss: 0.400
Actual params: [0.9909, 1.1332]
-Original Grad: 0.512, -lr * Pred Grad:  0.051, New P: 1.042
-Original Grad: 0.160, -lr * Pred Grad:  0.093, New P: 1.226
iter 1 loss: 0.402
Actual params: [1.0419, 1.2258]
-Original Grad: -0.119, -lr * Pred Grad:  0.025, New P: 1.067
-Original Grad: -0.755, -lr * Pred Grad:  -0.026, New P: 1.200
iter 2 loss: 0.361
Actual params: [1.0665, 1.1999]
-Original Grad: -0.056, -lr * Pred Grad:  0.014, New P: 1.081
-Original Grad: 0.380, -lr * Pred Grad:  0.005, New P: 1.205
iter 3 loss: 0.362
Actual params: [1.0807, 1.2046]
-Original Grad: -0.191, -lr * Pred Grad:  -0.007, New P: 1.074
-Original Grad: 0.352, -lr * Pred Grad:  0.023, New P: 1.228
iter 4 loss: 0.361
Actual params: [1.0742, 1.2277]
-Original Grad: -1.550, -lr * Pred Grad:  -0.051, New P: 1.023
-Original Grad: 5.189, -lr * Pred Grad:  0.055, New P: 1.283
iter 5 loss: 0.359
Actual params: [1.0233, 1.2827]
-Original Grad: 0.442, -lr * Pred Grad:  -0.030, New P: 0.994
-Original Grad: 0.388, -lr * Pred Grad:  0.052, New P: 1.334
iter 6 loss: 0.350
Actual params: [0.9935, 1.3344]
-Original Grad: 5.652, -lr * Pred Grad:  0.040, New P: 1.033
-Original Grad: 4.294, -lr * Pred Grad:  0.067, New P: 1.401
iter 7 loss: 0.357
Actual params: [1.0333, 1.4009]
-Original Grad: 0.332, -lr * Pred Grad:  0.038, New P: 1.071
-Original Grad: 0.791, -lr * Pred Grad:  0.064, New P: 1.465
iter 8 loss: 0.313
Actual params: [1.0714, 1.4653]
-Original Grad: 0.387, -lr * Pred Grad:  0.037, New P: 1.108
-Original Grad: -0.632, -lr * Pred Grad:  0.053, New P: 1.518
iter 9 loss: 0.280
Actual params: [1.1084, 1.518 ]
-Original Grad: -0.399, -lr * Pred Grad:  0.030, New P: 1.138
-Original Grad: 0.627, -lr * Pred Grad:  0.051, New P: 1.569
iter 10 loss: 0.263
Actual params: [1.1383, 1.5695]
-Original Grad: -0.535, -lr * Pred Grad:  0.022, New P: 1.161
-Original Grad: 0.650, -lr * Pred Grad:  0.051, New P: 1.620
iter 11 loss: 0.248
Actual params: [1.1607, 1.62  ]
-Original Grad: -0.318, -lr * Pred Grad:  0.018, New P: 1.178
-Original Grad: 0.314, -lr * Pred Grad:  0.048, New P: 1.668
iter 12 loss: 0.239
Actual params: [1.1784, 1.6678]
-Original Grad: -0.075, -lr * Pred Grad:  0.015, New P: 1.194
-Original Grad: -0.023, -lr * Pred Grad:  0.043, New P: 1.711
iter 13 loss: 0.234
Actual params: [1.1937, 1.7107]
-Original Grad: -0.363, -lr * Pred Grad:  0.011, New P: 1.205
-Original Grad: 0.319, -lr * Pred Grad:  0.041, New P: 1.752
iter 14 loss: 0.232
Actual params: [1.2045, 1.7518]
-Original Grad: -0.217, -lr * Pred Grad:  0.008, New P: 1.213
-Original Grad: 0.125, -lr * Pred Grad:  0.038, New P: 1.790
iter 15 loss: 0.233
Actual params: [1.2126, 1.7898]
-Original Grad: -0.125, -lr * Pred Grad:  0.006, New P: 1.219
-Original Grad: 0.021, -lr * Pred Grad:  0.035, New P: 1.825
iter 16 loss: 0.234
Actual params: [1.2189, 1.8246]
-Original Grad: -0.111, -lr * Pred Grad:  0.005, New P: 1.224
-Original Grad: 0.002, -lr * Pred Grad:  0.032, New P: 1.856
iter 17 loss: 0.235
Actual params: [1.2237, 1.8561]
-Original Grad: -0.033, -lr * Pred Grad:  0.004, New P: 1.228
-Original Grad: -0.080, -lr * Pred Grad:  0.028, New P: 1.884
iter 18 loss: 0.238
Actual params: [1.2277, 1.8842]
-Original Grad: 0.318, -lr * Pred Grad:  0.006, New P: 1.234
-Original Grad: -0.437, -lr * Pred Grad:  0.022, New P: 1.906
iter 19 loss: 0.241
Actual params: [1.2341, 1.9065]
-Original Grad: 0.048, -lr * Pred Grad:  0.006, New P: 1.240
-Original Grad: -0.136, -lr * Pred Grad:  0.019, New P: 1.926
iter 20 loss: 0.245
Actual params: [1.2403, 1.9257]
Target params: [1.1812, 0.2779]
Actual params: [1.1249, 0.5545]
-Original Grad: 0.214, -lr * Pred Grad:  0.100, New P: 1.225
-Original Grad: 0.731, -lr * Pred Grad:  0.100, New P: 0.655
iter 0 loss: 0.149
Actual params: [1.2249, 0.6545]
-Original Grad: 0.148, -lr * Pred Grad:  0.097, New P: 1.322
-Original Grad: 0.361, -lr * Pred Grad:  0.093, New P: 0.748
iter 1 loss: 0.098
Actual params: [1.3224, 0.7476]
-Original Grad: 0.540, -lr * Pred Grad:  0.090, New P: 1.413
-Original Grad: -0.947, -lr * Pred Grad:  -0.001, New P: 0.746
iter 2 loss: 0.128
Actual params: [1.4126, 0.7461]
-Original Grad: -0.170, -lr * Pred Grad:  0.055, New P: 1.468
-Original Grad: -0.455, -lr * Pred Grad:  -0.021, New P: 0.725
iter 3 loss: 0.101
Actual params: [1.4678, 0.7251]
-Original Grad: -0.430, -lr * Pred Grad:  0.007, New P: 1.475
-Original Grad: 0.047, -lr * Pred Grad:  -0.016, New P: 0.709
iter 4 loss: 0.114
Actual params: [1.4751, 0.7092]
-Original Grad: -0.483, -lr * Pred Grad:  -0.023, New P: 1.452
-Original Grad: 0.280, -lr * Pred Grad:  -0.003, New P: 0.707
iter 5 loss: 0.121
Actual params: [1.4524, 0.7067]
-Original Grad: -0.519, -lr * Pred Grad:  -0.043, New P: 1.410
-Original Grad: 0.349, -lr * Pred Grad:  0.010, New P: 0.717
iter 6 loss: 0.110
Actual params: [1.4099, 0.7171]
-Original Grad: -0.287, -lr * Pred Grad:  -0.049, New P: 1.360
-Original Grad: -0.167, -lr * Pred Grad:  0.003, New P: 0.720
iter 7 loss: 0.091
Actual params: [1.3605, 0.7204]
-Original Grad: 0.251, -lr * Pred Grad:  -0.032, New P: 1.329
-Original Grad: -0.767, -lr * Pred Grad:  -0.021, New P: 0.700
iter 8 loss: 0.091
Actual params: [1.3289, 0.6995]
-Original Grad: 0.201, -lr * Pred Grad:  -0.019, New P: 1.310
-Original Grad: -0.434, -lr * Pred Grad:  -0.031, New P: 0.669
iter 9 loss: 0.085
Actual params: [1.3099, 0.6689]
-Original Grad: 0.037, -lr * Pred Grad:  -0.015, New P: 1.294
-Original Grad: 0.239, -lr * Pred Grad:  -0.020, New P: 0.649
iter 10 loss: 0.084
Actual params: [1.2944, 0.6487]
-Original Grad: 0.020, -lr * Pred Grad:  -0.013, New P: 1.281
-Original Grad: 0.503, -lr * Pred Grad:  -0.004, New P: 0.645
iter 11 loss: 0.093
Actual params: [1.2814, 0.645 ]
-Original Grad: 0.062, -lr * Pred Grad:  -0.009, New P: 1.272
-Original Grad: 0.500, -lr * Pred Grad:  0.010, New P: 0.655
iter 12 loss: 0.095
Actual params: [1.2724, 0.6551]
-Original Grad: 0.109, -lr * Pred Grad:  -0.003, New P: 1.269
-Original Grad: 0.320, -lr * Pred Grad:  0.017, New P: 0.672
iter 13 loss: 0.091
Actual params: [1.2689, 0.6725]
-Original Grad: 0.223, -lr * Pred Grad:  0.006, New P: 1.275
-Original Grad: -0.113, -lr * Pred Grad:  0.013, New P: 0.685
iter 14 loss: 0.089
Actual params: [1.2752, 0.6852]
-Original Grad: 0.290, -lr * Pred Grad:  0.018, New P: 1.293
-Original Grad: -0.465, -lr * Pred Grad:  -0.001, New P: 0.684
iter 15 loss: 0.092
Actual params: [1.2928, 0.6844]
-Original Grad: 0.238, -lr * Pred Grad:  0.025, New P: 1.318
-Original Grad: -0.369, -lr * Pred Grad:  -0.010, New P: 0.674
iter 16 loss: 0.087
Actual params: [1.3182, 0.6744]
-Original Grad: 0.065, -lr * Pred Grad:  0.026, New P: 1.344
-Original Grad: 0.146, -lr * Pred Grad:  -0.005, New P: 0.669
iter 17 loss: 0.083
Actual params: [1.3439, 0.669 ]
-Original Grad: -0.109, -lr * Pred Grad:  0.019, New P: 1.363
-Original Grad: 0.413, -lr * Pred Grad:  0.006, New P: 0.675
iter 18 loss: 0.085
Actual params: [1.3626, 0.6747]
-Original Grad: -0.157, -lr * Pred Grad:  0.010, New P: 1.373
-Original Grad: 0.407, -lr * Pred Grad:  0.015, New P: 0.690
iter 19 loss: 0.085
Actual params: [1.373 , 0.6899]
-Original Grad: -0.142, -lr * Pred Grad:  0.003, New P: 1.377
-Original Grad: 0.197, -lr * Pred Grad:  0.019, New P: 0.709
iter 20 loss: 0.083
Actual params: [1.3765, 0.7086]
Target params: [1.1812, 0.2779]
Actual params: [0.6859, 0.3761]
-Original Grad: 0.112, -lr * Pred Grad:  0.100, New P: 0.786
-Original Grad: -0.053, -lr * Pred Grad:  -0.100, New P: 0.276
iter 0 loss: 0.244
Actual params: [0.7859, 0.2761]
-Original Grad: 0.187, -lr * Pred Grad:  0.098, New P: 0.884
-Original Grad: 0.056, -lr * Pred Grad:  0.008, New P: 0.284
iter 1 loss: 0.232
Actual params: [0.8842, 0.2837]
-Original Grad: 0.076, -lr * Pred Grad:  0.093, New P: 0.977
-Original Grad: 0.059, -lr * Pred Grad:  0.043, New P: 0.327
iter 2 loss: 0.219
Actual params: [0.9769, 0.3272]
-Original Grad: -0.003, -lr * Pred Grad:  0.075, New P: 1.052
-Original Grad: 0.018, -lr * Pred Grad:  0.046, New P: 0.373
iter 3 loss: 0.214
Actual params: [1.052, 0.373]
-Original Grad: -0.007, -lr * Pred Grad:  0.062, New P: 1.114
-Original Grad: 0.003, -lr * Pred Grad:  0.040, New P: 0.413
iter 4 loss: 0.214
Actual params: [1.1138, 0.4134]
-Original Grad: -0.017, -lr * Pred Grad:  0.049, New P: 1.163
-Original Grad: -0.011, -lr * Pred Grad:  0.029, New P: 0.442
iter 5 loss: 0.214
Actual params: [1.1632, 0.4425]
-Original Grad: -0.020, -lr * Pred Grad:  0.039, New P: 1.202
-Original Grad: -0.006, -lr * Pred Grad:  0.022, New P: 0.465
iter 6 loss: 0.215
Actual params: [1.2018, 0.4646]
-Original Grad: -0.030, -lr * Pred Grad:  0.027, New P: 1.229
-Original Grad: -0.004, -lr * Pred Grad:  0.017, New P: 0.482
iter 7 loss: 0.217
Actual params: [1.2291, 0.4819]
-Original Grad: -0.035, -lr * Pred Grad:  0.017, New P: 1.246
-Original Grad: -0.013, -lr * Pred Grad:  0.009, New P: 0.491
iter 8 loss: 0.218
Actual params: [1.2458, 0.4906]
-Original Grad: -0.030, -lr * Pred Grad:  0.009, New P: 1.254
-Original Grad: -0.020, -lr * Pred Grad:  -0.002, New P: 0.489
iter 9 loss: 0.218
Actual params: [1.2544, 0.4885]
-Original Grad: -0.029, -lr * Pred Grad:  0.002, New P: 1.256
-Original Grad: -0.018, -lr * Pred Grad:  -0.010, New P: 0.478
iter 10 loss: 0.219
Actual params: [1.2563, 0.4782]
-Original Grad: -0.033, -lr * Pred Grad:  -0.005, New P: 1.251
-Original Grad: -0.015, -lr * Pred Grad:  -0.016, New P: 0.462
iter 11 loss: 0.218
Actual params: [1.2514, 0.4623]
-Original Grad: -0.029, -lr * Pred Grad:  -0.010, New P: 1.241
-Original Grad: 0.004, -lr * Pred Grad:  -0.013, New P: 0.450
iter 12 loss: 0.218
Actual params: [1.2412, 0.4497]
-Original Grad: -0.039, -lr * Pred Grad:  -0.017, New P: 1.224
-Original Grad: -0.002, -lr * Pred Grad:  -0.012, New P: 0.437
iter 13 loss: 0.218
Actual params: [1.2245, 0.4374]
-Original Grad: -0.035, -lr * Pred Grad:  -0.022, New P: 1.203
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: 0.426
iter 14 loss: 0.217
Actual params: [1.2026, 0.4259]
-Original Grad: -0.032, -lr * Pred Grad:  -0.026, New P: 1.177
-Original Grad: -0.020, -lr * Pred Grad:  -0.020, New P: 0.406
iter 15 loss: 0.216
Actual params: [1.1767, 0.4063]
-Original Grad: -0.022, -lr * Pred Grad:  -0.028, New P: 1.149
-Original Grad: -0.013, -lr * Pred Grad:  -0.024, New P: 0.383
iter 16 loss: 0.215
Actual params: [1.149 , 0.3826]
-Original Grad: -0.015, -lr * Pred Grad:  -0.028, New P: 1.121
-Original Grad: -0.015, -lr * Pred Grad:  -0.028, New P: 0.354
iter 17 loss: 0.214
Actual params: [1.121 , 0.3542]
-Original Grad: -0.014, -lr * Pred Grad:  -0.028, New P: 1.093
-Original Grad: 0.008, -lr * Pred Grad:  -0.022, New P: 0.332
iter 18 loss: 0.214
Actual params: [1.0927, 0.3321]
-Original Grad: -0.008, -lr * Pred Grad:  -0.027, New P: 1.065
-Original Grad: 0.022, -lr * Pred Grad:  -0.010, New P: 0.322
iter 19 loss: 0.214
Actual params: [1.0654, 0.3224]
-Original Grad: 0.001, -lr * Pred Grad:  -0.025, New P: 1.041
-Original Grad: 0.016, -lr * Pred Grad:  -0.001, New P: 0.321
iter 20 loss: 0.214
Actual params: [1.0408, 0.3213]
Target params: [1.1812, 0.2779]
Actual params: [0.7976, 0.9218]
-Original Grad: -1.224, -lr * Pred Grad:  -0.100, New P: 0.698
-Original Grad: -1.434, -lr * Pred Grad:  -0.100, New P: 0.822
iter 0 loss: 0.486
Actual params: [0.6976, 0.8218]
-Original Grad: -0.733, -lr * Pred Grad:  -0.096, New P: 0.602
-Original Grad: -0.729, -lr * Pred Grad:  -0.093, New P: 0.728
iter 1 loss: 0.328
Actual params: [0.6019, 0.7283]
-Original Grad: 0.213, -lr * Pred Grad:  -0.064, New P: 0.538
-Original Grad: 0.015, -lr * Pred Grad:  -0.072, New P: 0.657
iter 2 loss: 0.304
Actual params: [0.5381, 0.6567]
-Original Grad: 0.213, -lr * Pred Grad:  -0.043, New P: 0.495
-Original Grad: 0.274, -lr * Pred Grad:  -0.048, New P: 0.609
iter 3 loss: 0.335
Actual params: [0.495 , 0.6086]
-Original Grad: 0.170, -lr * Pred Grad:  -0.030, New P: 0.465
-Original Grad: 0.113, -lr * Pred Grad:  -0.037, New P: 0.572
iter 4 loss: 0.348
Actual params: [0.465 , 0.5719]
-Original Grad: 0.132, -lr * Pred Grad:  -0.021, New P: 0.444
-Original Grad: 0.128, -lr * Pred Grad:  -0.027, New P: 0.544
iter 5 loss: 0.357
Actual params: [0.4441, 0.5444]
-Original Grad: 0.120, -lr * Pred Grad:  -0.014, New P: 0.430
-Original Grad: 0.090, -lr * Pred Grad:  -0.021, New P: 0.523
iter 6 loss: 0.363
Actual params: [0.4299, 0.5232]
-Original Grad: 0.114, -lr * Pred Grad:  -0.009, New P: 0.421
-Original Grad: 0.070, -lr * Pred Grad:  -0.017, New P: 0.507
iter 7 loss: 0.366
Actual params: [0.4213, 0.5067]
-Original Grad: 0.129, -lr * Pred Grad:  -0.003, New P: 0.418
-Original Grad: 0.046, -lr * Pred Grad:  -0.013, New P: 0.493
iter 8 loss: 0.368
Actual params: [0.418 , 0.4933]
-Original Grad: 0.123, -lr * Pred Grad:  0.001, New P: 0.419
-Original Grad: 0.050, -lr * Pred Grad:  -0.010, New P: 0.483
iter 9 loss: 0.369
Actual params: [0.419 , 0.4829]
-Original Grad: 0.240, -lr * Pred Grad:  0.009, New P: 0.428
-Original Grad: -0.030, -lr * Pred Grad:  -0.010, New P: 0.473
iter 10 loss: 0.369
Actual params: [0.4276, 0.4727]
-Original Grad: 0.296, -lr * Pred Grad:  0.017, New P: 0.444
-Original Grad: -0.061, -lr * Pred Grad:  -0.011, New P: 0.462
iter 11 loss: 0.368
Actual params: [0.4444, 0.4618]
-Original Grad: 0.074, -lr * Pred Grad:  0.018, New P: 0.462
-Original Grad: 0.071, -lr * Pred Grad:  -0.008, New P: 0.454
iter 12 loss: 0.368
Actual params: [0.4619, 0.454 ]
-Original Grad: 0.118, -lr * Pred Grad:  0.019, New P: 0.481
-Original Grad: 0.057, -lr * Pred Grad:  -0.005, New P: 0.449
iter 13 loss: 0.366
Actual params: [0.4814, 0.4486]
-Original Grad: 0.133, -lr * Pred Grad:  0.022, New P: 0.503
-Original Grad: 0.056, -lr * Pred Grad:  -0.003, New P: 0.445
iter 14 loss: 0.364
Actual params: [0.5031, 0.4454]
-Original Grad: 0.140, -lr * Pred Grad:  0.024, New P: 0.527
-Original Grad: 0.061, -lr * Pred Grad:  -0.001, New P: 0.444
iter 15 loss: 0.361
Actual params: [0.5272, 0.4444]
-Original Grad: 0.136, -lr * Pred Grad:  0.026, New P: 0.553
-Original Grad: 0.060, -lr * Pred Grad:  0.001, New P: 0.445
iter 16 loss: 0.358
Actual params: [0.5533, 0.4452]
-Original Grad: 0.161, -lr * Pred Grad:  0.029, New P: 0.582
-Original Grad: 0.065, -lr * Pred Grad:  0.003, New P: 0.448
iter 17 loss: 0.354
Actual params: [0.5819, 0.4479]
-Original Grad: 0.195, -lr * Pred Grad:  0.032, New P: 0.614
-Original Grad: 0.077, -lr * Pred Grad:  0.005, New P: 0.453
iter 18 loss: 0.349
Actual params: [0.614 , 0.4527]
-Original Grad: 0.226, -lr * Pred Grad:  0.036, New P: 0.650
-Original Grad: 0.084, -lr * Pred Grad:  0.007, New P: 0.460
iter 19 loss: 0.342
Actual params: [0.65  , 0.4597]
-Original Grad: 0.273, -lr * Pred Grad:  0.041, New P: 0.691
-Original Grad: 0.087, -lr * Pred Grad:  0.009, New P: 0.469
iter 20 loss: 0.332
Actual params: [0.691 , 0.4687]
Target params: [1.1812, 0.2779]
Actual params: [0.7765, 0.9   ]
-Original Grad: -0.989, -lr * Pred Grad:  -0.100, New P: 0.676
-Original Grad: -0.227, -lr * Pred Grad:  -0.100, New P: 0.800
iter 0 loss: 0.363
Actual params: [0.6765, 0.8   ]
-Original Grad: 0.285, -lr * Pred Grad:  -0.044, New P: 0.633
-Original Grad: -0.307, -lr * Pred Grad:  -0.100, New P: 0.700
iter 1 loss: 0.338
Actual params: [0.6328, 0.7004]
-Original Grad: 0.133, -lr * Pred Grad:  -0.025, New P: 0.607
-Original Grad: -0.083, -lr * Pred Grad:  -0.089, New P: 0.611
iter 2 loss: 0.329
Actual params: [0.6074, 0.6115]
-Original Grad: 0.266, -lr * Pred Grad:  -0.006, New P: 0.602
-Original Grad: -0.048, -lr * Pred Grad:  -0.079, New P: 0.532
iter 3 loss: 0.327
Actual params: [0.6017, 0.5321]
-Original Grad: 0.492, -lr * Pred Grad:  0.018, New P: 0.620
-Original Grad: -0.009, -lr * Pred Grad:  -0.068, New P: 0.464
iter 4 loss: 0.327
Actual params: [0.6202, 0.4637]
-Original Grad: -0.064, -lr * Pred Grad:  0.013, New P: 0.633
-Original Grad: 0.127, -lr * Pred Grad:  -0.040, New P: 0.424
iter 5 loss: 0.325
Actual params: [0.6332, 0.4237]
-Original Grad: -0.004, -lr * Pred Grad:  0.011, New P: 0.644
-Original Grad: 0.133, -lr * Pred Grad:  -0.018, New P: 0.406
iter 6 loss: 0.319
Actual params: [0.6444, 0.4059]
-Original Grad: 0.016, -lr * Pred Grad:  0.011, New P: 0.655
-Original Grad: 0.146, -lr * Pred Grad:  0.001, New P: 0.407
iter 7 loss: 0.322
Actual params: [0.655 , 0.4069]
-Original Grad: 0.820, -lr * Pred Grad:  0.036, New P: 0.691
-Original Grad: 0.091, -lr * Pred Grad:  0.010, New P: 0.417
iter 8 loss: 0.317
Actual params: [0.6906, 0.4172]
-Original Grad: 0.003, -lr * Pred Grad:  0.032, New P: 0.722
-Original Grad: 0.177, -lr * Pred Grad:  0.026, New P: 0.443
iter 9 loss: 0.301
Actual params: [0.7225, 0.4431]
-Original Grad: 0.271, -lr * Pred Grad:  0.037, New P: 0.760
-Original Grad: 0.179, -lr * Pred Grad:  0.038, New P: 0.481
iter 10 loss: 0.285
Actual params: [0.7595, 0.4812]
-Original Grad: -0.038, -lr * Pred Grad:  0.032, New P: 0.792
-Original Grad: 0.188, -lr * Pred Grad:  0.048, New P: 0.530
iter 11 loss: 0.255
Actual params: [0.7916, 0.5296]
-Original Grad: -1.662, -lr * Pred Grad:  -0.017, New P: 0.774
-Original Grad: 0.277, -lr * Pred Grad:  0.060, New P: 0.590
iter 12 loss: 0.219
Actual params: [0.7744, 0.5901]
-Original Grad: 1.613, -lr * Pred Grad:  0.016, New P: 0.790
-Original Grad: -0.168, -lr * Pred Grad:  0.040, New P: 0.630
iter 13 loss: 0.236
Actual params: [0.7904, 0.6302]
-Original Grad: -1.704, -lr * Pred Grad:  -0.013, New P: 0.777
-Original Grad: 0.304, -lr * Pred Grad:  0.054, New P: 0.684
iter 14 loss: 0.227
Actual params: [0.7769, 0.6838]
-Original Grad: 1.200, -lr * Pred Grad:  0.006, New P: 0.783
-Original Grad: -0.441, -lr * Pred Grad:  0.016, New P: 0.699
iter 15 loss: 0.255
Actual params: [0.7826, 0.6994]
-Original Grad: 1.143, -lr * Pred Grad:  0.020, New P: 0.803
-Original Grad: -0.412, -lr * Pred Grad:  -0.009, New P: 0.690
iter 16 loss: 0.255
Actual params: [0.803 , 0.6904]
-Original Grad: 23.827, -lr * Pred Grad:  0.052, New P: 0.855
-Original Grad: -6.790, -lr * Pred Grad:  -0.050, New P: 0.640
iter 17 loss: 0.230
Actual params: [0.855, 0.64 ]
-Original Grad: 0.069, -lr * Pred Grad:  0.047, New P: 0.902
-Original Grad: -0.023, -lr * Pred Grad:  -0.046, New P: 0.594
iter 18 loss: 0.147
Actual params: [0.9023, 0.5941]
-Original Grad: 0.373, -lr * Pred Grad:  0.044, New P: 0.946
-Original Grad: 0.067, -lr * Pred Grad:  -0.041, New P: 0.553
iter 19 loss: 0.108
Actual params: [0.9461, 0.5528]
-Original Grad: -0.393, -lr * Pred Grad:  0.039, New P: 0.985
-Original Grad: 0.106, -lr * Pred Grad:  -0.037, New P: 0.516
iter 20 loss: 0.111
Actual params: [0.9852, 0.5161]
Target params: [1.1812, 0.2779]
Actual params: [0.9202, 0.6989]
-Original Grad: -0.135, -lr * Pred Grad:  -0.100, New P: 0.820
-Original Grad: -1.176, -lr * Pred Grad:  -0.100, New P: 0.599
iter 0 loss: 0.624
Actual params: [0.8202, 0.5989]
-Original Grad: 0.040, -lr * Pred Grad:  -0.043, New P: 0.777
-Original Grad: -3.134, -lr * Pred Grad:  -0.093, New P: 0.506
iter 1 loss: 0.405
Actual params: [0.7767, 0.5057]
-Original Grad: 0.129, -lr * Pred Grad:  0.018, New P: 0.795
-Original Grad: -1.336, -lr * Pred Grad:  -0.091, New P: 0.415
iter 2 loss: 0.194
Actual params: [0.7951, 0.4151]
-Original Grad: 0.171, -lr * Pred Grad:  0.050, New P: 0.845
-Original Grad: -0.058, -lr * Pred Grad:  -0.075, New P: 0.340
iter 3 loss: 0.139
Actual params: [0.8451, 0.3399]
-Original Grad: 0.096, -lr * Pred Grad:  0.059, New P: 0.904
-Original Grad: 0.001, -lr * Pred Grad:  -0.063, New P: 0.276
iter 4 loss: 0.131
Actual params: [0.9037, 0.2764]
-Original Grad: 0.109, -lr * Pred Grad:  0.066, New P: 0.970
-Original Grad: 0.045, -lr * Pred Grad:  -0.054, New P: 0.222
iter 5 loss: 0.127
Actual params: [0.9699, 0.2223]
-Original Grad: 0.146, -lr * Pred Grad:  0.074, New P: 1.044
-Original Grad: 0.056, -lr * Pred Grad:  -0.046, New P: 0.176
iter 6 loss: 0.121
Actual params: [1.0443, 0.1759]
-Original Grad: 0.149, -lr * Pred Grad:  0.080, New P: 1.124
-Original Grad: 0.069, -lr * Pred Grad:  -0.040, New P: 0.136
iter 7 loss: 0.112
Actual params: [1.1245, 0.1359]
-Original Grad: 0.130, -lr * Pred Grad:  0.084, New P: 1.208
-Original Grad: 0.109, -lr * Pred Grad:  -0.034, New P: 0.102
iter 8 loss: 0.105
Actual params: [1.208, 0.102]
-Original Grad: 0.126, -lr * Pred Grad:  0.086, New P: 1.294
-Original Grad: 0.122, -lr * Pred Grad:  -0.029, New P: 0.073
iter 9 loss: 0.098
Actual params: [1.294 , 0.0733]
-Original Grad: 0.090, -lr * Pred Grad:  0.086, New P: 1.380
-Original Grad: 0.128, -lr * Pred Grad:  -0.024, New P: 0.049
iter 10 loss: 0.092
Actual params: [1.3797, 0.0493]
-Original Grad: 0.056, -lr * Pred Grad:  0.083, New P: 1.463
-Original Grad: 0.122, -lr * Pred Grad:  -0.020, New P: 0.029
iter 11 loss: 0.089
Actual params: [1.4626, 0.0294]
-Original Grad: 0.056, -lr * Pred Grad:  0.080, New P: 1.543
-Original Grad: 0.117, -lr * Pred Grad:  -0.016, New P: 0.013
iter 12 loss: 0.087
Actual params: [1.543 , 0.0131]
-Original Grad: 0.068, -lr * Pred Grad:  0.079, New P: 1.622
-Original Grad: 0.125, -lr * Pred Grad:  -0.013, New P: 0.000
iter 13 loss: 0.084
Actual params: [1.6225e+00, 1.0567e-05]
-Original Grad: 0.072, -lr * Pred Grad:  0.079, New P: 1.702
-Original Grad: 0.127, -lr * Pred Grad:  -0.010, New P: -0.010
iter 14 loss: 0.080
Actual params: [ 1.7015, -0.0101]
-Original Grad: 0.095, -lr * Pred Grad:  0.080, New P: 1.782
-Original Grad: 0.219, -lr * Pred Grad:  -0.006, New P: -0.016
iter 15 loss: 0.072
Actual params: [ 1.782 , -0.0163]
-Original Grad: 0.042, -lr * Pred Grad:  0.077, New P: 1.859
-Original Grad: 0.093, -lr * Pred Grad:  -0.004, New P: -0.021
iter 16 loss: 0.068
Actual params: [ 1.8594, -0.0206]
-Original Grad: 0.034, -lr * Pred Grad:  0.074, New P: 1.933
-Original Grad: 0.083, -lr * Pred Grad:  -0.003, New P: -0.023
iter 17 loss: 0.066
Actual params: [ 1.9333, -0.0234]
-Original Grad: 0.027, -lr * Pred Grad:  0.070, New P: 2.003
-Original Grad: 0.079, -lr * Pred Grad:  -0.001, New P: -0.025
iter 18 loss: 0.064
Actual params: [ 2.0033, -0.0248]
-Original Grad: 0.001, -lr * Pred Grad:  0.064, New P: 2.067
-Original Grad: 0.021, -lr * Pred Grad:  -0.001, New P: -0.026
iter 19 loss: 0.063
Actual params: [ 2.0671, -0.0258]
-Original Grad: -0.015, -lr * Pred Grad:  0.056, New P: 2.123
-Original Grad: -0.006, -lr * Pred Grad:  -0.001, New P: -0.027
iter 20 loss: 0.063
Actual params: [ 2.1234, -0.0268]
