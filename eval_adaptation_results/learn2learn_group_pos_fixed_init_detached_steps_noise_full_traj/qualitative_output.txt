Target params: [1.1812, 0.2779]
iter 0 loss: 0.779
Actual params: [0.5941, 0.5941]
-Original Grad: 0.119, -lr * Pred Grad: 0.066, New P: 0.660
-Original Grad: -0.099, -lr * Pred Grad: -0.048, New P: 0.546
iter 1 loss: 0.755
Actual params: [0.6601, 0.5464]
-Original Grad: 0.366, -lr * Pred Grad: 0.084, New P: 0.745
-Original Grad: -0.090, -lr * Pred Grad: -0.075, New P: 0.471
iter 2 loss: 0.707
Actual params: [0.7446, 0.4712]
-Original Grad: 0.053, -lr * Pred Grad: 0.087, New P: 0.832
-Original Grad: -0.281, -lr * Pred Grad: -0.082, New P: 0.389
iter 3 loss: 0.611
Actual params: [0.832 , 0.3895]
-Original Grad: -0.133, -lr * Pred Grad: 0.080, New P: 0.912
-Original Grad: 0.038, -lr * Pred Grad: -0.077, New P: 0.313
iter 4 loss: 0.473
Actual params: [0.9122, 0.3128]
-Original Grad: 0.000, -lr * Pred Grad: -0.021, New P: 0.891
-Original Grad: -0.173, -lr * Pred Grad: -0.076, New P: 0.237
iter 5 loss: 0.448
Actual params: [0.8913, 0.2372]
-Original Grad: -0.367, -lr * Pred Grad: -0.055, New P: 0.836
-Original Grad: -0.134, -lr * Pred Grad: -0.075, New P: 0.162
iter 6 loss: 0.497
Actual params: [0.8364, 0.1621]
-Original Grad: 0.058, -lr * Pred Grad: -0.056, New P: 0.780
-Original Grad: -0.197, -lr * Pred Grad: -0.078, New P: 0.084
iter 7 loss: 0.545
Actual params: [0.7804, 0.0839]
-Original Grad: 0.269, -lr * Pred Grad: -0.018, New P: 0.762
-Original Grad: -0.012, -lr * Pred Grad: -0.072, New P: 0.012
iter 8 loss: 0.553
Actual params: [0.7624, 0.0117]
-Original Grad: 0.378, -lr * Pred Grad: 0.032, New P: 0.795
-Original Grad: 0.140, -lr * Pred Grad: -0.065, New P: -0.053
iter 9 loss: 0.511
Actual params: [ 0.7945, -0.0534]
-Original Grad: 0.118, -lr * Pred Grad: 0.074, New P: 0.868
-Original Grad: 0.035, -lr * Pred Grad: -0.045, New P: -0.098
iter 10 loss: 0.434
Actual params: [ 0.8682, -0.0984]
-Original Grad: 0.370, -lr * Pred Grad: 0.086, New P: 0.954
-Original Grad: 0.260, -lr * Pred Grad: -0.031, New P: -0.130
iter 11 loss: 0.384
Actual params: [ 0.954 , -0.1298]
-Original Grad: -0.426, -lr * Pred Grad: 0.022, New P: 0.976
-Original Grad: 0.375, -lr * Pred Grad: -0.025, New P: -0.155
iter 12 loss: 1.164
Actual params: [ 0.9759, -0.1549]
-Original Grad: -0.412, -lr * Pred Grad: -0.048, New P: 0.928
-Original Grad: 0.147, -lr * Pred Grad: -0.021, New P: -0.176
iter 13 loss: 1.602
Actual params: [ 0.9283, -0.1756]
-Original Grad: 0.490, -lr * Pred Grad: 0.017, New P: 0.946
-Original Grad: -0.203, -lr * Pred Grad: -0.016, New P: -0.192
iter 14 loss: 1.610
Actual params: [ 0.9457, -0.1917]
-Original Grad: -0.036, -lr * Pred Grad: 0.071, New P: 1.016
-Original Grad: 0.406, -lr * Pred Grad: 0.005, New P: -0.187
iter 15 loss: 1.595
Actual params: [ 1.0163, -0.1866]
-Original Grad: 0.031, -lr * Pred Grad: 0.057, New P: 1.073
-Original Grad: -0.068, -lr * Pred Grad: 0.050, New P: -0.137
iter 16 loss: 1.603
Actual params: [ 1.0733, -0.1371]
-Original Grad: 0.178, -lr * Pred Grad: 0.079, New P: 1.152
-Original Grad: -0.015, -lr * Pred Grad: 0.031, New P: -0.106
iter 17 loss: 1.186
Actual params: [ 1.1519, -0.1056]
-Original Grad: 0.480, -lr * Pred Grad: 0.086, New P: 1.238
-Original Grad: -0.002, -lr * Pred Grad: -0.017, New P: -0.123
iter 18 loss: 1.111
Actual params: [ 1.2382, -0.1229]
-Original Grad: -0.217, -lr * Pred Grad: 0.083, New P: 1.322
-Original Grad: 0.111, -lr * Pred Grad: 0.049, New P: -0.074
iter 19 loss: 0.476
Actual params: [ 1.3215, -0.0738]
-Original Grad: -0.126, -lr * Pred Grad: -0.031, New P: 1.291
-Original Grad: 0.079, -lr * Pred Grad: 0.073, New P: -0.001
iter 20 loss: 0.453
Actual params: [ 1.2909, -0.0013]
-Original Grad: -0.214, -lr * Pred Grad: -0.054, New P: 1.237
-Original Grad: 0.186, -lr * Pred Grad: 0.082, New P: 0.081
Target params: [1.1812, 0.2779]
iter 0 loss: 0.340
Actual params: [0.5941, 0.5941]
-Original Grad: -0.066, -lr * Pred Grad: -0.028, New P: 0.566
-Original Grad: 0.077, -lr * Pred Grad: 0.063, New P: 0.657
iter 1 loss: 0.346
Actual params: [0.5659, 0.6571]
-Original Grad: -0.082, -lr * Pred Grad: -0.070, New P: 0.496
-Original Grad: 0.105, -lr * Pred Grad: 0.081, New P: 0.739
iter 2 loss: 0.384
Actual params: [0.4958, 0.7385]
-Original Grad: 0.014, -lr * Pred Grad: -0.071, New P: 0.425
-Original Grad: -0.052, -lr * Pred Grad: 0.001, New P: 0.739
iter 3 loss: 0.402
Actual params: [0.4251, 0.7393]
-Original Grad: -0.148, -lr * Pred Grad: -0.069, New P: 0.356
-Original Grad: 0.109, -lr * Pred Grad: 0.050, New P: 0.790
iter 4 loss: 0.431
Actual params: [0.356 , 0.7896]
-Original Grad: -0.046, -lr * Pred Grad: -0.067, New P: 0.289
-Original Grad: 0.102, -lr * Pred Grad: 0.074, New P: 0.863
iter 5 loss: 0.483
Actual params: [0.2888, 0.8635]
-Original Grad: 0.038, -lr * Pred Grad: -0.060, New P: 0.229
-Original Grad: 0.043, -lr * Pred Grad: 0.067, New P: 0.930
iter 6 loss: 0.522
Actual params: [0.2293, 0.9302]
-Original Grad: -0.010, -lr * Pred Grad: -0.044, New P: 0.185
-Original Grad: 0.137, -lr * Pred Grad: 0.079, New P: 1.009
iter 7 loss: 0.547
Actual params: [0.1852, 1.0094]
-Original Grad: 0.034, -lr * Pred Grad: -0.026, New P: 0.159
-Original Grad: 0.031, -lr * Pred Grad: 0.058, New P: 1.068
iter 8 loss: 0.552
Actual params: [0.1588, 1.0676]
-Original Grad: 0.074, -lr * Pred Grad: -0.000, New P: 0.159
-Original Grad: 0.021, -lr * Pred Grad: 0.024, New P: 1.092
iter 9 loss: 0.553
Actual params: [0.1585, 1.0918]
-Original Grad: -0.054, -lr * Pred Grad: 0.005, New P: 0.164
-Original Grad: -0.125, -lr * Pred Grad: -0.042, New P: 1.050
iter 10 loss: 0.550
Actual params: [0.1639, 1.0499]
-Original Grad: 0.135, -lr * Pred Grad: 0.044, New P: 0.208
-Original Grad: -0.112, -lr * Pred Grad: -0.056, New P: 0.994
iter 11 loss: 0.540
Actual params: [0.2075, 0.9936]
-Original Grad: -0.047, -lr * Pred Grad: -0.008, New P: 0.200
-Original Grad: -0.060, -lr * Pred Grad: -0.057, New P: 0.936
iter 12 loss: 0.532
Actual params: [0.1996, 0.9362]
-Original Grad: -0.090, -lr * Pred Grad: -0.049, New P: 0.151
-Original Grad: -0.139, -lr * Pred Grad: -0.059, New P: 0.877
iter 13 loss: 0.529
Actual params: [0.1507, 0.8775]
-Original Grad: 0.068, -lr * Pred Grad: -0.023, New P: 0.128
-Original Grad: -0.002, -lr * Pred Grad: -0.055, New P: 0.823
iter 14 loss: 0.521
Actual params: [0.1281, 0.8228]
-Original Grad: 0.035, -lr * Pred Grad: 0.025, New P: 0.153
-Original Grad: 0.079, -lr * Pred Grad: -0.030, New P: 0.792
iter 15 loss: 0.506
Actual params: [0.1528, 0.7924]
-Original Grad: 0.035, -lr * Pred Grad: 0.030, New P: 0.183
-Original Grad: 0.024, -lr * Pred Grad: -0.008, New P: 0.784
iter 16 loss: 0.495
Actual params: [0.1826, 0.7841]
-Original Grad: -0.030, -lr * Pred Grad: -0.020, New P: 0.162
-Original Grad: 0.044, -lr * Pred Grad: 0.021, New P: 0.805
iter 17 loss: 0.505
Actual params: [0.1622, 0.8051]
-Original Grad: -0.087, -lr * Pred Grad: -0.050, New P: 0.112
-Original Grad: 0.073, -lr * Pred Grad: 0.055, New P: 0.860
iter 18 loss: 0.533
Actual params: [0.1122, 0.8599]
-Original Grad: -0.039, -lr * Pred Grad: -0.056, New P: 0.056
-Original Grad: 0.011, -lr * Pred Grad: 0.021, New P: 0.880
iter 19 loss: 0.541
Actual params: [0.0564, 0.8805]
-Original Grad: -0.023, -lr * Pred Grad: -0.047, New P: 0.009
-Original Grad: 0.105, -lr * Pred Grad: 0.063, New P: 0.944
iter 20 loss: 0.564
Actual params: [0.0089, 0.9437]
-Original Grad: -0.033, -lr * Pred Grad: -0.038, New P: -0.030
-Original Grad: 0.152, -lr * Pred Grad: 0.080, New P: 1.024
Target params: [1.1812, 0.2779]
iter 0 loss: 0.415
Actual params: [0.5941, 0.5941]
-Original Grad: 0.130, -lr * Pred Grad: 0.066, New P: 0.660
-Original Grad: -0.378, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.329
Actual params: [0.6605, 0.5309]
-Original Grad: -0.002, -lr * Pred Grad: 0.079, New P: 0.740
-Original Grad: -0.713, -lr * Pred Grad: -0.080, New P: 0.451
iter 2 loss: 0.297
Actual params: [0.74  , 0.4506]
-Original Grad: -0.015, -lr * Pred Grad: -0.012, New P: 0.728
-Original Grad: -0.180, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.309
Actual params: [0.7282, 0.367 ]
-Original Grad: 0.015, -lr * Pred Grad: -0.002, New P: 0.726
-Original Grad: -0.051, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.325
Actual params: [0.7264, 0.2829]
-Original Grad: -0.015, -lr * Pred Grad: -0.013, New P: 0.714
-Original Grad: -0.122, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.337
Actual params: [0.7135, 0.1987]
-Original Grad: 0.033, -lr * Pred Grad: 0.023, New P: 0.736
-Original Grad: -0.058, -lr * Pred Grad: -0.084, New P: 0.115
iter 6 loss: 0.335
Actual params: [0.7362, 0.115 ]
-Original Grad: -0.018, -lr * Pred Grad: -0.020, New P: 0.716
-Original Grad: -0.065, -lr * Pred Grad: -0.081, New P: 0.034
iter 7 loss: 0.340
Actual params: [0.7162, 0.0342]
-Original Grad: 0.032, -lr * Pred Grad: 0.010, New P: 0.726
-Original Grad: -0.113, -lr * Pred Grad: -0.077, New P: -0.043
iter 8 loss: 0.343
Actual params: [ 0.7262, -0.043 ]
-Original Grad: -0.008, -lr * Pred Grad: -0.016, New P: 0.711
-Original Grad: 0.006, -lr * Pred Grad: -0.071, New P: -0.114
iter 9 loss: 0.348
Actual params: [ 0.7106, -0.1139]
-Original Grad: 0.028, -lr * Pred Grad: 0.007, New P: 0.718
-Original Grad: -0.097, -lr * Pred Grad: -0.069, New P: -0.183
iter 10 loss: 0.353
Actual params: [ 0.7178, -0.1828]
-Original Grad: 0.024, -lr * Pred Grad: 0.008, New P: 0.726
-Original Grad: -0.048, -lr * Pred Grad: -0.067, New P: -0.250
iter 11 loss: 0.359
Actual params: [ 0.7258, -0.2497]
-Original Grad: -0.007, -lr * Pred Grad: -0.009, New P: 0.717
-Original Grad: 0.012, -lr * Pred Grad: -0.060, New P: -0.310
iter 12 loss: 0.363
Actual params: [ 0.7167, -0.3099]
-Original Grad: -0.014, -lr * Pred Grad: -0.021, New P: 0.696
-Original Grad: 0.027, -lr * Pred Grad: -0.043, New P: -0.353
iter 13 loss: 0.369
Actual params: [ 0.6956, -0.3525]
-Original Grad: 0.048, -lr * Pred Grad: 0.005, New P: 0.700
-Original Grad: -0.019, -lr * Pred Grad: -0.035, New P: -0.388
iter 14 loss: 0.370
Actual params: [ 0.7004, -0.388 ]
-Original Grad: 0.033, -lr * Pred Grad: 0.023, New P: 0.723
-Original Grad: 0.082, -lr * Pred Grad: -0.026, New P: -0.414
iter 15 loss: 0.369
Actual params: [ 0.723 , -0.4144]
-Original Grad: 0.024, -lr * Pred Grad: 0.015, New P: 0.737
-Original Grad: 0.036, -lr * Pred Grad: -0.024, New P: -0.439
iter 16 loss: 0.369
Actual params: [ 0.7375, -0.4387]
-Original Grad: 0.032, -lr * Pred Grad: 0.014, New P: 0.751
-Original Grad: 0.071, -lr * Pred Grad: -0.021, New P: -0.460
iter 17 loss: 0.370
Actual params: [ 0.7511, -0.4599]
-Original Grad: -0.007, -lr * Pred Grad: -0.005, New P: 0.746
-Original Grad: 0.037, -lr * Pred Grad: -0.021, New P: -0.480
iter 18 loss: 0.372
Actual params: [ 0.7457, -0.4805]
-Original Grad: 0.023, -lr * Pred Grad: -0.002, New P: 0.743
-Original Grad: 0.230, -lr * Pred Grad: -0.017, New P: -0.498
iter 19 loss: 0.374
Actual params: [ 0.7435, -0.4976]
-Original Grad: -0.009, -lr * Pred Grad: -0.008, New P: 0.735
-Original Grad: 0.046, -lr * Pred Grad: -0.008, New P: -0.506
iter 20 loss: 0.376
Actual params: [ 0.7355, -0.5059]
-Original Grad: 0.009, -lr * Pred Grad: -0.008, New P: 0.728
-Original Grad: 0.042, -lr * Pred Grad: 0.016, New P: -0.490
Target params: [1.1812, 0.2779]
iter 0 loss: 1.381
Actual params: [0.5941, 0.5941]
-Original Grad: 0.168, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.078, -lr * Pred Grad: -0.037, New P: 0.557
iter 1 loss: 1.362
Actual params: [0.6612, 0.557 ]
-Original Grad: -0.019, -lr * Pred Grad: 0.082, New P: 0.743
-Original Grad: -0.067, -lr * Pred Grad: -0.071, New P: 0.486
iter 2 loss: 1.331
Actual params: [0.7431, 0.4863]
-Original Grad: 0.168, -lr * Pred Grad: 0.084, New P: 0.827
-Original Grad: -0.125, -lr * Pred Grad: -0.076, New P: 0.410
iter 3 loss: 1.265
Actual params: [0.8267, 0.4101]
-Original Grad: 0.155, -lr * Pred Grad: 0.085, New P: 0.912
-Original Grad: -0.134, -lr * Pred Grad: -0.075, New P: 0.335
iter 4 loss: 1.069
Actual params: [0.9115, 0.3349]
-Original Grad: 0.236, -lr * Pred Grad: 0.087, New P: 0.998
-Original Grad: -0.127, -lr * Pred Grad: -0.073, New P: 0.262
iter 5 loss: 0.860
Actual params: [0.9983, 0.2615]
-Original Grad: -0.094, -lr * Pred Grad: 0.074, New P: 1.073
-Original Grad: 0.010, -lr * Pred Grad: -0.069, New P: 0.192
iter 6 loss: 0.766
Actual params: [1.0726, 0.1924]
-Original Grad: 0.016, -lr * Pred Grad: -0.008, New P: 1.065
-Original Grad: -0.038, -lr * Pred Grad: -0.066, New P: 0.126
iter 7 loss: 0.749
Actual params: [1.0651, 0.1262]
-Original Grad: 0.063, -lr * Pred Grad: 0.047, New P: 1.112
-Original Grad: -0.002, -lr * Pred Grad: -0.059, New P: 0.067
iter 8 loss: 0.730
Actual params: [1.1124, 0.0673]
-Original Grad: 0.095, -lr * Pred Grad: 0.071, New P: 1.184
-Original Grad: 0.001, -lr * Pred Grad: -0.045, New P: 0.023
iter 9 loss: 0.736
Actual params: [1.1839, 0.0228]
-Original Grad: -0.056, -lr * Pred Grad: -0.020, New P: 1.164
-Original Grad: -0.054, -lr * Pred Grad: -0.047, New P: -0.024
iter 10 loss: 0.729
Actual params: [ 1.1637, -0.0237]
-Original Grad: 0.040, -lr * Pred Grad: 0.015, New P: 1.179
-Original Grad: -0.002, -lr * Pred Grad: -0.041, New P: -0.064
iter 11 loss: 0.734
Actual params: [ 1.1788, -0.0644]
-Original Grad: -0.043, -lr * Pred Grad: -0.036, New P: 1.143
-Original Grad: 0.019, -lr * Pred Grad: -0.025, New P: -0.089
iter 12 loss: 0.721
Actual params: [ 1.1427, -0.089 ]
-Original Grad: -0.072, -lr * Pred Grad: -0.049, New P: 1.094
-Original Grad: 0.019, -lr * Pred Grad: -0.006, New P: -0.095
iter 13 loss: 0.726
Actual params: [ 1.094 , -0.0949]
-Original Grad: -0.009, -lr * Pred Grad: -0.040, New P: 1.054
-Original Grad: 0.018, -lr * Pred Grad: 0.006, New P: -0.089
iter 14 loss: 0.736
Actual params: [ 1.0545, -0.089 ]
-Original Grad: 0.040, -lr * Pred Grad: -0.000, New P: 1.054
-Original Grad: -0.020, -lr * Pred Grad: -0.023, New P: -0.112
iter 15 loss: 0.735
Actual params: [ 1.0544, -0.1116]
-Original Grad: 0.106, -lr * Pred Grad: 0.047, New P: 1.102
-Original Grad: 0.011, -lr * Pred Grad: -0.024, New P: -0.135
iter 16 loss: 0.723
Actual params: [ 1.1018, -0.1355]
-Original Grad: 0.142, -lr * Pred Grad: 0.074, New P: 1.176
-Original Grad: 0.017, -lr * Pred Grad: -0.014, New P: -0.149
iter 17 loss: 0.731
Actual params: [ 1.1762, -0.1491]
-Original Grad: 0.041, -lr * Pred Grad: 0.077, New P: 1.253
-Original Grad: 0.010, -lr * Pred Grad: -0.008, New P: -0.157
iter 18 loss: 0.767
Actual params: [ 1.253 , -0.1575]
-Original Grad: -0.005, -lr * Pred Grad: -0.000, New P: 1.253
-Original Grad: 0.001, -lr * Pred Grad: -0.014, New P: -0.172
iter 19 loss: 0.768
Actual params: [ 1.2528, -0.1717]
-Original Grad: -0.088, -lr * Pred Grad: -0.038, New P: 1.215
-Original Grad: 0.025, -lr * Pred Grad: -0.010, New P: -0.182
iter 20 loss: 0.747
Actual params: [ 1.2149, -0.1817]
-Original Grad: 0.026, -lr * Pred Grad: -0.023, New P: 1.192
-Original Grad: 0.014, -lr * Pred Grad: -0.006, New P: -0.188
Target params: [1.1812, 0.2779]
iter 0 loss: 0.916
Actual params: [0.5941, 0.5941]
-Original Grad: 0.104, -lr * Pred Grad: 0.065, New P: 0.659
-Original Grad: -0.180, -lr * Pred Grad: -0.061, New P: 0.533
iter 1 loss: 0.889
Actual params: [0.6594, 0.5328]
-Original Grad: 0.129, -lr * Pred Grad: 0.083, New P: 0.743
-Original Grad: 0.019, -lr * Pred Grad: -0.075, New P: 0.458
iter 2 loss: 0.835
Actual params: [0.7425, 0.4583]
-Original Grad: 0.201, -lr * Pred Grad: 0.086, New P: 0.829
-Original Grad: -0.046, -lr * Pred Grad: -0.071, New P: 0.387
iter 3 loss: 0.755
Actual params: [0.829 , 0.3871]
-Original Grad: 0.198, -lr * Pred Grad: 0.087, New P: 0.916
-Original Grad: -0.031, -lr * Pred Grad: -0.068, New P: 0.319
iter 4 loss: 0.681
Actual params: [0.9163, 0.3187]
-Original Grad: 0.050, -lr * Pred Grad: 0.086, New P: 1.003
-Original Grad: 0.047, -lr * Pred Grad: -0.061, New P: 0.258
iter 5 loss: 0.613
Actual params: [1.0028, 0.2579]
-Original Grad: 0.057, -lr * Pred Grad: 0.075, New P: 1.078
-Original Grad: 0.032, -lr * Pred Grad: -0.040, New P: 0.218
iter 6 loss: 0.563
Actual params: [1.0781, 0.2182]
-Original Grad: 0.058, -lr * Pred Grad: 0.050, New P: 1.128
-Original Grad: -0.001, -lr * Pred Grad: -0.020, New P: 0.198
iter 7 loss: 0.560
Actual params: [1.128 , 0.1982]
-Original Grad: 0.041, -lr * Pred Grad: 0.063, New P: 1.191
-Original Grad: -0.064, -lr * Pred Grad: -0.032, New P: 0.166
iter 8 loss: 0.567
Actual params: [1.1906, 0.1662]
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 1.191
-Original Grad: -0.077, -lr * Pred Grad: -0.055, New P: 0.111
iter 9 loss: 0.565
Actual params: [1.1914, 0.1114]
-Original Grad: -0.035, -lr * Pred Grad: -0.006, New P: 1.185
-Original Grad: -0.043, -lr * Pred Grad: -0.057, New P: 0.054
iter 10 loss: 0.552
Actual params: [1.1853, 0.0545]
-Original Grad: -0.044, -lr * Pred Grad: -0.042, New P: 1.143
-Original Grad: -0.038, -lr * Pred Grad: -0.052, New P: 0.003
iter 11 loss: 0.531
Actual params: [1.1431, 0.0026]
-Original Grad: -0.031, -lr * Pred Grad: -0.042, New P: 1.101
-Original Grad: -0.056, -lr * Pred Grad: -0.050, New P: -0.048
iter 12 loss: 0.523
Actual params: [ 1.1007, -0.0475]
-Original Grad: 0.060, -lr * Pred Grad: -0.002, New P: 1.098
-Original Grad: -0.065, -lr * Pred Grad: -0.053, New P: -0.101
iter 13 loss: 0.520
Actual params: [ 1.0985, -0.1006]
-Original Grad: -0.022, -lr * Pred Grad: -0.000, New P: 1.098
-Original Grad: 0.012, -lr * Pred Grad: -0.041, New P: -0.142
iter 14 loss: 0.521
Actual params: [ 1.0983, -0.1421]
-Original Grad: 0.038, -lr * Pred Grad: 0.006, New P: 1.104
-Original Grad: -0.082, -lr * Pred Grad: -0.044, New P: -0.186
iter 15 loss: 0.524
Actual params: [ 1.1044, -0.1863]
-Original Grad: 0.043, -lr * Pred Grad: 0.024, New P: 1.128
-Original Grad: -0.080, -lr * Pred Grad: -0.053, New P: -0.239
iter 16 loss: 0.527
Actual params: [ 1.1285, -0.2391]
-Original Grad: 0.014, -lr * Pred Grad: 0.009, New P: 1.137
-Original Grad: -0.008, -lr * Pred Grad: -0.050, New P: -0.289
iter 17 loss: 0.528
Actual params: [ 1.1372, -0.2887]
-Original Grad: 0.020, -lr * Pred Grad: 0.005, New P: 1.142
-Original Grad: -0.019, -lr * Pred Grad: -0.040, New P: -0.328
iter 18 loss: 0.528
Actual params: [ 1.1421, -0.3284]
-Original Grad: 0.010, -lr * Pred Grad: -0.004, New P: 1.138
-Original Grad: 0.004, -lr * Pred Grad: -0.029, New P: -0.358
iter 19 loss: 0.527
Actual params: [ 1.1385, -0.3575]
-Original Grad: 0.037, -lr * Pred Grad: 0.011, New P: 1.149
-Original Grad: -0.058, -lr * Pred Grad: -0.035, New P: -0.393
iter 20 loss: 0.530
Actual params: [ 1.1491, -0.3929]
-Original Grad: 0.027, -lr * Pred Grad: 0.014, New P: 1.163
-Original Grad: 0.003, -lr * Pred Grad: -0.034, New P: -0.427
Target params: [1.1812, 0.2779]
iter 0 loss: 0.651
Actual params: [0.5941, 0.5941]
-Original Grad: -0.009, -lr * Pred Grad: 0.028, New P: 0.622
-Original Grad: -0.063, -lr * Pred Grad: -0.026, New P: 0.568
iter 1 loss: 0.620
Actual params: [0.6219, 0.5683]
-Original Grad: -0.049, -lr * Pred Grad: -0.048, New P: 0.573
-Original Grad: -0.200, -lr * Pred Grad: -0.072, New P: 0.496
iter 2 loss: 0.614
Actual params: [0.5735, 0.4963]
-Original Grad: -0.012, -lr * Pred Grad: -0.066, New P: 0.507
-Original Grad: -0.115, -lr * Pred Grad: -0.081, New P: 0.415
iter 3 loss: 0.624
Actual params: [0.5075, 0.415 ]
-Original Grad: -0.073, -lr * Pred Grad: -0.066, New P: 0.441
-Original Grad: -0.113, -lr * Pred Grad: -0.081, New P: 0.334
iter 4 loss: 0.641
Actual params: [0.4411, 0.3339]
-Original Grad: 0.077, -lr * Pred Grad: -0.049, New P: 0.392
-Original Grad: -0.027, -lr * Pred Grad: -0.074, New P: 0.260
iter 5 loss: 0.651
Actual params: [0.3919, 0.2599]
-Original Grad: 0.148, -lr * Pred Grad: -0.008, New P: 0.384
-Original Grad: -0.048, -lr * Pred Grad: -0.070, New P: 0.190
iter 6 loss: 0.648
Actual params: [0.3841, 0.1901]
-Original Grad: 0.038, -lr * Pred Grad: 0.046, New P: 0.430
-Original Grad: -0.051, -lr * Pred Grad: -0.068, New P: 0.123
iter 7 loss: 0.628
Actual params: [0.4297, 0.1226]
-Original Grad: -0.022, -lr * Pred Grad: 0.012, New P: 0.442
-Original Grad: -0.103, -lr * Pred Grad: -0.066, New P: 0.056
iter 8 loss: 0.617
Actual params: [0.4419, 0.0564]
-Original Grad: 0.091, -lr * Pred Grad: 0.049, New P: 0.491
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -0.006
iter 9 loss: 0.595
Actual params: [ 0.4908, -0.0057]
-Original Grad: -0.015, -lr * Pred Grad: -0.012, New P: 0.479
-Original Grad: -0.011, -lr * Pred Grad: -0.051, New P: -0.057
iter 10 loss: 0.595
Actual params: [ 0.4789, -0.0571]
-Original Grad: 0.006, -lr * Pred Grad: 0.009, New P: 0.488
-Original Grad: -0.023, -lr * Pred Grad: -0.044, New P: -0.101
iter 11 loss: 0.589
Actual params: [ 0.4883, -0.1012]
-Original Grad: 0.145, -lr * Pred Grad: 0.060, New P: 0.548
-Original Grad: -0.031, -lr * Pred Grad: -0.045, New P: -0.146
iter 12 loss: 0.569
Actual params: [ 0.5484, -0.146 ]
-Original Grad: 0.193, -lr * Pred Grad: 0.081, New P: 0.629
-Original Grad: -0.026, -lr * Pred Grad: -0.045, New P: -0.191
iter 13 loss: 0.537
Actual params: [ 0.6295, -0.1909]
-Original Grad: -0.029, -lr * Pred Grad: 0.069, New P: 0.699
-Original Grad: -0.040, -lr * Pred Grad: -0.047, New P: -0.237
iter 14 loss: 0.505
Actual params: [ 0.6988, -0.2375]
-Original Grad: 0.105, -lr * Pred Grad: 0.062, New P: 0.760
-Original Grad: -0.041, -lr * Pred Grad: -0.046, New P: -0.284
iter 15 loss: 0.464
Actual params: [ 0.7603, -0.2836]
-Original Grad: 0.103, -lr * Pred Grad: 0.076, New P: 0.836
-Original Grad: 0.017, -lr * Pred Grad: -0.029, New P: -0.313
iter 16 loss: 0.406
Actual params: [ 0.8364, -0.3131]
-Original Grad: 0.099, -lr * Pred Grad: 0.078, New P: 0.914
-Original Grad: -0.029, -lr * Pred Grad: -0.028, New P: -0.341
iter 17 loss: 0.358
Actual params: [ 0.9144, -0.3406]
-Original Grad: 0.204, -lr * Pred Grad: 0.085, New P: 0.999
-Original Grad: -0.007, -lr * Pred Grad: -0.028, New P: -0.369
iter 18 loss: 0.337
Actual params: [ 0.9989, -0.3689]
-Original Grad: 0.074, -lr * Pred Grad: 0.085, New P: 1.084
-Original Grad: 0.014, -lr * Pred Grad: -0.022, New P: -0.390
iter 19 loss: 0.333
Actual params: [ 1.0837, -0.3904]
-Original Grad: 0.017, -lr * Pred Grad: 0.057, New P: 1.141
-Original Grad: -0.055, -lr * Pred Grad: -0.032, New P: -0.423
iter 20 loss: 0.337
Actual params: [ 1.1408, -0.4228]
-Original Grad: -0.007, -lr * Pred Grad: -0.000, New P: 1.140
-Original Grad: -0.009, -lr * Pred Grad: -0.037, New P: -0.459
Target params: [1.1812, 0.2779]
iter 0 loss: 0.458
Actual params: [0.5941, 0.5941]
-Original Grad: 0.011, -lr * Pred Grad: 0.043, New P: 0.637
-Original Grad: -0.261, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.437
Actual params: [0.6367, 0.531 ]
-Original Grad: 0.036, -lr * Pred Grad: 0.012, New P: 0.648
-Original Grad: -0.151, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.432
Actual params: [0.6484, 0.4504]
-Original Grad: 0.030, -lr * Pred Grad: -0.024, New P: 0.625
-Original Grad: -0.073, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.443
Actual params: [0.6246, 0.3671]
-Original Grad: -0.006, -lr * Pred Grad: -0.022, New P: 0.603
-Original Grad: -0.020, -lr * Pred Grad: -0.081, New P: 0.286
iter 4 loss: 0.451
Actual params: [0.603, 0.286]
-Original Grad: -0.001, -lr * Pred Grad: -0.006, New P: 0.597
-Original Grad: -0.105, -lr * Pred Grad: -0.077, New P: 0.209
iter 5 loss: 0.450
Actual params: [0.5974, 0.2091]
-Original Grad: -0.014, -lr * Pred Grad: -0.015, New P: 0.582
-Original Grad: 0.002, -lr * Pred Grad: -0.071, New P: 0.138
iter 6 loss: 0.449
Actual params: [0.5824, 0.1384]
-Original Grad: 0.061, -lr * Pred Grad: 0.025, New P: 0.608
-Original Grad: -0.015, -lr * Pred Grad: -0.068, New P: 0.070
iter 7 loss: 0.435
Actual params: [0.6078, 0.0704]
-Original Grad: -0.040, -lr * Pred Grad: -0.020, New P: 0.588
-Original Grad: -0.069, -lr * Pred Grad: -0.065, New P: 0.005
iter 8 loss: 0.438
Actual params: [0.5877, 0.0053]
-Original Grad: -0.024, -lr * Pred Grad: -0.029, New P: 0.559
-Original Grad: 0.042, -lr * Pred Grad: -0.052, New P: -0.047
iter 9 loss: 0.444
Actual params: [ 0.5592, -0.0472]
-Original Grad: 0.030, -lr * Pred Grad: -0.005, New P: 0.554
-Original Grad: -0.005, -lr * Pred Grad: -0.038, New P: -0.085
iter 10 loss: 0.443
Actual params: [ 0.5543, -0.0854]
-Original Grad: 0.055, -lr * Pred Grad: 0.030, New P: 0.584
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.118
iter 11 loss: 0.432
Actual params: [ 0.5841, -0.1181]
-Original Grad: 0.024, -lr * Pred Grad: 0.025, New P: 0.609
-Original Grad: -0.001, -lr * Pred Grad: -0.032, New P: -0.150
iter 12 loss: 0.422
Actual params: [ 0.6088, -0.1502]
-Original Grad: -0.049, -lr * Pred Grad: -0.023, New P: 0.586
-Original Grad: 0.014, -lr * Pred Grad: -0.026, New P: -0.176
iter 13 loss: 0.428
Actual params: [ 0.5858, -0.1758]
-Original Grad: 0.077, -lr * Pred Grad: 0.013, New P: 0.599
-Original Grad: -0.000, -lr * Pred Grad: -0.016, New P: -0.192
iter 14 loss: 0.422
Actual params: [ 0.5988, -0.192 ]
-Original Grad: -0.009, -lr * Pred Grad: 0.004, New P: 0.603
-Original Grad: 0.017, -lr * Pred Grad: -0.004, New P: -0.196
iter 15 loss: 0.421
Actual params: [ 0.6026, -0.1955]
-Original Grad: -0.023, -lr * Pred Grad: -0.021, New P: 0.582
-Original Grad: 0.024, -lr * Pred Grad: -0.002, New P: -0.197
iter 16 loss: 0.427
Actual params: [ 0.5819, -0.1972]
-Original Grad: -0.017, -lr * Pred Grad: -0.028, New P: 0.554
-Original Grad: 0.000, -lr * Pred Grad: -0.012, New P: -0.209
iter 17 loss: 0.434
Actual params: [ 0.5541, -0.209 ]
-Original Grad: 0.042, -lr * Pred Grad: -0.003, New P: 0.551
-Original Grad: 0.017, -lr * Pred Grad: -0.012, New P: -0.221
iter 18 loss: 0.434
Actual params: [ 0.551 , -0.2215]
-Original Grad: 0.029, -lr * Pred Grad: 0.018, New P: 0.569
-Original Grad: 0.038, -lr * Pred Grad: 0.001, New P: -0.220
iter 19 loss: 0.429
Actual params: [ 0.5692, -0.22  ]
-Original Grad: 0.034, -lr * Pred Grad: 0.021, New P: 0.591
-Original Grad: -0.006, -lr * Pred Grad: -0.009, New P: -0.229
iter 20 loss: 0.422
Actual params: [ 0.5906, -0.2287]
-Original Grad: 0.009, -lr * Pred Grad: 0.004, New P: 0.595
-Original Grad: 0.006, -lr * Pred Grad: -0.019, New P: -0.248
Target params: [1.1812, 0.2779]
iter 0 loss: 0.809
Actual params: [0.5941, 0.5941]
-Original Grad: -0.231, -lr * Pred Grad: -0.063, New P: 0.531
-Original Grad: -0.145, -lr * Pred Grad: -0.058, New P: 0.536
iter 1 loss: 0.802
Actual params: [0.5313, 0.5357]
-Original Grad: -0.052, -lr * Pred Grad: -0.080, New P: 0.451
-Original Grad: 0.017, -lr * Pred Grad: -0.071, New P: 0.464
iter 2 loss: 0.797
Actual params: [0.4511, 0.4643]
-Original Grad: 0.118, -lr * Pred Grad: -0.075, New P: 0.376
-Original Grad: 0.011, -lr * Pred Grad: -0.069, New P: 0.396
iter 3 loss: 0.797
Actual params: [0.3757, 0.3956]
-Original Grad: 0.041, -lr * Pred Grad: -0.068, New P: 0.307
-Original Grad: -0.196, -lr * Pred Grad: -0.069, New P: 0.327
iter 4 loss: 0.796
Actual params: [0.3073, 0.327 ]
-Original Grad: -0.010, -lr * Pred Grad: -0.056, New P: 0.251
-Original Grad: -0.001, -lr * Pred Grad: -0.067, New P: 0.260
iter 5 loss: 0.793
Actual params: [0.251 , 0.2603]
-Original Grad: 0.008, -lr * Pred Grad: -0.029, New P: 0.222
-Original Grad: 0.009, -lr * Pred Grad: -0.060, New P: 0.200
iter 6 loss: 0.789
Actual params: [0.2223, 0.2005]
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: 0.224
-Original Grad: 0.028, -lr * Pred Grad: -0.041, New P: 0.159
iter 7 loss: 0.785
Actual params: [0.2237, 0.1591]
-Original Grad: 0.036, -lr * Pred Grad: 0.017, New P: 0.240
-Original Grad: 0.040, -lr * Pred Grad: -0.024, New P: 0.135
iter 8 loss: 0.781
Actual params: [0.2403, 0.1351]
-Original Grad: 0.024, -lr * Pred Grad: 0.014, New P: 0.255
-Original Grad: 0.020, -lr * Pred Grad: -0.005, New P: 0.130
iter 9 loss: 0.779
Actual params: [0.2546, 0.1303]
-Original Grad: 0.029, -lr * Pred Grad: 0.017, New P: 0.272
-Original Grad: -0.077, -lr * Pred Grad: -0.035, New P: 0.096
iter 10 loss: 0.773
Actual params: [0.2718, 0.0956]
-Original Grad: 0.027, -lr * Pred Grad: 0.013, New P: 0.285
-Original Grad: -0.008, -lr * Pred Grad: -0.044, New P: 0.051
iter 11 loss: 0.766
Actual params: [0.2848, 0.0512]
-Original Grad: 0.026, -lr * Pred Grad: 0.013, New P: 0.298
-Original Grad: 0.007, -lr * Pred Grad: -0.027, New P: 0.024
iter 12 loss: 0.761
Actual params: [0.2976, 0.0239]
-Original Grad: 0.057, -lr * Pred Grad: 0.032, New P: 0.330
-Original Grad: -0.022, -lr * Pred Grad: -0.024, New P: -0.000
iter 13 loss: 0.754
Actual params: [ 3.2961e-01, -5.2925e-05]
-Original Grad: 0.037, -lr * Pred Grad: 0.029, New P: 0.358
-Original Grad: -0.034, -lr * Pred Grad: -0.035, New P: -0.035
iter 14 loss: 0.746
Actual params: [ 0.3585, -0.0348]
-Original Grad: 0.031, -lr * Pred Grad: 0.021, New P: 0.379
-Original Grad: -0.022, -lr * Pred Grad: -0.039, New P: -0.074
iter 15 loss: 0.739
Actual params: [ 0.3795, -0.0736]
-Original Grad: 0.055, -lr * Pred Grad: 0.033, New P: 0.413
-Original Grad: 0.003, -lr * Pred Grad: -0.032, New P: -0.106
iter 16 loss: 0.731
Actual params: [ 0.4126, -0.1057]
-Original Grad: 0.033, -lr * Pred Grad: 0.024, New P: 0.436
-Original Grad: -0.025, -lr * Pred Grad: -0.030, New P: -0.136
iter 17 loss: 0.725
Actual params: [ 0.4365, -0.1359]
-Original Grad: 0.021, -lr * Pred Grad: 0.012, New P: 0.449
-Original Grad: -0.043, -lr * Pred Grad: -0.038, New P: -0.174
iter 18 loss: 0.720
Actual params: [ 0.4486, -0.1736]
-Original Grad: 0.106, -lr * Pred Grad: 0.048, New P: 0.497
-Original Grad: -0.010, -lr * Pred Grad: -0.038, New P: -0.212
iter 19 loss: 0.709
Actual params: [ 0.4966, -0.2119]
-Original Grad: 0.050, -lr * Pred Grad: 0.054, New P: 0.550
-Original Grad: -0.032, -lr * Pred Grad: -0.039, New P: -0.251
iter 20 loss: 0.697
Actual params: [ 0.5503, -0.2507]
-Original Grad: 0.083, -lr * Pred Grad: 0.060, New P: 0.610
-Original Grad: -0.015, -lr * Pred Grad: -0.037, New P: -0.288
Target params: [1.1812, 0.2779]
iter 0 loss: 0.789
Actual params: [0.5941, 0.5941]
-Original Grad: 0.150, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.052, -lr * Pred Grad: -0.016, New P: 0.578
iter 1 loss: 0.754
Actual params: [0.6609, 0.5781]
-Original Grad: 0.103, -lr * Pred Grad: 0.084, New P: 0.745
-Original Grad: -0.049, -lr * Pred Grad: -0.064, New P: 0.514
iter 2 loss: 0.693
Actual params: [0.7446, 0.5144]
-Original Grad: -0.064, -lr * Pred Grad: 0.064, New P: 0.809
-Original Grad: 0.002, -lr * Pred Grad: -0.068, New P: 0.446
iter 3 loss: 0.623
Actual params: [0.8091, 0.4463]
-Original Grad: -0.613, -lr * Pred Grad: -0.035, New P: 0.774
-Original Grad: -0.056, -lr * Pred Grad: -0.066, New P: 0.380
iter 4 loss: 0.655
Actual params: [0.7736, 0.3798]
-Original Grad: -0.070, -lr * Pred Grad: -0.067, New P: 0.707
-Original Grad: 0.160, -lr * Pred Grad: -0.044, New P: 0.336
iter 5 loss: 0.718
Actual params: [0.7067, 0.336 ]
-Original Grad: 0.352, -lr * Pred Grad: -0.029, New P: 0.677
-Original Grad: 0.009, -lr * Pred Grad: -0.005, New P: 0.331
iter 6 loss: 0.744
Actual params: [0.6774, 0.3307]
-Original Grad: 0.100, -lr * Pred Grad: 0.034, New P: 0.712
-Original Grad: 0.082, -lr * Pred Grad: 0.044, New P: 0.375
iter 7 loss: 0.715
Actual params: [0.7116, 0.3748]
-Original Grad: 0.201, -lr * Pred Grad: 0.075, New P: 0.787
-Original Grad: 0.183, -lr * Pred Grad: 0.077, New P: 0.452
iter 8 loss: 0.646
Actual params: [0.787 , 0.4519]
-Original Grad: 0.027, -lr * Pred Grad: 0.083, New P: 0.870
-Original Grad: 0.024, -lr * Pred Grad: 0.077, New P: 0.529
iter 9 loss: 0.565
Actual params: [0.8701, 0.5286]
-Original Grad: -0.061, -lr * Pred Grad: -0.007, New P: 0.863
-Original Grad: -0.099, -lr * Pred Grad: -0.028, New P: 0.500
iter 10 loss: 0.568
Actual params: [0.8628, 0.5002]
-Original Grad: -0.146, -lr * Pred Grad: -0.050, New P: 0.813
-Original Grad: -0.077, -lr * Pred Grad: -0.052, New P: 0.448
iter 11 loss: 0.618
Actual params: [0.8132, 0.4483]
-Original Grad: 0.070, -lr * Pred Grad: -0.025, New P: 0.789
-Original Grad: -0.185, -lr * Pred Grad: -0.061, New P: 0.388
iter 12 loss: 0.640
Actual params: [0.7886, 0.3875]
-Original Grad: -0.004, -lr * Pred Grad: 0.016, New P: 0.805
-Original Grad: 0.021, -lr * Pred Grad: -0.060, New P: 0.327
iter 13 loss: 0.609
Actual params: [0.8045, 0.3272]
-Original Grad: -0.211, -lr * Pred Grad: -0.045, New P: 0.759
-Original Grad: 0.186, -lr * Pred Grad: -0.029, New P: 0.298
iter 14 loss: 0.667
Actual params: [0.7591, 0.2984]
-Original Grad: -0.368, -lr * Pred Grad: -0.063, New P: 0.696
-Original Grad: -0.023, -lr * Pred Grad: 0.007, New P: 0.305
iter 15 loss: 0.730
Actual params: [0.6957, 0.3053]
-Original Grad: 0.221, -lr * Pred Grad: -0.052, New P: 0.643
-Original Grad: 0.109, -lr * Pred Grad: 0.051, New P: 0.357
iter 16 loss: 0.762
Actual params: [0.6434, 0.3565]
-Original Grad: 0.234, -lr * Pred Grad: -0.006, New P: 0.637
-Original Grad: 0.107, -lr * Pred Grad: 0.076, New P: 0.433
iter 17 loss: 0.765
Actual params: [0.6372, 0.4326]
-Original Grad: 0.050, -lr * Pred Grad: 0.047, New P: 0.685
-Original Grad: 0.027, -lr * Pred Grad: 0.050, New P: 0.482
iter 18 loss: 0.736
Actual params: [0.6847, 0.4822]
-Original Grad: 0.166, -lr * Pred Grad: 0.077, New P: 0.762
-Original Grad: -0.025, -lr * Pred Grad: -0.013, New P: 0.470
iter 19 loss: 0.675
Actual params: [0.762 , 0.4697]
-Original Grad: 0.022, -lr * Pred Grad: 0.080, New P: 0.842
-Original Grad: 0.082, -lr * Pred Grad: 0.043, New P: 0.513
iter 20 loss: 0.596
Actual params: [0.8417, 0.5131]
-Original Grad: -0.049, -lr * Pred Grad: -0.016, New P: 0.826
-Original Grad: -0.092, -lr * Pred Grad: -0.035, New P: 0.478
Target params: [1.1812, 0.2779]
iter 0 loss: 0.859
Actual params: [0.5941, 0.5941]
-Original Grad: 0.021, -lr * Pred Grad: 0.048, New P: 0.642
-Original Grad: -0.130, -lr * Pred Grad: -0.056, New P: 0.538
iter 1 loss: 0.764
Actual params: [0.6422, 0.538 ]
-Original Grad: 0.072, -lr * Pred Grad: 0.056, New P: 0.699
-Original Grad: -0.078, -lr * Pred Grad: -0.077, New P: 0.461
iter 2 loss: 0.569
Actual params: [0.6987, 0.4608]
-Original Grad: 0.027, -lr * Pred Grad: -0.014, New P: 0.685
-Original Grad: -0.188, -lr * Pred Grad: -0.082, New P: 0.379
iter 3 loss: 0.462
Actual params: [0.6852, 0.379 ]
-Original Grad: 0.034, -lr * Pred Grad: 0.014, New P: 0.699
-Original Grad: -0.027, -lr * Pred Grad: -0.077, New P: 0.302
iter 4 loss: 0.399
Actual params: [0.6988, 0.3021]
-Original Grad: 0.043, -lr * Pred Grad: 0.032, New P: 0.731
-Original Grad: -0.169, -lr * Pred Grad: -0.076, New P: 0.226
iter 5 loss: 0.364
Actual params: [0.7311, 0.2259]
-Original Grad: 0.029, -lr * Pred Grad: 0.026, New P: 0.757
-Original Grad: 0.072, -lr * Pred Grad: -0.070, New P: 0.156
iter 6 loss: 0.348
Actual params: [0.7575, 0.1559]
-Original Grad: 0.060, -lr * Pred Grad: 0.046, New P: 0.803
-Original Grad: -0.054, -lr * Pred Grad: -0.067, New P: 0.089
iter 7 loss: 0.333
Actual params: [0.8035, 0.089 ]
-Original Grad: 0.012, -lr * Pred Grad: 0.000, New P: 0.804
-Original Grad: -0.020, -lr * Pred Grad: -0.061, New P: 0.028
iter 8 loss: 0.324
Actual params: [0.8035, 0.0279]
-Original Grad: 0.060, -lr * Pred Grad: 0.045, New P: 0.848
-Original Grad: -0.073, -lr * Pred Grad: -0.059, New P: -0.031
iter 9 loss: 0.320
Actual params: [ 0.8483, -0.0308]
-Original Grad: -0.019, -lr * Pred Grad: -0.021, New P: 0.828
-Original Grad: -0.020, -lr * Pred Grad: -0.054, New P: -0.085
iter 10 loss: 0.317
Actual params: [ 0.8276, -0.0848]
-Original Grad: -0.007, -lr * Pred Grad: -0.009, New P: 0.819
-Original Grad: -0.018, -lr * Pred Grad: -0.046, New P: -0.131
iter 11 loss: 0.316
Actual params: [ 0.819 , -0.1313]
-Original Grad: 0.009, -lr * Pred Grad: -0.012, New P: 0.807
-Original Grad: -0.037, -lr * Pred Grad: -0.045, New P: -0.177
iter 12 loss: 0.318
Actual params: [ 0.8074, -0.1767]
-Original Grad: 0.027, -lr * Pred Grad: 0.010, New P: 0.817
-Original Grad: -0.023, -lr * Pred Grad: -0.041, New P: -0.218
iter 13 loss: 0.321
Actual params: [ 0.8175, -0.2181]
-Original Grad: 0.011, -lr * Pred Grad: 0.002, New P: 0.819
-Original Grad: -0.015, -lr * Pred Grad: -0.033, New P: -0.251
iter 14 loss: 0.326
Actual params: [ 0.8191, -0.2513]
-Original Grad: 0.018, -lr * Pred Grad: 0.004, New P: 0.823
-Original Grad: -0.037, -lr * Pred Grad: -0.036, New P: -0.287
iter 15 loss: 0.332
Actual params: [ 0.8227, -0.2871]
-Original Grad: -0.040, -lr * Pred Grad: -0.025, New P: 0.797
-Original Grad: -0.037, -lr * Pred Grad: -0.041, New P: -0.328
iter 16 loss: 0.342
Actual params: [ 0.7973, -0.3277]
-Original Grad: -0.021, -lr * Pred Grad: -0.032, New P: 0.765
-Original Grad: 0.096, -lr * Pred Grad: -0.014, New P: -0.342
iter 17 loss: 0.347
Actual params: [ 0.7651, -0.3421]
-Original Grad: 0.018, -lr * Pred Grad: -0.016, New P: 0.749
-Original Grad: -0.008, -lr * Pred Grad: 0.004, New P: -0.338
iter 18 loss: 0.348
Actual params: [ 0.7493, -0.3379]
-Original Grad: 0.009, -lr * Pred Grad: -0.001, New P: 0.748
-Original Grad: 0.054, -lr * Pred Grad: 0.017, New P: -0.321
iter 19 loss: 0.346
Actual params: [ 0.7481, -0.321 ]
-Original Grad: -0.002, -lr * Pred Grad: -0.006, New P: 0.742
-Original Grad: -0.037, -lr * Pred Grad: -0.016, New P: -0.337
iter 20 loss: 0.348
Actual params: [ 0.7424, -0.3369]
-Original Grad: 0.034, -lr * Pred Grad: 0.005, New P: 0.748
-Original Grad: -0.007, -lr * Pred Grad: -0.031, New P: -0.368
Target params: [1.1812, 0.2779]
iter 0 loss: 0.601
Actual params: [0.5941, 0.5941]
-Original Grad: -0.086, -lr * Pred Grad: -0.042, New P: 0.553
-Original Grad: -0.091, -lr * Pred Grad: -0.044, New P: 0.550
iter 1 loss: 0.654
Actual params: [0.5525, 0.5498]
-Original Grad: -0.063, -lr * Pred Grad: -0.071, New P: 0.481
-Original Grad: 0.039, -lr * Pred Grad: -0.057, New P: 0.493
iter 2 loss: 0.716
Actual params: [0.4812, 0.4926]
-Original Grad: 0.020, -lr * Pred Grad: -0.070, New P: 0.411
-Original Grad: 0.013, -lr * Pred Grad: -0.060, New P: 0.433
iter 3 loss: 0.758
Actual params: [0.4114, 0.433 ]
-Original Grad: 0.088, -lr * Pred Grad: -0.058, New P: 0.353
-Original Grad: 0.083, -lr * Pred Grad: -0.026, New P: 0.407
iter 4 loss: 0.776
Actual params: [0.353, 0.407]
-Original Grad: -0.096, -lr * Pred Grad: -0.047, New P: 0.306
-Original Grad: 0.004, -lr * Pred Grad: 0.020, New P: 0.427
iter 5 loss: 0.775
Actual params: [0.3058, 0.4267]
-Original Grad: -0.183, -lr * Pred Grad: -0.062, New P: 0.244
-Original Grad: -0.044, -lr * Pred Grad: -0.031, New P: 0.396
iter 6 loss: 0.793
Actual params: [0.2438, 0.3959]
-Original Grad: -0.022, -lr * Pred Grad: -0.063, New P: 0.180
-Original Grad: 0.013, -lr * Pred Grad: -0.020, New P: 0.376
iter 7 loss: 0.805
Actual params: [0.1805, 0.3758]
-Original Grad: -0.012, -lr * Pred Grad: -0.056, New P: 0.124
-Original Grad: 0.078, -lr * Pred Grad: 0.028, New P: 0.404
iter 8 loss: 0.802
Actual params: [0.124, 0.404]
-Original Grad: 0.070, -lr * Pred Grad: -0.028, New P: 0.096
-Original Grad: 0.111, -lr * Pred Grad: 0.066, New P: 0.470
iter 9 loss: 0.788
Actual params: [0.096 , 0.4699]
-Original Grad: 0.053, -lr * Pred Grad: 0.004, New P: 0.100
-Original Grad: 0.010, -lr * Pred Grad: 0.041, New P: 0.511
iter 10 loss: 0.776
Actual params: [0.1005, 0.5111]
-Original Grad: -0.101, -lr * Pred Grad: -0.029, New P: 0.071
-Original Grad: -0.047, -lr * Pred Grad: -0.027, New P: 0.484
iter 11 loss: 0.786
Actual params: [0.0714, 0.4845]
-Original Grad: -0.015, -lr * Pred Grad: -0.047, New P: 0.025
-Original Grad: 0.023, -lr * Pred Grad: -0.008, New P: 0.476
iter 12 loss: 0.791
Actual params: [0.0248, 0.4764]
-Original Grad: 0.027, -lr * Pred Grad: -0.027, New P: -0.003
-Original Grad: 0.051, -lr * Pred Grad: 0.030, New P: 0.506
iter 13 loss: 0.785
Actual params: [-0.0026,  0.5062]
-Original Grad: 0.057, -lr * Pred Grad: 0.010, New P: 0.007
-Original Grad: 0.086, -lr * Pred Grad: 0.058, New P: 0.564
iter 14 loss: 0.771
Actual params: [0.0072, 0.5641]
-Original Grad: 0.053, -lr * Pred Grad: 0.038, New P: 0.045
-Original Grad: 0.046, -lr * Pred Grad: 0.046, New P: 0.611
iter 15 loss: 0.760
Actual params: [0.0452, 0.6106]
-Original Grad: 0.051, -lr * Pred Grad: 0.038, New P: 0.083
-Original Grad: 0.057, -lr * Pred Grad: 0.051, New P: 0.661
iter 16 loss: 0.748
Actual params: [0.0832, 0.6613]
-Original Grad: -0.011, -lr * Pred Grad: -0.003, New P: 0.080
-Original Grad: -0.002, -lr * Pred Grad: -0.009, New P: 0.652
iter 17 loss: 0.750
Actual params: [0.0798, 0.6519]
-Original Grad: 0.043, -lr * Pred Grad: 0.010, New P: 0.090
-Original Grad: 0.048, -lr * Pred Grad: 0.037, New P: 0.689
iter 18 loss: 0.744
Actual params: [0.09  , 0.6888]
-Original Grad: 0.014, -lr * Pred Grad: 0.001, New P: 0.091
-Original Grad: -0.013, -lr * Pred Grad: -0.021, New P: 0.668
iter 19 loss: 0.747
Actual params: [0.091 , 0.6677]
-Original Grad: -0.013, -lr * Pred Grad: -0.019, New P: 0.072
-Original Grad: 0.007, -lr * Pred Grad: 0.002, New P: 0.670
iter 20 loss: 0.748
Actual params: [0.0722, 0.67  ]
-Original Grad: -0.028, -lr * Pred Grad: -0.037, New P: 0.036
-Original Grad: -0.008, -lr * Pred Grad: -0.022, New P: 0.648
Target params: [1.1812, 0.2779]
iter 0 loss: 0.756
Actual params: [0.5941, 0.5941]
-Original Grad: -0.011, -lr * Pred Grad: 0.026, New P: 0.620
-Original Grad: -0.042, -lr * Pred Grad: -0.006, New P: 0.588
iter 1 loss: 0.757
Actual params: [0.6196, 0.5877]
-Original Grad: -0.182, -lr * Pred Grad: -0.056, New P: 0.564
-Original Grad: -0.043, -lr * Pred Grad: -0.060, New P: 0.527
iter 2 loss: 0.714
Actual params: [0.5638, 0.5272]
-Original Grad: 0.009, -lr * Pred Grad: -0.074, New P: 0.490
-Original Grad: -0.020, -lr * Pred Grad: -0.068, New P: 0.459
iter 3 loss: 0.679
Actual params: [0.4901, 0.4592]
-Original Grad: -0.060, -lr * Pred Grad: -0.070, New P: 0.420
-Original Grad: 0.040, -lr * Pred Grad: -0.061, New P: 0.398
iter 4 loss: 0.655
Actual params: [0.4202, 0.3985]
-Original Grad: 0.049, -lr * Pred Grad: -0.063, New P: 0.357
-Original Grad: -0.057, -lr * Pred Grad: -0.049, New P: 0.350
iter 5 loss: 0.633
Actual params: [0.3573, 0.3496]
-Original Grad: -0.020, -lr * Pred Grad: -0.045, New P: 0.313
-Original Grad: -0.142, -lr * Pred Grad: -0.060, New P: 0.289
iter 6 loss: 0.604
Actual params: [0.3127, 0.2891]
-Original Grad: 0.006, -lr * Pred Grad: -0.017, New P: 0.296
-Original Grad: -0.073, -lr * Pred Grad: -0.063, New P: 0.226
iter 7 loss: 0.579
Actual params: [0.2958, 0.2257]
-Original Grad: 0.026, -lr * Pred Grad: 0.014, New P: 0.310
-Original Grad: -0.007, -lr * Pred Grad: -0.058, New P: 0.167
iter 8 loss: 0.556
Actual params: [0.3102, 0.1673]
-Original Grad: 0.051, -lr * Pred Grad: 0.029, New P: 0.340
-Original Grad: -0.070, -lr * Pred Grad: -0.055, New P: 0.112
iter 9 loss: 0.544
Actual params: [0.3396, 0.1124]
-Original Grad: -0.012, -lr * Pred Grad: -0.005, New P: 0.335
-Original Grad: -0.048, -lr * Pred Grad: -0.052, New P: 0.060
iter 10 loss: 0.533
Actual params: [0.3346, 0.06  ]
-Original Grad: 0.057, -lr * Pred Grad: 0.028, New P: 0.363
-Original Grad: -0.036, -lr * Pred Grad: -0.047, New P: 0.013
iter 11 loss: 0.519
Actual params: [0.3626, 0.0126]
-Original Grad: 0.029, -lr * Pred Grad: 0.018, New P: 0.381
-Original Grad: -0.029, -lr * Pred Grad: -0.041, New P: -0.029
iter 12 loss: 0.510
Actual params: [ 0.3807, -0.0288]
-Original Grad: -0.032, -lr * Pred Grad: -0.019, New P: 0.362
-Original Grad: -0.062, -lr * Pred Grad: -0.044, New P: -0.073
iter 13 loss: 0.511
Actual params: [ 0.3616, -0.0733]
-Original Grad: 0.030, -lr * Pred Grad: -0.003, New P: 0.358
-Original Grad: -0.060, -lr * Pred Grad: -0.049, New P: -0.123
iter 14 loss: 0.509
Actual params: [ 0.3581, -0.1227]
-Original Grad: 0.018, -lr * Pred Grad: 0.005, New P: 0.364
-Original Grad: 0.019, -lr * Pred Grad: -0.040, New P: -0.163
iter 15 loss: 0.501
Actual params: [ 0.3635, -0.1625]
-Original Grad: 0.024, -lr * Pred Grad: 0.011, New P: 0.375
-Original Grad: -0.026, -lr * Pred Grad: -0.032, New P: -0.195
iter 16 loss: 0.495
Actual params: [ 0.3745, -0.1945]
-Original Grad: 0.077, -lr * Pred Grad: 0.042, New P: 0.416
-Original Grad: -0.049, -lr * Pred Grad: -0.037, New P: -0.232
iter 17 loss: 0.487
Actual params: [ 0.4161, -0.2315]
-Original Grad: -0.039, -lr * Pred Grad: -0.012, New P: 0.404
-Original Grad: -0.036, -lr * Pred Grad: -0.041, New P: -0.273
iter 18 loss: 0.485
Actual params: [ 0.4042, -0.2728]
-Original Grad: 0.059, -lr * Pred Grad: 0.016, New P: 0.421
-Original Grad: 0.017, -lr * Pred Grad: -0.034, New P: -0.307
iter 19 loss: 0.482
Actual params: [ 0.4206, -0.307 ]
-Original Grad: 0.225, -lr * Pred Grad: 0.063, New P: 0.484
-Original Grad: 0.018, -lr * Pred Grad: -0.022, New P: -0.329
iter 20 loss: 0.465
Actual params: [ 0.4839, -0.3287]
-Original Grad: -0.083, -lr * Pred Grad: 0.042, New P: 0.526
-Original Grad: -0.060, -lr * Pred Grad: -0.030, New P: -0.359
Target params: [1.1812, 0.2779]
iter 0 loss: 0.720
Actual params: [0.5941, 0.5941]
-Original Grad: -0.043, -lr * Pred Grad: -0.007, New P: 0.587
-Original Grad: -0.012, -lr * Pred Grad: 0.025, New P: 0.619
iter 1 loss: 0.718
Actual params: [0.5867, 0.6186]
-Original Grad: -0.097, -lr * Pred Grad: -0.065, New P: 0.521
-Original Grad: 0.024, -lr * Pred Grad: -0.021, New P: 0.597
iter 2 loss: 0.755
Actual params: [0.5214, 0.5975]
-Original Grad: 0.014, -lr * Pred Grad: -0.071, New P: 0.450
-Original Grad: 0.001, -lr * Pred Grad: -0.049, New P: 0.548
iter 3 loss: 0.789
Actual params: [0.4504, 0.5482]
-Original Grad: 0.048, -lr * Pred Grad: -0.063, New P: 0.387
-Original Grad: 0.050, -lr * Pred Grad: -0.015, New P: 0.533
iter 4 loss: 0.811
Actual params: [0.3869, 0.5329]
-Original Grad: -0.046, -lr * Pred Grad: -0.049, New P: 0.338
-Original Grad: 0.012, -lr * Pred Grad: 0.022, New P: 0.555
iter 5 loss: 0.820
Actual params: [0.3381, 0.555 ]
-Original Grad: 0.011, -lr * Pred Grad: -0.021, New P: 0.318
-Original Grad: 0.037, -lr * Pred Grad: 0.020, New P: 0.575
iter 6 loss: 0.822
Actual params: [0.3175, 0.5749]
-Original Grad: 0.010, -lr * Pred Grad: -0.001, New P: 0.316
-Original Grad: -0.001, -lr * Pred Grad: 0.001, New P: 0.576
iter 7 loss: 0.822
Actual params: [0.3161, 0.5759]
-Original Grad: -0.055, -lr * Pred Grad: -0.035, New P: 0.281
-Original Grad: -0.024, -lr * Pred Grad: -0.023, New P: 0.552
iter 8 loss: 0.830
Actual params: [0.2811, 0.5525]
-Original Grad: -0.008, -lr * Pred Grad: -0.037, New P: 0.245
-Original Grad: 0.037, -lr * Pred Grad: 0.004, New P: 0.557
iter 9 loss: 0.835
Actual params: [0.2446, 0.5566]
-Original Grad: 0.028, -lr * Pred Grad: -0.015, New P: 0.229
-Original Grad: 0.021, -lr * Pred Grad: 0.014, New P: 0.571
iter 10 loss: 0.836
Actual params: [0.2295, 0.5706]
-Original Grad: 0.022, -lr * Pred Grad: 0.005, New P: 0.234
-Original Grad: -0.011, -lr * Pred Grad: -0.009, New P: 0.561
iter 11 loss: 0.836
Actual params: [0.2344, 0.5612]
-Original Grad: -0.008, -lr * Pred Grad: -0.008, New P: 0.226
-Original Grad: -0.010, -lr * Pred Grad: -0.018, New P: 0.543
iter 12 loss: 0.839
Actual params: [0.226 , 0.5435]
-Original Grad: 0.055, -lr * Pred Grad: 0.008, New P: 0.234
-Original Grad: -0.024, -lr * Pred Grad: -0.025, New P: 0.518
iter 13 loss: 0.839
Actual params: [0.2341, 0.518 ]
-Original Grad: 0.025, -lr * Pred Grad: 0.015, New P: 0.249
-Original Grad: -0.010, -lr * Pred Grad: -0.024, New P: 0.494
iter 14 loss: 0.838
Actual params: [0.2492, 0.4944]
-Original Grad: 0.019, -lr * Pred Grad: 0.006, New P: 0.255
-Original Grad: -0.040, -lr * Pred Grad: -0.029, New P: 0.465
iter 15 loss: 0.839
Actual params: [0.2549, 0.4652]
-Original Grad: -0.070, -lr * Pred Grad: -0.031, New P: 0.223
-Original Grad: -0.014, -lr * Pred Grad: -0.029, New P: 0.437
iter 16 loss: 0.844
Actual params: [0.2235, 0.4366]
-Original Grad: 0.154, -lr * Pred Grad: 0.012, New P: 0.236
-Original Grad: 0.013, -lr * Pred Grad: -0.016, New P: 0.420
iter 17 loss: 0.844
Actual params: [0.2358, 0.4203]
-Original Grad: 0.076, -lr * Pred Grad: 0.056, New P: 0.292
-Original Grad: -0.040, -lr * Pred Grad: -0.022, New P: 0.399
iter 18 loss: 0.838
Actual params: [0.2917, 0.3988]
-Original Grad: -0.046, -lr * Pred Grad: 0.002, New P: 0.294
-Original Grad: -0.044, -lr * Pred Grad: -0.032, New P: 0.367
iter 19 loss: 0.839
Actual params: [0.2939, 0.3666]
-Original Grad: -0.052, -lr * Pred Grad: -0.041, New P: 0.253
-Original Grad: -0.007, -lr * Pred Grad: -0.031, New P: 0.336
iter 20 loss: 0.846
Actual params: [0.2529, 0.3359]
-Original Grad: 0.006, -lr * Pred Grad: -0.039, New P: 0.213
-Original Grad: -0.016, -lr * Pred Grad: -0.025, New P: 0.311
Target params: [1.1812, 0.2779]
iter 0 loss: 0.694
Actual params: [0.5941, 0.5941]
-Original Grad: -0.039, -lr * Pred Grad: -0.003, New P: 0.591
-Original Grad: 0.103, -lr * Pred Grad: 0.065, New P: 0.659
iter 1 loss: 0.766
Actual params: [0.5912, 0.6593]
-Original Grad: 0.012, -lr * Pred Grad: -0.044, New P: 0.548
-Original Grad: 0.142, -lr * Pred Grad: 0.083, New P: 0.743
iter 2 loss: 0.864
Actual params: [0.5477, 0.7426]
-Original Grad: 0.003, -lr * Pred Grad: -0.057, New P: 0.490
-Original Grad: -0.027, -lr * Pred Grad: 0.072, New P: 0.814
iter 3 loss: 0.937
Actual params: [0.4904, 0.8144]
-Original Grad: -0.095, -lr * Pred Grad: -0.063, New P: 0.428
-Original Grad: -0.053, -lr * Pred Grad: -0.032, New P: 0.783
iter 4 loss: 0.812
Actual params: [0.4275, 0.7826]
-Original Grad: -0.029, -lr * Pred Grad: -0.061, New P: 0.367
-Original Grad: -0.140, -lr * Pred Grad: -0.055, New P: 0.728
iter 5 loss: 0.674
Actual params: [0.3668, 0.7281]
-Original Grad: -0.077, -lr * Pred Grad: -0.060, New P: 0.307
-Original Grad: 0.010, -lr * Pred Grad: -0.048, New P: 0.680
iter 6 loss: 0.588
Actual params: [0.3069, 0.6802]
-Original Grad: -0.183, -lr * Pred Grad: -0.064, New P: 0.243
-Original Grad: -0.088, -lr * Pred Grad: -0.048, New P: 0.632
iter 7 loss: 0.527
Actual params: [0.2427, 0.6318]
-Original Grad: -0.049, -lr * Pred Grad: -0.065, New P: 0.178
-Original Grad: -0.030, -lr * Pred Grad: -0.046, New P: 0.586
iter 8 loss: 0.476
Actual params: [0.178 , 0.5858]
-Original Grad: 0.059, -lr * Pred Grad: -0.052, New P: 0.126
-Original Grad: 0.055, -lr * Pred Grad: -0.010, New P: 0.576
iter 9 loss: 0.453
Actual params: [0.1261, 0.5758]
-Original Grad: -0.054, -lr * Pred Grad: -0.041, New P: 0.085
-Original Grad: 0.051, -lr * Pred Grad: 0.031, New P: 0.607
iter 10 loss: 0.456
Actual params: [0.0848, 0.6069]
-Original Grad: 0.016, -lr * Pred Grad: -0.026, New P: 0.059
-Original Grad: -0.032, -lr * Pred Grad: -0.001, New P: 0.606
iter 11 loss: 0.446
Actual params: [0.0586, 0.6061]
-Original Grad: -0.007, -lr * Pred Grad: -0.017, New P: 0.042
-Original Grad: -0.068, -lr * Pred Grad: -0.039, New P: 0.567
iter 12 loss: 0.434
Actual params: [0.042 , 0.5668]
-Original Grad: 0.007, -lr * Pred Grad: -0.013, New P: 0.029
-Original Grad: 0.010, -lr * Pred Grad: -0.031, New P: 0.536
iter 13 loss: 0.431
Actual params: [0.0295, 0.5363]
-Original Grad: -0.019, -lr * Pred Grad: -0.024, New P: 0.006
-Original Grad: 0.067, -lr * Pred Grad: 0.015, New P: 0.551
iter 14 loss: 0.432
Actual params: [0.0059, 0.5512]
-Original Grad: -0.065, -lr * Pred Grad: -0.041, New P: -0.035
-Original Grad: 0.021, -lr * Pred Grad: 0.034, New P: 0.586
iter 15 loss: 0.438
Actual params: [-0.0353,  0.5856]
-Original Grad: -0.028, -lr * Pred Grad: -0.047, New P: -0.082
-Original Grad: 0.069, -lr * Pred Grad: 0.044, New P: 0.629
iter 16 loss: 0.448
Actual params: [-0.0825,  0.6294]
-Original Grad: 0.037, -lr * Pred Grad: -0.031, New P: -0.114
-Original Grad: 0.017, -lr * Pred Grad: 0.018, New P: 0.647
iter 17 loss: 0.451
Actual params: [-0.1136,  0.6474]
-Original Grad: -0.007, -lr * Pred Grad: -0.017, New P: -0.131
-Original Grad: -0.028, -lr * Pred Grad: -0.020, New P: 0.628
iter 18 loss: 0.445
Actual params: [-0.1308,  0.6277]
-Original Grad: 0.033, -lr * Pred Grad: -0.001, New P: -0.131
-Original Grad: -0.018, -lr * Pred Grad: -0.030, New P: 0.598
iter 19 loss: 0.440
Actual params: [-0.1313,  0.5977]
-Original Grad: 0.002, -lr * Pred Grad: -0.002, New P: -0.134
-Original Grad: 0.033, -lr * Pred Grad: -0.004, New P: 0.593
iter 20 loss: 0.440
Actual params: [-0.1338,  0.5934]
-Original Grad: 0.032, -lr * Pred Grad: -0.001, New P: -0.134
-Original Grad: -0.011, -lr * Pred Grad: -0.006, New P: 0.587
Target params: [1.1812, 0.2779]
iter 0 loss: 0.534
Actual params: [0.5941, 0.5941]
-Original Grad: 0.036, -lr * Pred Grad: 0.054, New P: 0.648
-Original Grad: -0.243, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.537
Actual params: [0.6485, 0.5312]
-Original Grad: 0.062, -lr * Pred Grad: 0.061, New P: 0.709
-Original Grad: -0.112, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.530
Actual params: [0.7094, 0.4506]
-Original Grad: -0.051, -lr * Pred Grad: -0.039, New P: 0.671
-Original Grad: -0.172, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.583
Actual params: [0.6707, 0.3672]
-Original Grad: 0.106, -lr * Pred Grad: 0.006, New P: 0.677
-Original Grad: -0.021, -lr * Pred Grad: -0.083, New P: 0.285
iter 4 loss: 0.588
Actual params: [0.6771, 0.2847]
-Original Grad: -0.025, -lr * Pred Grad: 0.015, New P: 0.692
-Original Grad: -0.031, -lr * Pred Grad: -0.076, New P: 0.209
iter 5 loss: 0.580
Actual params: [0.6917, 0.2086]
-Original Grad: 0.116, -lr * Pred Grad: 0.060, New P: 0.752
-Original Grad: -0.061, -lr * Pred Grad: -0.071, New P: 0.138
iter 6 loss: 0.550
Actual params: [0.7518, 0.1377]
-Original Grad: 0.067, -lr * Pred Grad: 0.065, New P: 0.817
-Original Grad: -0.062, -lr * Pred Grad: -0.069, New P: 0.069
iter 7 loss: 0.529
Actual params: [0.8166, 0.069 ]
-Original Grad: 0.040, -lr * Pred Grad: 0.041, New P: 0.858
-Original Grad: -0.003, -lr * Pred Grad: -0.065, New P: 0.004
iter 8 loss: 0.521
Actual params: [0.8578, 0.0037]
-Original Grad: -0.030, -lr * Pred Grad: -0.018, New P: 0.840
-Original Grad: -0.000, -lr * Pred Grad: -0.055, New P: -0.051
iter 9 loss: 0.529
Actual params: [ 0.84  , -0.0514]
-Original Grad: 0.174, -lr * Pred Grad: 0.052, New P: 0.892
-Original Grad: -0.024, -lr * Pred Grad: -0.045, New P: -0.096
iter 10 loss: 0.524
Actual params: [ 0.892 , -0.0963]
-Original Grad: 0.033, -lr * Pred Grad: 0.072, New P: 0.964
-Original Grad: 0.009, -lr * Pred Grad: -0.037, New P: -0.133
iter 11 loss: 0.519
Actual params: [ 0.9639, -0.1332]
-Original Grad: -0.019, -lr * Pred Grad: -0.016, New P: 0.948
-Original Grad: -0.045, -lr * Pred Grad: -0.044, New P: -0.177
iter 12 loss: 0.524
Actual params: [ 0.9476, -0.1768]
-Original Grad: -0.021, -lr * Pred Grad: -0.013, New P: 0.935
-Original Grad: -0.023, -lr * Pred Grad: -0.045, New P: -0.222
iter 13 loss: 0.530
Actual params: [ 0.9346, -0.2222]
-Original Grad: 0.057, -lr * Pred Grad: 0.016, New P: 0.950
-Original Grad: -0.017, -lr * Pred Grad: -0.040, New P: -0.263
iter 14 loss: 0.534
Actual params: [ 0.9504, -0.2627]
-Original Grad: -0.076, -lr * Pred Grad: -0.035, New P: 0.915
-Original Grad: -0.050, -lr * Pred Grad: -0.043, New P: -0.306
iter 15 loss: 0.536
Actual params: [ 0.9152, -0.3056]
-Original Grad: -0.009, -lr * Pred Grad: -0.033, New P: 0.882
-Original Grad: 0.002, -lr * Pred Grad: -0.033, New P: -0.339
iter 16 loss: 0.546
Actual params: [ 0.8823, -0.3389]
-Original Grad: -0.009, -lr * Pred Grad: -0.020, New P: 0.862
-Original Grad: 0.106, -lr * Pred Grad: 0.004, New P: -0.335
iter 17 loss: 0.549
Actual params: [ 0.8619, -0.3347]
-Original Grad: 0.010, -lr * Pred Grad: -0.006, New P: 0.856
-Original Grad: 0.012, -lr * Pred Grad: 0.026, New P: -0.308
iter 18 loss: 0.548
Actual params: [ 0.856 , -0.3082]
-Original Grad: 0.062, -lr * Pred Grad: 0.027, New P: 0.883
-Original Grad: 0.084, -lr * Pred Grad: 0.043, New P: -0.265
iter 19 loss: 0.540
Actual params: [ 0.8832, -0.2654]
-Original Grad: 0.053, -lr * Pred Grad: 0.045, New P: 0.928
-Original Grad: -0.045, -lr * Pred Grad: -0.009, New P: -0.275
iter 20 loss: 0.537
Actual params: [ 0.9279, -0.2746]
-Original Grad: 0.058, -lr * Pred Grad: 0.047, New P: 0.975
-Original Grad: 0.034, -lr * Pred Grad: -0.012, New P: -0.286
Target params: [1.1812, 0.2779]
iter 0 loss: 0.589
Actual params: [0.5941, 0.5941]
-Original Grad: 0.119, -lr * Pred Grad: 0.066, New P: 0.660
-Original Grad: -0.046, -lr * Pred Grad: -0.010, New P: 0.584
iter 1 loss: 0.632
Actual params: [0.6601, 0.5838]
-Original Grad: 0.076, -lr * Pred Grad: 0.083, New P: 0.743
-Original Grad: -0.132, -lr * Pred Grad: -0.067, New P: 0.517
iter 2 loss: 0.646
Actual params: [0.7428, 0.5168]
-Original Grad: 0.122, -lr * Pred Grad: 0.084, New P: 0.827
-Original Grad: -0.018, -lr * Pred Grad: -0.075, New P: 0.442
iter 3 loss: 0.639
Actual params: [0.827 , 0.4419]
-Original Grad: -0.114, -lr * Pred Grad: -0.017, New P: 0.810
-Original Grad: -0.042, -lr * Pred Grad: -0.070, New P: 0.372
iter 4 loss: 0.603
Actual params: [0.8098, 0.372 ]
-Original Grad: 0.108, -lr * Pred Grad: 0.038, New P: 0.848
-Original Grad: 0.036, -lr * Pred Grad: -0.064, New P: 0.308
iter 5 loss: 0.584
Actual params: [0.8478, 0.3079]
-Original Grad: -0.111, -lr * Pred Grad: -0.034, New P: 0.813
-Original Grad: -0.017, -lr * Pred Grad: -0.048, New P: 0.260
iter 6 loss: 0.557
Actual params: [0.8134, 0.2595]
-Original Grad: 0.089, -lr * Pred Grad: 0.019, New P: 0.833
-Original Grad: 0.006, -lr * Pred Grad: -0.025, New P: 0.235
iter 7 loss: 0.554
Actual params: [0.8327, 0.2346]
-Original Grad: 0.076, -lr * Pred Grad: 0.060, New P: 0.892
-Original Grad: 0.018, -lr * Pred Grad: 0.006, New P: 0.241
iter 8 loss: 0.562
Actual params: [0.8923, 0.2407]
-Original Grad: 0.031, -lr * Pred Grad: 0.024, New P: 0.916
-Original Grad: 0.009, -lr * Pred Grad: -0.000, New P: 0.241
iter 9 loss: 0.562
Actual params: [0.9159, 0.2405]
-Original Grad: -0.025, -lr * Pred Grad: -0.011, New P: 0.905
-Original Grad: 0.002, -lr * Pred Grad: -0.012, New P: 0.228
iter 10 loss: 0.559
Actual params: [0.9054, 0.2281]
-Original Grad: -0.019, -lr * Pred Grad: -0.033, New P: 0.873
-Original Grad: 0.001, -lr * Pred Grad: -0.017, New P: 0.211
iter 11 loss: 0.554
Actual params: [0.8726, 0.2111]
-Original Grad: 0.025, -lr * Pred Grad: -0.002, New P: 0.871
-Original Grad: 0.001, -lr * Pred Grad: -0.019, New P: 0.192
iter 12 loss: 0.548
Actual params: [0.8709, 0.192 ]
-Original Grad: -0.040, -lr * Pred Grad: -0.026, New P: 0.845
-Original Grad: 0.003, -lr * Pred Grad: -0.019, New P: 0.173
iter 13 loss: 0.540
Actual params: [0.8451, 0.1734]
-Original Grad: 0.001, -lr * Pred Grad: -0.019, New P: 0.826
-Original Grad: -0.025, -lr * Pred Grad: -0.029, New P: 0.145
iter 14 loss: 0.530
Actual params: [0.8264, 0.1446]
-Original Grad: -0.057, -lr * Pred Grad: -0.035, New P: 0.791
-Original Grad: -0.046, -lr * Pred Grad: -0.042, New P: 0.103
iter 15 loss: 0.517
Actual params: [0.7912, 0.1028]
-Original Grad: 0.067, -lr * Pred Grad: -0.002, New P: 0.789
-Original Grad: -0.005, -lr * Pred Grad: -0.040, New P: 0.062
iter 16 loss: 0.511
Actual params: [0.7889, 0.0624]
-Original Grad: 0.118, -lr * Pred Grad: 0.045, New P: 0.834
-Original Grad: -0.016, -lr * Pred Grad: -0.035, New P: 0.027
iter 17 loss: 0.516
Actual params: [0.8336, 0.0273]
-Original Grad: 0.020, -lr * Pred Grad: 0.061, New P: 0.894
-Original Grad: 0.012, -lr * Pred Grad: -0.024, New P: 0.004
iter 18 loss: 0.519
Actual params: [0.8944, 0.0037]
-Original Grad: 0.069, -lr * Pred Grad: 0.041, New P: 0.935
-Original Grad: -0.034, -lr * Pred Grad: -0.028, New P: -0.024
iter 19 loss: 0.517
Actual params: [ 0.9353, -0.0242]
-Original Grad: -0.178, -lr * Pred Grad: -0.039, New P: 0.897
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: -0.054
iter 20 loss: 0.516
Actual params: [ 0.8966, -0.0536]
-Original Grad: -0.063, -lr * Pred Grad: -0.055, New P: 0.842
-Original Grad: 0.016, -lr * Pred Grad: -0.020, New P: -0.073
Target params: [1.1812, 0.2779]
iter 0 loss: 0.423
Actual params: [0.5941, 0.5941]
-Original Grad: 0.164, -lr * Pred Grad: 0.067, New P: 0.661
-Original Grad: -0.117, -lr * Pred Grad: -0.053, New P: 0.541
iter 1 loss: 0.366
Actual params: [0.6612, 0.5407]
-Original Grad: 0.241, -lr * Pred Grad: 0.085, New P: 0.746
-Original Grad: -0.169, -lr * Pred Grad: -0.079, New P: 0.462
iter 2 loss: 0.318
Actual params: [0.7457, 0.4621]
-Original Grad: 0.095, -lr * Pred Grad: 0.087, New P: 0.833
-Original Grad: -0.102, -lr * Pred Grad: -0.082, New P: 0.380
iter 3 loss: 0.322
Actual params: [0.833 , 0.3797]
-Original Grad: 0.105, -lr * Pred Grad: 0.087, New P: 0.920
-Original Grad: -0.091, -lr * Pred Grad: -0.081, New P: 0.299
iter 4 loss: 0.318
Actual params: [0.9205, 0.2992]
-Original Grad: -0.018, -lr * Pred Grad: 0.080, New P: 1.000
-Original Grad: -0.070, -lr * Pred Grad: -0.075, New P: 0.224
iter 5 loss: 0.318
Actual params: [1.0003, 0.2244]
-Original Grad: 0.009, -lr * Pred Grad: -0.003, New P: 0.998
-Original Grad: -0.084, -lr * Pred Grad: -0.071, New P: 0.153
iter 6 loss: 0.322
Actual params: [0.9976, 0.1534]
-Original Grad: 0.013, -lr * Pred Grad: 0.040, New P: 1.038
-Original Grad: -0.012, -lr * Pred Grad: -0.068, New P: 0.085
iter 7 loss: 0.330
Actual params: [1.0377, 0.0851]
-Original Grad: -0.056, -lr * Pred Grad: -0.030, New P: 1.008
-Original Grad: 0.014, -lr * Pred Grad: -0.062, New P: 0.023
iter 8 loss: 0.330
Actual params: [1.0077, 0.0229]
-Original Grad: 0.019, -lr * Pred Grad: 0.000, New P: 1.008
-Original Grad: -0.057, -lr * Pred Grad: -0.056, New P: -0.033
iter 9 loss: 0.332
Actual params: [ 1.008 , -0.0329]
-Original Grad: -0.012, -lr * Pred Grad: -0.020, New P: 0.988
-Original Grad: -0.024, -lr * Pred Grad: -0.051, New P: -0.084
iter 10 loss: 0.332
Actual params: [ 0.9876, -0.0838]
-Original Grad: -0.050, -lr * Pred Grad: -0.038, New P: 0.949
-Original Grad: 0.001, -lr * Pred Grad: -0.042, New P: -0.125
iter 11 loss: 0.331
Actual params: [ 0.9495, -0.1255]
-Original Grad: -0.013, -lr * Pred Grad: -0.033, New P: 0.916
-Original Grad: -0.039, -lr * Pred Grad: -0.043, New P: -0.169
iter 12 loss: 0.332
Actual params: [ 0.9163, -0.1685]
-Original Grad: 0.025, -lr * Pred Grad: -0.004, New P: 0.912
-Original Grad: -0.059, -lr * Pred Grad: -0.051, New P: -0.220
iter 13 loss: 0.333
Actual params: [ 0.9119, -0.2199]
-Original Grad: 0.009, -lr * Pred Grad: 0.006, New P: 0.917
-Original Grad: -0.003, -lr * Pred Grad: -0.042, New P: -0.262
iter 14 loss: 0.335
Actual params: [ 0.9175, -0.2619]
-Original Grad: 0.074, -lr * Pred Grad: 0.037, New P: 0.954
-Original Grad: -0.005, -lr * Pred Grad: -0.027, New P: -0.289
iter 15 loss: 0.337
Actual params: [ 0.9543, -0.2887]
-Original Grad: 0.042, -lr * Pred Grad: 0.042, New P: 0.996
-Original Grad: -0.037, -lr * Pred Grad: -0.032, New P: -0.321
iter 16 loss: 0.339
Actual params: [ 0.996 , -0.3205]
-Original Grad: 0.010, -lr * Pred Grad: 0.008, New P: 1.004
-Original Grad: -0.109, -lr * Pred Grad: -0.050, New P: -0.371
iter 17 loss: 0.346
Actual params: [ 1.0037, -0.371 ]
-Original Grad: 0.029, -lr * Pred Grad: 0.014, New P: 1.018
-Original Grad: -0.120, -lr * Pred Grad: -0.061, New P: -0.432
iter 18 loss: 0.351
Actual params: [ 1.0175, -0.432 ]
-Original Grad: -0.073, -lr * Pred Grad: -0.033, New P: 0.984
-Original Grad: -0.037, -lr * Pred Grad: -0.062, New P: -0.494
iter 19 loss: 0.355
Actual params: [ 0.9845, -0.4941]
-Original Grad: -0.012, -lr * Pred Grad: -0.038, New P: 0.946
-Original Grad: -0.099, -lr * Pred Grad: -0.063, New P: -0.557
iter 20 loss: 0.359
Actual params: [ 0.9464, -0.557 ]
-Original Grad: -0.092, -lr * Pred Grad: -0.047, New P: 0.900
-Original Grad: 0.064, -lr * Pred Grad: -0.045, New P: -0.602
Target params: [1.1812, 0.2779]
iter 0 loss: 0.561
Actual params: [0.5941, 0.5941]
-Original Grad: 0.097, -lr * Pred Grad: 0.065, New P: 0.659
-Original Grad: -0.120, -lr * Pred Grad: -0.054, New P: 0.540
iter 1 loss: 0.515
Actual params: [0.659, 0.54 ]
-Original Grad: 0.188, -lr * Pred Grad: 0.084, New P: 0.743
-Original Grad: -0.079, -lr * Pred Grad: -0.077, New P: 0.463
iter 2 loss: 0.466
Actual params: [0.7425, 0.4634]
-Original Grad: 0.099, -lr * Pred Grad: 0.086, New P: 0.829
-Original Grad: -0.121, -lr * Pred Grad: -0.080, New P: 0.384
iter 3 loss: 0.417
Actual params: [0.8286, 0.3839]
-Original Grad: -0.024, -lr * Pred Grad: 0.058, New P: 0.887
-Original Grad: -0.114, -lr * Pred Grad: -0.077, New P: 0.307
iter 4 loss: 0.377
Actual params: [0.8871, 0.3068]
-Original Grad: 0.066, -lr * Pred Grad: 0.041, New P: 0.928
-Original Grad: -0.002, -lr * Pred Grad: -0.071, New P: 0.236
iter 5 loss: 0.360
Actual params: [0.928, 0.236]
-Original Grad: 0.144, -lr * Pred Grad: 0.076, New P: 1.004
-Original Grad: -0.002, -lr * Pred Grad: -0.068, New P: 0.168
iter 6 loss: 0.344
Actual params: [1.0039, 0.1684]
-Original Grad: 0.074, -lr * Pred Grad: 0.080, New P: 1.084
-Original Grad: -0.017, -lr * Pred Grad: -0.061, New P: 0.107
iter 7 loss: 0.328
Actual params: [1.0843, 0.1074]
-Original Grad: 0.031, -lr * Pred Grad: 0.054, New P: 1.138
-Original Grad: -0.022, -lr * Pred Grad: -0.051, New P: 0.057
iter 8 loss: 0.319
Actual params: [1.1379, 0.0566]
-Original Grad: 0.099, -lr * Pred Grad: 0.071, New P: 1.209
-Original Grad: 0.006, -lr * Pred Grad: -0.040, New P: 0.017
iter 9 loss: 0.310
Actual params: [1.2093, 0.017 ]
-Original Grad: 0.103, -lr * Pred Grad: 0.073, New P: 1.282
-Original Grad: -0.061, -lr * Pred Grad: -0.045, New P: -0.028
iter 10 loss: 0.299
Actual params: [ 1.2823, -0.0278]
-Original Grad: 0.063, -lr * Pred Grad: 0.068, New P: 1.351
-Original Grad: -0.052, -lr * Pred Grad: -0.049, New P: -0.077
iter 11 loss: 0.298
Actual params: [ 1.3506, -0.0773]
-Original Grad: 0.109, -lr * Pred Grad: 0.075, New P: 1.426
-Original Grad: -0.056, -lr * Pred Grad: -0.052, New P: -0.130
iter 12 loss: 0.299
Actual params: [ 1.4256, -0.1296]
-Original Grad: -0.078, -lr * Pred Grad: -0.022, New P: 1.404
-Original Grad: 0.031, -lr * Pred Grad: -0.033, New P: -0.162
iter 13 loss: 0.301
Actual params: [ 1.4038, -0.1624]
-Original Grad: 0.005, -lr * Pred Grad: -0.013, New P: 1.391
-Original Grad: -0.041, -lr * Pred Grad: -0.028, New P: -0.190
iter 14 loss: 0.303
Actual params: [ 1.391 , -0.1903]
-Original Grad: 0.037, -lr * Pred Grad: 0.010, New P: 1.401
-Original Grad: -0.008, -lr * Pred Grad: -0.029, New P: -0.219
iter 15 loss: 0.305
Actual params: [ 1.4008, -0.2191]
-Original Grad: -0.035, -lr * Pred Grad: -0.027, New P: 1.374
-Original Grad: 0.000, -lr * Pred Grad: -0.025, New P: -0.245
iter 16 loss: 0.307
Actual params: [ 1.3736, -0.2445]
-Original Grad: -0.027, -lr * Pred Grad: -0.033, New P: 1.341
-Original Grad: 0.049, -lr * Pred Grad: -0.003, New P: -0.248
iter 17 loss: 0.309
Actual params: [ 1.3407, -0.2477]
-Original Grad: 0.021, -lr * Pred Grad: -0.013, New P: 1.328
-Original Grad: -0.037, -lr * Pred Grad: -0.014, New P: -0.261
iter 18 loss: 0.310
Actual params: [ 1.328 , -0.2614]
-Original Grad: 0.029, -lr * Pred Grad: 0.011, New P: 1.339
-Original Grad: -0.009, -lr * Pred Grad: -0.028, New P: -0.290
iter 19 loss: 0.311
Actual params: [ 1.3393, -0.2897]
-Original Grad: 0.135, -lr * Pred Grad: 0.056, New P: 1.395
-Original Grad: -0.048, -lr * Pred Grad: -0.039, New P: -0.329
iter 20 loss: 0.312
Actual params: [ 1.3951, -0.3289]
-Original Grad: -0.036, -lr * Pred Grad: 0.017, New P: 1.412
-Original Grad: 0.009, -lr * Pred Grad: -0.037, New P: -0.366
Target params: [1.1812, 0.2779]
iter 0 loss: 0.763
Actual params: [0.5941, 0.5941]
-Original Grad: -0.095, -lr * Pred Grad: -0.046, New P: 0.548
-Original Grad: -0.207, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.628
Actual params: [0.5482, 0.5318]
-Original Grad: -0.046, -lr * Pred Grad: -0.071, New P: 0.477
-Original Grad: -0.068, -lr * Pred Grad: -0.080, New P: 0.452
iter 2 loss: 0.510
Actual params: [0.4775, 0.4517]
-Original Grad: 0.020, -lr * Pred Grad: -0.069, New P: 0.408
-Original Grad: -0.132, -lr * Pred Grad: -0.083, New P: 0.369
iter 3 loss: 0.464
Actual params: [0.4084, 0.369 ]
-Original Grad: -0.049, -lr * Pred Grad: -0.067, New P: 0.342
-Original Grad: 0.105, -lr * Pred Grad: -0.075, New P: 0.294
iter 4 loss: 0.452
Actual params: [0.3416, 0.294 ]
-Original Grad: -0.038, -lr * Pred Grad: -0.063, New P: 0.279
-Original Grad: -0.122, -lr * Pred Grad: -0.070, New P: 0.224
iter 5 loss: 0.444
Actual params: [0.2787, 0.2238]
-Original Grad: 0.000, -lr * Pred Grad: -0.052, New P: 0.227
-Original Grad: -0.095, -lr * Pred Grad: -0.068, New P: 0.155
iter 6 loss: 0.437
Actual params: [0.2266, 0.1555]
-Original Grad: 0.007, -lr * Pred Grad: -0.032, New P: 0.194
-Original Grad: -0.067, -lr * Pred Grad: -0.067, New P: 0.089
iter 7 loss: 0.430
Actual params: [0.1943, 0.0888]
-Original Grad: -0.014, -lr * Pred Grad: -0.016, New P: 0.179
-Original Grad: -0.047, -lr * Pred Grad: -0.065, New P: 0.024
iter 8 loss: 0.424
Actual params: [0.1787, 0.0241]
-Original Grad: 0.000, -lr * Pred Grad: -0.013, New P: 0.166
-Original Grad: -0.129, -lr * Pred Grad: -0.065, New P: -0.041
iter 9 loss: 0.419
Actual params: [ 0.1659, -0.0408]
-Original Grad: -0.065, -lr * Pred Grad: -0.041, New P: 0.125
-Original Grad: -0.000, -lr * Pred Grad: -0.062, New P: -0.102
iter 10 loss: 0.411
Actual params: [ 0.1245, -0.1023]
-Original Grad: 0.025, -lr * Pred Grad: -0.032, New P: 0.093
-Original Grad: -0.150, -lr * Pred Grad: -0.064, New P: -0.166
iter 11 loss: 0.403
Actual params: [ 0.093, -0.166]
-Original Grad: -0.039, -lr * Pred Grad: -0.031, New P: 0.062
-Original Grad: -0.129, -lr * Pred Grad: -0.065, New P: -0.231
iter 12 loss: 0.388
Actual params: [ 0.0617, -0.2309]
-Original Grad: -0.074, -lr * Pred Grad: -0.046, New P: 0.016
-Original Grad: -0.047, -lr * Pred Grad: -0.064, New P: -0.295
iter 13 loss: 0.381
Actual params: [ 0.016 , -0.2954]
-Original Grad: -0.086, -lr * Pred Grad: -0.057, New P: -0.041
-Original Grad: -0.058, -lr * Pred Grad: -0.063, New P: -0.358
iter 14 loss: 0.372
Actual params: [-0.0406, -0.3582]
-Original Grad: -0.057, -lr * Pred Grad: -0.060, New P: -0.100
-Original Grad: -0.064, -lr * Pred Grad: -0.061, New P: -0.420
iter 15 loss: 0.369
Actual params: [-0.1004, -0.4197]
-Original Grad: -0.081, -lr * Pred Grad: -0.061, New P: -0.161
-Original Grad: -0.074, -lr * Pred Grad: -0.061, New P: -0.481
iter 16 loss: 0.366
Actual params: [-0.1614, -0.4811]
-Original Grad: -0.064, -lr * Pred Grad: -0.061, New P: -0.222
-Original Grad: 0.037, -lr * Pred Grad: -0.048, New P: -0.529
iter 17 loss: 0.368
Actual params: [-0.2222, -0.5292]
-Original Grad: -0.012, -lr * Pred Grad: -0.052, New P: -0.274
-Original Grad: -0.091, -lr * Pred Grad: -0.052, New P: -0.581
iter 18 loss: 0.370
Actual params: [-0.2742, -0.5813]
-Original Grad: -0.041, -lr * Pred Grad: -0.043, New P: -0.317
-Original Grad: 0.056, -lr * Pred Grad: -0.036, New P: -0.617
iter 19 loss: 0.371
Actual params: [-0.3167, -0.6172]
-Original Grad: -0.013, -lr * Pred Grad: -0.036, New P: -0.352
-Original Grad: -0.033, -lr * Pred Grad: -0.034, New P: -0.652
iter 20 loss: 0.370
Actual params: [-0.3524, -0.6516]
-Original Grad: -0.023, -lr * Pred Grad: -0.034, New P: -0.386
-Original Grad: 0.150, -lr * Pred Grad: -0.020, New P: -0.672
Target params: [1.1812, 0.2779]
iter 0 loss: 0.546
Actual params: [0.5941, 0.5941]
-Original Grad: -0.208, -lr * Pred Grad: -0.062, New P: 0.532
-Original Grad: -0.512, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.555
Actual params: [0.5318, 0.5313]
-Original Grad: 0.132, -lr * Pred Grad: -0.068, New P: 0.464
-Original Grad: -0.257, -lr * Pred Grad: -0.081, New P: 0.451
iter 2 loss: 0.567
Actual params: [0.4636, 0.4507]
-Original Grad: 0.023, -lr * Pred Grad: -0.063, New P: 0.400
-Original Grad: -0.179, -lr * Pred Grad: -0.084, New P: 0.367
iter 3 loss: 0.577
Actual params: [0.4005, 0.3671]
-Original Grad: 0.063, -lr * Pred Grad: -0.037, New P: 0.363
-Original Grad: -0.141, -lr * Pred Grad: -0.084, New P: 0.283
iter 4 loss: 0.575
Actual params: [0.3632, 0.2829]
-Original Grad: 0.109, -lr * Pred Grad: 0.018, New P: 0.381
-Original Grad: -0.100, -lr * Pred Grad: -0.084, New P: 0.199
iter 5 loss: 0.555
Actual params: [0.3811, 0.1987]
-Original Grad: 0.228, -lr * Pred Grad: 0.063, New P: 0.445
-Original Grad: -0.190, -lr * Pred Grad: -0.084, New P: 0.114
iter 6 loss: 0.522
Actual params: [0.4445, 0.1144]
-Original Grad: 0.093, -lr * Pred Grad: 0.081, New P: 0.526
-Original Grad: -0.103, -lr * Pred Grad: -0.084, New P: 0.030
iter 7 loss: 0.495
Actual params: [0.5259, 0.0304]
-Original Grad: 0.047, -lr * Pred Grad: 0.081, New P: 0.607
-Original Grad: -0.124, -lr * Pred Grad: -0.084, New P: -0.053
iter 8 loss: 0.475
Actual params: [ 0.607 , -0.0533]
-Original Grad: -0.034, -lr * Pred Grad: -0.005, New P: 0.602
-Original Grad: -0.124, -lr * Pred Grad: -0.083, New P: -0.136
iter 9 loss: 0.471
Actual params: [ 0.6018, -0.1362]
-Original Grad: -0.018, -lr * Pred Grad: 0.002, New P: 0.604
-Original Grad: -0.064, -lr * Pred Grad: -0.079, New P: -0.215
iter 10 loss: 0.469
Actual params: [ 0.6041, -0.2152]
-Original Grad: 0.100, -lr * Pred Grad: 0.047, New P: 0.651
-Original Grad: -0.127, -lr * Pred Grad: -0.076, New P: -0.292
iter 11 loss: 0.463
Actual params: [ 0.6512, -0.2916]
-Original Grad: 0.012, -lr * Pred Grad: 0.019, New P: 0.671
-Original Grad: -0.034, -lr * Pred Grad: -0.071, New P: -0.363
iter 12 loss: 0.461
Actual params: [ 0.6706, -0.3627]
-Original Grad: 0.040, -lr * Pred Grad: 0.032, New P: 0.703
-Original Grad: 0.028, -lr * Pred Grad: -0.068, New P: -0.431
iter 13 loss: 0.459
Actual params: [ 0.703 , -0.4309]
-Original Grad: -0.054, -lr * Pred Grad: -0.034, New P: 0.669
-Original Grad: 0.016, -lr * Pred Grad: -0.059, New P: -0.490
iter 14 loss: 0.459
Actual params: [ 0.6686, -0.4898]
-Original Grad: 0.002, -lr * Pred Grad: -0.027, New P: 0.641
-Original Grad: 0.084, -lr * Pred Grad: -0.040, New P: -0.529
iter 15 loss: 0.455
Actual params: [ 0.6412, -0.5295]
-Original Grad: 0.032, -lr * Pred Grad: 0.009, New P: 0.650
-Original Grad: 0.182, -lr * Pred Grad: -0.030, New P: -0.559
iter 16 loss: 0.448
Actual params: [ 0.6499, -0.5594]
-Original Grad: -0.031, -lr * Pred Grad: -0.020, New P: 0.630
-Original Grad: -0.026, -lr * Pred Grad: -0.025, New P: -0.585
iter 17 loss: 0.416
Actual params: [ 0.6296, -0.5848]
-Original Grad: 0.033, -lr * Pred Grad: 0.005, New P: 0.634
-Original Grad: 0.092, -lr * Pred Grad: -0.025, New P: -0.609
iter 18 loss: 0.326
Actual params: [ 0.6343, -0.6095]
-Original Grad: 0.007, -lr * Pred Grad: -0.002, New P: 0.633
-Original Grad: 0.121, -lr * Pred Grad: -0.022, New P: -0.632
iter 19 loss: 0.398
Actual params: [ 0.6326, -0.6317]
-Original Grad: 0.023, -lr * Pred Grad: 0.006, New P: 0.639
-Original Grad: 0.074, -lr * Pred Grad: -0.020, New P: -0.652
iter 20 loss: 0.403
Actual params: [ 0.639 , -0.6517]
-Original Grad: 0.112, -lr * Pred Grad: 0.050, New P: 0.689
-Original Grad: 0.178, -lr * Pred Grad: -0.019, New P: -0.671
Target params: [1.1812, 0.2779]
iter 0 loss: 1.000
Actual params: [0.5941, 0.5941]
-Original Grad: 0.006, -lr * Pred Grad: 0.040, New P: 0.634
-Original Grad: -0.045, -lr * Pred Grad: -0.009, New P: 0.585
iter 1 loss: 0.981
Actual params: [0.6336, 0.5849]
-Original Grad: -0.006, -lr * Pred Grad: -0.028, New P: 0.606
-Original Grad: -0.084, -lr * Pred Grad: -0.065, New P: 0.520
iter 2 loss: 0.983
Actual params: [0.6058, 0.5196]
-Original Grad: 0.111, -lr * Pred Grad: 0.006, New P: 0.612
-Original Grad: -0.162, -lr * Pred Grad: -0.079, New P: 0.441
iter 3 loss: 0.966
Actual params: [0.6116, 0.4408]
-Original Grad: 0.131, -lr * Pred Grad: 0.063, New P: 0.674
-Original Grad: -0.163, -lr * Pred Grad: -0.080, New P: 0.361
iter 4 loss: 0.899
Actual params: [0.6743, 0.3608]
-Original Grad: 0.010, -lr * Pred Grad: 0.049, New P: 0.723
-Original Grad: -0.137, -lr * Pred Grad: -0.078, New P: 0.283
iter 5 loss: 0.844
Actual params: [0.7231, 0.2827]
-Original Grad: 0.074, -lr * Pred Grad: 0.060, New P: 0.783
-Original Grad: -0.228, -lr * Pred Grad: -0.081, New P: 0.202
iter 6 loss: 0.814
Actual params: [0.7832, 0.2019]
-Original Grad: 0.010, -lr * Pred Grad: 0.001, New P: 0.784
-Original Grad: -0.111, -lr * Pred Grad: -0.080, New P: 0.122
iter 7 loss: 0.797
Actual params: [0.7841, 0.1222]
-Original Grad: 0.034, -lr * Pred Grad: 0.040, New P: 0.824
-Original Grad: -0.085, -lr * Pred Grad: -0.076, New P: 0.046
iter 8 loss: 0.782
Actual params: [0.8244, 0.0461]
-Original Grad: -0.061, -lr * Pred Grad: -0.036, New P: 0.789
-Original Grad: -0.004, -lr * Pred Grad: -0.070, New P: -0.024
iter 9 loss: 0.777
Actual params: [ 0.7887, -0.0243]
-Original Grad: -0.022, -lr * Pred Grad: -0.039, New P: 0.750
-Original Grad: -0.067, -lr * Pred Grad: -0.068, New P: -0.093
iter 10 loss: 0.771
Actual params: [ 0.7497, -0.0925]
-Original Grad: -0.018, -lr * Pred Grad: -0.027, New P: 0.722
-Original Grad: -0.098, -lr * Pred Grad: -0.067, New P: -0.159
iter 11 loss: 0.762
Actual params: [ 0.7224, -0.1592]
-Original Grad: -0.086, -lr * Pred Grad: -0.045, New P: 0.677
-Original Grad: -0.084, -lr * Pred Grad: -0.066, New P: -0.225
iter 12 loss: 0.757
Actual params: [ 0.6772, -0.225 ]
-Original Grad: -0.135, -lr * Pred Grad: -0.056, New P: 0.621
-Original Grad: -0.065, -lr * Pred Grad: -0.065, New P: -0.290
iter 13 loss: 0.758
Actual params: [ 0.6215, -0.2897]
-Original Grad: 0.052, -lr * Pred Grad: -0.041, New P: 0.581
-Original Grad: -0.042, -lr * Pred Grad: -0.062, New P: -0.352
iter 14 loss: 0.748
Actual params: [ 0.5807, -0.3517]
-Original Grad: 0.018, -lr * Pred Grad: -0.006, New P: 0.574
-Original Grad: -0.006, -lr * Pred Grad: -0.053, New P: -0.405
iter 15 loss: 0.703
Actual params: [ 0.5744, -0.4052]
-Original Grad: -0.034, -lr * Pred Grad: -0.005, New P: 0.569
-Original Grad: 0.178, -lr * Pred Grad: -0.033, New P: -0.438
iter 16 loss: 0.690
Actual params: [ 0.5695, -0.4382]
-Original Grad: 0.062, -lr * Pred Grad: 0.018, New P: 0.588
-Original Grad: -0.096, -lr * Pred Grad: -0.027, New P: -0.466
iter 17 loss: 0.664
Actual params: [ 0.5876, -0.4656]
-Original Grad: 0.034, -lr * Pred Grad: 0.031, New P: 0.619
-Original Grad: 0.119, -lr * Pred Grad: -0.024, New P: -0.490
iter 18 loss: 0.661
Actual params: [ 0.6187, -0.4899]
-Original Grad: -0.250, -lr * Pred Grad: -0.039, New P: 0.580
-Original Grad: 0.051, -lr * Pred Grad: -0.021, New P: -0.511
iter 19 loss: 0.662
Actual params: [ 0.5796, -0.5109]
-Original Grad: 0.153, -lr * Pred Grad: -0.016, New P: 0.563
-Original Grad: -0.006, -lr * Pred Grad: -0.023, New P: -0.534
iter 20 loss: 0.664
Actual params: [ 0.5632, -0.5338]
-Original Grad: -0.030, -lr * Pred Grad: 0.014, New P: 0.577
-Original Grad: -0.204, -lr * Pred Grad: -0.061, New P: -0.594
Target params: [1.1812, 0.2779]
iter 0 loss: 0.749
Actual params: [0.5941, 0.5941]
-Original Grad: 0.128, -lr * Pred Grad: 0.066, New P: 0.660
-Original Grad: 0.046, -lr * Pred Grad: 0.058, New P: 0.652
iter 1 loss: 0.746
Actual params: [0.6604, 0.6517]
-Original Grad: 0.095, -lr * Pred Grad: 0.083, New P: 0.744
-Original Grad: 0.021, -lr * Pred Grad: 0.038, New P: 0.689
iter 2 loss: 0.741
Actual params: [0.7436, 0.6892]
-Original Grad: 0.023, -lr * Pred Grad: 0.079, New P: 0.823
-Original Grad: 0.172, -lr * Pred Grad: 0.073, New P: 0.762
iter 3 loss: 0.733
Actual params: [0.8229, 0.7622]
-Original Grad: 0.250, -lr * Pred Grad: 0.084, New P: 0.907
-Original Grad: 0.308, -lr * Pred Grad: 0.085, New P: 0.847
iter 4 loss: 0.746
Actual params: [0.907 , 0.8469]
-Original Grad: 0.240, -lr * Pred Grad: 0.087, New P: 0.994
-Original Grad: 0.008, -lr * Pred Grad: 0.086, New P: 0.932
iter 5 loss: 0.775
Actual params: [0.994 , 0.9325]
-Original Grad: 0.006, -lr * Pred Grad: 0.086, New P: 1.080
-Original Grad: -0.102, -lr * Pred Grad: -0.009, New P: 0.924
iter 6 loss: 0.802
Actual params: [1.0803, 0.9236]
-Original Grad: 0.014, -lr * Pred Grad: 0.048, New P: 1.129
-Original Grad: -0.061, -lr * Pred Grad: -0.043, New P: 0.881
iter 7 loss: 0.800
Actual params: [1.1287, 0.8809]
-Original Grad: -0.090, -lr * Pred Grad: -0.031, New P: 1.097
-Original Grad: -0.124, -lr * Pred Grad: -0.054, New P: 0.826
iter 8 loss: 0.787
Actual params: [1.0974, 0.8264]
-Original Grad: -0.068, -lr * Pred Grad: -0.046, New P: 1.051
-Original Grad: -0.104, -lr * Pred Grad: -0.058, New P: 0.769
iter 9 loss: 0.763
Actual params: [1.0512, 0.7689]
-Original Grad: 0.021, -lr * Pred Grad: -0.022, New P: 1.029
-Original Grad: -0.119, -lr * Pred Grad: -0.059, New P: 0.710
iter 10 loss: 0.736
Actual params: [1.0293, 0.7099]
-Original Grad: 0.101, -lr * Pred Grad: 0.034, New P: 1.064
-Original Grad: -0.016, -lr * Pred Grad: -0.057, New P: 0.653
iter 11 loss: 0.708
Actual params: [1.0636, 0.6531]
-Original Grad: 0.117, -lr * Pred Grad: 0.071, New P: 1.134
-Original Grad: 0.018, -lr * Pred Grad: -0.041, New P: 0.612
iter 12 loss: 0.686
Actual params: [1.1343, 0.6117]
-Original Grad: -0.035, -lr * Pred Grad: 0.002, New P: 1.137
-Original Grad: -0.137, -lr * Pred Grad: -0.051, New P: 0.560
iter 13 loss: 0.623
Actual params: [1.1365, 0.5602]
-Original Grad: -0.128, -lr * Pred Grad: -0.047, New P: 1.090
-Original Grad: 0.089, -lr * Pred Grad: -0.025, New P: 0.535
iter 14 loss: 0.589
Actual params: [1.0898, 0.5349]
-Original Grad: 0.113, -lr * Pred Grad: -0.007, New P: 1.083
-Original Grad: 0.141, -lr * Pred Grad: 0.004, New P: 0.539
iter 15 loss: 0.595
Actual params: [1.0833, 0.5392]
-Original Grad: 0.028, -lr * Pred Grad: 0.045, New P: 1.128
-Original Grad: 0.040, -lr * Pred Grad: 0.033, New P: 0.572
iter 16 loss: 0.639
Actual params: [1.128 , 0.5722]
-Original Grad: -0.156, -lr * Pred Grad: -0.039, New P: 1.089
-Original Grad: 0.061, -lr * Pred Grad: 0.057, New P: 0.629
iter 17 loss: 0.696
Actual params: [1.0889, 0.6295]
-Original Grad: 0.145, -lr * Pred Grad: 0.012, New P: 1.101
-Original Grad: 0.102, -lr * Pred Grad: 0.071, New P: 0.701
iter 18 loss: 0.743
Actual params: [1.1008, 0.7005]
-Original Grad: 0.089, -lr * Pred Grad: 0.064, New P: 1.165
-Original Grad: 0.070, -lr * Pred Grad: 0.070, New P: 0.771
iter 19 loss: 0.771
Actual params: [1.1646, 0.7707]
-Original Grad: 0.016, -lr * Pred Grad: 0.044, New P: 1.208
-Original Grad: -0.047, -lr * Pred Grad: -0.016, New P: 0.755
iter 20 loss: 0.765
Actual params: [1.2084, 0.7548]
-Original Grad: -0.003, -lr * Pred Grad: -0.007, New P: 1.202
-Original Grad: -0.296, -lr * Pred Grad: -0.051, New P: 0.703
Target params: [1.1812, 0.2779]
iter 0 loss: 1.042
Actual params: [0.5941, 0.5941]
-Original Grad: -0.165, -lr * Pred Grad: -0.060, New P: 0.534
-Original Grad: -0.345, -lr * Pred Grad: -0.063, New P: 0.531
iter 1 loss: 0.993
Actual params: [0.5338, 0.5309]
-Original Grad: -0.100, -lr * Pred Grad: -0.080, New P: 0.454
-Original Grad: -0.180, -lr * Pred Grad: -0.081, New P: 0.450
iter 2 loss: 0.969
Actual params: [0.4542, 0.4503]
-Original Grad: 0.029, -lr * Pred Grad: -0.077, New P: 0.377
-Original Grad: -0.040, -lr * Pred Grad: -0.083, New P: 0.367
iter 3 loss: 0.960
Actual params: [0.3775, 0.3669]
-Original Grad: 0.088, -lr * Pred Grad: -0.070, New P: 0.308
-Original Grad: -0.065, -lr * Pred Grad: -0.083, New P: 0.284
iter 4 loss: 0.953
Actual params: [0.308 , 0.2837]
-Original Grad: -0.070, -lr * Pred Grad: -0.064, New P: 0.244
-Original Grad: -0.092, -lr * Pred Grad: -0.081, New P: 0.203
iter 5 loss: 0.947
Actual params: [0.2436, 0.2028]
-Original Grad: -0.041, -lr * Pred Grad: -0.059, New P: 0.185
-Original Grad: -0.034, -lr * Pred Grad: -0.074, New P: 0.129
iter 6 loss: 0.943
Actual params: [0.1849, 0.1287]
-Original Grad: -0.023, -lr * Pred Grad: -0.050, New P: 0.135
-Original Grad: -0.089, -lr * Pred Grad: -0.071, New P: 0.058
iter 7 loss: 0.939
Actual params: [0.1352, 0.0581]
-Original Grad: 0.006, -lr * Pred Grad: -0.029, New P: 0.106
-Original Grad: -0.061, -lr * Pred Grad: -0.068, New P: -0.010
iter 8 loss: 0.937
Actual params: [ 0.106 , -0.0103]
-Original Grad: 0.006, -lr * Pred Grad: -0.009, New P: 0.097
-Original Grad: -0.000, -lr * Pred Grad: -0.065, New P: -0.075
iter 9 loss: 0.936
Actual params: [ 0.097 , -0.0754]
-Original Grad: -0.013, -lr * Pred Grad: -0.015, New P: 0.082
-Original Grad: 0.008, -lr * Pred Grad: -0.054, New P: -0.129
iter 10 loss: 0.936
Actual params: [ 0.0816, -0.1293]
-Original Grad: 0.008, -lr * Pred Grad: -0.017, New P: 0.064
-Original Grad: -0.017, -lr * Pred Grad: -0.042, New P: -0.172
iter 11 loss: 0.936
Actual params: [ 0.0643, -0.1717]
-Original Grad: -0.013, -lr * Pred Grad: -0.023, New P: 0.041
-Original Grad: 0.006, -lr * Pred Grad: -0.035, New P: -0.207
iter 12 loss: 0.935
Actual params: [ 0.0413, -0.2071]
-Original Grad: 0.001, -lr * Pred Grad: -0.023, New P: 0.018
-Original Grad: 0.010, -lr * Pred Grad: -0.031, New P: -0.238
iter 13 loss: 0.934
Actual params: [ 0.018 , -0.2383]
-Original Grad: -0.007, -lr * Pred Grad: -0.024, New P: -0.006
-Original Grad: 0.021, -lr * Pred Grad: -0.027, New P: -0.265
iter 14 loss: 0.934
Actual params: [-0.0059, -0.2654]
-Original Grad: 0.003, -lr * Pred Grad: -0.022, New P: -0.028
-Original Grad: -0.010, -lr * Pred Grad: -0.031, New P: -0.296
iter 15 loss: 0.933
Actual params: [-0.0275, -0.2961]
-Original Grad: 0.006, -lr * Pred Grad: -0.018, New P: -0.045
-Original Grad: 0.026, -lr * Pred Grad: -0.020, New P: -0.316
iter 16 loss: 0.932
Actual params: [-0.0452, -0.3161]
-Original Grad: 0.003, -lr * Pred Grad: -0.016, New P: -0.061
-Original Grad: 0.023, -lr * Pred Grad: 0.001, New P: -0.315
iter 17 loss: 0.932
Actual params: [-0.0613, -0.3151]
-Original Grad: 0.011, -lr * Pred Grad: -0.014, New P: -0.075
-Original Grad: 0.009, -lr * Pred Grad: -0.001, New P: -0.316
iter 18 loss: 0.932
Actual params: [-0.0753, -0.3156]
-Original Grad: -0.012, -lr * Pred Grad: -0.021, New P: -0.096
-Original Grad: 0.051, -lr * Pred Grad: 0.011, New P: -0.305
iter 19 loss: 0.932
Actual params: [-0.0959, -0.305 ]
-Original Grad: 0.010, -lr * Pred Grad: -0.021, New P: -0.117
-Original Grad: 0.033, -lr * Pred Grad: 0.018, New P: -0.287
iter 20 loss: 0.933
Actual params: [-0.1169, -0.287 ]
-Original Grad: 0.005, -lr * Pred Grad: -0.018, New P: -0.135
-Original Grad: -0.019, -lr * Pred Grad: -0.015, New P: -0.302
Target params: [1.1812, 0.2779]
iter 0 loss: 0.434
Actual params: [0.5941, 0.5941]
-Original Grad: 0.259, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.204, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 0.402
Actual params: [0.6616, 0.5319]
-Original Grad: 0.003, -lr * Pred Grad: 0.084, New P: 0.746
-Original Grad: -0.095, -lr * Pred Grad: -0.080, New P: 0.452
iter 2 loss: 0.379
Actual params: [0.7457, 0.4516]
-Original Grad: 0.049, -lr * Pred Grad: 0.085, New P: 0.831
-Original Grad: -0.138, -lr * Pred Grad: -0.083, New P: 0.369
iter 3 loss: 0.372
Actual params: [0.8309, 0.3686]
-Original Grad: 0.129, -lr * Pred Grad: 0.083, New P: 0.914
-Original Grad: -0.188, -lr * Pred Grad: -0.084, New P: 0.285
iter 4 loss: 0.374
Actual params: [0.914, 0.285]
-Original Grad: 0.194, -lr * Pred Grad: 0.085, New P: 0.999
-Original Grad: -0.175, -lr * Pred Grad: -0.084, New P: 0.201
iter 5 loss: 0.374
Actual params: [0.9995, 0.2013]
-Original Grad: -0.080, -lr * Pred Grad: 0.052, New P: 1.051
-Original Grad: -0.112, -lr * Pred Grad: -0.083, New P: 0.118
iter 6 loss: 0.372
Actual params: [1.0515, 0.1181]
-Original Grad: -0.032, -lr * Pred Grad: -0.023, New P: 1.028
-Original Grad: -0.055, -lr * Pred Grad: -0.080, New P: 0.038
iter 7 loss: 0.370
Actual params: [1.0284, 0.0383]
-Original Grad: 0.020, -lr * Pred Grad: 0.017, New P: 1.045
-Original Grad: -0.072, -lr * Pred Grad: -0.074, New P: -0.036
iter 8 loss: 0.370
Actual params: [ 1.045 , -0.0358]
-Original Grad: 0.039, -lr * Pred Grad: 0.028, New P: 1.073
-Original Grad: -0.028, -lr * Pred Grad: -0.070, New P: -0.106
iter 9 loss: 0.369
Actual params: [ 1.073 , -0.1058]
-Original Grad: 0.032, -lr * Pred Grad: 0.017, New P: 1.090
-Original Grad: -0.092, -lr * Pred Grad: -0.068, New P: -0.174
iter 10 loss: 0.369
Actual params: [ 1.0898, -0.174 ]
-Original Grad: 0.012, -lr * Pred Grad: -0.000, New P: 1.090
-Original Grad: -0.067, -lr * Pred Grad: -0.067, New P: -0.241
iter 11 loss: 0.367
Actual params: [ 1.0896, -0.2406]
-Original Grad: -0.047, -lr * Pred Grad: -0.035, New P: 1.054
-Original Grad: -0.068, -lr * Pred Grad: -0.065, New P: -0.306
iter 12 loss: 0.362
Actual params: [ 1.0544, -0.3057]
-Original Grad: -0.044, -lr * Pred Grad: -0.044, New P: 1.011
-Original Grad: -0.053, -lr * Pred Grad: -0.063, New P: -0.369
iter 13 loss: 0.359
Actual params: [ 1.0107, -0.3687]
-Original Grad: -0.011, -lr * Pred Grad: -0.033, New P: 0.978
-Original Grad: -0.029, -lr * Pred Grad: -0.058, New P: -0.427
iter 14 loss: 0.358
Actual params: [ 0.9778, -0.427 ]
-Original Grad: -0.009, -lr * Pred Grad: -0.019, New P: 0.959
-Original Grad: -0.055, -lr * Pred Grad: -0.056, New P: -0.483
iter 15 loss: 0.356
Actual params: [ 0.9593, -0.4829]
-Original Grad: 0.032, -lr * Pred Grad: 0.004, New P: 0.963
-Original Grad: -0.088, -lr * Pred Grad: -0.060, New P: -0.543
iter 16 loss: 0.352
Actual params: [ 0.9631, -0.5425]
-Original Grad: 0.030, -lr * Pred Grad: 0.018, New P: 0.981
-Original Grad: -0.077, -lr * Pred Grad: -0.062, New P: -0.604
iter 17 loss: 0.348
Actual params: [ 0.9808, -0.6042]
-Original Grad: 0.004, -lr * Pred Grad: 0.004, New P: 0.985
-Original Grad: 0.013, -lr * Pred Grad: -0.053, New P: -0.658
iter 18 loss: 0.346
Actual params: [ 0.9845, -0.6575]
-Original Grad: 0.098, -lr * Pred Grad: 0.040, New P: 1.025
-Original Grad: -0.037, -lr * Pred Grad: -0.046, New P: -0.704
iter 19 loss: 0.346
Actual params: [ 1.0246, -0.7038]
-Original Grad: 0.010, -lr * Pred Grad: 0.029, New P: 1.053
-Original Grad: -0.152, -lr * Pred Grad: -0.061, New P: -0.765
iter 20 loss: 0.342
Actual params: [ 1.0533, -0.7648]
-Original Grad: 0.101, -lr * Pred Grad: 0.053, New P: 1.106
-Original Grad: 0.183, -lr * Pred Grad: -0.036, New P: -0.800
Target params: [1.1812, 0.2779]
iter 0 loss: 0.406
Actual params: [0.5941, 0.5941]
-Original Grad: 0.223, -lr * Pred Grad: 0.068, New P: 0.662
-Original Grad: -0.160, -lr * Pred Grad: -0.060, New P: 0.534
iter 1 loss: 0.373
Actual params: [0.6616, 0.5341]
-Original Grad: -0.088, -lr * Pred Grad: 0.082, New P: 0.744
-Original Grad: 0.014, -lr * Pred Grad: -0.073, New P: 0.461
iter 2 loss: 0.362
Actual params: [0.7435, 0.4609]
-Original Grad: -0.124, -lr * Pred Grad: -0.031, New P: 0.712
-Original Grad: -0.063, -lr * Pred Grad: -0.071, New P: 0.390
iter 3 loss: 0.363
Actual params: [0.7122, 0.3901]
-Original Grad: -0.110, -lr * Pred Grad: -0.058, New P: 0.654
-Original Grad: -0.041, -lr * Pred Grad: -0.068, New P: 0.322
iter 4 loss: 0.367
Actual params: [0.6545, 0.3217]
-Original Grad: 0.049, -lr * Pred Grad: -0.043, New P: 0.611
-Original Grad: -0.034, -lr * Pred Grad: -0.066, New P: 0.256
iter 5 loss: 0.368
Actual params: [0.6111, 0.2559]
-Original Grad: 0.161, -lr * Pred Grad: 0.011, New P: 0.622
-Original Grad: -0.101, -lr * Pred Grad: -0.065, New P: 0.191
iter 6 loss: 0.367
Actual params: [0.6222, 0.1911]
-Original Grad: -0.085, -lr * Pred Grad: 0.043, New P: 0.665
-Original Grad: -0.038, -lr * Pred Grad: -0.063, New P: 0.128
iter 7 loss: 0.363
Actual params: [0.6648, 0.1283]
-Original Grad: -0.027, -lr * Pred Grad: -0.037, New P: 0.628
-Original Grad: -0.005, -lr * Pred Grad: -0.055, New P: 0.074
iter 8 loss: 0.365
Actual params: [0.6277, 0.0737]
-Original Grad: 0.064, -lr * Pred Grad: 0.010, New P: 0.638
-Original Grad: -0.033, -lr * Pred Grad: -0.047, New P: 0.027
iter 9 loss: 0.363
Actual params: [0.6377, 0.0267]
-Original Grad: 0.003, -lr * Pred Grad: 0.019, New P: 0.657
-Original Grad: -0.028, -lr * Pred Grad: -0.044, New P: -0.017
iter 10 loss: 0.363
Actual params: [ 0.657 , -0.0174]
-Original Grad: -0.062, -lr * Pred Grad: -0.035, New P: 0.622
-Original Grad: -0.021, -lr * Pred Grad: -0.038, New P: -0.055
iter 11 loss: 0.363
Actual params: [ 0.6218, -0.0552]
-Original Grad: 0.034, -lr * Pred Grad: -0.007, New P: 0.614
-Original Grad: -0.086, -lr * Pred Grad: -0.049, New P: -0.105
iter 12 loss: 0.363
Actual params: [ 0.6145, -0.1046]
-Original Grad: 0.023, -lr * Pred Grad: 0.016, New P: 0.631
-Original Grad: -0.048, -lr * Pred Grad: -0.054, New P: -0.158
iter 13 loss: 0.364
Actual params: [ 0.6309, -0.1583]
-Original Grad: -0.024, -lr * Pred Grad: -0.019, New P: 0.612
-Original Grad: -0.036, -lr * Pred Grad: -0.050, New P: -0.209
iter 14 loss: 0.363
Actual params: [ 0.612 , -0.2087]
-Original Grad: -0.040, -lr * Pred Grad: -0.032, New P: 0.580
-Original Grad: -0.019, -lr * Pred Grad: -0.042, New P: -0.250
iter 15 loss: 0.363
Actual params: [ 0.58  , -0.2504]
-Original Grad: 0.205, -lr * Pred Grad: 0.027, New P: 0.607
-Original Grad: -0.050, -lr * Pred Grad: -0.041, New P: -0.291
iter 16 loss: 0.364
Actual params: [ 0.6074, -0.2915]
-Original Grad: -0.066, -lr * Pred Grad: 0.045, New P: 0.653
-Original Grad: -0.001, -lr * Pred Grad: -0.036, New P: -0.327
iter 17 loss: 0.369
Actual params: [ 0.6526, -0.3274]
-Original Grad: -0.139, -lr * Pred Grad: -0.038, New P: 0.614
-Original Grad: -0.021, -lr * Pred Grad: -0.033, New P: -0.360
iter 18 loss: 0.366
Actual params: [ 0.6141, -0.3602]
-Original Grad: -0.107, -lr * Pred Grad: -0.055, New P: 0.559
-Original Grad: 0.025, -lr * Pred Grad: -0.023, New P: -0.383
iter 19 loss: 0.364
Actual params: [ 0.5587, -0.3834]
-Original Grad: 0.254, -lr * Pred Grad: -0.011, New P: 0.547
-Original Grad: -0.089, -lr * Pred Grad: -0.037, New P: -0.420
iter 20 loss: 0.364
Actual params: [ 0.5474, -0.4202]
-Original Grad: 0.307, -lr * Pred Grad: 0.045, New P: 0.593
-Original Grad: -0.072, -lr * Pred Grad: -0.046, New P: -0.466
Target params: [1.1812, 0.2779]
iter 0 loss: 0.788
Actual params: [0.5941, 0.5941]
-Original Grad: -0.017, -lr * Pred Grad: 0.019, New P: 0.614
-Original Grad: -0.126, -lr * Pred Grad: -0.055, New P: 0.539
iter 1 loss: 0.718
Actual params: [0.6135, 0.5387]
-Original Grad: 0.001, -lr * Pred Grad: -0.038, New P: 0.575
-Original Grad: 0.034, -lr * Pred Grad: -0.067, New P: 0.471
iter 2 loss: 0.731
Actual params: [0.5752, 0.4712]
-Original Grad: 0.115, -lr * Pred Grad: -0.004, New P: 0.571
-Original Grad: -0.165, -lr * Pred Grad: -0.072, New P: 0.399
iter 3 loss: 0.680
Actual params: [0.5712, 0.3992]
-Original Grad: -0.127, -lr * Pred Grad: -0.051, New P: 0.520
-Original Grad: -0.210, -lr * Pred Grad: -0.077, New P: 0.322
iter 4 loss: 0.701
Actual params: [0.5202, 0.3224]
-Original Grad: -0.089, -lr * Pred Grad: -0.063, New P: 0.457
-Original Grad: -0.100, -lr * Pred Grad: -0.075, New P: 0.247
iter 5 loss: 0.725
Actual params: [0.4572, 0.2473]
-Original Grad: -0.062, -lr * Pred Grad: -0.064, New P: 0.393
-Original Grad: -0.104, -lr * Pred Grad: -0.073, New P: 0.175
iter 6 loss: 0.742
Actual params: [0.3934, 0.1745]
-Original Grad: 0.007, -lr * Pred Grad: -0.055, New P: 0.339
-Original Grad: -0.127, -lr * Pred Grad: -0.072, New P: 0.103
iter 7 loss: 0.754
Actual params: [0.3385, 0.1028]
-Original Grad: 0.051, -lr * Pred Grad: -0.024, New P: 0.314
-Original Grad: -0.036, -lr * Pred Grad: -0.069, New P: 0.034
iter 8 loss: 0.759
Actual params: [0.3143, 0.0339]
-Original Grad: 0.290, -lr * Pred Grad: 0.019, New P: 0.333
-Original Grad: -0.005, -lr * Pred Grad: -0.066, New P: -0.032
iter 9 loss: 0.754
Actual params: [ 0.3333, -0.0318]
-Original Grad: 0.221, -lr * Pred Grad: 0.060, New P: 0.393
-Original Grad: 0.010, -lr * Pred Grad: -0.056, New P: -0.088
iter 10 loss: 0.734
Actual params: [ 0.3934, -0.0876]
-Original Grad: -0.011, -lr * Pred Grad: 0.080, New P: 0.473
-Original Grad: -0.026, -lr * Pred Grad: -0.045, New P: -0.133
iter 11 loss: 0.688
Actual params: [ 0.473 , -0.1326]
-Original Grad: -0.111, -lr * Pred Grad: -0.014, New P: 0.459
-Original Grad: -0.025, -lr * Pred Grad: -0.044, New P: -0.176
iter 12 loss: 0.705
Actual params: [ 0.4586, -0.1765]
-Original Grad: -0.127, -lr * Pred Grad: -0.051, New P: 0.407
-Original Grad: -0.081, -lr * Pred Grad: -0.055, New P: -0.232
iter 13 loss: 0.741
Actual params: [ 0.4074, -0.2319]
-Original Grad: -0.069, -lr * Pred Grad: -0.057, New P: 0.350
-Original Grad: -0.079, -lr * Pred Grad: -0.060, New P: -0.292
iter 14 loss: 0.769
Actual params: [ 0.35  , -0.2923]
-Original Grad: 0.068, -lr * Pred Grad: -0.032, New P: 0.318
-Original Grad: 0.052, -lr * Pred Grad: -0.045, New P: -0.337
iter 15 loss: 0.780
Actual params: [ 0.3184, -0.3375]
-Original Grad: 0.131, -lr * Pred Grad: 0.018, New P: 0.336
-Original Grad: -0.009, -lr * Pred Grad: -0.030, New P: -0.368
iter 16 loss: 0.776
Actual params: [ 0.3363, -0.3679]
-Original Grad: 0.004, -lr * Pred Grad: 0.058, New P: 0.394
-Original Grad: 0.019, -lr * Pred Grad: -0.022, New P: -0.390
iter 17 loss: 0.756
Actual params: [ 0.394 , -0.3898]
-Original Grad: 0.058, -lr * Pred Grad: 0.025, New P: 0.419
-Original Grad: 0.015, -lr * Pred Grad: -0.014, New P: -0.404
iter 18 loss: 0.745
Actual params: [ 0.4188, -0.4038]
-Original Grad: 0.249, -lr * Pred Grad: 0.073, New P: 0.492
-Original Grad: 0.034, -lr * Pred Grad: -0.000, New P: -0.404
iter 19 loss: 0.689
Actual params: [ 0.4916, -0.4042]
-Original Grad: -0.116, -lr * Pred Grad: 0.024, New P: 0.516
-Original Grad: -0.040, -lr * Pred Grad: -0.023, New P: -0.427
iter 20 loss: 0.658
Actual params: [ 0.5156, -0.4273]
-Original Grad: -0.094, -lr * Pred Grad: -0.043, New P: 0.473
-Original Grad: -0.014, -lr * Pred Grad: -0.035, New P: -0.463
Target params: [1.1812, 0.2779]
iter 0 loss: 0.819
Actual params: [0.5941, 0.5941]
-Original Grad: 0.023, -lr * Pred Grad: 0.049, New P: 0.643
-Original Grad: -0.103, -lr * Pred Grad: -0.049, New P: 0.545
iter 1 loss: 0.771
Actual params: [0.6432, 0.5449]
-Original Grad: 0.048, -lr * Pred Grad: 0.038, New P: 0.681
-Original Grad: -0.319, -lr * Pred Grad: -0.078, New P: 0.467
iter 2 loss: 0.706
Actual params: [0.6815, 0.4674]
-Original Grad: -0.034, -lr * Pred Grad: -0.043, New P: 0.639
-Original Grad: -0.026, -lr * Pred Grad: -0.082, New P: 0.385
iter 3 loss: 0.682
Actual params: [0.6387, 0.3849]
-Original Grad: -0.101, -lr * Pred Grad: -0.063, New P: 0.576
-Original Grad: -0.224, -lr * Pred Grad: -0.084, New P: 0.301
iter 4 loss: 0.673
Actual params: [0.5758, 0.3013]
-Original Grad: 0.012, -lr * Pred Grad: -0.059, New P: 0.517
-Original Grad: -0.159, -lr * Pred Grad: -0.084, New P: 0.218
iter 5 loss: 0.667
Actual params: [0.5172, 0.2176]
-Original Grad: -0.063, -lr * Pred Grad: -0.051, New P: 0.466
-Original Grad: -0.081, -lr * Pred Grad: -0.083, New P: 0.135
iter 6 loss: 0.670
Actual params: [0.4662, 0.1347]
-Original Grad: 0.030, -lr * Pred Grad: -0.023, New P: 0.444
-Original Grad: -0.102, -lr * Pred Grad: -0.081, New P: 0.054
iter 7 loss: 0.660
Actual params: [0.4436, 0.0538]
-Original Grad: 0.001, -lr * Pred Grad: 0.004, New P: 0.448
-Original Grad: -0.056, -lr * Pred Grad: -0.075, New P: -0.021
iter 8 loss: 0.625
Actual params: [ 0.4478, -0.0213]
-Original Grad: -0.058, -lr * Pred Grad: -0.035, New P: 0.413
-Original Grad: -0.095, -lr * Pred Grad: -0.072, New P: -0.093
iter 9 loss: 0.634
Actual params: [ 0.4131, -0.0929]
-Original Grad: -0.010, -lr * Pred Grad: -0.036, New P: 0.377
-Original Grad: -0.064, -lr * Pred Grad: -0.069, New P: -0.162
iter 10 loss: 0.629
Actual params: [ 0.3774, -0.1621]
-Original Grad: -0.126, -lr * Pred Grad: -0.055, New P: 0.322
-Original Grad: -0.093, -lr * Pred Grad: -0.068, New P: -0.230
iter 11 loss: 0.631
Actual params: [ 0.3223, -0.2297]
-Original Grad: -0.230, -lr * Pred Grad: -0.065, New P: 0.258
-Original Grad: -0.018, -lr * Pred Grad: -0.065, New P: -0.295
iter 12 loss: 0.626
Actual params: [ 0.2576, -0.2949]
-Original Grad: 0.039, -lr * Pred Grad: -0.064, New P: 0.194
-Original Grad: -0.033, -lr * Pred Grad: -0.060, New P: -0.355
iter 13 loss: 0.619
Actual params: [ 0.1939, -0.3552]
-Original Grad: 0.071, -lr * Pred Grad: -0.040, New P: 0.154
-Original Grad: -0.009, -lr * Pred Grad: -0.050, New P: -0.406
iter 14 loss: 0.606
Actual params: [ 0.1535, -0.4055]
-Original Grad: 0.088, -lr * Pred Grad: -0.009, New P: 0.145
-Original Grad: -0.029, -lr * Pred Grad: -0.045, New P: -0.451
iter 15 loss: 0.590
Actual params: [ 0.1447, -0.451 ]
-Original Grad: 0.022, -lr * Pred Grad: 0.020, New P: 0.164
-Original Grad: -0.031, -lr * Pred Grad: -0.046, New P: -0.497
iter 16 loss: 0.572
Actual params: [ 0.1644, -0.497 ]
-Original Grad: 0.044, -lr * Pred Grad: 0.041, New P: 0.206
-Original Grad: -0.013, -lr * Pred Grad: -0.043, New P: -0.540
iter 17 loss: 0.555
Actual params: [ 0.2056, -0.5401]
-Original Grad: 0.033, -lr * Pred Grad: 0.023, New P: 0.228
-Original Grad: -0.090, -lr * Pred Grad: -0.056, New P: -0.596
iter 18 loss: 0.539
Actual params: [ 0.2284, -0.5964]
-Original Grad: 0.096, -lr * Pred Grad: 0.059, New P: 0.287
-Original Grad: -0.044, -lr * Pred Grad: -0.058, New P: -0.654
iter 19 loss: 0.509
Actual params: [ 0.2872, -0.6544]
-Original Grad: 0.057, -lr * Pred Grad: 0.057, New P: 0.344
-Original Grad: -0.094, -lr * Pred Grad: -0.061, New P: -0.715
iter 20 loss: 0.465
Actual params: [ 0.3442, -0.7153]
-Original Grad: -0.114, -lr * Pred Grad: -0.034, New P: 0.311
-Original Grad: -0.083, -lr * Pred Grad: -0.063, New P: -0.778
Target params: [1.1812, 0.2779]
iter 0 loss: 0.590
Actual params: [0.5941, 0.5941]
-Original Grad: -0.079, -lr * Pred Grad: -0.037, New P: 0.557
-Original Grad: -0.064, -lr * Pred Grad: -0.027, New P: 0.567
iter 1 loss: 0.592
Actual params: [0.5566, 0.5675]
-Original Grad: -0.088, -lr * Pred Grad: -0.073, New P: 0.484
-Original Grad: -0.047, -lr * Pred Grad: -0.066, New P: 0.502
iter 2 loss: 0.589
Actual params: [0.484 , 0.5016]
-Original Grad: -0.107, -lr * Pred Grad: -0.077, New P: 0.407
-Original Grad: -0.076, -lr * Pred Grad: -0.070, New P: 0.431
iter 3 loss: 0.572
Actual params: [0.4066, 0.4311]
-Original Grad: 0.187, -lr * Pred Grad: -0.067, New P: 0.339
-Original Grad: -0.015, -lr * Pred Grad: -0.068, New P: 0.363
iter 4 loss: 0.554
Actual params: [0.3391, 0.3632]
-Original Grad: 0.133, -lr * Pred Grad: -0.042, New P: 0.297
-Original Grad: 0.030, -lr * Pred Grad: -0.059, New P: 0.304
iter 5 loss: 0.539
Actual params: [0.2973, 0.3041]
-Original Grad: 0.008, -lr * Pred Grad: 0.016, New P: 0.313
-Original Grad: 0.044, -lr * Pred Grad: -0.034, New P: 0.271
iter 6 loss: 0.527
Actual params: [0.313 , 0.2705]
-Original Grad: 0.021, -lr * Pred Grad: 0.045, New P: 0.359
-Original Grad: 0.098, -lr * Pred Grad: 0.003, New P: 0.273
iter 7 loss: 0.525
Actual params: [0.3585, 0.2733]
-Original Grad: 0.089, -lr * Pred Grad: 0.043, New P: 0.402
-Original Grad: -0.133, -lr * Pred Grad: -0.027, New P: 0.246
iter 8 loss: 0.512
Actual params: [0.4018, 0.2462]
-Original Grad: 0.106, -lr * Pred Grad: 0.074, New P: 0.476
-Original Grad: 0.139, -lr * Pred Grad: 0.020, New P: 0.266
iter 9 loss: 0.510
Actual params: [0.4756, 0.2658]
-Original Grad: -0.016, -lr * Pred Grad: 0.009, New P: 0.485
-Original Grad: -0.047, -lr * Pred Grad: -0.006, New P: 0.259
iter 10 loss: 0.506
Actual params: [0.4845, 0.2593]
-Original Grad: -0.090, -lr * Pred Grad: -0.040, New P: 0.445
-Original Grad: -0.011, -lr * Pred Grad: -0.026, New P: 0.233
iter 11 loss: 0.503
Actual params: [0.4448, 0.2331]
-Original Grad: -0.045, -lr * Pred Grad: -0.051, New P: 0.394
-Original Grad: 0.013, -lr * Pred Grad: -0.020, New P: 0.213
iter 12 loss: 0.502
Actual params: [0.3941, 0.2128]
-Original Grad: 0.118, -lr * Pred Grad: -0.003, New P: 0.391
-Original Grad: -0.000, -lr * Pred Grad: -0.016, New P: 0.196
iter 13 loss: 0.497
Actual params: [0.3907, 0.1964]
-Original Grad: 0.141, -lr * Pred Grad: 0.052, New P: 0.443
-Original Grad: -0.089, -lr * Pred Grad: -0.044, New P: 0.152
iter 14 loss: 0.476
Actual params: [0.4428, 0.1523]
-Original Grad: -0.105, -lr * Pred Grad: -0.005, New P: 0.438
-Original Grad: -0.017, -lr * Pred Grad: -0.050, New P: 0.102
iter 15 loss: 0.462
Actual params: [0.4383, 0.102 ]
-Original Grad: 0.044, -lr * Pred Grad: -0.009, New P: 0.429
-Original Grad: -0.058, -lr * Pred Grad: -0.051, New P: 0.051
iter 16 loss: 0.449
Actual params: [0.4295, 0.0506]
-Original Grad: -0.036, -lr * Pred Grad: -0.027, New P: 0.402
-Original Grad: 0.079, -lr * Pred Grad: -0.023, New P: 0.027
iter 17 loss: 0.448
Actual params: [0.4022, 0.0273]
-Original Grad: 0.080, -lr * Pred Grad: 0.024, New P: 0.426
-Original Grad: 0.023, -lr * Pred Grad: 0.007, New P: 0.035
iter 18 loss: 0.446
Actual params: [0.4257, 0.0346]
-Original Grad: 0.114, -lr * Pred Grad: 0.065, New P: 0.491
-Original Grad: -0.065, -lr * Pred Grad: -0.023, New P: 0.011
iter 19 loss: 0.426
Actual params: [0.4906, 0.0113]
-Original Grad: -0.071, -lr * Pred Grad: -0.017, New P: 0.474
-Original Grad: 0.078, -lr * Pred Grad: -0.006, New P: 0.006
iter 20 loss: 0.428
Actual params: [0.4741, 0.0055]
-Original Grad: -0.006, -lr * Pred Grad: -0.022, New P: 0.452
-Original Grad: 0.053, -lr * Pred Grad: 0.027, New P: 0.033
Target params: [1.1812, 0.2779]
iter 0 loss: 1.198
Actual params: [0.5941, 0.5941]
-Original Grad: 0.061, -lr * Pred Grad: 0.061, New P: 0.655
-Original Grad: -0.187, -lr * Pred Grad: -0.062, New P: 0.532
iter 1 loss: 1.132
Actual params: [0.6548, 0.5325]
-Original Grad: 0.136, -lr * Pred Grad: 0.081, New P: 0.736
-Original Grad: 0.004, -lr * Pred Grad: -0.076, New P: 0.456
iter 2 loss: 1.055
Actual params: [0.7362, 0.4562]
-Original Grad: 0.139, -lr * Pred Grad: 0.084, New P: 0.821
-Original Grad: -0.060, -lr * Pred Grad: -0.073, New P: 0.383
iter 3 loss: 0.965
Actual params: [0.8207, 0.3828]
-Original Grad: 0.383, -lr * Pred Grad: 0.087, New P: 0.908
-Original Grad: -0.004, -lr * Pred Grad: -0.069, New P: 0.314
iter 4 loss: 0.853
Actual params: [0.908 , 0.3135]
-Original Grad: 0.255, -lr * Pred Grad: 0.088, New P: 0.996
-Original Grad: 0.017, -lr * Pred Grad: -0.064, New P: 0.249
iter 5 loss: 0.749
Actual params: [0.996 , 0.2493]
-Original Grad: -0.070, -lr * Pred Grad: 0.087, New P: 1.083
-Original Grad: -0.007, -lr * Pred Grad: -0.051, New P: 0.198
iter 6 loss: 0.648
Actual params: [1.0834, 0.1984]
-Original Grad: 0.077, -lr * Pred Grad: 0.077, New P: 1.161
-Original Grad: -0.007, -lr * Pred Grad: -0.037, New P: 0.162
iter 7 loss: 0.578
Actual params: [1.1606, 0.1616]
-Original Grad: -0.012, -lr * Pred Grad: 0.005, New P: 1.166
-Original Grad: -0.072, -lr * Pred Grad: -0.043, New P: 0.118
iter 8 loss: 0.600
Actual params: [1.1657, 0.1184]
-Original Grad: 0.087, -lr * Pred Grad: 0.055, New P: 1.221
-Original Grad: -0.013, -lr * Pred Grad: -0.039, New P: 0.079
iter 9 loss: 0.580
Actual params: [1.221 , 0.0789]
-Original Grad: 0.113, -lr * Pred Grad: 0.077, New P: 1.298
-Original Grad: 0.017, -lr * Pred Grad: -0.019, New P: 0.060
iter 10 loss: 0.551
Actual params: [1.298 , 0.0599]
-Original Grad: 0.045, -lr * Pred Grad: 0.067, New P: 1.365
-Original Grad: -0.043, -lr * Pred Grad: -0.028, New P: 0.032
iter 11 loss: 0.550
Actual params: [1.3646, 0.0318]
-Original Grad: 0.032, -lr * Pred Grad: 0.028, New P: 1.392
-Original Grad: -0.063, -lr * Pred Grad: -0.045, New P: -0.013
iter 12 loss: 0.561
Actual params: [ 1.3923, -0.0127]
-Original Grad: 0.021, -lr * Pred Grad: 0.025, New P: 1.417
-Original Grad: 0.054, -lr * Pred Grad: -0.028, New P: -0.041
iter 13 loss: 0.567
Actual params: [ 1.4171, -0.041 ]
-Original Grad: 0.072, -lr * Pred Grad: 0.037, New P: 1.454
-Original Grad: -0.062, -lr * Pred Grad: -0.030, New P: -0.071
iter 14 loss: 0.570
Actual params: [ 1.4539, -0.0711]
-Original Grad: 0.075, -lr * Pred Grad: 0.058, New P: 1.512
-Original Grad: 0.011, -lr * Pred Grad: -0.028, New P: -0.100
iter 15 loss: 0.576
Actual params: [ 1.5119, -0.0995]
-Original Grad: 0.062, -lr * Pred Grad: 0.048, New P: 1.560
-Original Grad: -0.005, -lr * Pred Grad: -0.024, New P: -0.123
iter 16 loss: 0.582
Actual params: [ 1.5595, -0.1233]
-Original Grad: 0.044, -lr * Pred Grad: 0.041, New P: 1.600
-Original Grad: -0.092, -lr * Pred Grad: -0.041, New P: -0.165
iter 17 loss: 0.585
Actual params: [ 1.6002, -0.1647]
-Original Grad: 0.000, -lr * Pred Grad: -0.011, New P: 1.589
-Original Grad: -0.054, -lr * Pred Grad: -0.050, New P: -0.215
iter 18 loss: 0.580
Actual params: [ 1.5895, -0.2148]
-Original Grad: -0.014, -lr * Pred Grad: -0.015, New P: 1.575
-Original Grad: 0.017, -lr * Pred Grad: -0.044, New P: -0.259
iter 19 loss: 0.573
Actual params: [ 1.5748, -0.2587]
-Original Grad: 0.067, -lr * Pred Grad: 0.024, New P: 1.599
-Original Grad: -0.024, -lr * Pred Grad: -0.034, New P: -0.293
iter 20 loss: 0.574
Actual params: [ 1.5986, -0.2932]
-Original Grad: -0.010, -lr * Pred Grad: -0.006, New P: 1.592
-Original Grad: -0.058, -lr * Pred Grad: -0.041, New P: -0.334
Target params: [1.1812, 0.2779]
iter 0 loss: 0.485
Actual params: [0.5941, 0.5941]
-Original Grad: 0.057, -lr * Pred Grad: 0.060, New P: 0.654
-Original Grad: -0.150, -lr * Pred Grad: -0.059, New P: 0.535
iter 1 loss: 0.460
Actual params: [0.6541, 0.5351]
-Original Grad: 0.038, -lr * Pred Grad: 0.062, New P: 0.717
-Original Grad: -0.139, -lr * Pred Grad: -0.080, New P: 0.455
iter 2 loss: 0.426
Actual params: [0.7165, 0.4555]
-Original Grad: 0.043, -lr * Pred Grad: -0.001, New P: 0.715
-Original Grad: -0.130, -lr * Pred Grad: -0.083, New P: 0.373
iter 3 loss: 0.406
Actual params: [0.7154, 0.3726]
-Original Grad: 0.011, -lr * Pred Grad: 0.005, New P: 0.721
-Original Grad: -0.042, -lr * Pred Grad: -0.080, New P: 0.293
iter 4 loss: 0.397
Actual params: [0.7207, 0.2927]
-Original Grad: 0.002, -lr * Pred Grad: -0.007, New P: 0.713
-Original Grad: -0.036, -lr * Pred Grad: -0.073, New P: 0.220
iter 5 loss: 0.396
Actual params: [0.7133, 0.22  ]
-Original Grad: 0.027, -lr * Pred Grad: 0.022, New P: 0.735
-Original Grad: -0.032, -lr * Pred Grad: -0.069, New P: 0.151
iter 6 loss: 0.394
Actual params: [0.7351, 0.1508]
-Original Grad: 0.026, -lr * Pred Grad: 0.007, New P: 0.742
-Original Grad: 0.035, -lr * Pred Grad: -0.064, New P: 0.087
iter 7 loss: 0.395
Actual params: [0.7419, 0.087 ]
-Original Grad: 0.024, -lr * Pred Grad: 0.015, New P: 0.757
-Original Grad: 0.003, -lr * Pred Grad: -0.048, New P: 0.039
iter 8 loss: 0.395
Actual params: [0.7571, 0.0389]
-Original Grad: 0.015, -lr * Pred Grad: -0.002, New P: 0.755
-Original Grad: -0.002, -lr * Pred Grad: -0.037, New P: 0.002
iter 9 loss: 0.395
Actual params: [0.7547, 0.0021]
-Original Grad: 0.018, -lr * Pred Grad: 0.004, New P: 0.759
-Original Grad: 0.024, -lr * Pred Grad: -0.028, New P: -0.026
iter 10 loss: 0.396
Actual params: [ 0.7588, -0.0258]
-Original Grad: -0.009, -lr * Pred Grad: -0.015, New P: 0.744
-Original Grad: -0.007, -lr * Pred Grad: -0.023, New P: -0.049
iter 11 loss: 0.397
Actual params: [ 0.744 , -0.0489]
-Original Grad: -0.005, -lr * Pred Grad: -0.018, New P: 0.726
-Original Grad: -0.017, -lr * Pred Grad: -0.020, New P: -0.069
iter 12 loss: 0.399
Actual params: [ 0.7263, -0.0694]
-Original Grad: 0.020, -lr * Pred Grad: -0.005, New P: 0.721
-Original Grad: 0.035, -lr * Pred Grad: -0.002, New P: -0.071
iter 13 loss: 0.399
Actual params: [ 0.7209, -0.0712]
-Original Grad: -0.006, -lr * Pred Grad: -0.009, New P: 0.712
-Original Grad: 0.001, -lr * Pred Grad: -0.008, New P: -0.079
iter 14 loss: 0.400
Actual params: [ 0.7119, -0.0795]
-Original Grad: 0.014, -lr * Pred Grad: -0.006, New P: 0.706
-Original Grad: 0.015, -lr * Pred Grad: -0.009, New P: -0.089
iter 15 loss: 0.401
Actual params: [ 0.7061, -0.0887]
-Original Grad: 0.028, -lr * Pred Grad: 0.006, New P: 0.712
-Original Grad: 0.026, -lr * Pred Grad: -0.003, New P: -0.092
iter 16 loss: 0.401
Actual params: [ 0.7124, -0.0915]
-Original Grad: 0.014, -lr * Pred Grad: 0.006, New P: 0.718
-Original Grad: 0.014, -lr * Pred Grad: -0.004, New P: -0.096
iter 17 loss: 0.401
Actual params: [ 0.7181, -0.0955]
-Original Grad: 0.009, -lr * Pred Grad: -0.001, New P: 0.717
-Original Grad: 0.035, -lr * Pred Grad: 0.003, New P: -0.092
iter 18 loss: 0.401
Actual params: [ 0.7169, -0.0922]
-Original Grad: 0.002, -lr * Pred Grad: -0.007, New P: 0.710
-Original Grad: 0.003, -lr * Pred Grad: -0.007, New P: -0.099
iter 19 loss: 0.402
Actual params: [ 0.71 , -0.099]
-Original Grad: 0.023, -lr * Pred Grad: -0.000, New P: 0.710
-Original Grad: 0.048, -lr * Pred Grad: 0.003, New P: -0.096
iter 20 loss: 0.401
Actual params: [ 0.7099, -0.0963]
-Original Grad: 0.026, -lr * Pred Grad: 0.010, New P: 0.720
-Original Grad: 0.004, -lr * Pred Grad: -0.003, New P: -0.100
