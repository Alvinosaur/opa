Target params: [1.3344, 1.5708]
iter 0 loss: 0.077
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.051, -lr * Pred Grad:  -0.389, New P: -0.861
-Original Grad: -0.002, -lr * Pred Grad:  -0.010, New P: -0.006
iter 1 loss: 0.071
Actual params: [-0.8613, -0.0064]
-Original Grad: -0.004, -lr * Pred Grad:  -0.037, New P: -0.898
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.005
iter 2 loss: 0.071
Actual params: [-0.8984,  0.0051]
-Original Grad: -0.004, -lr * Pred Grad:  -0.039, New P: -0.937
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.015
iter 3 loss: 0.071
Actual params: [-0.9371,  0.0153]
-Original Grad: -0.003, -lr * Pred Grad:  -0.032, New P: -0.969
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.028
iter 4 loss: 0.071
Actual params: [-0.9694,  0.0278]
-Original Grad: -0.003, -lr * Pred Grad:  -0.032, New P: -1.001
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.039
iter 5 loss: 0.071
Actual params: [-1.0011,  0.039 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: -1.023
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.048
iter 6 loss: 0.071
Actual params: [-1.0235,  0.0476]
-Original Grad: -0.002, -lr * Pred Grad:  -0.026, New P: -1.049
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.054
iter 7 loss: 0.071
Actual params: [-1.0491,  0.0542]
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -1.073
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.058
iter 8 loss: 0.071
Actual params: [-1.0733,  0.058 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -1.096
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.064
iter 9 loss: 0.071
Actual params: [-1.0955,  0.0641]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -1.113
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.069
iter 10 loss: 0.071
Actual params: [-1.1134,  0.0689]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -1.132
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.075
iter 11 loss: 0.071
Actual params: [-1.1317,  0.0751]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -1.147
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.081
iter 12 loss: 0.071
Actual params: [-1.1469,  0.0808]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -1.166
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.087
iter 13 loss: 0.071
Actual params: [-1.1657,  0.0873]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -1.182
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.093
iter 14 loss: 0.071
Actual params: [-1.182 ,  0.0929]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.195
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.096
iter 15 loss: 0.071
Actual params: [-1.1953,  0.0964]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -1.215
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.100
iter 16 loss: 0.071
Actual params: [-1.2154,  0.1003]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.229
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.103
iter 17 loss: 0.071
Actual params: [-1.2289,  0.1027]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.242
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.106
iter 18 loss: 0.071
Actual params: [-1.2419,  0.1056]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.259
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.109
iter 19 loss: 0.071
Actual params: [-1.259 ,  0.1092]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.274
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.114
iter 20 loss: 0.071
Actual params: [-1.2735,  0.1136]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.288
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.117
iter 21 loss: 0.071
Actual params: [-1.2877,  0.1172]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.305
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.119
iter 22 loss: 0.071
Actual params: [-1.3052,  0.1192]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.316
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.121
iter 23 loss: 0.071
Actual params: [-1.3158,  0.1213]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.333
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.129
iter 24 loss: 0.071
Actual params: [-1.3333,  0.1292]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.345
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.131
iter 25 loss: 0.071
Actual params: [-1.3453,  0.1313]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.358
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.136
iter 26 loss: 0.071
Actual params: [-1.3576,  0.1361]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.373
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.141
iter 27 loss: 0.071
Actual params: [-1.3729,  0.1411]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.386
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.147
iter 28 loss: 0.071
Actual params: [-1.3863,  0.1471]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.400
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.153
iter 29 loss: 0.071
Actual params: [-1.4004,  0.1535]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.415
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.160
iter 30 loss: 0.071
Actual params: [-1.4149,  0.1601]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.429
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.167
Target params: [1.3344, 1.5708]
iter 0 loss: 0.443
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: -0.459
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.013
iter 1 loss: 0.443
Actual params: [-0.4588,  0.0135]
-Original Grad: 0.002, -lr * Pred Grad:  0.023, New P: -0.436
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.030
iter 2 loss: 0.443
Actual params: [-0.4357,  0.0297]
-Original Grad: 0.002, -lr * Pred Grad:  0.030, New P: -0.406
-Original Grad: 0.001, -lr * Pred Grad:  0.020, New P: 0.050
iter 3 loss: 0.443
Actual params: [-0.4057,  0.0499]
-Original Grad: 0.003, -lr * Pred Grad:  0.044, New P: -0.362
-Original Grad: 0.002, -lr * Pred Grad:  0.029, New P: 0.079
iter 4 loss: 0.443
Actual params: [-0.3616,  0.0786]
-Original Grad: 0.005, -lr * Pred Grad:  0.081, New P: -0.280
-Original Grad: 0.003, -lr * Pred Grad:  0.050, New P: 0.129
iter 5 loss: 0.442
Actual params: [-0.2804,  0.129 ]
-Original Grad: 0.009, -lr * Pred Grad:  0.175, New P: -0.105
-Original Grad: 0.005, -lr * Pred Grad:  0.101, New P: 0.230
iter 6 loss: 0.437
Actual params: [-0.1054,  0.2303]
-Original Grad: 0.092, -lr * Pred Grad:  0.956, New P: 0.851
-Original Grad: 0.053, -lr * Pred Grad:  0.550, New P: 0.780
iter 7 loss: 0.118
Actual params: [0.8507, 0.78  ]
-Original Grad: 0.002, -lr * Pred Grad:  -0.075, New P: 0.776
-Original Grad: 0.337, -lr * Pred Grad:  0.394, New P: 1.174
iter 8 loss: 0.058
Actual params: [0.7756, 1.1741]
-Original Grad: 0.264, -lr * Pred Grad:  0.187, New P: 0.963
-Original Grad: 0.305, -lr * Pred Grad:  0.054, New P: 1.228
iter 9 loss: 0.030
Actual params: [0.9626, 1.2284]
-Original Grad: -0.062, -lr * Pred Grad:  -0.120, New P: 0.843
-Original Grad: 0.203, -lr * Pred Grad:  0.103, New P: 1.332
iter 10 loss: 0.023
Actual params: [0.8427, 1.3316]
-Original Grad: 0.193, -lr * Pred Grad:  0.062, New P: 0.905
-Original Grad: 0.125, -lr * Pred Grad:  0.013, New P: 1.344
iter 11 loss: 0.017
Actual params: [0.9045, 1.3444]
-Original Grad: 0.075, -lr * Pred Grad:  0.009, New P: 0.914
-Original Grad: 0.135, -lr * Pred Grad:  0.033, New P: 1.377
iter 12 loss: 0.014
Actual params: [0.914 , 1.3775]
-Original Grad: 0.134, -lr * Pred Grad:  0.026, New P: 0.940
-Original Grad: 0.083, -lr * Pred Grad:  0.011, New P: 1.389
iter 13 loss: 0.012
Actual params: [0.9403, 1.3889]
-Original Grad: -0.020, -lr * Pred Grad:  -0.014, New P: 0.926
-Original Grad: 0.144, -lr * Pred Grad:  0.034, New P: 1.423
iter 14 loss: 0.011
Actual params: [0.9259, 1.4226]
-Original Grad: 0.036, -lr * Pred Grad:  0.004, New P: 0.929
-Original Grad: 0.110, -lr * Pred Grad:  0.020, New P: 1.443
iter 15 loss: 0.010
Actual params: [0.9295, 1.443 ]
-Original Grad: 0.053, -lr * Pred Grad:  0.011, New P: 0.941
-Original Grad: 0.069, -lr * Pred Grad:  0.011, New P: 1.454
iter 16 loss: 0.009
Actual params: [0.9409, 1.4538]
-Original Grad: -0.011, -lr * Pred Grad:  -0.006, New P: 0.935
-Original Grad: 0.042, -lr * Pred Grad:  0.009, New P: 1.463
iter 17 loss: 0.009
Actual params: [0.9352, 1.4629]
-Original Grad: 0.124, -lr * Pred Grad:  0.018, New P: 0.953
-Original Grad: 0.073, -lr * Pred Grad:  0.011, New P: 1.473
iter 18 loss: 0.008
Actual params: [0.9528, 1.4734]
-Original Grad: 0.020, -lr * Pred Grad:  0.002, New P: 0.954
-Original Grad: 0.061, -lr * Pred Grad:  0.011, New P: 1.485
iter 19 loss: 0.008
Actual params: [0.9543, 1.4845]
-Original Grad: 0.031, -lr * Pred Grad:  0.003, New P: 0.958
-Original Grad: 0.043, -lr * Pred Grad:  0.008, New P: 1.492
iter 20 loss: 0.008
Actual params: [0.9578, 1.4921]
-Original Grad: -0.008, -lr * Pred Grad:  -0.001, New P: 0.957
-Original Grad: -0.025, -lr * Pred Grad:  -0.005, New P: 1.487
iter 21 loss: 0.008
Actual params: [0.9573, 1.4872]
-Original Grad: -0.005, -lr * Pred Grad:  -0.002, New P: 0.955
-Original Grad: 0.035, -lr * Pred Grad:  0.007, New P: 1.494
iter 22 loss: 0.008
Actual params: [0.9554, 1.4945]
-Original Grad: 0.054, -lr * Pred Grad:  0.009, New P: 0.965
-Original Grad: 0.025, -lr * Pred Grad:  0.004, New P: 1.498
iter 23 loss: 0.008
Actual params: [0.9645, 1.4983]
-Original Grad: 0.007, -lr * Pred Grad:  0.000, New P: 0.965
-Original Grad: 0.044, -lr * Pred Grad:  0.009, New P: 1.507
iter 24 loss: 0.008
Actual params: [0.9646, 1.5073]
-Original Grad: 0.007, -lr * Pred Grad:  0.001, New P: 0.966
-Original Grad: 0.002, -lr * Pred Grad:  0.000, New P: 1.507
iter 25 loss: 0.008
Actual params: [0.9661, 1.5074]
-Original Grad: 0.035, -lr * Pred Grad:  0.007, New P: 0.973
-Original Grad: 0.016, -lr * Pred Grad:  0.002, New P: 1.510
iter 26 loss: 0.008
Actual params: [0.9733, 1.5096]
-Original Grad: 0.024, -lr * Pred Grad:  0.006, New P: 0.979
-Original Grad: -0.005, -lr * Pred Grad:  -0.002, New P: 1.508
iter 27 loss: 0.008
Actual params: [0.9791, 1.5078]
-Original Grad: 0.024, -lr * Pred Grad:  0.005, New P: 0.984
-Original Grad: 0.013, -lr * Pred Grad:  0.002, New P: 1.510
iter 28 loss: 0.008
Actual params: [0.9844, 1.5097]
-Original Grad: 0.009, -lr * Pred Grad:  0.003, New P: 0.987
-Original Grad: -0.016, -lr * Pred Grad:  -0.003, New P: 1.506
iter 29 loss: 0.008
Actual params: [0.9872, 1.5064]
-Original Grad: 0.046, -lr * Pred Grad:  0.011, New P: 0.998
-Original Grad: -0.015, -lr * Pred Grad:  -0.004, New P: 1.502
iter 30 loss: 0.008
Actual params: [0.9983, 1.5021]
-Original Grad: 0.022, -lr * Pred Grad:  0.004, New P: 1.002
-Original Grad: 0.047, -lr * Pred Grad:  0.009, New P: 1.511
Target params: [1.3344, 1.5708]
iter 0 loss: 0.725
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.017, -lr * Pred Grad:  0.179, New P: -0.293
-Original Grad: 0.019, -lr * Pred Grad:  0.208, New P: 0.211
iter 1 loss: 0.710
Actual params: [-0.293 ,  0.2112]
-Original Grad: 0.053, -lr * Pred Grad:  0.437, New P: 0.144
-Original Grad: 0.070, -lr * Pred Grad:  0.578, New P: 0.789
iter 2 loss: 0.593
Actual params: [0.1437, 0.789 ]
-Original Grad: 0.183, -lr * Pred Grad:  0.641, New P: 0.785
-Original Grad: 0.123, -lr * Pred Grad:  0.319, New P: 1.108
iter 3 loss: 0.258
Actual params: [0.7848, 1.1081]
-Original Grad: 0.343, -lr * Pred Grad:  0.411, New P: 1.196
-Original Grad: 0.106, -lr * Pred Grad:  -0.029, New P: 1.079
iter 4 loss: 0.213
Actual params: [1.1956, 1.0793]
-Original Grad: -0.018, -lr * Pred Grad:  -0.144, New P: 1.052
-Original Grad: 0.244, -lr * Pred Grad:  0.518, New P: 1.598
iter 5 loss: 0.111
Actual params: [1.0515, 1.5976]
-Original Grad: 0.490, -lr * Pred Grad:  0.120, New P: 1.172
-Original Grad: 0.165, -lr * Pred Grad:  0.014, New P: 1.611
iter 6 loss: 0.093
Actual params: [1.172 , 1.6115]
-Original Grad: 0.122, -lr * Pred Grad:  -0.028, New P: 1.144
-Original Grad: 0.157, -lr * Pred Grad:  0.171, New P: 1.782
iter 7 loss: 0.080
Actual params: [1.1438, 1.7821]
-Original Grad: 0.406, -lr * Pred Grad:  0.069, New P: 1.213
-Original Grad: -0.007, -lr * Pred Grad:  -0.042, New P: 1.740
iter 8 loss: 0.074
Actual params: [1.213 , 1.7403]
-Original Grad: 0.091, -lr * Pred Grad:  0.002, New P: 1.215
-Original Grad: 0.174, -lr * Pred Grad:  0.075, New P: 1.815
iter 9 loss: 0.068
Actual params: [1.2154, 1.8153]
-Original Grad: 0.089, -lr * Pred Grad:  0.012, New P: 1.227
-Original Grad: 0.070, -lr * Pred Grad:  0.023, New P: 1.838
iter 10 loss: 0.065
Actual params: [1.2271, 1.8379]
-Original Grad: 0.134, -lr * Pred Grad:  0.015, New P: 1.242
-Original Grad: 0.151, -lr * Pred Grad:  0.046, New P: 1.884
iter 11 loss: 0.063
Actual params: [1.2419, 1.8841]
-Original Grad: 0.150, -lr * Pred Grad:  0.018, New P: 1.260
-Original Grad: 0.103, -lr * Pred Grad:  0.030, New P: 1.914
iter 12 loss: 0.061
Actual params: [1.26 , 1.914]
-Original Grad: 0.328, -lr * Pred Grad:  0.031, New P: 1.291
-Original Grad: -0.254, -lr * Pred Grad:  -0.040, New P: 1.874
iter 13 loss: 0.058
Actual params: [1.2914, 1.8738]
-Original Grad: -0.026, -lr * Pred Grad:  0.004, New P: 1.295
-Original Grad: 0.174, -lr * Pred Grad:  0.034, New P: 1.908
iter 14 loss: 0.057
Actual params: [1.2951, 1.9077]
-Original Grad: 0.217, -lr * Pred Grad:  0.027, New P: 1.322
-Original Grad: 0.121, -lr * Pred Grad:  0.029, New P: 1.937
iter 15 loss: 0.054
Actual params: [1.3223, 1.9368]
-Original Grad: 0.358, -lr * Pred Grad:  0.026, New P: 1.348
-Original Grad: -0.198, -lr * Pred Grad:  -0.015, New P: 1.922
iter 16 loss: 0.053
Actual params: [1.3482, 1.9217]
-Original Grad: -0.010, -lr * Pred Grad:  0.005, New P: 1.353
-Original Grad: 0.119, -lr * Pred Grad:  0.020, New P: 1.942
iter 17 loss: 0.052
Actual params: [1.3533, 1.9418]
-Original Grad: -0.093, -lr * Pred Grad:  -0.001, New P: 1.352
-Original Grad: 0.163, -lr * Pred Grad:  0.024, New P: 1.966
iter 18 loss: 0.051
Actual params: [1.3524, 1.9658]
-Original Grad: 0.022, -lr * Pred Grad:  0.008, New P: 1.361
-Original Grad: 0.082, -lr * Pred Grad:  0.017, New P: 1.983
iter 19 loss: 0.049
Actual params: [1.3607, 1.9828]
-Original Grad: 0.211, -lr * Pred Grad:  0.018, New P: 1.379
-Original Grad: -0.030, -lr * Pred Grad:  0.011, New P: 1.994
iter 20 loss: 0.047
Actual params: [1.3785, 1.9942]
-Original Grad: -0.012, -lr * Pred Grad:  0.009, New P: 1.387
-Original Grad: 0.115, -lr * Pred Grad:  0.023, New P: 2.017
iter 21 loss: 0.046
Actual params: [1.3873, 2.0172]
-Original Grad: 0.188, -lr * Pred Grad:  0.017, New P: 1.404
-Original Grad: -0.015, -lr * Pred Grad:  0.015, New P: 2.032
iter 22 loss: 0.044
Actual params: [1.4041, 2.0318]
-Original Grad: -0.046, -lr * Pred Grad:  0.006, New P: 1.411
-Original Grad: 0.106, -lr * Pred Grad:  0.019, New P: 2.051
iter 23 loss: 0.043
Actual params: [1.4106, 2.0512]
-Original Grad: 0.293, -lr * Pred Grad:  0.020, New P: 1.431
-Original Grad: -0.035, -lr * Pred Grad:  0.020, New P: 2.071
iter 24 loss: 0.041
Actual params: [1.4309, 2.0711]
-Original Grad: -0.219, -lr * Pred Grad:  -0.003, New P: 1.428
-Original Grad: 0.149, -lr * Pred Grad:  0.012, New P: 2.083
iter 25 loss: 0.040
Actual params: [1.428 , 2.0829]
-Original Grad: 0.245, -lr * Pred Grad:  0.010, New P: 1.438
-Original Grad: -0.104, -lr * Pred Grad:  0.003, New P: 2.086
iter 26 loss: 0.039
Actual params: [1.4378, 2.0855]
-Original Grad: -0.162, -lr * Pred Grad:  0.001, New P: 1.439
-Original Grad: 0.131, -lr * Pred Grad:  0.014, New P: 2.100
iter 27 loss: 0.038
Actual params: [1.4392, 2.0999]
-Original Grad: -0.022, -lr * Pred Grad:  0.011, New P: 1.450
-Original Grad: 0.095, -lr * Pred Grad:  0.024, New P: 2.124
iter 28 loss: 0.037
Actual params: [1.4499, 2.124 ]
-Original Grad: 0.134, -lr * Pred Grad:  0.016, New P: 1.465
-Original Grad: 0.022, -lr * Pred Grad:  0.025, New P: 2.149
iter 29 loss: 0.035
Actual params: [1.4655, 2.149 ]
-Original Grad: 0.105, -lr * Pred Grad:  0.014, New P: 1.480
-Original Grad: 0.030, -lr * Pred Grad:  0.024, New P: 2.173
iter 30 loss: 0.034
Actual params: [1.4799, 2.1733]
-Original Grad: -0.105, -lr * Pred Grad:  0.010, New P: 1.490
-Original Grad: 0.149, -lr * Pred Grad:  0.029, New P: 2.203
Target params: [1.3344, 1.5708]
iter 0 loss: 0.228
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad:  0.026, New P: -0.446
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.010
iter 1 loss: 0.228
Actual params: [-0.4459,  0.0104]
-Original Grad: 0.004, -lr * Pred Grad:  0.045, New P: -0.401
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.022
iter 2 loss: 0.228
Actual params: [-0.4014,  0.0219]
-Original Grad: 0.004, -lr * Pred Grad:  0.055, New P: -0.347
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.037
iter 3 loss: 0.228
Actual params: [-0.3466,  0.0374]
-Original Grad: 0.010, -lr * Pred Grad:  0.154, New P: -0.193
-Original Grad: 0.003, -lr * Pred Grad:  0.042, New P: 0.079
iter 4 loss: 0.226
Actual params: [-0.1928,  0.0793]
-Original Grad: 0.038, -lr * Pred Grad:  0.513, New P: 0.320
-Original Grad: 0.011, -lr * Pred Grad:  0.149, New P: 0.229
iter 5 loss: 0.097
Actual params: [0.3202, 0.2285]
-Original Grad: 0.169, -lr * Pred Grad:  0.391, New P: 0.711
-Original Grad: 0.045, -lr * Pred Grad:  0.180, New P: 0.409
iter 6 loss: 0.177
Actual params: [0.7112, 0.4085]
-Original Grad: -0.074, -lr * Pred Grad:  -0.089, New P: 0.622
-Original Grad: 0.473, -lr * Pred Grad:  0.401, New P: 0.809
iter 7 loss: 0.077
Actual params: [0.6222, 0.8093]
-Original Grad: -0.247, -lr * Pred Grad:  -0.149, New P: 0.474
-Original Grad: 0.341, -lr * Pred Grad:  0.093, New P: 0.902
iter 8 loss: 0.054
Actual params: [0.4735, 0.9024]
-Original Grad: 0.152, -lr * Pred Grad:  0.155, New P: 0.629
-Original Grad: -0.012, -lr * Pred Grad:  0.064, New P: 0.967
iter 9 loss: 0.054
Actual params: [0.6289, 0.9668]
-Original Grad: -0.032, -lr * Pred Grad:  0.044, New P: 0.673
-Original Grad: 0.149, -lr * Pred Grad:  0.079, New P: 1.046
iter 10 loss: 0.048
Actual params: [0.6727, 1.0461]
-Original Grad: -0.014, -lr * Pred Grad:  0.066, New P: 0.739
-Original Grad: 0.183, -lr * Pred Grad:  0.086, New P: 1.132
iter 11 loss: 0.039
Actual params: [0.7389, 1.132 ]
-Original Grad: 0.074, -lr * Pred Grad:  0.085, New P: 0.824
-Original Grad: 0.060, -lr * Pred Grad:  0.052, New P: 1.184
iter 12 loss: 0.035
Actual params: [0.8241, 1.1844]
-Original Grad: -0.170, -lr * Pred Grad:  -0.035, New P: 0.789
-Original Grad: 0.054, -lr * Pred Grad:  -0.004, New P: 1.181
iter 13 loss: 0.034
Actual params: [0.7892, 1.1809]
-Original Grad: 0.064, -lr * Pred Grad:  0.024, New P: 0.813
-Original Grad: 0.053, -lr * Pred Grad:  0.029, New P: 1.210
iter 14 loss: 0.032
Actual params: [0.8133, 1.21  ]
-Original Grad: -0.044, -lr * Pred Grad:  0.000, New P: 0.813
-Original Grad: 0.082, -lr * Pred Grad:  0.024, New P: 1.234
iter 15 loss: 0.030
Actual params: [0.8135, 1.2341]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.816
-Original Grad: 0.019, -lr * Pred Grad:  0.008, New P: 1.242
iter 16 loss: 0.029
Actual params: [0.8156, 1.2416]
-Original Grad: 0.123, -lr * Pred Grad:  0.019, New P: 0.835
-Original Grad: -0.008, -lr * Pred Grad:  0.010, New P: 1.251
iter 17 loss: 0.029
Actual params: [0.8351, 1.2513]
-Original Grad: 0.116, -lr * Pred Grad:  0.017, New P: 0.852
-Original Grad: 0.007, -lr * Pred Grad:  0.014, New P: 1.266
iter 18 loss: 0.028
Actual params: [0.8517, 1.2656]
-Original Grad: -0.088, -lr * Pred Grad:  -0.005, New P: 0.847
-Original Grad: 0.053, -lr * Pred Grad:  0.017, New P: 1.282
iter 19 loss: 0.026
Actual params: [0.8467, 1.2824]
-Original Grad: 0.079, -lr * Pred Grad:  0.012, New P: 0.858
-Original Grad: 0.033, -lr * Pred Grad:  0.024, New P: 1.306
iter 20 loss: 0.025
Actual params: [0.8583, 1.3062]
-Original Grad: -0.032, -lr * Pred Grad:  0.001, New P: 0.859
-Original Grad: 0.040, -lr * Pred Grad:  0.018, New P: 1.324
iter 21 loss: 0.024
Actual params: [0.8592, 1.3242]
-Original Grad: 0.015, -lr * Pred Grad:  0.005, New P: 0.865
-Original Grad: 0.032, -lr * Pred Grad:  0.020, New P: 1.344
iter 22 loss: 0.022
Actual params: [0.8646, 1.344 ]
-Original Grad: 0.098, -lr * Pred Grad:  0.012, New P: 0.876
-Original Grad: 0.010, -lr * Pred Grad:  0.017, New P: 1.361
iter 23 loss: 0.021
Actual params: [0.8765, 1.3614]
-Original Grad: -0.010, -lr * Pred Grad:  0.006, New P: 0.882
-Original Grad: 0.061, -lr * Pred Grad:  0.036, New P: 1.397
iter 24 loss: 0.020
Actual params: [0.8823, 1.397 ]
-Original Grad: 0.205, -lr * Pred Grad:  0.020, New P: 0.903
-Original Grad: 0.058, -lr * Pred Grad:  0.046, New P: 1.443
iter 25 loss: 0.017
Actual params: [0.9027, 1.443 ]
-Original Grad: 0.065, -lr * Pred Grad:  0.007, New P: 0.910
-Original Grad: 0.022, -lr * Pred Grad:  0.016, New P: 1.459
iter 26 loss: 0.017
Actual params: [0.9099, 1.4594]
-Original Grad: 0.047, -lr * Pred Grad:  0.003, New P: 0.913
-Original Grad: -0.022, -lr * Pred Grad:  -0.008, New P: 1.451
iter 27 loss: 0.017
Actual params: [0.9127, 1.4515]
-Original Grad: 0.021, -lr * Pred Grad:  0.005, New P: 0.918
-Original Grad: 0.049, -lr * Pred Grad:  0.025, New P: 1.476
iter 28 loss: 0.016
Actual params: [0.918, 1.476]
-Original Grad: 0.038, -lr * Pred Grad:  0.005, New P: 0.923
-Original Grad: 0.009, -lr * Pred Grad:  0.007, New P: 1.483
iter 29 loss: 0.016
Actual params: [0.9228, 1.483 ]
-Original Grad: 0.139, -lr * Pred Grad:  0.014, New P: 0.936
-Original Grad: -0.015, -lr * Pred Grad:  0.002, New P: 1.485
iter 30 loss: 0.015
Actual params: [0.9364, 1.4853]
-Original Grad: 0.077, -lr * Pred Grad:  0.009, New P: 0.946
-Original Grad: 0.019, -lr * Pred Grad:  0.013, New P: 1.499
Target params: [1.3344, 1.5708]
iter 0 loss: 0.466
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.487
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.009
iter 1 loss: 0.466
Actual params: [-0.4874,  0.0089]
-Original Grad: -0.002, -lr * Pred Grad:  -0.021, New P: -0.508
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.016
iter 2 loss: 0.466
Actual params: [-0.5083,  0.0155]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.522
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.020
iter 3 loss: 0.466
Actual params: [-0.5223,  0.0205]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.538
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.026
iter 4 loss: 0.466
Actual params: [-0.5375,  0.0261]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.552
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.032
iter 5 loss: 0.466
Actual params: [-0.5525,  0.0316]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.567
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.037
iter 6 loss: 0.466
Actual params: [-0.5674,  0.0374]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.580
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.042
iter 7 loss: 0.466
Actual params: [-0.5798,  0.0422]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.594
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.048
iter 8 loss: 0.466
Actual params: [-0.594 ,  0.0478]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.606
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.053
iter 9 loss: 0.466
Actual params: [-0.6062,  0.0527]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.614
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.056
iter 10 loss: 0.466
Actual params: [-0.6145,  0.0561]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.627
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.061
iter 11 loss: 0.466
Actual params: [-0.6272,  0.0614]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.639
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.067
iter 12 loss: 0.466
Actual params: [-0.6395,  0.0666]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.650
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.071
iter 13 loss: 0.466
Actual params: [-0.6502,  0.0713]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.077
iter 14 loss: 0.466
Actual params: [-0.6628,  0.0769]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.677
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.084
iter 15 loss: 0.466
Actual params: [-0.6774,  0.0836]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.688
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.088
iter 16 loss: 0.466
Actual params: [-0.6877,  0.0885]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.699
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.094
iter 17 loss: 0.466
Actual params: [-0.6988,  0.0939]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.712
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.100
iter 18 loss: 0.466
Actual params: [-0.7118,  0.1004]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.723
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.106
iter 19 loss: 0.466
Actual params: [-0.7225,  0.106 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.732
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.111
iter 20 loss: 0.466
Actual params: [-0.7323,  0.1112]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.741
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.116
iter 21 loss: 0.466
Actual params: [-0.741 ,  0.1159]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.752
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.122
iter 22 loss: 0.466
Actual params: [-0.752,  0.122]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.763
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.128
iter 23 loss: 0.466
Actual params: [-0.7627,  0.1283]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.772
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.134
iter 24 loss: 0.466
Actual params: [-0.7723,  0.134 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.781
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.139
iter 25 loss: 0.466
Actual params: [-0.7811,  0.1394]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.791
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.145
iter 26 loss: 0.466
Actual params: [-0.7907,  0.1455]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.801
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.153
iter 27 loss: 0.466
Actual params: [-0.8013,  0.1526]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.811
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.159
iter 28 loss: 0.466
Actual params: [-0.8108,  0.1593]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.820
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.166
iter 29 loss: 0.466
Actual params: [-0.8197,  0.1657]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.829
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.173
iter 30 loss: 0.466
Actual params: [-0.8286,  0.1725]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.836
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.178
Target params: [1.3344, 1.5708]
iter 0 loss: 0.228
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad:  0.027, New P: -0.446
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.010
iter 1 loss: 0.228
Actual params: [-0.4458,  0.0104]
-Original Grad: 0.004, -lr * Pred Grad:  0.048, New P: -0.397
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.023
iter 2 loss: 0.228
Actual params: [-0.3975,  0.023 ]
-Original Grad: 0.004, -lr * Pred Grad:  0.056, New P: -0.341
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.039
iter 3 loss: 0.228
Actual params: [-0.341 ,  0.0386]
-Original Grad: 0.008, -lr * Pred Grad:  0.123, New P: -0.218
-Original Grad: 0.002, -lr * Pred Grad:  0.035, New P: 0.073
iter 4 loss: 0.226
Actual params: [-0.2175,  0.0734]
-Original Grad: 0.029, -lr * Pred Grad:  0.438, New P: 0.221
-Original Grad: 0.008, -lr * Pred Grad:  0.128, New P: 0.202
iter 5 loss: 0.139
Actual params: [0.2206, 0.2016]
-Original Grad: 0.132, -lr * Pred Grad:  0.529, New P: 0.749
-Original Grad: 0.044, -lr * Pred Grad:  0.216, New P: 0.418
iter 6 loss: 0.196
Actual params: [0.7493, 0.418 ]
-Original Grad: -0.082, -lr * Pred Grad:  -0.089, New P: 0.660
-Original Grad: 0.474, -lr * Pred Grad:  0.389, New P: 0.807
iter 7 loss: 0.085
Actual params: [0.6603, 0.8072]
-Original Grad: -0.134, -lr * Pred Grad:  -0.128, New P: 0.533
-Original Grad: 0.347, -lr * Pred Grad:  0.125, New P: 0.933
iter 8 loss: 0.053
Actual params: [0.5326, 0.9325]
-Original Grad: 0.051, -lr * Pred Grad:  0.083, New P: 0.615
-Original Grad: 0.068, -lr * Pred Grad:  0.063, New P: 0.995
iter 9 loss: 0.051
Actual params: [0.6152, 0.9952]
-Original Grad: -0.024, -lr * Pred Grad:  0.037, New P: 0.652
-Original Grad: 0.134, -lr * Pred Grad:  0.066, New P: 1.061
iter 10 loss: 0.046
Actual params: [0.652 , 1.0615]
-Original Grad: 0.037, -lr * Pred Grad:  0.076, New P: 0.728
-Original Grad: 0.114, -lr * Pred Grad:  0.070, New P: 1.132
iter 11 loss: 0.039
Actual params: [0.7282, 1.1319]
-Original Grad: 0.051, -lr * Pred Grad:  0.072, New P: 0.800
-Original Grad: 0.097, -lr * Pred Grad:  0.056, New P: 1.188
iter 12 loss: 0.034
Actual params: [0.7998, 1.1883]
-Original Grad: 0.076, -lr * Pred Grad:  0.055, New P: 0.855
-Original Grad: 0.052, -lr * Pred Grad:  0.035, New P: 1.224
iter 13 loss: 0.032
Actual params: [0.8552, 1.2237]
-Original Grad: 0.070, -lr * Pred Grad:  0.030, New P: 0.885
-Original Grad: 0.030, -lr * Pred Grad:  0.021, New P: 1.245
iter 14 loss: 0.030
Actual params: [0.885 , 1.2451]
-Original Grad: -0.136, -lr * Pred Grad:  -0.029, New P: 0.856
-Original Grad: 0.106, -lr * Pred Grad:  0.018, New P: 1.263
iter 15 loss: 0.028
Actual params: [0.8557, 1.2635]
-Original Grad: -0.128, -lr * Pred Grad:  -0.017, New P: 0.839
-Original Grad: 0.065, -lr * Pred Grad:  0.010, New P: 1.273
iter 16 loss: 0.027
Actual params: [0.8386, 1.2733]
-Original Grad: 0.005, -lr * Pred Grad:  0.005, New P: 0.844
-Original Grad: 0.033, -lr * Pred Grad:  0.015, New P: 1.288
iter 17 loss: 0.026
Actual params: [0.8436, 1.2881]
-Original Grad: 0.111, -lr * Pred Grad:  0.016, New P: 0.860
-Original Grad: 0.011, -lr * Pred Grad:  0.017, New P: 1.305
iter 18 loss: 0.025
Actual params: [0.86  , 1.3052]
-Original Grad: -0.118, -lr * Pred Grad:  -0.006, New P: 0.854
-Original Grad: 0.070, -lr * Pred Grad:  0.020, New P: 1.325
iter 19 loss: 0.024
Actual params: [0.8536, 1.3255]
-Original Grad: -0.032, -lr * Pred Grad:  0.001, New P: 0.855
-Original Grad: 0.043, -lr * Pred Grad:  0.018, New P: 1.344
iter 20 loss: 0.023
Actual params: [0.855 , 1.3436]
-Original Grad: -0.035, -lr * Pred Grad:  0.000, New P: 0.855
-Original Grad: 0.033, -lr * Pred Grad:  0.014, New P: 1.358
iter 21 loss: 0.022
Actual params: [0.8552, 1.3575]
-Original Grad: -0.067, -lr * Pred Grad:  -0.003, New P: 0.853
-Original Grad: 0.036, -lr * Pred Grad:  0.013, New P: 1.370
iter 22 loss: 0.021
Actual params: [0.8526, 1.3704]
-Original Grad: 0.118, -lr * Pred Grad:  0.013, New P: 0.865
-Original Grad: -0.001, -lr * Pred Grad:  0.016, New P: 1.386
iter 23 loss: 0.021
Actual params: [0.8652, 1.3862]
-Original Grad: 0.047, -lr * Pred Grad:  0.008, New P: 0.873
-Original Grad: 0.021, -lr * Pred Grad:  0.021, New P: 1.407
iter 24 loss: 0.020
Actual params: [0.8734, 1.407 ]
-Original Grad: 0.102, -lr * Pred Grad:  0.014, New P: 0.887
-Original Grad: 0.041, -lr * Pred Grad:  0.037, New P: 1.444
iter 25 loss: 0.018
Actual params: [0.8872, 1.4435]
-Original Grad: 0.120, -lr * Pred Grad:  0.013, New P: 0.900
-Original Grad: 0.021, -lr * Pred Grad:  0.025, New P: 1.469
iter 26 loss: 0.017
Actual params: [0.9002, 1.4687]
-Original Grad: 0.057, -lr * Pred Grad:  0.010, New P: 0.911
-Original Grad: 0.051, -lr * Pred Grad:  0.035, New P: 1.504
iter 27 loss: 0.016
Actual params: [0.9106, 1.5038]
-Original Grad: 0.117, -lr * Pred Grad:  0.010, New P: 0.921
-Original Grad: -0.012, -lr * Pred Grad:  0.005, New P: 1.508
iter 28 loss: 0.016
Actual params: [0.9209, 1.5084]
-Original Grad: 0.074, -lr * Pred Grad:  0.006, New P: 0.927
-Original Grad: -0.013, -lr * Pred Grad:  -0.001, New P: 1.508
iter 29 loss: 0.016
Actual params: [0.9273, 1.5078]
-Original Grad: 0.069, -lr * Pred Grad:  0.009, New P: 0.936
-Original Grad: 0.017, -lr * Pred Grad:  0.016, New P: 1.524
iter 30 loss: 0.016
Actual params: [0.936 , 1.5236]
-Original Grad: 0.070, -lr * Pred Grad:  0.009, New P: 0.945
-Original Grad: 0.009, -lr * Pred Grad:  0.011, New P: 1.535
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.131, -lr * Pred Grad:  0.546, New P: 0.074
-Original Grad: 0.013, -lr * Pred Grad:  0.052, New P: 0.056
iter 1 loss: 0.050
Actual params: [0.074 , 0.0559]
-Original Grad: 0.157, -lr * Pred Grad:  0.209, New P: 0.283
-Original Grad: 0.059, -lr * Pred Grad:  0.240, New P: 0.296
iter 2 loss: 0.036
Actual params: [0.2834, 0.2963]
-Original Grad: -0.046, -lr * Pred Grad:  0.014, New P: 0.297
-Original Grad: -0.029, -lr * Pred Grad:  -0.086, New P: 0.210
iter 3 loss: 0.034
Actual params: [0.2974, 0.21  ]
-Original Grad: -0.229, -lr * Pred Grad:  0.012, New P: 0.310
-Original Grad: -0.106, -lr * Pred Grad:  -0.122, New P: 0.088
iter 4 loss: 0.034
Actual params: [0.3095, 0.0877]
-Original Grad: 0.075, -lr * Pred Grad:  -0.038, New P: 0.272
-Original Grad: 0.058, -lr * Pred Grad:  0.140, New P: 0.228
iter 5 loss: 0.033
Actual params: [0.2718, 0.2279]
-Original Grad: 0.021, -lr * Pred Grad:  0.026, New P: 0.298
-Original Grad: -0.017, -lr * Pred Grad:  -0.077, New P: 0.151
iter 6 loss: 0.033
Actual params: [0.2975, 0.1512]
-Original Grad: -0.006, -lr * Pred Grad:  -0.020, New P: 0.277
-Original Grad: 0.033, -lr * Pred Grad:  0.077, New P: 0.228
iter 7 loss: 0.033
Actual params: [0.2772, 0.2279]
-Original Grad: -0.204, -lr * Pred Grad:  -0.005, New P: 0.272
-Original Grad: -0.070, -lr * Pred Grad:  -0.028, New P: 0.200
iter 8 loss: 0.033
Actual params: [0.2722, 0.1995]
-Original Grad: 0.055, -lr * Pred Grad:  0.008, New P: 0.280
-Original Grad: 0.009, -lr * Pred Grad:  -0.016, New P: 0.183
iter 9 loss: 0.033
Actual params: [0.2804, 0.1833]
-Original Grad: 0.114, -lr * Pred Grad:  0.006, New P: 0.286
-Original Grad: 0.036, -lr * Pred Grad:  0.008, New P: 0.191
iter 10 loss: 0.033
Actual params: [0.2862, 0.1909]
-Original Grad: -0.301, -lr * Pred Grad:  -0.010, New P: 0.277
-Original Grad: -0.104, -lr * Pred Grad:  -0.021, New P: 0.170
iter 11 loss: 0.032
Actual params: [0.2767, 0.1704]
-Original Grad: -0.045, -lr * Pred Grad:  -0.026, New P: 0.250
-Original Grad: 0.038, -lr * Pred Grad:  0.086, New P: 0.256
iter 12 loss: 0.033
Actual params: [0.2503, 0.2564]
-Original Grad: 0.049, -lr * Pred Grad:  0.010, New P: 0.260
-Original Grad: -0.001, -lr * Pred Grad:  -0.027, New P: 0.230
iter 13 loss: 0.033
Actual params: [0.2603, 0.2296]
-Original Grad: 0.067, -lr * Pred Grad:  0.001, New P: 0.262
-Original Grad: 0.023, -lr * Pred Grad:  0.008, New P: 0.237
iter 14 loss: 0.033
Actual params: [0.2616, 0.2371]
-Original Grad: -0.049, -lr * Pred Grad:  0.008, New P: 0.270
-Original Grad: -0.034, -lr * Pred Grad:  -0.038, New P: 0.199
iter 15 loss: 0.033
Actual params: [0.2701, 0.1995]
-Original Grad: -0.038, -lr * Pred Grad:  0.002, New P: 0.272
-Original Grad: -0.019, -lr * Pred Grad:  -0.014, New P: 0.185
iter 16 loss: 0.032
Actual params: [0.2724, 0.185 ]
-Original Grad: -0.063, -lr * Pred Grad:  -0.003, New P: 0.270
-Original Grad: -0.018, -lr * Pred Grad:  -0.000, New P: 0.185
iter 17 loss: 0.032
Actual params: [0.2697, 0.1849]
-Original Grad: 0.035, -lr * Pred Grad:  0.010, New P: 0.280
-Original Grad: -0.007, -lr * Pred Grad:  -0.031, New P: 0.154
iter 18 loss: 0.033
Actual params: [0.2801, 0.1536]
-Original Grad: 0.156, -lr * Pred Grad:  -0.002, New P: 0.278
-Original Grad: 0.062, -lr * Pred Grad:  0.035, New P: 0.189
iter 19 loss: 0.033
Actual params: [0.2777, 0.189 ]
-Original Grad: -0.032, -lr * Pred Grad:  -0.013, New P: 0.265
-Original Grad: 0.010, -lr * Pred Grad:  0.040, New P: 0.229
iter 20 loss: 0.033
Actual params: [0.265 , 0.2294]
-Original Grad: 0.052, -lr * Pred Grad:  0.020, New P: 0.285
-Original Grad: -0.016, -lr * Pred Grad:  -0.061, New P: 0.168
iter 21 loss: 0.032
Actual params: [0.2847, 0.1681]
-Original Grad: -0.066, -lr * Pred Grad:  -0.008, New P: 0.277
-Original Grad: -0.011, -lr * Pred Grad:  0.016, New P: 0.184
iter 22 loss: 0.032
Actual params: [0.2771, 0.1842]
-Original Grad: 0.059, -lr * Pred Grad:  0.007, New P: 0.284
-Original Grad: 0.010, -lr * Pred Grad:  -0.014, New P: 0.170
iter 23 loss: 0.032
Actual params: [0.2839, 0.17  ]
-Original Grad: 0.043, -lr * Pred Grad:  0.001, New P: 0.285
-Original Grad: 0.014, -lr * Pred Grad:  0.004, New P: 0.174
iter 24 loss: 0.033
Actual params: [0.2849, 0.1738]
-Original Grad: 0.074, -lr * Pred Grad:  0.011, New P: 0.296
-Original Grad: 0.011, -lr * Pred Grad:  -0.023, New P: 0.151
iter 25 loss: 0.033
Actual params: [0.2956, 0.1505]
-Original Grad: 0.007, -lr * Pred Grad:  -0.009, New P: 0.286
-Original Grad: 0.031, -lr * Pred Grad:  0.039, New P: 0.190
iter 26 loss: 0.033
Actual params: [0.2862, 0.19  ]
-Original Grad: -0.299, -lr * Pred Grad:  -0.005, New P: 0.281
-Original Grad: -0.101, -lr * Pred Grad:  -0.026, New P: 0.164
iter 27 loss: 0.032
Actual params: [0.2812, 0.1643]
-Original Grad: 0.159, -lr * Pred Grad:  -0.006, New P: 0.275
-Original Grad: 0.076, -lr * Pred Grad:  0.046, New P: 0.211
iter 28 loss: 0.033
Actual params: [0.2754, 0.2107]
-Original Grad: 0.021, -lr * Pred Grad:  0.004, New P: 0.280
-Original Grad: -0.002, -lr * Pred Grad:  -0.012, New P: 0.199
iter 29 loss: 0.033
Actual params: [0.2797, 0.1988]
-Original Grad: -0.065, -lr * Pred Grad:  0.001, New P: 0.281
-Original Grad: -0.027, -lr * Pred Grad:  -0.012, New P: 0.187
iter 30 loss: 0.033
Actual params: [0.2805, 0.1871]
-Original Grad: -0.193, -lr * Pred Grad:  0.001, New P: 0.281
-Original Grad: -0.076, -lr * Pred Grad:  -0.026, New P: 0.162
Target params: [1.3344, 1.5708]
iter 0 loss: 0.008
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.029, -lr * Pred Grad:  0.196, New P: -0.276
-Original Grad: -0.011, -lr * Pred Grad:  -0.074, New P: -0.070
iter 1 loss: 0.004
Actual params: [-0.2759, -0.0701]
-Original Grad: 0.046, -lr * Pred Grad:  0.070, New P: -0.205
-Original Grad: -0.013, -lr * Pred Grad:  -0.020, New P: -0.090
iter 2 loss: 0.002
Actual params: [-0.2055, -0.0902]
-Original Grad: 0.022, -lr * Pred Grad:  0.017, New P: -0.188
-Original Grad: -0.007, -lr * Pred Grad:  -0.018, New P: -0.108
iter 3 loss: 0.002
Actual params: [-0.1881, -0.108 ]
-Original Grad: 0.016, -lr * Pred Grad:  0.016, New P: -0.173
-Original Grad: -0.003, -lr * Pred Grad:  0.006, New P: -0.102
iter 4 loss: 0.002
Actual params: [-0.1726, -0.1021]
-Original Grad: 0.002, -lr * Pred Grad:  -0.001, New P: -0.174
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.115
iter 5 loss: 0.002
Actual params: [-0.1736, -0.1152]
-Original Grad: 0.002, -lr * Pred Grad:  -0.001, New P: -0.174
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.126
iter 6 loss: 0.002
Actual params: [-0.1743, -0.1261]
-Original Grad: 0.002, -lr * Pred Grad:  0.002, New P: -0.172
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: -0.128
iter 7 loss: 0.002
Actual params: [-0.172 , -0.1282]
-Original Grad: 0.007, -lr * Pred Grad:  0.004, New P: -0.168
-Original Grad: -0.003, -lr * Pred Grad:  -0.018, New P: -0.146
iter 8 loss: 0.002
Actual params: [-0.1683, -0.1464]
-Original Grad: 0.002, -lr * Pred Grad:  -0.001, New P: -0.169
-Original Grad: -0.002, -lr * Pred Grad:  -0.017, New P: -0.163
iter 9 loss: 0.002
Actual params: [-0.1691, -0.1633]
-Original Grad: 0.001, -lr * Pred Grad:  -0.001, New P: -0.170
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.178
iter 10 loss: 0.002
Actual params: [-0.1702, -0.1784]
-Original Grad: 0.001, -lr * Pred Grad:  -0.002, New P: -0.172
-Original Grad: -0.002, -lr * Pred Grad:  -0.021, New P: -0.199
iter 11 loss: 0.002
Actual params: [-0.1723, -0.1993]
-Original Grad: 0.007, -lr * Pred Grad:  0.014, New P: -0.158
-Original Grad: 0.001, -lr * Pred Grad:  0.026, New P: -0.173
iter 12 loss: 0.002
Actual params: [-0.1583, -0.1729]
-Original Grad: 0.003, -lr * Pred Grad:  0.003, New P: -0.155
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.183
iter 13 loss: 0.002
Actual params: [-0.1554, -0.1831]
-Original Grad: 0.006, -lr * Pred Grad:  0.014, New P: -0.141
-Original Grad: 0.002, -lr * Pred Grad:  0.033, New P: -0.150
iter 14 loss: 0.002
Actual params: [-0.1414, -0.1504]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -0.145
-Original Grad: -0.003, -lr * Pred Grad:  -0.042, New P: -0.192
iter 15 loss: 0.002
Actual params: [-0.1449, -0.1924]
-Original Grad: 0.004, -lr * Pred Grad:  0.010, New P: -0.135
-Original Grad: 0.001, -lr * Pred Grad:  0.021, New P: -0.171
iter 16 loss: 0.002
Actual params: [-0.1351, -0.1712]
-Original Grad: 0.003, -lr * Pred Grad:  0.005, New P: -0.130
-Original Grad: -0.001, -lr * Pred Grad:  -0.007, New P: -0.178
iter 17 loss: 0.002
Actual params: [-0.1301, -0.1777]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.133
-Original Grad: -0.003, -lr * Pred Grad:  -0.044, New P: -0.222
iter 18 loss: 0.002
Actual params: [-0.1331, -0.222 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.005, New P: -0.128
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.212
iter 19 loss: 0.002
Actual params: [-0.1278, -0.2125]
-Original Grad: 0.006, -lr * Pred Grad:  0.014, New P: -0.114
-Original Grad: 0.002, -lr * Pred Grad:  0.023, New P: -0.190
iter 20 loss: 0.002
Actual params: [-0.1136, -0.1899]
-Original Grad: -0.005, -lr * Pred Grad:  -0.009, New P: -0.122
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: -0.178
iter 21 loss: 0.002
Actual params: [-0.1223, -0.1784]
-Original Grad: 0.002, -lr * Pred Grad:  0.005, New P: -0.118
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -0.186
iter 22 loss: 0.002
Actual params: [-0.1178, -0.1859]
-Original Grad: -0.005, -lr * Pred Grad:  -0.008, New P: -0.126
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.183
iter 23 loss: 0.002
Actual params: [-0.1263, -0.1825]
-Original Grad: 0.003, -lr * Pred Grad:  0.006, New P: -0.121
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.187
iter 24 loss: 0.002
Actual params: [-0.1208, -0.1873]
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: -0.122
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.196
iter 25 loss: 0.002
Actual params: [-0.1224, -0.1963]
-Original Grad: -0.009, -lr * Pred Grad:  -0.012, New P: -0.134
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.212
iter 26 loss: 0.002
Actual params: [-0.1345, -0.212 ]
-Original Grad: 0.003, -lr * Pred Grad:  0.005, New P: -0.130
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: -0.204
iter 27 loss: 0.002
Actual params: [-0.1297, -0.2043]
-Original Grad: -0.003, -lr * Pred Grad:  -0.004, New P: -0.133
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: -0.192
iter 28 loss: 0.002
Actual params: [-0.1333, -0.1925]
-Original Grad: 0.004, -lr * Pred Grad:  0.006, New P: -0.127
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: -0.186
iter 29 loss: 0.002
Actual params: [-0.127 , -0.1859]
-Original Grad: 0.001, -lr * Pred Grad:  0.003, New P: -0.124
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.204
iter 30 loss: 0.002
Actual params: [-0.1243, -0.2042]
-Original Grad: 0.005, -lr * Pred Grad:  0.008, New P: -0.116
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.194
Target params: [1.3344, 1.5708]
iter 0 loss: 0.537
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -0.497
-Original Grad: 0.025, -lr * Pred Grad:  0.268, New P: 0.271
iter 1 loss: 0.528
Actual params: [-0.4966,  0.2711]
-Original Grad: 0.010, -lr * Pred Grad:  0.118, New P: -0.378
-Original Grad: 0.023, -lr * Pred Grad:  0.266, New P: 0.537
iter 2 loss: 0.514
Actual params: [-0.3784,  0.5368]
-Original Grad: 0.030, -lr * Pred Grad:  0.364, New P: -0.014
-Original Grad: 0.025, -lr * Pred Grad:  0.283, New P: 0.820
iter 3 loss: 0.469
Actual params: [-0.0144,  0.8197]
-Original Grad: 0.055, -lr * Pred Grad:  0.635, New P: 0.620
-Original Grad: 0.004, -lr * Pred Grad:  0.003, New P: 0.823
iter 4 loss: 0.349
Actual params: [0.6203, 0.8227]
-Original Grad: 0.079, -lr * Pred Grad:  0.492, New P: 1.112
-Original Grad: 0.098, -lr * Pred Grad:  0.722, New P: 1.545
iter 5 loss: 0.121
Actual params: [1.1124, 1.5451]
-Original Grad: 0.363, -lr * Pred Grad:  0.209, New P: 1.321
-Original Grad: 0.094, -lr * Pred Grad:  0.099, New P: 1.644
iter 6 loss: 0.100
Actual params: [1.321 , 1.6439]
-Original Grad: 0.111, -lr * Pred Grad:  -0.019, New P: 1.302
-Original Grad: 0.101, -lr * Pred Grad:  0.315, New P: 1.959
iter 7 loss: 0.082
Actual params: [1.3016, 1.9586]
-Original Grad: 0.160, -lr * Pred Grad:  0.120, New P: 1.421
-Original Grad: 0.006, -lr * Pred Grad:  -0.149, New P: 1.810
iter 8 loss: 0.090
Actual params: [1.4213, 1.81  ]
-Original Grad: 0.011, -lr * Pred Grad:  -0.048, New P: 1.373
-Original Grad: 0.091, -lr * Pred Grad:  0.241, New P: 2.051
iter 9 loss: 0.079
Actual params: [1.3733, 2.0515]
-Original Grad: 0.168, -lr * Pred Grad:  0.091, New P: 1.464
-Original Grad: 0.016, -lr * Pred Grad:  -0.059, New P: 1.992
iter 10 loss: 0.080
Actual params: [1.464, 1.992]
-Original Grad: -0.040, -lr * Pred Grad:  -0.035, New P: 1.429
-Original Grad: 0.072, -lr * Pred Grad:  0.139, New P: 2.131
iter 11 loss: 0.078
Actual params: [1.4294, 2.1314]
-Original Grad: 0.115, -lr * Pred Grad:  0.053, New P: 1.482
-Original Grad: -0.001, -lr * Pred Grad:  -0.027, New P: 2.105
iter 12 loss: 0.077
Actual params: [1.482 , 2.1049]
-Original Grad: -0.079, -lr * Pred Grad:  -0.031, New P: 1.451
-Original Grad: 0.081, -lr * Pred Grad:  0.110, New P: 2.214
iter 13 loss: 0.077
Actual params: [1.4513, 2.2144]
-Original Grad: 0.074, -lr * Pred Grad:  0.032, New P: 1.483
-Original Grad: -0.005, -lr * Pred Grad:  -0.009, New P: 2.206
iter 14 loss: 0.077
Actual params: [1.4828, 2.2059]
-Original Grad: 0.067, -lr * Pred Grad:  0.031, New P: 1.514
-Original Grad: -0.023, -lr * Pred Grad:  -0.036, New P: 2.170
iter 15 loss: 0.077
Actual params: [1.5138, 2.1703]
-Original Grad: -0.079, -lr * Pred Grad:  -0.031, New P: 1.483
-Original Grad: 0.026, -lr * Pred Grad:  0.027, New P: 2.198
iter 16 loss: 0.077
Actual params: [1.4829, 2.1976]
-Original Grad: 0.092, -lr * Pred Grad:  0.038, New P: 1.521
-Original Grad: -0.017, -lr * Pred Grad:  -0.016, New P: 2.182
iter 17 loss: 0.077
Actual params: [1.5211, 2.1821]
-Original Grad: -0.105, -lr * Pred Grad:  -0.033, New P: 1.488
-Original Grad: 0.053, -lr * Pred Grad:  0.051, New P: 2.233
iter 18 loss: 0.076
Actual params: [1.4883, 2.2326]
-Original Grad: 0.071, -lr * Pred Grad:  0.028, New P: 1.516
-Original Grad: -0.014, -lr * Pred Grad:  -0.004, New P: 2.228
iter 19 loss: 0.076
Actual params: [1.5159, 2.2282]
-Original Grad: -0.037, -lr * Pred Grad:  -0.008, New P: 1.508
-Original Grad: 0.035, -lr * Pred Grad:  0.042, New P: 2.270
iter 20 loss: 0.075
Actual params: [1.5079, 2.2698]
-Original Grad: 0.048, -lr * Pred Grad:  0.011, New P: 1.519
-Original Grad: -0.047, -lr * Pred Grad:  -0.059, New P: 2.211
iter 21 loss: 0.076
Actual params: [1.5186, 2.2105]
-Original Grad: -0.031, -lr * Pred Grad:  -0.008, New P: 1.511
-Original Grad: 0.024, -lr * Pred Grad:  0.025, New P: 2.236
iter 22 loss: 0.075
Actual params: [1.5107, 2.2357]
-Original Grad: 0.055, -lr * Pred Grad:  0.017, New P: 1.528
-Original Grad: -0.035, -lr * Pred Grad:  -0.038, New P: 2.198
iter 23 loss: 0.077
Actual params: [1.528 , 2.1979]
-Original Grad: -0.123, -lr * Pred Grad:  -0.038, New P: 1.490
-Original Grad: 0.055, -lr * Pred Grad:  0.026, New P: 2.224
iter 24 loss: 0.076
Actual params: [1.4896, 2.2237]
-Original Grad: 0.068, -lr * Pred Grad:  0.011, New P: 1.501
-Original Grad: -0.051, -lr * Pred Grad:  -0.052, New P: 2.172
iter 25 loss: 0.077
Actual params: [1.5007, 2.172 ]
-Original Grad: 0.024, -lr * Pred Grad:  0.006, New P: 1.507
-Original Grad: -0.015, -lr * Pred Grad:  -0.013, New P: 2.159
iter 26 loss: 0.077
Actual params: [1.5068, 2.1591]
-Original Grad: 0.003, -lr * Pred Grad:  0.023, New P: 1.530
-Original Grad: 0.038, -lr * Pred Grad:  0.062, New P: 2.221
iter 27 loss: 0.076
Actual params: [1.5299, 2.2207]
-Original Grad: 0.014, -lr * Pred Grad:  0.008, New P: 1.538
-Original Grad: -0.001, -lr * Pred Grad:  0.006, New P: 2.226
iter 28 loss: 0.076
Actual params: [1.5383, 2.2265]
-Original Grad: -0.043, -lr * Pred Grad:  -0.029, New P: 1.510
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: 2.198
iter 29 loss: 0.076
Actual params: [1.5097, 2.1983]
-Original Grad: -0.136, -lr * Pred Grad:  -0.033, New P: 1.477
-Original Grad: 0.078, -lr * Pred Grad:  0.037, New P: 2.235
iter 30 loss: 0.076
Actual params: [1.4767, 2.2355]
-Original Grad: 0.105, -lr * Pred Grad:  0.043, New P: 1.519
-Original Grad: -0.031, -lr * Pred Grad:  0.014, New P: 2.250
Target params: [1.3344, 1.5708]
iter 0 loss: 0.577
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.472
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.005
iter 1 loss: 0.577
Actual params: [-0.4717,  0.0049]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.469
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.010
iter 2 loss: 0.576
Actual params: [-0.4693,  0.0097]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.463
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.019
iter 3 loss: 0.576
Actual params: [-0.4632,  0.0188]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.458
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.030
iter 4 loss: 0.576
Actual params: [-0.458 ,  0.0301]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.453
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.040
iter 5 loss: 0.576
Actual params: [-0.4532,  0.0396]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.451
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.042
iter 6 loss: 0.576
Actual params: [-0.451 ,  0.0419]
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: -0.437
-Original Grad: 0.001, -lr * Pred Grad:  0.019, New P: 0.061
iter 7 loss: 0.576
Actual params: [-0.4373,  0.061 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: -0.429
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.075
iter 8 loss: 0.576
Actual params: [-0.4287,  0.075 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: -0.421
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.090
iter 9 loss: 0.575
Actual params: [-0.4207,  0.0901]
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: -0.412
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.105
iter 10 loss: 0.575
Actual params: [-0.4118,  0.1045]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.407
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.111
iter 11 loss: 0.575
Actual params: [-0.4067,  0.1113]
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: -0.390
-Original Grad: 0.001, -lr * Pred Grad:  0.024, New P: 0.136
iter 12 loss: 0.574
Actual params: [-0.39  ,  0.1356]
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: -0.383
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.147
iter 13 loss: 0.574
Actual params: [-0.3825,  0.1469]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.380
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.154
iter 14 loss: 0.574
Actual params: [-0.3802,  0.1542]
-Original Grad: 0.001, -lr * Pred Grad:  0.031, New P: -0.349
-Original Grad: 0.001, -lr * Pred Grad:  0.052, New P: 0.206
iter 15 loss: 0.571
Actual params: [-0.3489,  0.2064]
-Original Grad: 0.001, -lr * Pred Grad:  0.059, New P: -0.290
-Original Grad: 0.001, -lr * Pred Grad:  0.078, New P: 0.285
iter 16 loss: 0.565
Actual params: [-0.29  ,  0.2848]
-Original Grad: 0.000, -lr * Pred Grad:  0.022, New P: -0.268
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.307
iter 17 loss: 0.562
Actual params: [-0.2679,  0.3074]
-Original Grad: 0.000, -lr * Pred Grad:  0.027, New P: -0.240
-Original Grad: 0.001, -lr * Pred Grad:  0.041, New P: 0.349
iter 18 loss: 0.556
Actual params: [-0.2405,  0.3487]
-Original Grad: 0.001, -lr * Pred Grad:  0.047, New P: -0.193
-Original Grad: 0.001, -lr * Pred Grad:  0.052, New P: 0.401
iter 19 loss: 0.548
Actual params: [-0.1934,  0.4006]
-Original Grad: 0.001, -lr * Pred Grad:  0.064, New P: -0.130
-Original Grad: 0.001, -lr * Pred Grad:  0.097, New P: 0.498
iter 20 loss: 0.533
Actual params: [-0.1297,  0.4979]
-Original Grad: 0.003, -lr * Pred Grad:  0.246, New P: 0.116
-Original Grad: 0.004, -lr * Pred Grad:  0.319, New P: 0.817
iter 21 loss: 0.463
Actual params: [0.1164, 0.8167]
-Original Grad: 0.007, -lr * Pred Grad:  0.632, New P: 0.748
-Original Grad: 0.006, -lr * Pred Grad:  0.521, New P: 1.338
iter 22 loss: 0.323
Actual params: [0.7484, 1.3381]
-Original Grad: 0.573, -lr * Pred Grad:  0.254, New P: 1.002
-Original Grad: -0.001, -lr * Pred Grad:  -0.575, New P: 0.763
iter 23 loss: 0.288
Actual params: [1.0025, 0.7627]
-Original Grad: 1.202, -lr * Pred Grad:  0.039, New P: 1.041
-Original Grad: 0.345, -lr * Pred Grad:  0.089, New P: 0.852
iter 24 loss: 0.239
Actual params: [1.041, 0.852]
-Original Grad: 1.912, -lr * Pred Grad:  0.054, New P: 1.095
-Original Grad: 0.639, -lr * Pred Grad:  -0.076, New P: 0.776
iter 25 loss: 0.207
Actual params: [1.0954, 0.7764]
-Original Grad: 2.956, -lr * Pred Grad:  0.054, New P: 1.150
-Original Grad: 0.695, -lr * Pred Grad:  -0.100, New P: 0.676
iter 26 loss: 0.178
Actual params: [1.1499, 0.676 ]
-Original Grad: 1.259, -lr * Pred Grad:  0.016, New P: 1.166
-Original Grad: 0.281, -lr * Pred Grad:  -0.023, New P: 0.653
iter 27 loss: 0.179
Actual params: [1.1662, 0.6529]
-Original Grad: 0.520, -lr * Pred Grad:  -0.013, New P: 1.153
-Original Grad: 0.473, -lr * Pred Grad:  0.057, New P: 0.709
iter 28 loss: 0.161
Actual params: [1.1534, 0.7094]
-Original Grad: 0.430, -lr * Pred Grad:  0.011, New P: 1.164
-Original Grad: -0.008, -lr * Pred Grad:  -0.025, New P: 0.685
iter 29 loss: 0.164
Actual params: [1.1644, 0.6846]
-Original Grad: 0.778, -lr * Pred Grad:  0.014, New P: 1.179
-Original Grad: 0.084, -lr * Pred Grad:  -0.025, New P: 0.659
iter 30 loss: 0.169
Actual params: [1.1787, 0.6594]
-Original Grad: 0.619, -lr * Pred Grad:  0.005, New P: 1.184
-Original Grad: 0.195, -lr * Pred Grad:  0.004, New P: 0.664
Target params: [1.3344, 1.5708]
iter 0 loss: 0.486
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.024, -lr * Pred Grad:  0.243, New P: -0.230
-Original Grad: 0.039, -lr * Pred Grad:  0.394, New P: 0.397
iter 1 loss: 0.434
Actual params: [-0.2297,  0.3971]
-Original Grad: 0.090, -lr * Pred Grad:  0.528, New P: 0.299
-Original Grad: 0.095, -lr * Pred Grad:  0.535, New P: 0.932
iter 2 loss: 0.193
Actual params: [0.2988, 0.9318]
-Original Grad: 0.075, -lr * Pred Grad:  0.530, New P: 0.829
-Original Grad: -0.003, -lr * Pred Grad:  -0.204, New P: 0.728
iter 3 loss: 0.139
Actual params: [0.8286, 0.7283]
-Original Grad: 0.255, -lr * Pred Grad:  0.120, New P: 0.949
-Original Grad: 0.280, -lr * Pred Grad:  0.271, New P: 0.999
iter 4 loss: 0.070
Actual params: [0.9488, 0.9993]
-Original Grad: 0.206, -lr * Pred Grad:  0.096, New P: 1.045
-Original Grad: -0.072, -lr * Pred Grad:  -0.074, New P: 0.925
iter 5 loss: 0.068
Actual params: [1.0447, 0.9249]
-Original Grad: 0.155, -lr * Pred Grad:  0.062, New P: 1.107
-Original Grad: 0.084, -lr * Pred Grad:  0.043, New P: 0.968
iter 6 loss: 0.051
Actual params: [1.1067, 0.9683]
-Original Grad: 0.090, -lr * Pred Grad:  0.041, New P: 1.147
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: 0.958
iter 7 loss: 0.046
Actual params: [1.1473, 0.9585]
-Original Grad: 0.065, -lr * Pred Grad:  0.036, New P: 1.183
-Original Grad: -0.067, -lr * Pred Grad:  -0.044, New P: 0.914
iter 8 loss: 0.048
Actual params: [1.183 , 0.9143]
-Original Grad: 0.095, -lr * Pred Grad:  0.025, New P: 1.208
-Original Grad: 0.147, -lr * Pred Grad:  0.047, New P: 0.961
iter 9 loss: 0.037
Actual params: [1.2075, 0.9608]
-Original Grad: 0.031, -lr * Pred Grad:  0.024, New P: 1.232
-Original Grad: -0.056, -lr * Pred Grad:  -0.029, New P: 0.932
iter 10 loss: 0.038
Actual params: [1.2317, 0.9317]
-Original Grad: 0.040, -lr * Pred Grad:  0.023, New P: 1.255
-Original Grad: -0.003, -lr * Pred Grad:  -0.007, New P: 0.924
iter 11 loss: 0.036
Actual params: [1.2546, 0.9244]
-Original Grad: 0.052, -lr * Pred Grad:  0.028, New P: 1.283
-Original Grad: 0.016, -lr * Pred Grad:  -0.002, New P: 0.922
iter 12 loss: 0.034
Actual params: [1.2827, 0.9219]
-Original Grad: 0.024, -lr * Pred Grad:  0.026, New P: 1.309
-Original Grad: -0.063, -lr * Pred Grad:  -0.030, New P: 0.891
iter 13 loss: 0.034
Actual params: [1.309 , 0.8915]
-Original Grad: 0.044, -lr * Pred Grad:  0.032, New P: 1.341
-Original Grad: -0.009, -lr * Pred Grad:  -0.012, New P: 0.880
iter 14 loss: 0.032
Actual params: [1.3412, 0.8796]
-Original Grad: 0.025, -lr * Pred Grad:  0.031, New P: 1.372
-Original Grad: -0.065, -lr * Pred Grad:  -0.033, New P: 0.847
iter 15 loss: 0.032
Actual params: [1.3721, 0.8471]
-Original Grad: 0.113, -lr * Pred Grad:  0.043, New P: 1.415
-Original Grad: 0.135, -lr * Pred Grad:  0.021, New P: 0.868
iter 16 loss: 0.026
Actual params: [1.4152, 0.8679]
-Original Grad: 0.045, -lr * Pred Grad:  0.026, New P: 1.441
-Original Grad: 0.026, -lr * Pred Grad:  -0.003, New P: 0.865
iter 17 loss: 0.025
Actual params: [1.4413, 0.8645]
-Original Grad: 0.029, -lr * Pred Grad:  0.041, New P: 1.482
-Original Grad: -0.063, -lr * Pred Grad:  -0.033, New P: 0.831
iter 18 loss: 0.024
Actual params: [1.4825, 0.8313]
-Original Grad: 0.028, -lr * Pred Grad:  0.032, New P: 1.514
-Original Grad: -0.026, -lr * Pred Grad:  -0.020, New P: 0.812
iter 19 loss: 0.023
Actual params: [1.5142, 0.8116]
-Original Grad: 0.051, -lr * Pred Grad:  0.037, New P: 1.551
-Original Grad: 0.013, -lr * Pred Grad:  -0.011, New P: 0.800
iter 20 loss: 0.022
Actual params: [1.5513, 0.8002]
-Original Grad: -0.003, -lr * Pred Grad:  0.015, New P: 1.567
-Original Grad: -0.050, -lr * Pred Grad:  -0.023, New P: 0.778
iter 21 loss: 0.022
Actual params: [1.5667, 0.7777]
-Original Grad: 0.020, -lr * Pred Grad:  0.021, New P: 1.588
-Original Grad: -0.005, -lr * Pred Grad:  -0.010, New P: 0.768
iter 22 loss: 0.022
Actual params: [1.5878, 0.7676]
-Original Grad: 0.041, -lr * Pred Grad:  0.028, New P: 1.615
-Original Grad: 0.035, -lr * Pred Grad:  -0.000, New P: 0.768
iter 23 loss: 0.022
Actual params: [1.6154, 0.7675]
-Original Grad: 0.003, -lr * Pred Grad:  0.028, New P: 1.643
-Original Grad: -0.051, -lr * Pred Grad:  -0.030, New P: 0.738
iter 24 loss: 0.022
Actual params: [1.6432, 0.7377]
-Original Grad: 0.009, -lr * Pred Grad:  0.029, New P: 1.672
-Original Grad: -0.035, -lr * Pred Grad:  -0.025, New P: 0.713
iter 25 loss: 0.023
Actual params: [1.6722, 0.7127]
-Original Grad: -0.006, -lr * Pred Grad:  -0.004, New P: 1.668
-Original Grad: -0.007, -lr * Pred Grad:  -0.001, New P: 0.712
iter 26 loss: 0.023
Actual params: [1.6684, 0.7116]
-Original Grad: 0.004, -lr * Pred Grad:  0.013, New P: 1.682
-Original Grad: -0.016, -lr * Pred Grad:  -0.013, New P: 0.699
iter 27 loss: 0.023
Actual params: [1.6818, 0.6988]
-Original Grad: 0.016, -lr * Pred Grad:  0.023, New P: 1.705
-Original Grad: -0.005, -lr * Pred Grad:  -0.013, New P: 0.685
iter 28 loss: 0.024
Actual params: [1.7049, 0.6854]
-Original Grad: -0.040, -lr * Pred Grad:  -0.029, New P: 1.676
-Original Grad: -0.020, -lr * Pred Grad:  0.006, New P: 0.691
iter 29 loss: 0.023
Actual params: [1.6759, 0.6909]
-Original Grad: 0.014, -lr * Pred Grad:  0.022, New P: 1.698
-Original Grad: -0.016, -lr * Pred Grad:  -0.018, New P: 0.673
iter 30 loss: 0.024
Actual params: [1.6983, 0.6727]
-Original Grad: 0.016, -lr * Pred Grad:  0.030, New P: 1.728
-Original Grad: -0.023, -lr * Pred Grad:  -0.026, New P: 0.646
Target params: [1.3344, 1.5708]
iter 0 loss: 0.170
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.205, -lr * Pred Grad:  0.634, New P: 0.162
-Original Grad: -0.007, -lr * Pred Grad:  0.009, New P: 0.013
iter 1 loss: 0.075
Actual params: [0.162 , 0.0129]
-Original Grad: 0.386, -lr * Pred Grad:  0.180, New P: 0.342
-Original Grad: 0.103, -lr * Pred Grad:  0.213, New P: 0.226
iter 2 loss: 0.024
Actual params: [0.3419, 0.2257]
-Original Grad: 0.232, -lr * Pred Grad:  0.045, New P: 0.387
-Original Grad: 0.087, -lr * Pred Grad:  0.102, New P: 0.327
iter 3 loss: 0.013
Actual params: [0.3866, 0.3274]
-Original Grad: 0.210, -lr * Pred Grad:  0.040, New P: 0.427
-Original Grad: 0.062, -lr * Pred Grad:  -0.006, New P: 0.321
iter 4 loss: 0.009
Actual params: [0.4266, 0.3213]
-Original Grad: 0.079, -lr * Pred Grad:  -0.005, New P: 0.421
-Original Grad: 0.045, -lr * Pred Grad:  0.056, New P: 0.377
iter 5 loss: 0.008
Actual params: [0.4212, 0.377 ]
-Original Grad: 0.100, -lr * Pred Grad:  0.013, New P: 0.434
-Original Grad: 0.041, -lr * Pred Grad:  0.012, New P: 0.389
iter 6 loss: 0.007
Actual params: [0.4339, 0.389 ]
-Original Grad: 0.073, -lr * Pred Grad:  0.022, New P: 0.456
-Original Grad: 0.016, -lr * Pred Grad:  -0.026, New P: 0.363
iter 7 loss: 0.006
Actual params: [0.4556, 0.3631]
-Original Grad: 0.042, -lr * Pred Grad:  0.024, New P: 0.480
-Original Grad: -0.009, -lr * Pred Grad:  -0.045, New P: 0.318
iter 8 loss: 0.006
Actual params: [0.48  , 0.3177]
-Original Grad: 0.036, -lr * Pred Grad:  0.011, New P: 0.491
-Original Grad: 0.007, -lr * Pred Grad:  -0.013, New P: 0.305
iter 9 loss: 0.006
Actual params: [0.4909, 0.3048]
-Original Grad: 0.051, -lr * Pred Grad:  -0.007, New P: 0.484
-Original Grad: 0.052, -lr * Pred Grad:  0.034, New P: 0.339
iter 10 loss: 0.005
Actual params: [0.4838, 0.3386]
-Original Grad: 0.007, -lr * Pred Grad:  0.003, New P: 0.486
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 0.335
iter 11 loss: 0.005
Actual params: [0.4865, 0.3351]
-Original Grad: 0.042, -lr * Pred Grad:  0.004, New P: 0.490
-Original Grad: 0.024, -lr * Pred Grad:  0.003, New P: 0.338
iter 12 loss: 0.005
Actual params: [0.4902, 0.3378]
-Original Grad: 0.024, -lr * Pred Grad:  0.003, New P: 0.494
-Original Grad: 0.011, -lr * Pred Grad:  -0.001, New P: 0.337
iter 13 loss: 0.005
Actual params: [0.4937, 0.337 ]
-Original Grad: 0.023, -lr * Pred Grad:  0.009, New P: 0.503
-Original Grad: -0.004, -lr * Pred Grad:  -0.013, New P: 0.324
iter 14 loss: 0.005
Actual params: [0.503 , 0.3244]
-Original Grad: 0.025, -lr * Pred Grad:  0.008, New P: 0.511
-Original Grad: -0.003, -lr * Pred Grad:  -0.011, New P: 0.313
iter 15 loss: 0.005
Actual params: [0.5112, 0.3134]
-Original Grad: 0.033, -lr * Pred Grad:  0.009, New P: 0.521
-Original Grad: 0.001, -lr * Pred Grad:  -0.011, New P: 0.303
iter 16 loss: 0.005
Actual params: [0.5206, 0.3027]
-Original Grad: 0.046, -lr * Pred Grad:  0.012, New P: 0.532
-Original Grad: 0.006, -lr * Pred Grad:  -0.012, New P: 0.290
iter 17 loss: 0.005
Actual params: [0.5322, 0.2904]
-Original Grad: 0.012, -lr * Pred Grad:  -0.004, New P: 0.528
-Original Grad: 0.023, -lr * Pred Grad:  0.011, New P: 0.301
iter 18 loss: 0.004
Actual params: [0.5282, 0.3012]
-Original Grad: 0.025, -lr * Pred Grad:  0.006, New P: 0.534
-Original Grad: -0.001, -lr * Pred Grad:  -0.007, New P: 0.294
iter 19 loss: 0.004
Actual params: [0.5339, 0.2939]
-Original Grad: -0.002, -lr * Pred Grad:  0.003, New P: 0.537
-Original Grad: -0.015, -lr * Pred Grad:  -0.008, New P: 0.286
iter 20 loss: 0.004
Actual params: [0.5373, 0.2862]
-Original Grad: 0.042, -lr * Pred Grad:  0.000, New P: 0.538
-Original Grad: 0.039, -lr * Pred Grad:  0.009, New P: 0.295
iter 21 loss: 0.004
Actual params: [0.5375, 0.2954]
-Original Grad: 0.033, -lr * Pred Grad:  0.003, New P: 0.541
-Original Grad: 0.019, -lr * Pred Grad:  0.001, New P: 0.297
iter 22 loss: 0.004
Actual params: [0.5408, 0.2965]
-Original Grad: 0.038, -lr * Pred Grad:  0.006, New P: 0.547
-Original Grad: 0.013, -lr * Pred Grad:  -0.003, New P: 0.294
iter 23 loss: 0.004
Actual params: [0.5469, 0.2938]
-Original Grad: -0.009, -lr * Pred Grad:  0.000, New P: 0.547
-Original Grad: -0.012, -lr * Pred Grad:  -0.003, New P: 0.291
iter 24 loss: 0.004
Actual params: [0.5474, 0.2907]
-Original Grad: 0.005, -lr * Pred Grad:  0.002, New P: 0.549
-Original Grad: -0.003, -lr * Pred Grad:  -0.003, New P: 0.288
iter 25 loss: 0.004
Actual params: [0.5494, 0.2881]
-Original Grad: 0.025, -lr * Pred Grad:  -0.004, New P: 0.545
-Original Grad: 0.048, -lr * Pred Grad:  0.015, New P: 0.303
iter 26 loss: 0.004
Actual params: [0.5451, 0.3028]
-Original Grad: 0.017, -lr * Pred Grad:  0.008, New P: 0.553
-Original Grad: -0.019, -lr * Pred Grad:  -0.012, New P: 0.291
iter 27 loss: 0.004
Actual params: [0.5533, 0.2912]
-Original Grad: 0.032, -lr * Pred Grad:  0.006, New P: 0.559
-Original Grad: 0.009, -lr * Pred Grad:  -0.003, New P: 0.288
iter 28 loss: 0.004
Actual params: [0.5594, 0.2878]
-Original Grad: 0.035, -lr * Pred Grad:  0.008, New P: 0.567
-Original Grad: 0.002, -lr * Pred Grad:  -0.007, New P: 0.281
iter 29 loss: 0.004
Actual params: [0.5674, 0.2805]
-Original Grad: 0.028, -lr * Pred Grad:  0.004, New P: 0.572
-Original Grad: 0.011, -lr * Pred Grad:  -0.001, New P: 0.279
iter 30 loss: 0.004
Actual params: [0.5717, 0.2793]
-Original Grad: 0.007, -lr * Pred Grad:  0.004, New P: 0.576
-Original Grad: -0.011, -lr * Pred Grad:  -0.007, New P: 0.272
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: -0.464
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.005
iter 1 loss: 0.363
Actual params: [-0.464 ,  0.0054]
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: -0.452
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.008
iter 2 loss: 0.363
Actual params: [-0.4523,  0.008 ]
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: -0.438
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.011
iter 3 loss: 0.363
Actual params: [-0.4385,  0.0111]
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: -0.424
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.014
iter 4 loss: 0.363
Actual params: [-0.4244,  0.0141]
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: -0.406
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.018
iter 5 loss: 0.363
Actual params: [-0.4064,  0.0179]
-Original Grad: 0.002, -lr * Pred Grad:  0.031, New P: -0.376
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.024
iter 6 loss: 0.363
Actual params: [-0.3758,  0.0244]
-Original Grad: 0.001, -lr * Pred Grad:  0.023, New P: -0.353
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.029
iter 7 loss: 0.363
Actual params: [-0.3528,  0.0291]
-Original Grad: 0.002, -lr * Pred Grad:  0.038, New P: -0.315
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.037
iter 8 loss: 0.363
Actual params: [-0.315 ,  0.0366]
-Original Grad: 0.002, -lr * Pred Grad:  0.058, New P: -0.257
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.048
iter 9 loss: 0.363
Actual params: [-0.2567,  0.0483]
-Original Grad: 0.003, -lr * Pred Grad:  0.093, New P: -0.163
-Original Grad: 0.001, -lr * Pred Grad:  0.019, New P: 0.067
iter 10 loss: 0.362
Actual params: [-0.1633,  0.0674]
-Original Grad: 0.007, -lr * Pred Grad:  0.218, New P: 0.055
-Original Grad: 0.001, -lr * Pred Grad:  0.045, New P: 0.112
iter 11 loss: 0.355
Actual params: [0.0549, 0.1122]
-Original Grad: 0.032, -lr * Pred Grad:  0.927, New P: 0.982
-Original Grad: 0.007, -lr * Pred Grad:  0.210, New P: 0.322
iter 12 loss: 0.238
Actual params: [0.982 , 0.3221]
-Original Grad: -0.013, -lr * Pred Grad:  -0.015, New P: 0.967
-Original Grad: 0.535, -lr * Pred Grad:  0.393, New P: 0.715
iter 13 loss: 0.087
Actual params: [0.967 , 0.7152]
-Original Grad: 0.007, -lr * Pred Grad:  0.262, New P: 1.229
-Original Grad: 0.381, -lr * Pred Grad:  0.152, New P: 0.867
iter 14 loss: 0.024
Actual params: [1.2289, 0.8673]
-Original Grad: 0.286, -lr * Pred Grad:  0.108, New P: 1.337
-Original Grad: 0.455, -lr * Pred Grad:  0.020, New P: 0.887
iter 15 loss: 0.015
Actual params: [1.3369, 0.8869]
-Original Grad: 0.005, -lr * Pred Grad:  -0.025, New P: 1.312
-Original Grad: 0.065, -lr * Pred Grad:  0.021, New P: 0.908
iter 16 loss: 0.014
Actual params: [1.3121, 0.9077]
-Original Grad: 0.049, -lr * Pred Grad:  -0.023, New P: 1.289
-Original Grad: 0.167, -lr * Pred Grad:  0.030, New P: 0.938
iter 17 loss: 0.013
Actual params: [1.289 , 0.9376]
-Original Grad: 0.235, -lr * Pred Grad:  0.041, New P: 1.330
-Original Grad: 0.123, -lr * Pred Grad:  -0.012, New P: 0.925
iter 18 loss: 0.012
Actual params: [1.3296, 0.9255]
-Original Grad: 0.004, -lr * Pred Grad:  -0.020, New P: 1.310
-Original Grad: 0.215, -lr * Pred Grad:  0.030, New P: 0.955
iter 19 loss: 0.011
Actual params: [1.3096, 0.9553]
-Original Grad: 0.042, -lr * Pred Grad:  -0.002, New P: 1.308
-Original Grad: 0.110, -lr * Pred Grad:  0.011, New P: 0.966
iter 20 loss: 0.011
Actual params: [1.3076, 0.9664]
-Original Grad: 0.086, -lr * Pred Grad:  0.002, New P: 1.310
-Original Grad: 0.125, -lr * Pred Grad:  0.011, New P: 0.977
iter 21 loss: 0.011
Actual params: [1.3098, 0.9771]
-Original Grad: 0.082, -lr * Pred Grad:  0.001, New P: 1.311
-Original Grad: 0.135, -lr * Pred Grad:  0.012, New P: 0.989
iter 22 loss: 0.011
Actual params: [1.3109, 0.9895]
-Original Grad: 0.163, -lr * Pred Grad:  0.008, New P: 1.319
-Original Grad: 0.116, -lr * Pred Grad:  0.007, New P: 0.996
iter 23 loss: 0.010
Actual params: [1.3187, 0.9965]
-Original Grad: 0.081, -lr * Pred Grad:  0.004, New P: 1.322
-Original Grad: 0.062, -lr * Pred Grad:  0.004, New P: 1.001
iter 24 loss: 0.010
Actual params: [1.3223, 1.001 ]
-Original Grad: 0.070, -lr * Pred Grad:  0.005, New P: 1.328
-Original Grad: -0.011, -lr * Pred Grad:  -0.004, New P: 0.997
iter 25 loss: 0.010
Actual params: [1.3277, 0.9971]
-Original Grad: 0.124, -lr * Pred Grad:  0.006, New P: 1.334
-Original Grad: 0.061, -lr * Pred Grad:  0.004, New P: 1.001
iter 26 loss: 0.010
Actual params: [1.3339, 1.001 ]
-Original Grad: 0.012, -lr * Pred Grad:  0.001, New P: 1.335
-Original Grad: 0.003, -lr * Pred Grad:  0.000, New P: 1.001
iter 27 loss: 0.010
Actual params: [1.3347, 1.0011]
-Original Grad: -0.041, -lr * Pred Grad:  -0.004, New P: 1.330
-Original Grad: 0.106, -lr * Pred Grad:  0.014, New P: 1.015
iter 28 loss: 0.010
Actual params: [1.3302, 1.0148]
-Original Grad: 0.077, -lr * Pred Grad:  0.005, New P: 1.335
-Original Grad: 0.012, -lr * Pred Grad:  0.000, New P: 1.015
iter 29 loss: 0.010
Actual params: [1.3348, 1.0151]
-Original Grad: 0.117, -lr * Pred Grad:  0.008, New P: 1.342
-Original Grad: -0.030, -lr * Pred Grad:  -0.006, New P: 1.009
iter 30 loss: 0.010
Actual params: [1.3423, 1.0094]
-Original Grad: 0.028, -lr * Pred Grad:  0.001, New P: 1.343
-Original Grad: 0.075, -lr * Pred Grad:  0.010, New P: 1.019
Target params: [1.3344, 1.5708]
iter 0 loss: 0.320
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.053, -lr * Pred Grad:  -0.505, New P: -0.977
-Original Grad: 0.009, -lr * Pred Grad:  0.094, New P: 0.097
iter 1 loss: 0.315
Actual params: [-0.977 ,  0.0974]
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.986
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.101
iter 2 loss: 0.315
Actual params: [-0.9855,  0.1011]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.998
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.108
iter 3 loss: 0.315
Actual params: [-0.9985,  0.1078]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -1.008
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.113
iter 4 loss: 0.315
Actual params: [-1.0085,  0.1126]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -1.021
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.119
iter 5 loss: 0.315
Actual params: [-1.0209,  0.1187]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -1.036
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.126
iter 6 loss: 0.315
Actual params: [-1.0364,  0.1257]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -1.049
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.131
iter 7 loss: 0.315
Actual params: [-1.0489,  0.1314]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -1.061
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.137
iter 8 loss: 0.315
Actual params: [-1.0612,  0.137 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.071
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.143
iter 9 loss: 0.315
Actual params: [-1.0711,  0.1425]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.083
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.149
iter 10 loss: 0.315
Actual params: [-1.083 ,  0.1492]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.095
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.157
iter 11 loss: 0.315
Actual params: [-1.0949,  0.1567]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.102
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.161
iter 12 loss: 0.315
Actual params: [-1.1025,  0.1609]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.116
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.168
iter 13 loss: 0.315
Actual params: [-1.1155,  0.1678]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.123
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.173
iter 14 loss: 0.315
Actual params: [-1.1228,  0.1728]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.129
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.177
iter 15 loss: 0.315
Actual params: [-1.1294,  0.1769]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.143
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.185
iter 16 loss: 0.315
Actual params: [-1.1426,  0.1845]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.152
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.191
iter 17 loss: 0.315
Actual params: [-1.1519,  0.1908]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.160
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.197
iter 18 loss: 0.315
Actual params: [-1.1603,  0.1967]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.170
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.203
iter 19 loss: 0.315
Actual params: [-1.1695,  0.203 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.175
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.207
iter 20 loss: 0.315
Actual params: [-1.1751,  0.2071]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.182
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.212
iter 21 loss: 0.315
Actual params: [-1.1823,  0.2124]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.198
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.223
iter 22 loss: 0.315
Actual params: [-1.198 ,  0.2235]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.205
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.229
iter 23 loss: 0.315
Actual params: [-1.2055,  0.2291]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.216
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.237
iter 24 loss: 0.315
Actual params: [-1.2162,  0.2365]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.223
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.242
iter 25 loss: 0.315
Actual params: [-1.2227,  0.2416]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.237
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.252
iter 26 loss: 0.315
Actual params: [-1.2367,  0.2522]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.246
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.260
iter 27 loss: 0.315
Actual params: [-1.2464,  0.26  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.255
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.267
iter 28 loss: 0.315
Actual params: [-1.2549,  0.2672]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.262
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.273
iter 29 loss: 0.315
Actual params: [-1.2622,  0.2734]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.268
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.279
iter 30 loss: 0.315
Actual params: [-1.2677,  0.2788]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.276
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.287
Target params: [1.3344, 1.5708]
iter 0 loss: 0.583
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.010, -lr * Pred Grad:  0.108, New P: -0.365
-Original Grad: 0.006, -lr * Pred Grad:  0.064, New P: 0.068
iter 1 loss: 0.578
Actual params: [-0.3646,  0.0676]
-Original Grad: 0.045, -lr * Pred Grad:  0.450, New P: 0.085
-Original Grad: 0.021, -lr * Pred Grad:  0.214, New P: 0.282
iter 2 loss: 0.533
Actual params: [0.0851, 0.2816]
-Original Grad: 0.134, -lr * Pred Grad:  0.631, New P: 0.716
-Original Grad: 0.076, -lr * Pred Grad:  0.381, New P: 0.663
iter 3 loss: 0.309
Actual params: [0.7163, 0.663 ]
-Original Grad: 0.357, -lr * Pred Grad:  0.366, New P: 1.082
-Original Grad: 0.240, -lr * Pred Grad:  0.118, New P: 0.781
iter 4 loss: 0.207
Actual params: [1.0819, 0.781 ]
-Original Grad: 0.090, -lr * Pred Grad:  -0.197, New P: 0.885
-Original Grad: 0.317, -lr * Pred Grad:  0.481, New P: 1.261
iter 5 loss: 0.145
Actual params: [0.8851, 1.2615]
-Original Grad: 0.893, -lr * Pred Grad:  0.175, New P: 1.060
-Original Grad: -0.060, -lr * Pred Grad:  -0.096, New P: 1.165
iter 6 loss: 0.117
Actual params: [1.0599, 1.1652]
-Original Grad: 0.648, -lr * Pred Grad:  0.062, New P: 1.122
-Original Grad: 0.148, -lr * Pred Grad:  0.127, New P: 1.292
iter 7 loss: 0.089
Actual params: [1.1224, 1.2918]
-Original Grad: 0.244, -lr * Pred Grad:  0.019, New P: 1.142
-Original Grad: 0.167, -lr * Pred Grad:  0.129, New P: 1.421
iter 8 loss: 0.073
Actual params: [1.1417, 1.4206]
-Original Grad: 0.078, -lr * Pred Grad:  0.007, New P: 1.149
-Original Grad: 0.194, -lr * Pred Grad:  0.111, New P: 1.532
iter 9 loss: 0.066
Actual params: [1.1488, 1.5318]
-Original Grad: 0.329, -lr * Pred Grad:  0.029, New P: 1.177
-Original Grad: 0.078, -lr * Pred Grad:  0.042, New P: 1.574
iter 10 loss: 0.061
Actual params: [1.1773, 1.5735]
-Original Grad: 0.296, -lr * Pred Grad:  0.025, New P: 1.202
-Original Grad: 0.057, -lr * Pred Grad:  0.026, New P: 1.599
iter 11 loss: 0.057
Actual params: [1.2021, 1.5994]
-Original Grad: 0.296, -lr * Pred Grad:  0.023, New P: 1.225
-Original Grad: 0.119, -lr * Pred Grad:  0.048, New P: 1.647
iter 12 loss: 0.054
Actual params: [1.2247, 1.647 ]
-Original Grad: 0.407, -lr * Pred Grad:  0.030, New P: 1.255
-Original Grad: 0.069, -lr * Pred Grad:  0.016, New P: 1.663
iter 13 loss: 0.050
Actual params: [1.2545, 1.6629]
-Original Grad: 0.218, -lr * Pred Grad:  0.018, New P: 1.273
-Original Grad: -0.055, -lr * Pred Grad:  -0.025, New P: 1.638
iter 14 loss: 0.048
Actual params: [1.273 , 1.6375]
-Original Grad: 0.043, -lr * Pred Grad:  0.002, New P: 1.275
-Original Grad: 0.134, -lr * Pred Grad:  0.039, New P: 1.676
iter 15 loss: 0.047
Actual params: [1.2748, 1.6762]
-Original Grad: 0.121, -lr * Pred Grad:  0.010, New P: 1.285
-Original Grad: 0.020, -lr * Pred Grad:  0.005, New P: 1.681
iter 16 loss: 0.046
Actual params: [1.2847, 1.6811]
-Original Grad: 0.207, -lr * Pred Grad:  0.018, New P: 1.302
-Original Grad: -0.064, -lr * Pred Grad:  -0.016, New P: 1.666
iter 17 loss: 0.045
Actual params: [1.3023, 1.6656]
-Original Grad: 0.243, -lr * Pred Grad:  0.020, New P: 1.322
-Original Grad: 0.043, -lr * Pred Grad:  0.011, New P: 1.677
iter 18 loss: 0.043
Actual params: [1.3221, 1.6766]
-Original Grad: -0.008, -lr * Pred Grad:  0.001, New P: 1.323
-Original Grad: 0.159, -lr * Pred Grad:  0.038, New P: 1.714
iter 19 loss: 0.042
Actual params: [1.3226, 1.7142]
-Original Grad: 0.029, -lr * Pred Grad:  0.002, New P: 1.324
-Original Grad: -0.049, -lr * Pred Grad:  -0.010, New P: 1.704
iter 20 loss: 0.042
Actual params: [1.3243, 1.7043]
-Original Grad: 0.107, -lr * Pred Grad:  0.010, New P: 1.334
-Original Grad: 0.016, -lr * Pred Grad:  0.006, New P: 1.711
iter 21 loss: 0.041
Actual params: [1.3343, 1.7105]
-Original Grad: -0.050, -lr * Pred Grad:  0.001, New P: 1.335
-Original Grad: 0.167, -lr * Pred Grad:  0.032, New P: 1.743
iter 22 loss: 0.041
Actual params: [1.3352, 1.7427]
-Original Grad: 0.035, -lr * Pred Grad:  0.006, New P: 1.342
-Original Grad: 0.081, -lr * Pred Grad:  0.017, New P: 1.760
iter 23 loss: 0.040
Actual params: [1.3417, 1.76  ]
-Original Grad: 0.119, -lr * Pred Grad:  0.007, New P: 1.349
-Original Grad: -0.113, -lr * Pred Grad:  -0.016, New P: 1.744
iter 24 loss: 0.040
Actual params: [1.3487, 1.7437]
-Original Grad: 0.012, -lr * Pred Grad:  0.007, New P: 1.355
-Original Grad: 0.106, -lr * Pred Grad:  0.020, New P: 1.764
iter 25 loss: 0.039
Actual params: [1.3555, 1.7641]
-Original Grad: 0.186, -lr * Pred Grad:  0.018, New P: 1.374
-Original Grad: -0.031, -lr * Pred Grad:  0.005, New P: 1.769
iter 26 loss: 0.038
Actual params: [1.3738, 1.7688]
-Original Grad: 0.057, -lr * Pred Grad:  0.009, New P: 1.383
-Original Grad: 0.046, -lr * Pred Grad:  0.013, New P: 1.781
iter 27 loss: 0.037
Actual params: [1.3831, 1.7815]
-Original Grad: 0.134, -lr * Pred Grad:  0.012, New P: 1.395
-Original Grad: -0.013, -lr * Pred Grad:  0.007, New P: 1.788
iter 28 loss: 0.037
Actual params: [1.3954, 1.7882]
-Original Grad: 0.134, -lr * Pred Grad:  0.009, New P: 1.404
-Original Grad: -0.073, -lr * Pred Grad:  -0.005, New P: 1.784
iter 29 loss: 0.036
Actual params: [1.404 , 1.7837]
-Original Grad: 0.047, -lr * Pred Grad:  0.011, New P: 1.415
-Original Grad: 0.071, -lr * Pred Grad:  0.018, New P: 1.802
iter 30 loss: 0.035
Actual params: [1.4148, 1.8019]
-Original Grad: 0.041, -lr * Pred Grad:  0.009, New P: 1.424
-Original Grad: 0.047, -lr * Pred Grad:  0.013, New P: 1.815
Target params: [1.3344, 1.5708]
iter 0 loss: 0.018
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.004, -lr * Pred Grad:  0.044, New P: -0.429
-Original Grad: -0.002, -lr * Pred Grad:  -0.025, New P: -0.022
iter 1 loss: 0.017
Actual params: [-0.4287, -0.0218]
-Original Grad: 0.006, -lr * Pred Grad:  0.071, New P: -0.358
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.045
iter 2 loss: 0.015
Actual params: [-0.3581, -0.0447]
-Original Grad: 0.004, -lr * Pred Grad:  0.054, New P: -0.305
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -0.069
iter 3 loss: 0.014
Actual params: [-0.3046, -0.0688]
-Original Grad: 0.004, -lr * Pred Grad:  0.054, New P: -0.251
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.092
iter 4 loss: 0.013
Actual params: [-0.2508, -0.0919]
-Original Grad: 0.004, -lr * Pred Grad:  0.056, New P: -0.195
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: -0.114
iter 5 loss: 0.012
Actual params: [-0.195 , -0.1138]
-Original Grad: 0.004, -lr * Pred Grad:  0.057, New P: -0.138
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.133
iter 6 loss: 0.012
Actual params: [-0.1376, -0.133 ]
-Original Grad: 0.009, -lr * Pred Grad:  0.141, New P: 0.004
-Original Grad: -0.003, -lr * Pred Grad:  -0.037, New P: -0.170
iter 7 loss: 0.012
Actual params: [ 0.0038, -0.17  ]
-Original Grad: -0.016, -lr * Pred Grad:  -0.040, New P: -0.036
-Original Grad: 0.006, -lr * Pred Grad:  0.031, New P: -0.139
iter 8 loss: 0.011
Actual params: [-0.0363, -0.1386]
-Original Grad: 0.005, -lr * Pred Grad:  0.019, New P: -0.018
-Original Grad: -0.001, -lr * Pred Grad:  0.002, New P: -0.136
iter 9 loss: 0.011
Actual params: [-0.0177, -0.1361]
-Original Grad: 0.006, -lr * Pred Grad:  0.024, New P: 0.007
-Original Grad: -0.002, -lr * Pred Grad:  0.004, New P: -0.132
iter 10 loss: 0.012
Actual params: [ 0.0065, -0.1319]
-Original Grad: -0.016, -lr * Pred Grad:  -0.027, New P: -0.020
-Original Grad: 0.005, -lr * Pred Grad:  0.013, New P: -0.119
iter 11 loss: 0.011
Actual params: [-0.0204, -0.1192]
-Original Grad: 0.008, -lr * Pred Grad:  0.015, New P: -0.006
-Original Grad: -0.002, -lr * Pred Grad:  -0.003, New P: -0.122
iter 12 loss: 0.012
Actual params: [-0.0057, -0.122 ]
-Original Grad: 0.006, -lr * Pred Grad:  0.015, New P: 0.009
-Original Grad: -0.002, -lr * Pred Grad:  0.003, New P: -0.119
iter 13 loss: 0.012
Actual params: [ 0.0092, -0.1185]
-Original Grad: 0.006, -lr * Pred Grad:  0.016, New P: 0.025
-Original Grad: -0.002, -lr * Pred Grad:  0.001, New P: -0.117
iter 14 loss: 0.012
Actual params: [ 0.0247, -0.1175]
-Original Grad: -0.017, -lr * Pred Grad:  -0.024, New P: 0.001
-Original Grad: 0.005, -lr * Pred Grad:  0.008, New P: -0.110
iter 15 loss: 0.012
Actual params: [ 0.0009, -0.1099]
-Original Grad: 0.007, -lr * Pred Grad:  0.012, New P: 0.012
-Original Grad: -0.002, -lr * Pred Grad:  -0.000, New P: -0.110
iter 16 loss: 0.012
Actual params: [ 0.0124, -0.1099]
-Original Grad: -0.033, -lr * Pred Grad:  -0.025, New P: -0.012
-Original Grad: 0.010, -lr * Pred Grad:  0.024, New P: -0.086
iter 17 loss: 0.011
Actual params: [-0.0123, -0.0856]
-Original Grad: 0.006, -lr * Pred Grad:  0.002, New P: -0.010
-Original Grad: -0.002, -lr * Pred Grad:  -0.015, New P: -0.100
iter 18 loss: 0.011
Actual params: [-0.0101, -0.1003]
-Original Grad: 0.007, -lr * Pred Grad:  0.004, New P: -0.006
-Original Grad: -0.002, -lr * Pred Grad:  -0.013, New P: -0.113
iter 19 loss: 0.012
Actual params: [-0.0059, -0.113 ]
-Original Grad: 0.006, -lr * Pred Grad:  0.008, New P: 0.002
-Original Grad: -0.002, -lr * Pred Grad:  0.001, New P: -0.112
iter 20 loss: 0.012
Actual params: [ 0.0022, -0.1117]
-Original Grad: 0.006, -lr * Pred Grad:  0.009, New P: 0.012
-Original Grad: -0.002, -lr * Pred Grad:  0.003, New P: -0.108
iter 21 loss: 0.012
Actual params: [ 0.0116, -0.1083]
-Original Grad: 0.009, -lr * Pred Grad:  0.018, New P: 0.029
-Original Grad: -0.002, -lr * Pred Grad:  0.013, New P: -0.095
iter 22 loss: 0.012
Actual params: [ 0.0294, -0.0954]
-Original Grad: -0.041, -lr * Pred Grad:  -0.038, New P: -0.009
-Original Grad: 0.012, -lr * Pred Grad:  -0.012, New P: -0.108
iter 23 loss: 0.011
Actual params: [-0.0087, -0.1078]
-Original Grad: 0.006, -lr * Pred Grad:  0.004, New P: -0.005
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: -0.112
iter 24 loss: 0.012
Actual params: [-0.0046, -0.1118]
-Original Grad: 0.008, -lr * Pred Grad:  0.009, New P: 0.004
-Original Grad: -0.002, -lr * Pred Grad:  0.002, New P: -0.110
iter 25 loss: 0.012
Actual params: [ 0.0042, -0.1096]
-Original Grad: 0.008, -lr * Pred Grad:  -0.004, New P: 0.000
-Original Grad: -0.003, -lr * Pred Grad:  -0.045, New P: -0.154
iter 26 loss: 0.012
Actual params: [ 1.3558e-04, -1.5432e-01]
-Original Grad: -0.031, -lr * Pred Grad:  0.021, New P: 0.021
-Original Grad: 0.011, -lr * Pred Grad:  0.149, New P: -0.005
iter 27 loss: 0.012
Actual params: [ 0.0213, -0.005 ]
-Original Grad: 0.008, -lr * Pred Grad:  -0.031, New P: -0.010
-Original Grad: -0.003, -lr * Pred Grad:  -0.123, New P: -0.128
iter 28 loss: 0.012
Actual params: [-0.0099, -0.1278]
-Original Grad: 0.008, -lr * Pred Grad:  0.029, New P: 0.019
-Original Grad: -0.002, -lr * Pred Grad:  0.071, New P: -0.057
iter 29 loss: 0.012
Actual params: [ 0.0192, -0.0571]
-Original Grad: -0.032, -lr * Pred Grad:  -0.051, New P: -0.031
-Original Grad: 0.009, -lr * Pred Grad:  -0.095, New P: -0.152
iter 30 loss: 0.011
Actual params: [-0.0314, -0.1521]
-Original Grad: 0.006, -lr * Pred Grad:  0.032, New P: 0.001
-Original Grad: -0.001, -lr * Pred Grad:  0.092, New P: -0.060
Target params: [1.3344, 1.5708]
iter 0 loss: 0.612
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.042, -lr * Pred Grad:  0.380, New P: -0.092
-Original Grad: 0.023, -lr * Pred Grad:  0.213, New P: 0.216
iter 1 loss: 0.581
Actual params: [-0.0923,  0.2164]
-Original Grad: 0.080, -lr * Pred Grad:  0.523, New P: 0.431
-Original Grad: 0.037, -lr * Pred Grad:  0.232, New P: 0.449
iter 2 loss: 0.445
Actual params: [0.4308, 0.4485]
-Original Grad: 0.179, -lr * Pred Grad:  0.483, New P: 0.914
-Original Grad: 0.037, -lr * Pred Grad:  -0.011, New P: 0.438
iter 3 loss: 0.300
Actual params: [0.9142, 0.4376]
-Original Grad: 0.409, -lr * Pred Grad:  0.140, New P: 1.054
-Original Grad: 0.173, -lr * Pred Grad:  0.511, New P: 0.948
iter 4 loss: 0.196
Actual params: [1.0544, 0.9484]
-Original Grad: 0.107, -lr * Pred Grad:  -0.000, New P: 1.054
-Original Grad: 0.217, -lr * Pred Grad:  0.390, New P: 1.338
iter 5 loss: 0.165
Actual params: [1.0542, 1.3384]
-Original Grad: 0.397, -lr * Pred Grad:  0.072, New P: 1.126
-Original Grad: 0.180, -lr * Pred Grad:  0.187, New P: 1.526
iter 6 loss: 0.155
Actual params: [1.1262, 1.5257]
-Original Grad: -0.119, -lr * Pred Grad:  -0.027, New P: 1.099
-Original Grad: 0.061, -lr * Pred Grad:  0.076, New P: 1.602
iter 7 loss: 0.163
Actual params: [1.0993, 1.6019]
-Original Grad: -0.008, -lr * Pred Grad:  -0.001, New P: 1.098
-Original Grad: 0.025, -lr * Pred Grad:  0.030, New P: 1.632
iter 8 loss: 0.165
Actual params: [1.098 , 1.6316]
-Original Grad: 0.220, -lr * Pred Grad:  0.030, New P: 1.128
-Original Grad: -0.018, -lr * Pred Grad:  -0.001, New P: 1.630
iter 9 loss: 0.160
Actual params: [1.1276, 1.6304]
-Original Grad: 0.068, -lr * Pred Grad:  0.004, New P: 1.132
-Original Grad: -0.061, -lr * Pred Grad:  -0.068, New P: 1.563
iter 10 loss: 0.156
Actual params: [1.1316, 1.5628]
-Original Grad: 0.244, -lr * Pred Grad:  0.026, New P: 1.158
-Original Grad: -0.028, -lr * Pred Grad:  -0.004, New P: 1.558
iter 11 loss: 0.151
Actual params: [1.1581, 1.5583]
-Original Grad: 0.081, -lr * Pred Grad:  0.007, New P: 1.165
-Original Grad: -0.013, -lr * Pred Grad:  -0.007, New P: 1.552
iter 12 loss: 0.150
Actual params: [1.1653, 1.5515]
-Original Grad: 0.225, -lr * Pred Grad:  0.022, New P: 1.187
-Original Grad: -0.010, -lr * Pred Grad:  0.019, New P: 1.571
iter 13 loss: 0.148
Actual params: [1.1871, 1.5705]
-Original Grad: 0.528, -lr * Pred Grad:  0.034, New P: 1.221
-Original Grad: -0.098, -lr * Pred Grad:  -0.061, New P: 1.510
iter 14 loss: 0.143
Actual params: [1.2213, 1.5099]
-Original Grad: 0.033, -lr * Pred Grad:  -0.003, New P: 1.218
-Original Grad: -0.039, -lr * Pred Grad:  -0.050, New P: 1.460
iter 15 loss: 0.142
Actual params: [1.2179, 1.4601]
-Original Grad: -0.188, -lr * Pred Grad:  -0.016, New P: 1.201
-Original Grad: 0.022, -lr * Pred Grad:  -0.003, New P: 1.457
iter 16 loss: 0.144
Actual params: [1.2014, 1.4574]
-Original Grad: 0.078, -lr * Pred Grad:  0.021, New P: 1.222
-Original Grad: 0.059, -lr * Pred Grad:  0.106, New P: 1.563
iter 17 loss: 0.145
Actual params: [1.2221, 1.5633]
-Original Grad: 0.063, -lr * Pred Grad:  0.011, New P: 1.233
-Original Grad: 0.013, -lr * Pred Grad:  0.034, New P: 1.597
iter 18 loss: 0.146
Actual params: [1.2327, 1.5971]
-Original Grad: -0.041, -lr * Pred Grad:  -0.005, New P: 1.228
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: 1.588
iter 19 loss: 0.146
Actual params: [1.2276, 1.5881]
-Original Grad: -0.055, -lr * Pred Grad:  -0.006, New P: 1.221
-Original Grad: 0.003, -lr * Pred Grad:  -0.007, New P: 1.581
iter 20 loss: 0.146
Actual params: [1.2214, 1.5809]
-Original Grad: 0.064, -lr * Pred Grad:  -0.004, New P: 1.217
-Original Grad: -0.063, -lr * Pred Grad:  -0.089, New P: 1.492
iter 21 loss: 0.143
Actual params: [1.217 , 1.4922]
-Original Grad: 0.039, -lr * Pred Grad:  -0.002, New P: 1.215
-Original Grad: -0.032, -lr * Pred Grad:  -0.047, New P: 1.445
iter 22 loss: 0.142
Actual params: [1.2152, 1.445 ]
-Original Grad: 0.042, -lr * Pred Grad:  0.010, New P: 1.225
-Original Grad: 0.027, -lr * Pred Grad:  0.055, New P: 1.500
iter 23 loss: 0.143
Actual params: [1.2249, 1.5004]
-Original Grad: 0.062, -lr * Pred Grad:  0.011, New P: 1.236
-Original Grad: 0.020, -lr * Pred Grad:  0.049, New P: 1.550
iter 24 loss: 0.143
Actual params: [1.2357, 1.5497]
-Original Grad: -0.039, -lr * Pred Grad:  -0.011, New P: 1.225
-Original Grad: -0.032, -lr * Pred Grad:  -0.066, New P: 1.484
iter 25 loss: 0.142
Actual params: [1.2251, 1.4836]
-Original Grad: -0.210, -lr * Pred Grad:  -0.020, New P: 1.205
-Original Grad: 0.025, -lr * Pred Grad:  0.005, New P: 1.489
iter 26 loss: 0.144
Actual params: [1.2047, 1.4887]
-Original Grad: 0.045, -lr * Pred Grad:  0.008, New P: 1.213
-Original Grad: 0.013, -lr * Pred Grad:  0.036, New P: 1.525
iter 27 loss: 0.144
Actual params: [1.213 , 1.5248]
-Original Grad: 0.013, -lr * Pred Grad:  0.014, New P: 1.227
-Original Grad: 0.052, -lr * Pred Grad:  0.110, New P: 1.635
iter 28 loss: 0.149
Actual params: [1.2274, 1.6352]
-Original Grad: -0.094, -lr * Pred Grad:  -0.018, New P: 1.210
-Original Grad: -0.029, -lr * Pred Grad:  -0.075, New P: 1.560
iter 29 loss: 0.146
Actual params: [1.2097, 1.5602]
-Original Grad: -0.147, -lr * Pred Grad:  -0.014, New P: 1.196
-Original Grad: 0.020, -lr * Pred Grad:  0.008, New P: 1.568
iter 30 loss: 0.147
Actual params: [1.1957, 1.5678]
-Original Grad: 0.087, -lr * Pred Grad:  0.012, New P: 1.207
-Original Grad: 0.003, -lr * Pred Grad:  0.027, New P: 1.595
Target params: [1.3344, 1.5708]
iter 0 loss: 0.082
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.029, -lr * Pred Grad:  -0.202, New P: -0.674
-Original Grad: 0.022, -lr * Pred Grad:  0.156, New P: 0.159
iter 1 loss: 0.073
Actual params: [-0.6743,  0.1591]
-Original Grad: -0.001, -lr * Pred Grad:  -0.005, New P: -0.680
-Original Grad: 0.001, -lr * Pred Grad:  0.005, New P: 0.164
iter 2 loss: 0.073
Actual params: [-0.6797,  0.1638]
-Original Grad: -0.004, -lr * Pred Grad:  -0.036, New P: -0.715
-Original Grad: 0.004, -lr * Pred Grad:  0.038, New P: 0.202
iter 3 loss: 0.072
Actual params: [-0.7153,  0.2019]
-Original Grad: -0.002, -lr * Pred Grad:  -0.018, New P: -0.733
-Original Grad: 0.002, -lr * Pred Grad:  0.021, New P: 0.223
iter 4 loss: 0.072
Actual params: [-0.7332,  0.2226]
-Original Grad: -0.001, -lr * Pred Grad:  -0.006, New P: -0.739
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.227
iter 5 loss: 0.071
Actual params: [-0.7388,  0.2267]
-Original Grad: -0.001, -lr * Pred Grad:  -0.007, New P: -0.746
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.232
iter 6 loss: 0.071
Actual params: [-0.7455,  0.2317]
-Original Grad: -0.003, -lr * Pred Grad:  -0.031, New P: -0.777
-Original Grad: 0.003, -lr * Pred Grad:  0.039, New P: 0.271
iter 7 loss: 0.071
Actual params: [-0.7766,  0.2711]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.792
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 0.288
iter 8 loss: 0.071
Actual params: [-0.7921,  0.2879]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.798
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.293
iter 9 loss: 0.071
Actual params: [-0.7984,  0.2928]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.806
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.299
iter 10 loss: 0.071
Actual params: [-0.8058,  0.2987]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.814
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.305
iter 11 loss: 0.071
Actual params: [-0.8139,  0.3051]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.823
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.312
iter 12 loss: 0.070
Actual params: [-0.8232,  0.3124]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.833
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.320
iter 13 loss: 0.070
Actual params: [-0.8325,  0.3197]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.842
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.327
iter 14 loss: 0.070
Actual params: [-0.8416,  0.327 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.024, New P: -0.865
-Original Grad: 0.001, -lr * Pred Grad:  0.025, New P: 0.352
iter 15 loss: 0.070
Actual params: [-0.8654,  0.3524]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.874
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.359
iter 16 loss: 0.070
Actual params: [-0.8742,  0.3593]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.888
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.370
iter 17 loss: 0.070
Actual params: [-0.8875,  0.3702]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -0.909
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.390
iter 18 loss: 0.070
Actual params: [-0.9086,  0.3899]
-Original Grad: -0.001, -lr * Pred Grad:  -0.027, New P: -0.935
-Original Grad: 0.001, -lr * Pred Grad:  0.028, New P: 0.418
iter 19 loss: 0.070
Actual params: [-0.9354,  0.418 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.023, New P: -0.959
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.442
iter 20 loss: 0.070
Actual params: [-0.9589,  0.4421]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.972
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.453
iter 21 loss: 0.070
Actual params: [-0.9717,  0.4532]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.985
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.465
iter 22 loss: 0.070
Actual params: [-0.9847,  0.4645]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.998
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.476
iter 23 loss: 0.070
Actual params: [-0.9977,  0.4758]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.010
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.487
iter 24 loss: 0.070
Actual params: [-1.0105,  0.4867]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.027
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.503
iter 25 loss: 0.070
Actual params: [-1.0272,  0.5025]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.048
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.523
iter 26 loss: 0.070
Actual params: [-1.0475,  0.5227]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.066
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.542
iter 27 loss: 0.070
Actual params: [-1.0661,  0.5418]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.079
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.555
iter 28 loss: 0.070
Actual params: [-1.0794,  0.5545]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.091
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.565
iter 29 loss: 0.070
Actual params: [-1.091,  0.565]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.106
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.579
iter 30 loss: 0.070
Actual params: [-1.1059,  0.5793]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.123
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.597
Target params: [1.3344, 1.5708]
iter 0 loss: 0.049
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.017, -lr * Pred Grad:  -0.182, New P: -0.654
-Original Grad: 0.010, -lr * Pred Grad:  0.102, New P: 0.106
iter 1 loss: 0.047
Actual params: [-0.6544,  0.1057]
-Original Grad: -0.004, -lr * Pred Grad:  -0.041, New P: -0.696
-Original Grad: 0.002, -lr * Pred Grad:  0.025, New P: 0.131
iter 2 loss: 0.047
Actual params: [-0.6958,  0.1308]
-Original Grad: -0.002, -lr * Pred Grad:  -0.027, New P: -0.723
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 0.148
iter 3 loss: 0.047
Actual params: [-0.7228,  0.1477]
-Original Grad: -0.002, -lr * Pred Grad:  -0.028, New P: -0.751
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 0.165
iter 4 loss: 0.047
Actual params: [-0.7505,  0.1651]
-Original Grad: -0.002, -lr * Pred Grad:  -0.030, New P: -0.780
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: 0.184
iter 5 loss: 0.046
Actual params: [-0.7801,  0.1836]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.793
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.192
iter 6 loss: 0.046
Actual params: [-0.7928,  0.1916]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.810
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.202
iter 7 loss: 0.046
Actual params: [-0.8101,  0.2023]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.822
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.210
iter 8 loss: 0.046
Actual params: [-0.8223,  0.2104]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.838
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.220
iter 9 loss: 0.046
Actual params: [-0.8377,  0.2202]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.850
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.228
iter 10 loss: 0.046
Actual params: [-0.8496,  0.2281]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.861
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.236
iter 11 loss: 0.046
Actual params: [-0.8612,  0.2357]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.872
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.243
iter 12 loss: 0.046
Actual params: [-0.8724,  0.2432]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.883
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.250
iter 13 loss: 0.046
Actual params: [-0.8833,  0.2503]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.898
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.260
iter 14 loss: 0.046
Actual params: [-0.8982,  0.2599]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.909
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.267
iter 15 loss: 0.046
Actual params: [-0.9093,  0.2674]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.921
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.275
iter 16 loss: 0.046
Actual params: [-0.9207,  0.2753]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.932
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.283
iter 17 loss: 0.046
Actual params: [-0.9321,  0.283 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.940
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.288
iter 18 loss: 0.046
Actual params: [-0.9399,  0.2884]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.950
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.295
iter 19 loss: 0.046
Actual params: [-0.95  ,  0.2954]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.962
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.303
iter 20 loss: 0.046
Actual params: [-0.962 ,  0.3035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.976
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.313
iter 21 loss: 0.046
Actual params: [-0.976 ,  0.3127]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.988
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.321
iter 22 loss: 0.046
Actual params: [-0.9881,  0.3207]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.997
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.327
iter 23 loss: 0.046
Actual params: [-0.9974,  0.3273]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.007
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.334
iter 24 loss: 0.046
Actual params: [-1.0068,  0.3339]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.017
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.341
iter 25 loss: 0.046
Actual params: [-1.0165,  0.3406]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.026
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.348
iter 26 loss: 0.046
Actual params: [-1.026 ,  0.3475]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.037
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.355
iter 27 loss: 0.046
Actual params: [-1.0373,  0.3551]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.046
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.361
iter 28 loss: 0.046
Actual params: [-1.046 ,  0.3615]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.060
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.370
iter 29 loss: 0.046
Actual params: [-1.06  ,  0.3703]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.070
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.378
iter 30 loss: 0.046
Actual params: [-1.0704,  0.3775]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.080
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.384
Target params: [1.3344, 1.5708]
iter 0 loss: 0.194
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.037, New P: -0.509
-Original Grad: 0.003, -lr * Pred Grad:  0.031, New P: 0.034
iter 1 loss: 0.192
Actual params: [-0.509 ,  0.0342]
-Original Grad: -0.005, -lr * Pred Grad:  -0.063, New P: -0.572
-Original Grad: 0.004, -lr * Pred Grad:  0.053, New P: 0.087
iter 2 loss: 0.188
Actual params: [-0.5719,  0.0867]
-Original Grad: -0.003, -lr * Pred Grad:  -0.039, New P: -0.611
-Original Grad: 0.003, -lr * Pred Grad:  0.035, New P: 0.122
iter 3 loss: 0.186
Actual params: [-0.6106,  0.1218]
-Original Grad: -0.002, -lr * Pred Grad:  -0.033, New P: -0.644
-Original Grad: 0.002, -lr * Pred Grad:  0.031, New P: 0.153
iter 4 loss: 0.184
Actual params: [-0.6436,  0.1531]
-Original Grad: -0.002, -lr * Pred Grad:  -0.039, New P: -0.683
-Original Grad: 0.002, -lr * Pred Grad:  0.038, New P: 0.191
iter 5 loss: 0.183
Actual params: [-0.6826,  0.1909]
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -0.705
-Original Grad: 0.001, -lr * Pred Grad:  0.023, New P: 0.214
iter 6 loss: 0.182
Actual params: [-0.7049,  0.2137]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -0.725
-Original Grad: 0.001, -lr * Pred Grad:  0.022, New P: 0.235
iter 7 loss: 0.182
Actual params: [-0.7248,  0.2353]
-Original Grad: -0.002, -lr * Pred Grad:  -0.038, New P: -0.763
-Original Grad: 0.002, -lr * Pred Grad:  0.038, New P: 0.274
iter 8 loss: 0.181
Actual params: [-0.763 ,  0.2736]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.775
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.286
iter 9 loss: 0.181
Actual params: [-0.7752,  0.286 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.029, New P: -0.804
-Original Grad: 0.001, -lr * Pred Grad:  0.032, New P: 0.318
iter 10 loss: 0.180
Actual params: [-0.8041,  0.3175]
-Original Grad: -0.001, -lr * Pred Grad:  -0.038, New P: -0.842
-Original Grad: 0.001, -lr * Pred Grad:  0.043, New P: 0.361
iter 11 loss: 0.180
Actual params: [-0.8422,  0.3605]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.854
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.372
iter 12 loss: 0.179
Actual params: [-0.8537,  0.3716]
-Original Grad: -0.001, -lr * Pred Grad:  -0.028, New P: -0.881
-Original Grad: 0.001, -lr * Pred Grad:  0.031, New P: 0.402
iter 13 loss: 0.179
Actual params: [-0.8812,  0.4024]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.896
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.416
iter 14 loss: 0.179
Actual params: [-0.896 ,  0.4157]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.913
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.432
iter 15 loss: 0.179
Actual params: [-0.9127,  0.4315]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -0.936
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.456
iter 16 loss: 0.179
Actual params: [-0.9355,  0.4563]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -0.965
-Original Grad: 0.001, -lr * Pred Grad:  0.033, New P: 0.490
iter 17 loss: 0.179
Actual params: [-0.9647,  0.4895]
-Original Grad: -0.001, -lr * Pred Grad:  -0.046, New P: -1.011
-Original Grad: 0.001, -lr * Pred Grad:  0.054, New P: 0.544
iter 18 loss: 0.178
Actual params: [-1.011 ,  0.5439]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.028
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.557
iter 19 loss: 0.178
Actual params: [-1.0278,  0.5574]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.056
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: 0.588
iter 20 loss: 0.178
Actual params: [-1.0561,  0.5878]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.080
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.613
iter 21 loss: 0.178
Actual params: [-1.0801,  0.6126]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.103
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.637
iter 22 loss: 0.178
Actual params: [-1.1029,  0.6368]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.120
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.652
iter 23 loss: 0.178
Actual params: [-1.1202,  0.6517]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.145
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.677
iter 24 loss: 0.178
Actual params: [-1.1453,  0.6766]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.167
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.696
iter 25 loss: 0.178
Actual params: [-1.1667,  0.696 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.195
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 0.722
iter 26 loss: 0.178
Actual params: [-1.1948,  0.7216]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.212
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.737
iter 27 loss: 0.178
Actual params: [-1.2117,  0.7369]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.234
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.757
iter 28 loss: 0.178
Actual params: [-1.2336,  0.7567]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.252
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.774
iter 29 loss: 0.178
Actual params: [-1.2517,  0.7738]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.278
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.798
iter 30 loss: 0.178
Actual params: [-1.278 ,  0.7984]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.292
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.812
Target params: [1.3344, 1.5708]
iter 0 loss: 0.029
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.036, -lr * Pred Grad:  -0.181, New P: -0.653
-Original Grad: 0.005, -lr * Pred Grad:  0.026, New P: 0.030
iter 1 loss: 0.023
Actual params: [-0.6535,  0.0297]
-Original Grad: -0.001, -lr * Pred Grad:  -0.003, New P: -0.657
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.030
iter 2 loss: 0.022
Actual params: [-0.6566,  0.0303]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.657
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.031
iter 3 loss: 0.022
Actual params: [-0.6567,  0.031 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.657
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.033
iter 4 loss: 0.022
Actual params: [-0.6572,  0.0325]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.658
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.034
iter 5 loss: 0.022
Actual params: [-0.6577,  0.0341]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.658
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.036
iter 6 loss: 0.022
Actual params: [-0.658 ,  0.0357]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.659
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.037
iter 7 loss: 0.022
Actual params: [-0.6585,  0.0373]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.659
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.040
iter 8 loss: 0.022
Actual params: [-0.6592,  0.0395]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.662
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.045
iter 9 loss: 0.022
Actual params: [-0.6621,  0.0446]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.047
iter 10 loss: 0.022
Actual params: [-0.6625,  0.0468]
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -0.671
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.049
iter 11 loss: 0.022
Actual params: [-0.6707,  0.0495]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.671
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.052
iter 12 loss: 0.022
Actual params: [-0.6712,  0.0521]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.681
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.054
iter 13 loss: 0.022
Actual params: [-0.6808,  0.0545]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.682
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.057
iter 14 loss: 0.022
Actual params: [-0.6815,  0.0572]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.693
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.061
iter 15 loss: 0.021
Actual params: [-0.6931,  0.0608]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.694
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.066
iter 16 loss: 0.021
Actual params: [-0.6943,  0.066 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.695
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.070
iter 17 loss: 0.021
Actual params: [-0.6953,  0.0704]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.698
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.077
iter 18 loss: 0.021
Actual params: [-0.698 ,  0.0771]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.700
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.087
iter 19 loss: 0.021
Actual params: [-0.7004,  0.0865]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.702
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.093
iter 20 loss: 0.021
Actual params: [-0.7017,  0.0934]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.705
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.103
iter 21 loss: 0.021
Actual params: [-0.705 ,  0.1032]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.706
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.111
iter 22 loss: 0.021
Actual params: [-0.7061,  0.1111]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -0.728
-Original Grad: 0.000, -lr * Pred Grad:  0.027, New P: 0.138
iter 23 loss: 0.021
Actual params: [-0.7281,  0.1377]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -0.751
-Original Grad: 0.000, -lr * Pred Grad:  0.033, New P: 0.171
iter 24 loss: 0.020
Actual params: [-0.7512,  0.1711]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.755
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.190
iter 25 loss: 0.020
Actual params: [-0.7551,  0.1901]
-Original Grad: -0.001, -lr * Pred Grad:  -0.050, New P: -0.805
-Original Grad: 0.001, -lr * Pred Grad:  0.084, New P: 0.274
iter 26 loss: 0.019
Actual params: [-0.8055,  0.2745]
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: -0.805
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.285
iter 27 loss: 0.019
Actual params: [-0.8051,  0.2849]
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: -0.805
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.298
iter 28 loss: 0.019
Actual params: [-0.8049,  0.2977]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.806
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.310
iter 29 loss: 0.019
Actual params: [-0.8056,  0.3098]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -0.805
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.319
iter 30 loss: 0.018
Actual params: [-0.8049,  0.3188]
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: -0.804
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.335
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: -0.463
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.006
iter 1 loss: 0.363
Actual params: [-0.4632,  0.0055]
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: -0.454
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.008
iter 2 loss: 0.363
Actual params: [-0.4538,  0.0076]
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: -0.442
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.010
iter 3 loss: 0.363
Actual params: [-0.4421,  0.0103]
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: -0.428
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.013
iter 4 loss: 0.363
Actual params: [-0.4278,  0.0134]
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: -0.410
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.017
iter 5 loss: 0.363
Actual params: [-0.4095,  0.0173]
-Original Grad: 0.001, -lr * Pred Grad:  0.022, New P: -0.388
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.022
iter 6 loss: 0.363
Actual params: [-0.3879,  0.0217]
-Original Grad: 0.001, -lr * Pred Grad:  0.023, New P: -0.365
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.026
iter 7 loss: 0.363
Actual params: [-0.3647,  0.0265]
-Original Grad: 0.002, -lr * Pred Grad:  0.035, New P: -0.329
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.033
iter 8 loss: 0.363
Actual params: [-0.3294,  0.0335]
-Original Grad: 0.003, -lr * Pred Grad:  0.064, New P: -0.265
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.047
iter 9 loss: 0.363
Actual params: [-0.265 ,  0.0466]
-Original Grad: 0.003, -lr * Pred Grad:  0.088, New P: -0.177
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: 0.065
iter 10 loss: 0.362
Actual params: [-0.177 ,  0.0648]
-Original Grad: 0.006, -lr * Pred Grad:  0.191, New P: 0.014
-Original Grad: 0.001, -lr * Pred Grad:  0.039, New P: 0.104
iter 11 loss: 0.358
Actual params: [0.0145, 0.1038]
-Original Grad: 0.030, -lr * Pred Grad:  0.867, New P: 0.881
-Original Grad: 0.006, -lr * Pred Grad:  0.186, New P: 0.290
iter 12 loss: 0.252
Actual params: [0.8812, 0.2897]
-Original Grad: 0.023, -lr * Pred Grad:  0.009, New P: 0.890
-Original Grad: 0.515, -lr * Pred Grad:  0.425, New P: 0.715
iter 13 loss: 0.096
Actual params: [0.8898, 0.7146]
-Original Grad: 0.037, -lr * Pred Grad:  0.313, New P: 1.203
-Original Grad: 0.380, -lr * Pred Grad:  0.140, New P: 0.854
iter 14 loss: 0.028
Actual params: [1.203 , 0.8544]
-Original Grad: 0.151, -lr * Pred Grad:  0.070, New P: 1.273
-Original Grad: 0.264, -lr * Pred Grad:  0.044, New P: 0.898
iter 15 loss: 0.017
Actual params: [1.2726, 0.8984]
-Original Grad: -0.017, -lr * Pred Grad:  -0.027, New P: 1.245
-Original Grad: 0.209, -lr * Pred Grad:  0.045, New P: 0.943
iter 16 loss: 0.015
Actual params: [1.2453, 0.9434]
-Original Grad: 0.108, -lr * Pred Grad:  0.023, New P: 1.268
-Original Grad: 0.113, -lr * Pred Grad:  0.016, New P: 0.959
iter 17 loss: 0.013
Actual params: [1.2683, 0.9593]
-Original Grad: 0.266, -lr * Pred Grad:  0.043, New P: 1.311
-Original Grad: 0.133, -lr * Pred Grad:  0.007, New P: 0.967
iter 18 loss: 0.011
Actual params: [1.311 , 0.9667]
-Original Grad: 0.205, -lr * Pred Grad:  0.021, New P: 1.332
-Original Grad: 0.097, -lr * Pred Grad:  0.007, New P: 0.973
iter 19 loss: 0.010
Actual params: [1.3317, 0.9735]
-Original Grad: 0.106, -lr * Pred Grad:  0.006, New P: 1.338
-Original Grad: 0.075, -lr * Pred Grad:  0.010, New P: 0.983
iter 20 loss: 0.010
Actual params: [1.3381, 0.9831]
-Original Grad: 0.005, -lr * Pred Grad:  -0.001, New P: 1.337
-Original Grad: 0.047, -lr * Pred Grad:  0.009, New P: 0.992
iter 21 loss: 0.010
Actual params: [1.3366, 0.9922]
-Original Grad: -0.031, -lr * Pred Grad:  -0.005, New P: 1.332
-Original Grad: 0.071, -lr * Pred Grad:  0.014, New P: 1.006
iter 22 loss: 0.010
Actual params: [1.332 , 1.0062]
-Original Grad: 0.005, -lr * Pred Grad:  -0.000, New P: 1.332
-Original Grad: 0.086, -lr * Pred Grad:  0.013, New P: 1.019
iter 23 loss: 0.010
Actual params: [1.3317, 1.0191]
-Original Grad: 0.057, -lr * Pred Grad:  0.004, New P: 1.335
-Original Grad: 0.041, -lr * Pred Grad:  0.006, New P: 1.025
iter 24 loss: 0.010
Actual params: [1.3353, 1.025 ]
-Original Grad: 0.102, -lr * Pred Grad:  0.006, New P: 1.342
-Original Grad: 0.046, -lr * Pred Grad:  0.006, New P: 1.031
iter 25 loss: 0.010
Actual params: [1.3415, 1.0309]
-Original Grad: -0.020, -lr * Pred Grad:  -0.002, New P: 1.340
-Original Grad: 0.012, -lr * Pred Grad:  0.002, New P: 1.033
iter 26 loss: 0.010
Actual params: [1.34  , 1.0333]
-Original Grad: 0.050, -lr * Pred Grad:  0.004, New P: 1.344
-Original Grad: -0.019, -lr * Pred Grad:  -0.004, New P: 1.029
iter 27 loss: 0.010
Actual params: [1.3436, 1.0293]
-Original Grad: -0.011, -lr * Pred Grad:  -0.001, New P: 1.343
-Original Grad: -0.025, -lr * Pred Grad:  -0.005, New P: 1.024
iter 28 loss: 0.010
Actual params: [1.343 , 1.0244]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 1.343
-Original Grad: 0.003, -lr * Pred Grad:  0.001, New P: 1.025
iter 29 loss: 0.010
Actual params: [1.343 , 1.0252]
-Original Grad: -0.053, -lr * Pred Grad:  -0.004, New P: 1.339
-Original Grad: 0.020, -lr * Pred Grad:  0.005, New P: 1.030
iter 30 loss: 0.010
Actual params: [1.3388, 1.0298]
-Original Grad: 0.040, -lr * Pred Grad:  0.003, New P: 1.342
-Original Grad: -0.018, -lr * Pred Grad:  -0.004, New P: 1.025
Target params: [1.3344, 1.5708]
iter 0 loss: 0.829
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.471
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.004
iter 1 loss: 0.829
Actual params: [-0.4715,  0.004 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.469
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.005
iter 2 loss: 0.829
Actual params: [-0.4688,  0.0048]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.469
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.006
iter 3 loss: 0.829
Actual params: [-0.4694,  0.0056]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.471
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.006
iter 4 loss: 0.829
Actual params: [-0.4712,  0.0059]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.470
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.007
iter 5 loss: 0.829
Actual params: [-0.4699,  0.0065]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.470
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.007
iter 6 loss: 0.829
Actual params: [-0.4701,  0.0073]
-Original Grad: -0.001, -lr * Pred Grad:  -0.028, New P: -0.498
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 0.005
iter 7 loss: 0.829
Actual params: [-0.4978,  0.0048]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.500
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.005
iter 8 loss: 0.829
Actual params: [-0.4997,  0.0051]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.499
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.006
iter 9 loss: 0.829
Actual params: [-0.4993,  0.0059]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.505
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.006
iter 10 loss: 0.829
Actual params: [-0.5046,  0.0058]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.503
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.007
iter 11 loss: 0.829
Actual params: [-0.503 ,  0.0069]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.509
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.007
iter 12 loss: 0.829
Actual params: [-0.5086,  0.0066]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.504
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.008
iter 13 loss: 0.829
Actual params: [-0.5036,  0.0083]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.498
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.010
iter 14 loss: 0.829
Actual params: [-0.498 ,  0.0103]
-Original Grad: -0.001, -lr * Pred Grad:  -0.038, New P: -0.536
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 0.008
iter 15 loss: 0.829
Actual params: [-0.536 ,  0.0084]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.532
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.014
iter 16 loss: 0.829
Actual params: [-0.5318,  0.0135]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.538
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: 0.014
iter 17 loss: 0.829
Actual params: [-0.538 ,  0.0135]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.535
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.015
iter 18 loss: 0.829
Actual params: [-0.535,  0.015]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.540
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.020
iter 19 loss: 0.829
Actual params: [-0.54  ,  0.0199]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.546
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.022
iter 20 loss: 0.829
Actual params: [-0.5459,  0.0216]
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: -0.534
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.025
iter 21 loss: 0.829
Actual params: [-0.5338,  0.0253]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.546
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.025
iter 22 loss: 0.829
Actual params: [-0.5464,  0.025 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.540
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.028
iter 23 loss: 0.829
Actual params: [-0.5398,  0.0283]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.537
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.032
iter 24 loss: 0.829
Actual params: [-0.5365,  0.032 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.539
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.034
iter 25 loss: 0.829
Actual params: [-0.539 ,  0.0342]
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: -0.531
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.040
iter 26 loss: 0.829
Actual params: [-0.5308,  0.0396]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -0.561
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.042
iter 27 loss: 0.829
Actual params: [-0.5613,  0.0421]
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: -0.545
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.048
iter 28 loss: 0.829
Actual params: [-0.5445,  0.0476]
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: -0.516
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.057
iter 29 loss: 0.829
Actual params: [-0.5156,  0.0575]
-Original Grad: 0.000, -lr * Pred Grad:  0.076, New P: -0.440
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: 0.087
iter 30 loss: 0.829
Actual params: [-0.44 ,  0.087]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.451
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.099
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.011, -lr * Pred Grad:  -0.125, New P: -0.598
-Original Grad: -0.007, -lr * Pred Grad:  -0.071, New P: -0.068
iter 1 loss: 0.118
Actual params: [-0.5976, -0.0678]
-Original Grad: -0.002, -lr * Pred Grad:  -0.028, New P: -0.626
-Original Grad: -0.002, -lr * Pred Grad:  -0.021, New P: -0.089
iter 2 loss: 0.118
Actual params: [-0.6257, -0.0891]
-Original Grad: -0.003, -lr * Pred Grad:  -0.041, New P: -0.667
-Original Grad: -0.002, -lr * Pred Grad:  -0.027, New P: -0.116
iter 3 loss: 0.118
Actual params: [-0.667 , -0.1159]
-Original Grad: -0.002, -lr * Pred Grad:  -0.028, New P: -0.695
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.135
iter 4 loss: 0.118
Actual params: [-0.695, -0.135]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -0.716
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.150
iter 5 loss: 0.118
Actual params: [-0.716, -0.15 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.735
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.164
iter 6 loss: 0.118
Actual params: [-0.7351, -0.1636]
-Original Grad: -0.001, -lr * Pred Grad:  -0.025, New P: -0.760
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.180
iter 7 loss: 0.118
Actual params: [-0.7597, -0.1798]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.777
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.192
iter 8 loss: 0.118
Actual params: [-0.7765, -0.1916]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.793
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.203
iter 9 loss: 0.118
Actual params: [-0.7933, -0.2034]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.806
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.212
iter 10 loss: 0.118
Actual params: [-0.8057, -0.2123]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.822
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.224
iter 11 loss: 0.118
Actual params: [-0.8217, -0.2236]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.836
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.234
iter 12 loss: 0.118
Actual params: [-0.8356, -0.2337]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.847
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.242
iter 13 loss: 0.118
Actual params: [-0.8468, -0.2423]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.858
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.251
iter 14 loss: 0.118
Actual params: [-0.858 , -0.2511]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.868
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.259
iter 15 loss: 0.118
Actual params: [-0.8684, -0.2591]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.881
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.269
iter 16 loss: 0.118
Actual params: [-0.8811, -0.2686]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.893
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.277
iter 17 loss: 0.118
Actual params: [-0.8927, -0.2774]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.906
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.288
iter 18 loss: 0.118
Actual params: [-0.906 , -0.2882]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.919
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.298
iter 19 loss: 0.118
Actual params: [-0.9194, -0.2979]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.930
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.306
iter 20 loss: 0.118
Actual params: [-0.9302, -0.3059]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.945
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.316
iter 21 loss: 0.118
Actual params: [-0.9449, -0.3164]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.957
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.325
iter 22 loss: 0.118
Actual params: [-0.9568, -0.3251]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.967
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.332
iter 23 loss: 0.118
Actual params: [-0.9667, -0.3323]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.980
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.342
iter 24 loss: 0.118
Actual params: [-0.9796, -0.3416]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.993
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.351
iter 25 loss: 0.118
Actual params: [-0.9928, -0.351 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.003
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.359
iter 26 loss: 0.118
Actual params: [-1.0034, -0.3588]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.015
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.367
iter 27 loss: 0.118
Actual params: [-1.015 , -0.3671]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.026
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.375
iter 28 loss: 0.118
Actual params: [-1.0265, -0.3752]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.040
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.385
iter 29 loss: 0.118
Actual params: [-1.0396, -0.3846]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.050
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.392
iter 30 loss: 0.118
Actual params: [-1.0503, -0.3924]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.061
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.400
Target params: [1.3344, 1.5708]
iter 0 loss: 0.081
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.007, -lr * Pred Grad:  -0.078, New P: -0.550
-Original Grad: 0.005, -lr * Pred Grad:  0.054, New P: 0.058
iter 1 loss: 0.080
Actual params: [-0.5499,  0.0576]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.568
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: 0.075
iter 2 loss: 0.080
Actual params: [-0.568 ,  0.0754]
-Original Grad: -0.002, -lr * Pred Grad:  -0.027, New P: -0.595
-Original Grad: 0.002, -lr * Pred Grad:  0.023, New P: 0.098
iter 3 loss: 0.080
Actual params: [-0.595 ,  0.0982]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.610
-Original Grad: 0.001, -lr * Pred Grad:  0.020, New P: 0.118
iter 4 loss: 0.080
Actual params: [-0.6097,  0.1178]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.624
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.130
iter 5 loss: 0.080
Actual params: [-0.6245,  0.1302]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.638
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.144
iter 6 loss: 0.080
Actual params: [-0.6379,  0.1439]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.650
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.153
iter 7 loss: 0.080
Actual params: [-0.6498,  0.1531]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.665
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.165
iter 8 loss: 0.080
Actual params: [-0.6654,  0.1651]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.677
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.179
iter 9 loss: 0.080
Actual params: [-0.6771,  0.1794]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.686
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.191
iter 10 loss: 0.080
Actual params: [-0.6863,  0.1906]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.690
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.196
iter 11 loss: 0.080
Actual params: [-0.6903,  0.1965]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.704
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.207
iter 12 loss: 0.080
Actual params: [-0.7043,  0.207 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.713
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.214
iter 13 loss: 0.080
Actual params: [-0.7126,  0.2144]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.718
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.227
iter 14 loss: 0.080
Actual params: [-0.7177,  0.2274]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.725
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.237
iter 15 loss: 0.080
Actual params: [-0.7245,  0.2366]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.727
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.247
iter 16 loss: 0.080
Actual params: [-0.7269,  0.2473]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.726
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.257
iter 17 loss: 0.080
Actual params: [-0.7265,  0.2566]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.722
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.263
iter 18 loss: 0.080
Actual params: [-0.7225,  0.2625]
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: -0.712
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.267
iter 19 loss: 0.080
Actual params: [-0.7122,  0.2667]
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: -0.697
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.277
iter 20 loss: 0.080
Actual params: [-0.6971,  0.2767]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -0.728
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.294
iter 21 loss: 0.080
Actual params: [-0.7282,  0.2942]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.725
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.306
iter 22 loss: 0.080
Actual params: [-0.7254,  0.3055]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.742
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.325
iter 23 loss: 0.080
Actual params: [-0.7418,  0.3253]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.742
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.333
iter 24 loss: 0.080
Actual params: [-0.7425,  0.333 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: -0.716
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.344
iter 25 loss: 0.080
Actual params: [-0.7161,  0.344 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.717
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.357
iter 26 loss: 0.080
Actual params: [-0.7172,  0.3571]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.713
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.371
iter 27 loss: 0.080
Actual params: [-0.7125,  0.3713]
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -0.745
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.396
iter 28 loss: 0.080
Actual params: [-0.7453,  0.3959]
-Original Grad: 0.000, -lr * Pred Grad:  0.051, New P: -0.694
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.399
iter 29 loss: 0.080
Actual params: [-0.694 ,  0.3989]
-Original Grad: 0.000, -lr * Pred Grad:  0.040, New P: -0.654
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.423
iter 30 loss: 0.080
Actual params: [-0.654 ,  0.4226]
-Original Grad: 0.000, -lr * Pred Grad:  0.061, New P: -0.593
-Original Grad: 0.000, -lr * Pred Grad:  0.031, New P: 0.454
Target params: [1.3344, 1.5708]
iter 0 loss: 0.492
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.061, -lr * Pred Grad:  0.030, New P: -0.442
-Original Grad: 0.136, -lr * Pred Grad:  0.613, New P: 0.616
iter 1 loss: 0.430
Actual params: [-0.4421,  0.6164]
-Original Grad: 0.174, -lr * Pred Grad:  0.117, New P: -0.325
-Original Grad: 0.158, -lr * Pred Grad:  0.390, New P: 1.007
iter 2 loss: 0.374
Actual params: [-0.3252,  1.0067]
-Original Grad: 0.354, -lr * Pred Grad:  0.323, New P: -0.002
-Original Grad: 0.052, -lr * Pred Grad:  -0.099, New P: 0.907
iter 3 loss: 0.261
Actual params: [-0.0024,  0.9075]
-Original Grad: 0.448, -lr * Pred Grad:  0.179, New P: 0.176
-Original Grad: 0.187, -lr * Pred Grad:  0.168, New P: 1.076
iter 4 loss: 0.178
Actual params: [0.1761, 1.0756]
-Original Grad: 0.595, -lr * Pred Grad:  0.083, New P: 0.259
-Original Grad: 0.321, -lr * Pred Grad:  0.176, New P: 1.252
iter 5 loss: 0.115
Actual params: [0.259 , 1.2521]
-Original Grad: 0.549, -lr * Pred Grad:  0.040, New P: 0.299
-Original Grad: 0.294, -lr * Pred Grad:  0.099, New P: 1.351
iter 6 loss: 0.083
Actual params: [0.2988, 1.3506]
-Original Grad: 0.350, -lr * Pred Grad:  -0.016, New P: 0.283
-Original Grad: 0.359, -lr * Pred Grad:  0.135, New P: 1.485
iter 7 loss: 0.069
Actual params: [0.2826, 1.4852]
-Original Grad: 0.397, -lr * Pred Grad:  0.027, New P: 0.309
-Original Grad: 0.220, -lr * Pred Grad:  0.028, New P: 1.514
iter 8 loss: 0.057
Actual params: [0.3092, 1.5136]
-Original Grad: 0.331, -lr * Pred Grad:  0.026, New P: 0.336
-Original Grad: 0.121, -lr * Pred Grad:  0.004, New P: 1.518
iter 9 loss: 0.049
Actual params: [0.3356, 1.5177]
-Original Grad: 0.230, -lr * Pred Grad:  0.015, New P: 0.350
-Original Grad: 0.125, -lr * Pred Grad:  0.016, New P: 1.534
iter 10 loss: 0.043
Actual params: [0.3502, 1.5337]
-Original Grad: 0.373, -lr * Pred Grad:  0.027, New P: 0.377
-Original Grad: 0.087, -lr * Pred Grad:  -0.001, New P: 1.532
iter 11 loss: 0.037
Actual params: [0.3773, 1.5322]
-Original Grad: 0.246, -lr * Pred Grad:  0.013, New P: 0.390
-Original Grad: 0.117, -lr * Pred Grad:  0.014, New P: 1.546
iter 12 loss: 0.034
Actual params: [0.3903, 1.5459]
-Original Grad: 0.256, -lr * Pred Grad:  0.014, New P: 0.404
-Original Grad: 0.097, -lr * Pred Grad:  0.009, New P: 1.555
iter 13 loss: 0.030
Actual params: [0.4041, 1.5551]
-Original Grad: 0.217, -lr * Pred Grad:  0.012, New P: 0.416
-Original Grad: 0.079, -lr * Pred Grad:  0.007, New P: 1.562
iter 14 loss: 0.028
Actual params: [0.4158, 1.5618]
-Original Grad: 0.113, -lr * Pred Grad:  0.006, New P: 0.422
-Original Grad: 0.053, -lr * Pred Grad:  0.007, New P: 1.569
iter 15 loss: 0.027
Actual params: [0.4217, 1.5686]
-Original Grad: 0.114, -lr * Pred Grad:  0.005, New P: 0.426
-Original Grad: 0.096, -lr * Pred Grad:  0.016, New P: 1.585
iter 16 loss: 0.025
Actual params: [0.4262, 1.5847]
-Original Grad: 0.268, -lr * Pred Grad:  0.015, New P: 0.441
-Original Grad: 0.108, -lr * Pred Grad:  0.011, New P: 1.596
iter 17 loss: 0.023
Actual params: [0.4411, 1.5962]
-Original Grad: 0.232, -lr * Pred Grad:  0.013, New P: 0.454
-Original Grad: 0.078, -lr * Pred Grad:  0.006, New P: 1.603
iter 18 loss: 0.021
Actual params: [0.4542, 1.6026]
-Original Grad: 0.132, -lr * Pred Grad:  0.006, New P: 0.460
-Original Grad: 0.077, -lr * Pred Grad:  0.011, New P: 1.613
iter 19 loss: 0.020
Actual params: [0.4604, 1.6135]
-Original Grad: 0.131, -lr * Pred Grad:  0.004, New P: 0.464
-Original Grad: 0.136, -lr * Pred Grad:  0.022, New P: 1.636
iter 20 loss: 0.019
Actual params: [0.4642, 1.6358]
-Original Grad: 0.140, -lr * Pred Grad:  0.007, New P: 0.471
-Original Grad: 0.100, -lr * Pred Grad:  0.014, New P: 1.650
iter 21 loss: 0.018
Actual params: [0.4709, 1.6498]
-Original Grad: 0.105, -lr * Pred Grad:  0.004, New P: 0.475
-Original Grad: 0.129, -lr * Pred Grad:  0.020, New P: 1.670
iter 22 loss: 0.018
Actual params: [0.4746, 1.6696]
-Original Grad: 0.118, -lr * Pred Grad:  0.010, New P: 0.484
-Original Grad: -0.009, -lr * Pred Grad:  -0.006, New P: 1.664
iter 23 loss: 0.017
Actual params: [0.4844, 1.6635]
-Original Grad: 0.148, -lr * Pred Grad:  0.008, New P: 0.493
-Original Grad: 0.098, -lr * Pred Grad:  0.012, New P: 1.676
iter 24 loss: 0.017
Actual params: [0.4927, 1.6755]
-Original Grad: 0.073, -lr * Pred Grad:  0.005, New P: 0.498
-Original Grad: 0.020, -lr * Pred Grad:  0.001, New P: 1.676
iter 25 loss: 0.016
Actual params: [0.498 , 1.6761]
-Original Grad: 0.015, -lr * Pred Grad:  0.001, New P: 0.499
-Original Grad: 0.014, -lr * Pred Grad:  0.002, New P: 1.678
iter 26 loss: 0.016
Actual params: [0.4985, 1.678 ]
-Original Grad: 0.069, -lr * Pred Grad:  0.005, New P: 0.504
-Original Grad: 0.016, -lr * Pred Grad:  0.000, New P: 1.678
iter 27 loss: 0.016
Actual params: [0.5037, 1.678 ]
-Original Grad: 0.031, -lr * Pred Grad:  0.001, New P: 0.505
-Original Grad: 0.040, -lr * Pred Grad:  0.006, New P: 1.684
iter 28 loss: 0.016
Actual params: [0.5046, 1.6839]
-Original Grad: 0.102, -lr * Pred Grad:  0.006, New P: 0.511
-Original Grad: 0.056, -lr * Pred Grad:  0.005, New P: 1.689
iter 29 loss: 0.016
Actual params: [0.5111, 1.6891]
-Original Grad: 0.008, -lr * Pred Grad:  0.002, New P: 0.513
-Original Grad: -0.021, -lr * Pred Grad:  -0.004, New P: 1.685
iter 30 loss: 0.016
Actual params: [0.5127, 1.6849]
-Original Grad: -0.071, -lr * Pred Grad:  -0.006, New P: 0.507
-Original Grad: -0.002, -lr * Pred Grad:  0.003, New P: 1.688
Target params: [1.3344, 1.5708]
iter 0 loss: 0.239
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.004, -lr * Pred Grad:  0.047, New P: -0.425
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.011
iter 1 loss: 0.238
Actual params: [-0.4252, -0.0112]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.425
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.011
iter 2 loss: 0.238
Actual params: [-0.4247, -0.0111]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.424
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.011
iter 3 loss: 0.238
Actual params: [-0.4241, -0.0109]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.423
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.011
iter 4 loss: 0.238
Actual params: [-0.4234, -0.0107]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.423
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.011
iter 5 loss: 0.238
Actual params: [-0.4229, -0.0106]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.422
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.010
iter 6 loss: 0.238
Actual params: [-0.4218, -0.0103]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.421
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.010
iter 7 loss: 0.238
Actual params: [-0.421, -0.01 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.420
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.010
iter 8 loss: 0.238
Actual params: [-0.4197, -0.0097]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.419
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.009
iter 9 loss: 0.238
Actual params: [-0.4186, -0.0093]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.417
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.009
iter 10 loss: 0.238
Actual params: [-0.4172, -0.0089]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.416
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.009
iter 11 loss: 0.238
Actual params: [-0.4161, -0.0086]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.415
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.009
iter 12 loss: 0.238
Actual params: [-0.4151, -0.0086]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.414
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.008
iter 13 loss: 0.238
Actual params: [-0.4138, -0.0084]
-Original Grad: 0.003, -lr * Pred Grad:  0.134, New P: -0.280
-Original Grad: -0.001, -lr * Pred Grad:  -0.042, New P: -0.050
iter 14 loss: 0.234
Actual params: [-0.2797, -0.05  ]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.277
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.050
iter 15 loss: 0.234
Actual params: [-0.2766, -0.0497]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.273
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.049
iter 16 loss: 0.234
Actual params: [-0.273 , -0.0493]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.268
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.049
iter 17 loss: 0.234
Actual params: [-0.2684, -0.0486]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.262
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.047
iter 18 loss: 0.234
Actual params: [-0.2624, -0.0474]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.257
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.047
iter 19 loss: 0.233
Actual params: [-0.2568, -0.0466]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.251
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.046
iter 20 loss: 0.233
Actual params: [-0.2512, -0.0461]
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: -0.243
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.045
iter 21 loss: 0.233
Actual params: [-0.2434, -0.0448]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.237
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.044
iter 22 loss: 0.233
Actual params: [-0.2375, -0.0445]
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: -0.228
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.043
iter 23 loss: 0.232
Actual params: [-0.2284, -0.0434]
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: -0.220
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.042
iter 24 loss: 0.232
Actual params: [-0.22  , -0.0418]
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: -0.208
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.040
iter 25 loss: 0.232
Actual params: [-0.2076, -0.0401]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.201
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.040
iter 26 loss: 0.231
Actual params: [-0.2014, -0.0401]
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: -0.184
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.038
iter 27 loss: 0.231
Actual params: [-0.1844, -0.0376]
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: -0.168
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.034
iter 28 loss: 0.230
Actual params: [-0.1678, -0.0343]
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: -0.157
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.034
iter 29 loss: 0.230
Actual params: [-0.157 , -0.0341]
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: -0.132
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.031
iter 30 loss: 0.229
Actual params: [-0.1323, -0.0307]
-Original Grad: 0.000, -lr * Pred Grad:  0.031, New P: -0.101
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.027
Target params: [1.3344, 1.5708]
iter 0 loss: 0.065
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: -0.466
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.011
iter 1 loss: 0.064
Actual params: [-0.4657,  0.0109]
-Original Grad: 0.002, -lr * Pred Grad:  0.024, New P: -0.442
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.024
iter 2 loss: 0.063
Actual params: [-0.4416,  0.0241]
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: -0.434
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.031
iter 3 loss: 0.063
Actual params: [-0.4338,  0.0305]
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: -0.422
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.045
iter 4 loss: 0.062
Actual params: [-0.4224,  0.0446]
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: -0.408
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.058
iter 5 loss: 0.061
Actual params: [-0.4081,  0.0583]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.403
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 0.075
iter 6 loss: 0.061
Actual params: [-0.4032,  0.0749]
-Original Grad: 0.014, -lr * Pred Grad:  0.253, New P: -0.150
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.081
iter 7 loss: 0.039
Actual params: [-0.1499,  0.0811]
-Original Grad: 0.005, -lr * Pred Grad:  0.091, New P: -0.059
-Original Grad: 0.003, -lr * Pred Grad:  0.063, New P: 0.144
iter 8 loss: 0.034
Actual params: [-0.0592,  0.1436]
-Original Grad: 0.013, -lr * Pred Grad:  0.229, New P: 0.170
-Original Grad: 0.006, -lr * Pred Grad:  0.132, New P: 0.276
iter 9 loss: 0.038
Actual params: [0.1701, 0.2755]
-Original Grad: -0.078, -lr * Pred Grad:  -0.011, New P: 0.160
-Original Grad: 0.038, -lr * Pred Grad:  0.116, New P: 0.392
iter 10 loss: 0.030
Actual params: [0.1595, 0.392 ]
-Original Grad: 0.010, -lr * Pred Grad:  0.106, New P: 0.266
-Original Grad: 0.007, -lr * Pred Grad:  0.239, New P: 0.631
iter 11 loss: 0.028
Actual params: [0.2659, 0.6309]
-Original Grad: 0.022, -lr * Pred Grad:  0.165, New P: 0.431
-Original Grad: 0.009, -lr * Pred Grad:  0.362, New P: 0.992
iter 12 loss: 0.025
Actual params: [0.431 , 0.9924]
-Original Grad: 0.057, -lr * Pred Grad:  0.156, New P: 0.587
-Original Grad: -0.005, -lr * Pred Grad:  0.274, New P: 1.267
iter 13 loss: 0.023
Actual params: [0.5867, 1.2666]
-Original Grad: 0.087, -lr * Pred Grad:  0.143, New P: 0.730
-Original Grad: -0.007, -lr * Pred Grad:  0.255, New P: 1.521
iter 14 loss: 0.021
Actual params: [0.7301, 1.5212]
-Original Grad: 0.068, -lr * Pred Grad:  0.093, New P: 0.824
-Original Grad: -0.001, -lr * Pred Grad:  0.182, New P: 1.703
iter 15 loss: 0.027
Actual params: [0.8235, 1.7029]
-Original Grad: -0.001, -lr * Pred Grad:  -0.039, New P: 0.785
-Original Grad: -0.067, -lr * Pred Grad:  -0.200, New P: 1.503
iter 16 loss: 0.019
Actual params: [0.7847, 1.5026]
-Original Grad: 0.069, -lr * Pred Grad:  0.050, New P: 0.835
-Original Grad: -0.014, -lr * Pred Grad:  -0.009, New P: 1.493
iter 17 loss: 0.017
Actual params: [0.8346, 1.4933]
-Original Grad: 0.009, -lr * Pred Grad:  -0.002, New P: 0.832
-Original Grad: -0.032, -lr * Pred Grad:  -0.069, New P: 1.424
iter 18 loss: 0.016
Actual params: [0.8322, 1.4239]
-Original Grad: 0.077, -lr * Pred Grad:  0.049, New P: 0.881
-Original Grad: -0.027, -lr * Pred Grad:  -0.037, New P: 1.387
iter 19 loss: 0.015
Actual params: [0.8809, 1.387 ]
-Original Grad: 0.142, -lr * Pred Grad:  0.072, New P: 0.952
-Original Grad: -0.012, -lr * Pred Grad:  -0.000, New P: 1.387
iter 20 loss: 0.013
Actual params: [0.9524, 1.3866]
-Original Grad: 0.051, -lr * Pred Grad:  0.020, New P: 0.972
-Original Grad: -0.037, -lr * Pred Grad:  -0.050, New P: 1.337
iter 21 loss: 0.014
Actual params: [0.972 , 1.3366]
-Original Grad: 0.023, -lr * Pred Grad:  0.011, New P: 0.983
-Original Grad: -0.070, -lr * Pred Grad:  -0.070, New P: 1.267
iter 22 loss: 0.017
Actual params: [0.9828, 1.2669]
-Original Grad: 0.025, -lr * Pred Grad:  0.008, New P: 0.990
-Original Grad: 0.123, -lr * Pred Grad:  0.069, New P: 1.336
iter 23 loss: 0.013
Actual params: [0.9904, 1.3357]
-Original Grad: 0.036, -lr * Pred Grad:  0.015, New P: 1.006
-Original Grad: -0.048, -lr * Pred Grad:  -0.027, New P: 1.309
iter 24 loss: 0.014
Actual params: [1.0058, 1.3092]
-Original Grad: 0.059, -lr * Pred Grad:  0.022, New P: 1.028
-Original Grad: -0.039, -lr * Pred Grad:  -0.024, New P: 1.285
iter 25 loss: 0.015
Actual params: [1.0283, 1.2852]
-Original Grad: 0.122, -lr * Pred Grad:  0.039, New P: 1.067
-Original Grad: -0.007, -lr * Pred Grad:  -0.010, New P: 1.276
iter 26 loss: 0.016
Actual params: [1.0673, 1.2757]
-Original Grad: 0.058, -lr * Pred Grad:  0.019, New P: 1.086
-Original Grad: 0.010, -lr * Pred Grad:  0.004, New P: 1.280
iter 27 loss: 0.015
Actual params: [1.0863, 1.2796]
-Original Grad: 0.046, -lr * Pred Grad:  0.015, New P: 1.101
-Original Grad: 0.043, -lr * Pred Grad:  0.023, New P: 1.302
iter 28 loss: 0.014
Actual params: [1.1009, 1.3023]
-Original Grad: 0.040, -lr * Pred Grad:  0.015, New P: 1.116
-Original Grad: 0.001, -lr * Pred Grad:  -0.001, New P: 1.301
iter 29 loss: 0.014
Actual params: [1.116 , 1.3011]
-Original Grad: 0.035, -lr * Pred Grad:  0.016, New P: 1.132
-Original Grad: -0.034, -lr * Pred Grad:  -0.021, New P: 1.281
iter 30 loss: 0.015
Actual params: [1.1317, 1.2806]
-Original Grad: 0.043, -lr * Pred Grad:  0.018, New P: 1.150
-Original Grad: 0.004, -lr * Pred Grad:  -0.000, New P: 1.280
Target params: [1.3344, 1.5708]
iter 0 loss: 0.575
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.061, -lr * Pred Grad:  -0.489, New P: -0.962
-Original Grad: 0.065, -lr * Pred Grad:  0.525, New P: 0.528
iter 1 loss: 0.542
Actual params: [-0.9616,  0.5281]
-Original Grad: 0.001, -lr * Pred Grad:  0.027, New P: -0.934
-Original Grad: 0.007, -lr * Pred Grad:  0.078, New P: 0.606
iter 2 loss: 0.541
Actual params: [-0.9341,  0.6062]
-Original Grad: 0.003, -lr * Pred Grad:  0.051, New P: -0.883
-Original Grad: 0.006, -lr * Pred Grad:  0.075, New P: 0.681
iter 3 loss: 0.540
Actual params: [-0.8831,  0.6807]
-Original Grad: 0.011, -lr * Pred Grad:  0.161, New P: -0.722
-Original Grad: 0.007, -lr * Pred Grad:  0.112, New P: 0.793
iter 4 loss: 0.537
Actual params: [-0.7217,  0.7931]
-Original Grad: 0.010, -lr * Pred Grad:  0.147, New P: -0.575
-Original Grad: 0.002, -lr * Pred Grad:  0.046, New P: 0.839
iter 5 loss: 0.531
Actual params: [-0.5751,  0.8394]
-Original Grad: 0.017, -lr * Pred Grad:  0.256, New P: -0.320
-Original Grad: -0.002, -lr * Pred Grad:  0.001, New P: 0.841
iter 6 loss: 0.513
Actual params: [-0.3195,  0.8407]
-Original Grad: 0.035, -lr * Pred Grad:  0.534, New P: 0.215
-Original Grad: -0.003, -lr * Pred Grad:  0.049, New P: 0.890
iter 7 loss: 0.434
Actual params: [0.2148, 0.8901]
-Original Grad: 0.162, -lr * Pred Grad:  0.750, New P: 0.965
-Original Grad: 0.071, -lr * Pred Grad:  0.527, New P: 1.417
iter 8 loss: 0.169
Actual params: [0.965 , 1.4168]
-Original Grad: 0.326, -lr * Pred Grad:  0.064, New P: 1.029
-Original Grad: 0.312, -lr * Pred Grad:  0.534, New P: 1.951
iter 9 loss: 0.074
Actual params: [1.0294, 1.9512]
-Original Grad: 0.207, -lr * Pred Grad:  0.015, New P: 1.044
-Original Grad: 0.229, -lr * Pred Grad:  0.234, New P: 2.186
iter 10 loss: 0.076
Actual params: [1.0439, 2.1856]
-Original Grad: 0.218, -lr * Pred Grad:  0.132, New P: 1.175
-Original Grad: 0.026, -lr * Pred Grad:  -0.074, New P: 2.111
iter 11 loss: 0.065
Actual params: [1.1755, 2.1111]
-Original Grad: 0.024, -lr * Pred Grad:  -0.050, New P: 1.126
-Original Grad: 0.203, -lr * Pred Grad:  0.127, New P: 2.238
iter 12 loss: 0.077
Actual params: [1.1256, 2.2385]
-Original Grad: 0.057, -lr * Pred Grad:  0.058, New P: 1.183
-Original Grad: -0.207, -lr * Pred Grad:  -0.086, New P: 2.153
iter 13 loss: 0.067
Actual params: [1.1832, 2.1525]
-Original Grad: 0.033, -lr * Pred Grad:  -0.003, New P: 1.180
-Original Grad: 0.160, -lr * Pred Grad:  0.050, New P: 2.203
iter 14 loss: 0.071
Actual params: [1.1802, 2.2028]
-Original Grad: 0.033, -lr * Pred Grad:  0.011, New P: 1.191
-Original Grad: 0.070, -lr * Pred Grad:  0.017, New P: 2.219
iter 15 loss: 0.072
Actual params: [1.191 , 2.2194]
-Original Grad: 0.020, -lr * Pred Grad:  0.014, New P: 1.205
-Original Grad: -0.042, -lr * Pred Grad:  -0.011, New P: 2.208
iter 16 loss: 0.071
Actual params: [1.205 , 2.2081]
-Original Grad: 0.031, -lr * Pred Grad:  0.015, New P: 1.220
-Original Grad: 0.049, -lr * Pred Grad:  0.009, New P: 2.217
iter 17 loss: 0.071
Actual params: [1.2202, 2.217 ]
-Original Grad: 0.034, -lr * Pred Grad:  0.022, New P: 1.242
-Original Grad: -0.003, -lr * Pred Grad:  -0.003, New P: 2.214
iter 18 loss: 0.071
Actual params: [1.2418, 2.2144]
-Original Grad: -0.019, -lr * Pred Grad:  -0.016, New P: 1.226
-Original Grad: 0.066, -lr * Pred Grad:  0.014, New P: 2.228
iter 19 loss: 0.073
Actual params: [1.2255, 2.2282]
-Original Grad: -0.006, -lr * Pred Grad:  -0.002, New P: 1.224
-Original Grad: -0.089, -lr * Pred Grad:  -0.016, New P: 2.213
iter 20 loss: 0.071
Actual params: [1.2236, 2.2126]
-Original Grad: 0.018, -lr * Pred Grad:  0.016, New P: 1.240
-Original Grad: -0.050, -lr * Pred Grad:  -0.009, New P: 2.204
iter 21 loss: 0.070
Actual params: [1.2396, 2.2037]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 1.236
-Original Grad: 0.119, -lr * Pred Grad:  0.019, New P: 2.223
iter 22 loss: 0.072
Actual params: [1.2363, 2.2231]
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: 1.236
-Original Grad: -0.048, -lr * Pred Grad:  -0.007, New P: 2.216
iter 23 loss: 0.071
Actual params: [1.2355, 2.2157]
-Original Grad: -0.013, -lr * Pred Grad:  -0.013, New P: 1.223
-Original Grad: -0.090, -lr * Pred Grad:  -0.013, New P: 2.203
iter 24 loss: 0.070
Actual params: [1.223 , 2.2028]
-Original Grad: 0.010, -lr * Pred Grad:  0.007, New P: 1.230
-Original Grad: 0.221, -lr * Pred Grad:  0.032, New P: 2.235
iter 25 loss: 0.073
Actual params: [1.2301, 2.2351]
-Original Grad: 0.005, -lr * Pred Grad:  0.007, New P: 1.237
-Original Grad: -0.260, -lr * Pred Grad:  -0.036, New P: 2.200
iter 26 loss: 0.070
Actual params: [1.2375, 2.1996]
-Original Grad: 0.003, -lr * Pred Grad:  0.004, New P: 1.241
-Original Grad: 0.016, -lr * Pred Grad:  0.002, New P: 2.202
iter 27 loss: 0.070
Actual params: [1.2411, 2.2017]
-Original Grad: 0.012, -lr * Pred Grad:  0.016, New P: 1.257
-Original Grad: -0.054, -lr * Pred Grad:  -0.007, New P: 2.194
iter 28 loss: 0.069
Actual params: [1.2573, 2.1945]
-Original Grad: 0.034, -lr * Pred Grad:  0.047, New P: 1.304
-Original Grad: 0.032, -lr * Pred Grad:  0.004, New P: 2.198
iter 29 loss: 0.069
Actual params: [1.3039, 2.1981]
-Original Grad: -0.020, -lr * Pred Grad:  -0.032, New P: 1.272
-Original Grad: 0.225, -lr * Pred Grad:  0.031, New P: 2.229
iter 30 loss: 0.073
Actual params: [1.2722, 2.229 ]
-Original Grad: -0.020, -lr * Pred Grad:  -0.032, New P: 1.240
-Original Grad: -0.043, -lr * Pred Grad:  -0.006, New P: 2.223
Target params: [1.3344, 1.5708]
iter 0 loss: 0.309
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.481
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.011
iter 1 loss: 0.309
Actual params: [-0.4812,  0.0107]
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -0.489
-Original Grad: 0.001, -lr * Pred Grad:  0.006, New P: 0.017
iter 2 loss: 0.309
Actual params: [-0.4894,  0.0169]
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.498
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.023
iter 3 loss: 0.309
Actual params: [-0.4982,  0.0231]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.506
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.028
iter 4 loss: 0.309
Actual params: [-0.5057,  0.0284]
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.514
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.039
iter 5 loss: 0.309
Actual params: [-0.5144,  0.0392]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.520
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.048
iter 6 loss: 0.309
Actual params: [-0.52  ,  0.0476]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.528
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.054
iter 7 loss: 0.309
Actual params: [-0.5278,  0.0543]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.537
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.066
iter 8 loss: 0.309
Actual params: [-0.5373,  0.0656]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.544
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.079
iter 9 loss: 0.309
Actual params: [-0.5442,  0.0786]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.550
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.088
iter 10 loss: 0.309
Actual params: [-0.5497,  0.0877]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.556
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.101
iter 11 loss: 0.309
Actual params: [-0.5564,  0.1011]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.560
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.118
iter 12 loss: 0.309
Actual params: [-0.5599,  0.1183]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.561
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.130
iter 13 loss: 0.309
Actual params: [-0.5606,  0.1301]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.559
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.150
iter 14 loss: 0.309
Actual params: [-0.5589,  0.1502]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.558
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.168
iter 15 loss: 0.309
Actual params: [-0.5576,  0.1682]
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: -0.548
-Original Grad: 0.001, -lr * Pred Grad:  0.028, New P: 0.196
iter 16 loss: 0.309
Actual params: [-0.5479,  0.1962]
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: -0.537
-Original Grad: 0.000, -lr * Pred Grad:  0.027, New P: 0.223
iter 17 loss: 0.309
Actual params: [-0.5366,  0.2229]
-Original Grad: 0.001, -lr * Pred Grad:  0.041, New P: -0.496
-Original Grad: 0.001, -lr * Pred Grad:  0.060, New P: 0.283
iter 18 loss: 0.309
Actual params: [-0.496 ,  0.2833]
-Original Grad: 0.002, -lr * Pred Grad:  0.132, New P: -0.364
-Original Grad: 0.002, -lr * Pred Grad:  0.133, New P: 0.416
iter 19 loss: 0.308
Actual params: [-0.3644,  0.4158]
-Original Grad: 0.009, -lr * Pred Grad:  0.688, New P: 0.323
-Original Grad: 0.007, -lr * Pred Grad:  0.540, New P: 0.955
iter 20 loss: 0.099
Actual params: [0.3233, 0.9555]
-Original Grad: 0.352, -lr * Pred Grad:  0.070, New P: 0.394
-Original Grad: 0.078, -lr * Pred Grad:  0.401, New P: 1.356
iter 21 loss: 0.061
Actual params: [0.3935, 1.3565]
-Original Grad: 0.273, -lr * Pred Grad:  -0.048, New P: 0.346
-Original Grad: 0.119, -lr * Pred Grad:  0.553, New P: 1.909
iter 22 loss: 0.072
Actual params: [0.3457, 1.9091]
-Original Grad: 0.185, -lr * Pred Grad:  0.073, New P: 0.418
-Original Grad: -0.085, -lr * Pred Grad:  -0.224, New P: 1.685
iter 23 loss: 0.048
Actual params: [0.4184, 1.6847]
-Original Grad: 0.102, -lr * Pred Grad:  0.021, New P: 0.439
-Original Grad: 0.030, -lr * Pred Grad:  0.042, New P: 1.727
iter 24 loss: 0.046
Actual params: [0.439 , 1.7265]
-Original Grad: 0.136, -lr * Pred Grad:  0.029, New P: 0.468
-Original Grad: 0.030, -lr * Pred Grad:  0.035, New P: 1.761
iter 25 loss: 0.043
Actual params: [0.4685, 1.7611]
-Original Grad: 0.179, -lr * Pred Grad:  0.040, New P: 0.509
-Original Grad: -0.024, -lr * Pred Grad:  -0.061, New P: 1.700
iter 26 loss: 0.036
Actual params: [0.5088, 1.6998]
-Original Grad: 0.175, -lr * Pred Grad:  0.028, New P: 0.537
-Original Grad: 0.031, -lr * Pred Grad:  0.043, New P: 1.743
iter 27 loss: 0.034
Actual params: [0.5367, 1.7426]
-Original Grad: 0.081, -lr * Pred Grad:  0.015, New P: 0.552
-Original Grad: -0.029, -lr * Pred Grad:  -0.039, New P: 1.704
iter 28 loss: 0.032
Actual params: [0.5518, 1.7039]
-Original Grad: 0.115, -lr * Pred Grad:  0.021, New P: 0.572
-Original Grad: 0.038, -lr * Pred Grad:  0.050, New P: 1.754
iter 29 loss: 0.031
Actual params: [0.5723, 1.7536]
-Original Grad: 0.163, -lr * Pred Grad:  0.026, New P: 0.598
-Original Grad: -0.028, -lr * Pred Grad:  -0.028, New P: 1.725
iter 30 loss: 0.028
Actual params: [0.5982, 1.7251]
-Original Grad: 0.137, -lr * Pred Grad:  0.020, New P: 0.618
-Original Grad: 0.007, -lr * Pred Grad:  0.014, New P: 1.739
Target params: [1.3344, 1.5708]
iter 0 loss: 0.049
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.024, -lr * Pred Grad:  -0.241, New P: -0.714
-Original Grad: 0.013, -lr * Pred Grad:  0.135, New P: 0.138
iter 1 loss: 0.047
Actual params: [-0.7138,  0.1383]
-Original Grad: -0.002, -lr * Pred Grad:  -0.026, New P: -0.740
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.154
iter 2 loss: 0.047
Actual params: [-0.7398,  0.1545]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.755
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.164
iter 3 loss: 0.046
Actual params: [-0.7554,  0.1645]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.770
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.174
iter 4 loss: 0.046
Actual params: [-0.7699,  0.1737]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.788
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.185
iter 5 loss: 0.046
Actual params: [-0.7882,  0.1848]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.799
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.192
iter 6 loss: 0.046
Actual params: [-0.7991,  0.192 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.816
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.202
iter 7 loss: 0.046
Actual params: [-0.8156,  0.2024]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.832
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.213
iter 8 loss: 0.046
Actual params: [-0.8316,  0.2128]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.848
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.223
iter 9 loss: 0.046
Actual params: [-0.8475,  0.2229]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.866
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.234
iter 10 loss: 0.046
Actual params: [-0.8661,  0.2345]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.879
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.243
iter 11 loss: 0.046
Actual params: [-0.879 ,  0.2426]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.894
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.252
iter 12 loss: 0.046
Actual params: [-0.8936,  0.2518]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.906
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.260
iter 13 loss: 0.046
Actual params: [-0.9062,  0.2599]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.917
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.267
iter 14 loss: 0.046
Actual params: [-0.9166,  0.267 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.924
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.272
iter 15 loss: 0.046
Actual params: [-0.924 ,  0.2721]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.932
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.278
iter 16 loss: 0.046
Actual params: [-0.9318,  0.2776]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.941
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.284
iter 17 loss: 0.046
Actual params: [-0.9407,  0.2837]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.953
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.292
iter 18 loss: 0.046
Actual params: [-0.9534,  0.2921]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.970
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.302
iter 19 loss: 0.046
Actual params: [-0.9703,  0.3024]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.978
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.308
iter 20 loss: 0.046
Actual params: [-0.9784,  0.3083]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.990
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.316
iter 21 loss: 0.046
Actual params: [-0.99  ,  0.3156]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.998
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.321
iter 22 loss: 0.046
Actual params: [-0.9978,  0.3211]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.005
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.326
iter 23 loss: 0.046
Actual params: [-1.0053,  0.3264]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.016
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.334
iter 24 loss: 0.046
Actual params: [-1.0162,  0.3339]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.026
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.340
iter 25 loss: 0.046
Actual params: [-1.0259,  0.3405]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.035
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.347
iter 26 loss: 0.046
Actual params: [-1.0353,  0.347 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.044
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.353
iter 27 loss: 0.046
Actual params: [-1.0443,  0.3528]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.052
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.358
iter 28 loss: 0.046
Actual params: [-1.0518,  0.3584]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.061
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.365
iter 29 loss: 0.046
Actual params: [-1.0608,  0.365 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.070
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.372
iter 30 loss: 0.046
Actual params: [-1.0697,  0.3716]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.081
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.380
Target params: [1.3344, 1.5708]
iter 0 loss: 0.431
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.004, -lr * Pred Grad:  0.039, New P: -0.433
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 0.000
iter 1 loss: 0.431
Actual params: [-4.3341e-01,  3.6162e-04]
-Original Grad: 0.008, -lr * Pred Grad:  0.095, New P: -0.339
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.005
iter 2 loss: 0.430
Actual params: [-0.3386, -0.0051]
-Original Grad: 0.013, -lr * Pred Grad:  0.169, New P: -0.170
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -0.013
iter 3 loss: 0.427
Actual params: [-0.1698, -0.0132]
-Original Grad: 0.046, -lr * Pred Grad:  0.550, New P: 0.380
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.031
iter 4 loss: 0.315
Actual params: [ 0.3802, -0.0312]
-Original Grad: 0.535, -lr * Pred Grad:  0.290, New P: 0.670
-Original Grad: 0.128, -lr * Pred Grad:  0.298, New P: 0.267
iter 5 loss: 0.122
Actual params: [0.6704, 0.2671]
-Original Grad: 0.467, -lr * Pred Grad:  0.007, New P: 0.677
-Original Grad: 0.204, -lr * Pred Grad:  0.337, New P: 0.605
iter 6 loss: 0.057
Actual params: [0.6771, 0.6046]
-Original Grad: 0.266, -lr * Pred Grad:  -0.025, New P: 0.652
-Original Grad: 0.167, -lr * Pred Grad:  0.212, New P: 0.816
iter 7 loss: 0.036
Actual params: [0.6518, 0.8164]
-Original Grad: 0.269, -lr * Pred Grad:  0.014, New P: 0.666
-Original Grad: 0.129, -lr * Pred Grad:  0.071, New P: 0.887
iter 8 loss: 0.024
Actual params: [0.6656, 0.8872]
-Original Grad: 0.197, -lr * Pred Grad:  -0.019, New P: 0.647
-Original Grad: 0.163, -lr * Pred Grad:  0.099, New P: 0.986
iter 9 loss: 0.014
Actual params: [0.6467, 0.9857]
-Original Grad: 0.203, -lr * Pred Grad:  -0.007, New P: 0.640
-Original Grad: 0.156, -lr * Pred Grad:  0.050, New P: 1.036
iter 10 loss: 0.010
Actual params: [0.6398, 1.0358]
-Original Grad: 0.176, -lr * Pred Grad:  0.006, New P: 0.646
-Original Grad: 0.107, -lr * Pred Grad:  0.016, New P: 1.052
iter 11 loss: 0.009
Actual params: [0.646 , 1.0522]
-Original Grad: 0.079, -lr * Pred Grad:  -0.009, New P: 0.637
-Original Grad: 0.093, -lr * Pred Grad:  0.031, New P: 1.083
iter 12 loss: 0.009
Actual params: [0.6375, 1.0832]
-Original Grad: 0.139, -lr * Pred Grad:  0.010, New P: 0.648
-Original Grad: 0.063, -lr * Pred Grad:  0.000, New P: 1.084
iter 13 loss: 0.008
Actual params: [0.6476, 1.0836]
-Original Grad: 0.005, -lr * Pred Grad:  0.006, New P: 0.654
-Original Grad: -0.030, -lr * Pred Grad:  -0.013, New P: 1.071
iter 14 loss: 0.008
Actual params: [0.6536, 1.0708]
-Original Grad: -0.007, -lr * Pred Grad:  0.003, New P: 0.656
-Original Grad: -0.020, -lr * Pred Grad:  -0.006, New P: 1.064
iter 15 loss: 0.008
Actual params: [0.6562, 1.0643]
-Original Grad: -0.138, -lr * Pred Grad:  -0.004, New P: 0.652
-Original Grad: -0.082, -lr * Pred Grad:  -0.006, New P: 1.059
iter 16 loss: 0.009
Actual params: [0.6522, 1.0586]
-Original Grad: 0.026, -lr * Pred Grad:  0.008, New P: 0.660
-Original Grad: -0.039, -lr * Pred Grad:  -0.014, New P: 1.044
iter 17 loss: 0.009
Actual params: [0.6599, 1.0442]
-Original Grad: 0.065, -lr * Pred Grad:  0.004, New P: 0.664
-Original Grad: 0.023, -lr * Pred Grad:  -0.002, New P: 1.042
iter 18 loss: 0.008
Actual params: [0.6637, 1.0421]
-Original Grad: 0.063, -lr * Pred Grad:  -0.002, New P: 0.662
-Original Grad: 0.069, -lr * Pred Grad:  0.010, New P: 1.053
iter 19 loss: 0.008
Actual params: [0.6618, 1.0526]
-Original Grad: 0.016, -lr * Pred Grad:  0.003, New P: 0.665
-Original Grad: -0.008, -lr * Pred Grad:  -0.004, New P: 1.048
iter 20 loss: 0.008
Actual params: [0.6645, 1.0484]
-Original Grad: -0.072, -lr * Pred Grad:  -0.003, New P: 0.662
-Original Grad: -0.037, -lr * Pred Grad:  -0.000, New P: 1.048
iter 21 loss: 0.008
Actual params: [0.6616, 1.048 ]
-Original Grad: 0.013, -lr * Pred Grad:  -0.000, New P: 0.661
-Original Grad: 0.013, -lr * Pred Grad:  0.002, New P: 1.050
iter 22 loss: 0.008
Actual params: [0.6612, 1.0499]
-Original Grad: -0.003, -lr * Pred Grad:  0.001, New P: 0.662
-Original Grad: -0.010, -lr * Pred Grad:  -0.002, New P: 1.048
iter 23 loss: 0.008
Actual params: [0.6622, 1.0476]
-Original Grad: 0.026, -lr * Pred Grad:  0.003, New P: 0.665
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: 1.044
iter 24 loss: 0.008
Actual params: [0.665 , 1.0436]
-Original Grad: 0.060, -lr * Pred Grad:  0.006, New P: 0.671
-Original Grad: 0.003, -lr * Pred Grad:  -0.008, New P: 1.036
iter 25 loss: 0.008
Actual params: [0.6707, 1.036 ]
-Original Grad: -0.057, -lr * Pred Grad:  0.002, New P: 0.673
-Original Grad: -0.059, -lr * Pred Grad:  -0.008, New P: 1.028
iter 26 loss: 0.008
Actual params: [0.6731, 1.0275]
-Original Grad: 0.047, -lr * Pred Grad:  0.002, New P: 0.675
-Original Grad: 0.025, -lr * Pred Grad:  0.000, New P: 1.028
iter 27 loss: 0.008
Actual params: [0.6746, 1.0277]
-Original Grad: -0.011, -lr * Pred Grad:  0.001, New P: 0.675
-Original Grad: -0.012, -lr * Pred Grad:  -0.002, New P: 1.026
iter 28 loss: 0.008
Actual params: [0.6753, 1.0258]
-Original Grad: 0.011, -lr * Pred Grad:  0.004, New P: 0.679
-Original Grad: -0.015, -lr * Pred Grad:  -0.006, New P: 1.020
iter 29 loss: 0.008
Actual params: [0.679 , 1.0198]
-Original Grad: 0.090, -lr * Pred Grad:  0.003, New P: 0.682
-Original Grad: 0.055, -lr * Pred Grad:  0.001, New P: 1.021
iter 30 loss: 0.008
Actual params: [0.6816, 1.0209]
-Original Grad: -0.027, -lr * Pred Grad:  0.003, New P: 0.685
-Original Grad: -0.042, -lr * Pred Grad:  -0.007, New P: 1.014
Target params: [1.3344, 1.5708]
iter 0 loss: 0.731
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.009, -lr * Pred Grad:  0.102, New P: -0.370
-Original Grad: -0.003, -lr * Pred Grad:  -0.035, New P: -0.032
iter 1 loss: 0.729
Actual params: [-0.3703, -0.0315]
-Original Grad: 0.030, -lr * Pred Grad:  0.317, New P: -0.053
-Original Grad: -0.010, -lr * Pred Grad:  -0.101, New P: -0.133
iter 2 loss: 0.698
Actual params: [-0.0535, -0.1327]
-Original Grad: 0.197, -lr * Pred Grad:  0.406, New P: 0.353
-Original Grad: -0.038, -lr * Pred Grad:  -0.071, New P: -0.204
iter 3 loss: 0.605
Actual params: [ 0.3526, -0.2041]
-Original Grad: 0.219, -lr * Pred Grad:  0.222, New P: 0.575
-Original Grad: 0.111, -lr * Pred Grad:  0.535, New P: 0.331
iter 4 loss: 0.416
Actual params: [0.5745, 0.3307]
-Original Grad: 0.528, -lr * Pred Grad:  0.240, New P: 0.815
-Original Grad: 0.094, -lr * Pred Grad:  0.110, New P: 0.440
iter 5 loss: 0.286
Actual params: [0.8148, 0.4403]
-Original Grad: 0.498, -lr * Pred Grad:  0.086, New P: 0.901
-Original Grad: 0.195, -lr * Pred Grad:  0.345, New P: 0.785
iter 6 loss: 0.187
Actual params: [0.9007, 0.785 ]
-Original Grad: 1.417, -lr * Pred Grad:  0.121, New P: 1.022
-Original Grad: 0.185, -lr * Pred Grad:  -0.110, New P: 0.675
iter 7 loss: 0.189
Actual params: [1.0216, 0.6752]
-Original Grad: 0.230, -lr * Pred Grad:  -0.043, New P: 0.979
-Original Grad: 0.351, -lr * Pred Grad:  0.370, New P: 1.045
iter 8 loss: 0.129
Actual params: [0.9789, 1.0455]
-Original Grad: 0.933, -lr * Pred Grad:  0.032, New P: 1.011
-Original Grad: 0.223, -lr * Pred Grad:  0.083, New P: 1.128
iter 9 loss: 0.114
Actual params: [1.0111, 1.1281]
-Original Grad: 0.296, -lr * Pred Grad:  0.005, New P: 1.016
-Original Grad: 0.121, -lr * Pred Grad:  0.062, New P: 1.190
iter 10 loss: 0.108
Actual params: [1.016 , 1.1902]
-Original Grad: 0.834, -lr * Pred Grad:  0.028, New P: 1.044
-Original Grad: 0.212, -lr * Pred Grad:  0.041, New P: 1.231
iter 11 loss: 0.100
Actual params: [1.0436, 1.2312]
-Original Grad: 0.477, -lr * Pred Grad:  0.033, New P: 1.077
-Original Grad: -0.048, -lr * Pred Grad:  -0.083, New P: 1.148
iter 12 loss: 0.103
Actual params: [1.077 , 1.1482]
-Original Grad: 0.632, -lr * Pred Grad:  0.003, New P: 1.080
-Original Grad: 0.303, -lr * Pred Grad:  0.094, New P: 1.242
iter 13 loss: 0.094
Actual params: [1.0799, 1.2425]
-Original Grad: 0.923, -lr * Pred Grad:  0.015, New P: 1.095
-Original Grad: 0.273, -lr * Pred Grad:  0.045, New P: 1.288
iter 14 loss: 0.089
Actual params: [1.0947, 1.2878]
-Original Grad: 0.716, -lr * Pred Grad:  0.020, New P: 1.115
-Original Grad: 0.100, -lr * Pred Grad:  -0.020, New P: 1.268
iter 15 loss: 0.088
Actual params: [1.115 , 1.2681]
-Original Grad: 0.270, -lr * Pred Grad:  0.006, New P: 1.121
-Original Grad: 0.068, -lr * Pred Grad:  0.005, New P: 1.273
iter 16 loss: 0.087
Actual params: [1.1207, 1.2732]
-Original Grad: 0.337, -lr * Pred Grad:  0.016, New P: 1.137
-Original Grad: -0.031, -lr * Pred Grad:  -0.037, New P: 1.237
iter 17 loss: 0.088
Actual params: [1.137 , 1.2366]
-Original Grad: 0.504, -lr * Pred Grad:  0.002, New P: 1.139
-Original Grad: 0.251, -lr * Pred Grad:  0.050, New P: 1.287
iter 18 loss: 0.084
Actual params: [1.1394, 1.2868]
-Original Grad: 0.269, -lr * Pred Grad:  0.009, New P: 1.148
-Original Grad: 0.030, -lr * Pred Grad:  -0.008, New P: 1.279
iter 19 loss: 0.084
Actual params: [1.1484, 1.2788]
-Original Grad: 0.424, -lr * Pred Grad:  0.009, New P: 1.157
-Original Grad: 0.148, -lr * Pred Grad:  0.017, New P: 1.296
iter 20 loss: 0.081
Actual params: [1.157 , 1.2961]
-Original Grad: 0.360, -lr * Pred Grad:  0.013, New P: 1.170
-Original Grad: 0.053, -lr * Pred Grad:  -0.007, New P: 1.289
iter 21 loss: 0.081
Actual params: [1.1697, 1.2889]
-Original Grad: 0.345, -lr * Pred Grad:  0.013, New P: 1.183
-Original Grad: 0.043, -lr * Pred Grad:  -0.009, New P: 1.280
iter 22 loss: 0.081
Actual params: [1.1828, 1.2801]
-Original Grad: 0.257, -lr * Pred Grad:  0.009, New P: 1.192
-Original Grad: 0.053, -lr * Pred Grad:  -0.000, New P: 1.280
iter 23 loss: 0.080
Actual params: [1.1919, 1.2797]
-Original Grad: 0.270, -lr * Pred Grad:  0.011, New P: 1.203
-Original Grad: 0.037, -lr * Pred Grad:  -0.005, New P: 1.275
iter 24 loss: 0.080
Actual params: [1.2029, 1.2751]
-Original Grad: 0.217, -lr * Pred Grad:  0.010, New P: 1.213
-Original Grad: 0.020, -lr * Pred Grad:  -0.005, New P: 1.270
iter 25 loss: 0.079
Actual params: [1.2125, 1.2704]
-Original Grad: 0.140, -lr * Pred Grad:  0.000, New P: 1.213
-Original Grad: 0.174, -lr * Pred Grad:  0.031, New P: 1.301
iter 26 loss: 0.077
Actual params: [1.2128, 1.301 ]
-Original Grad: 0.195, -lr * Pred Grad:  0.006, New P: 1.218
-Original Grad: 0.122, -lr * Pred Grad:  0.018, New P: 1.319
iter 27 loss: 0.075
Actual params: [1.2184, 1.3185]
-Original Grad: 0.315, -lr * Pred Grad:  0.018, New P: 1.236
-Original Grad: -0.036, -lr * Pred Grad:  -0.017, New P: 1.302
iter 28 loss: 0.075
Actual params: [1.2362, 1.3015]
-Original Grad: 0.145, -lr * Pred Grad:  0.000, New P: 1.236
-Original Grad: 0.242, -lr * Pred Grad:  0.042, New P: 1.343
iter 29 loss: 0.072
Actual params: [1.2362, 1.3432]
-Original Grad: 0.328, -lr * Pred Grad:  0.016, New P: 1.252
-Original Grad: 0.089, -lr * Pred Grad:  0.006, New P: 1.349
iter 30 loss: 0.070
Actual params: [1.2523, 1.349 ]
-Original Grad: 0.152, -lr * Pred Grad:  0.008, New P: 1.260
-Original Grad: 0.043, -lr * Pred Grad:  0.003, New P: 1.352
Target params: [1.3344, 1.5708]
iter 0 loss: 0.563
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.034, New P: -0.506
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.014
iter 1 loss: 0.563
Actual params: [-0.5065,  0.0144]
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: -0.528
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.022
iter 2 loss: 0.563
Actual params: [-0.5281,  0.0216]
-Original Grad: -0.002, -lr * Pred Grad:  -0.032, New P: -0.560
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.032
iter 3 loss: 0.563
Actual params: [-0.56  ,  0.0317]
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -0.582
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.039
iter 4 loss: 0.563
Actual params: [-0.5819,  0.0387]
-Original Grad: -0.002, -lr * Pred Grad:  -0.028, New P: -0.610
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.047
iter 5 loss: 0.563
Actual params: [-0.6102,  0.0474]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.627
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.053
iter 6 loss: 0.563
Actual params: [-0.6269,  0.0525]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.642
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.057
iter 7 loss: 0.563
Actual params: [-0.6417,  0.0571]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.657
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.062
iter 8 loss: 0.563
Actual params: [-0.6574,  0.0618]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.672
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.066
iter 9 loss: 0.563
Actual params: [-0.6724,  0.0662]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.685
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.070
iter 10 loss: 0.563
Actual params: [-0.6855,  0.0702]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.698
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.074
iter 11 loss: 0.563
Actual params: [-0.6976,  0.0738]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.713
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.078
iter 12 loss: 0.563
Actual params: [-0.7132,  0.0784]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.728
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.083
iter 13 loss: 0.563
Actual params: [-0.7276,  0.0827]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.741
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.087
iter 14 loss: 0.563
Actual params: [-0.7411,  0.0866]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.756
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.091
iter 15 loss: 0.563
Actual params: [-0.7555,  0.0908]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.770
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.095
iter 16 loss: 0.563
Actual params: [-0.7701,  0.0951]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.780
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.098
iter 17 loss: 0.563
Actual params: [-0.7801,  0.0981]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.792
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.102
iter 18 loss: 0.563
Actual params: [-0.7916,  0.1016]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.802
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.105
iter 19 loss: 0.563
Actual params: [-0.8015,  0.1046]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.815
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.109
iter 20 loss: 0.563
Actual params: [-0.8151,  0.1085]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.827
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.112
iter 21 loss: 0.563
Actual params: [-0.827 ,  0.1121]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.837
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.115
iter 22 loss: 0.563
Actual params: [-0.8374,  0.1152]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.851
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.119
iter 23 loss: 0.563
Actual params: [-0.851 ,  0.1191]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.864
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.123
iter 24 loss: 0.563
Actual params: [-0.8638,  0.1229]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.875
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.126
iter 25 loss: 0.563
Actual params: [-0.875 ,  0.1263]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.884
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.129
iter 26 loss: 0.563
Actual params: [-0.8842,  0.1292]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.897
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.133
iter 27 loss: 0.563
Actual params: [-0.8971,  0.133 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.909
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.136
iter 28 loss: 0.563
Actual params: [-0.9088,  0.1364]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.920
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.140
iter 29 loss: 0.563
Actual params: [-0.9199,  0.1397]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.931
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.143
iter 30 loss: 0.563
Actual params: [-0.9305,  0.1429]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.943
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.146
Target params: [1.3344, 1.5708]
iter 0 loss: 0.495
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: -0.465
-Original Grad: 0.001, -lr * Pred Grad:  0.006, New P: 0.009
iter 1 loss: 0.495
Actual params: [-0.4653,  0.0092]
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: -0.458
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.016
iter 2 loss: 0.495
Actual params: [-0.4576,  0.0157]
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: -0.449
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.023
iter 3 loss: 0.495
Actual params: [-0.4488,  0.0226]
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: -0.438
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: 0.031
iter 4 loss: 0.495
Actual params: [-0.4383,  0.0308]
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: -0.420
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.043
iter 5 loss: 0.495
Actual params: [-0.4201,  0.0434]
-Original Grad: 0.001, -lr * Pred Grad:  0.024, New P: -0.396
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.060
iter 6 loss: 0.495
Actual params: [-0.3956,  0.0595]
-Original Grad: 0.002, -lr * Pred Grad:  0.038, New P: -0.358
-Original Grad: 0.001, -lr * Pred Grad:  0.023, New P: 0.082
iter 7 loss: 0.495
Actual params: [-0.3578,  0.0822]
-Original Grad: 0.003, -lr * Pred Grad:  0.062, New P: -0.296
-Original Grad: 0.001, -lr * Pred Grad:  0.035, New P: 0.117
iter 8 loss: 0.495
Actual params: [-0.2959,  0.1167]
-Original Grad: 0.005, -lr * Pred Grad:  0.123, New P: -0.173
-Original Grad: 0.002, -lr * Pred Grad:  0.061, New P: 0.178
iter 9 loss: 0.494
Actual params: [-0.173,  0.178]
-Original Grad: 0.017, -lr * Pred Grad:  0.461, New P: 0.288
-Original Grad: 0.007, -lr * Pred Grad:  0.200, New P: 0.378
iter 10 loss: 0.446
Actual params: [0.2881, 0.3778]
-Original Grad: 0.218, -lr * Pred Grad:  0.551, New P: 0.839
-Original Grad: 0.193, -lr * Pred Grad:  0.533, New P: 0.911
iter 11 loss: 0.130
Actual params: [0.8389, 0.9107]
-Original Grad: 0.001, -lr * Pred Grad:  -0.172, New P: 0.667
-Original Grad: 0.327, -lr * Pred Grad:  0.291, New P: 1.201
iter 12 loss: 0.120
Actual params: [0.6673, 1.2014]
-Original Grad: 0.525, -lr * Pred Grad:  0.159, New P: 0.826
-Original Grad: 0.073, -lr * Pred Grad:  0.000, New P: 1.202
iter 13 loss: 0.075
Actual params: [0.8261, 1.2018]
-Original Grad: 0.136, -lr * Pred Grad:  0.036, New P: 0.862
-Original Grad: 0.065, -lr * Pred Grad:  0.043, New P: 1.245
iter 14 loss: 0.064
Actual params: [0.862 , 1.2449]
-Original Grad: 0.025, -lr * Pred Grad:  -0.004, New P: 0.858
-Original Grad: 0.142, -lr * Pred Grad:  0.092, New P: 1.337
iter 15 loss: 0.053
Actual params: [0.8584, 1.3372]
-Original Grad: 0.075, -lr * Pred Grad:  0.017, New P: 0.875
-Original Grad: 0.105, -lr * Pred Grad:  0.063, New P: 1.400
iter 16 loss: 0.043
Actual params: [0.8751, 1.4002]
-Original Grad: 0.125, -lr * Pred Grad:  0.036, New P: 0.911
-Original Grad: 0.086, -lr * Pred Grad:  0.042, New P: 1.442
iter 17 loss: 0.034
Actual params: [0.9113, 1.4419]
-Original Grad: 0.130, -lr * Pred Grad:  0.036, New P: 0.947
-Original Grad: 0.082, -lr * Pred Grad:  0.037, New P: 1.479
iter 18 loss: 0.028
Actual params: [0.9468, 1.4785]
-Original Grad: 0.146, -lr * Pred Grad:  0.029, New P: 0.976
-Original Grad: 0.088, -lr * Pred Grad:  0.035, New P: 1.513
iter 19 loss: 0.023
Actual params: [0.9763, 1.5133]
-Original Grad: 0.083, -lr * Pred Grad:  0.011, New P: 0.987
-Original Grad: 0.067, -lr * Pred Grad:  0.031, New P: 1.545
iter 20 loss: 0.020
Actual params: [0.9871, 1.5446]
-Original Grad: 0.137, -lr * Pred Grad:  0.018, New P: 1.005
-Original Grad: 0.083, -lr * Pred Grad:  0.024, New P: 1.569
iter 21 loss: 0.017
Actual params: [1.0051, 1.5687]
-Original Grad: 0.105, -lr * Pred Grad:  0.010, New P: 1.015
-Original Grad: 0.080, -lr * Pred Grad:  0.027, New P: 1.596
iter 22 loss: 0.015
Actual params: [1.0153, 1.5959]
-Original Grad: 0.064, -lr * Pred Grad:  0.003, New P: 1.018
-Original Grad: 0.073, -lr * Pred Grad:  0.028, New P: 1.624
iter 23 loss: 0.013
Actual params: [1.018 , 1.6243]
-Original Grad: 0.077, -lr * Pred Grad:  0.008, New P: 1.026
-Original Grad: 0.059, -lr * Pred Grad:  0.019, New P: 1.643
iter 24 loss: 0.012
Actual params: [1.0263, 1.6434]
-Original Grad: 0.111, -lr * Pred Grad:  0.015, New P: 1.041
-Original Grad: 0.058, -lr * Pred Grad:  0.013, New P: 1.656
iter 25 loss: 0.011
Actual params: [1.0414, 1.6564]
-Original Grad: 0.051, -lr * Pred Grad:  0.002, New P: 1.043
-Original Grad: 0.063, -lr * Pred Grad:  0.024, New P: 1.681
iter 26 loss: 0.009
Actual params: [1.0432, 1.6808]
-Original Grad: 0.042, -lr * Pred Grad:  0.003, New P: 1.046
-Original Grad: 0.043, -lr * Pred Grad:  0.016, New P: 1.697
iter 27 loss: 0.008
Actual params: [1.0461, 1.697 ]
-Original Grad: 0.058, -lr * Pred Grad:  0.005, New P: 1.051
-Original Grad: 0.055, -lr * Pred Grad:  0.021, New P: 1.718
iter 28 loss: 0.008
Actual params: [1.051, 1.718]
-Original Grad: 0.017, -lr * Pred Grad:  -0.000, New P: 1.051
-Original Grad: 0.028, -lr * Pred Grad:  0.012, New P: 1.730
iter 29 loss: 0.007
Actual params: [1.0506, 1.7303]
-Original Grad: 0.048, -lr * Pred Grad:  0.006, New P: 1.057
-Original Grad: 0.036, -lr * Pred Grad:  0.012, New P: 1.742
iter 30 loss: 0.007
Actual params: [1.0569, 1.7425]
-Original Grad: 0.051, -lr * Pred Grad:  0.006, New P: 1.063
-Original Grad: 0.039, -lr * Pred Grad:  0.014, New P: 1.757
Target params: [1.3344, 1.5708]
iter 0 loss: 0.118
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.111, -lr * Pred Grad:  -0.539, New P: -1.011
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.011
iter 1 loss: 0.106
Actual params: [-1.0112, -0.0109]
-Original Grad: -0.002, -lr * Pred Grad:  -0.010, New P: -1.022
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.009
iter 2 loss: 0.106
Actual params: [-1.0216, -0.009 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.010, New P: -1.031
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.006
iter 3 loss: 0.106
Actual params: [-1.0313, -0.0056]
-Original Grad: -0.002, -lr * Pred Grad:  -0.010, New P: -1.042
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.002
iter 4 loss: 0.106
Actual params: [-1.0417, -0.0025]
-Original Grad: -0.002, -lr * Pred Grad:  -0.011, New P: -1.053
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.000
iter 5 loss: 0.106
Actual params: [-1.0529e+00,  2.7832e-04]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -1.064
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.003
iter 6 loss: 0.106
Actual params: [-1.0644,  0.0031]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -1.075
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.007
iter 7 loss: 0.106
Actual params: [-1.0751,  0.0069]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -1.087
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.011
iter 8 loss: 0.106
Actual params: [-1.0871,  0.0106]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -1.100
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.013
iter 9 loss: 0.106
Actual params: [-1.1004,  0.0133]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -1.113
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.017
iter 10 loss: 0.106
Actual params: [-1.1126,  0.0174]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -1.123
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.021
iter 11 loss: 0.106
Actual params: [-1.1229,  0.021 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -1.136
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.024
iter 12 loss: 0.106
Actual params: [-1.1356,  0.0237]
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -1.145
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.028
iter 13 loss: 0.106
Actual params: [-1.1446,  0.0281]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -1.158
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.032
iter 14 loss: 0.106
Actual params: [-1.1581,  0.0317]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -1.170
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.036
iter 15 loss: 0.106
Actual params: [-1.17 ,  0.036]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -1.182
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.042
iter 16 loss: 0.106
Actual params: [-1.1821,  0.0421]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.194
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.048
iter 17 loss: 0.106
Actual params: [-1.1938,  0.048 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.208
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.054
iter 18 loss: 0.106
Actual params: [-1.2079,  0.0542]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.220
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.060
iter 19 loss: 0.106
Actual params: [-1.2204,  0.0604]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.232
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.064
iter 20 loss: 0.106
Actual params: [-1.2323,  0.0641]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.247
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.067
iter 21 loss: 0.106
Actual params: [-1.2471,  0.0674]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.259
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.073
iter 22 loss: 0.106
Actual params: [-1.2589,  0.0728]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.269
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.078
iter 23 loss: 0.106
Actual params: [-1.2687,  0.078 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.281
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.086
iter 24 loss: 0.106
Actual params: [-1.2811,  0.086 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.294
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.090
iter 25 loss: 0.106
Actual params: [-1.2942,  0.0902]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.307
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.096
iter 26 loss: 0.106
Actual params: [-1.3074,  0.0958]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.323
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.101
iter 27 loss: 0.106
Actual params: [-1.3227,  0.1011]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.336
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.109
iter 28 loss: 0.106
Actual params: [-1.3359,  0.1087]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.349
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.115
iter 29 loss: 0.106
Actual params: [-1.3489,  0.1152]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.361
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.120
iter 30 loss: 0.106
Actual params: [-1.3613,  0.1201]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.376
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.130
Target params: [1.3344, 1.5708]
iter 0 loss: 0.583
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.003, -lr * Pred Grad:  0.035, New P: -0.438
-Original Grad: 0.003, -lr * Pred Grad:  0.031, New P: 0.035
iter 1 loss: 0.583
Actual params: [-0.4376,  0.0349]
-Original Grad: 0.006, -lr * Pred Grad:  0.078, New P: -0.359
-Original Grad: 0.005, -lr * Pred Grad:  0.063, New P: 0.098
iter 2 loss: 0.582
Actual params: [-0.3594,  0.0976]
-Original Grad: 0.018, -lr * Pred Grad:  0.238, New P: -0.121
-Original Grad: 0.014, -lr * Pred Grad:  0.190, New P: 0.288
iter 3 loss: 0.562
Actual params: [-0.1212,  0.2879]
-Original Grad: 0.224, -lr * Pred Grad:  0.549, New P: 0.427
-Original Grad: 0.167, -lr * Pred Grad:  0.413, New P: 0.701
iter 4 loss: 0.371
Actual params: [0.4274, 0.7008]
-Original Grad: 0.199, -lr * Pred Grad:  -0.099, New P: 0.328
-Original Grad: 0.274, -lr * Pred Grad:  0.603, New P: 1.304
iter 5 loss: 0.194
Actual params: [0.3284, 1.3043]
-Original Grad: 0.493, -lr * Pred Grad:  0.147, New P: 0.475
-Original Grad: 0.294, -lr * Pred Grad:  0.081, New P: 1.385
iter 6 loss: 0.125
Actual params: [0.475 , 1.3848]
-Original Grad: 0.560, -lr * Pred Grad:  0.103, New P: 0.578
-Original Grad: 0.277, -lr * Pred Grad:  0.018, New P: 1.403
iter 7 loss: 0.095
Actual params: [0.5778, 1.4029]
-Original Grad: 0.245, -lr * Pred Grad:  -0.025, New P: 0.553
-Original Grad: 0.204, -lr * Pred Grad:  0.151, New P: 1.554
iter 8 loss: 0.077
Actual params: [0.5527, 1.5541]
-Original Grad: 0.292, -lr * Pred Grad:  0.062, New P: 0.614
-Original Grad: 0.100, -lr * Pred Grad:  -0.033, New P: 1.521
iter 9 loss: 0.068
Actual params: [0.6143, 1.5211]
-Original Grad: 0.293, -lr * Pred Grad:  0.074, New P: 0.688
-Original Grad: 0.039, -lr * Pred Grad:  -0.085, New P: 1.436
iter 10 loss: 0.064
Actual params: [0.6885, 1.4359]
-Original Grad: 0.099, -lr * Pred Grad:  -0.035, New P: 0.653
-Original Grad: 0.180, -lr * Pred Grad:  0.137, New P: 1.573
iter 11 loss: 0.054
Actual params: [0.6535, 1.5727]
-Original Grad: 0.369, -lr * Pred Grad:  0.055, New P: 0.709
-Original Grad: 0.040, -lr * Pred Grad:  -0.046, New P: 1.527
iter 12 loss: 0.049
Actual params: [0.7087, 1.5272]
-Original Grad: 0.196, -lr * Pred Grad:  0.008, New P: 0.716
-Original Grad: 0.135, -lr * Pred Grad:  0.060, New P: 1.587
iter 13 loss: 0.042
Actual params: [0.7164, 1.5871]
-Original Grad: 0.139, -lr * Pred Grad:  -0.001, New P: 0.715
-Original Grad: 0.151, -lr * Pred Grad:  0.071, New P: 1.658
iter 14 loss: 0.037
Actual params: [0.7149, 1.6576]
-Original Grad: 0.156, -lr * Pred Grad:  0.016, New P: 0.731
-Original Grad: 0.060, -lr * Pred Grad:  0.013, New P: 1.671
iter 15 loss: 0.034
Actual params: [0.7307, 1.6711]
-Original Grad: 0.166, -lr * Pred Grad:  0.013, New P: 0.744
-Original Grad: 0.100, -lr * Pred Grad:  0.033, New P: 1.704
iter 16 loss: 0.031
Actual params: [0.7437, 1.7043]
-Original Grad: 0.152, -lr * Pred Grad:  0.019, New P: 0.763
-Original Grad: 0.033, -lr * Pred Grad:  -0.000, New P: 1.704
iter 17 loss: 0.029
Actual params: [0.763 , 1.7043]
-Original Grad: 0.200, -lr * Pred Grad:  0.023, New P: 0.786
-Original Grad: 0.038, -lr * Pred Grad:  0.001, New P: 1.705
iter 18 loss: 0.026
Actual params: [0.7864, 1.7054]
-Original Grad: 0.088, -lr * Pred Grad:  -0.000, New P: 0.786
-Original Grad: 0.137, -lr * Pred Grad:  0.056, New P: 1.761
iter 19 loss: 0.025
Actual params: [0.7861, 1.7613]
-Original Grad: 0.213, -lr * Pred Grad:  0.024, New P: 0.810
-Original Grad: 0.022, -lr * Pred Grad:  -0.005, New P: 1.756
iter 20 loss: 0.023
Actual params: [0.8097, 1.7565]
-Original Grad: 0.176, -lr * Pred Grad:  0.011, New P: 0.821
-Original Grad: 0.116, -lr * Pred Grad:  0.040, New P: 1.796
iter 21 loss: 0.022
Actual params: [0.8209, 1.7961]
-Original Grad: 0.075, -lr * Pred Grad:  0.004, New P: 0.825
-Original Grad: 0.070, -lr * Pred Grad:  0.025, New P: 1.821
iter 22 loss: 0.022
Actual params: [0.8249, 1.8212]
-Original Grad: 0.078, -lr * Pred Grad:  0.004, New P: 0.829
-Original Grad: 0.082, -lr * Pred Grad:  0.029, New P: 1.850
iter 23 loss: 0.022
Actual params: [0.829 , 1.8499]
-Original Grad: 0.071, -lr * Pred Grad:  0.006, New P: 0.835
-Original Grad: 0.050, -lr * Pred Grad:  0.016, New P: 1.865
iter 24 loss: 0.022
Actual params: [0.835 , 1.8655]
-Original Grad: 0.095, -lr * Pred Grad:  0.009, New P: 0.844
-Original Grad: 0.066, -lr * Pred Grad:  0.020, New P: 1.885
iter 25 loss: 0.023
Actual params: [0.8438, 1.8854]
-Original Grad: 0.120, -lr * Pred Grad:  0.013, New P: 0.857
-Original Grad: 0.052, -lr * Pred Grad:  0.013, New P: 1.898
iter 26 loss: 0.023
Actual params: [0.857 , 1.8979]
-Original Grad: 0.031, -lr * Pred Grad:  0.002, New P: 0.859
-Original Grad: 0.047, -lr * Pred Grad:  0.015, New P: 1.912
iter 27 loss: 0.024
Actual params: [0.8587, 1.9125]
-Original Grad: 0.236, -lr * Pred Grad:  0.023, New P: 0.881
-Original Grad: -0.044, -lr * Pred Grad:  -0.022, New P: 1.891
iter 28 loss: 0.021
Actual params: [0.8815, 1.8909]
-Original Grad: 0.044, -lr * Pred Grad:  0.005, New P: 0.886
-Original Grad: -0.016, -lr * Pred Grad:  -0.006, New P: 1.885
iter 29 loss: 0.021
Actual params: [0.8863, 1.8846]
-Original Grad: 0.028, -lr * Pred Grad:  0.003, New P: 0.890
-Original Grad: -0.015, -lr * Pred Grad:  -0.005, New P: 1.880
iter 30 loss: 0.020
Actual params: [0.8897, 1.8796]
-Original Grad: 0.070, -lr * Pred Grad:  0.010, New P: 0.899
-Original Grad: -0.077, -lr * Pred Grad:  -0.022, New P: 1.858
Target params: [1.3344, 1.5708]
iter 0 loss: 0.267
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.014, -lr * Pred Grad:  -0.148, New P: -0.621
-Original Grad: -0.008, -lr * Pred Grad:  -0.083, New P: -0.080
iter 1 loss: 0.266
Actual params: [-0.6207, -0.0798]
-Original Grad: -0.003, -lr * Pred Grad:  -0.038, New P: -0.658
-Original Grad: -0.002, -lr * Pred Grad:  -0.030, New P: -0.110
iter 2 loss: 0.266
Actual params: [-0.6584, -0.1098]
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -0.683
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.122
iter 3 loss: 0.266
Actual params: [-0.6828, -0.1217]
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -0.707
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.136
iter 4 loss: 0.266
Actual params: [-0.7069, -0.1359]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.721
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -0.144
iter 5 loss: 0.266
Actual params: [-0.7206, -0.1443]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.736
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.155
iter 6 loss: 0.266
Actual params: [-0.7356, -0.1548]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.753
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.166
iter 7 loss: 0.266
Actual params: [-0.7528, -0.1663]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.769
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.177
iter 8 loss: 0.266
Actual params: [-0.7688, -0.1768]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.780
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.185
iter 9 loss: 0.266
Actual params: [-0.7801, -0.1846]
-Original Grad: -0.001, -lr * Pred Grad:  -0.025, New P: -0.805
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.204
iter 10 loss: 0.266
Actual params: [-0.8055, -0.2039]
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -0.828
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.218
iter 11 loss: 0.266
Actual params: [-0.8276, -0.2177]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.841
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.227
iter 12 loss: 0.266
Actual params: [-0.841 , -0.2271]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.857
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.239
iter 13 loss: 0.266
Actual params: [-0.8569, -0.2386]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.869
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.246
iter 14 loss: 0.266
Actual params: [-0.8687, -0.246 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.882
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.256
iter 15 loss: 0.266
Actual params: [-0.8824, -0.2556]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.889
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.260
iter 16 loss: 0.266
Actual params: [-0.8891, -0.2599]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.899
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.266
iter 17 loss: 0.266
Actual params: [-0.899 , -0.2662]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.915
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.278
iter 18 loss: 0.266
Actual params: [-0.9152, -0.2778]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.930
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.288
iter 19 loss: 0.266
Actual params: [-0.9301, -0.2884]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.943
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.297
iter 20 loss: 0.266
Actual params: [-0.9432, -0.2974]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.955
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.306
iter 21 loss: 0.266
Actual params: [-0.9553, -0.3058]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.962
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.310
iter 22 loss: 0.266
Actual params: [-0.9621, -0.3101]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.972
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.316
iter 23 loss: 0.266
Actual params: [-0.9719, -0.3156]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.985
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.324
iter 24 loss: 0.266
Actual params: [-0.9846, -0.3237]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.002
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.335
iter 25 loss: 0.266
Actual params: [-1.0016, -0.3354]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.016
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.345
iter 26 loss: 0.266
Actual params: [-1.0156, -0.3449]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.026
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.351
iter 27 loss: 0.266
Actual params: [-1.0257, -0.3509]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.038
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.359
iter 28 loss: 0.266
Actual params: [-1.0382, -0.359 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.049
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.365
iter 29 loss: 0.266
Actual params: [-1.049 , -0.3652]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.063
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.373
iter 30 loss: 0.266
Actual params: [-1.0626, -0.3731]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.076
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.382
Target params: [1.3344, 1.5708]
iter 0 loss: 0.048
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.090, -lr * Pred Grad:  -0.390, New P: -0.862
-Original Grad: 0.047, -lr * Pred Grad:  0.227, New P: 0.230
iter 1 loss: 0.043
Actual params: [-0.8623,  0.2302]
-Original Grad: -0.004, -lr * Pred Grad:  -0.021, New P: -0.883
-Original Grad: 0.003, -lr * Pred Grad:  0.017, New P: 0.247
iter 2 loss: 0.043
Actual params: [-0.8828,  0.2467]
-Original Grad: -0.004, -lr * Pred Grad:  -0.023, New P: -0.905
-Original Grad: 0.002, -lr * Pred Grad:  0.016, New P: 0.262
iter 3 loss: 0.043
Actual params: [-0.9054,  0.2624]
-Original Grad: -0.003, -lr * Pred Grad:  -0.016, New P: -0.922
-Original Grad: 0.002, -lr * Pred Grad:  0.013, New P: 0.276
iter 4 loss: 0.043
Actual params: [-0.9216,  0.2756]
-Original Grad: -0.003, -lr * Pred Grad:  -0.022, New P: -0.943
-Original Grad: 0.002, -lr * Pred Grad:  0.013, New P: 0.289
iter 5 loss: 0.043
Actual params: [-0.9435,  0.2887]
-Original Grad: -0.003, -lr * Pred Grad:  -0.020, New P: -0.964
-Original Grad: 0.002, -lr * Pred Grad:  0.014, New P: 0.303
iter 6 loss: 0.043
Actual params: [-0.9639,  0.3031]
-Original Grad: -0.002, -lr * Pred Grad:  -0.018, New P: -0.981
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.317
iter 7 loss: 0.043
Actual params: [-0.9815,  0.3166]
-Original Grad: -0.003, -lr * Pred Grad:  -0.023, New P: -1.004
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.330
iter 8 loss: 0.043
Actual params: [-1.004 ,  0.3295]
-Original Grad: -0.002, -lr * Pred Grad:  -0.018, New P: -1.022
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.340
iter 9 loss: 0.043
Actual params: [-1.0216,  0.3398]
-Original Grad: -0.002, -lr * Pred Grad:  -0.019, New P: -1.041
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.351
iter 10 loss: 0.043
Actual params: [-1.041 ,  0.3513]
-Original Grad: -0.002, -lr * Pred Grad:  -0.020, New P: -1.061
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.362
iter 11 loss: 0.043
Actual params: [-1.061 ,  0.3623]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -1.080
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.373
iter 12 loss: 0.043
Actual params: [-1.0795,  0.3731]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -1.095
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.381
iter 13 loss: 0.043
Actual params: [-1.095 ,  0.3808]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -1.114
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.391
iter 14 loss: 0.043
Actual params: [-1.1141,  0.3907]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -1.133
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.402
iter 15 loss: 0.043
Actual params: [-1.1332,  0.4015]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -1.149
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.410
iter 16 loss: 0.043
Actual params: [-1.1494,  0.4099]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -1.165
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.418
iter 17 loss: 0.043
Actual params: [-1.1649,  0.4177]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -1.180
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.426
iter 18 loss: 0.043
Actual params: [-1.1803,  0.4259]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -1.198
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.436
iter 19 loss: 0.043
Actual params: [-1.1976,  0.4356]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -1.214
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.445
iter 20 loss: 0.043
Actual params: [-1.2138,  0.4448]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.229
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.453
iter 21 loss: 0.043
Actual params: [-1.2292,  0.4533]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.242
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.460
iter 22 loss: 0.043
Actual params: [-1.2423,  0.4596]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.253
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.468
iter 23 loss: 0.043
Actual params: [-1.2533,  0.4683]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.270
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.478
iter 24 loss: 0.043
Actual params: [-1.2698,  0.4782]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.286
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.487
iter 25 loss: 0.043
Actual params: [-1.2856,  0.4874]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.298
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.493
iter 26 loss: 0.043
Actual params: [-1.2979,  0.4931]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.309
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.500
iter 27 loss: 0.043
Actual params: [-1.3091,  0.4997]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.329
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.513
iter 28 loss: 0.043
Actual params: [-1.3286,  0.5128]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.338
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.518
iter 29 loss: 0.043
Actual params: [-1.3381,  0.5176]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.348
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.524
iter 30 loss: 0.043
Actual params: [-1.3481,  0.5238]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.358
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.530
Target params: [1.3344, 1.5708]
iter 0 loss: 0.548
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.004, -lr * Pred Grad:  -0.049, New P: -0.521
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.003
iter 1 loss: 0.548
Actual params: [-0.5214,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.034, New P: -0.555
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.004
iter 2 loss: 0.548
Actual params: [-0.5554,  0.0036]
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.578
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.004
iter 3 loss: 0.548
Actual params: [-0.5783,  0.004 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.601
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.004
iter 4 loss: 0.548
Actual params: [-0.6012,  0.0044]
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -0.623
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.005
iter 5 loss: 0.548
Actual params: [-0.6234,  0.0048]
-Original Grad: -0.001, -lr * Pred Grad:  -0.023, New P: -0.646
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.005
iter 6 loss: 0.548
Actual params: [-0.6463,  0.005 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.661
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.006
iter 7 loss: 0.548
Actual params: [-0.661 ,  0.0056]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.678
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.006
iter 8 loss: 0.548
Actual params: [-0.6779,  0.006 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.694
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.007
iter 9 loss: 0.548
Actual params: [-0.6944,  0.0066]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.709
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.007
iter 10 loss: 0.548
Actual params: [-0.7089,  0.0071]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.726
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.007
iter 11 loss: 0.548
Actual params: [-0.7259,  0.0071]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.742
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.007
iter 12 loss: 0.548
Actual params: [-0.7417,  0.0067]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.756
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.006
iter 13 loss: 0.548
Actual params: [-0.7564,  0.0063]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.769
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.006
iter 14 loss: 0.548
Actual params: [-0.7689,  0.0062]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.782
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.006
iter 15 loss: 0.548
Actual params: [-0.7816,  0.006 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.793
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.006
iter 16 loss: 0.548
Actual params: [-0.7935,  0.0061]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.806
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.005
iter 17 loss: 0.548
Actual params: [-0.8056,  0.0054]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.818
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.005
iter 18 loss: 0.548
Actual params: [-0.8185,  0.0052]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.831
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.004
iter 19 loss: 0.548
Actual params: [-0.8313,  0.0044]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.845
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.003
iter 20 loss: 0.548
Actual params: [-0.8453,  0.0031]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.857
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.003
iter 21 loss: 0.548
Actual params: [-0.8574,  0.0025]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.869
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.002
iter 22 loss: 0.548
Actual params: [-0.8686,  0.0019]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.884
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 0.000
iter 23 loss: 0.548
Actual params: [-8.8405e-01,  5.8869e-05]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.898
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.001
iter 24 loss: 0.548
Actual params: [-0.8978, -0.0012]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.912
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.003
iter 25 loss: 0.548
Actual params: [-0.9119, -0.0028]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.924
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.004
iter 26 loss: 0.548
Actual params: [-0.9237, -0.004 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.935
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.005
iter 27 loss: 0.548
Actual params: [-0.9349, -0.0055]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.949
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.007
iter 28 loss: 0.548
Actual params: [-0.9489, -0.0073]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.958
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.009
iter 29 loss: 0.548
Actual params: [-0.9579, -0.0086]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.971
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.010
iter 30 loss: 0.548
Actual params: [-0.9707, -0.0103]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.985
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.012
Target params: [1.3344, 1.5708]
iter 0 loss: 0.498
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.006, -lr * Pred Grad:  0.062, New P: -0.411
-Original Grad: 0.039, -lr * Pred Grad:  0.407, New P: 0.410
iter 1 loss: 0.461
Actual params: [-0.4108,  0.4102]
-Original Grad: 0.056, -lr * Pred Grad:  0.425, New P: 0.014
-Original Grad: 0.097, -lr * Pred Grad:  0.694, New P: 1.104
iter 2 loss: 0.293
Actual params: [0.0142, 1.104 ]
-Original Grad: 0.258, -lr * Pred Grad:  0.558, New P: 0.572
-Original Grad: 0.108, -lr * Pred Grad:  0.123, New P: 1.227
iter 3 loss: 0.113
Actual params: [0.5717, 1.2271]
-Original Grad: 0.386, -lr * Pred Grad:  0.232, New P: 0.803
-Original Grad: 0.168, -lr * Pred Grad:  -0.005, New P: 1.222
iter 4 loss: 0.069
Actual params: [0.8033, 1.2217]
-Original Grad: 0.248, -lr * Pred Grad:  0.023, New P: 0.826
-Original Grad: 0.170, -lr * Pred Grad:  0.159, New P: 1.381
iter 5 loss: 0.037
Actual params: [0.8262, 1.3811]
-Original Grad: 0.119, -lr * Pred Grad:  -0.051, New P: 0.776
-Original Grad: 0.175, -lr * Pred Grad:  0.171, New P: 1.552
iter 6 loss: 0.021
Actual params: [0.7756, 1.5518]
-Original Grad: 0.152, -lr * Pred Grad:  0.034, New P: 0.809
-Original Grad: 0.110, -lr * Pred Grad:  0.025, New P: 1.577
iter 7 loss: 0.015
Actual params: [0.8092, 1.5769]
-Original Grad: 0.154, -lr * Pred Grad:  0.029, New P: 0.839
-Original Grad: 0.100, -lr * Pred Grad:  0.016, New P: 1.593
iter 8 loss: 0.012
Actual params: [0.8386, 1.5928]
-Original Grad: 0.144, -lr * Pred Grad:  0.020, New P: 0.859
-Original Grad: 0.089, -lr * Pred Grad:  0.016, New P: 1.609
iter 9 loss: 0.010
Actual params: [0.8586, 1.6085]
-Original Grad: 0.139, -lr * Pred Grad:  0.012, New P: 0.870
-Original Grad: 0.093, -lr * Pred Grad:  0.022, New P: 1.630
iter 10 loss: 0.008
Actual params: [0.8702, 1.6301]
-Original Grad: 0.107, -lr * Pred Grad:  0.004, New P: 0.875
-Original Grad: 0.085, -lr * Pred Grad:  0.023, New P: 1.653
iter 11 loss: 0.007
Actual params: [0.8745, 1.6533]
-Original Grad: 0.097, -lr * Pred Grad:  0.001, New P: 0.876
-Original Grad: 0.091, -lr * Pred Grad:  0.025, New P: 1.679
iter 12 loss: 0.007
Actual params: [0.8757, 1.6786]
-Original Grad: 0.118, -lr * Pred Grad:  0.010, New P: 0.886
-Original Grad: 0.066, -lr * Pred Grad:  0.008, New P: 1.687
iter 13 loss: 0.006
Actual params: [0.8859, 1.6865]
-Original Grad: -0.009, -lr * Pred Grad:  -0.001, New P: 0.884
-Original Grad: -0.002, -lr * Pred Grad:  0.001, New P: 1.688
iter 14 loss: 0.006
Actual params: [0.8844, 1.6876]
-Original Grad: 0.002, -lr * Pred Grad:  -0.008, New P: 0.877
-Original Grad: 0.037, -lr * Pred Grad:  0.018, New P: 1.706
iter 15 loss: 0.007
Actual params: [0.8767, 1.7056]
-Original Grad: 0.040, -lr * Pred Grad:  0.006, New P: 0.883
-Original Grad: 0.010, -lr * Pred Grad:  -0.004, New P: 1.702
iter 16 loss: 0.006
Actual params: [0.8831, 1.7017]
-Original Grad: 0.032, -lr * Pred Grad:  0.004, New P: 0.887
-Original Grad: 0.010, -lr * Pred Grad:  -0.001, New P: 1.700
iter 17 loss: 0.006
Actual params: [0.8868, 1.7004]
-Original Grad: 0.048, -lr * Pred Grad:  0.001, New P: 0.888
-Original Grad: 0.036, -lr * Pred Grad:  0.009, New P: 1.709
iter 18 loss: 0.006
Actual params: [0.8877, 1.7093]
-Original Grad: 0.038, -lr * Pred Grad:  0.005, New P: 0.892
-Original Grad: 0.006, -lr * Pred Grad:  -0.004, New P: 1.705
iter 19 loss: 0.006
Actual params: [0.8923, 1.7051]
-Original Grad: 0.055, -lr * Pred Grad:  0.000, New P: 0.893
-Original Grad: 0.042, -lr * Pred Grad:  0.011, New P: 1.716
iter 20 loss: 0.006
Actual params: [0.8927, 1.7157]
-Original Grad: 0.020, -lr * Pred Grad:  0.001, New P: 0.894
-Original Grad: 0.005, -lr * Pred Grad:  -0.001, New P: 1.715
iter 21 loss: 0.006
Actual params: [0.8942, 1.7151]
-Original Grad: 0.042, -lr * Pred Grad:  0.001, New P: 0.895
-Original Grad: 0.025, -lr * Pred Grad:  0.005, New P: 1.720
iter 22 loss: 0.006
Actual params: [0.8952, 1.7199]
-Original Grad: 0.026, -lr * Pred Grad:  0.003, New P: 0.898
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 1.716
iter 23 loss: 0.006
Actual params: [0.8982, 1.7157]
-Original Grad: 0.058, -lr * Pred Grad:  0.006, New P: 0.904
-Original Grad: 0.002, -lr * Pred Grad:  -0.008, New P: 1.708
iter 24 loss: 0.006
Actual params: [0.904 , 1.7078]
-Original Grad: 0.004, -lr * Pred Grad:  -0.004, New P: 0.900
-Original Grad: 0.031, -lr * Pred Grad:  0.014, New P: 1.721
iter 25 loss: 0.006
Actual params: [0.8996, 1.7213]
-Original Grad: 0.079, -lr * Pred Grad:  0.005, New P: 0.905
-Original Grad: 0.018, -lr * Pred Grad:  -0.003, New P: 1.718
iter 26 loss: 0.006
Actual params: [0.905 , 1.7185]
-Original Grad: 0.038, -lr * Pred Grad:  0.003, New P: 0.908
-Original Grad: 0.010, -lr * Pred Grad:  -0.001, New P: 1.717
iter 27 loss: 0.006
Actual params: [0.9075, 1.7174]
-Original Grad: -0.036, -lr * Pred Grad:  -0.003, New P: 0.905
-Original Grad: -0.007, -lr * Pred Grad:  0.002, New P: 1.720
iter 28 loss: 0.006
Actual params: [0.9046, 1.7196]
-Original Grad: 0.076, -lr * Pred Grad:  0.001, New P: 0.906
-Original Grad: 0.053, -lr * Pred Grad:  0.011, New P: 1.731
iter 29 loss: 0.006
Actual params: [0.9058, 1.7306]
-Original Grad: -0.032, -lr * Pred Grad:  -0.002, New P: 0.903
-Original Grad: -0.009, -lr * Pred Grad:  0.001, New P: 1.731
iter 30 loss: 0.006
Actual params: [0.9034, 1.7314]
-Original Grad: 0.031, -lr * Pred Grad:  0.003, New P: 0.906
-Original Grad: 0.003, -lr * Pred Grad:  -0.003, New P: 1.728
Target params: [1.3344, 1.5708]
iter 0 loss: 0.170
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.250, -lr * Pred Grad:  0.611, New P: 0.139
-Original Grad: -0.028, -lr * Pred Grad:  -0.044, New P: -0.041
iter 1 loss: 0.082
Actual params: [ 0.1386, -0.041 ]
-Original Grad: 0.314, -lr * Pred Grad:  0.198, New P: 0.336
-Original Grad: 0.107, -lr * Pred Grad:  0.340, New P: 0.299
iter 2 loss: 0.020
Actual params: [0.3364, 0.2993]
-Original Grad: 0.252, -lr * Pred Grad:  0.043, New P: 0.379
-Original Grad: 0.095, -lr * Pred Grad:  0.082, New P: 0.381
iter 3 loss: 0.012
Actual params: [0.3792, 0.3811]
-Original Grad: 0.187, -lr * Pred Grad:  0.036, New P: 0.415
-Original Grad: 0.056, -lr * Pred Grad:  -0.007, New P: 0.375
iter 4 loss: 0.008
Actual params: [0.4154, 0.3746]
-Original Grad: 0.118, -lr * Pred Grad:  0.001, New P: 0.417
-Original Grad: 0.064, -lr * Pred Grad:  0.049, New P: 0.424
iter 5 loss: 0.008
Actual params: [0.4166, 0.4237]
-Original Grad: 0.074, -lr * Pred Grad:  0.026, New P: 0.442
-Original Grad: -0.006, -lr * Pred Grad:  -0.053, New P: 0.370
iter 6 loss: 0.007
Actual params: [0.4424, 0.3703]
-Original Grad: 0.057, -lr * Pred Grad:  0.007, New P: 0.450
-Original Grad: 0.018, -lr * Pred Grad:  -0.001, New P: 0.369
iter 7 loss: 0.006
Actual params: [0.4496, 0.369 ]
-Original Grad: 0.024, -lr * Pred Grad:  0.002, New P: 0.452
-Original Grad: 0.008, -lr * Pred Grad:  0.000, New P: 0.369
iter 8 loss: 0.006
Actual params: [0.452, 0.369]
-Original Grad: 0.050, -lr * Pred Grad:  0.004, New P: 0.456
-Original Grad: 0.019, -lr * Pred Grad:  0.001, New P: 0.370
iter 9 loss: 0.006
Actual params: [0.4562, 0.3703]
-Original Grad: 0.113, -lr * Pred Grad:  0.012, New P: 0.468
-Original Grad: 0.035, -lr * Pred Grad:  -0.008, New P: 0.362
iter 10 loss: 0.006
Actual params: [0.4683, 0.3622]
-Original Grad: 0.050, -lr * Pred Grad:  0.006, New P: 0.474
-Original Grad: 0.015, -lr * Pred Grad:  -0.005, New P: 0.357
iter 11 loss: 0.006
Actual params: [0.4745, 0.3572]
-Original Grad: 0.009, -lr * Pred Grad:  0.005, New P: 0.479
-Original Grad: -0.008, -lr * Pred Grad:  -0.010, New P: 0.348
iter 12 loss: 0.005
Actual params: [0.4792, 0.3475]
-Original Grad: 0.079, -lr * Pred Grad:  0.014, New P: 0.493
-Original Grad: 0.010, -lr * Pred Grad:  -0.018, New P: 0.329
iter 13 loss: 0.005
Actual params: [0.493 , 0.3294]
-Original Grad: 0.070, -lr * Pred Grad:  0.004, New P: 0.497
-Original Grad: 0.031, -lr * Pred Grad:  0.002, New P: 0.331
iter 14 loss: 0.005
Actual params: [0.4973, 0.3314]
-Original Grad: 0.028, -lr * Pred Grad:  0.003, New P: 0.500
-Original Grad: 0.009, -lr * Pred Grad:  -0.002, New P: 0.329
iter 15 loss: 0.005
Actual params: [0.5004, 0.3292]
-Original Grad: 0.058, -lr * Pred Grad:  0.006, New P: 0.506
-Original Grad: 0.020, -lr * Pred Grad:  -0.003, New P: 0.326
iter 16 loss: 0.005
Actual params: [0.5059, 0.3261]
-Original Grad: 0.019, -lr * Pred Grad:  0.006, New P: 0.512
-Original Grad: -0.007, -lr * Pred Grad:  -0.011, New P: 0.315
iter 17 loss: 0.005
Actual params: [0.5117, 0.3153]
-Original Grad: 0.030, -lr * Pred Grad:  0.012, New P: 0.524
-Original Grad: -0.021, -lr * Pred Grad:  -0.024, New P: 0.292
iter 18 loss: 0.005
Actual params: [0.5238, 0.2917]
-Original Grad: -0.038, -lr * Pred Grad:  -0.003, New P: 0.521
-Original Grad: -0.016, -lr * Pred Grad:  0.000, New P: 0.292
iter 19 loss: 0.005
Actual params: [0.521 , 0.2922]
-Original Grad: 0.027, -lr * Pred Grad:  0.003, New P: 0.524
-Original Grad: 0.007, -lr * Pred Grad:  -0.003, New P: 0.289
iter 20 loss: 0.005
Actual params: [0.5244, 0.289 ]
-Original Grad: 0.049, -lr * Pred Grad:  0.011, New P: 0.535
-Original Grad: -0.006, -lr * Pred Grad:  -0.017, New P: 0.272
iter 21 loss: 0.005
Actual params: [0.5354, 0.2724]
-Original Grad: 0.064, -lr * Pred Grad:  0.008, New P: 0.543
-Original Grad: 0.018, -lr * Pred Grad:  -0.006, New P: 0.266
iter 22 loss: 0.005
Actual params: [0.543 , 0.2663]
-Original Grad: 0.030, -lr * Pred Grad:  0.009, New P: 0.552
-Original Grad: -0.013, -lr * Pred Grad:  -0.016, New P: 0.251
iter 23 loss: 0.005
Actual params: [0.5523, 0.2507]
-Original Grad: 0.024, -lr * Pred Grad:  0.001, New P: 0.553
-Original Grad: 0.015, -lr * Pred Grad:  0.002, New P: 0.253
iter 24 loss: 0.005
Actual params: [0.5532, 0.253 ]
-Original Grad: 0.062, -lr * Pred Grad:  0.003, New P: 0.556
-Original Grad: 0.037, -lr * Pred Grad:  0.004, New P: 0.257
iter 25 loss: 0.004
Actual params: [0.5564, 0.2574]
-Original Grad: 0.063, -lr * Pred Grad:  0.006, New P: 0.562
-Original Grad: 0.024, -lr * Pred Grad:  -0.002, New P: 0.256
iter 26 loss: 0.004
Actual params: [0.5624, 0.2557]
-Original Grad: 0.033, -lr * Pred Grad:  0.005, New P: 0.567
-Original Grad: 0.005, -lr * Pred Grad:  -0.004, New P: 0.251
iter 27 loss: 0.004
Actual params: [0.5671, 0.2513]
-Original Grad: 0.031, -lr * Pred Grad:  0.003, New P: 0.570
-Original Grad: 0.014, -lr * Pred Grad:  0.000, New P: 0.252
iter 28 loss: 0.004
Actual params: [0.5697, 0.2516]
-Original Grad: 0.012, -lr * Pred Grad:  0.005, New P: 0.575
-Original Grad: -0.017, -lr * Pred Grad:  -0.010, New P: 0.242
iter 29 loss: 0.004
Actual params: [0.5751, 0.2418]
-Original Grad: 0.005, -lr * Pred Grad:  -0.001, New P: 0.574
-Original Grad: 0.011, -lr * Pred Grad:  0.003, New P: 0.245
iter 30 loss: 0.004
Actual params: [0.5739, 0.2453]
-Original Grad: 0.070, -lr * Pred Grad:  0.007, New P: 0.581
-Original Grad: 0.028, -lr * Pred Grad:  -0.001, New P: 0.244
Target params: [1.3344, 1.5708]
iter 0 loss: 0.514
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.106, -lr * Pred Grad:  0.331, New P: -0.141
-Original Grad: 0.010, -lr * Pred Grad:  -0.020, New P: -0.016
iter 1 loss: 0.466
Actual params: [-0.141 , -0.0163]
-Original Grad: 0.319, -lr * Pred Grad:  0.192, New P: 0.051
-Original Grad: 0.103, -lr * Pred Grad:  0.389, New P: 0.373
iter 2 loss: 0.382
Actual params: [0.051 , 0.3726]
-Original Grad: 0.543, -lr * Pred Grad:  0.048, New P: 0.099
-Original Grad: 0.257, -lr * Pred Grad:  0.178, New P: 0.551
iter 3 loss: 0.344
Actual params: [0.0988, 0.5509]
-Original Grad: 1.708, -lr * Pred Grad:  0.057, New P: 0.156
-Original Grad: -0.233, -lr * Pred Grad:  -0.099, New P: 0.452
iter 4 loss: 0.298
Actual params: [0.1558, 0.4519]
-Original Grad: 0.945, -lr * Pred Grad:  0.030, New P: 0.185
-Original Grad: 0.189, -lr * Pred Grad:  0.104, New P: 0.556
iter 5 loss: 0.271
Actual params: [0.1854, 0.5556]
-Original Grad: 1.142, -lr * Pred Grad:  0.030, New P: 0.215
-Original Grad: 0.165, -lr * Pred Grad:  0.055, New P: 0.611
iter 6 loss: 0.245
Actual params: [0.2152, 0.6107]
-Original Grad: 1.103, -lr * Pred Grad:  0.026, New P: 0.241
-Original Grad: 0.183, -lr * Pred Grad:  0.044, New P: 0.655
iter 7 loss: 0.222
Actual params: [0.2408, 0.655 ]
-Original Grad: 1.158, -lr * Pred Grad:  0.026, New P: 0.267
-Original Grad: 0.127, -lr * Pred Grad:  0.015, New P: 0.670
iter 8 loss: 0.202
Actual params: [0.2671, 0.6703]
-Original Grad: 0.971, -lr * Pred Grad:  0.016, New P: 0.283
-Original Grad: 0.297, -lr * Pred Grad:  0.061, New P: 0.732
iter 9 loss: 0.183
Actual params: [0.2832, 0.7316]
-Original Grad: 1.166, -lr * Pred Grad:  0.014, New P: 0.298
-Original Grad: 0.589, -lr * Pred Grad:  0.082, New P: 0.814
iter 10 loss: 0.164
Actual params: [0.2976, 0.8135]
-Original Grad: 1.324, -lr * Pred Grad:  0.024, New P: 0.321
-Original Grad: 0.241, -lr * Pred Grad:  0.014, New P: 0.827
iter 11 loss: 0.143
Actual params: [0.3214, 0.8273]
-Original Grad: 0.829, -lr * Pred Grad:  0.009, New P: 0.331
-Original Grad: 0.494, -lr * Pred Grad:  0.050, New P: 0.877
iter 12 loss: 0.132
Actual params: [0.3306, 0.8773]
-Original Grad: 1.126, -lr * Pred Grad:  0.021, New P: 0.352
-Original Grad: 0.073, -lr * Pred Grad:  -0.009, New P: 0.868
iter 13 loss: 0.117
Actual params: [0.352 , 0.8678]
-Original Grad: 1.129, -lr * Pred Grad:  0.016, New P: 0.368
-Original Grad: 0.326, -lr * Pred Grad:  0.023, New P: 0.891
iter 14 loss: 0.104
Actual params: [0.3682, 0.8908]
-Original Grad: 0.950, -lr * Pred Grad:  0.014, New P: 0.382
-Original Grad: 0.225, -lr * Pred Grad:  0.013, New P: 0.904
iter 15 loss: 0.094
Actual params: [0.3822, 0.9038]
-Original Grad: 1.034, -lr * Pred Grad:  0.015, New P: 0.398
-Original Grad: 0.197, -lr * Pred Grad:  0.007, New P: 0.911
iter 16 loss: 0.085
Actual params: [0.3976, 0.9113]
-Original Grad: 0.630, -lr * Pred Grad:  0.008, New P: 0.405
-Original Grad: 0.236, -lr * Pred Grad:  0.020, New P: 0.931
iter 17 loss: 0.079
Actual params: [0.4052, 0.9309]
-Original Grad: 0.684, -lr * Pred Grad:  0.008, New P: 0.413
-Original Grad: 0.261, -lr * Pred Grad:  0.021, New P: 0.952
iter 18 loss: 0.074
Actual params: [0.4134, 0.9518]
-Original Grad: 0.773, -lr * Pred Grad:  0.014, New P: 0.427
-Original Grad: 0.057, -lr * Pred Grad:  -0.007, New P: 0.945
iter 19 loss: 0.069
Actual params: [0.4272, 0.9449]
-Original Grad: 0.908, -lr * Pred Grad:  0.013, New P: 0.440
-Original Grad: 0.162, -lr * Pred Grad:  0.006, New P: 0.951
iter 20 loss: 0.063
Actual params: [0.4403, 0.9505]
-Original Grad: 0.606, -lr * Pred Grad:  0.009, New P: 0.449
-Original Grad: 0.110, -lr * Pred Grad:  0.004, New P: 0.955
iter 21 loss: 0.060
Actual params: [0.449 , 0.9548]
-Original Grad: 0.639, -lr * Pred Grad:  0.008, New P: 0.457
-Original Grad: 0.204, -lr * Pred Grad:  0.017, New P: 0.971
iter 22 loss: 0.056
Actual params: [0.4568, 0.9714]
-Original Grad: 0.563, -lr * Pred Grad:  0.009, New P: 0.465
-Original Grad: 0.076, -lr * Pred Grad:  0.001, New P: 0.973
iter 23 loss: 0.053
Actual params: [0.4654, 0.9726]
-Original Grad: 0.757, -lr * Pred Grad:  0.009, New P: 0.474
-Original Grad: 0.247, -lr * Pred Grad:  0.021, New P: 0.993
iter 24 loss: 0.049
Actual params: [0.4744, 0.9933]
-Original Grad: 0.408, -lr * Pred Grad:  0.007, New P: 0.481
-Original Grad: 0.058, -lr * Pred Grad:  0.001, New P: 0.994
iter 25 loss: 0.048
Actual params: [0.4809, 0.994 ]
-Original Grad: 0.210, -lr * Pred Grad:  0.002, New P: 0.483
-Original Grad: 0.120, -lr * Pred Grad:  0.013, New P: 1.007
iter 26 loss: 0.047
Actual params: [0.4826, 1.007 ]
-Original Grad: 0.088, -lr * Pred Grad:  0.001, New P: 0.483
-Original Grad: 0.058, -lr * Pred Grad:  0.006, New P: 1.013
iter 27 loss: 0.046
Actual params: [0.4833, 1.0133]
-Original Grad: 0.427, -lr * Pred Grad:  0.006, New P: 0.489
-Original Grad: 0.171, -lr * Pred Grad:  0.015, New P: 1.029
iter 28 loss: 0.044
Actual params: [0.489 , 1.0287]
-Original Grad: 0.482, -lr * Pred Grad:  0.005, New P: 0.494
-Original Grad: 0.294, -lr * Pred Grad:  0.028, New P: 1.056
iter 29 loss: 0.041
Actual params: [0.4939, 1.0564]
-Original Grad: 0.125, -lr * Pred Grad:  0.003, New P: 0.497
-Original Grad: -0.046, -lr * Pred Grad:  -0.007, New P: 1.049
iter 30 loss: 0.041
Actual params: [0.4969, 1.0493]
-Original Grad: 0.016, -lr * Pred Grad:  0.000, New P: 0.497
-Original Grad: 0.006, -lr * Pred Grad:  0.000, New P: 1.050
Target params: [1.3344, 1.5708]
iter 0 loss: 0.542
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.087, -lr * Pred Grad:  0.564, New P: 0.091
-Original Grad: 0.032, -lr * Pred Grad:  0.212, New P: 0.215
iter 1 loss: 0.443
Actual params: [0.0915, 0.215 ]
-Original Grad: 0.446, -lr * Pred Grad:  0.350, New P: 0.442
-Original Grad: 0.240, -lr * Pred Grad:  0.104, New P: 0.319
iter 2 loss: 0.317
Actual params: [0.4415, 0.3188]
-Original Grad: 0.470, -lr * Pred Grad:  0.185, New P: 0.627
-Original Grad: 0.223, -lr * Pred Grad:  0.079, New P: 0.398
iter 3 loss: 0.234
Actual params: [0.6266, 0.3982]
-Original Grad: 0.449, -lr * Pred Grad:  0.061, New P: 0.688
-Original Grad: 0.263, -lr * Pred Grad:  0.178, New P: 0.576
iter 4 loss: 0.178
Actual params: [0.6877, 0.5762]
-Original Grad: 0.545, -lr * Pred Grad:  0.089, New P: 0.776
-Original Grad: 0.300, -lr * Pred Grad:  0.074, New P: 0.650
iter 5 loss: 0.137
Actual params: [0.7763, 0.6502]
-Original Grad: 0.617, -lr * Pred Grad:  0.073, New P: 0.849
-Original Grad: 0.310, -lr * Pred Grad:  0.046, New P: 0.696
iter 6 loss: 0.112
Actual params: [0.8493, 0.6964]
-Original Grad: 0.690, -lr * Pred Grad:  0.046, New P: 0.895
-Original Grad: 0.290, -lr * Pred Grad:  0.036, New P: 0.732
iter 7 loss: 0.100
Actual params: [0.895 , 0.7322]
-Original Grad: 0.165, -lr * Pred Grad:  -0.042, New P: 0.853
-Original Grad: 0.197, -lr * Pred Grad:  0.143, New P: 0.875
iter 8 loss: 0.087
Actual params: [0.8528, 0.8754]
-Original Grad: 0.351, -lr * Pred Grad:  -0.016, New P: 0.837
-Original Grad: 0.280, -lr * Pred Grad:  0.113, New P: 0.988
iter 9 loss: 0.078
Actual params: [0.8371, 0.9884]
-Original Grad: 0.209, -lr * Pred Grad:  -0.004, New P: 0.833
-Original Grad: 0.161, -lr * Pred Grad:  0.055, New P: 1.043
iter 10 loss: 0.075
Actual params: [0.8334, 1.0433]
-Original Grad: 0.421, -lr * Pred Grad:  0.016, New P: 0.850
-Original Grad: 0.174, -lr * Pred Grad:  0.019, New P: 1.062
iter 11 loss: 0.073
Actual params: [0.8496, 1.0621]
-Original Grad: 0.212, -lr * Pred Grad:  -0.007, New P: 0.843
-Original Grad: 0.197, -lr * Pred Grad:  0.056, New P: 1.118
iter 12 loss: 0.072
Actual params: [0.8429, 1.1178]
-Original Grad: 0.067, -lr * Pred Grad:  -0.015, New P: 0.828
-Original Grad: 0.176, -lr * Pred Grad:  0.056, New P: 1.173
iter 13 loss: 0.073
Actual params: [0.8277, 1.1734]
-Original Grad: -0.237, -lr * Pred Grad:  0.001, New P: 0.828
-Original Grad: -0.201, -lr * Pred Grad:  -0.035, New P: 1.138
iter 14 loss: 0.073
Actual params: [0.8284, 1.138 ]
-Original Grad: 0.231, -lr * Pred Grad:  -0.002, New P: 0.827
-Original Grad: 0.222, -lr * Pred Grad:  0.038, New P: 1.176
iter 15 loss: 0.073
Actual params: [0.8267, 1.1758]
-Original Grad: 0.019, -lr * Pred Grad:  0.007, New P: 0.834
-Original Grad: -0.069, -lr * Pred Grad:  -0.018, New P: 1.158
iter 16 loss: 0.072
Actual params: [0.8339, 1.1581]
-Original Grad: 0.087, -lr * Pred Grad:  0.004, New P: 0.837
-Original Grad: 0.044, -lr * Pred Grad:  0.003, New P: 1.161
iter 17 loss: 0.072
Actual params: [0.8375, 1.1612]
-Original Grad: -0.168, -lr * Pred Grad:  -0.001, New P: 0.837
-Original Grad: -0.152, -lr * Pred Grad:  -0.020, New P: 1.141
iter 18 loss: 0.072
Actual params: [0.8365, 1.1408]
-Original Grad: 0.007, -lr * Pred Grad:  -0.008, New P: 0.828
-Original Grad: 0.112, -lr * Pred Grad:  0.023, New P: 1.164
iter 19 loss: 0.073
Actual params: [0.8283, 1.1641]
-Original Grad: -0.042, -lr * Pred Grad:  0.002, New P: 0.831
-Original Grad: -0.072, -lr * Pred Grad:  -0.012, New P: 1.152
iter 20 loss: 0.073
Actual params: [0.8307, 1.152 ]
-Original Grad: 0.201, -lr * Pred Grad:  0.003, New P: 0.834
-Original Grad: 0.133, -lr * Pred Grad:  0.013, New P: 1.165
iter 21 loss: 0.073
Actual params: [0.8341, 1.1654]
-Original Grad: -0.259, -lr * Pred Grad:  -0.009, New P: 0.825
-Original Grad: -0.078, -lr * Pred Grad:  0.001, New P: 1.167
iter 22 loss: 0.073
Actual params: [0.8251, 1.1665]
-Original Grad: 0.209, -lr * Pred Grad:  0.005, New P: 0.830
-Original Grad: 0.090, -lr * Pred Grad:  0.005, New P: 1.172
iter 23 loss: 0.073
Actual params: [0.83  , 1.1718]
-Original Grad: 0.066, -lr * Pred Grad:  0.003, New P: 0.833
-Original Grad: -0.003, -lr * Pred Grad:  -0.004, New P: 1.168
iter 24 loss: 0.073
Actual params: [0.8334, 1.1675]
-Original Grad: -0.122, -lr * Pred Grad:  -0.000, New P: 0.833
-Original Grad: -0.093, -lr * Pred Grad:  -0.011, New P: 1.156
iter 25 loss: 0.072
Actual params: [0.8332, 1.1564]
-Original Grad: 0.206, -lr * Pred Grad:  0.012, New P: 0.846
-Original Grad: -0.023, -lr * Pred Grad:  -0.019, New P: 1.137
iter 26 loss: 0.072
Actual params: [0.8456, 1.1373]
-Original Grad: 0.452, -lr * Pred Grad:  0.014, New P: 0.860
-Original Grad: 0.117, -lr * Pred Grad:  -0.003, New P: 1.135
iter 27 loss: 0.072
Actual params: [0.8596, 1.1347]
-Original Grad: 0.273, -lr * Pred Grad:  0.012, New P: 0.872
-Original Grad: -0.015, -lr * Pred Grad:  -0.019, New P: 1.116
iter 28 loss: 0.071
Actual params: [0.8721, 1.1162]
-Original Grad: 0.135, -lr * Pred Grad:  0.003, New P: 0.875
-Original Grad: 0.047, -lr * Pred Grad:  0.003, New P: 1.119
iter 29 loss: 0.071
Actual params: [0.8751, 1.1188]
-Original Grad: 0.228, -lr * Pred Grad:  0.006, New P: 0.881
-Original Grad: 0.054, -lr * Pred Grad:  -0.000, New P: 1.118
iter 30 loss: 0.071
Actual params: [0.8812, 1.1185]
-Original Grad: -0.278, -lr * Pred Grad:  -0.009, New P: 0.872
-Original Grad: -0.043, -lr * Pred Grad:  0.005, New P: 1.124
Target params: [1.3344, 1.5708]
iter 0 loss: 0.206
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.092, -lr * Pred Grad:  0.571, New P: 0.098
-Original Grad: 0.062, -lr * Pred Grad:  0.371, New P: 0.375
iter 1 loss: 0.088
Actual params: [0.0984, 0.3745]
-Original Grad: 0.216, -lr * Pred Grad:  0.345, New P: 0.444
-Original Grad: -0.023, -lr * Pred Grad:  -0.209, New P: 0.166
iter 2 loss: 0.251
Actual params: [0.4438, 0.1655]
-Original Grad: -1.515, -lr * Pred Grad:  0.010, New P: 0.454
-Original Grad: 0.514, -lr * Pred Grad:  0.448, New P: 0.614
iter 3 loss: 0.139
Actual params: [0.4539, 0.6137]
-Original Grad: -0.250, -lr * Pred Grad:  0.032, New P: 0.485
-Original Grad: 0.441, -lr * Pred Grad:  0.268, New P: 0.882
iter 4 loss: 0.097
Actual params: [0.4855, 0.8817]
-Original Grad: -0.419, -lr * Pred Grad:  0.007, New P: 0.492
-Original Grad: 0.296, -lr * Pred Grad:  0.116, New P: 0.997
iter 5 loss: 0.080
Actual params: [0.4923, 0.9974]
-Original Grad: -0.197, -lr * Pred Grad:  0.013, New P: 0.505
-Original Grad: 0.267, -lr * Pred Grad:  0.096, New P: 1.094
iter 6 loss: 0.069
Actual params: [0.5049, 1.0939]
-Original Grad: -0.124, -lr * Pred Grad:  0.007, New P: 0.512
-Original Grad: 0.165, -lr * Pred Grad:  0.055, New P: 1.149
iter 7 loss: 0.064
Actual params: [0.5124, 1.1494]
-Original Grad: 0.106, -lr * Pred Grad:  0.014, New P: 0.527
-Original Grad: 0.093, -lr * Pred Grad:  0.046, New P: 1.195
iter 8 loss: 0.060
Actual params: [0.5266, 1.1949]
-Original Grad: 0.056, -lr * Pred Grad:  0.013, New P: 0.540
-Original Grad: 0.122, -lr * Pred Grad:  0.048, New P: 1.243
iter 9 loss: 0.056
Actual params: [0.5401, 1.2431]
-Original Grad: 0.059, -lr * Pred Grad:  0.010, New P: 0.550
-Original Grad: 0.066, -lr * Pred Grad:  0.028, New P: 1.271
iter 10 loss: 0.053
Actual params: [0.5497, 1.2713]
-Original Grad: 0.013, -lr * Pred Grad:  0.009, New P: 0.558
-Original Grad: 0.083, -lr * Pred Grad:  0.030, New P: 1.301
iter 11 loss: 0.051
Actual params: [0.5583, 1.301 ]
-Original Grad: 0.091, -lr * Pred Grad:  0.010, New P: 0.568
-Original Grad: 0.031, -lr * Pred Grad:  0.020, New P: 1.321
iter 12 loss: 0.049
Actual params: [0.5683, 1.3207]
-Original Grad: 0.120, -lr * Pred Grad:  0.011, New P: 0.579
-Original Grad: 0.004, -lr * Pred Grad:  0.014, New P: 1.335
iter 13 loss: 0.048
Actual params: [0.579 , 1.3351]
-Original Grad: 0.038, -lr * Pred Grad:  0.014, New P: 0.593
-Original Grad: 0.095, -lr * Pred Grad:  0.038, New P: 1.373
iter 14 loss: 0.046
Actual params: [0.5925, 1.3726]
-Original Grad: 0.123, -lr * Pred Grad:  0.014, New P: 0.607
-Original Grad: 0.021, -lr * Pred Grad:  0.021, New P: 1.394
iter 15 loss: 0.044
Actual params: [0.6066, 1.3937]
-Original Grad: -0.009, -lr * Pred Grad:  0.006, New P: 0.613
-Original Grad: 0.062, -lr * Pred Grad:  0.021, New P: 1.415
iter 16 loss: 0.043
Actual params: [0.6128, 1.4147]
-Original Grad: 0.032, -lr * Pred Grad:  0.007, New P: 0.620
-Original Grad: 0.035, -lr * Pred Grad:  0.017, New P: 1.431
iter 17 loss: 0.042
Actual params: [0.6202, 1.4314]
-Original Grad: 0.137, -lr * Pred Grad:  0.012, New P: 0.632
-Original Grad: -0.021, -lr * Pred Grad:  0.010, New P: 1.441
iter 18 loss: 0.041
Actual params: [0.6322, 1.4412]
-Original Grad: 0.096, -lr * Pred Grad:  0.016, New P: 0.648
-Original Grad: 0.040, -lr * Pred Grad:  0.028, New P: 1.469
iter 19 loss: 0.040
Actual params: [0.6483, 1.4692]
-Original Grad: 0.234, -lr * Pred Grad:  0.014, New P: 0.663
-Original Grad: -0.095, -lr * Pred Grad:  -0.005, New P: 1.464
iter 20 loss: 0.039
Actual params: [0.6626, 1.4638]
-Original Grad: 0.106, -lr * Pred Grad:  0.014, New P: 0.677
-Original Grad: 0.003, -lr * Pred Grad:  0.018, New P: 1.482
iter 21 loss: 0.038
Actual params: [0.6768, 1.4816]
-Original Grad: 0.156, -lr * Pred Grad:  0.024, New P: 0.700
-Original Grad: 0.025, -lr * Pred Grad:  0.033, New P: 1.514
iter 22 loss: 0.036
Actual params: [0.7004, 1.5144]
-Original Grad: 0.184, -lr * Pred Grad:  0.023, New P: 0.723
-Original Grad: -0.008, -lr * Pred Grad:  0.023, New P: 1.538
iter 23 loss: 0.035
Actual params: [0.7231, 1.5379]
-Original Grad: 0.096, -lr * Pred Grad:  0.010, New P: 0.734
-Original Grad: -0.017, -lr * Pred Grad:  0.008, New P: 1.546
iter 24 loss: 0.034
Actual params: [0.7335, 1.5458]
-Original Grad: 0.136, -lr * Pred Grad:  0.021, New P: 0.754
-Original Grad: 0.012, -lr * Pred Grad:  0.025, New P: 1.570
iter 25 loss: 0.033
Actual params: [0.7542, 1.5705]
-Original Grad: 0.183, -lr * Pred Grad:  0.014, New P: 0.768
-Original Grad: -0.081, -lr * Pred Grad:  -0.003, New P: 1.567
iter 26 loss: 0.033
Actual params: [0.7682, 1.5674]
-Original Grad: 0.115, -lr * Pred Grad:  0.009, New P: 0.778
-Original Grad: -0.052, -lr * Pred Grad:  -0.002, New P: 1.566
iter 27 loss: 0.032
Actual params: [0.7777, 1.5656]
-Original Grad: 0.164, -lr * Pred Grad:  0.012, New P: 0.790
-Original Grad: -0.084, -lr * Pred Grad:  -0.006, New P: 1.559
iter 28 loss: 0.031
Actual params: [0.79  , 1.5595]
-Original Grad: 0.196, -lr * Pred Grad:  0.027, New P: 0.817
-Original Grad: -0.021, -lr * Pred Grad:  0.024, New P: 1.583
iter 29 loss: 0.030
Actual params: [0.8171, 1.5831]
-Original Grad: 0.069, -lr * Pred Grad:  0.018, New P: 0.835
-Original Grad: 0.048, -lr * Pred Grad:  0.027, New P: 1.611
iter 30 loss: 0.029
Actual params: [0.835 , 1.6105]
-Original Grad: 0.158, -lr * Pred Grad:  0.004, New P: 0.839
-Original Grad: -0.141, -lr * Pred Grad:  -0.027, New P: 1.584
Target params: [1.3344, 1.5708]
iter 0 loss: 0.778
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.482
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.005
iter 1 loss: 0.778
Actual params: [-0.482 ,  0.0053]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.492
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.007
iter 2 loss: 0.778
Actual params: [-0.4917,  0.0071]
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.501
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.009
iter 3 loss: 0.778
Actual params: [-0.5006,  0.0088]
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.510
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.010
iter 4 loss: 0.778
Actual params: [-0.5096,  0.0104]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.520
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.012
iter 5 loss: 0.778
Actual params: [-0.5198,  0.0123]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.529
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.014
iter 6 loss: 0.778
Actual params: [-0.5294,  0.014 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.539
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.016
iter 7 loss: 0.778
Actual params: [-0.539 ,  0.0158]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.549
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.018
iter 8 loss: 0.778
Actual params: [-0.5493,  0.0177]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.561
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.020
iter 9 loss: 0.778
Actual params: [-0.5609,  0.0198]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.022
iter 10 loss: 0.778
Actual params: [-0.5715,  0.0218]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.582
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.024
iter 11 loss: 0.778
Actual params: [-0.5819,  0.0237]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.592
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.026
iter 12 loss: 0.778
Actual params: [-0.5923,  0.0255]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.603
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.027
iter 13 loss: 0.778
Actual params: [-0.6029,  0.0275]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.613
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.029
iter 14 loss: 0.778
Actual params: [-0.6135,  0.0294]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.624
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.031
iter 15 loss: 0.778
Actual params: [-0.6245,  0.0314]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.636
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.033
iter 16 loss: 0.778
Actual params: [-0.6358,  0.0334]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.648
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.035
iter 17 loss: 0.778
Actual params: [-0.6481,  0.0355]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.659
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.037
iter 18 loss: 0.778
Actual params: [-0.6586,  0.0373]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.669
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.039
iter 19 loss: 0.778
Actual params: [-0.6692,  0.039 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.681
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.041
iter 20 loss: 0.778
Actual params: [-0.6806,  0.0409]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.694
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.043
iter 21 loss: 0.778
Actual params: [-0.6942,  0.0432]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.707
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.045
iter 22 loss: 0.778
Actual params: [-0.7071,  0.0454]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.718
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.047
iter 23 loss: 0.778
Actual params: [-0.7183,  0.0472]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.729
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.049
iter 24 loss: 0.778
Actual params: [-0.7291,  0.0489]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.739
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.051
iter 25 loss: 0.778
Actual params: [-0.7394,  0.0505]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.752
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.053
iter 26 loss: 0.778
Actual params: [-0.7517,  0.0526]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.763
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.054
iter 27 loss: 0.778
Actual params: [-0.7632,  0.0544]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.776
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.056
iter 28 loss: 0.778
Actual params: [-0.7757,  0.0565]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.788
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.058
iter 29 loss: 0.778
Actual params: [-0.7878,  0.0584]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.801
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.060
iter 30 loss: 0.778
Actual params: [-0.801 ,  0.0605]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.813
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.062
Target params: [1.3344, 1.5708]
iter 0 loss: 0.440
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.017, -lr * Pred Grad:  0.188, New P: -0.285
-Original Grad: 0.007, -lr * Pred Grad:  0.080, New P: 0.084
iter 1 loss: 0.436
Actual params: [-0.2848,  0.0838]
-Original Grad: 0.077, -lr * Pred Grad:  0.352, New P: 0.067
-Original Grad: 0.034, -lr * Pred Grad:  0.171, New P: 0.255
iter 2 loss: 0.337
Actual params: [0.067 , 0.2546]
-Original Grad: 0.775, -lr * Pred Grad:  0.201, New P: 0.268
-Original Grad: 0.377, -lr * Pred Grad:  0.062, New P: 0.316
iter 3 loss: 0.168
Actual params: [0.2678, 0.3165]
-Original Grad: 1.010, -lr * Pred Grad:  0.124, New P: 0.392
-Original Grad: 0.505, -lr * Pred Grad:  -0.027, New P: 0.290
iter 4 loss: 0.108
Actual params: [0.392 , 0.2899]
-Original Grad: 0.513, -lr * Pred Grad:  -0.016, New P: 0.376
-Original Grad: 0.418, -lr * Pred Grad:  0.123, New P: 0.413
iter 5 loss: 0.092
Actual params: [0.3757, 0.4127]
-Original Grad: 0.428, -lr * Pred Grad:  -0.002, New P: 0.373
-Original Grad: 0.449, -lr * Pred Grad:  0.080, New P: 0.493
iter 6 loss: 0.080
Actual params: [0.3735, 0.493 ]
-Original Grad: 0.505, -lr * Pred Grad:  0.017, New P: 0.391
-Original Grad: 0.327, -lr * Pred Grad:  0.033, New P: 0.526
iter 7 loss: 0.069
Actual params: [0.391 , 0.5261]
-Original Grad: 0.412, -lr * Pred Grad:  0.016, New P: 0.407
-Original Grad: 0.245, -lr * Pred Grad:  0.020, New P: 0.546
iter 8 loss: 0.062
Actual params: [0.4069, 0.5463]
-Original Grad: 0.477, -lr * Pred Grad:  0.019, New P: 0.426
-Original Grad: 0.244, -lr * Pred Grad:  0.015, New P: 0.562
iter 9 loss: 0.055
Actual params: [0.4256, 0.5616]
-Original Grad: 0.383, -lr * Pred Grad:  0.020, New P: 0.446
-Original Grad: 0.118, -lr * Pred Grad:  -0.002, New P: 0.559
iter 10 loss: 0.050
Actual params: [0.4455, 0.5592]
-Original Grad: 0.262, -lr * Pred Grad:  0.007, New P: 0.453
-Original Grad: 0.171, -lr * Pred Grad:  0.017, New P: 0.577
iter 11 loss: 0.046
Actual params: [0.4528, 0.5767]
-Original Grad: 0.274, -lr * Pred Grad:  0.013, New P: 0.466
-Original Grad: 0.095, -lr * Pred Grad:  0.002, New P: 0.578
iter 12 loss: 0.043
Actual params: [0.4661, 0.5783]
-Original Grad: 0.194, -lr * Pred Grad:  0.006, New P: 0.472
-Original Grad: 0.140, -lr * Pred Grad:  0.016, New P: 0.594
iter 13 loss: 0.040
Actual params: [0.4716, 0.5941]
-Original Grad: 0.245, -lr * Pred Grad:  0.012, New P: 0.483
-Original Grad: 0.062, -lr * Pred Grad:  0.001, New P: 0.595
iter 14 loss: 0.038
Actual params: [0.4834, 0.5951]
-Original Grad: 0.141, -lr * Pred Grad:  0.005, New P: 0.489
-Original Grad: 0.081, -lr * Pred Grad:  0.009, New P: 0.604
iter 15 loss: 0.036
Actual params: [0.4887, 0.6042]
-Original Grad: 0.167, -lr * Pred Grad:  0.008, New P: 0.496
-Original Grad: 0.073, -lr * Pred Grad:  0.007, New P: 0.611
iter 16 loss: 0.034
Actual params: [0.4963, 0.611 ]
-Original Grad: 0.184, -lr * Pred Grad:  0.008, New P: 0.504
-Original Grad: 0.099, -lr * Pred Grad:  0.011, New P: 0.622
iter 17 loss: 0.032
Actual params: [0.5043, 0.6223]
-Original Grad: 0.212, -lr * Pred Grad:  0.011, New P: 0.515
-Original Grad: 0.074, -lr * Pred Grad:  0.006, New P: 0.629
iter 18 loss: 0.030
Actual params: [0.5149, 0.6286]
-Original Grad: 0.250, -lr * Pred Grad:  0.012, New P: 0.527
-Original Grad: 0.099, -lr * Pred Grad:  0.010, New P: 0.638
iter 19 loss: 0.027
Actual params: [0.527 , 0.6384]
-Original Grad: 0.132, -lr * Pred Grad:  0.004, New P: 0.531
-Original Grad: 0.132, -lr * Pred Grad:  0.019, New P: 0.657
iter 20 loss: 0.026
Actual params: [0.5314, 0.6573]
-Original Grad: 0.210, -lr * Pred Grad:  0.010, New P: 0.541
-Original Grad: 0.109, -lr * Pred Grad:  0.013, New P: 0.670
iter 21 loss: 0.024
Actual params: [0.5413, 0.6702]
-Original Grad: 0.110, -lr * Pred Grad:  0.004, New P: 0.545
-Original Grad: 0.129, -lr * Pred Grad:  0.019, New P: 0.689
iter 22 loss: 0.022
Actual params: [0.5449, 0.689 ]
-Original Grad: 0.174, -lr * Pred Grad:  0.011, New P: 0.556
-Original Grad: 0.023, -lr * Pred Grad:  -0.001, New P: 0.688
iter 23 loss: 0.021
Actual params: [0.5556, 0.6885]
-Original Grad: 0.157, -lr * Pred Grad:  0.007, New P: 0.562
-Original Grad: 0.127, -lr * Pred Grad:  0.019, New P: 0.707
iter 24 loss: 0.020
Actual params: [0.5623, 0.7071]
-Original Grad: 0.153, -lr * Pred Grad:  0.008, New P: 0.570
-Original Grad: 0.072, -lr * Pred Grad:  0.009, New P: 0.716
iter 25 loss: 0.019
Actual params: [0.5702, 0.7164]
-Original Grad: 0.110, -lr * Pred Grad:  0.005, New P: 0.575
-Original Grad: 0.096, -lr * Pred Grad:  0.015, New P: 0.731
iter 26 loss: 0.018
Actual params: [0.575 , 0.7313]
-Original Grad: 0.174, -lr * Pred Grad:  0.010, New P: 0.585
-Original Grad: 0.034, -lr * Pred Grad:  0.002, New P: 0.734
iter 27 loss: 0.017
Actual params: [0.5852, 0.7337]
-Original Grad: 0.109, -lr * Pred Grad:  0.006, New P: 0.591
-Original Grad: 0.040, -lr * Pred Grad:  0.005, New P: 0.739
iter 28 loss: 0.016
Actual params: [0.5914, 0.7392]
-Original Grad: 0.076, -lr * Pred Grad:  0.004, New P: 0.595
-Original Grad: 0.066, -lr * Pred Grad:  0.012, New P: 0.751
iter 29 loss: 0.016
Actual params: [0.5951, 0.7512]
-Original Grad: 0.110, -lr * Pred Grad:  0.007, New P: 0.602
-Original Grad: 0.051, -lr * Pred Grad:  0.008, New P: 0.760
iter 30 loss: 0.015
Actual params: [0.6016, 0.7596]
-Original Grad: 0.042, -lr * Pred Grad:  0.002, New P: 0.604
-Original Grad: 0.058, -lr * Pred Grad:  0.012, New P: 0.771
Target params: [1.3344, 1.5708]
iter 0 loss: 0.586
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.013, -lr * Pred Grad:  -0.145, New P: -0.617
-Original Grad: 0.011, -lr * Pred Grad:  0.122, New P: 0.125
iter 1 loss: 0.585
Actual params: [-0.617 ,  0.1253]
-Original Grad: -0.002, -lr * Pred Grad:  -0.021, New P: -0.638
-Original Grad: 0.002, -lr * Pred Grad:  0.022, New P: 0.147
iter 2 loss: 0.585
Actual params: [-0.6379,  0.1472]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.655
-Original Grad: 0.001, -lr * Pred Grad:  0.020, New P: 0.167
iter 3 loss: 0.585
Actual params: [-0.6554,  0.1668]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.669
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 0.184
iter 4 loss: 0.585
Actual params: [-0.6694,  0.1836]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.680
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.197
iter 5 loss: 0.585
Actual params: [-0.6802,  0.1972]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.691
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.211
iter 6 loss: 0.585
Actual params: [-0.6907,  0.2113]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.700
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.224
iter 7 loss: 0.585
Actual params: [-0.6996,  0.2239]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.707
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.236
iter 8 loss: 0.585
Actual params: [-0.7075,  0.2359]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.714
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.247
iter 9 loss: 0.585
Actual params: [-0.7142,  0.2471]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.721
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.258
iter 10 loss: 0.585
Actual params: [-0.7205,  0.2584]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.726
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.269
iter 11 loss: 0.585
Actual params: [-0.726 ,  0.2691]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.731
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.280
iter 12 loss: 0.585
Actual params: [-0.7309,  0.28  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.735
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.292
iter 13 loss: 0.585
Actual params: [-0.7355,  0.2917]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.739
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.304
iter 14 loss: 0.585
Actual params: [-0.7393,  0.3038]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.742
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.315
iter 15 loss: 0.585
Actual params: [-0.742 ,  0.3151]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.744
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.325
iter 16 loss: 0.585
Actual params: [-0.744 ,  0.3254]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.745
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.337
iter 17 loss: 0.585
Actual params: [-0.7452,  0.3367]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.745
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.349
iter 18 loss: 0.585
Actual params: [-0.7452,  0.3489]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.744
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.361
iter 19 loss: 0.585
Actual params: [-0.744 ,  0.3611]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.742
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.372
iter 20 loss: 0.585
Actual params: [-0.7419,  0.3724]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.738
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.385
iter 21 loss: 0.585
Actual params: [-0.7382,  0.3845]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.732
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.397
iter 22 loss: 0.585
Actual params: [-0.7324,  0.3973]
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: -0.724
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.411
iter 23 loss: 0.585
Actual params: [-0.7236,  0.4114]
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: -0.712
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.426
iter 24 loss: 0.585
Actual params: [-0.712 ,  0.4257]
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: -0.695
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.442
iter 25 loss: 0.585
Actual params: [-0.6947,  0.442 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: -0.671
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.460
iter 26 loss: 0.585
Actual params: [-0.6708,  0.4603]
-Original Grad: 0.000, -lr * Pred Grad:  0.040, New P: -0.631
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.483
iter 27 loss: 0.585
Actual params: [-0.6313,  0.4832]
-Original Grad: 0.000, -lr * Pred Grad:  0.060, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 0.510
iter 28 loss: 0.585
Actual params: [-0.5717,  0.5096]
-Original Grad: 0.001, -lr * Pred Grad:  0.108, New P: -0.463
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: 0.545
iter 29 loss: 0.585
Actual params: [-0.4634,  0.5451]
-Original Grad: 0.001, -lr * Pred Grad:  0.334, New P: -0.129
-Original Grad: 0.000, -lr * Pred Grad:  0.077, New P: 0.622
iter 30 loss: 0.583
Actual params: [-0.1293,  0.6218]
-Original Grad: 0.034, -lr * Pred Grad:  3.556, New P: 3.426
-Original Grad: 0.003, -lr * Pred Grad:  0.393, New P: 1.015
Target params: [1.3344, 1.5708]
iter 0 loss: 0.293
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.043, -lr * Pred Grad:  -0.407, New P: -0.879
-Original Grad: 0.042, -lr * Pred Grad:  0.397, New P: 0.401
iter 1 loss: 0.280
Actual params: [-0.8794,  0.4007]
-Original Grad: 0.024, -lr * Pred Grad:  0.278, New P: -0.602
-Original Grad: 0.038, -lr * Pred Grad:  0.409, New P: 0.810
iter 2 loss: 0.258
Actual params: [-0.6017,  0.8101]
-Original Grad: 0.072, -lr * Pred Grad:  0.623, New P: 0.022
-Original Grad: 0.055, -lr * Pred Grad:  0.464, New P: 1.275
iter 3 loss: 0.179
Actual params: [0.0215, 1.2745]
-Original Grad: 0.186, -lr * Pred Grad:  0.567, New P: 0.588
-Original Grad: 0.065, -lr * Pred Grad:  0.103, New P: 1.378
iter 4 loss: 0.050
Actual params: [0.5883, 1.3777]
-Original Grad: 0.314, -lr * Pred Grad:  0.108, New P: 0.696
-Original Grad: 0.188, -lr * Pred Grad:  0.163, New P: 1.541
iter 5 loss: 0.020
Actual params: [0.696 , 1.5407]
-Original Grad: 0.223, -lr * Pred Grad:  0.060, New P: 0.756
-Original Grad: 0.116, -lr * Pred Grad:  0.008, New P: 1.548
iter 6 loss: 0.014
Actual params: [0.7561, 1.5484]
-Original Grad: 0.069, -lr * Pred Grad:  -0.056, New P: 0.700
-Original Grad: 0.105, -lr * Pred Grad:  0.132, New P: 1.680
iter 7 loss: 0.013
Actual params: [0.7   , 1.6799]
-Original Grad: 0.193, -lr * Pred Grad:  0.071, New P: 0.771
-Original Grad: 0.037, -lr * Pred Grad:  -0.075, New P: 1.605
iter 8 loss: 0.010
Actual params: [0.7712, 1.6052]
-Original Grad: 0.159, -lr * Pred Grad:  0.037, New P: 0.808
-Original Grad: 0.033, -lr * Pred Grad:  -0.032, New P: 1.573
iter 9 loss: 0.009
Actual params: [0.8082, 1.5733]
-Original Grad: 0.050, -lr * Pred Grad:  -0.012, New P: 0.796
-Original Grad: 0.073, -lr * Pred Grad:  0.053, New P: 1.626
iter 10 loss: 0.008
Actual params: [0.7964, 1.6258]
-Original Grad: 0.094, -lr * Pred Grad:  0.019, New P: 0.815
-Original Grad: 0.018, -lr * Pred Grad:  -0.013, New P: 1.613
iter 11 loss: 0.007
Actual params: [0.815 , 1.6129]
-Original Grad: 0.068, -lr * Pred Grad:  0.004, New P: 0.819
-Original Grad: 0.048, -lr * Pred Grad:  0.020, New P: 1.633
iter 12 loss: 0.006
Actual params: [0.8186, 1.6333]
-Original Grad: 0.063, -lr * Pred Grad:  0.009, New P: 0.828
-Original Grad: 0.023, -lr * Pred Grad:  0.002, New P: 1.635
iter 13 loss: 0.006
Actual params: [0.8279, 1.6351]
-Original Grad: 0.047, -lr * Pred Grad:  0.001, New P: 0.829
-Original Grad: 0.043, -lr * Pred Grad:  0.020, New P: 1.655
iter 14 loss: 0.005
Actual params: [0.829, 1.655]
-Original Grad: 0.077, -lr * Pred Grad:  0.011, New P: 0.840
-Original Grad: 0.020, -lr * Pred Grad:  -0.001, New P: 1.654
iter 15 loss: 0.005
Actual params: [0.8401, 1.6539]
-Original Grad: 0.058, -lr * Pred Grad:  0.007, New P: 0.847
-Original Grad: 0.025, -lr * Pred Grad:  0.007, New P: 1.661
iter 16 loss: 0.004
Actual params: [0.8467, 1.6607]
-Original Grad: 0.057, -lr * Pred Grad:  0.007, New P: 0.854
-Original Grad: 0.022, -lr * Pred Grad:  0.005, New P: 1.666
iter 17 loss: 0.004
Actual params: [0.8539, 1.6656]
-Original Grad: 0.067, -lr * Pred Grad:  0.008, New P: 0.862
-Original Grad: 0.025, -lr * Pred Grad:  0.005, New P: 1.670
iter 18 loss: 0.004
Actual params: [0.8623, 1.6703]
-Original Grad: 0.036, -lr * Pred Grad:  0.000, New P: 0.862
-Original Grad: 0.043, -lr * Pred Grad:  0.019, New P: 1.689
iter 19 loss: 0.004
Actual params: [0.8625, 1.6895]
-Original Grad: 0.055, -lr * Pred Grad:  0.007, New P: 0.869
-Original Grad: 0.021, -lr * Pred Grad:  0.004, New P: 1.694
iter 20 loss: 0.003
Actual params: [0.8692, 1.6938]
-Original Grad: 0.040, -lr * Pred Grad:  0.004, New P: 0.873
-Original Grad: 0.024, -lr * Pred Grad:  0.008, New P: 1.701
iter 21 loss: 0.003
Actual params: [0.8732, 1.7014]
-Original Grad: 0.042, -lr * Pred Grad:  0.006, New P: 0.879
-Original Grad: 0.007, -lr * Pred Grad:  -0.001, New P: 1.701
iter 22 loss: 0.003
Actual params: [0.8787, 1.7006]
-Original Grad: 0.019, -lr * Pred Grad:  0.002, New P: 0.880
-Original Grad: 0.012, -lr * Pred Grad:  0.004, New P: 1.705
iter 23 loss: 0.003
Actual params: [0.8804, 1.7049]
-Original Grad: 0.011, -lr * Pred Grad:  -0.000, New P: 0.880
-Original Grad: 0.017, -lr * Pred Grad:  0.007, New P: 1.712
iter 24 loss: 0.003
Actual params: [0.8803, 1.7122]
-Original Grad: 0.068, -lr * Pred Grad:  0.009, New P: 0.889
-Original Grad: -0.001, -lr * Pred Grad:  -0.005, New P: 1.707
iter 25 loss: 0.003
Actual params: [0.8889, 1.7068]
-Original Grad: 0.071, -lr * Pred Grad:  0.006, New P: 0.895
-Original Grad: 0.021, -lr * Pred Grad:  0.005, New P: 1.712
iter 26 loss: 0.003
Actual params: [0.8952, 1.712 ]
-Original Grad: 0.063, -lr * Pred Grad:  0.006, New P: 0.901
-Original Grad: 0.002, -lr * Pred Grad:  -0.003, New P: 1.709
iter 27 loss: 0.003
Actual params: [0.9008, 1.7094]
-Original Grad: -0.010, -lr * Pred Grad:  -0.002, New P: 0.899
-Original Grad: 0.010, -lr * Pred Grad:  0.005, New P: 1.715
iter 28 loss: 0.003
Actual params: [0.8993, 1.7147]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 0.899
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 1.715
iter 29 loss: 0.003
Actual params: [0.8994, 1.7147]
-Original Grad: 0.007, -lr * Pred Grad:  0.000, New P: 0.900
-Original Grad: 0.007, -lr * Pred Grad:  0.003, New P: 1.718
iter 30 loss: 0.003
Actual params: [0.8996, 1.7182]
-Original Grad: 0.084, -lr * Pred Grad:  0.006, New P: 0.906
-Original Grad: 0.012, -lr * Pred Grad:  0.002, New P: 1.720
Target params: [1.3344, 1.5708]
iter 0 loss: 0.267
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.016, -lr * Pred Grad:  -0.176, New P: -0.648
-Original Grad: -0.010, -lr * Pred Grad:  -0.113, New P: -0.110
iter 1 loss: 0.266
Actual params: [-0.6481, -0.1096]
-Original Grad: -0.002, -lr * Pred Grad:  -0.019, New P: -0.667
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.121
iter 2 loss: 0.266
Actual params: [-0.667 , -0.1207]
-Original Grad: -0.002, -lr * Pred Grad:  -0.031, New P: -0.698
-Original Grad: -0.002, -lr * Pred Grad:  -0.025, New P: -0.145
iter 3 loss: 0.266
Actual params: [-0.6977, -0.1453]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.712
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.155
iter 4 loss: 0.266
Actual params: [-0.7122, -0.1547]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.727
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.165
iter 5 loss: 0.266
Actual params: [-0.7266, -0.1645]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.743
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.176
iter 6 loss: 0.266
Actual params: [-0.7433, -0.1757]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.752
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.182
iter 7 loss: 0.266
Actual params: [-0.7523, -0.1822]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.769
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.194
iter 8 loss: 0.266
Actual params: [-0.7689, -0.1945]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -0.789
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.208
iter 9 loss: 0.266
Actual params: [-0.7892, -0.208 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.797
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.214
iter 10 loss: 0.266
Actual params: [-0.7973, -0.2138]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.810
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.223
iter 11 loss: 0.266
Actual params: [-0.8102, -0.223 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.826
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.234
iter 12 loss: 0.266
Actual params: [-0.8261, -0.2344]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.835
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.240
iter 13 loss: 0.266
Actual params: [-0.8347, -0.2402]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.852
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.252
iter 14 loss: 0.266
Actual params: [-0.8516, -0.2522]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -0.873
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.269
iter 15 loss: 0.266
Actual params: [-0.8732, -0.2687]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.884
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.276
iter 16 loss: 0.266
Actual params: [-0.8835, -0.2757]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.897
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.286
iter 17 loss: 0.266
Actual params: [-0.8974, -0.2857]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.910
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.293
iter 18 loss: 0.266
Actual params: [-0.9096, -0.2932]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -0.928
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.306
iter 19 loss: 0.266
Actual params: [-0.9279, -0.3063]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.936
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.311
iter 20 loss: 0.266
Actual params: [-0.9362, -0.3114]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -0.955
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.325
iter 21 loss: 0.266
Actual params: [-0.9552, -0.3249]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -0.975
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.339
iter 22 loss: 0.266
Actual params: [-0.9748, -0.339 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.984
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.345
iter 23 loss: 0.266
Actual params: [-0.9844, -0.3452]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.994
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.351
iter 24 loss: 0.266
Actual params: [-0.9937, -0.3512]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.005
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.358
iter 25 loss: 0.266
Actual params: [-1.0048, -0.3583]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.017
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.366
iter 26 loss: 0.266
Actual params: [-1.0166, -0.3661]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.036
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.380
iter 27 loss: 0.266
Actual params: [-1.0365, -0.3799]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.052
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.391
iter 28 loss: 0.266
Actual params: [-1.0524, -0.3906]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.065
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.399
iter 29 loss: 0.266
Actual params: [-1.0646, -0.3988]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.076
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.406
iter 30 loss: 0.266
Actual params: [-1.0755, -0.4057]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.084
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.411
Target params: [1.3344, 1.5708]
iter 0 loss: 0.432
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.031, -lr * Pred Grad:  -0.329, New P: -0.801
-Original Grad: -0.003, -lr * Pred Grad:  -0.028, New P: -0.025
iter 1 loss: 0.430
Actual params: [-0.8009, -0.0245]
-Original Grad: -0.004, -lr * Pred Grad:  -0.043, New P: -0.844
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.042
iter 2 loss: 0.430
Actual params: [-0.8442, -0.042 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.031, New P: -0.875
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.055
iter 3 loss: 0.430
Actual params: [-0.8748, -0.0551]
-Original Grad: -0.002, -lr * Pred Grad:  -0.032, New P: -0.907
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.069
iter 4 loss: 0.430
Actual params: [-0.9066, -0.0687]
-Original Grad: -0.002, -lr * Pred Grad:  -0.026, New P: -0.933
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.080
iter 5 loss: 0.430
Actual params: [-0.9326, -0.0801]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.948
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.087
iter 6 loss: 0.430
Actual params: [-0.9482, -0.0872]
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -0.970
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.097
iter 7 loss: 0.430
Actual params: [-0.9705, -0.0975]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -0.992
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.108
iter 8 loss: 0.430
Actual params: [-0.9919, -0.1077]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -1.012
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.117
iter 9 loss: 0.430
Actual params: [-1.0123, -0.1169]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -1.032
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.126
iter 10 loss: 0.430
Actual params: [-1.0322, -0.1264]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -1.049
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.136
iter 11 loss: 0.430
Actual params: [-1.0494, -0.1361]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -1.067
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.146
iter 12 loss: 0.430
Actual params: [-1.0667, -0.1457]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.084
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.155
iter 13 loss: 0.430
Actual params: [-1.0837, -0.1549]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.101
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.165
iter 14 loss: 0.430
Actual params: [-1.1014, -0.1647]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.119
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.176
iter 15 loss: 0.430
Actual params: [-1.1191, -0.1756]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.136
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.187
iter 16 loss: 0.430
Actual params: [-1.1356, -0.1869]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.152
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.198
iter 17 loss: 0.430
Actual params: [-1.1516, -0.198 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.167
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.209
iter 18 loss: 0.430
Actual params: [-1.1667, -0.2088]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.182
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.220
iter 19 loss: 0.430
Actual params: [-1.182 , -0.2196]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.196
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.230
iter 20 loss: 0.430
Actual params: [-1.1962, -0.2298]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.209
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.239
iter 21 loss: 0.430
Actual params: [-1.2093, -0.2394]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.224
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.250
iter 22 loss: 0.430
Actual params: [-1.2236, -0.2497]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.237
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.260
iter 23 loss: 0.430
Actual params: [-1.2374, -0.2597]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.250
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.269
iter 24 loss: 0.430
Actual params: [-1.25 , -0.269]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.264
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.279
iter 25 loss: 0.430
Actual params: [-1.2635, -0.2789]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.276
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.288
iter 26 loss: 0.430
Actual params: [-1.2759, -0.288 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.288
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.297
iter 27 loss: 0.430
Actual params: [-1.2882, -0.2971]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.301
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.306
iter 28 loss: 0.430
Actual params: [-1.3008, -0.3064]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.313
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.315
iter 29 loss: 0.430
Actual params: [-1.3129, -0.3154]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.325
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.324
iter 30 loss: 0.429
Actual params: [-1.3247, -0.3241]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.337
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.333
Target params: [1.3344, 1.5708]
iter 0 loss: 0.292
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.007, -lr * Pred Grad:  0.081, New P: -0.392
-Original Grad: 0.019, -lr * Pred Grad:  0.204, New P: 0.207
iter 1 loss: 0.289
Actual params: [-0.3917,  0.2073]
-Original Grad: 0.054, -lr * Pred Grad:  0.546, New P: 0.154
-Original Grad: 0.037, -lr * Pred Grad:  0.366, New P: 0.574
iter 2 loss: 0.223
Actual params: [0.154 , 0.5737]
-Original Grad: 0.258, -lr * Pred Grad:  0.435, New P: 0.589
-Original Grad: 0.189, -lr * Pred Grad:  0.358, New P: 0.931
iter 3 loss: 0.096
Actual params: [0.5891, 0.9312]
-Original Grad: 0.263, -lr * Pred Grad:  0.078, New P: 0.667
-Original Grad: 0.266, -lr * Pred Grad:  0.230, New P: 1.161
iter 4 loss: 0.052
Actual params: [0.667 , 1.1611]
-Original Grad: 0.266, -lr * Pred Grad:  0.218, New P: 0.885
-Original Grad: 0.122, -lr * Pred Grad:  -0.145, New P: 1.016
iter 5 loss: 0.041
Actual params: [0.8847, 1.0158]
-Original Grad: -0.003, -lr * Pred Grad:  -0.132, New P: 0.752
-Original Grad: 0.278, -lr * Pred Grad:  0.211, New P: 1.227
iter 6 loss: 0.032
Actual params: [0.7524, 1.2267]
-Original Grad: 0.127, -lr * Pred Grad:  0.066, New P: 0.818
-Original Grad: 0.052, -lr * Pred Grad:  -0.015, New P: 1.212
iter 7 loss: 0.026
Actual params: [0.818 , 1.2117]
-Original Grad: 0.023, -lr * Pred Grad:  -0.031, New P: 0.787
-Original Grad: 0.152, -lr * Pred Grad:  0.072, New P: 1.283
iter 8 loss: 0.024
Actual params: [0.7868, 1.2833]
-Original Grad: 0.076, -lr * Pred Grad:  0.019, New P: 0.806
-Original Grad: 0.082, -lr * Pred Grad:  0.020, New P: 1.303
iter 9 loss: 0.020
Actual params: [0.8062, 1.3033]
-Original Grad: 0.094, -lr * Pred Grad:  0.037, New P: 0.843
-Original Grad: 0.021, -lr * Pred Grad:  -0.009, New P: 1.295
iter 10 loss: 0.019
Actual params: [0.8427, 1.2947]
-Original Grad: 0.037, -lr * Pred Grad:  0.004, New P: 0.847
-Original Grad: 0.073, -lr * Pred Grad:  0.024, New P: 1.318
iter 11 loss: 0.017
Actual params: [0.8468, 1.3182]
-Original Grad: 0.030, -lr * Pred Grad:  -0.001, New P: 0.846
-Original Grad: 0.099, -lr * Pred Grad:  0.030, New P: 1.348
iter 12 loss: 0.016
Actual params: [0.8462, 1.3481]
-Original Grad: 0.023, -lr * Pred Grad:  0.003, New P: 0.850
-Original Grad: 0.060, -lr * Pred Grad:  0.016, New P: 1.365
iter 13 loss: 0.015
Actual params: [0.8496, 1.3645]
-Original Grad: 0.009, -lr * Pred Grad:  -0.007, New P: 0.843
-Original Grad: 0.093, -lr * Pred Grad:  0.025, New P: 1.390
iter 14 loss: 0.014
Actual params: [0.8428, 1.3898]
-Original Grad: 0.125, -lr * Pred Grad:  0.030, New P: 0.873
-Original Grad: 0.046, -lr * Pred Grad:  0.003, New P: 1.393
iter 15 loss: 0.014
Actual params: [0.8729, 1.3931]
-Original Grad: 0.074, -lr * Pred Grad:  0.015, New P: 0.888
-Original Grad: 0.008, -lr * Pred Grad:  -0.003, New P: 1.390
iter 16 loss: 0.014
Actual params: [0.8882, 1.3899]
-Original Grad: 0.029, -lr * Pred Grad:  0.003, New P: 0.891
-Original Grad: 0.052, -lr * Pred Grad:  0.013, New P: 1.403
iter 17 loss: 0.014
Actual params: [0.891 , 1.4027]
-Original Grad: -0.027, -lr * Pred Grad:  -0.006, New P: 0.885
-Original Grad: 0.008, -lr * Pred Grad:  0.004, New P: 1.407
iter 18 loss: 0.014
Actual params: [0.8855, 1.4069]
-Original Grad: 0.099, -lr * Pred Grad:  0.015, New P: 0.900
-Original Grad: 0.004, -lr * Pred Grad:  -0.006, New P: 1.401
iter 19 loss: 0.014
Actual params: [0.9005, 1.4009]
-Original Grad: 0.113, -lr * Pred Grad:  0.014, New P: 0.914
-Original Grad: 0.009, -lr * Pred Grad:  -0.005, New P: 1.396
iter 20 loss: 0.014
Actual params: [0.9145, 1.3958]
-Original Grad: -0.011, -lr * Pred Grad:  -0.004, New P: 0.911
-Original Grad: 0.034, -lr * Pred Grad:  0.011, New P: 1.407
iter 21 loss: 0.014
Actual params: [0.9106, 1.407 ]
-Original Grad: -0.111, -lr * Pred Grad:  -0.011, New P: 0.900
-Original Grad: -0.036, -lr * Pred Grad:  -0.003, New P: 1.404
iter 22 loss: 0.014
Actual params: [0.8997, 1.4038]
-Original Grad: -0.052, -lr * Pred Grad:  -0.001, New P: 0.898
-Original Grad: -0.042, -lr * Pred Grad:  -0.011, New P: 1.393
iter 23 loss: 0.014
Actual params: [0.8985, 1.3932]
-Original Grad: 0.088, -lr * Pred Grad:  0.004, New P: 0.903
-Original Grad: 0.040, -lr * Pred Grad:  0.007, New P: 1.400
iter 24 loss: 0.014
Actual params: [0.9028, 1.4002]
-Original Grad: 0.103, -lr * Pred Grad:  0.006, New P: 0.909
-Original Grad: 0.015, -lr * Pred Grad:  -0.002, New P: 1.398
iter 25 loss: 0.014
Actual params: [0.9086, 1.398 ]
-Original Grad: 0.020, -lr * Pred Grad:  -0.001, New P: 0.907
-Original Grad: 0.039, -lr * Pred Grad:  0.011, New P: 1.409
iter 26 loss: 0.014
Actual params: [0.9073, 1.4094]
-Original Grad: 0.029, -lr * Pred Grad:  0.003, New P: 0.911
-Original Grad: -0.013, -lr * Pred Grad:  -0.007, New P: 1.403
iter 27 loss: 0.014
Actual params: [0.9106, 1.4028]
-Original Grad: 0.108, -lr * Pred Grad:  0.008, New P: 0.919
-Original Grad: -0.019, -lr * Pred Grad:  -0.015, New P: 1.388
iter 28 loss: 0.014
Actual params: [0.919 , 1.3876]
-Original Grad: -0.046, -lr * Pred Grad:  -0.006, New P: 0.913
-Original Grad: 0.038, -lr * Pred Grad:  0.017, New P: 1.404
iter 29 loss: 0.014
Actual params: [0.9132, 1.4043]
-Original Grad: 0.020, -lr * Pred Grad:  -0.000, New P: 0.913
-Original Grad: 0.021, -lr * Pred Grad:  0.006, New P: 1.410
iter 30 loss: 0.014
Actual params: [0.9131, 1.41  ]
-Original Grad: 0.026, -lr * Pred Grad:  0.004, New P: 0.917
-Original Grad: -0.033, -lr * Pred Grad:  -0.013, New P: 1.397
Target params: [1.3344, 1.5708]
iter 0 loss: 0.062
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.058, -lr * Pred Grad:  -0.291, New P: -0.763
-Original Grad: 0.052, -lr * Pred Grad:  0.257, New P: 0.260
iter 1 loss: 0.055
Actual params: [-0.7632,  0.2601]
-Original Grad: -0.006, -lr * Pred Grad:  -0.035, New P: -0.798
-Original Grad: 0.005, -lr * Pred Grad:  0.028, New P: 0.288
iter 2 loss: 0.055
Actual params: [-0.798 ,  0.2882]
-Original Grad: -0.004, -lr * Pred Grad:  -0.025, New P: -0.823
-Original Grad: 0.003, -lr * Pred Grad:  0.020, New P: 0.308
iter 3 loss: 0.055
Actual params: [-0.8227,  0.3085]
-Original Grad: -0.003, -lr * Pred Grad:  -0.019, New P: -0.842
-Original Grad: 0.002, -lr * Pred Grad:  0.014, New P: 0.322
iter 4 loss: 0.055
Actual params: [-0.8419,  0.3224]
-Original Grad: -0.002, -lr * Pred Grad:  -0.017, New P: -0.859
-Original Grad: 0.002, -lr * Pred Grad:  0.013, New P: 0.336
iter 5 loss: 0.055
Actual params: [-0.859 ,  0.3356]
-Original Grad: -0.002, -lr * Pred Grad:  -0.016, New P: -0.875
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.347
iter 6 loss: 0.055
Actual params: [-0.8753,  0.3469]
-Original Grad: -0.002, -lr * Pred Grad:  -0.021, New P: -0.897
-Original Grad: 0.002, -lr * Pred Grad:  0.013, New P: 0.360
iter 7 loss: 0.055
Actual params: [-0.8966,  0.3599]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.912
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.368
iter 8 loss: 0.054
Actual params: [-0.9121,  0.3685]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.927
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: 0.377
iter 9 loss: 0.054
Actual params: [-0.9271,  0.3767]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.944
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: 0.384
iter 10 loss: 0.054
Actual params: [-0.9444,  0.3845]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.958
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.392
iter 11 loss: 0.054
Actual params: [-0.9583,  0.3919]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.972
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: 0.400
iter 12 loss: 0.054
Actual params: [-0.9723,  0.3996]
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.982
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.406
iter 13 loss: 0.054
Actual params: [-0.9816,  0.4063]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.996
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.413
iter 14 loss: 0.054
Actual params: [-0.9963,  0.413 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -1.013
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.420
iter 15 loss: 0.054
Actual params: [-1.0127,  0.4196]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -1.027
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.427
iter 16 loss: 0.054
Actual params: [-1.0272,  0.4266]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.041
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.434
iter 17 loss: 0.054
Actual params: [-1.0414,  0.4338]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.057
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.440
iter 18 loss: 0.054
Actual params: [-1.0572,  0.44  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.071
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.446
iter 19 loss: 0.054
Actual params: [-1.0714,  0.4463]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.089
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.453
iter 20 loss: 0.054
Actual params: [-1.089 ,  0.4535]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.106
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.460
iter 21 loss: 0.054
Actual params: [-1.1065,  0.4602]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.121
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.466
iter 22 loss: 0.054
Actual params: [-1.1209,  0.4664]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.137
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.473
iter 23 loss: 0.054
Actual params: [-1.137 ,  0.4734]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.152
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.479
iter 24 loss: 0.054
Actual params: [-1.1521,  0.4793]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.170
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.486
iter 25 loss: 0.054
Actual params: [-1.1704,  0.4862]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.190
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.492
iter 26 loss: 0.054
Actual params: [-1.1898,  0.4925]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.203
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.497
iter 27 loss: 0.054
Actual params: [-1.2029,  0.4967]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.221
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.503
iter 28 loss: 0.054
Actual params: [-1.2214,  0.5026]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.232
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.506
iter 29 loss: 0.054
Actual params: [-1.232 ,  0.5064]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.248
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.512
iter 30 loss: 0.054
Actual params: [-1.248 ,  0.5116]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.268
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.517
Target params: [1.3344, 1.5708]
iter 0 loss: 0.481
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.021, -lr * Pred Grad:  0.228, New P: -0.244
-Original Grad: -0.006, -lr * Pred Grad:  -0.069, New P: -0.065
iter 1 loss: 0.472
Actual params: [-0.2439, -0.0652]
-Original Grad: 0.102, -lr * Pred Grad:  0.658, New P: 0.414
-Original Grad: -0.021, -lr * Pred Grad:  -0.132, New P: -0.197
iter 2 loss: 0.387
Actual params: [ 0.4141, -0.1967]
-Original Grad: 0.215, -lr * Pred Grad:  0.207, New P: 0.621
-Original Grad: 0.303, -lr * Pred Grad:  0.478, New P: 0.282
iter 3 loss: 0.232
Actual params: [0.6214, 0.2817]
-Original Grad: 0.451, -lr * Pred Grad:  0.361, New P: 0.982
-Original Grad: 0.352, -lr * Pred Grad:  0.023, New P: 0.305
iter 4 loss: 0.187
Actual params: [0.9822, 0.3048]
-Original Grad: -0.211, -lr * Pred Grad:  -0.241, New P: 0.741
-Original Grad: 0.509, -lr * Pred Grad:  0.298, New P: 0.603
iter 5 loss: 0.128
Actual params: [0.7409, 0.6032]
-Original Grad: 0.365, -lr * Pred Grad:  0.103, New P: 0.844
-Original Grad: 0.406, -lr * Pred Grad:  0.113, New P: 0.716
iter 6 loss: 0.091
Actual params: [0.8435, 0.7161]
-Original Grad: 0.088, -lr * Pred Grad:  -0.024, New P: 0.820
-Original Grad: 0.398, -lr * Pred Grad:  0.126, New P: 0.842
iter 7 loss: 0.071
Actual params: [0.8198, 0.8419]
-Original Grad: 0.210, -lr * Pred Grad:  0.030, New P: 0.850
-Original Grad: 0.316, -lr * Pred Grad:  0.069, New P: 0.911
iter 8 loss: 0.059
Actual params: [0.85  , 0.9111]
-Original Grad: 0.039, -lr * Pred Grad:  -0.024, New P: 0.826
-Original Grad: 0.317, -lr * Pred Grad:  0.080, New P: 0.991
iter 9 loss: 0.051
Actual params: [0.8261, 0.9909]
-Original Grad: 0.216, -lr * Pred Grad:  0.031, New P: 0.857
-Original Grad: 0.271, -lr * Pred Grad:  0.045, New P: 1.036
iter 10 loss: 0.045
Actual params: [0.8573, 1.0364]
-Original Grad: 0.125, -lr * Pred Grad:  0.011, New P: 0.868
-Original Grad: 0.220, -lr * Pred Grad:  0.040, New P: 1.077
iter 11 loss: 0.042
Actual params: [0.8682, 1.0768]
-Original Grad: 0.031, -lr * Pred Grad:  -0.018, New P: 0.850
-Original Grad: 0.260, -lr * Pred Grad:  0.055, New P: 1.132
iter 12 loss: 0.037
Actual params: [0.8499, 1.1316]
-Original Grad: 0.145, -lr * Pred Grad:  0.018, New P: 0.868
-Original Grad: 0.222, -lr * Pred Grad:  0.033, New P: 1.165
iter 13 loss: 0.036
Actual params: [0.8678, 1.1648]
-Original Grad: -0.096, -lr * Pred Grad:  -0.033, New P: 0.835
-Original Grad: 0.066, -lr * Pred Grad:  0.025, New P: 1.190
iter 14 loss: 0.034
Actual params: [0.8353, 1.1902]
-Original Grad: 0.200, -lr * Pred Grad:  0.016, New P: 0.851
-Original Grad: 0.153, -lr * Pred Grad:  0.016, New P: 1.206
iter 15 loss: 0.034
Actual params: [0.851 , 1.2057]
-Original Grad: -0.291, -lr * Pred Grad:  -0.022, New P: 0.829
-Original Grad: -0.056, -lr * Pred Grad:  0.009, New P: 1.215
iter 16 loss: 0.033
Actual params: [0.8289, 1.215 ]
-Original Grad: 0.172, -lr * Pred Grad:  0.004, New P: 0.833
-Original Grad: 0.165, -lr * Pred Grad:  0.023, New P: 1.238
iter 17 loss: 0.032
Actual params: [0.8325, 1.2377]
-Original Grad: -0.339, -lr * Pred Grad:  -0.024, New P: 0.809
-Original Grad: -0.042, -lr * Pred Grad:  0.018, New P: 1.256
iter 18 loss: 0.031
Actual params: [0.8087, 1.2557]
-Original Grad: -0.075, -lr * Pred Grad:  -0.012, New P: 0.797
-Original Grad: 0.073, -lr * Pred Grad:  0.024, New P: 1.280
iter 19 loss: 0.030
Actual params: [0.797 , 1.2795]
-Original Grad: -0.170, -lr * Pred Grad:  -0.009, New P: 0.788
-Original Grad: -0.037, -lr * Pred Grad:  0.006, New P: 1.286
iter 20 loss: 0.029
Actual params: [0.7882, 1.2856]
-Original Grad: 0.075, -lr * Pred Grad:  -0.007, New P: 0.782
-Original Grad: 0.124, -lr * Pred Grad:  0.026, New P: 1.312
iter 21 loss: 0.028
Actual params: [0.7817, 1.3118]
-Original Grad: 0.135, -lr * Pred Grad:  0.000, New P: 0.782
-Original Grad: 0.086, -lr * Pred Grad:  0.011, New P: 1.323
iter 22 loss: 0.028
Actual params: [0.7819, 1.3228]
-Original Grad: 0.133, -lr * Pred Grad:  -0.004, New P: 0.778
-Original Grad: 0.118, -lr * Pred Grad:  0.021, New P: 1.344
iter 23 loss: 0.027
Actual params: [0.7783, 1.3437]
-Original Grad: -0.372, -lr * Pred Grad:  -0.016, New P: 0.762
-Original Grad: -0.073, -lr * Pred Grad:  0.019, New P: 1.363
iter 24 loss: 0.027
Actual params: [0.762 , 1.3632]
-Original Grad: -0.009, -lr * Pred Grad:  -0.006, New P: 0.756
-Original Grad: 0.045, -lr * Pred Grad:  0.017, New P: 1.380
iter 25 loss: 0.026
Actual params: [0.7557, 1.3803]
-Original Grad: 0.146, -lr * Pred Grad:  -0.002, New P: 0.754
-Original Grad: 0.093, -lr * Pred Grad:  0.015, New P: 1.395
iter 26 loss: 0.026
Actual params: [0.7537, 1.3952]
-Original Grad: 0.087, -lr * Pred Grad:  0.001, New P: 0.754
-Original Grad: 0.041, -lr * Pred Grad:  0.004, New P: 1.399
iter 27 loss: 0.026
Actual params: [0.7542, 1.3989]
-Original Grad: -0.591, -lr * Pred Grad:  -0.018, New P: 0.736
-Original Grad: -0.165, -lr * Pred Grad:  0.020, New P: 1.419
iter 28 loss: 0.026
Actual params: [0.7358, 1.4189]
-Original Grad: 0.591, -lr * Pred Grad:  0.007, New P: 0.743
-Original Grad: 0.237, -lr * Pred Grad:  0.011, New P: 1.430
iter 29 loss: 0.026
Actual params: [0.7426, 1.4296]
-Original Grad: -0.056, -lr * Pred Grad:  -0.005, New P: 0.738
-Original Grad: 0.003, -lr * Pred Grad:  0.011, New P: 1.441
iter 30 loss: 0.025
Actual params: [0.7377, 1.4407]
-Original Grad: -0.164, -lr * Pred Grad:  0.004, New P: 0.742
-Original Grad: -0.096, -lr * Pred Grad:  -0.019, New P: 1.422
Target params: [1.3344, 1.5708]
iter 0 loss: 0.212
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.049, -lr * Pred Grad:  0.469, New P: -0.004
-Original Grad: -0.005, -lr * Pred Grad:  -0.051, New P: -0.047
iter 1 loss: 0.166
Actual params: [-0.0037, -0.0475]
-Original Grad: 0.148, -lr * Pred Grad:  0.452, New P: 0.449
-Original Grad: 0.065, -lr * Pred Grad:  0.219, New P: 0.172
iter 2 loss: 0.150
Actual params: [0.4486, 0.1716]
-Original Grad: -0.751, -lr * Pred Grad:  -0.048, New P: 0.400
-Original Grad: 0.510, -lr * Pred Grad:  0.342, New P: 0.514
iter 3 loss: 0.082
Actual params: [0.4004, 0.5138]
-Original Grad: -0.457, -lr * Pred Grad:  -0.019, New P: 0.382
-Original Grad: 0.223, -lr * Pred Grad:  0.089, New P: 0.603
iter 4 loss: 0.070
Actual params: [0.3816, 0.6026]
-Original Grad: -0.382, -lr * Pred Grad:  -0.013, New P: 0.368
-Original Grad: 0.143, -lr * Pred Grad:  0.037, New P: 0.640
iter 5 loss: 0.066
Actual params: [0.3682, 0.6396]
-Original Grad: 0.010, -lr * Pred Grad:  0.021, New P: 0.389
-Original Grad: 0.055, -lr * Pred Grad:  0.071, New P: 0.710
iter 6 loss: 0.064
Actual params: [0.3889, 0.7102]
-Original Grad: 0.160, -lr * Pred Grad:  0.031, New P: 0.420
-Original Grad: 0.018, -lr * Pred Grad:  0.079, New P: 0.789
iter 7 loss: 0.059
Actual params: [0.4203, 0.7893]
-Original Grad: -0.014, -lr * Pred Grad:  0.035, New P: 0.455
-Original Grad: 0.117, -lr * Pred Grad:  0.118, New P: 0.908
iter 8 loss: 0.050
Actual params: [0.4553, 0.9076]
-Original Grad: 0.292, -lr * Pred Grad:  0.036, New P: 0.491
-Original Grad: -0.031, -lr * Pred Grad:  0.061, New P: 0.968
iter 9 loss: 0.045
Actual params: [0.4912, 0.9682]
-Original Grad: 0.334, -lr * Pred Grad:  0.036, New P: 0.528
-Original Grad: -0.041, -lr * Pred Grad:  0.059, New P: 1.027
iter 10 loss: 0.040
Actual params: [0.5276, 1.0268]
-Original Grad: 0.228, -lr * Pred Grad:  0.022, New P: 0.549
-Original Grad: -0.042, -lr * Pred Grad:  0.025, New P: 1.052
iter 11 loss: 0.038
Actual params: [0.5492, 1.052 ]
-Original Grad: 0.194, -lr * Pred Grad:  0.031, New P: 0.580
-Original Grad: 0.005, -lr * Pred Grad:  0.063, New P: 1.115
iter 12 loss: 0.034
Actual params: [0.5798, 1.1152]
-Original Grad: 0.157, -lr * Pred Grad:  0.021, New P: 0.600
-Original Grad: -0.010, -lr * Pred Grad:  0.033, New P: 1.149
iter 13 loss: 0.032
Actual params: [0.6005, 1.1485]
-Original Grad: 0.202, -lr * Pred Grad:  0.017, New P: 0.617
-Original Grad: -0.051, -lr * Pred Grad:  0.007, New P: 1.155
iter 14 loss: 0.031
Actual params: [0.6175, 1.155 ]
-Original Grad: 0.253, -lr * Pred Grad:  0.021, New P: 0.639
-Original Grad: -0.067, -lr * Pred Grad:  0.006, New P: 1.161
iter 15 loss: 0.029
Actual params: [0.6386, 1.161 ]
-Original Grad: 0.263, -lr * Pred Grad:  0.024, New P: 0.663
-Original Grad: -0.063, -lr * Pred Grad:  0.015, New P: 1.176
iter 16 loss: 0.027
Actual params: [0.663 , 1.1759]
-Original Grad: 0.221, -lr * Pred Grad:  0.021, New P: 0.684
-Original Grad: -0.054, -lr * Pred Grad:  0.011, New P: 1.187
iter 17 loss: 0.026
Actual params: [0.6835, 1.1874]
-Original Grad: 0.219, -lr * Pred Grad:  0.021, New P: 0.705
-Original Grad: -0.052, -lr * Pred Grad:  0.014, New P: 1.201
iter 18 loss: 0.024
Actual params: [0.7048, 1.2013]
-Original Grad: 0.303, -lr * Pred Grad:  0.021, New P: 0.725
-Original Grad: -0.091, -lr * Pred Grad:  -0.009, New P: 1.192
iter 19 loss: 0.023
Actual params: [0.7254, 1.1924]
-Original Grad: 0.109, -lr * Pred Grad:  0.026, New P: 0.752
-Original Grad: 0.032, -lr * Pred Grad:  0.060, New P: 1.252
iter 20 loss: 0.020
Actual params: [0.7516, 1.2519]
-Original Grad: 0.249, -lr * Pred Grad:  0.018, New P: 0.769
-Original Grad: -0.080, -lr * Pred Grad:  -0.006, New P: 1.246
iter 21 loss: 0.020
Actual params: [0.7692, 1.2461]
-Original Grad: 0.253, -lr * Pred Grad:  0.023, New P: 0.792
-Original Grad: -0.057, -lr * Pred Grad:  0.015, New P: 1.261
iter 22 loss: 0.018
Actual params: [0.7923, 1.2612]
-Original Grad: 0.340, -lr * Pred Grad:  0.033, New P: 0.826
-Original Grad: -0.043, -lr * Pred Grad:  0.041, New P: 1.303
iter 23 loss: 0.017
Actual params: [0.8256, 1.3026]
-Original Grad: 0.149, -lr * Pred Grad:  0.005, New P: 0.831
-Original Grad: -0.060, -lr * Pred Grad:  -0.020, New P: 1.283
iter 24 loss: 0.017
Actual params: [0.8308, 1.2829]
-Original Grad: 0.134, -lr * Pred Grad:  0.000, New P: 0.831
-Original Grad: -0.073, -lr * Pred Grad:  -0.035, New P: 1.248
iter 25 loss: 0.017
Actual params: [0.8312, 1.2481]
-Original Grad: 0.190, -lr * Pred Grad:  0.008, New P: 0.839
-Original Grad: -0.076, -lr * Pred Grad:  -0.023, New P: 1.225
iter 26 loss: 0.018
Actual params: [0.8387, 1.2247]
-Original Grad: 0.215, -lr * Pred Grad:  0.014, New P: 0.853
-Original Grad: -0.059, -lr * Pred Grad:  -0.003, New P: 1.221
iter 27 loss: 0.018
Actual params: [0.8526, 1.2214]
-Original Grad: 0.063, -lr * Pred Grad:  0.004, New P: 0.856
-Original Grad: -0.019, -lr * Pred Grad:  -0.003, New P: 1.219
iter 28 loss: 0.018
Actual params: [0.8563, 1.2188]
-Original Grad: 0.087, -lr * Pred Grad:  0.008, New P: 0.864
-Original Grad: -0.016, -lr * Pred Grad:  0.007, New P: 1.225
iter 29 loss: 0.018
Actual params: [0.8642, 1.2254]
-Original Grad: 0.026, -lr * Pred Grad:  -0.001, New P: 0.863
-Original Grad: -0.020, -lr * Pred Grad:  -0.012, New P: 1.213
iter 30 loss: 0.018
Actual params: [0.8629, 1.2132]
-Original Grad: -0.013, -lr * Pred Grad:  0.010, New P: 0.873
-Original Grad: 0.055, -lr * Pred Grad:  0.041, New P: 1.254
Target params: [1.3344, 1.5708]
iter 0 loss: 0.057
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.017, -lr * Pred Grad:  -0.186, New P: -0.658
-Original Grad: 0.012, -lr * Pred Grad:  0.130, New P: 0.134
iter 1 loss: 0.057
Actual params: [-0.6579,  0.1339]
-Original Grad: -0.002, -lr * Pred Grad:  -0.027, New P: -0.685
-Original Grad: 0.002, -lr * Pred Grad:  0.022, New P: 0.156
iter 2 loss: 0.057
Actual params: [-0.6849,  0.1558]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.703
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.171
iter 3 loss: 0.057
Actual params: [-0.7032,  0.1709]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.721
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.186
iter 4 loss: 0.057
Actual params: [-0.721 ,  0.1859]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.736
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.199
iter 5 loss: 0.057
Actual params: [-0.7361,  0.1989]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.746
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.208
iter 6 loss: 0.057
Actual params: [-0.7463,  0.2075]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.759
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.219
iter 7 loss: 0.057
Actual params: [-0.7594,  0.219 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.768
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.227
iter 8 loss: 0.057
Actual params: [-0.7683,  0.227 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.775
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.233
iter 9 loss: 0.057
Actual params: [-0.7746,  0.2327]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.784
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.241
iter 10 loss: 0.057
Actual params: [-0.7836,  0.241 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.794
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.251
iter 11 loss: 0.057
Actual params: [-0.7944,  0.2509]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.805
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.261
iter 12 loss: 0.057
Actual params: [-0.8046,  0.2606]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.815
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.270
iter 13 loss: 0.057
Actual params: [-0.8146,  0.2701]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.822
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.278
iter 14 loss: 0.057
Actual params: [-0.8222,  0.2779]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.831
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.287
iter 15 loss: 0.057
Actual params: [-0.8314,  0.2872]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.840
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.296
iter 16 loss: 0.057
Actual params: [-0.8401,  0.296 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.849
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.305
iter 17 loss: 0.057
Actual params: [-0.8487,  0.3051]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.857
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.314
iter 18 loss: 0.057
Actual params: [-0.8571,  0.314 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.864
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.321
iter 19 loss: 0.057
Actual params: [-0.8638,  0.3214]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.871
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.330
iter 20 loss: 0.057
Actual params: [-0.8714,  0.3303]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.877
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.337
iter 21 loss: 0.057
Actual params: [-0.8769,  0.3369]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.885
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.346
iter 22 loss: 0.057
Actual params: [-0.8848,  0.3465]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.891
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.355
iter 23 loss: 0.057
Actual params: [-0.8914,  0.355 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.898
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.364
iter 24 loss: 0.057
Actual params: [-0.8978,  0.3637]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.904
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.373
iter 25 loss: 0.057
Actual params: [-0.9042,  0.3727]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.911
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.382
iter 26 loss: 0.057
Actual params: [-0.9106,  0.382 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.916
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.390
iter 27 loss: 0.057
Actual params: [-0.916 ,  0.3902]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.921
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.398
iter 28 loss: 0.057
Actual params: [-0.9207,  0.3981]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.926
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.408
iter 29 loss: 0.057
Actual params: [-0.9261,  0.4075]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.931
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.417
iter 30 loss: 0.057
Actual params: [-0.9314,  0.417 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.935
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.425
Target params: [1.3344, 1.5708]
iter 0 loss: 0.432
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.031, -lr * Pred Grad:  -0.328, New P: -0.800
-Original Grad: -0.004, -lr * Pred Grad:  -0.041, New P: -0.038
iter 1 loss: 0.430
Actual params: [-0.8001, -0.0378]
-Original Grad: -0.003, -lr * Pred Grad:  -0.040, New P: -0.840
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.055
iter 2 loss: 0.430
Actual params: [-0.8398, -0.055 ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.035, New P: -0.875
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.071
iter 3 loss: 0.430
Actual params: [-0.8745, -0.0705]
-Original Grad: -0.002, -lr * Pred Grad:  -0.030, New P: -0.905
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.084
iter 4 loss: 0.430
Actual params: [-0.905 , -0.0837]
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -0.929
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.095
iter 5 loss: 0.430
Actual params: [-0.9294, -0.0948]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -0.950
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.105
iter 6 loss: 0.430
Actual params: [-0.9504, -0.1053]
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -0.972
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.116
iter 7 loss: 0.430
Actual params: [-0.9722, -0.1159]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -0.994
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.127
iter 8 loss: 0.430
Actual params: [-0.9936, -0.1271]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -1.012
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.136
iter 9 loss: 0.430
Actual params: [-1.0119, -0.1364]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -1.031
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.146
iter 10 loss: 0.430
Actual params: [-1.0306, -0.1458]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -1.049
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.156
iter 11 loss: 0.430
Actual params: [-1.0493, -0.1559]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -1.068
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.166
iter 12 loss: 0.430
Actual params: [-1.0676, -0.1658]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.084
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.176
iter 13 loss: 0.430
Actual params: [-1.084 , -0.1759]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.100
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.187
iter 14 loss: 0.430
Actual params: [-1.1004, -0.1869]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.115
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.197
iter 15 loss: 0.430
Actual params: [-1.1153, -0.1973]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.131
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.209
iter 16 loss: 0.430
Actual params: [-1.131 , -0.2086]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.145
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.219
iter 17 loss: 0.430
Actual params: [-1.1448, -0.2186]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.159
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.229
iter 18 loss: 0.430
Actual params: [-1.1595, -0.2293]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.174
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.240
iter 19 loss: 0.430
Actual params: [-1.1742, -0.2401]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.188
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.250
iter 20 loss: 0.430
Actual params: [-1.1876, -0.25  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.200
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.259
iter 21 loss: 0.430
Actual params: [-1.2001, -0.2592]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.212
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.268
iter 22 loss: 0.430
Actual params: [-1.2125, -0.2685]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.226
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.278
iter 23 loss: 0.430
Actual params: [-1.2256, -0.2782]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.238
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.288
iter 24 loss: 0.430
Actual params: [-1.2383, -0.2875]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.251
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.297
iter 25 loss: 0.430
Actual params: [-1.2509, -0.2968]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.263
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.306
iter 26 loss: 0.430
Actual params: [-1.2629, -0.3057]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.275
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.315
iter 27 loss: 0.430
Actual params: [-1.2751, -0.3148]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.287
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.324
iter 28 loss: 0.430
Actual params: [-1.2867, -0.3237]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.300
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.333
iter 29 loss: 0.430
Actual params: [-1.2996, -0.3332]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.311
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.342
iter 30 loss: 0.429
Actual params: [-1.3114, -0.3421]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.323
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.351
Target params: [1.3344, 1.5708]
iter 0 loss: 0.008
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.022, -lr * Pred Grad:  0.159, New P: -0.313
-Original Grad: -0.006, -lr * Pred Grad:  -0.046, New P: -0.042
iter 1 loss: 0.005
Actual params: [-0.313 , -0.0421]
-Original Grad: 0.051, -lr * Pred Grad:  0.079, New P: -0.234
-Original Grad: -0.013, -lr * Pred Grad:  -0.030, New P: -0.072
iter 2 loss: 0.003
Actual params: [-0.2336, -0.0725]
-Original Grad: 0.020, -lr * Pred Grad:  0.020, New P: -0.214
-Original Grad: -0.006, -lr * Pred Grad:  -0.023, New P: -0.095
iter 3 loss: 0.002
Actual params: [-0.2136, -0.0952]
-Original Grad: 0.008, -lr * Pred Grad:  0.012, New P: -0.202
-Original Grad: -0.001, -lr * Pred Grad:  0.009, New P: -0.086
iter 4 loss: 0.002
Actual params: [-0.202 , -0.0862]
-Original Grad: 0.016, -lr * Pred Grad:  0.009, New P: -0.193
-Original Grad: -0.005, -lr * Pred Grad:  -0.026, New P: -0.112
iter 5 loss: 0.002
Actual params: [-0.1925, -0.1124]
-Original Grad: 0.006, -lr * Pred Grad:  0.012, New P: -0.181
-Original Grad: 0.001, -lr * Pred Grad:  0.027, New P: -0.085
iter 6 loss: 0.002
Actual params: [-0.1805, -0.0851]
-Original Grad: 0.005, -lr * Pred Grad:  -0.001, New P: -0.181
-Original Grad: -0.003, -lr * Pred Grad:  -0.028, New P: -0.113
iter 7 loss: 0.002
Actual params: [-0.1812, -0.1132]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -0.187
-Original Grad: -0.002, -lr * Pred Grad:  -0.030, New P: -0.143
iter 8 loss: 0.002
Actual params: [-0.1869, -0.1432]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.201
-Original Grad: -0.004, -lr * Pred Grad:  -0.065, New P: -0.208
iter 9 loss: 0.002
Actual params: [-0.2015, -0.2084]
-Original Grad: 0.007, -lr * Pred Grad:  0.015, New P: -0.186
-Original Grad: 0.001, -lr * Pred Grad:  0.036, New P: -0.172
iter 10 loss: 0.002
Actual params: [-0.186, -0.172]
-Original Grad: 0.008, -lr * Pred Grad:  0.010, New P: -0.176
-Original Grad: -0.002, -lr * Pred Grad:  -0.003, New P: -0.175
iter 11 loss: 0.002
Actual params: [-0.1761, -0.1746]
-Original Grad: 0.002, -lr * Pred Grad:  -0.001, New P: -0.177
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: -0.197
iter 12 loss: 0.002
Actual params: [-0.1773, -0.1968]
-Original Grad: 0.008, -lr * Pred Grad:  0.015, New P: -0.162
-Original Grad: 0.001, -lr * Pred Grad:  0.033, New P: -0.163
iter 13 loss: 0.002
Actual params: [-0.162 , -0.1633]
-Original Grad: 0.004, -lr * Pred Grad:  0.007, New P: -0.155
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: -0.152
iter 14 loss: 0.002
Actual params: [-0.1547, -0.1525]
-Original Grad: 0.005, -lr * Pred Grad:  0.007, New P: -0.147
-Original Grad: -0.000, -lr * Pred Grad:  0.003, New P: -0.149
iter 15 loss: 0.002
Actual params: [-0.1473, -0.149 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.004, New P: -0.143
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -0.149
iter 16 loss: 0.002
Actual params: [-0.1431, -0.1485]
-Original Grad: 0.001, -lr * Pred Grad:  -0.002, New P: -0.145
-Original Grad: -0.003, -lr * Pred Grad:  -0.041, New P: -0.189
iter 17 loss: 0.002
Actual params: [-0.1451, -0.1893]
-Original Grad: 0.004, -lr * Pred Grad:  0.007, New P: -0.138
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -0.197
iter 18 loss: 0.002
Actual params: [-0.1381, -0.1973]
-Original Grad: 0.003, -lr * Pred Grad:  0.005, New P: -0.133
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.198
iter 19 loss: 0.002
Actual params: [-0.1328, -0.1981]
-Original Grad: 0.003, -lr * Pred Grad:  0.007, New P: -0.126
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: -0.198
iter 20 loss: 0.002
Actual params: [-0.1255, -0.1979]
-Original Grad: 0.004, -lr * Pred Grad:  0.009, New P: -0.116
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: -0.189
iter 21 loss: 0.002
Actual params: [-0.1162, -0.1886]
-Original Grad: -0.005, -lr * Pred Grad:  -0.009, New P: -0.125
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -0.190
iter 22 loss: 0.002
Actual params: [-0.1252, -0.19  ]
-Original Grad: 0.003, -lr * Pred Grad:  0.006, New P: -0.119
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.201
iter 23 loss: 0.002
Actual params: [-0.1192, -0.201 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: -0.123
-Original Grad: -0.002, -lr * Pred Grad:  -0.026, New P: -0.227
iter 24 loss: 0.002
Actual params: [-0.1231, -0.2269]
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -0.124
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: -0.209
iter 25 loss: 0.002
Actual params: [-0.1244, -0.2087]
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: -0.128
-Original Grad: -0.002, -lr * Pred Grad:  -0.030, New P: -0.239
iter 26 loss: 0.002
Actual params: [-0.1282, -0.2391]
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: -0.127
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: -0.223
iter 27 loss: 0.002
Actual params: [-0.1268, -0.223 ]
-Original Grad: 0.004, -lr * Pred Grad:  0.006, New P: -0.121
-Original Grad: 0.001, -lr * Pred Grad:  0.004, New P: -0.219
iter 28 loss: 0.002
Actual params: [-0.1205, -0.219 ]
-Original Grad: -0.006, -lr * Pred Grad:  -0.009, New P: -0.129
-Original Grad: -0.001, -lr * Pred Grad:  -0.005, New P: -0.224
iter 29 loss: 0.002
Actual params: [-0.1294, -0.2236]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.127
-Original Grad: -0.004, -lr * Pred Grad:  -0.053, New P: -0.277
iter 30 loss: 0.002
Actual params: [-0.1265, -0.2769]
-Original Grad: 0.003, -lr * Pred Grad:  0.003, New P: -0.124
-Original Grad: 0.003, -lr * Pred Grad:  0.030, New P: -0.247
Target params: [1.3344, 1.5708]
iter 0 loss: 0.293
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.041, -lr * Pred Grad:  -0.396, New P: -0.868
-Original Grad: 0.041, -lr * Pred Grad:  0.395, New P: 0.399
iter 1 loss: 0.280
Actual params: [-0.8682,  0.3988]
-Original Grad: 0.027, -lr * Pred Grad:  0.304, New P: -0.564
-Original Grad: 0.044, -lr * Pred Grad:  0.459, New P: 0.858
iter 2 loss: 0.255
Actual params: [-0.5639,  0.8582]
-Original Grad: 0.076, -lr * Pred Grad:  0.629, New P: 0.065
-Original Grad: 0.057, -lr * Pred Grad:  0.451, New P: 1.310
iter 3 loss: 0.170
Actual params: [0.065 , 1.3096]
-Original Grad: 0.192, -lr * Pred Grad:  0.583, New P: 0.648
-Original Grad: 0.061, -lr * Pred Grad:  0.081, New P: 1.391
iter 4 loss: 0.040
Actual params: [0.6483, 1.3908]
-Original Grad: 0.274, -lr * Pred Grad:  0.096, New P: 0.744
-Original Grad: 0.166, -lr * Pred Grad:  0.163, New P: 1.554
iter 5 loss: 0.015
Actual params: [0.7442, 1.5537]
-Original Grad: 0.126, -lr * Pred Grad:  0.011, New P: 0.755
-Original Grad: 0.095, -lr * Pred Grad:  0.086, New P: 1.640
iter 6 loss: 0.010
Actual params: [0.755 , 1.6397]
-Original Grad: 0.109, -lr * Pred Grad:  0.031, New P: 0.786
-Original Grad: 0.056, -lr * Pred Grad:  0.005, New P: 1.645
iter 7 loss: 0.008
Actual params: [0.7858, 1.645 ]
-Original Grad: 0.111, -lr * Pred Grad:  0.037, New P: 0.823
-Original Grad: 0.025, -lr * Pred Grad:  -0.028, New P: 1.617
iter 8 loss: 0.007
Actual params: [0.8226, 1.6171]
-Original Grad: 0.087, -lr * Pred Grad:  0.016, New P: 0.838
-Original Grad: 0.031, -lr * Pred Grad:  0.004, New P: 1.621
iter 9 loss: 0.006
Actual params: [0.8383, 1.6208]
-Original Grad: 0.065, -lr * Pred Grad:  0.009, New P: 0.847
-Original Grad: 0.031, -lr * Pred Grad:  0.012, New P: 1.633
iter 10 loss: 0.005
Actual params: [0.8471, 1.6327]
-Original Grad: 0.055, -lr * Pred Grad:  0.001, New P: 0.848
-Original Grad: 0.055, -lr * Pred Grad:  0.029, New P: 1.661
iter 11 loss: 0.004
Actual params: [0.848 , 1.6614]
-Original Grad: 0.051, -lr * Pred Grad:  0.004, New P: 0.852
-Original Grad: 0.040, -lr * Pred Grad:  0.016, New P: 1.677
iter 12 loss: 0.004
Actual params: [0.8522, 1.6775]
-Original Grad: 0.077, -lr * Pred Grad:  0.014, New P: 0.866
-Original Grad: 0.024, -lr * Pred Grad:  0.000, New P: 1.678
iter 13 loss: 0.004
Actual params: [0.8658, 1.6777]
-Original Grad: 0.021, -lr * Pred Grad:  0.000, New P: 0.866
-Original Grad: 0.026, -lr * Pred Grad:  0.013, New P: 1.690
iter 14 loss: 0.003
Actual params: [0.8659, 1.6902]
-Original Grad: 0.029, -lr * Pred Grad:  0.004, New P: 0.870
-Original Grad: 0.017, -lr * Pred Grad:  0.006, New P: 1.696
iter 15 loss: 0.003
Actual params: [0.8697, 1.6958]
-Original Grad: 0.054, -lr * Pred Grad:  0.008, New P: 0.878
-Original Grad: 0.015, -lr * Pred Grad:  0.001, New P: 1.697
iter 16 loss: 0.003
Actual params: [0.8779, 1.6972]
-Original Grad: 0.075, -lr * Pred Grad:  0.009, New P: 0.887
-Original Grad: 0.015, -lr * Pred Grad:  0.002, New P: 1.699
iter 17 loss: 0.003
Actual params: [0.8868, 1.6988]
-Original Grad: 0.091, -lr * Pred Grad:  0.010, New P: 0.896
-Original Grad: 0.013, -lr * Pred Grad:  -0.000, New P: 1.699
iter 18 loss: 0.003
Actual params: [0.8964, 1.6986]
-Original Grad: 0.032, -lr * Pred Grad:  0.002, New P: 0.899
-Original Grad: 0.009, -lr * Pred Grad:  0.004, New P: 1.702
iter 19 loss: 0.003
Actual params: [0.8988, 1.7021]
-Original Grad: 0.047, -lr * Pred Grad:  0.003, New P: 0.902
-Original Grad: 0.019, -lr * Pred Grad:  0.009, New P: 1.711
iter 20 loss: 0.003
Actual params: [0.9021, 1.7111]
-Original Grad: 0.031, -lr * Pred Grad:  0.003, New P: 0.905
-Original Grad: 0.004, -lr * Pred Grad:  0.000, New P: 1.711
iter 21 loss: 0.003
Actual params: [0.905 , 1.7115]
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: 0.906
-Original Grad: -0.014, -lr * Pred Grad:  -0.008, New P: 1.703
iter 22 loss: 0.003
Actual params: [0.9061, 1.7035]
-Original Grad: 0.019, -lr * Pred Grad:  0.001, New P: 0.907
-Original Grad: 0.019, -lr * Pred Grad:  0.009, New P: 1.713
iter 23 loss: 0.003
Actual params: [0.907 , 1.7129]
-Original Grad: 0.013, -lr * Pred Grad:  0.003, New P: 0.910
-Original Grad: -0.012, -lr * Pred Grad:  -0.007, New P: 1.706
iter 24 loss: 0.003
Actual params: [0.9095, 1.7058]
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: 0.908
-Original Grad: 0.015, -lr * Pred Grad:  0.008, New P: 1.714
iter 25 loss: 0.003
Actual params: [0.9082, 1.7142]
-Original Grad: 0.011, -lr * Pred Grad:  0.001, New P: 0.909
-Original Grad: 0.004, -lr * Pred Grad:  0.001, New P: 1.716
iter 26 loss: 0.003
Actual params: [0.9094, 1.7156]
-Original Grad: -0.004, -lr * Pred Grad:  -0.000, New P: 0.909
-Original Grad: -0.001, -lr * Pred Grad:  -0.000, New P: 1.715
iter 27 loss: 0.003
Actual params: [0.9089, 1.7151]
-Original Grad: -0.012, -lr * Pred Grad:  -0.002, New P: 0.907
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: 1.717
iter 28 loss: 0.003
Actual params: [0.9071, 1.7168]
-Original Grad: -0.010, -lr * Pred Grad:  -0.002, New P: 0.905
-Original Grad: 0.005, -lr * Pred Grad:  0.004, New P: 1.721
iter 29 loss: 0.003
Actual params: [0.9053, 1.7209]
-Original Grad: -0.041, -lr * Pred Grad:  -0.005, New P: 0.900
-Original Grad: -0.004, -lr * Pred Grad:  0.001, New P: 1.722
iter 30 loss: 0.003
Actual params: [0.9001, 1.7216]
-Original Grad: 0.034, -lr * Pred Grad:  0.005, New P: 0.905
-Original Grad: -0.009, -lr * Pred Grad:  -0.008, New P: 1.714
Target params: [1.3344, 1.5708]
iter 0 loss: 0.103
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.047, -lr * Pred Grad:  -0.399, New P: -0.871
-Original Grad: 0.031, -lr * Pred Grad:  0.267, New P: 0.271
iter 1 loss: 0.097
Actual params: [-0.8712,  0.2706]
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.894
-Original Grad: 0.002, -lr * Pred Grad:  0.015, New P: 0.286
iter 2 loss: 0.097
Actual params: [-0.8938,  0.2861]
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: -0.916
-Original Grad: 0.002, -lr * Pred Grad:  0.017, New P: 0.303
iter 3 loss: 0.097
Actual params: [-0.916 ,  0.3034]
-Original Grad: -0.002, -lr * Pred Grad:  -0.020, New P: -0.936
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.319
iter 4 loss: 0.097
Actual params: [-0.9358,  0.3195]
-Original Grad: -0.002, -lr * Pred Grad:  -0.020, New P: -0.956
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.335
iter 5 loss: 0.097
Actual params: [-0.9561,  0.3353]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -0.976
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.351
iter 6 loss: 0.097
Actual params: [-0.9758,  0.3511]
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -0.998
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 0.368
iter 7 loss: 0.097
Actual params: [-0.998 ,  0.3683]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -1.014
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.382
iter 8 loss: 0.097
Actual params: [-1.0144,  0.382 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -1.030
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.395
iter 9 loss: 0.097
Actual params: [-1.0296,  0.3948]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -1.042
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.406
iter 10 loss: 0.097
Actual params: [-1.0422,  0.4057]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -1.058
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.419
iter 11 loss: 0.097
Actual params: [-1.0583,  0.4195]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -1.072
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.431
iter 12 loss: 0.097
Actual params: [-1.072 ,  0.4307]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.086
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.443
iter 13 loss: 0.097
Actual params: [-1.0857,  0.4428]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.099
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.455
iter 14 loss: 0.097
Actual params: [-1.0987,  0.4547]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.112
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.468
iter 15 loss: 0.097
Actual params: [-1.1117,  0.4675]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.123
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.479
iter 16 loss: 0.097
Actual params: [-1.1229,  0.479 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.133
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.489
iter 17 loss: 0.097
Actual params: [-1.1327,  0.4893]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.143
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.501
iter 18 loss: 0.097
Actual params: [-1.143 ,  0.5008]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.153
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.512
iter 19 loss: 0.097
Actual params: [-1.1527,  0.5121]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.161
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.522
iter 20 loss: 0.097
Actual params: [-1.1614,  0.5223]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.170
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.533
iter 21 loss: 0.097
Actual params: [-1.1696,  0.5329]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.178
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.545
iter 22 loss: 0.097
Actual params: [-1.1777,  0.5451]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.184
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.555
iter 23 loss: 0.097
Actual params: [-1.1845,  0.5551]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.192
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.569
iter 24 loss: 0.097
Actual params: [-1.1918,  0.5689]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.197
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.578
iter 25 loss: 0.097
Actual params: [-1.1972,  0.5782]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.203
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.593
iter 26 loss: 0.097
Actual params: [-1.2031,  0.593 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.208
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.609
iter 27 loss: 0.097
Actual params: [-1.2079,  0.6089]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.212
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.623
iter 28 loss: 0.097
Actual params: [-1.2116,  0.6226]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -1.214
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.637
iter 29 loss: 0.097
Actual params: [-1.214 ,  0.6367]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -1.216
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.650
iter 30 loss: 0.097
Actual params: [-1.2156,  0.6497]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.215
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.670
Target params: [1.3344, 1.5708]
iter 0 loss: 0.017
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.028, -lr * Pred Grad:  -0.205, New P: -0.678
-Original Grad: 0.003, -lr * Pred Grad:  0.025, New P: 0.029
iter 1 loss: 0.016
Actual params: [-0.6776,  0.0287]
-Original Grad: -0.007, -lr * Pred Grad:  -0.056, New P: -0.734
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.040
iter 2 loss: 0.016
Actual params: [-0.7337,  0.0405]
-Original Grad: -0.004, -lr * Pred Grad:  -0.035, New P: -0.768
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.048
iter 3 loss: 0.015
Actual params: [-0.7684,  0.0477]
-Original Grad: -0.004, -lr * Pred Grad:  -0.038, New P: -0.806
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.060
iter 4 loss: 0.015
Actual params: [-0.8062,  0.0598]
-Original Grad: -0.003, -lr * Pred Grad:  -0.034, New P: -0.840
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.072
iter 5 loss: 0.015
Actual params: [-0.8398,  0.0725]
-Original Grad: -0.002, -lr * Pred Grad:  -0.020, New P: -0.860
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.082
iter 6 loss: 0.015
Actual params: [-0.8602,  0.0816]
-Original Grad: -0.002, -lr * Pred Grad:  -0.026, New P: -0.886
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.092
iter 7 loss: 0.015
Actual params: [-0.8859,  0.0919]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.901
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.098
iter 8 loss: 0.015
Actual params: [-0.9007,  0.098 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.920
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.107
iter 9 loss: 0.015
Actual params: [-0.9198,  0.1067]
-Original Grad: -0.001, -lr * Pred Grad:  -0.024, New P: -0.944
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.118
iter 10 loss: 0.015
Actual params: [-0.9441,  0.1184]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.962
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.127
iter 11 loss: 0.015
Actual params: [-0.962 ,  0.1269]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.979
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.134
iter 12 loss: 0.015
Actual params: [-0.979 ,  0.1339]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.996
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.142
iter 13 loss: 0.015
Actual params: [-0.9958,  0.1415]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -1.010
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.148
iter 14 loss: 0.015
Actual params: [-1.0101,  0.1483]
-Original Grad: -0.001, -lr * Pred Grad:  -0.027, New P: -1.038
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.162
iter 15 loss: 0.015
Actual params: [-1.0376,  0.1615]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.053
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.169
iter 16 loss: 0.015
Actual params: [-1.0527,  0.1691]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.069
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.176
iter 17 loss: 0.015
Actual params: [-1.0691,  0.1764]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -1.091
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.186
iter 18 loss: 0.015
Actual params: [-1.0906,  0.1864]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.105
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.192
iter 19 loss: 0.015
Actual params: [-1.1054,  0.1924]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.118
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.197
iter 20 loss: 0.015
Actual params: [-1.1182,  0.1973]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.133
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.204
iter 21 loss: 0.015
Actual params: [-1.1325,  0.2044]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.148
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.211
iter 22 loss: 0.015
Actual params: [-1.148 ,  0.2107]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.162
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.217
iter 23 loss: 0.015
Actual params: [-1.1616,  0.2168]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.179
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.225
iter 24 loss: 0.015
Actual params: [-1.1794,  0.2246]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.192
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.230
iter 25 loss: 0.015
Actual params: [-1.1916,  0.23  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.209
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.237
iter 26 loss: 0.015
Actual params: [-1.2088,  0.2367]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.223
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.243
iter 27 loss: 0.015
Actual params: [-1.2226,  0.2433]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.240
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.251
iter 28 loss: 0.015
Actual params: [-1.2403,  0.2509]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.257
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.258
iter 29 loss: 0.015
Actual params: [-1.257 ,  0.2582]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.272
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.264
iter 30 loss: 0.015
Actual params: [-1.2716,  0.2638]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.282
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.269
Target params: [1.3344, 1.5708]
iter 0 loss: 0.095
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.175, -lr * Pred Grad:  -0.314, New P: -0.786
-Original Grad: 0.151, -lr * Pred Grad:  0.333, New P: 0.337
iter 1 loss: 0.082
Actual params: [-0.7861,  0.3366]
-Original Grad: -0.004, -lr * Pred Grad:  -0.010, New P: -0.796
-Original Grad: 0.004, -lr * Pred Grad:  0.008, New P: 0.344
iter 2 loss: 0.082
Actual params: [-0.796 ,  0.3444]
-Original Grad: -0.006, -lr * Pred Grad:  -0.014, New P: -0.810
-Original Grad: 0.005, -lr * Pred Grad:  0.011, New P: 0.355
iter 3 loss: 0.082
Actual params: [-0.8103,  0.3551]
-Original Grad: -0.004, -lr * Pred Grad:  -0.010, New P: -0.820
-Original Grad: 0.004, -lr * Pred Grad:  0.013, New P: 0.368
iter 4 loss: 0.082
Actual params: [-0.8203,  0.3679]
-Original Grad: -0.005, -lr * Pred Grad:  -0.014, New P: -0.835
-Original Grad: 0.004, -lr * Pred Grad:  0.012, New P: 0.380
iter 5 loss: 0.082
Actual params: [-0.8346,  0.3801]
-Original Grad: -0.004, -lr * Pred Grad:  -0.015, New P: -0.850
-Original Grad: 0.004, -lr * Pred Grad:  0.011, New P: 0.391
iter 6 loss: 0.081
Actual params: [-0.8501,  0.3915]
-Original Grad: -0.005, -lr * Pred Grad:  -0.021, New P: -0.871
-Original Grad: 0.003, -lr * Pred Grad:  0.007, New P: 0.399
iter 7 loss: 0.081
Actual params: [-0.8708,  0.3989]
-Original Grad: -0.003, -lr * Pred Grad:  -0.014, New P: -0.885
-Original Grad: 0.003, -lr * Pred Grad:  0.009, New P: 0.408
iter 8 loss: 0.081
Actual params: [-0.8853,  0.4084]
-Original Grad: -0.003, -lr * Pred Grad:  -0.014, New P: -0.900
-Original Grad: 0.002, -lr * Pred Grad:  0.009, New P: 0.417
iter 9 loss: 0.081
Actual params: [-0.8998,  0.4174]
-Original Grad: -0.003, -lr * Pred Grad:  -0.015, New P: -0.915
-Original Grad: 0.002, -lr * Pred Grad:  0.009, New P: 0.426
iter 10 loss: 0.081
Actual params: [-0.9147,  0.4262]
-Original Grad: -0.002, -lr * Pred Grad:  -0.014, New P: -0.929
-Original Grad: 0.002, -lr * Pred Grad:  0.009, New P: 0.435
iter 11 loss: 0.081
Actual params: [-0.9287,  0.4351]
-Original Grad: -0.003, -lr * Pred Grad:  -0.020, New P: -0.949
-Original Grad: 0.002, -lr * Pred Grad:  0.006, New P: 0.441
iter 12 loss: 0.081
Actual params: [-0.9487,  0.4406]
-Original Grad: -0.002, -lr * Pred Grad:  -0.012, New P: -0.961
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: 0.449
iter 13 loss: 0.081
Actual params: [-0.9609,  0.4491]
-Original Grad: -0.002, -lr * Pred Grad:  -0.014, New P: -0.975
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.458
iter 14 loss: 0.081
Actual params: [-0.9746,  0.4582]
-Original Grad: -0.002, -lr * Pred Grad:  -0.015, New P: -0.990
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: 0.466
iter 15 loss: 0.081
Actual params: [-0.9899,  0.4662]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -1.006
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.473
iter 16 loss: 0.081
Actual params: [-1.0063,  0.4733]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -1.021
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: 0.481
iter 17 loss: 0.081
Actual params: [-1.0209,  0.4813]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -1.038
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.488
iter 18 loss: 0.081
Actual params: [-1.0383,  0.4881]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -1.054
-Original Grad: 0.001, -lr * Pred Grad:  0.005, New P: 0.493
iter 19 loss: 0.081
Actual params: [-1.0541,  0.4927]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -1.068
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: 0.501
iter 20 loss: 0.081
Actual params: [-1.0683,  0.5008]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -1.084
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.507
iter 21 loss: 0.081
Actual params: [-1.084 ,  0.5073]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -1.101
-Original Grad: 0.001, -lr * Pred Grad:  0.005, New P: 0.513
iter 22 loss: 0.081
Actual params: [-1.1009,  0.5126]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -1.113
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.520
iter 23 loss: 0.081
Actual params: [-1.113 ,  0.5201]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -1.130
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.530
iter 24 loss: 0.081
Actual params: [-1.13  ,  0.5295]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.144
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.536
iter 25 loss: 0.081
Actual params: [-1.144 ,  0.5359]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.160
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.543
iter 26 loss: 0.081
Actual params: [-1.1595,  0.5433]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.174
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.553
iter 27 loss: 0.081
Actual params: [-1.1742,  0.5528]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.188
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.557
iter 28 loss: 0.081
Actual params: [-1.1882,  0.5575]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.201
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.567
iter 29 loss: 0.081
Actual params: [-1.2007,  0.5668]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.212
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.574
iter 30 loss: 0.081
Actual params: [-1.2122,  0.5736]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.223
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.581
Target params: [1.3344, 1.5708]
iter 0 loss: 0.432
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.030, -lr * Pred Grad:  -0.318, New P: -0.791
-Original Grad: -0.005, -lr * Pred Grad:  -0.051, New P: -0.047
iter 1 loss: 0.430
Actual params: [-0.7908, -0.0472]
-Original Grad: -0.004, -lr * Pred Grad:  -0.045, New P: -0.835
-Original Grad: -0.002, -lr * Pred Grad:  -0.020, New P: -0.067
iter 2 loss: 0.430
Actual params: [-0.8353, -0.0669]
-Original Grad: -0.002, -lr * Pred Grad:  -0.032, New P: -0.867
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.082
iter 3 loss: 0.430
Actual params: [-0.8675, -0.0815]
-Original Grad: -0.002, -lr * Pred Grad:  -0.031, New P: -0.898
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.096
iter 4 loss: 0.430
Actual params: [-0.8984, -0.0956]
-Original Grad: -0.002, -lr * Pred Grad:  -0.026, New P: -0.924
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.108
iter 5 loss: 0.430
Actual params: [-0.9242, -0.1082]
-Original Grad: -0.001, -lr * Pred Grad:  -0.024, New P: -0.948
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.120
iter 6 loss: 0.430
Actual params: [-0.948 , -0.1199]
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -0.970
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.131
iter 7 loss: 0.430
Actual params: [-0.9695, -0.1307]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -0.990
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.141
iter 8 loss: 0.430
Actual params: [-0.9904, -0.1406]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -1.008
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.151
iter 9 loss: 0.430
Actual params: [-1.0078, -0.1513]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -1.022
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.159
iter 10 loss: 0.430
Actual params: [-1.0216, -0.1594]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -1.041
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.170
iter 11 loss: 0.430
Actual params: [-1.0405, -0.1703]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -1.058
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.181
iter 12 loss: 0.430
Actual params: [-1.0578, -0.1812]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.074
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.192
iter 13 loss: 0.430
Actual params: [-1.0742, -0.1925]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.088
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.203
iter 14 loss: 0.430
Actual params: [-1.0885, -0.203 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.105
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.215
iter 15 loss: 0.430
Actual params: [-1.1051, -0.2146]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.119
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.225
iter 16 loss: 0.430
Actual params: [-1.1192, -0.2252]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.132
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.235
iter 17 loss: 0.430
Actual params: [-1.1325, -0.2349]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.147
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.246
iter 18 loss: 0.430
Actual params: [-1.1473, -0.2455]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.162
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.256
iter 19 loss: 0.430
Actual params: [-1.1615, -0.2559]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.176
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.266
iter 20 loss: 0.430
Actual params: [-1.1755, -0.2663]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.189
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.276
iter 21 loss: 0.430
Actual params: [-1.1886, -0.2758]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.202
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.286
iter 22 loss: 0.430
Actual params: [-1.2022, -0.2857]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.215
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.295
iter 23 loss: 0.430
Actual params: [-1.2154, -0.2952]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.227
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.304
iter 24 loss: 0.430
Actual params: [-1.2274, -0.3043]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.239
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.313
iter 25 loss: 0.430
Actual params: [-1.2395, -0.3132]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.252
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.322
iter 26 loss: 0.430
Actual params: [-1.252 , -0.3223]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.264
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.331
iter 27 loss: 0.430
Actual params: [-1.2638, -0.3313]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.276
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.340
iter 28 loss: 0.430
Actual params: [-1.2763, -0.3404]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.289
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.350
iter 29 loss: 0.430
Actual params: [-1.2894, -0.35  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.302
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.359
iter 30 loss: 0.429
Actual params: [-1.3017, -0.359 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.313
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.367
Target params: [1.3344, 1.5708]
iter 0 loss: 0.274
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.091, -lr * Pred Grad:  -0.424, New P: -0.897
-Original Grad: 0.561, -lr * Pred Grad:  0.417, New P: 0.421
iter 1 loss: 0.282
Actual params: [-0.8966,  0.4206]
-Original Grad: 0.223, -lr * Pred Grad:  0.315, New P: -0.582
-Original Grad: 0.031, -lr * Pred Grad:  -0.101, New P: 0.320
iter 2 loss: 0.237
Actual params: [-0.5818,  0.3199]
-Original Grad: 0.318, -lr * Pred Grad:  0.084, New P: -0.497
-Original Grad: 0.112, -lr * Pred Grad:  0.029, New P: 0.348
iter 3 loss: 0.214
Actual params: [-0.4973,  0.3485]
-Original Grad: 0.341, -lr * Pred Grad:  0.061, New P: -0.437
-Original Grad: 0.282, -lr * Pred Grad:  0.109, New P: 0.457
iter 4 loss: 0.189
Actual params: [-0.4367,  0.457 ]
-Original Grad: 0.363, -lr * Pred Grad:  0.073, New P: -0.364
-Original Grad: 0.186, -lr * Pred Grad:  0.045, New P: 0.502
iter 5 loss: 0.166
Actual params: [-0.3638,  0.5022]
-Original Grad: 0.442, -lr * Pred Grad:  0.082, New P: -0.282
-Original Grad: 0.164, -lr * Pred Grad:  0.017, New P: 0.519
iter 6 loss: 0.142
Actual params: [-0.2817,  0.5191]
-Original Grad: 0.680, -lr * Pred Grad:  0.092, New P: -0.190
-Original Grad: 0.075, -lr * Pred Grad:  -0.046, New P: 0.473
iter 7 loss: 0.117
Actual params: [-0.1895,  0.473 ]
-Original Grad: 0.583, -lr * Pred Grad:  0.048, New P: -0.142
-Original Grad: 0.114, -lr * Pred Grad:  0.018, New P: 0.491
iter 8 loss: 0.102
Actual params: [-0.142 ,  0.4913]
-Original Grad: 0.616, -lr * Pred Grad:  0.040, New P: -0.102
-Original Grad: 0.101, -lr * Pred Grad:  0.018, New P: 0.510
iter 9 loss: 0.090
Actual params: [-0.1017,  0.5098]
-Original Grad: 0.252, -lr * Pred Grad:  0.016, New P: -0.086
-Original Grad: 0.055, -lr * Pred Grad:  0.019, New P: 0.528
iter 10 loss: 0.086
Actual params: [-0.0862,  0.5284]
-Original Grad: 0.374, -lr * Pred Grad:  0.022, New P: -0.065
-Original Grad: 0.097, -lr * Pred Grad:  0.040, New P: 0.568
iter 11 loss: 0.081
Actual params: [-0.0646,  0.5683]
-Original Grad: 0.347, -lr * Pred Grad:  0.025, New P: -0.040
-Original Grad: 0.030, -lr * Pred Grad:  -0.007, New P: 0.562
iter 12 loss: 0.076
Actual params: [-0.0397,  0.5616]
-Original Grad: 0.269, -lr * Pred Grad:  0.017, New P: -0.023
-Original Grad: 0.053, -lr * Pred Grad:  0.022, New P: 0.583
iter 13 loss: 0.072
Actual params: [-0.0226,  0.5832]
-Original Grad: 0.273, -lr * Pred Grad:  0.019, New P: -0.003
-Original Grad: 0.031, -lr * Pred Grad:  0.007, New P: 0.590
iter 14 loss: 0.069
Actual params: [-0.0034,  0.5903]
-Original Grad: 0.217, -lr * Pred Grad:  0.014, New P: 0.011
-Original Grad: 0.045, -lr * Pred Grad:  0.025, New P: 0.615
iter 15 loss: 0.066
Actual params: [0.011 , 0.6151]
-Original Grad: 0.250, -lr * Pred Grad:  0.020, New P: 0.031
-Original Grad: 0.013, -lr * Pred Grad:  -0.007, New P: 0.608
iter 16 loss: 0.064
Actual params: [0.0312, 0.6079]
-Original Grad: 0.294, -lr * Pred Grad:  0.023, New P: 0.054
-Original Grad: 0.016, -lr * Pred Grad:  -0.003, New P: 0.605
iter 17 loss: 0.061
Actual params: [0.0538, 0.6051]
-Original Grad: 0.195, -lr * Pred Grad:  0.016, New P: 0.070
-Original Grad: -0.019, -lr * Pred Grad:  -0.029, New P: 0.576
iter 18 loss: 0.062
Actual params: [0.07  , 0.5761]
-Original Grad: 0.185, -lr * Pred Grad:  0.014, New P: 0.084
-Original Grad: 0.090, -lr * Pred Grad:  0.064, New P: 0.640
iter 19 loss: 0.056
Actual params: [0.0838, 0.6405]
-Original Grad: 0.176, -lr * Pred Grad:  0.013, New P: 0.097
-Original Grad: 0.024, -lr * Pred Grad:  0.020, New P: 0.661
iter 20 loss: 0.054
Actual params: [0.0971, 0.6609]
-Original Grad: 0.190, -lr * Pred Grad:  0.014, New P: 0.111
-Original Grad: -0.003, -lr * Pred Grad:  0.007, New P: 0.668
iter 21 loss: 0.053
Actual params: [0.1108, 0.6679]
-Original Grad: 0.369, -lr * Pred Grad:  0.023, New P: 0.134
-Original Grad: -0.045, -lr * Pred Grad:  -0.006, New P: 0.661
iter 22 loss: 0.051
Actual params: [0.1336, 0.6615]
-Original Grad: 0.294, -lr * Pred Grad:  0.020, New P: 0.153
-Original Grad: -0.014, -lr * Pred Grad:  0.010, New P: 0.671
iter 23 loss: 0.049
Actual params: [0.1535, 0.6711]
-Original Grad: 0.191, -lr * Pred Grad:  0.011, New P: 0.165
-Original Grad: -0.017, -lr * Pred Grad:  0.006, New P: 0.677
iter 24 loss: 0.049
Actual params: [0.1648, 0.6774]
-Original Grad: -0.147, -lr * Pred Grad:  0.003, New P: 0.168
-Original Grad: 0.105, -lr * Pred Grad:  0.050, New P: 0.727
iter 25 loss: 0.046
Actual params: [0.1679, 0.7272]
-Original Grad: -0.060, -lr * Pred Grad:  -0.000, New P: 0.168
-Original Grad: 0.030, -lr * Pred Grad:  0.013, New P: 0.740
iter 26 loss: 0.046
Actual params: [0.1677, 0.7401]
-Original Grad: 0.073, -lr * Pred Grad:  0.006, New P: 0.174
-Original Grad: 0.006, -lr * Pred Grad:  0.015, New P: 0.755
iter 27 loss: 0.045
Actual params: [0.1735, 0.7551]
-Original Grad: 0.117, -lr * Pred Grad:  0.007, New P: 0.181
-Original Grad: -0.009, -lr * Pred Grad:  0.011, New P: 0.766
iter 28 loss: 0.045
Actual params: [0.1807, 0.7664]
-Original Grad: 0.080, -lr * Pred Grad:  0.006, New P: 0.187
-Original Grad: -0.002, -lr * Pred Grad:  0.012, New P: 0.778
iter 29 loss: 0.044
Actual params: [0.1866, 0.7782]
-Original Grad: 0.137, -lr * Pred Grad:  0.011, New P: 0.198
-Original Grad: -0.000, -lr * Pred Grad:  0.026, New P: 0.804
iter 30 loss: 0.043
Actual params: [0.1977, 0.8041]
-Original Grad: -0.017, -lr * Pred Grad:  0.003, New P: 0.201
-Original Grad: 0.022, -lr * Pred Grad:  0.020, New P: 0.825
Target params: [1.3344, 1.5708]
iter 0 loss: 0.365
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.073, -lr * Pred Grad:  -0.523, New P: -0.995
-Original Grad: -0.047, -lr * Pred Grad:  -0.335, New P: -0.331
iter 1 loss: 0.357
Actual params: [-0.995 , -0.3311]
-Original Grad: -0.001, -lr * Pred Grad:  -0.006, New P: -1.001
-Original Grad: -0.001, -lr * Pred Grad:  -0.007, New P: -0.338
iter 2 loss: 0.357
Actual params: [-1.0014, -0.3383]
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -1.009
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -0.346
iter 3 loss: 0.357
Actual params: [-1.0089, -0.3463]
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -1.017
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -0.355
iter 4 loss: 0.357
Actual params: [-1.0166, -0.3546]
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -1.024
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -0.363
iter 5 loss: 0.357
Actual params: [-1.0244, -0.3631]
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -1.032
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.372
iter 6 loss: 0.357
Actual params: [-1.0323, -0.3719]
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -1.040
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.381
iter 7 loss: 0.357
Actual params: [-1.0404, -0.3807]
-Original Grad: -0.001, -lr * Pred Grad:  -0.007, New P: -1.048
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.389
iter 8 loss: 0.357
Actual params: [-1.0478, -0.3889]
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -1.056
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.399
iter 9 loss: 0.357
Actual params: [-1.0561, -0.3987]
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -1.065
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.409
iter 10 loss: 0.357
Actual params: [-1.0649, -0.4087]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.073
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.418
iter 11 loss: 0.357
Actual params: [-1.0733, -0.4183]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.082
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.428
iter 12 loss: 0.357
Actual params: [-1.0815, -0.4281]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.089
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.437
iter 13 loss: 0.357
Actual params: [-1.0893, -0.4373]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.097
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.446
iter 14 loss: 0.357
Actual params: [-1.097, -0.446]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.105
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.455
iter 15 loss: 0.357
Actual params: [-1.1049, -0.4553]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.112
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.464
iter 16 loss: 0.357
Actual params: [-1.1119, -0.4636]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.120
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.473
iter 17 loss: 0.357
Actual params: [-1.1195, -0.4727]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.128
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.482
iter 18 loss: 0.357
Actual params: [-1.1276, -0.4821]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.135
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.491
iter 19 loss: 0.357
Actual params: [-1.1354, -0.4913]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.143
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.500
iter 20 loss: 0.357
Actual params: [-1.1432, -0.5004]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.152
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.511
iter 21 loss: 0.357
Actual params: [-1.1518, -0.5107]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.160
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.520
iter 22 loss: 0.357
Actual params: [-1.1596, -0.52  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.168
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.530
iter 23 loss: 0.357
Actual params: [-1.1678, -0.5299]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.176
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.540
iter 24 loss: 0.357
Actual params: [-1.1757, -0.5397]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.184
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.550
iter 25 loss: 0.357
Actual params: [-1.1844, -0.5502]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.193
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.560
iter 26 loss: 0.357
Actual params: [-1.1926, -0.5599]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.201
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.570
iter 27 loss: 0.357
Actual params: [-1.2009, -0.5696]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.209
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.580
iter 28 loss: 0.357
Actual params: [-1.2094, -0.5802]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.218
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.590
iter 29 loss: 0.357
Actual params: [-1.218 , -0.5904]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.226
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.600
iter 30 loss: 0.357
Actual params: [-1.2264, -0.6003]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.234
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.609
Target params: [1.3344, 1.5708]
iter 0 loss: 0.104
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.004, -lr * Pred Grad:  0.042, New P: -0.431
-Original Grad: 0.004, -lr * Pred Grad:  0.047, New P: 0.050
iter 1 loss: 0.101
Actual params: [-0.4305,  0.05  ]
-Original Grad: 0.007, -lr * Pred Grad:  0.082, New P: -0.348
-Original Grad: 0.007, -lr * Pred Grad:  0.080, New P: 0.130
iter 2 loss: 0.096
Actual params: [-0.3481,  0.1302]
-Original Grad: 0.009, -lr * Pred Grad:  0.115, New P: -0.233
-Original Grad: 0.008, -lr * Pred Grad:  0.106, New P: 0.236
iter 3 loss: 0.087
Actual params: [-0.2335,  0.2361]
-Original Grad: 0.039, -lr * Pred Grad:  0.360, New P: 0.127
-Original Grad: 0.025, -lr * Pred Grad:  0.224, New P: 0.460
iter 4 loss: 0.057
Actual params: [0.1265, 0.4599]
-Original Grad: 0.018, -lr * Pred Grad:  0.172, New P: 0.298
-Original Grad: 0.004, -lr * Pred Grad:  0.011, New P: 0.471
iter 5 loss: 0.055
Actual params: [0.2984, 0.4706]
-Original Grad: 0.013, -lr * Pred Grad:  0.135, New P: 0.434
-Original Grad: 0.003, -lr * Pred Grad:  0.011, New P: 0.481
iter 6 loss: 0.065
Actual params: [0.4337, 0.4812]
-Original Grad: -0.151, -lr * Pred Grad:  -0.003, New P: 0.430
-Original Grad: 0.094, -lr * Pred Grad:  0.301, New P: 0.783
iter 7 loss: 0.051
Actual params: [0.4303, 0.7826]
-Original Grad: 0.032, -lr * Pred Grad:  0.077, New P: 0.507
-Original Grad: -0.001, -lr * Pred Grad:  0.119, New P: 0.902
iter 8 loss: 0.050
Actual params: [0.5069, 0.9019]
-Original Grad: 0.047, -lr * Pred Grad:  0.085, New P: 0.592
-Original Grad: -0.009, -lr * Pred Grad:  0.100, New P: 1.002
iter 9 loss: 0.048
Actual params: [0.5918, 1.0017]
-Original Grad: 0.051, -lr * Pred Grad:  0.117, New P: 0.709
-Original Grad: 0.001, -lr * Pred Grad:  0.161, New P: 1.163
iter 10 loss: 0.043
Actual params: [0.7089, 1.1632]
-Original Grad: 0.088, -lr * Pred Grad:  0.086, New P: 0.795
-Original Grad: -0.035, -lr * Pred Grad:  0.003, New P: 1.166
iter 11 loss: 0.037
Actual params: [0.7953, 1.1664]
-Original Grad: 0.148, -lr * Pred Grad:  0.155, New P: 0.950
-Original Grad: -0.041, -lr * Pred Grad:  0.106, New P: 1.272
iter 12 loss: 0.025
Actual params: [0.95  , 1.2724]
-Original Grad: 0.117, -lr * Pred Grad:  0.034, New P: 0.984
-Original Grad: -0.058, -lr * Pred Grad:  -0.108, New P: 1.164
iter 13 loss: 0.023
Actual params: [0.9843, 1.1642]
-Original Grad: 0.149, -lr * Pred Grad:  0.072, New P: 1.056
-Original Grad: -0.051, -lr * Pred Grad:  0.001, New P: 1.165
iter 14 loss: 0.019
Actual params: [1.0561, 1.1651]
-Original Grad: 0.074, -lr * Pred Grad:  0.024, New P: 1.080
-Original Grad: -0.034, -lr * Pred Grad:  -0.033, New P: 1.132
iter 15 loss: 0.018
Actual params: [1.0802, 1.1325]
-Original Grad: 0.071, -lr * Pred Grad:  0.050, New P: 1.130
-Original Grad: 0.017, -lr * Pred Grad:  0.059, New P: 1.191
iter 16 loss: 0.016
Actual params: [1.1304, 1.1915]
-Original Grad: 0.023, -lr * Pred Grad:  -0.003, New P: 1.127
-Original Grad: -0.038, -lr * Pred Grad:  -0.052, New P: 1.139
iter 17 loss: 0.016
Actual params: [1.1274, 1.1393]
-Original Grad: 0.051, -lr * Pred Grad:  0.011, New P: 1.138
-Original Grad: -0.047, -lr * Pred Grad:  -0.053, New P: 1.086
iter 18 loss: 0.017
Actual params: [1.1381, 1.0861]
-Original Grad: 0.007, -lr * Pred Grad:  -0.009, New P: 1.129
-Original Grad: -0.024, -lr * Pred Grad:  -0.038, New P: 1.048
iter 19 loss: 0.018
Actual params: [1.1295, 1.0481]
-Original Grad: 0.050, -lr * Pred Grad:  0.036, New P: 1.165
-Original Grad: 0.013, -lr * Pred Grad:  0.034, New P: 1.082
iter 20 loss: 0.016
Actual params: [1.1653, 1.0819]
-Original Grad: -0.003, -lr * Pred Grad:  -0.002, New P: 1.163
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: 1.082
iter 21 loss: 0.016
Actual params: [1.1632, 1.0815]
-Original Grad: 0.036, -lr * Pred Grad:  0.016, New P: 1.180
-Original Grad: -0.032, -lr * Pred Grad:  -0.032, New P: 1.050
iter 22 loss: 0.016
Actual params: [1.1797, 1.0496]
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: 1.178
-Original Grad: -0.003, -lr * Pred Grad:  -0.004, New P: 1.045
iter 23 loss: 0.016
Actual params: [1.1782, 1.0453]
-Original Grad: -0.003, -lr * Pred Grad:  -0.002, New P: 1.176
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: 1.045
iter 24 loss: 0.016
Actual params: [1.1761, 1.0448]
-Original Grad: 0.060, -lr * Pred Grad:  0.030, New P: 1.206
-Original Grad: 0.013, -lr * Pred Grad:  0.016, New P: 1.061
iter 25 loss: 0.015
Actual params: [1.206 , 1.0605]
-Original Grad: 0.015, -lr * Pred Grad:  0.007, New P: 1.213
-Original Grad: 0.004, -lr * Pred Grad:  0.004, New P: 1.064
iter 26 loss: 0.015
Actual params: [1.2131, 1.0643]
-Original Grad: -0.012, -lr * Pred Grad:  -0.006, New P: 1.207
-Original Grad: -0.016, -lr * Pred Grad:  -0.020, New P: 1.044
iter 27 loss: 0.015
Actual params: [1.2074, 1.0444]
-Original Grad: -0.010, -lr * Pred Grad:  -0.006, New P: 1.202
-Original Grad: 0.004, -lr * Pred Grad:  0.006, New P: 1.051
iter 28 loss: 0.015
Actual params: [1.2017, 1.0508]
-Original Grad: -0.019, -lr * Pred Grad:  -0.010, New P: 1.192
-Original Grad: -0.010, -lr * Pred Grad:  -0.013, New P: 1.037
iter 29 loss: 0.016
Actual params: [1.1917, 1.0374]
-Original Grad: 0.063, -lr * Pred Grad:  0.023, New P: 1.215
-Original Grad: 0.032, -lr * Pred Grad:  0.034, New P: 1.071
iter 30 loss: 0.015
Actual params: [1.2145, 1.0712]
-Original Grad: 0.022, -lr * Pred Grad:  0.010, New P: 1.225
-Original Grad: -0.003, -lr * Pred Grad:  -0.009, New P: 1.062
Target params: [1.3344, 1.5708]
iter 0 loss: 0.044
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.012, -lr * Pred Grad:  -0.130, New P: -0.603
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.004
iter 1 loss: 0.043
Actual params: [-0.6027,  0.0039]
-Original Grad: -0.007, -lr * Pred Grad:  -0.082, New P: -0.685
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.003
iter 2 loss: 0.043
Actual params: [-0.6847,  0.0034]
-Original Grad: -0.004, -lr * Pred Grad:  -0.055, New P: -0.739
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.005
iter 3 loss: 0.043
Actual params: [-0.7393,  0.005 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.028, New P: -0.768
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.006
iter 4 loss: 0.042
Actual params: [-0.7676,  0.0063]
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -0.792
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.010
iter 5 loss: 0.042
Actual params: [-0.7918,  0.01  ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.028, New P: -0.819
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.014
iter 6 loss: 0.042
Actual params: [-0.8194,  0.0137]
-Original Grad: -0.002, -lr * Pred Grad:  -0.038, New P: -0.858
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.019
iter 7 loss: 0.042
Actual params: [-0.8578,  0.0187]
-Original Grad: -0.001, -lr * Pred Grad:  -0.023, New P: -0.881
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.023
iter 8 loss: 0.042
Actual params: [-0.8812,  0.0233]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -0.903
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.029
iter 9 loss: 0.042
Actual params: [-0.9025,  0.0293]
-Original Grad: -0.001, -lr * Pred Grad:  -0.031, New P: -0.933
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.036
iter 10 loss: 0.042
Actual params: [-0.9335,  0.0358]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -0.954
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.044
iter 11 loss: 0.042
Actual params: [-0.9542,  0.0444]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.972
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.051
iter 12 loss: 0.042
Actual params: [-0.9723,  0.0505]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.986
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.056
iter 13 loss: 0.042
Actual params: [-0.986 ,  0.0558]
-Original Grad: -0.001, -lr * Pred Grad:  -0.023, New P: -1.009
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.063
iter 14 loss: 0.042
Actual params: [-1.0093,  0.0633]
-Original Grad: -0.001, -lr * Pred Grad:  -0.023, New P: -1.032
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.072
iter 15 loss: 0.042
Actual params: [-1.0324,  0.0717]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.045
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.078
iter 16 loss: 0.042
Actual params: [-1.0445,  0.0778]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.059
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.084
iter 17 loss: 0.042
Actual params: [-1.0592,  0.0836]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.081
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.091
iter 18 loss: 0.042
Actual params: [-1.0809,  0.0908]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.101
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.101
iter 19 loss: 0.042
Actual params: [-1.1014,  0.1009]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.126
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.110
iter 20 loss: 0.042
Actual params: [-1.1257,  0.1097]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.152
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.122
iter 21 loss: 0.042
Actual params: [-1.1518,  0.1216]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.164
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.128
iter 22 loss: 0.042
Actual params: [-1.1642,  0.1283]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.195
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.141
iter 23 loss: 0.042
Actual params: [-1.195 ,  0.1414]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.226
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.160
iter 24 loss: 0.042
Actual params: [-1.2263,  0.1597]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.250
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.170
iter 25 loss: 0.042
Actual params: [-1.2501,  0.1696]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.281
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.183
iter 26 loss: 0.042
Actual params: [-1.2807,  0.1826]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.299
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.191
iter 27 loss: 0.042
Actual params: [-1.299 ,  0.1909]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.324
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.200
iter 28 loss: 0.042
Actual params: [-1.3241,  0.2001]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.342
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.207
iter 29 loss: 0.042
Actual params: [-1.3421,  0.207 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.358
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.214
iter 30 loss: 0.042
Actual params: [-1.3581,  0.2145]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.378
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.226
Target params: [1.3344, 1.5708]
iter 0 loss: 0.170
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.246, -lr * Pred Grad:  0.643, New P: 0.171
-Original Grad: -0.024, -lr * Pred Grad:  -0.028, New P: -0.025
iter 1 loss: 0.076
Actual params: [ 0.1711, -0.0249]
-Original Grad: 0.274, -lr * Pred Grad:  0.199, New P: 0.370
-Original Grad: 0.080, -lr * Pred Grad:  0.247, New P: 0.222
iter 2 loss: 0.020
Actual params: [0.3698, 0.2221]
-Original Grad: 0.194, -lr * Pred Grad:  0.028, New P: 0.398
-Original Grad: 0.119, -lr * Pred Grad:  0.130, New P: 0.352
iter 3 loss: 0.010
Actual params: [0.3981, 0.3517]
-Original Grad: 0.135, -lr * Pred Grad:  0.041, New P: 0.439
-Original Grad: 0.056, -lr * Pred Grad:  -0.008, New P: 0.344
iter 4 loss: 0.007
Actual params: [0.4389, 0.3441]
-Original Grad: 0.099, -lr * Pred Grad:  0.027, New P: 0.466
-Original Grad: 0.041, -lr * Pred Grad:  -0.009, New P: 0.335
iter 5 loss: 0.006
Actual params: [0.466 , 0.3349]
-Original Grad: 0.083, -lr * Pred Grad:  0.021, New P: 0.487
-Original Grad: 0.033, -lr * Pred Grad:  -0.009, New P: 0.325
iter 6 loss: 0.005
Actual params: [0.487 , 0.3255]
-Original Grad: 0.057, -lr * Pred Grad:  0.004, New P: 0.491
-Original Grad: 0.034, -lr * Pred Grad:  0.016, New P: 0.341
iter 7 loss: 0.005
Actual params: [0.4907, 0.3411]
-Original Grad: 0.063, -lr * Pred Grad:  0.023, New P: 0.514
-Original Grad: 0.016, -lr * Pred Grad:  -0.024, New P: 0.317
iter 8 loss: 0.005
Actual params: [0.5135, 0.3171]
-Original Grad: -0.003, -lr * Pred Grad:  0.017, New P: 0.530
-Original Grad: -0.022, -lr * Pred Grad:  -0.036, New P: 0.281
iter 9 loss: 0.005
Actual params: [0.5301, 0.2813]
-Original Grad: -0.003, -lr * Pred Grad:  0.005, New P: 0.535
-Original Grad: -0.009, -lr * Pred Grad:  -0.011, New P: 0.271
iter 10 loss: 0.005
Actual params: [0.535 , 0.2708]
-Original Grad: 0.058, -lr * Pred Grad:  0.005, New P: 0.540
-Original Grad: 0.035, -lr * Pred Grad:  0.006, New P: 0.277
iter 11 loss: 0.004
Actual params: [0.5402, 0.2767]
-Original Grad: -0.036, -lr * Pred Grad:  -0.008, New P: 0.532
-Original Grad: -0.012, -lr * Pred Grad:  0.007, New P: 0.284
iter 12 loss: 0.005
Actual params: [0.5319, 0.2837]
-Original Grad: -0.006, -lr * Pred Grad:  -0.000, New P: 0.532
-Original Grad: -0.004, -lr * Pred Grad:  -0.001, New P: 0.282
iter 13 loss: 0.005
Actual params: [0.5319, 0.2825]
-Original Grad: 0.017, -lr * Pred Grad:  0.006, New P: 0.538
-Original Grad: 0.002, -lr * Pred Grad:  -0.007, New P: 0.276
iter 14 loss: 0.005
Actual params: [0.5381, 0.2757]
-Original Grad: 0.001, -lr * Pred Grad:  -0.008, New P: 0.531
-Original Grad: 0.018, -lr * Pred Grad:  0.013, New P: 0.289
iter 15 loss: 0.005
Actual params: [0.5305, 0.2889]
-Original Grad: 0.028, -lr * Pred Grad:  0.012, New P: 0.542
-Original Grad: -0.008, -lr * Pred Grad:  -0.015, New P: 0.274
iter 16 loss: 0.004
Actual params: [0.5424, 0.2741]
-Original Grad: 0.008, -lr * Pred Grad:  0.007, New P: 0.549
-Original Grad: -0.015, -lr * Pred Grad:  -0.011, New P: 0.263
iter 17 loss: 0.004
Actual params: [0.5495, 0.263 ]
-Original Grad: 0.025, -lr * Pred Grad:  0.003, New P: 0.552
-Original Grad: 0.015, -lr * Pred Grad:  0.000, New P: 0.263
iter 18 loss: 0.004
Actual params: [0.5524, 0.2632]
-Original Grad: 0.025, -lr * Pred Grad:  0.004, New P: 0.557
-Original Grad: 0.009, -lr * Pred Grad:  -0.002, New P: 0.261
iter 19 loss: 0.004
Actual params: [0.5567, 0.2607]
-Original Grad: 0.039, -lr * Pred Grad:  0.006, New P: 0.563
-Original Grad: 0.014, -lr * Pred Grad:  -0.003, New P: 0.257
iter 20 loss: 0.004
Actual params: [0.563 , 0.2573]
-Original Grad: 0.017, -lr * Pred Grad:  0.006, New P: 0.569
-Original Grad: -0.005, -lr * Pred Grad:  -0.007, New P: 0.250
iter 21 loss: 0.004
Actual params: [0.569 , 0.2499]
-Original Grad: -0.012, -lr * Pred Grad:  -0.007, New P: 0.562
-Original Grad: 0.013, -lr * Pred Grad:  0.009, New P: 0.259
iter 22 loss: 0.004
Actual params: [0.5622, 0.2592]
-Original Grad: 0.050, -lr * Pred Grad:  -0.000, New P: 0.562
-Original Grad: 0.054, -lr * Pred Grad:  0.011, New P: 0.271
iter 23 loss: 0.004
Actual params: [0.5619, 0.2706]
-Original Grad: 0.012, -lr * Pred Grad:  0.001, New P: 0.563
-Original Grad: 0.007, -lr * Pred Grad:  0.000, New P: 0.271
iter 24 loss: 0.004
Actual params: [0.5632, 0.2708]
-Original Grad: 0.024, -lr * Pred Grad:  0.000, New P: 0.564
-Original Grad: 0.025, -lr * Pred Grad:  0.005, New P: 0.275
iter 25 loss: 0.004
Actual params: [0.5636, 0.2755]
-Original Grad: 0.016, -lr * Pred Grad:  0.006, New P: 0.570
-Original Grad: -0.010, -lr * Pred Grad:  -0.008, New P: 0.268
iter 26 loss: 0.004
Actual params: [0.5701, 0.2679]
-Original Grad: 0.061, -lr * Pred Grad:  0.006, New P: 0.576
-Original Grad: 0.047, -lr * Pred Grad:  0.005, New P: 0.273
iter 27 loss: 0.004
Actual params: [0.5756, 0.2729]
-Original Grad: 0.006, -lr * Pred Grad:  0.005, New P: 0.581
-Original Grad: -0.018, -lr * Pred Grad:  -0.008, New P: 0.265
iter 28 loss: 0.004
Actual params: [0.5809, 0.2651]
-Original Grad: 0.020, -lr * Pred Grad:  0.007, New P: 0.587
-Original Grad: -0.007, -lr * Pred Grad:  -0.007, New P: 0.259
iter 29 loss: 0.004
Actual params: [0.5874, 0.2585]
-Original Grad: 0.010, -lr * Pred Grad:  0.005, New P: 0.593
-Original Grad: -0.014, -lr * Pred Grad:  -0.007, New P: 0.251
iter 30 loss: 0.004
Actual params: [0.5929, 0.2515]
-Original Grad: 0.069, -lr * Pred Grad:  0.011, New P: 0.604
-Original Grad: 0.024, -lr * Pred Grad:  -0.004, New P: 0.247
Target params: [1.3344, 1.5708]
iter 0 loss: 0.591
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.055, -lr * Pred Grad:  0.461, New P: -0.011
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.012
iter 1 loss: 0.537
Actual params: [-0.0113, -0.0117]
-Original Grad: 0.254, -lr * Pred Grad:  0.503, New P: 0.492
-Original Grad: 0.003, -lr * Pred Grad:  0.014, New P: 0.002
iter 2 loss: 0.373
Actual params: [0.4917, 0.0022]
-Original Grad: 0.355, -lr * Pred Grad:  0.163, New P: 0.654
-Original Grad: 0.115, -lr * Pred Grad:  0.165, New P: 0.167
iter 3 loss: 0.296
Actual params: [0.6544, 0.1673]
-Original Grad: 0.307, -lr * Pred Grad:  0.135, New P: 0.789
-Original Grad: 0.066, -lr * Pred Grad:  -0.050, New P: 0.117
iter 4 loss: 0.267
Actual params: [0.7893, 0.1169]
-Original Grad: 0.446, -lr * Pred Grad:  0.033, New P: 0.823
-Original Grad: 0.210, -lr * Pred Grad:  0.339, New P: 0.456
iter 5 loss: 0.208
Actual params: [0.8227, 0.4558]
-Original Grad: 0.487, -lr * Pred Grad:  0.054, New P: 0.876
-Original Grad: 0.183, -lr * Pred Grad:  0.142, New P: 0.598
iter 6 loss: 0.180
Actual params: [0.8764, 0.5976]
-Original Grad: 0.300, -lr * Pred Grad:  0.059, New P: 0.935
-Original Grad: 0.005, -lr * Pred Grad:  -0.074, New P: 0.523
iter 7 loss: 0.175
Actual params: [0.9353, 0.5234]
-Original Grad: 0.150, -lr * Pred Grad:  0.018, New P: 0.953
-Original Grad: 0.053, -lr * Pred Grad:  0.021, New P: 0.544
iter 8 loss: 0.169
Actual params: [0.9532, 0.5442]
-Original Grad: 0.599, -lr * Pred Grad:  0.024, New P: 0.977
-Original Grad: 0.319, -lr * Pred Grad:  0.151, New P: 0.695
iter 9 loss: 0.152
Actual params: [0.9768, 0.6954]
-Original Grad: 0.129, -lr * Pred Grad:  0.004, New P: 0.981
-Original Grad: 0.099, -lr * Pred Grad:  0.042, New P: 0.737
iter 10 loss: 0.148
Actual params: [0.9808, 0.7374]
-Original Grad: 0.286, -lr * Pred Grad:  0.009, New P: 0.990
-Original Grad: 0.243, -lr * Pred Grad:  0.088, New P: 0.825
iter 11 loss: 0.144
Actual params: [0.9898, 0.8253]
-Original Grad: 0.094, -lr * Pred Grad:  0.008, New P: 0.998
-Original Grad: 0.041, -lr * Pred Grad:  0.008, New P: 0.834
iter 12 loss: 0.143
Actual params: [0.9981, 0.8336]
-Original Grad: 0.172, -lr * Pred Grad:  0.011, New P: 1.009
-Original Grad: 0.146, -lr * Pred Grad:  0.041, New P: 0.875
iter 13 loss: 0.141
Actual params: [1.0091, 0.875 ]
-Original Grad: 0.306, -lr * Pred Grad:  0.033, New P: 1.043
-Original Grad: -0.008, -lr * Pred Grad:  -0.017, New P: 0.858
iter 14 loss: 0.136
Actual params: [1.0426, 0.8577]
-Original Grad: 0.395, -lr * Pred Grad:  0.032, New P: 1.074
-Original Grad: 0.123, -lr * Pred Grad:  0.026, New P: 0.884
iter 15 loss: 0.132
Actual params: [1.0744, 0.884 ]
-Original Grad: 0.325, -lr * Pred Grad:  0.024, New P: 1.098
-Original Grad: 0.107, -lr * Pred Grad:  0.027, New P: 0.911
iter 16 loss: 0.128
Actual params: [1.0983, 0.9109]
-Original Grad: 0.243, -lr * Pred Grad:  0.017, New P: 1.115
-Original Grad: 0.085, -lr * Pred Grad:  0.024, New P: 0.935
iter 17 loss: 0.125
Actual params: [1.1151, 0.935 ]
-Original Grad: 0.625, -lr * Pred Grad:  0.037, New P: 1.152
-Original Grad: 0.064, -lr * Pred Grad:  0.014, New P: 0.949
iter 18 loss: 0.121
Actual params: [1.1521, 0.9493]
-Original Grad: 0.139, -lr * Pred Grad:  0.007, New P: 1.159
-Original Grad: 0.136, -lr * Pred Grad:  0.049, New P: 0.999
iter 19 loss: 0.119
Actual params: [1.1594, 0.9987]
-Original Grad: 0.011, -lr * Pred Grad:  0.001, New P: 1.160
-Original Grad: 0.033, -lr * Pred Grad:  0.012, New P: 1.011
iter 20 loss: 0.119
Actual params: [1.16  , 1.0108]
-Original Grad: 0.207, -lr * Pred Grad:  0.014, New P: 1.174
-Original Grad: 0.046, -lr * Pred Grad:  0.018, New P: 1.029
iter 21 loss: 0.117
Actual params: [1.1741, 1.0287]
-Original Grad: 0.179, -lr * Pred Grad:  0.012, New P: 1.187
-Original Grad: -0.006, -lr * Pred Grad:  -0.001, New P: 1.027
iter 22 loss: 0.116
Actual params: [1.1866, 1.0274]
-Original Grad: 0.201, -lr * Pred Grad:  0.015, New P: 1.202
-Original Grad: 0.062, -lr * Pred Grad:  0.026, New P: 1.054
iter 23 loss: 0.114
Actual params: [1.2018, 1.0538]
-Original Grad: 0.191, -lr * Pred Grad:  0.015, New P: 1.217
-Original Grad: 0.057, -lr * Pred Grad:  0.026, New P: 1.080
iter 24 loss: 0.111
Actual params: [1.2173, 1.0798]
-Original Grad: -0.064, -lr * Pred Grad:  -0.004, New P: 1.214
-Original Grad: 0.089, -lr * Pred Grad:  0.037, New P: 1.117
iter 25 loss: 0.110
Actual params: [1.2137, 1.1167]
-Original Grad: -0.022, -lr * Pred Grad:  0.000, New P: 1.214
-Original Grad: 0.089, -lr * Pred Grad:  0.038, New P: 1.155
iter 26 loss: 0.108
Actual params: [1.2141, 1.155 ]
-Original Grad: 0.117, -lr * Pred Grad:  0.015, New P: 1.229
-Original Grad: 0.118, -lr * Pred Grad:  0.054, New P: 1.209
iter 27 loss: 0.104
Actual params: [1.2286, 1.2089]
-Original Grad: 0.138, -lr * Pred Grad:  0.014, New P: 1.243
-Original Grad: 0.023, -lr * Pred Grad:  0.015, New P: 1.224
iter 28 loss: 0.102
Actual params: [1.2426, 1.2244]
-Original Grad: -0.055, -lr * Pred Grad:  -0.002, New P: 1.241
-Original Grad: 0.077, -lr * Pred Grad:  0.032, New P: 1.256
iter 29 loss: 0.101
Actual params: [1.2407, 1.2562]
-Original Grad: -0.216, -lr * Pred Grad:  -0.019, New P: 1.222
-Original Grad: 0.055, -lr * Pred Grad:  0.015, New P: 1.271
iter 30 loss: 0.102
Actual params: [1.2218, 1.2709]
-Original Grad: -0.008, -lr * Pred Grad:  0.000, New P: 1.222
-Original Grad: 0.021, -lr * Pred Grad:  0.009, New P: 1.280
Target params: [1.3344, 1.5708]
iter 0 loss: 0.746
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad:  0.019, New P: -0.453
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 0.001
iter 1 loss: 0.746
Actual params: [-0.453 ,  0.0008]
-Original Grad: 0.002, -lr * Pred Grad:  0.019, New P: -0.434
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.001
iter 2 loss: 0.746
Actual params: [-0.4336, -0.0011]
-Original Grad: 0.003, -lr * Pred Grad:  0.035, New P: -0.399
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.007
iter 3 loss: 0.746
Actual params: [-0.3988, -0.0069]
-Original Grad: 0.003, -lr * Pred Grad:  0.040, New P: -0.359
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.011
iter 4 loss: 0.746
Actual params: [-0.3586, -0.011 ]
-Original Grad: 0.004, -lr * Pred Grad:  0.066, New P: -0.293
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.019
iter 5 loss: 0.745
Actual params: [-0.2929, -0.0189]
-Original Grad: 0.007, -lr * Pred Grad:  0.127, New P: -0.166
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.032
iter 6 loss: 0.744
Actual params: [-0.166 , -0.0318]
-Original Grad: 0.016, -lr * Pred Grad:  0.327, New P: 0.161
-Original Grad: -0.001, -lr * Pred Grad:  -0.026, New P: -0.058
iter 7 loss: 0.727
Actual params: [ 0.1611, -0.0579]
-Original Grad: 0.064, -lr * Pred Grad:  0.928, New P: 1.089
-Original Grad: 0.025, -lr * Pred Grad:  0.370, New P: 0.312
iter 8 loss: 0.460
Actual params: [1.0893, 0.3125]
-Original Grad: -0.056, -lr * Pred Grad:  0.032, New P: 1.122
-Original Grad: 0.359, -lr * Pred Grad:  0.638, New P: 0.951
iter 9 loss: 0.241
Actual params: [1.1217, 0.9505]
-Original Grad: 0.334, -lr * Pred Grad:  0.096, New P: 1.218
-Original Grad: 0.290, -lr * Pred Grad:  0.275, New P: 1.225
iter 10 loss: 0.172
Actual params: [1.2179, 1.2254]
-Original Grad: 0.015, -lr * Pred Grad:  -0.013, New P: 1.205
-Original Grad: 0.195, -lr * Pred Grad:  0.180, New P: 1.405
iter 11 loss: 0.147
Actual params: [1.2048, 1.4053]
-Original Grad: 0.014, -lr * Pred Grad:  -0.008, New P: 1.197
-Original Grad: 0.199, -lr * Pred Grad:  0.157, New P: 1.562
iter 12 loss: 0.104
Actual params: [1.1966, 1.5621]
-Original Grad: 0.074, -lr * Pred Grad:  0.038, New P: 1.235
-Original Grad: 0.382, -lr * Pred Grad:  0.090, New P: 1.652
iter 13 loss: 0.077
Actual params: [1.2348, 1.6518]
-Original Grad: 0.370, -lr * Pred Grad:  0.058, New P: 1.293
-Original Grad: 0.058, -lr * Pred Grad:  0.028, New P: 1.680
iter 14 loss: 0.070
Actual params: [1.2933, 1.68  ]
-Original Grad: 0.067, -lr * Pred Grad:  0.013, New P: 1.306
-Original Grad: 0.089, -lr * Pred Grad:  0.025, New P: 1.705
iter 15 loss: 0.065
Actual params: [1.3063, 1.7048]
-Original Grad: -0.045, -lr * Pred Grad:  -0.002, New P: 1.305
-Original Grad: 0.118, -lr * Pred Grad:  0.029, New P: 1.734
iter 16 loss: 0.060
Actual params: [1.3045, 1.7343]
-Original Grad: 0.053, -lr * Pred Grad:  0.012, New P: 1.316
-Original Grad: 0.122, -lr * Pred Grad:  0.035, New P: 1.769
iter 17 loss: 0.056
Actual params: [1.316 , 1.7689]
-Original Grad: 0.033, -lr * Pred Grad:  0.007, New P: 1.323
-Original Grad: 0.081, -lr * Pred Grad:  0.024, New P: 1.793
iter 18 loss: 0.054
Actual params: [1.3229, 1.7928]
-Original Grad: 0.037, -lr * Pred Grad:  0.009, New P: 1.332
-Original Grad: 0.116, -lr * Pred Grad:  0.035, New P: 1.828
iter 19 loss: 0.052
Actual params: [1.3316, 1.8277]
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: 1.334
-Original Grad: 0.092, -lr * Pred Grad:  0.028, New P: 1.856
iter 20 loss: 0.052
Actual params: [1.3342, 1.8557]
-Original Grad: 0.224, -lr * Pred Grad:  0.034, New P: 1.368
-Original Grad: 0.089, -lr * Pred Grad:  0.030, New P: 1.886
iter 21 loss: 0.050
Actual params: [1.3681, 1.8857]
-Original Grad: 0.021, -lr * Pred Grad:  0.004, New P: 1.372
-Original Grad: 0.050, -lr * Pred Grad:  0.015, New P: 1.901
iter 22 loss: 0.050
Actual params: [1.3717, 1.9011]
-Original Grad: -0.182, -lr * Pred Grad:  -0.023, New P: 1.348
-Original Grad: 0.128, -lr * Pred Grad:  0.035, New P: 1.936
iter 23 loss: 0.056
Actual params: [1.3483, 1.9358]
-Original Grad: -0.010, -lr * Pred Grad:  -0.001, New P: 1.347
-Original Grad: 0.018, -lr * Pred Grad:  0.005, New P: 1.940
iter 24 loss: 0.056
Actual params: [1.3473, 1.9404]
-Original Grad: 0.013, -lr * Pred Grad:  0.001, New P: 1.348
-Original Grad: -0.025, -lr * Pred Grad:  -0.006, New P: 1.935
iter 25 loss: 0.055
Actual params: [1.3485, 1.9345]
-Original Grad: -0.072, -lr * Pred Grad:  -0.009, New P: 1.340
-Original Grad: 0.050, -lr * Pred Grad:  0.008, New P: 1.943
iter 26 loss: 0.057
Actual params: [1.3398, 1.9425]
-Original Grad: 0.066, -lr * Pred Grad:  0.011, New P: 1.350
-Original Grad: -0.003, -lr * Pred Grad:  0.003, New P: 1.946
iter 27 loss: 0.056
Actual params: [1.3503, 1.9455]
-Original Grad: 0.109, -lr * Pred Grad:  0.010, New P: 1.360
-Original Grad: -0.141, -lr * Pred Grad:  -0.019, New P: 1.926
iter 28 loss: 0.053
Actual params: [1.3603, 1.9265]
-Original Grad: 0.075, -lr * Pred Grad:  0.012, New P: 1.372
-Original Grad: -0.015, -lr * Pred Grad:  0.001, New P: 1.928
iter 29 loss: 0.052
Actual params: [1.3718, 1.9277]
-Original Grad: 0.019, -lr * Pred Grad:  0.005, New P: 1.377
-Original Grad: 0.031, -lr * Pred Grad:  0.006, New P: 1.934
iter 30 loss: 0.052
Actual params: [1.3766, 1.9342]
-Original Grad: 0.127, -lr * Pred Grad:  0.016, New P: 1.393
-Original Grad: -0.103, -lr * Pred Grad:  -0.009, New P: 1.925
Target params: [1.3344, 1.5708]
iter 0 loss: 0.065
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.005, -lr * Pred Grad:  0.059, New P: -0.414
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.005
iter 1 loss: 0.061
Actual params: [-0.4137,  0.0051]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.410
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.012
iter 2 loss: 0.061
Actual params: [-0.4104,  0.0124]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.409
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.022
iter 3 loss: 0.061
Actual params: [-0.4092,  0.0219]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.407
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.034
iter 4 loss: 0.061
Actual params: [-0.4068,  0.0339]
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.397
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.048
iter 5 loss: 0.060
Actual params: [-0.397 ,  0.0477]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.394
-Original Grad: 0.001, -lr * Pred Grad:  0.027, New P: 0.075
iter 6 loss: 0.061
Actual params: [-0.3942,  0.0746]
-Original Grad: 0.002, -lr * Pred Grad:  0.050, New P: -0.344
-Original Grad: 0.001, -lr * Pred Grad:  0.022, New P: 0.096
iter 7 loss: 0.057
Actual params: [-0.3443,  0.0965]
-Original Grad: 0.024, -lr * Pred Grad:  0.337, New P: -0.008
-Original Grad: -0.001, -lr * Pred Grad:  0.002, New P: 0.099
iter 8 loss: 0.032
Actual params: [-0.0077,  0.0986]
-Original Grad: 0.018, -lr * Pred Grad:  0.218, New P: 0.211
-Original Grad: 0.008, -lr * Pred Grad:  0.186, New P: 0.285
iter 9 loss: 0.044
Actual params: [0.2105, 0.285 ]
-Original Grad: -0.142, -lr * Pred Grad:  0.046, New P: 0.257
-Original Grad: 0.088, -lr * Pred Grad:  0.298, New P: 0.583
iter 10 loss: 0.030
Actual params: [0.2567, 0.5826]
-Original Grad: 0.022, -lr * Pred Grad:  0.163, New P: 0.420
-Original Grad: 0.008, -lr * Pred Grad:  0.303, New P: 0.885
iter 11 loss: 0.027
Actual params: [0.4196, 0.8853]
-Original Grad: 0.056, -lr * Pred Grad:  0.211, New P: 0.631
-Original Grad: 0.005, -lr * Pred Grad:  0.372, New P: 1.257
iter 12 loss: 0.023
Actual params: [0.6309, 1.257 ]
-Original Grad: 0.064, -lr * Pred Grad:  0.128, New P: 0.759
-Original Grad: -0.001, -lr * Pred Grad:  0.212, New P: 1.469
iter 13 loss: 0.019
Actual params: [0.7591, 1.4689]
-Original Grad: 0.046, -lr * Pred Grad:  0.065, New P: 0.824
-Original Grad: -0.000, -lr * Pred Grad:  0.104, New P: 1.573
iter 14 loss: 0.019
Actual params: [0.8244, 1.5729]
-Original Grad: 0.127, -lr * Pred Grad:  0.110, New P: 0.935
-Original Grad: -0.005, -lr * Pred Grad:  0.161, New P: 1.734
iter 15 loss: 0.027
Actual params: [0.9346, 1.7341]
-Original Grad: 0.081, -lr * Pred Grad:  0.013, New P: 0.947
-Original Grad: -0.134, -lr * Pred Grad:  -0.200, New P: 1.534
iter 16 loss: 0.014
Actual params: [0.9474, 1.5337]
-Original Grad: 0.027, -lr * Pred Grad:  0.008, New P: 0.956
-Original Grad: -0.092, -lr * Pred Grad:  -0.094, New P: 1.439
iter 17 loss: 0.012
Actual params: [0.9555, 1.4394]
-Original Grad: -0.004, -lr * Pred Grad:  0.001, New P: 0.957
-Original Grad: -0.063, -lr * Pred Grad:  -0.052, New P: 1.387
iter 18 loss: 0.013
Actual params: [0.9566, 1.3871]
-Original Grad: 0.139, -lr * Pred Grad:  0.031, New P: 0.988
-Original Grad: -0.053, -lr * Pred Grad:  -0.043, New P: 1.344
iter 19 loss: 0.013
Actual params: [0.988 , 1.3443]
-Original Grad: 0.080, -lr * Pred Grad:  0.018, New P: 1.006
-Original Grad: -0.041, -lr * Pred Grad:  -0.031, New P: 1.313
iter 20 loss: 0.014
Actual params: [1.0059, 1.3135]
-Original Grad: 0.113, -lr * Pred Grad:  0.024, New P: 1.030
-Original Grad: 0.011, -lr * Pred Grad:  0.005, New P: 1.319
iter 21 loss: 0.014
Actual params: [1.0297, 1.3188]
-Original Grad: 0.056, -lr * Pred Grad:  0.013, New P: 1.043
-Original Grad: -0.042, -lr * Pred Grad:  -0.030, New P: 1.289
iter 22 loss: 0.015
Actual params: [1.0429, 1.2889]
-Original Grad: 0.039, -lr * Pred Grad:  0.009, New P: 1.052
-Original Grad: -0.007, -lr * Pred Grad:  -0.005, New P: 1.284
iter 23 loss: 0.015
Actual params: [1.0521, 1.284 ]
-Original Grad: 0.061, -lr * Pred Grad:  0.014, New P: 1.066
-Original Grad: 0.036, -lr * Pred Grad:  0.018, New P: 1.302
iter 24 loss: 0.014
Actual params: [1.0661, 1.3018]
-Original Grad: 0.013, -lr * Pred Grad:  0.005, New P: 1.071
-Original Grad: -0.033, -lr * Pred Grad:  -0.019, New P: 1.283
iter 25 loss: 0.015
Actual params: [1.0711, 1.2832]
-Original Grad: 0.058, -lr * Pred Grad:  0.013, New P: 1.084
-Original Grad: 0.043, -lr * Pred Grad:  0.019, New P: 1.302
iter 26 loss: 0.014
Actual params: [1.0839, 1.3024]
-Original Grad: 0.024, -lr * Pred Grad:  0.009, New P: 1.093
-Original Grad: -0.036, -lr * Pred Grad:  -0.020, New P: 1.282
iter 27 loss: 0.015
Actual params: [1.0927, 1.282 ]
-Original Grad: 0.021, -lr * Pred Grad:  0.004, New P: 1.097
-Original Grad: 0.037, -lr * Pred Grad:  0.017, New P: 1.299
iter 28 loss: 0.014
Actual params: [1.0967, 1.2991]
-Original Grad: 0.032, -lr * Pred Grad:  0.009, New P: 1.106
-Original Grad: 0.026, -lr * Pred Grad:  0.010, New P: 1.309
iter 29 loss: 0.014
Actual params: [1.1055, 1.3094]
-Original Grad: 0.029, -lr * Pred Grad:  0.010, New P: 1.115
-Original Grad: 0.001, -lr * Pred Grad:  -0.001, New P: 1.308
iter 30 loss: 0.014
Actual params: [1.1155, 1.3084]
-Original Grad: 0.020, -lr * Pred Grad:  0.010, New P: 1.126
-Original Grad: -0.063, -lr * Pred Grad:  -0.025, New P: 1.283
Target params: [1.3344, 1.5708]
iter 0 loss: 0.514
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.121, -lr * Pred Grad:  0.431, New P: -0.042
-Original Grad: 0.015, -lr * Pred Grad:  0.052, New P: 0.056
iter 1 loss: 0.443
Actual params: [-0.0418,  0.056 ]
-Original Grad: 0.147, -lr * Pred Grad:  0.009, New P: -0.032
-Original Grad: 0.112, -lr * Pred Grad:  0.500, New P: 0.556
iter 2 loss: 0.428
Actual params: [-0.0323,  0.5557]
-Original Grad: 0.533, -lr * Pred Grad:  0.057, New P: 0.025
-Original Grad: -0.057, -lr * Pred Grad:  -0.010, New P: 0.546
iter 3 loss: 0.400
Actual params: [0.0248, 0.5457]
-Original Grad: 0.726, -lr * Pred Grad:  0.044, New P: 0.069
-Original Grad: -0.189, -lr * Pred Grad:  -0.148, New P: 0.398
iter 4 loss: 0.370
Actual params: [0.069 , 0.3981]
-Original Grad: 0.987, -lr * Pred Grad:  0.037, New P: 0.106
-Original Grad: 0.043, -lr * Pred Grad:  0.032, New P: 0.430
iter 5 loss: 0.344
Actual params: [0.1061, 0.4296]
-Original Grad: 0.782, -lr * Pred Grad:  0.028, New P: 0.134
-Original Grad: 0.039, -lr * Pred Grad:  0.027, New P: 0.457
iter 6 loss: 0.316
Actual params: [0.1341, 0.4567]
-Original Grad: 1.101, -lr * Pred Grad:  0.030, New P: 0.164
-Original Grad: 0.121, -lr * Pred Grad:  0.077, New P: 0.533
iter 7 loss: 0.286
Actual params: [0.1643, 0.5335]
-Original Grad: 1.003, -lr * Pred Grad:  0.027, New P: 0.192
-Original Grad: 0.076, -lr * Pred Grad:  0.030, New P: 0.564
iter 8 loss: 0.265
Actual params: [0.1916, 0.564 ]
-Original Grad: 1.002, -lr * Pred Grad:  0.024, New P: 0.216
-Original Grad: 0.112, -lr * Pred Grad:  0.033, New P: 0.597
iter 9 loss: 0.246
Actual params: [0.2159, 0.5973]
-Original Grad: 1.446, -lr * Pred Grad:  0.019, New P: 0.235
-Original Grad: 0.440, -lr * Pred Grad:  0.119, New P: 0.717
iter 10 loss: 0.222
Actual params: [0.2348, 0.7167]
-Original Grad: 1.081, -lr * Pred Grad:  0.025, New P: 0.260
-Original Grad: 0.064, -lr * Pred Grad:  -0.015, New P: 0.702
iter 11 loss: 0.203
Actual params: [0.2596, 0.702 ]
-Original Grad: 1.418, -lr * Pred Grad:  0.029, New P: 0.289
-Original Grad: 0.098, -lr * Pred Grad:  -0.012, New P: 0.690
iter 12 loss: 0.184
Actual params: [0.2886, 0.6897]
-Original Grad: 1.227, -lr * Pred Grad:  0.017, New P: 0.305
-Original Grad: 0.332, -lr * Pred Grad:  0.057, New P: 0.747
iter 13 loss: 0.164
Actual params: [0.3055, 0.7467]
-Original Grad: 1.188, -lr * Pred Grad:  0.018, New P: 0.324
-Original Grad: 0.218, -lr * Pred Grad:  0.024, New P: 0.770
iter 14 loss: 0.148
Actual params: [0.3238, 0.7702]
-Original Grad: 0.895, -lr * Pred Grad:  0.014, New P: 0.337
-Original Grad: 0.170, -lr * Pred Grad:  0.018, New P: 0.788
iter 15 loss: 0.137
Actual params: [0.3373, 0.7882]
-Original Grad: 1.012, -lr * Pred Grad:  0.016, New P: 0.354
-Original Grad: 0.151, -lr * Pred Grad:  0.009, New P: 0.797
iter 16 loss: 0.125
Actual params: [0.3536, 0.7974]
-Original Grad: 1.192, -lr * Pred Grad:  0.010, New P: 0.364
-Original Grad: 0.490, -lr * Pred Grad:  0.067, New P: 0.865
iter 17 loss: 0.109
Actual params: [0.3637, 0.8649]
-Original Grad: 0.907, -lr * Pred Grad:  0.015, New P: 0.379
-Original Grad: 0.072, -lr * Pred Grad:  -0.007, New P: 0.858
iter 18 loss: 0.101
Actual params: [0.379 , 0.8579]
-Original Grad: 0.959, -lr * Pred Grad:  0.010, New P: 0.389
-Original Grad: 0.378, -lr * Pred Grad:  0.043, New P: 0.901
iter 19 loss: 0.091
Actual params: [0.3886, 0.9012]
-Original Grad: 1.154, -lr * Pred Grad:  0.015, New P: 0.403
-Original Grad: 0.266, -lr * Pred Grad:  0.019, New P: 0.920
iter 20 loss: 0.081
Actual params: [0.4034, 0.9203]
-Original Grad: 0.867, -lr * Pred Grad:  0.011, New P: 0.415
-Original Grad: 0.195, -lr * Pred Grad:  0.013, New P: 0.933
iter 21 loss: 0.075
Actual params: [0.4147, 0.9331]
-Original Grad: 0.706, -lr * Pred Grad:  0.008, New P: 0.423
-Original Grad: 0.202, -lr * Pred Grad:  0.017, New P: 0.950
iter 22 loss: 0.070
Actual params: [0.423 , 0.9503]
-Original Grad: 0.957, -lr * Pred Grad:  0.013, New P: 0.436
-Original Grad: 0.205, -lr * Pred Grad:  0.011, New P: 0.961
iter 23 loss: 0.064
Actual params: [0.4357, 0.9612]
-Original Grad: 0.363, -lr * Pred Grad:  0.002, New P: 0.438
-Original Grad: 0.220, -lr * Pred Grad:  0.026, New P: 0.988
iter 24 loss: 0.060
Actual params: [0.4377, 0.9875]
-Original Grad: 0.791, -lr * Pred Grad:  0.013, New P: 0.451
-Original Grad: 0.050, -lr * Pred Grad:  -0.008, New P: 0.979
iter 25 loss: 0.057
Actual params: [0.4507, 0.9792]
-Original Grad: 0.802, -lr * Pred Grad:  0.013, New P: 0.463
-Original Grad: 0.070, -lr * Pred Grad:  -0.005, New P: 0.974
iter 26 loss: 0.054
Actual params: [0.4633, 0.9744]
-Original Grad: 0.466, -lr * Pred Grad:  0.004, New P: 0.467
-Original Grad: 0.247, -lr * Pred Grad:  0.028, New P: 1.003
iter 27 loss: 0.051
Actual params: [0.4669, 1.0026]
-Original Grad: 0.535, -lr * Pred Grad:  0.007, New P: 0.474
-Original Grad: 0.147, -lr * Pred Grad:  0.012, New P: 1.015
iter 28 loss: 0.048
Actual params: [0.4738, 1.0146]
-Original Grad: 0.547, -lr * Pred Grad:  0.008, New P: 0.481
-Original Grad: 0.123, -lr * Pred Grad:  0.009, New P: 1.023
iter 29 loss: 0.046
Actual params: [0.4815, 1.0233]
-Original Grad: 0.431, -lr * Pred Grad:  0.007, New P: 0.488
-Original Grad: 0.036, -lr * Pred Grad:  -0.000, New P: 1.023
iter 30 loss: 0.044
Actual params: [0.4883, 1.023 ]
-Original Grad: 0.392, -lr * Pred Grad:  0.006, New P: 0.494
-Original Grad: 0.069, -lr * Pred Grad:  0.005, New P: 1.028
Target params: [1.3344, 1.5708]
iter 0 loss: 0.274
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.215, -lr * Pred Grad:  -0.395, New P: -0.867
-Original Grad: 0.649, -lr * Pred Grad:  0.376, New P: 0.379
iter 1 loss: 0.280
Actual params: [-0.8675,  0.3792]
-Original Grad: 0.220, -lr * Pred Grad:  0.491, New P: -0.376
-Original Grad: 0.092, -lr * Pred Grad:  -0.215, New P: 0.164
iter 2 loss: 0.220
Actual params: [-0.3765,  0.1641]
-Original Grad: 0.242, -lr * Pred Grad:  0.101, New P: -0.275
-Original Grad: 0.381, -lr * Pred Grad:  0.048, New P: 0.213
iter 3 loss: 0.186
Actual params: [-0.2753,  0.2125]
-Original Grad: 0.767, -lr * Pred Grad:  0.178, New P: -0.097
-Original Grad: 0.565, -lr * Pred Grad:  -0.049, New P: 0.164
iter 4 loss: 0.144
Actual params: [-0.0969,  0.1638]
-Original Grad: 0.283, -lr * Pred Grad:  -0.040, New P: -0.137
-Original Grad: 0.434, -lr * Pred Grad:  0.104, New P: 0.267
iter 5 loss: 0.122
Actual params: [-0.137 ,  0.2674]
-Original Grad: 0.338, -lr * Pred Grad:  0.025, New P: -0.112
-Original Grad: 0.299, -lr * Pred Grad:  0.025, New P: 0.293
iter 6 loss: 0.113
Actual params: [-0.112 ,  0.2928]
-Original Grad: 0.296, -lr * Pred Grad:  0.012, New P: -0.100
-Original Grad: 0.288, -lr * Pred Grad:  0.036, New P: 0.328
iter 7 loss: 0.106
Actual params: [-0.1003,  0.3285]
-Original Grad: 0.117, -lr * Pred Grad:  -0.013, New P: -0.114
-Original Grad: 0.193, -lr * Pred Grad:  0.043, New P: 0.371
iter 8 loss: 0.103
Actual params: [-0.1138,  0.3713]
-Original Grad: 0.491, -lr * Pred Grad:  0.059, New P: -0.055
-Original Grad: 0.167, -lr * Pred Grad:  -0.026, New P: 0.345
iter 9 loss: 0.101
Actual params: [-0.0551,  0.3455]
-Original Grad: 0.300, -lr * Pred Grad:  0.015, New P: -0.040
-Original Grad: 0.225, -lr * Pred Grad:  0.026, New P: 0.371
iter 10 loss: 0.096
Actual params: [-0.0405,  0.3715]
-Original Grad: 0.211, -lr * Pred Grad:  0.009, New P: -0.032
-Original Grad: 0.172, -lr * Pred Grad:  0.024, New P: 0.395
iter 11 loss: 0.091
Actual params: [-0.0318,  0.3951]
-Original Grad: 0.110, -lr * Pred Grad:  0.002, New P: -0.030
-Original Grad: 0.108, -lr * Pred Grad:  0.020, New P: 0.415
iter 12 loss: 0.088
Actual params: [-0.0299,  0.4149]
-Original Grad: 0.570, -lr * Pred Grad:  0.053, New P: 0.023
-Original Grad: 0.214, -lr * Pred Grad:  -0.010, New P: 0.405
iter 13 loss: 0.089
Actual params: [0.0234, 0.4048]
-Original Grad: -0.113, -lr * Pred Grad:  -0.015, New P: 0.008
-Original Grad: 0.318, -lr * Pred Grad:  0.052, New P: 0.457
iter 14 loss: 0.079
Actual params: [0.008 , 0.4571]
-Original Grad: 0.290, -lr * Pred Grad:  0.016, New P: 0.024
-Original Grad: 0.169, -lr * Pred Grad:  0.022, New P: 0.479
iter 15 loss: 0.075
Actual params: [0.0243, 0.4795]
-Original Grad: 0.145, -lr * Pred Grad:  0.008, New P: 0.033
-Original Grad: 0.116, -lr * Pred Grad:  0.017, New P: 0.496
iter 16 loss: 0.073
Actual params: [0.0325, 0.4963]
-Original Grad: 0.255, -lr * Pred Grad:  0.014, New P: 0.047
-Original Grad: 0.084, -lr * Pred Grad:  0.013, New P: 0.509
iter 17 loss: 0.070
Actual params: [0.047 , 0.5094]
-Original Grad: 0.420, -lr * Pred Grad:  0.023, New P: 0.070
-Original Grad: 0.079, -lr * Pred Grad:  0.013, New P: 0.523
iter 18 loss: 0.068
Actual params: [0.0697, 0.5229]
-Original Grad: 0.288, -lr * Pred Grad:  0.014, New P: 0.083
-Original Grad: 0.074, -lr * Pred Grad:  0.016, New P: 0.539
iter 19 loss: 0.065
Actual params: [0.0833, 0.5392]
-Original Grad: -0.000, -lr * Pred Grad:  0.004, New P: 0.088
-Original Grad: 0.172, -lr * Pred Grad:  0.025, New P: 0.564
iter 20 loss: 0.062
Actual params: [0.0875, 0.5643]
-Original Grad: 0.303, -lr * Pred Grad:  0.013, New P: 0.101
-Original Grad: 0.033, -lr * Pred Grad:  0.014, New P: 0.579
iter 21 loss: 0.060
Actual params: [0.1008, 0.5786]
-Original Grad: -0.091, -lr * Pred Grad:  0.002, New P: 0.102
-Original Grad: 0.155, -lr * Pred Grad:  0.020, New P: 0.598
iter 22 loss: 0.058
Actual params: [0.1025, 0.5981]
-Original Grad: 0.261, -lr * Pred Grad:  0.010, New P: 0.113
-Original Grad: -0.032, -lr * Pred Grad:  0.005, New P: 0.604
iter 23 loss: 0.057
Actual params: [0.1129, 0.6036]
-Original Grad: 0.200, -lr * Pred Grad:  0.011, New P: 0.124
-Original Grad: 0.045, -lr * Pred Grad:  0.017, New P: 0.620
iter 24 loss: 0.055
Actual params: [0.1241, 0.6204]
-Original Grad: 0.060, -lr * Pred Grad:  0.008, New P: 0.133
-Original Grad: 0.109, -lr * Pred Grad:  0.022, New P: 0.643
iter 25 loss: 0.052
Actual params: [0.1325, 0.6428]
-Original Grad: 0.631, -lr * Pred Grad:  0.022, New P: 0.154
-Original Grad: -0.106, -lr * Pred Grad:  0.012, New P: 0.655
iter 26 loss: 0.050
Actual params: [0.1545, 0.6553]
-Original Grad: 0.022, -lr * Pred Grad:  0.005, New P: 0.160
-Original Grad: 0.070, -lr * Pred Grad:  0.015, New P: 0.671
iter 27 loss: 0.049
Actual params: [0.1597, 0.6708]
-Original Grad: 0.233, -lr * Pred Grad:  0.010, New P: 0.170
-Original Grad: -0.011, -lr * Pred Grad:  0.012, New P: 0.683
iter 28 loss: 0.048
Actual params: [0.17  , 0.6831]
-Original Grad: -0.182, -lr * Pred Grad:  -0.003, New P: 0.167
-Original Grad: 0.075, -lr * Pred Grad:  0.006, New P: 0.689
iter 29 loss: 0.048
Actual params: [0.1667, 0.689 ]
-Original Grad: 0.363, -lr * Pred Grad:  0.014, New P: 0.181
-Original Grad: -0.045, -lr * Pred Grad:  0.015, New P: 0.704
iter 30 loss: 0.047
Actual params: [0.1805, 0.7038]
-Original Grad: 0.163, -lr * Pred Grad:  0.008, New P: 0.189
-Original Grad: -0.001, -lr * Pred Grad:  0.013, New P: 0.717
Target params: [1.3344, 1.5708]
iter 0 loss: 0.187
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad:  0.017, New P: -0.455
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.008
iter 1 loss: 0.187
Actual params: [-0.4552,  0.0076]
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: -0.442
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.011
iter 2 loss: 0.187
Actual params: [-0.442 ,  0.0112]
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: -0.424
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.016
iter 3 loss: 0.187
Actual params: [-0.4244,  0.016 ]
-Original Grad: 0.001, -lr * Pred Grad:  0.020, New P: -0.405
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.021
iter 4 loss: 0.187
Actual params: [-0.4047,  0.0215]
-Original Grad: 0.002, -lr * Pred Grad:  0.029, New P: -0.376
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.029
iter 5 loss: 0.187
Actual params: [-0.3759,  0.0293]
-Original Grad: 0.003, -lr * Pred Grad:  0.047, New P: -0.329
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.042
iter 6 loss: 0.186
Actual params: [-0.3285,  0.0417]
-Original Grad: 0.005, -lr * Pred Grad:  0.114, New P: -0.215
-Original Grad: 0.001, -lr * Pred Grad:  0.029, New P: 0.071
iter 7 loss: 0.186
Actual params: [-0.2146,  0.0708]
-Original Grad: 0.013, -lr * Pred Grad:  0.277, New P: 0.062
-Original Grad: 0.003, -lr * Pred Grad:  0.067, New P: 0.138
iter 8 loss: 0.171
Actual params: [0.0623, 0.138 ]
-Original Grad: 0.135, -lr * Pred Grad:  0.808, New P: 0.871
-Original Grad: 0.037, -lr * Pred Grad:  0.214, New P: 0.352
iter 9 loss: 0.297
Actual params: [0.8707, 0.3518]
-Original Grad: -0.083, -lr * Pred Grad:  -0.007, New P: 0.863
-Original Grad: 0.573, -lr * Pred Grad:  0.408, New P: 0.760
iter 10 loss: 0.163
Actual params: [0.8632, 0.7595]
-Original Grad: 0.031, -lr * Pred Grad:  0.181, New P: 1.045
-Original Grad: 0.257, -lr * Pred Grad:  0.153, New P: 0.913
iter 11 loss: 0.125
Actual params: [1.0446, 0.9127]
-Original Grad: 0.126, -lr * Pred Grad:  0.311, New P: 1.356
-Original Grad: 0.314, -lr * Pred Grad:  0.118, New P: 1.030
iter 12 loss: 0.070
Actual params: [1.356 , 1.0303]
-Original Grad: 0.237, -lr * Pred Grad:  0.256, New P: 1.612
-Original Grad: 0.092, -lr * Pred Grad:  0.009, New P: 1.040
iter 13 loss: 0.033
Actual params: [1.6123, 1.0397]
-Original Grad: 0.149, -lr * Pred Grad:  0.115, New P: 1.728
-Original Grad: 0.047, -lr * Pred Grad:  0.001, New P: 1.040
iter 14 loss: 0.026
Actual params: [1.7276, 1.0402]
-Original Grad: 0.080, -lr * Pred Grad:  0.055, New P: 1.783
-Original Grad: 0.058, -lr * Pred Grad:  0.016, New P: 1.056
iter 15 loss: 0.022
Actual params: [1.7828, 1.056 ]
-Original Grad: 0.121, -lr * Pred Grad:  0.062, New P: 1.845
-Original Grad: 0.106, -lr * Pred Grad:  0.025, New P: 1.081
iter 16 loss: 0.020
Actual params: [1.8448, 1.0807]
-Original Grad: 0.156, -lr * Pred Grad:  0.058, New P: 1.903
-Original Grad: 0.141, -lr * Pred Grad:  0.025, New P: 1.106
iter 17 loss: 0.019
Actual params: [1.903 , 1.1061]
-Original Grad: 0.001, -lr * Pred Grad:  -0.002, New P: 1.901
-Original Grad: 0.007, -lr * Pred Grad:  0.004, New P: 1.110
iter 18 loss: 0.019
Actual params: [1.9008, 1.11  ]
-Original Grad: -0.019, -lr * Pred Grad:  -0.009, New P: 1.892
-Original Grad: -0.013, -lr * Pred Grad:  0.001, New P: 1.111
iter 19 loss: 0.019
Actual params: [1.8922, 1.111 ]
-Original Grad: 0.005, -lr * Pred Grad:  0.000, New P: 1.893
-Original Grad: 0.007, -lr * Pred Grad:  0.002, New P: 1.113
iter 20 loss: 0.019
Actual params: [1.8926, 1.1132]
-Original Grad: -0.034, -lr * Pred Grad:  -0.024, New P: 1.869
-Original Grad: -0.013, -lr * Pred Grad:  0.012, New P: 1.126
iter 21 loss: 0.019
Actual params: [1.8691, 1.1256]
-Original Grad: 0.015, -lr * Pred Grad:  -0.000, New P: 1.869
-Original Grad: 0.020, -lr * Pred Grad:  0.006, New P: 1.132
iter 22 loss: 0.019
Actual params: [1.8688, 1.1318]
-Original Grad: 0.037, -lr * Pred Grad:  0.011, New P: 1.880
-Original Grad: 0.034, -lr * Pred Grad:  0.001, New P: 1.133
iter 23 loss: 0.019
Actual params: [1.8797, 1.1331]
-Original Grad: -0.011, -lr * Pred Grad:  -0.007, New P: 1.872
-Original Grad: -0.006, -lr * Pred Grad:  0.004, New P: 1.137
iter 24 loss: 0.019
Actual params: [1.8722, 1.1373]
-Original Grad: -0.021, -lr * Pred Grad:  -0.008, New P: 1.864
-Original Grad: -0.019, -lr * Pred Grad:  0.001, New P: 1.138
iter 25 loss: 0.019
Actual params: [1.8643, 1.1383]
-Original Grad: 0.027, -lr * Pred Grad:  -0.001, New P: 1.864
-Original Grad: 0.032, -lr * Pred Grad:  0.009, New P: 1.147
iter 26 loss: 0.020
Actual params: [1.8637, 1.1475]
-Original Grad: -0.061, -lr * Pred Grad:  -0.019, New P: 1.845
-Original Grad: -0.055, -lr * Pred Grad:  0.004, New P: 1.151
iter 27 loss: 0.020
Actual params: [1.845 , 1.1511]
-Original Grad: -0.016, -lr * Pred Grad:  -0.001, New P: 1.844
-Original Grad: -0.018, -lr * Pred Grad:  -0.003, New P: 1.148
iter 28 loss: 0.020
Actual params: [1.8441, 1.1478]
-Original Grad: -0.031, -lr * Pred Grad:  -0.001, New P: 1.843
-Original Grad: -0.033, -lr * Pred Grad:  -0.006, New P: 1.142
iter 29 loss: 0.019
Actual params: [1.8427, 1.1417]
-Original Grad: -0.017, -lr * Pred Grad:  -0.008, New P: 1.835
-Original Grad: -0.014, -lr * Pred Grad:  0.004, New P: 1.146
iter 30 loss: 0.019
Actual params: [1.835 , 1.1461]
-Original Grad: 0.063, -lr * Pred Grad:  0.003, New P: 1.839
-Original Grad: 0.066, -lr * Pred Grad:  0.009, New P: 1.155
Target params: [1.3344, 1.5708]
iter 0 loss: 0.422
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.087, -lr * Pred Grad:  0.465, New P: -0.007
-Original Grad: 0.115, -lr * Pred Grad:  0.606, New P: 0.609
iter 1 loss: 0.302
Actual params: [-0.007 ,  0.6093]
-Original Grad: 0.284, -lr * Pred Grad:  0.585, New P: 0.578
-Original Grad: 0.157, -lr * Pred Grad:  0.116, New P: 0.725
iter 2 loss: 0.142
Actual params: [0.578, 0.725]
-Original Grad: 0.496, -lr * Pred Grad:  0.113, New P: 0.691
-Original Grad: 0.405, -lr * Pred Grad:  0.262, New P: 0.987
iter 3 loss: 0.077
Actual params: [0.6907, 0.9866]
-Original Grad: 0.250, -lr * Pred Grad:  0.041, New P: 0.732
-Original Grad: 0.220, -lr * Pred Grad:  0.107, New P: 1.094
iter 4 loss: 0.058
Actual params: [0.7318, 1.0941]
-Original Grad: 0.298, -lr * Pred Grad:  0.110, New P: 0.842
-Original Grad: 0.194, -lr * Pred Grad:  -0.013, New P: 1.082
iter 5 loss: 0.050
Actual params: [0.8419, 1.0816]
-Original Grad: 0.054, -lr * Pred Grad:  -0.087, New P: 0.755
-Original Grad: 0.213, -lr * Pred Grad:  0.157, New P: 1.238
iter 6 loss: 0.043
Actual params: [0.7553, 1.2384]
-Original Grad: 0.185, -lr * Pred Grad:  0.039, New P: 0.794
-Original Grad: 0.154, -lr * Pred Grad:  0.028, New P: 1.267
iter 7 loss: 0.039
Actual params: [0.7939, 1.2667]
-Original Grad: 0.227, -lr * Pred Grad:  0.046, New P: 0.840
-Original Grad: 0.134, -lr * Pred Grad:  0.010, New P: 1.277
iter 8 loss: 0.036
Actual params: [0.8397, 1.2765]
-Original Grad: 0.040, -lr * Pred Grad:  0.007, New P: 0.846
-Original Grad: 0.032, -lr * Pred Grad:  0.005, New P: 1.281
iter 9 loss: 0.035
Actual params: [0.8465, 1.2814]
-Original Grad: 0.137, -lr * Pred Grad:  0.029, New P: 0.876
-Original Grad: 0.060, -lr * Pred Grad:  -0.003, New P: 1.279
iter 10 loss: 0.034
Actual params: [0.8757, 1.2786]
-Original Grad: 0.107, -lr * Pred Grad:  0.021, New P: 0.897
-Original Grad: 0.018, -lr * Pred Grad:  -0.009, New P: 1.270
iter 11 loss: 0.034
Actual params: [0.8967, 1.2696]
-Original Grad: 0.022, -lr * Pred Grad:  0.005, New P: 0.902
-Original Grad: -0.020, -lr * Pred Grad:  -0.008, New P: 1.261
iter 12 loss: 0.034
Actual params: [0.9019, 1.2615]
-Original Grad: 0.046, -lr * Pred Grad:  -0.001, New P: 0.901
-Original Grad: 0.084, -lr * Pred Grad:  0.020, New P: 1.281
iter 13 loss: 0.033
Actual params: [0.9013, 1.2811]
-Original Grad: 0.039, -lr * Pred Grad:  0.004, New P: 0.905
-Original Grad: 0.030, -lr * Pred Grad:  0.004, New P: 1.285
iter 14 loss: 0.033
Actual params: [0.9052, 1.2855]
-Original Grad: -0.061, -lr * Pred Grad:  -0.016, New P: 0.889
-Original Grad: 0.054, -lr * Pred Grad:  0.019, New P: 1.305
iter 15 loss: 0.033
Actual params: [0.8895, 1.3046]
-Original Grad: 0.153, -lr * Pred Grad:  0.015, New P: 0.904
-Original Grad: 0.069, -lr * Pred Grad:  0.006, New P: 1.311
iter 16 loss: 0.032
Actual params: [0.9041, 1.3107]
-Original Grad: 0.013, -lr * Pred Grad:  0.005, New P: 0.909
-Original Grad: -0.054, -lr * Pred Grad:  -0.013, New P: 1.298
iter 17 loss: 0.033
Actual params: [0.9093, 1.2977]
-Original Grad: -0.004, -lr * Pred Grad:  -0.005, New P: 0.905
-Original Grad: 0.063, -lr * Pred Grad:  0.014, New P: 1.312
iter 18 loss: 0.032
Actual params: [0.9047, 1.3116]
-Original Grad: 0.038, -lr * Pred Grad:  0.006, New P: 0.911
-Original Grad: -0.004, -lr * Pred Grad:  -0.003, New P: 1.308
iter 19 loss: 0.032
Actual params: [0.9106, 1.3082]
-Original Grad: 0.017, -lr * Pred Grad:  0.009, New P: 0.920
-Original Grad: -0.098, -lr * Pred Grad:  -0.021, New P: 1.287
iter 20 loss: 0.033
Actual params: [0.9201, 1.2869]
-Original Grad: 0.025, -lr * Pred Grad:  0.009, New P: 0.929
-Original Grad: -0.063, -lr * Pred Grad:  -0.015, New P: 1.272
iter 21 loss: 0.033
Actual params: [0.929 , 1.2718]
-Original Grad: 0.153, -lr * Pred Grad:  0.021, New P: 0.950
-Original Grad: 0.038, -lr * Pred Grad:  -0.002, New P: 1.269
iter 22 loss: 0.033
Actual params: [0.9501, 1.2694]
-Original Grad: -0.091, -lr * Pred Grad:  -0.008, New P: 0.942
-Original Grad: -0.008, -lr * Pred Grad:  0.002, New P: 1.272
iter 23 loss: 0.033
Actual params: [0.9425, 1.2715]
-Original Grad: -0.193, -lr * Pred Grad:  -0.013, New P: 0.930
-Original Grad: 0.006, -lr * Pred Grad:  0.007, New P: 1.279
iter 24 loss: 0.033
Actual params: [0.9297, 1.2787]
-Original Grad: -0.107, -lr * Pred Grad:  -0.007, New P: 0.923
-Original Grad: 0.018, -lr * Pred Grad:  0.006, New P: 1.285
iter 25 loss: 0.033
Actual params: [0.9226, 1.2849]
-Original Grad: 0.015, -lr * Pred Grad:  0.001, New P: 0.923
-Original Grad: 0.016, -lr * Pred Grad:  0.003, New P: 1.287
iter 26 loss: 0.033
Actual params: [0.9231, 1.2874]
-Original Grad: 0.017, -lr * Pred Grad:  0.001, New P: 0.925
-Original Grad: -0.007, -lr * Pred Grad:  -0.002, New P: 1.286
iter 27 loss: 0.033
Actual params: [0.9246, 1.2856]
-Original Grad: -0.014, -lr * Pred Grad:  -0.001, New P: 0.923
-Original Grad: 0.010, -lr * Pred Grad:  0.002, New P: 1.288
iter 28 loss: 0.033
Actual params: [0.9231, 1.2877]
-Original Grad: 0.013, -lr * Pred Grad:  0.001, New P: 0.925
-Original Grad: -0.010, -lr * Pred Grad:  -0.002, New P: 1.285
iter 29 loss: 0.033
Actual params: [0.9246, 1.2855]
-Original Grad: -0.034, -lr * Pred Grad:  -0.003, New P: 0.921
-Original Grad: 0.002, -lr * Pred Grad:  0.001, New P: 1.287
iter 30 loss: 0.033
Actual params: [0.9212, 1.287 ]
-Original Grad: 0.011, -lr * Pred Grad:  0.002, New P: 0.923
-Original Grad: -0.024, -lr * Pred Grad:  -0.005, New P: 1.282
Target params: [1.3344, 1.5708]
iter 0 loss: 0.565
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: -0.465
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.009
iter 1 loss: 0.565
Actual params: [-0.4655,  0.0088]
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: -0.458
-Original Grad: 0.001, -lr * Pred Grad:  0.006, New P: 0.015
iter 2 loss: 0.565
Actual params: [-0.4578,  0.015 ]
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.447
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.022
iter 3 loss: 0.565
Actual params: [-0.4473,  0.0223]
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: -0.436
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.032
iter 4 loss: 0.565
Actual params: [-0.4364,  0.0321]
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: -0.420
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.044
iter 5 loss: 0.565
Actual params: [-0.4203,  0.0441]
-Original Grad: 0.001, -lr * Pred Grad:  0.021, New P: -0.399
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.060
iter 6 loss: 0.565
Actual params: [-0.3993,  0.0599]
-Original Grad: 0.001, -lr * Pred Grad:  0.029, New P: -0.370
-Original Grad: 0.001, -lr * Pred Grad:  0.024, New P: 0.084
iter 7 loss: 0.565
Actual params: [-0.3703,  0.0841]
-Original Grad: 0.002, -lr * Pred Grad:  0.046, New P: -0.324
-Original Grad: 0.002, -lr * Pred Grad:  0.039, New P: 0.123
iter 8 loss: 0.564
Actual params: [-0.3244,  0.1231]
-Original Grad: 0.003, -lr * Pred Grad:  0.089, New P: -0.235
-Original Grad: 0.003, -lr * Pred Grad:  0.067, New P: 0.190
iter 9 loss: 0.563
Actual params: [-0.235,  0.19 ]
-Original Grad: 0.007, -lr * Pred Grad:  0.185, New P: -0.050
-Original Grad: 0.007, -lr * Pred Grad:  0.198, New P: 0.388
iter 10 loss: 0.550
Actual params: [-0.0503,  0.3881]
-Original Grad: 0.025, -lr * Pred Grad:  0.658, New P: 0.608
-Original Grad: 0.029, -lr * Pred Grad:  0.756, New P: 1.144
iter 11 loss: 0.273
Actual params: [0.6075, 1.1441]
-Original Grad: 0.224, -lr * Pred Grad:  0.614, New P: 1.222
-Original Grad: 0.186, -lr * Pred Grad:  0.391, New P: 1.535
iter 12 loss: 0.057
Actual params: [1.2216, 1.5351]
-Original Grad: 0.049, -lr * Pred Grad:  0.011, New P: 1.233
-Original Grad: 0.179, -lr * Pred Grad:  0.248, New P: 1.783
iter 13 loss: 0.036
Actual params: [1.2331, 1.7829]
-Original Grad: -0.025, -lr * Pred Grad:  -0.088, New P: 1.145
-Original Grad: 0.056, -lr * Pred Grad:  0.085, New P: 1.868
iter 14 loss: 0.033
Actual params: [1.1446, 1.8681]
-Original Grad: 0.149, -lr * Pred Grad:  0.107, New P: 1.252
-Original Grad: 0.063, -lr * Pred Grad:  0.030, New P: 1.898
iter 15 loss: 0.030
Actual params: [1.2517, 1.8984]
-Original Grad: 0.017, -lr * Pred Grad:  -0.007, New P: 1.245
-Original Grad: 0.064, -lr * Pred Grad:  0.072, New P: 1.970
iter 16 loss: 0.027
Actual params: [1.2448, 1.9702]
-Original Grad: 0.034, -lr * Pred Grad:  0.004, New P: 1.248
-Original Grad: 0.065, -lr * Pred Grad:  0.063, New P: 2.033
iter 17 loss: 0.025
Actual params: [1.2485, 2.0333]
-Original Grad: -0.003, -lr * Pred Grad:  -0.017, New P: 1.232
-Original Grad: 0.047, -lr * Pred Grad:  0.050, New P: 2.083
iter 18 loss: 0.027
Actual params: [1.2315, 2.0833]
-Original Grad: 0.017, -lr * Pred Grad:  0.007, New P: 1.239
-Original Grad: 0.010, -lr * Pred Grad:  0.007, New P: 2.090
iter 19 loss: 0.027
Actual params: [1.2386, 2.0904]
-Original Grad: -0.024, -lr * Pred Grad:  -0.014, New P: 1.225
-Original Grad: 0.027, -lr * Pred Grad:  0.029, New P: 2.120
iter 20 loss: 0.030
Actual params: [1.2247, 2.1197]
-Original Grad: 0.006, -lr * Pred Grad:  -0.000, New P: 1.224
-Original Grad: 0.033, -lr * Pred Grad:  0.034, New P: 2.154
iter 21 loss: 0.033
Actual params: [1.2244, 2.1539]
-Original Grad: 0.270, -lr * Pred Grad:  0.033, New P: 1.258
-Original Grad: -0.155, -lr * Pred Grad:  -0.025, New P: 2.129
iter 22 loss: 0.026
Actual params: [1.2576, 2.129 ]
-Original Grad: 0.041, -lr * Pred Grad:  0.018, New P: 1.275
-Original Grad: 0.054, -lr * Pred Grad:  0.034, New P: 2.163
iter 23 loss: 0.026
Actual params: [1.2754, 2.1627]
-Original Grad: 0.023, -lr * Pred Grad:  0.010, New P: 1.285
-Original Grad: 0.044, -lr * Pred Grad:  0.023, New P: 2.186
iter 24 loss: 0.027
Actual params: [1.2849, 2.1856]
-Original Grad: 0.001, -lr * Pred Grad:  0.003, New P: 1.288
-Original Grad: 0.020, -lr * Pred Grad:  0.010, New P: 2.196
iter 25 loss: 0.028
Actual params: [1.2877, 2.1957]
-Original Grad: -0.010, -lr * Pred Grad:  0.003, New P: 1.291
-Original Grad: 0.036, -lr * Pred Grad:  0.018, New P: 2.214
iter 26 loss: 0.029
Actual params: [1.2905, 2.2137]
-Original Grad: 0.091, -lr * Pred Grad:  0.008, New P: 1.298
-Original Grad: -0.066, -lr * Pred Grad:  -0.014, New P: 2.199
iter 27 loss: 0.027
Actual params: [1.2984, 2.1993]
-Original Grad: 0.040, -lr * Pred Grad:  0.016, New P: 1.314
-Original Grad: 0.039, -lr * Pred Grad:  0.026, New P: 2.225
iter 28 loss: 0.027
Actual params: [1.314 , 2.2251]
-Original Grad: 0.005, -lr * Pred Grad:  -0.001, New P: 1.313
-Original Grad: -0.013, -lr * Pred Grad:  -0.005, New P: 2.220
iter 29 loss: 0.027
Actual params: [1.3129, 2.22  ]
-Original Grad: 0.035, -lr * Pred Grad:  0.009, New P: 1.322
-Original Grad: 0.006, -lr * Pred Grad:  0.008, New P: 2.228
iter 30 loss: 0.027
Actual params: [1.3217, 2.2281]
-Original Grad: -0.017, -lr * Pred Grad:  0.001, New P: 1.323
-Original Grad: 0.031, -lr * Pred Grad:  0.011, New P: 2.239
Target params: [1.3344, 1.5708]
iter 0 loss: 0.548
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.005, -lr * Pred Grad:  -0.051, New P: -0.524
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.003
iter 1 loss: 0.548
Actual params: [-0.5238,  0.0033]
-Original Grad: -0.003, -lr * Pred Grad:  -0.034, New P: -0.558
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.003
iter 2 loss: 0.548
Actual params: [-0.5581,  0.003 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.028, New P: -0.586
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.003
iter 3 loss: 0.548
Actual params: [-0.5864,  0.0031]
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -0.608
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.004
iter 4 loss: 0.548
Actual params: [-0.6081,  0.0037]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.625
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.004
iter 5 loss: 0.548
Actual params: [-0.6249,  0.0043]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.643
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.005
iter 6 loss: 0.548
Actual params: [-0.6434,  0.0047]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.660
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.005
iter 7 loss: 0.548
Actual params: [-0.6603,  0.0051]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.677
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.006
iter 8 loss: 0.548
Actual params: [-0.6771,  0.0057]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.694
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.006
iter 9 loss: 0.548
Actual params: [-0.6938,  0.0061]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.709
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.006
iter 10 loss: 0.548
Actual params: [-0.709 ,  0.0061]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.724
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.006
iter 11 loss: 0.548
Actual params: [-0.7245,  0.006 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.738
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.006
iter 12 loss: 0.548
Actual params: [-0.7377,  0.0061]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.751
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.006
iter 13 loss: 0.548
Actual params: [-0.7514,  0.0058]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.763
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.006
iter 14 loss: 0.548
Actual params: [-0.7629,  0.0061]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.777
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.006
iter 15 loss: 0.548
Actual params: [-0.7767,  0.0061]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.792
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.005
iter 16 loss: 0.548
Actual params: [-0.7917,  0.0049]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.806
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.004
iter 17 loss: 0.548
Actual params: [-0.8063,  0.0037]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.820
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.003
iter 18 loss: 0.548
Actual params: [-0.8202,  0.003 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.829
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.003
iter 19 loss: 0.548
Actual params: [-0.8295,  0.0029]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.842
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.002
iter 20 loss: 0.548
Actual params: [-0.8415,  0.0023]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.856
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 0.001
iter 21 loss: 0.548
Actual params: [-8.5621e-01,  5.7966e-04]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.868
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.000
iter 22 loss: 0.548
Actual params: [-8.6843e-01, -2.2017e-04]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.883
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.002
iter 23 loss: 0.548
Actual params: [-0.8829, -0.0019]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.897
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.003
iter 24 loss: 0.548
Actual params: [-0.8967, -0.003 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.908
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.004
iter 25 loss: 0.548
Actual params: [-0.9077, -0.004 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.921
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.005
iter 26 loss: 0.548
Actual params: [-0.9213, -0.0055]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.935
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.007
iter 27 loss: 0.548
Actual params: [-0.9349, -0.0071]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.948
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.009
iter 28 loss: 0.548
Actual params: [-0.9477, -0.0087]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.961
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.010
iter 29 loss: 0.548
Actual params: [-0.9612, -0.0102]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.974
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.012
iter 30 loss: 0.548
Actual params: [-0.9743, -0.0119]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.986
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.014
Target params: [1.3344, 1.5708]
iter 0 loss: 0.288
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.060, -lr * Pred Grad:  -0.370, New P: -0.842
-Original Grad: 0.114, -lr * Pred Grad:  0.705, New P: 0.708
iter 1 loss: 0.260
Actual params: [-0.8423,  0.7082]
-Original Grad: 0.029, -lr * Pred Grad:  0.291, New P: -0.551
-Original Grad: -0.008, -lr * Pred Grad:  0.000, New P: 0.708
iter 2 loss: 0.252
Actual params: [-0.5511,  0.7084]
-Original Grad: 0.063, -lr * Pred Grad:  0.634, New P: 0.083
-Original Grad: 0.016, -lr * Pred Grad:  0.239, New P: 0.948
iter 3 loss: 0.193
Actual params: [0.0827, 0.9476]
-Original Grad: 0.155, -lr * Pred Grad:  0.547, New P: 0.630
-Original Grad: 0.101, -lr * Pred Grad:  0.383, New P: 1.330
iter 4 loss: 0.047
Actual params: [0.6298, 1.3304]
-Original Grad: 0.269, -lr * Pred Grad:  0.144, New P: 0.774
-Original Grad: 0.139, -lr * Pred Grad:  0.084, New P: 1.415
iter 5 loss: 0.025
Actual params: [0.774 , 1.4147]
-Original Grad: 0.102, -lr * Pred Grad:  -0.028, New P: 0.746
-Original Grad: 0.114, -lr * Pred Grad:  0.170, New P: 1.585
iter 6 loss: 0.014
Actual params: [0.7461, 1.5845]
-Original Grad: 0.163, -lr * Pred Grad:  0.073, New P: 0.819
-Original Grad: 0.039, -lr * Pred Grad:  -0.055, New P: 1.529
iter 7 loss: 0.013
Actual params: [0.8194, 1.5292]
-Original Grad: 0.043, -lr * Pred Grad:  -0.024, New P: 0.796
-Original Grad: 0.082, -lr * Pred Grad:  0.095, New P: 1.624
iter 8 loss: 0.010
Actual params: [0.7959, 1.6241]
-Original Grad: 0.096, -lr * Pred Grad:  0.022, New P: 0.818
-Original Grad: 0.038, -lr * Pred Grad:  0.008, New P: 1.632
iter 9 loss: 0.008
Actual params: [0.8179, 1.6324]
-Original Grad: 0.071, -lr * Pred Grad:  0.007, New P: 0.825
-Original Grad: 0.055, -lr * Pred Grad:  0.033, New P: 1.666
iter 10 loss: 0.007
Actual params: [0.8246, 1.6658]
-Original Grad: 0.100, -lr * Pred Grad:  0.018, New P: 0.842
-Original Grad: 0.024, -lr * Pred Grad:  0.001, New P: 1.667
iter 11 loss: 0.006
Actual params: [0.8424, 1.6666]
-Original Grad: 0.049, -lr * Pred Grad:  0.003, New P: 0.845
-Original Grad: 0.048, -lr * Pred Grad:  0.029, New P: 1.696
iter 12 loss: 0.006
Actual params: [0.845 , 1.6956]
-Original Grad: 0.077, -lr * Pred Grad:  0.012, New P: 0.857
-Original Grad: 0.027, -lr * Pred Grad:  0.009, New P: 1.704
iter 13 loss: 0.005
Actual params: [0.8567, 1.7044]
-Original Grad: 0.027, -lr * Pred Grad:  -0.000, New P: 0.856
-Original Grad: 0.050, -lr * Pred Grad:  0.027, New P: 1.732
iter 14 loss: 0.005
Actual params: [0.8563, 1.7318]
-Original Grad: 0.081, -lr * Pred Grad:  0.014, New P: 0.870
-Original Grad: 0.014, -lr * Pred Grad:  -0.000, New P: 1.732
iter 15 loss: 0.005
Actual params: [0.87  , 1.7317]
-Original Grad: 0.048, -lr * Pred Grad:  0.006, New P: 0.876
-Original Grad: 0.031, -lr * Pred Grad:  0.014, New P: 1.746
iter 16 loss: 0.004
Actual params: [0.8757, 1.7461]
-Original Grad: 0.013, -lr * Pred Grad:  0.002, New P: 0.877
-Original Grad: 0.002, -lr * Pred Grad:  -0.000, New P: 1.746
iter 17 loss: 0.004
Actual params: [0.8774, 1.7458]
-Original Grad: -0.029, -lr * Pred Grad:  -0.004, New P: 0.874
-Original Grad: -0.005, -lr * Pred Grad:  0.001, New P: 1.747
iter 18 loss: 0.004
Actual params: [0.8737, 1.7468]
-Original Grad: 0.058, -lr * Pred Grad:  0.006, New P: 0.880
-Original Grad: 0.013, -lr * Pred Grad:  0.001, New P: 1.748
iter 19 loss: 0.004
Actual params: [0.8797, 1.7481]
-Original Grad: 0.087, -lr * Pred Grad:  0.009, New P: 0.889
-Original Grad: 0.012, -lr * Pred Grad:  -0.002, New P: 1.747
iter 20 loss: 0.004
Actual params: [0.8886, 1.7466]
-Original Grad: -0.024, -lr * Pred Grad:  -0.004, New P: 0.885
-Original Grad: 0.009, -lr * Pred Grad:  0.008, New P: 1.754
iter 21 loss: 0.004
Actual params: [0.8848, 1.7542]
-Original Grad: 0.010, -lr * Pred Grad:  0.001, New P: 0.886
-Original Grad: 0.002, -lr * Pred Grad:  0.000, New P: 1.754
iter 22 loss: 0.004
Actual params: [0.8857, 1.7544]
-Original Grad: -0.007, -lr * Pred Grad:  0.001, New P: 0.886
-Original Grad: -0.017, -lr * Pred Grad:  -0.007, New P: 1.747
iter 23 loss: 0.004
Actual params: [0.8864, 1.7472]
-Original Grad: 0.031, -lr * Pred Grad:  0.002, New P: 0.889
-Original Grad: 0.014, -lr * Pred Grad:  0.004, New P: 1.751
iter 24 loss: 0.004
Actual params: [0.8886, 1.7515]
-Original Grad: 0.042, -lr * Pred Grad:  0.005, New P: 0.894
-Original Grad: -0.006, -lr * Pred Grad:  -0.006, New P: 1.746
iter 25 loss: 0.004
Actual params: [0.8937, 1.7456]
-Original Grad: 0.050, -lr * Pred Grad:  0.004, New P: 0.898
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: 1.741
iter 26 loss: 0.004
Actual params: [0.8982, 1.7413]
-Original Grad: 0.007, -lr * Pred Grad:  0.000, New P: 0.899
-Original Grad: 0.006, -lr * Pred Grad:  0.002, New P: 1.743
iter 27 loss: 0.004
Actual params: [0.8985, 1.7434]
-Original Grad: -0.033, -lr * Pred Grad:  -0.001, New P: 0.898
-Original Grad: -0.036, -lr * Pred Grad:  -0.013, New P: 1.731
iter 28 loss: 0.004
Actual params: [0.8976, 1.7308]
-Original Grad: 0.018, -lr * Pred Grad:  0.001, New P: 0.898
-Original Grad: 0.020, -lr * Pred Grad:  0.007, New P: 1.738
iter 29 loss: 0.004
Actual params: [0.8982, 1.7382]
-Original Grad: -0.039, -lr * Pred Grad:  -0.004, New P: 0.895
-Original Grad: -0.003, -lr * Pred Grad:  0.001, New P: 1.739
iter 30 loss: 0.004
Actual params: [0.8946, 1.7394]
-Original Grad: 0.004, -lr * Pred Grad:  0.000, New P: 0.895
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 1.739
Target params: [1.3344, 1.5708]
iter 0 loss: 0.879
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.164, -lr * Pred Grad:  -0.402, New P: -0.874
-Original Grad: 0.077, -lr * Pred Grad:  0.190, New P: 0.193
iter 1 loss: 0.837
Actual params: [-0.8739,  0.1932]
-Original Grad: -0.050, -lr * Pred Grad:  -0.018, New P: -0.892
-Original Grad: 0.054, -lr * Pred Grad:  0.331, New P: 0.524
iter 2 loss: 0.829
Actual params: [-0.892 ,  0.5241]
-Original Grad: -0.008, -lr * Pred Grad:  0.036, New P: -0.856
-Original Grad: 0.018, -lr * Pred Grad:  0.154, New P: 0.679
iter 3 loss: 0.827
Actual params: [-0.8563,  0.6785]
-Original Grad: 0.000, -lr * Pred Grad:  0.034, New P: -0.823
-Original Grad: 0.007, -lr * Pred Grad:  0.083, New P: 0.761
iter 4 loss: 0.827
Actual params: [-0.8227,  0.7615]
-Original Grad: 0.003, -lr * Pred Grad:  0.035, New P: -0.788
-Original Grad: 0.004, -lr * Pred Grad:  0.062, New P: 0.824
iter 5 loss: 0.827
Actual params: [-0.7878,  0.8235]
-Original Grad: 0.004, -lr * Pred Grad:  0.038, New P: -0.750
-Original Grad: 0.002, -lr * Pred Grad:  0.053, New P: 0.876
iter 6 loss: 0.827
Actual params: [-0.7498,  0.8761]
-Original Grad: 0.005, -lr * Pred Grad:  0.043, New P: -0.707
-Original Grad: 0.001, -lr * Pred Grad:  0.047, New P: 0.923
iter 7 loss: 0.826
Actual params: [-0.7068,  0.9232]
-Original Grad: 0.006, -lr * Pred Grad:  0.061, New P: -0.645
-Original Grad: 0.001, -lr * Pred Grad:  0.066, New P: 0.989
iter 8 loss: 0.826
Actual params: [-0.6453,  0.9894]
-Original Grad: 0.008, -lr * Pred Grad:  0.088, New P: -0.558
-Original Grad: 0.001, -lr * Pred Grad:  0.091, New P: 1.081
iter 9 loss: 0.825
Actual params: [-0.5577,  1.0809]
-Original Grad: 0.018, -lr * Pred Grad:  0.215, New P: -0.342
-Original Grad: 0.005, -lr * Pred Grad:  0.246, New P: 1.327
iter 10 loss: 0.815
Actual params: [-0.3423,  1.3267]
-Original Grad: 0.103, -lr * Pred Grad:  0.637, New P: 0.295
-Original Grad: 0.031, -lr * Pred Grad:  0.749, New P: 2.075
iter 11 loss: 0.571
Actual params: [0.2951, 2.0753]
-Original Grad: 0.593, -lr * Pred Grad:  0.333, New P: 0.628
-Original Grad: -0.163, -lr * Pred Grad:  -0.140, New P: 1.935
iter 12 loss: 0.258
Actual params: [0.6283, 1.9354]
-Original Grad: 0.501, -lr * Pred Grad:  0.130, New P: 0.758
-Original Grad: -0.212, -lr * Pred Grad:  -0.055, New P: 1.880
iter 13 loss: 0.161
Actual params: [0.7579, 1.8803]
-Original Grad: 0.476, -lr * Pred Grad:  -0.005, New P: 0.753
-Original Grad: -0.341, -lr * Pred Grad:  -0.176, New P: 1.704
iter 14 loss: 0.132
Actual params: [0.7525, 1.7041]
-Original Grad: 0.284, -lr * Pred Grad:  0.110, New P: 0.862
-Original Grad: 0.001, -lr * Pred Grad:  0.160, New P: 1.864
iter 15 loss: 0.117
Actual params: [0.8622, 1.8637]
-Original Grad: 0.169, -lr * Pred Grad:  -0.021, New P: 0.841
-Original Grad: -0.216, -lr * Pred Grad:  -0.111, New P: 1.753
iter 16 loss: 0.109
Actual params: [0.8413, 1.7528]
-Original Grad: 0.223, -lr * Pred Grad:  0.030, New P: 0.872
-Original Grad: -0.103, -lr * Pred Grad:  -0.008, New P: 1.744
iter 17 loss: 0.099
Actual params: [0.8718, 1.7444]
-Original Grad: 0.461, -lr * Pred Grad:  0.033, New P: 0.905
-Original Grad: -0.052, -lr * Pred Grad:  0.016, New P: 1.760
iter 18 loss: 0.092
Actual params: [0.9047, 1.7603]
-Original Grad: 0.107, -lr * Pred Grad:  -0.001, New P: 0.904
-Original Grad: -0.135, -lr * Pred Grad:  -0.045, New P: 1.716
iter 19 loss: 0.088
Actual params: [0.904 , 1.7157]
-Original Grad: 0.253, -lr * Pred Grad:  0.019, New P: 0.923
-Original Grad: -0.045, -lr * Pred Grad:  -0.000, New P: 1.715
iter 20 loss: 0.084
Actual params: [0.9228, 1.7155]
-Original Grad: 0.641, -lr * Pred Grad:  0.019, New P: 0.942
-Original Grad: -0.139, -lr * Pred Grad:  -0.022, New P: 1.693
iter 21 loss: 0.079
Actual params: [0.9418, 1.693 ]
-Original Grad: 0.136, -lr * Pred Grad:  0.003, New P: 0.945
-Original Grad: -0.082, -lr * Pred Grad:  -0.021, New P: 1.672
iter 22 loss: 0.077
Actual params: [0.9446, 1.6721]
-Original Grad: 0.214, -lr * Pred Grad:  0.010, New P: 0.954
-Original Grad: 0.016, -lr * Pred Grad:  0.013, New P: 1.685
iter 23 loss: 0.076
Actual params: [0.9544, 1.6848]
-Original Grad: 0.415, -lr * Pred Grad:  0.018, New P: 0.972
-Original Grad: 0.006, -lr * Pred Grad:  0.017, New P: 1.702
iter 24 loss: 0.075
Actual params: [0.9725, 1.7017]
-Original Grad: 0.103, -lr * Pred Grad:  0.003, New P: 0.976
-Original Grad: -0.047, -lr * Pred Grad:  -0.014, New P: 1.688
iter 25 loss: 0.073
Actual params: [0.9758, 1.6878]
-Original Grad: 0.097, -lr * Pred Grad:  0.003, New P: 0.979
-Original Grad: -0.055, -lr * Pred Grad:  -0.017, New P: 1.671
iter 26 loss: 0.072
Actual params: [0.9791, 1.6712]
-Original Grad: -0.018, -lr * Pred Grad:  -0.004, New P: 0.975
-Original Grad: -0.142, -lr * Pred Grad:  -0.042, New P: 1.629
iter 27 loss: 0.070
Actual params: [0.9755, 1.6289]
-Original Grad: 0.146, -lr * Pred Grad:  0.008, New P: 0.984
-Original Grad: -0.010, -lr * Pred Grad:  -0.001, New P: 1.628
iter 28 loss: 0.069
Actual params: [0.9838, 1.6282]
-Original Grad: 0.012, -lr * Pred Grad:  -0.000, New P: 0.984
-Original Grad: -0.093, -lr * Pred Grad:  -0.026, New P: 1.603
iter 29 loss: 0.067
Actual params: [0.9837, 1.6026]
-Original Grad: 0.014, -lr * Pred Grad:  0.001, New P: 0.984
-Original Grad: -0.078, -lr * Pred Grad:  -0.020, New P: 1.582
iter 30 loss: 0.066
Actual params: [0.9843, 1.5824]
-Original Grad: -0.030, -lr * Pred Grad:  -0.002, New P: 0.983
-Original Grad: -0.083, -lr * Pred Grad:  -0.020, New P: 1.562
Target params: [1.3344, 1.5708]
iter 0 loss: 0.118
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.112, -lr * Pred Grad:  -0.545, New P: -1.017
-Original Grad: 0.006, -lr * Pred Grad:  0.025, New P: 0.029
iter 1 loss: 0.106
Actual params: [-1.0169,  0.0286]
-Original Grad: -0.002, -lr * Pred Grad:  -0.010, New P: -1.027
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.030
iter 2 loss: 0.106
Actual params: [-1.0266,  0.0303]
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -1.035
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.035
iter 3 loss: 0.106
Actual params: [-1.0349,  0.0345]
-Original Grad: -0.002, -lr * Pred Grad:  -0.011, New P: -1.045
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.038
iter 4 loss: 0.106
Actual params: [-1.0454,  0.038 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.011, New P: -1.057
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.041
iter 5 loss: 0.106
Actual params: [-1.0566,  0.0409]
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -1.066
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.044
iter 6 loss: 0.106
Actual params: [-1.0656,  0.0444]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -1.075
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.048
iter 7 loss: 0.106
Actual params: [-1.0753,  0.0485]
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -1.084
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.055
iter 8 loss: 0.106
Actual params: [-1.0839,  0.0546]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -1.095
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.057
iter 9 loss: 0.106
Actual params: [-1.0953,  0.0573]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -1.106
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.062
iter 10 loss: 0.106
Actual params: [-1.1063,  0.0616]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -1.120
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.064
iter 11 loss: 0.106
Actual params: [-1.1198,  0.0639]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -1.132
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.067
iter 12 loss: 0.106
Actual params: [-1.132 ,  0.0674]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -1.144
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.071
iter 13 loss: 0.106
Actual params: [-1.1439,  0.0714]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -1.156
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.077
iter 14 loss: 0.106
Actual params: [-1.1561,  0.0767]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -1.167
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.081
iter 15 loss: 0.106
Actual params: [-1.1673,  0.0807]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.179
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.086
iter 16 loss: 0.106
Actual params: [-1.1789,  0.0855]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.192
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.091
iter 17 loss: 0.106
Actual params: [-1.1919,  0.0914]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.206
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.096
iter 18 loss: 0.106
Actual params: [-1.2059,  0.0958]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.219
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.102
iter 19 loss: 0.106
Actual params: [-1.2187,  0.1021]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.230
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.107
iter 20 loss: 0.106
Actual params: [-1.2304,  0.1074]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.243
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.112
iter 21 loss: 0.106
Actual params: [-1.2427,  0.1117]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.252
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.115
iter 22 loss: 0.106
Actual params: [-1.2519,  0.1148]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.263
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.122
iter 23 loss: 0.106
Actual params: [-1.2632,  0.1217]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.276
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.127
iter 24 loss: 0.106
Actual params: [-1.2761,  0.1274]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.286
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.133
iter 25 loss: 0.106
Actual params: [-1.2856,  0.1335]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.296
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.139
iter 26 loss: 0.106
Actual params: [-1.296 ,  0.1389]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.310
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.147
iter 27 loss: 0.106
Actual params: [-1.3099,  0.1475]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.322
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.154
iter 28 loss: 0.106
Actual params: [-1.3218,  0.1539]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.339
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.163
iter 29 loss: 0.106
Actual params: [-1.3393,  0.1625]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.355
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.170
iter 30 loss: 0.106
Actual params: [-1.3549,  0.1702]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.368
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.180
Target params: [1.3344, 1.5708]
iter 0 loss: 0.194
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.038, New P: -0.510
-Original Grad: 0.003, -lr * Pred Grad:  0.031, New P: 0.035
iter 1 loss: 0.192
Actual params: [-0.5101,  0.035 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.030, New P: -0.540
-Original Grad: 0.002, -lr * Pred Grad:  0.026, New P: 0.061
iter 2 loss: 0.190
Actual params: [-0.5397,  0.0608]
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.562
-Original Grad: 0.002, -lr * Pred Grad:  0.022, New P: 0.082
iter 3 loss: 0.188
Actual params: [-0.5625,  0.0824]
-Original Grad: -0.006, -lr * Pred Grad:  -0.084, New P: -0.646
-Original Grad: 0.005, -lr * Pred Grad:  0.076, New P: 0.159
iter 4 loss: 0.184
Actual params: [-0.6464,  0.1587]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.664
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: 0.177
iter 5 loss: 0.183
Actual params: [-0.6644,  0.1765]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -0.685
-Original Grad: 0.001, -lr * Pred Grad:  0.021, New P: 0.197
iter 6 loss: 0.183
Actual params: [-0.6848,  0.1973]
-Original Grad: -0.002, -lr * Pred Grad:  -0.035, New P: -0.720
-Original Grad: 0.002, -lr * Pred Grad:  0.035, New P: 0.232
iter 7 loss: 0.182
Actual params: [-0.7198,  0.2325]
-Original Grad: -0.001, -lr * Pred Grad:  -0.029, New P: -0.748
-Original Grad: 0.001, -lr * Pred Grad:  0.031, New P: 0.264
iter 8 loss: 0.181
Actual params: [-0.7484,  0.2636]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.762
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.278
iter 9 loss: 0.181
Actual params: [-0.7621,  0.2777]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.779
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 0.295
iter 10 loss: 0.180
Actual params: [-0.779 ,  0.2948]
-Original Grad: -0.001, -lr * Pred Grad:  -0.037, New P: -0.816
-Original Grad: 0.001, -lr * Pred Grad:  0.039, New P: 0.334
iter 11 loss: 0.180
Actual params: [-0.8161,  0.3341]
-Original Grad: -0.001, -lr * Pred Grad:  -0.049, New P: -0.865
-Original Grad: 0.002, -lr * Pred Grad:  0.055, New P: 0.389
iter 12 loss: 0.179
Actual params: [-0.8647,  0.389 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.026, New P: -0.890
-Original Grad: 0.001, -lr * Pred Grad:  0.027, New P: 0.416
iter 13 loss: 0.179
Actual params: [-0.8905,  0.4159]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.908
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.433
iter 14 loss: 0.179
Actual params: [-0.9078,  0.4331]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -0.927
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.454
iter 15 loss: 0.179
Actual params: [-0.9272,  0.4536]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -0.949
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.474
iter 16 loss: 0.179
Actual params: [-0.9486,  0.4744]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.963
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.485
iter 17 loss: 0.179
Actual params: [-0.9629,  0.4853]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -0.987
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 0.513
iter 18 loss: 0.179
Actual params: [-0.9867,  0.5133]
-Original Grad: -0.001, -lr * Pred Grad:  -0.037, New P: -1.024
-Original Grad: 0.001, -lr * Pred Grad:  0.043, New P: 0.556
iter 19 loss: 0.178
Actual params: [-1.0239,  0.556 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.050
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: 0.585
iter 20 loss: 0.178
Actual params: [-1.0502,  0.5847]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.065
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.599
iter 21 loss: 0.178
Actual params: [-1.0649,  0.5985]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.090
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 0.625
iter 22 loss: 0.178
Actual params: [-1.09  ,  0.6247]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.110
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.642
iter 23 loss: 0.178
Actual params: [-1.1096,  0.6418]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.128
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.658
iter 24 loss: 0.178
Actual params: [-1.1284,  0.6579]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.150
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.677
iter 25 loss: 0.178
Actual params: [-1.1497,  0.6767]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.166
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.692
iter 26 loss: 0.178
Actual params: [-1.1663,  0.6915]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.192
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.714
iter 27 loss: 0.178
Actual params: [-1.192 ,  0.7142]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.218
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.738
iter 28 loss: 0.178
Actual params: [-1.2184,  0.738 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.043, New P: -1.262
-Original Grad: 0.000, -lr * Pred Grad:  0.041, New P: 0.779
iter 29 loss: 0.178
Actual params: [-1.2618,  0.7786]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.284
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.800
iter 30 loss: 0.178
Actual params: [-1.2842,  0.8001]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.311
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.826
Target params: [1.3344, 1.5708]
iter 0 loss: 0.188
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.064, -lr * Pred Grad:  0.516, New P: 0.043
-Original Grad: -0.023, -lr * Pred Grad:  -0.170, New P: -0.167
iter 1 loss: 0.141
Actual params: [ 0.0434, -0.1667]
-Original Grad: -0.154, -lr * Pred Grad:  0.121, New P: 0.164
-Original Grad: 0.224, -lr * Pred Grad:  0.387, New P: 0.220
iter 2 loss: 0.081
Actual params: [0.1642, 0.2203]
-Original Grad: 0.244, -lr * Pred Grad:  0.212, New P: 0.376
-Original Grad: 0.049, -lr * Pred Grad:  0.220, New P: 0.441
iter 3 loss: 0.203
Actual params: [0.3762, 0.4406]
-Original Grad: -0.981, -lr * Pred Grad:  -0.088, New P: 0.288
-Original Grad: 0.693, -lr * Pred Grad:  0.137, New P: 0.578
iter 4 loss: 0.087
Actual params: [0.2881, 0.5775]
-Original Grad: -3.839, -lr * Pred Grad:  -0.011, New P: 0.277
-Original Grad: 0.662, -lr * Pred Grad:  0.043, New P: 0.620
iter 5 loss: 0.068
Actual params: [0.2772, 0.6201]
-Original Grad: -1.223, -lr * Pred Grad:  0.000, New P: 0.277
-Original Grad: 0.253, -lr * Pred Grad:  0.033, New P: 0.653
iter 6 loss: 0.063
Actual params: [0.2773, 0.6527]
-Original Grad: -1.646, -lr * Pred Grad:  -0.007, New P: 0.270
-Original Grad: 0.210, -lr * Pred Grad:  -0.014, New P: 0.639
iter 7 loss: 0.060
Actual params: [0.2698, 0.6389]
-Original Grad: -0.763, -lr * Pred Grad:  -0.001, New P: 0.269
-Original Grad: 0.122, -lr * Pred Grad:  0.008, New P: 0.647
iter 8 loss: 0.059
Actual params: [0.2686, 0.6468]
-Original Grad: 0.230, -lr * Pred Grad:  0.005, New P: 0.273
-Original Grad: 0.020, -lr * Pred Grad:  0.029, New P: 0.676
iter 9 loss: 0.059
Actual params: [0.2734, 0.676 ]
-Original Grad: -0.507, -lr * Pred Grad:  -0.002, New P: 0.272
-Original Grad: 0.068, -lr * Pred Grad:  0.000, New P: 0.676
iter 10 loss: 0.058
Actual params: [0.2717, 0.6762]
-Original Grad: -0.920, -lr * Pred Grad:  -0.002, New P: 0.270
-Original Grad: 0.133, -lr * Pred Grad:  0.008, New P: 0.685
iter 11 loss: 0.056
Actual params: [0.2698, 0.6846]
-Original Grad: -0.609, -lr * Pred Grad:  -0.002, New P: 0.267
-Original Grad: 0.070, -lr * Pred Grad:  -0.003, New P: 0.682
iter 12 loss: 0.056
Actual params: [0.2675, 0.6819]
-Original Grad: -0.271, -lr * Pred Grad:  0.002, New P: 0.269
-Original Grad: 0.064, -lr * Pred Grad:  0.021, New P: 0.702
iter 13 loss: 0.055
Actual params: [0.269 , 0.7024]
-Original Grad: -0.633, -lr * Pred Grad:  -0.002, New P: 0.267
-Original Grad: 0.065, -lr * Pred Grad:  -0.004, New P: 0.698
iter 14 loss: 0.054
Actual params: [0.2666, 0.6984]
-Original Grad: -0.429, -lr * Pred Grad:  -0.002, New P: 0.265
-Original Grad: 0.041, -lr * Pred Grad:  -0.004, New P: 0.695
iter 15 loss: 0.054
Actual params: [0.2648, 0.6947]
-Original Grad: -0.119, -lr * Pred Grad:  0.003, New P: 0.267
-Original Grad: 0.048, -lr * Pred Grad:  0.027, New P: 0.722
iter 16 loss: 0.053
Actual params: [0.2673, 0.7217]
-Original Grad: 0.350, -lr * Pred Grad:  0.005, New P: 0.272
-Original Grad: 0.003, -lr * Pred Grad:  0.034, New P: 0.755
iter 17 loss: 0.053
Actual params: [0.2722, 0.7553]
-Original Grad: -0.718, -lr * Pred Grad:  0.005, New P: 0.277
-Original Grad: 0.179, -lr * Pred Grad:  0.061, New P: 0.816
iter 18 loss: 0.050
Actual params: [0.2769, 0.816 ]
-Original Grad: 0.288, -lr * Pred Grad:  0.004, New P: 0.281
-Original Grad: -0.006, -lr * Pred Grad:  0.021, New P: 0.837
iter 19 loss: 0.050
Actual params: [0.2806, 0.8371]
-Original Grad: 0.028, -lr * Pred Grad:  0.003, New P: 0.283
-Original Grad: 0.025, -lr * Pred Grad:  0.022, New P: 0.860
iter 20 loss: 0.049
Actual params: [0.2835, 0.8596]
-Original Grad: -0.079, -lr * Pred Grad:  0.003, New P: 0.286
-Original Grad: 0.038, -lr * Pred Grad:  0.024, New P: 0.883
iter 21 loss: 0.049
Actual params: [0.2861, 0.8833]
-Original Grad: 0.052, -lr * Pred Grad:  0.003, New P: 0.289
-Original Grad: 0.022, -lr * Pred Grad:  0.025, New P: 0.908
iter 22 loss: 0.049
Actual params: [0.2895, 0.9084]
-Original Grad: 0.199, -lr * Pred Grad:  0.002, New P: 0.291
-Original Grad: -0.019, -lr * Pred Grad:  0.005, New P: 0.914
iter 23 loss: 0.049
Actual params: [0.2912, 0.9136]
-Original Grad: 0.252, -lr * Pred Grad:  0.001, New P: 0.293
-Original Grad: -0.032, -lr * Pred Grad:  -0.001, New P: 0.913
iter 24 loss: 0.049
Actual params: [0.2927, 0.9128]
-Original Grad: 0.097, -lr * Pred Grad:  0.004, New P: 0.297
-Original Grad: 0.011, -lr * Pred Grad:  0.026, New P: 0.938
iter 25 loss: 0.049
Actual params: [0.2966, 0.9383]
-Original Grad: 0.245, -lr * Pred Grad:  0.003, New P: 0.299
-Original Grad: -0.026, -lr * Pred Grad:  0.006, New P: 0.944
iter 26 loss: 0.049
Actual params: [0.2992, 0.9442]
-Original Grad: 0.241, -lr * Pred Grad:  0.004, New P: 0.303
-Original Grad: -0.020, -lr * Pred Grad:  0.013, New P: 0.957
iter 27 loss: 0.048
Actual params: [0.3029, 0.9575]
-Original Grad: 0.058, -lr * Pred Grad:  -0.000, New P: 0.303
-Original Grad: -0.012, -lr * Pred Grad:  -0.006, New P: 0.951
iter 28 loss: 0.048
Actual params: [0.3026, 0.9511]
-Original Grad: -0.327, -lr * Pred Grad:  0.003, New P: 0.305
-Original Grad: 0.075, -lr * Pred Grad:  0.042, New P: 0.993
iter 29 loss: 0.048
Actual params: [0.3054, 0.9929]
-Original Grad: 0.229, -lr * Pred Grad:  0.001, New P: 0.306
-Original Grad: -0.037, -lr * Pred Grad:  -0.008, New P: 0.985
iter 30 loss: 0.048
Actual params: [0.3065, 0.985 ]
-Original Grad: 0.184, -lr * Pred Grad:  0.005, New P: 0.312
-Original Grad: -0.010, -lr * Pred Grad:  0.024, New P: 1.009
Target params: [1.3344, 1.5708]
iter 0 loss: 0.548
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.036, New P: -0.509
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.004
iter 1 loss: 0.548
Actual params: [-0.5087,  0.0042]
-Original Grad: -0.004, -lr * Pred Grad:  -0.050, New P: -0.558
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.004
iter 2 loss: 0.548
Actual params: [-0.5582,  0.0036]
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.581
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.004
iter 3 loss: 0.548
Actual params: [-0.5809,  0.0041]
-Original Grad: -0.001, -lr * Pred Grad:  -0.023, New P: -0.604
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.004
iter 4 loss: 0.548
Actual params: [-0.6036,  0.0043]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.623
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.005
iter 5 loss: 0.548
Actual params: [-0.6228,  0.0048]
-Original Grad: -0.001, -lr * Pred Grad:  -0.023, New P: -0.646
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.005
iter 6 loss: 0.548
Actual params: [-0.6455,  0.0052]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.662
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.006
iter 7 loss: 0.548
Actual params: [-0.6623,  0.0058]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.679
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.006
iter 8 loss: 0.548
Actual params: [-0.6786,  0.0064]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.695
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.007
iter 9 loss: 0.548
Actual params: [-0.6954,  0.0069]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.711
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.007
iter 10 loss: 0.548
Actual params: [-0.7111,  0.0068]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.726
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.007
iter 11 loss: 0.548
Actual params: [-0.7263,  0.0071]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.740
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.007
iter 12 loss: 0.548
Actual params: [-0.7404,  0.0069]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.755
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.006
iter 13 loss: 0.548
Actual params: [-0.7555,  0.0061]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.770
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.005
iter 14 loss: 0.548
Actual params: [-0.7696,  0.0053]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.782
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.005
iter 15 loss: 0.548
Actual params: [-0.7821,  0.005 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.794
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.005
iter 16 loss: 0.548
Actual params: [-0.7942,  0.0046]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.809
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.003
iter 17 loss: 0.548
Actual params: [-0.8092,  0.0032]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.822
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.003
iter 18 loss: 0.548
Actual params: [-0.8218,  0.003 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.833
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.002
iter 19 loss: 0.548
Actual params: [-0.8327,  0.0025]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.845
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.002
iter 20 loss: 0.548
Actual params: [-0.8448,  0.0018]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.858
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.001
iter 21 loss: 0.548
Actual params: [-0.8581,  0.001 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.870
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.000
iter 22 loss: 0.548
Actual params: [-8.7039e-01, -5.0028e-05]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.882
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.001
iter 23 loss: 0.548
Actual params: [-0.8821, -0.0009]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.894
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.002
iter 24 loss: 0.548
Actual params: [-0.8937, -0.0021]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.906
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.003
iter 25 loss: 0.548
Actual params: [-0.906 , -0.0034]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.919
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.004
iter 26 loss: 0.548
Actual params: [-0.9193, -0.0044]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.933
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.006
iter 27 loss: 0.548
Actual params: [-0.9326, -0.0061]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.944
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.007
iter 28 loss: 0.548
Actual params: [-0.9443, -0.0074]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.958
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.009
iter 29 loss: 0.548
Actual params: [-0.9577, -0.0091]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.972
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.011
iter 30 loss: 0.548
Actual params: [-0.972 , -0.0107]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.986
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.013
Target params: [1.3344, 1.5708]
iter 0 loss: 0.003
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.006, New P: -0.478
-Original Grad: -0.001, -lr * Pred Grad:  -0.006, New P: -0.003
iter 1 loss: 0.003
Actual params: [-0.4781, -0.0028]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.483
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.008
iter 2 loss: 0.003
Actual params: [-0.4833, -0.0082]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.490
-Original Grad: -0.001, -lr * Pred Grad:  -0.007, New P: -0.015
iter 3 loss: 0.003
Actual params: [-0.4897, -0.0151]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.496
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.022
iter 4 loss: 0.003
Actual params: [-0.4964, -0.0224]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.500
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.027
iter 5 loss: 0.003
Actual params: [-0.5004, -0.0266]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.507
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.034
iter 6 loss: 0.003
Actual params: [-0.5074, -0.034 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.514
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.041
iter 7 loss: 0.003
Actual params: [-0.5144, -0.0406]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.522
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.048
iter 8 loss: 0.003
Actual params: [-0.5216, -0.0482]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.529
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.055
iter 9 loss: 0.003
Actual params: [-0.5287, -0.0552]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.534
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.061
iter 10 loss: 0.003
Actual params: [-0.5344, -0.0609]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.542
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.069
iter 11 loss: 0.003
Actual params: [-0.5425, -0.0692]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.550
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.076
iter 12 loss: 0.003
Actual params: [-0.5495, -0.0757]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.558
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.083
iter 13 loss: 0.003
Actual params: [-0.5578, -0.0834]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.565
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.090
iter 14 loss: 0.003
Actual params: [-0.5652, -0.09  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.574
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.097
iter 15 loss: 0.003
Actual params: [-0.5737, -0.0971]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.582
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.105
iter 16 loss: 0.003
Actual params: [-0.5825, -0.1051]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.591
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.112
iter 17 loss: 0.003
Actual params: [-0.5909, -0.1122]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.600
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.120
iter 18 loss: 0.003
Actual params: [-0.6003, -0.1195]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.610
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.127
iter 19 loss: 0.003
Actual params: [-0.6097, -0.1268]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.620
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.135
iter 20 loss: 0.003
Actual params: [-0.6198, -0.1345]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.630
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.142
iter 21 loss: 0.003
Actual params: [-0.6297, -0.1421]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.638
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.149
iter 22 loss: 0.003
Actual params: [-0.638 , -0.1495]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.647
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.157
iter 23 loss: 0.003
Actual params: [-0.6466, -0.1566]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.657
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.164
iter 24 loss: 0.003
Actual params: [-0.6568, -0.1643]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.667
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.172
iter 25 loss: 0.003
Actual params: [-0.6672, -0.1721]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.678
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.180
iter 26 loss: 0.003
Actual params: [-0.678 , -0.1801]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.689
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.188
iter 27 loss: 0.003
Actual params: [-0.689 , -0.1882]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.692
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.192
iter 28 loss: 0.003
Actual params: [-0.6923, -0.1922]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.702
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.200
iter 29 loss: 0.003
Actual params: [-0.7025, -0.1997]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.713
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.207
iter 30 loss: 0.003
Actual params: [-0.7129, -0.2074]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.723
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.214
Target params: [1.3344, 1.5708]
iter 0 loss: 0.879
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.487
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 0.002
iter 1 loss: 0.879
Actual params: [-0.4868,  0.0019]
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -0.511
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.000
iter 2 loss: 0.879
Actual params: [-5.1085e-01, -3.7167e-04]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.524
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.002
iter 3 loss: 0.879
Actual params: [-0.5241, -0.0017]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.534
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.003
iter 4 loss: 0.879
Actual params: [-0.5339, -0.0026]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.546
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.004
iter 5 loss: 0.879
Actual params: [-0.5457, -0.0038]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.557
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.005
iter 6 loss: 0.879
Actual params: [-0.5567, -0.005 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.569
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.006
iter 7 loss: 0.879
Actual params: [-0.5691, -0.0064]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.586
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.008
iter 8 loss: 0.879
Actual params: [-0.5857, -0.0078]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.593
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.009
iter 9 loss: 0.879
Actual params: [-0.5933, -0.009 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.606
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.010
iter 10 loss: 0.879
Actual params: [-0.6063, -0.0102]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.622
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.011
iter 11 loss: 0.879
Actual params: [-0.6221, -0.0115]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.632
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.013
iter 12 loss: 0.879
Actual params: [-0.6323, -0.0127]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.648
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.014
iter 13 loss: 0.879
Actual params: [-0.6477, -0.0142]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.658
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.016
iter 14 loss: 0.879
Actual params: [-0.6583, -0.016 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.671
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.018
iter 15 loss: 0.879
Actual params: [-0.6712, -0.0176]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.684
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.019
iter 16 loss: 0.879
Actual params: [-0.6841, -0.0189]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.696
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.020
iter 17 loss: 0.879
Actual params: [-0.6962, -0.0201]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.706
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.021
iter 18 loss: 0.879
Actual params: [-0.706 , -0.0213]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.717
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.022
iter 19 loss: 0.879
Actual params: [-0.7166, -0.0223]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.730
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.024
iter 20 loss: 0.879
Actual params: [-0.73  , -0.0239]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.737
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.025
iter 21 loss: 0.879
Actual params: [-0.7369, -0.025 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.751
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.027
iter 22 loss: 0.879
Actual params: [-0.7508, -0.0269]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.765
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.028
iter 23 loss: 0.879
Actual params: [-0.7649, -0.0284]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.780
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.031
iter 24 loss: 0.879
Actual params: [-0.7796, -0.0306]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -0.798
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.032
iter 25 loss: 0.879
Actual params: [-0.7983, -0.032 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.815
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.033
iter 26 loss: 0.879
Actual params: [-0.8154, -0.0332]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.823
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.034
iter 27 loss: 0.879
Actual params: [-0.8232, -0.0344]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.833
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.036
iter 28 loss: 0.879
Actual params: [-0.8325, -0.0365]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.848
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.039
iter 29 loss: 0.879
Actual params: [-0.8477, -0.0387]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.862
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.040
iter 30 loss: 0.879
Actual params: [-0.8624, -0.0401]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.875
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.043
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.021, -lr * Pred Grad:  -0.226, New P: -0.698
-Original Grad: -0.012, -lr * Pred Grad:  -0.131, New P: -0.127
iter 1 loss: 0.118
Actual params: [-0.6983, -0.127 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.715
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.139
iter 2 loss: 0.118
Actual params: [-0.7151, -0.1385]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.728
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.148
iter 3 loss: 0.118
Actual params: [-0.7277, -0.148 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.742
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.158
iter 4 loss: 0.118
Actual params: [-0.7415, -0.1577]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.756
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.168
iter 5 loss: 0.118
Actual params: [-0.7563, -0.1684]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.769
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.178
iter 6 loss: 0.118
Actual params: [-0.7688, -0.1778]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.782
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.187
iter 7 loss: 0.118
Actual params: [-0.7823, -0.1873]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.796
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.197
iter 8 loss: 0.118
Actual params: [-0.7957, -0.1968]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.812
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.208
iter 9 loss: 0.118
Actual params: [-0.8119, -0.2082]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.822
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.216
iter 10 loss: 0.118
Actual params: [-0.8221, -0.216 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.832
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.224
iter 11 loss: 0.118
Actual params: [-0.8325, -0.2242]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.844
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.233
iter 12 loss: 0.118
Actual params: [-0.8443, -0.2332]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.857
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.243
iter 13 loss: 0.118
Actual params: [-0.8573, -0.2427]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.867
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.251
iter 14 loss: 0.118
Actual params: [-0.867 , -0.2506]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.876
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.258
iter 15 loss: 0.118
Actual params: [-0.8763, -0.2578]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.885
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.265
iter 16 loss: 0.118
Actual params: [-0.8855, -0.2651]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.897
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.274
iter 17 loss: 0.118
Actual params: [-0.8971, -0.2739]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.908
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.282
iter 18 loss: 0.118
Actual params: [-0.9079, -0.2823]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.917
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.289
iter 19 loss: 0.118
Actual params: [-0.917 , -0.2894]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.927
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.298
iter 20 loss: 0.118
Actual params: [-0.9274, -0.2976]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.939
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.306
iter 21 loss: 0.118
Actual params: [-0.9391, -0.3063]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.951
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.315
iter 22 loss: 0.118
Actual params: [-0.9507, -0.3152]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.963
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.325
iter 23 loss: 0.118
Actual params: [-0.9628, -0.3245]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.974
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.333
iter 24 loss: 0.118
Actual params: [-0.9742, -0.333 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.988
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.343
iter 25 loss: 0.118
Actual params: [-0.9876, -0.3428]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.000
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.352
iter 26 loss: 0.118
Actual params: [-1.0002, -0.352 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.013
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.361
iter 27 loss: 0.118
Actual params: [-1.0128, -0.361 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.023
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.369
iter 28 loss: 0.118
Actual params: [-1.0234, -0.3686]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.035
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.377
iter 29 loss: 0.118
Actual params: [-1.0354, -0.3774]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.046
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.385
iter 30 loss: 0.118
Actual params: [-1.046 , -0.3851]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.059
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.394
Target params: [1.3344, 1.5708]
iter 0 loss: 1.009
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.013, -lr * Pred Grad:  0.141, New P: -0.331
-Original Grad: 0.006, -lr * Pred Grad:  0.066, New P: 0.069
iter 1 loss: 1.000
Actual params: [-0.3311,  0.0691]
-Original Grad: 0.030, -lr * Pred Grad:  0.306, New P: -0.025
-Original Grad: 0.014, -lr * Pred Grad:  0.141, New P: 0.210
iter 2 loss: 0.964
Actual params: [-0.0249,  0.2102]
-Original Grad: 0.124, -lr * Pred Grad:  0.511, New P: 0.486
-Original Grad: 0.033, -lr * Pred Grad:  0.116, New P: 0.326
iter 3 loss: 0.798
Actual params: [0.4859, 0.3258]
-Original Grad: 0.292, -lr * Pred Grad:  0.368, New P: 0.854
-Original Grad: 0.089, -lr * Pred Grad:  0.132, New P: 0.458
iter 4 loss: 0.541
Actual params: [0.8535, 0.4576]
-Original Grad: 0.567, -lr * Pred Grad:  0.095, New P: 0.949
-Original Grad: 0.315, -lr * Pred Grad:  0.414, New P: 0.871
iter 5 loss: 0.361
Actual params: [0.9489, 0.8712]
-Original Grad: 0.426, -lr * Pred Grad:  0.025, New P: 0.974
-Original Grad: 0.396, -lr * Pred Grad:  0.264, New P: 1.135
iter 6 loss: 0.304
Actual params: [0.974 , 1.1347]
-Original Grad: 0.816, -lr * Pred Grad:  0.035, New P: 1.009
-Original Grad: -0.047, -lr * Pred Grad:  0.035, New P: 1.170
iter 7 loss: 0.265
Actual params: [1.0086, 1.1696]
-Original Grad: 0.500, -lr * Pred Grad:  0.030, New P: 1.039
-Original Grad: 0.340, -lr * Pred Grad:  0.138, New P: 1.307
iter 8 loss: 0.201
Actual params: [1.0389, 1.3072]
-Original Grad: 0.383, -lr * Pred Grad:  0.018, New P: 1.057
-Original Grad: 0.141, -lr * Pred Grad:  0.061, New P: 1.369
iter 9 loss: 0.193
Actual params: [1.0571, 1.3687]
-Original Grad: 0.352, -lr * Pred Grad:  0.020, New P: 1.078
-Original Grad: 0.242, -lr * Pred Grad:  0.067, New P: 1.436
iter 10 loss: 0.186
Actual params: [1.0776, 1.4359]
-Original Grad: 0.796, -lr * Pred Grad:  0.024, New P: 1.101
-Original Grad: -0.016, -lr * Pred Grad:  0.026, New P: 1.462
iter 11 loss: 0.172
Actual params: [1.1011, 1.4623]
-Original Grad: 0.902, -lr * Pred Grad:  0.013, New P: 1.114
-Original Grad: -0.386, -lr * Pred Grad:  -0.041, New P: 1.422
iter 12 loss: 0.157
Actual params: [1.1142, 1.4217]
-Original Grad: 0.006, -lr * Pred Grad:  0.015, New P: 1.129
-Original Grad: 0.506, -lr * Pred Grad:  0.064, New P: 1.485
iter 13 loss: 0.158
Actual params: [1.1295, 1.4854]
-Original Grad: 0.283, -lr * Pred Grad:  0.011, New P: 1.140
-Original Grad: 0.053, -lr * Pred Grad:  0.015, New P: 1.500
iter 14 loss: 0.156
Actual params: [1.1404, 1.5004]
-Original Grad: 0.252, -lr * Pred Grad:  0.008, New P: 1.149
-Original Grad: -0.023, -lr * Pred Grad:  0.005, New P: 1.506
iter 15 loss: 0.153
Actual params: [1.1485, 1.5055]
-Original Grad: 0.520, -lr * Pred Grad:  0.012, New P: 1.161
-Original Grad: -0.213, -lr * Pred Grad:  -0.009, New P: 1.496
iter 16 loss: 0.146
Actual params: [1.1608, 1.4961]
-Original Grad: 0.524, -lr * Pred Grad:  0.012, New P: 1.173
-Original Grad: -0.248, -lr * Pred Grad:  -0.007, New P: 1.490
iter 17 loss: 0.139
Actual params: [1.1725, 1.4895]
-Original Grad: 0.502, -lr * Pred Grad:  0.011, New P: 1.183
-Original Grad: -0.286, -lr * Pred Grad:  -0.008, New P: 1.481
iter 18 loss: 0.134
Actual params: [1.1831, 1.4813]
-Original Grad: 0.481, -lr * Pred Grad:  0.006, New P: 1.189
-Original Grad: -0.431, -lr * Pred Grad:  -0.017, New P: 1.464
iter 19 loss: 0.131
Actual params: [1.1894, 1.4641]
-Original Grad: 0.155, -lr * Pred Grad:  0.014, New P: 1.203
-Original Grad: 0.225, -lr * Pred Grad:  0.021, New P: 1.485
iter 20 loss: 0.129
Actual params: [1.2031, 1.4854]
-Original Grad: 0.207, -lr * Pred Grad:  0.005, New P: 1.209
-Original Grad: -0.128, -lr * Pred Grad:  -0.002, New P: 1.483
iter 21 loss: 0.128
Actual params: [1.2086, 1.4834]
-Original Grad: 0.033, -lr * Pred Grad:  0.005, New P: 1.214
-Original Grad: 0.093, -lr * Pred Grad:  0.008, New P: 1.491
iter 22 loss: 0.127
Actual params: [1.2138, 1.4913]
-Original Grad: 0.214, -lr * Pred Grad:  0.016, New P: 1.230
-Original Grad: 0.118, -lr * Pred Grad:  0.017, New P: 1.508
iter 23 loss: 0.125
Actual params: [1.2296, 1.5081]
-Original Grad: -0.064, -lr * Pred Grad:  0.005, New P: 1.235
-Original Grad: 0.222, -lr * Pred Grad:  0.013, New P: 1.522
iter 24 loss: 0.126
Actual params: [1.2348, 1.5216]
-Original Grad: 0.322, -lr * Pred Grad:  0.013, New P: 1.247
-Original Grad: -0.174, -lr * Pred Grad:  0.002, New P: 1.524
iter 25 loss: 0.123
Actual params: [1.2473, 1.5239]
-Original Grad: 0.179, -lr * Pred Grad:  0.009, New P: 1.257
-Original Grad: -0.056, -lr * Pred Grad:  0.004, New P: 1.528
iter 26 loss: 0.122
Actual params: [1.2568, 1.5282]
-Original Grad: 0.045, -lr * Pred Grad:  0.004, New P: 1.260
-Original Grad: 0.005, -lr * Pred Grad:  0.003, New P: 1.531
iter 27 loss: 0.122
Actual params: [1.2603, 1.5307]
-Original Grad: 0.112, -lr * Pred Grad:  0.005, New P: 1.265
-Original Grad: -0.081, -lr * Pred Grad:  0.000, New P: 1.531
iter 28 loss: 0.121
Actual params: [1.2651, 1.5309]
-Original Grad: -0.067, -lr * Pred Grad:  -0.001, New P: 1.264
-Original Grad: 0.090, -lr * Pred Grad:  0.003, New P: 1.534
iter 29 loss: 0.122
Actual params: [1.2644, 1.5337]
-Original Grad: 0.056, -lr * Pred Grad:  -0.002, New P: 1.263
-Original Grad: -0.114, -lr * Pred Grad:  -0.005, New P: 1.529
iter 30 loss: 0.121
Actual params: [1.2626, 1.5286]
-Original Grad: 0.082, -lr * Pred Grad:  0.014, New P: 1.277
-Original Grad: 0.085, -lr * Pred Grad:  0.012, New P: 1.541
Target params: [1.3344, 1.5708]
iter 0 loss: 0.466
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.488
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.009
iter 1 loss: 0.466
Actual params: [-0.4877,  0.0085]
-Original Grad: -0.002, -lr * Pred Grad:  -0.021, New P: -0.509
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.016
iter 2 loss: 0.466
Actual params: [-0.5092,  0.0155]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.528
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.022
iter 3 loss: 0.466
Actual params: [-0.5276,  0.0218]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.540
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.026
iter 4 loss: 0.466
Actual params: [-0.5395,  0.0262]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.550
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.030
iter 5 loss: 0.466
Actual params: [-0.5501,  0.0302]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.564
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.035
iter 6 loss: 0.466
Actual params: [-0.5636,  0.0352]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.579
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.041
iter 7 loss: 0.466
Actual params: [-0.5792,  0.041 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.591
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.046
iter 8 loss: 0.466
Actual params: [-0.5913,  0.0457]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.605
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.051
iter 9 loss: 0.466
Actual params: [-0.6046,  0.051 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.619
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.057
iter 10 loss: 0.466
Actual params: [-0.619 ,  0.0569]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.632
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.062
iter 11 loss: 0.466
Actual params: [-0.6316,  0.0622]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.643
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.067
iter 12 loss: 0.466
Actual params: [-0.6435,  0.0673]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.656
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.073
iter 13 loss: 0.466
Actual params: [-0.6557,  0.0727]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.667
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.078
iter 14 loss: 0.466
Actual params: [-0.6666,  0.0776]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.678
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.083
iter 15 loss: 0.466
Actual params: [-0.6775,  0.0827]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.688
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.088
iter 16 loss: 0.466
Actual params: [-0.6881,  0.0877]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.698
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.092
iter 17 loss: 0.466
Actual params: [-0.6979,  0.0925]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.706
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.096
iter 18 loss: 0.466
Actual params: [-0.7061,  0.0965]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.716
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.102
iter 19 loss: 0.466
Actual params: [-0.716 ,  0.1015]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.727
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.107
iter 20 loss: 0.466
Actual params: [-0.7269,  0.1071]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.738
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.113
iter 21 loss: 0.466
Actual params: [-0.7376,  0.1129]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.748
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.119
iter 22 loss: 0.466
Actual params: [-0.7484,  0.1189]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.761
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.126
iter 23 loss: 0.466
Actual params: [-0.7613,  0.1262]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.772
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.133
iter 24 loss: 0.466
Actual params: [-0.7724,  0.1328]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.784
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.140
iter 25 loss: 0.466
Actual params: [-0.7838,  0.1399]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.796
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.148
iter 26 loss: 0.466
Actual params: [-0.7958,  0.1475]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.805
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.154
iter 27 loss: 0.466
Actual params: [-0.8052,  0.1539]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.814
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.160
iter 28 loss: 0.466
Actual params: [-0.8135,  0.1597]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.823
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.167
iter 29 loss: 0.466
Actual params: [-0.8233,  0.1669]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.831
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.173
iter 30 loss: 0.466
Actual params: [-0.8307,  0.1726]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.840
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.180
Target params: [1.3344, 1.5708]
iter 0 loss: 0.644
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.030, New P: -0.503
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.013
iter 1 loss: 0.644
Actual params: [-0.5025, -0.013 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.027, New P: -0.529
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.028
iter 2 loss: 0.644
Actual params: [-0.5294, -0.028 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.025, New P: -0.554
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.046
iter 3 loss: 0.644
Actual params: [-0.5541, -0.046 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.568
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.051
iter 4 loss: 0.644
Actual params: [-0.5677, -0.0505]
-Original Grad: -0.003, -lr * Pred Grad:  -0.044, New P: -0.611
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.069
iter 5 loss: 0.644
Actual params: [-0.6113, -0.0695]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.623
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.077
iter 6 loss: 0.644
Actual params: [-0.6228, -0.0775]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.639
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.085
iter 7 loss: 0.644
Actual params: [-0.6388, -0.0853]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.656
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.092
iter 8 loss: 0.644
Actual params: [-0.6557, -0.0922]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.674
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.102
iter 9 loss: 0.644
Actual params: [-0.6743, -0.1024]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.691
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.109
iter 10 loss: 0.644
Actual params: [-0.6908, -0.1092]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.708
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.119
iter 11 loss: 0.644
Actual params: [-0.7081, -0.1189]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.726
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.129
iter 12 loss: 0.644
Actual params: [-0.7258, -0.1292]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.741
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.139
iter 13 loss: 0.644
Actual params: [-0.7412, -0.1392]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.747
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.141
iter 14 loss: 0.644
Actual params: [-0.747 , -0.1407]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -0.765
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.151
iter 15 loss: 0.644
Actual params: [-0.765 , -0.1512]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.776
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.157
iter 16 loss: 0.644
Actual params: [-0.7764, -0.1567]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.781
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.157
iter 17 loss: 0.644
Actual params: [-0.7805, -0.157 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -0.804
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.169
iter 18 loss: 0.644
Actual params: [-0.8044, -0.1692]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.822
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.178
iter 19 loss: 0.644
Actual params: [-0.8216, -0.1784]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.819
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.174
iter 20 loss: 0.644
Actual params: [-0.8187, -0.1737]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.826
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.178
iter 21 loss: 0.644
Actual params: [-0.8264, -0.1782]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.832
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.179
iter 22 loss: 0.644
Actual params: [-0.8321, -0.1788]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.840
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.181
iter 23 loss: 0.644
Actual params: [-0.8401, -0.1815]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -0.860
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.194
iter 24 loss: 0.644
Actual params: [-0.8602, -0.1941]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -0.880
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.206
iter 25 loss: 0.644
Actual params: [-0.8805, -0.2059]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.895
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.213
iter 26 loss: 0.644
Actual params: [-0.8952, -0.2127]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.907
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.219
iter 27 loss: 0.644
Actual params: [-0.9075, -0.2188]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -0.925
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.230
iter 28 loss: 0.644
Actual params: [-0.9251, -0.23  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.939
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.236
iter 29 loss: 0.644
Actual params: [-0.9385, -0.2358]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.946
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.239
iter 30 loss: 0.644
Actual params: [-0.9458, -0.239 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -0.968
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.250
Target params: [1.3344, 1.5708]
iter 0 loss: 0.511
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.039, -lr * Pred Grad:  0.360, New P: -0.112
-Original Grad: 0.047, -lr * Pred Grad:  0.431, New P: 0.434
iter 1 loss: 0.448
Actual params: [-0.1124,  0.4344]
-Original Grad: 0.155, -lr * Pred Grad:  0.504, New P: 0.391
-Original Grad: 0.160, -lr * Pred Grad:  0.507, New P: 0.941
iter 2 loss: 0.223
Actual params: [0.3913, 0.9414]
-Original Grad: 0.433, -lr * Pred Grad:  0.276, New P: 0.667
-Original Grad: 0.307, -lr * Pred Grad:  0.189, New P: 1.130
iter 3 loss: 0.121
Actual params: [0.667 , 1.1305]
-Original Grad: 0.304, -lr * Pred Grad:  0.212, New P: 0.879
-Original Grad: 0.161, -lr * Pred Grad:  -0.100, New P: 1.030
iter 4 loss: 0.091
Actual params: [0.8792, 1.0303]
-Original Grad: 0.129, -lr * Pred Grad:  -0.083, New P: 0.796
-Original Grad: 0.192, -lr * Pred Grad:  0.240, New P: 1.271
iter 5 loss: 0.083
Actual params: [0.7962, 1.2705]
-Original Grad: 0.311, -lr * Pred Grad:  0.124, New P: 0.920
-Original Grad: 0.145, -lr * Pred Grad:  -0.041, New P: 1.229
iter 6 loss: 0.066
Actual params: [0.92  , 1.2294]
-Original Grad: 0.214, -lr * Pred Grad:  0.045, New P: 0.965
-Original Grad: 0.138, -lr * Pred Grad:  0.031, New P: 1.261
iter 7 loss: 0.058
Actual params: [0.9654, 1.2609]
-Original Grad: 0.165, -lr * Pred Grad:  -0.001, New P: 0.964
-Original Grad: 0.190, -lr * Pred Grad:  0.084, New P: 1.345
iter 8 loss: 0.053
Actual params: [0.9644, 1.3453]
-Original Grad: 0.139, -lr * Pred Grad:  0.022, New P: 0.986
-Original Grad: 0.086, -lr * Pred Grad:  0.019, New P: 1.364
iter 9 loss: 0.050
Actual params: [0.9864, 1.3638]
-Original Grad: 0.039, -lr * Pred Grad:  -0.010, New P: 0.977
-Original Grad: 0.120, -lr * Pred Grad:  0.047, New P: 1.411
iter 10 loss: 0.050
Actual params: [0.9765, 1.4108]
-Original Grad: 0.155, -lr * Pred Grad:  0.043, New P: 1.020
-Original Grad: 0.021, -lr * Pred Grad:  -0.018, New P: 1.393
iter 11 loss: 0.047
Actual params: [1.0198, 1.393 ]
-Original Grad: 0.194, -lr * Pred Grad:  0.046, New P: 1.066
-Original Grad: 0.043, -lr * Pred Grad:  -0.012, New P: 1.381
iter 12 loss: 0.044
Actual params: [1.0658, 1.3813]
-Original Grad: 0.053, -lr * Pred Grad:  -0.000, New P: 1.066
-Original Grad: 0.112, -lr * Pred Grad:  0.039, New P: 1.420
iter 13 loss: 0.043
Actual params: [1.0657, 1.4202]
-Original Grad: 0.086, -lr * Pred Grad:  0.011, New P: 1.077
-Original Grad: 0.082, -lr * Pred Grad:  0.023, New P: 1.443
iter 14 loss: 0.042
Actual params: [1.0766, 1.4435]
-Original Grad: 0.042, -lr * Pred Grad:  0.006, New P: 1.082
-Original Grad: 0.048, -lr * Pred Grad:  0.014, New P: 1.458
iter 15 loss: 0.042
Actual params: [1.0822, 1.4578]
-Original Grad: 0.107, -lr * Pred Grad:  0.020, New P: 1.103
-Original Grad: 0.038, -lr * Pred Grad:  0.007, New P: 1.465
iter 16 loss: 0.041
Actual params: [1.1027, 1.4649]
-Original Grad: 0.026, -lr * Pred Grad:  0.004, New P: 1.107
-Original Grad: 0.048, -lr * Pred Grad:  0.015, New P: 1.480
iter 17 loss: 0.040
Actual params: [1.1065, 1.4801]
-Original Grad: 0.090, -lr * Pred Grad:  0.019, New P: 1.126
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: 1.478
iter 18 loss: 0.039
Actual params: [1.1257, 1.478 ]
-Original Grad: 0.068, -lr * Pred Grad:  0.011, New P: 1.136
-Original Grad: 0.076, -lr * Pred Grad:  0.023, New P: 1.501
iter 19 loss: 0.038
Actual params: [1.1363, 1.5006]
-Original Grad: 0.041, -lr * Pred Grad:  0.007, New P: 1.144
-Original Grad: 0.005, -lr * Pred Grad:  0.002, New P: 1.502
iter 20 loss: 0.038
Actual params: [1.1435, 1.5023]
-Original Grad: 0.107, -lr * Pred Grad:  0.015, New P: 1.159
-Original Grad: 0.057, -lr * Pred Grad:  0.017, New P: 1.520
iter 21 loss: 0.037
Actual params: [1.1587, 1.5197]
-Original Grad: 0.008, -lr * Pred Grad:  0.001, New P: 1.160
-Original Grad: 0.070, -lr * Pred Grad:  0.020, New P: 1.540
iter 22 loss: 0.037
Actual params: [1.1601, 1.54  ]
-Original Grad: 0.121, -lr * Pred Grad:  0.017, New P: 1.178
-Original Grad: -0.044, -lr * Pred Grad:  -0.012, New P: 1.528
iter 23 loss: 0.036
Actual params: [1.1775, 1.5284]
-Original Grad: 0.025, -lr * Pred Grad:  0.004, New P: 1.182
-Original Grad: 0.016, -lr * Pred Grad:  0.005, New P: 1.534
iter 24 loss: 0.035
Actual params: [1.182 , 1.5339]
-Original Grad: 0.021, -lr * Pred Grad:  0.005, New P: 1.187
-Original Grad: 0.038, -lr * Pred Grad:  0.012, New P: 1.546
iter 25 loss: 0.035
Actual params: [1.1871, 1.5461]
-Original Grad: 0.083, -lr * Pred Grad:  0.012, New P: 1.199
-Original Grad: -0.037, -lr * Pred Grad:  -0.004, New P: 1.542
iter 26 loss: 0.035
Actual params: [1.1991, 1.5417]
-Original Grad: -0.008, -lr * Pred Grad:  0.003, New P: 1.202
-Original Grad: 0.059, -lr * Pred Grad:  0.016, New P: 1.557
iter 27 loss: 0.034
Actual params: [1.2022, 1.5574]
-Original Grad: 0.112, -lr * Pred Grad:  0.022, New P: 1.225
-Original Grad: 0.002, -lr * Pred Grad:  0.009, New P: 1.567
iter 28 loss: 0.033
Actual params: [1.2245, 1.5667]
-Original Grad: 0.097, -lr * Pred Grad:  0.021, New P: 1.246
-Original Grad: 0.050, -lr * Pred Grad:  0.021, New P: 1.588
iter 29 loss: 0.032
Actual params: [1.2458, 1.588 ]
-Original Grad: 0.107, -lr * Pred Grad:  0.019, New P: 1.265
-Original Grad: 0.006, -lr * Pred Grad:  0.007, New P: 1.595
iter 30 loss: 0.031
Actual params: [1.2651, 1.5954]
-Original Grad: 0.020, -lr * Pred Grad:  0.007, New P: 1.272
-Original Grad: 0.060, -lr * Pred Grad:  0.019, New P: 1.615
Target params: [1.3344, 1.5708]
iter 0 loss: 0.563
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.462
-Original Grad: 0.003, -lr * Pred Grad:  0.035, New P: 0.039
iter 1 loss: 0.563
Actual params: [-0.4623,  0.0388]
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: -0.449
-Original Grad: 0.003, -lr * Pred Grad:  0.032, New P: 0.071
iter 2 loss: 0.563
Actual params: [-0.4489,  0.0709]
-Original Grad: 0.002, -lr * Pred Grad:  0.024, New P: -0.425
-Original Grad: 0.003, -lr * Pred Grad:  0.045, New P: 0.116
iter 3 loss: 0.562
Actual params: [-0.4247,  0.1156]
-Original Grad: 0.001, -lr * Pred Grad:  0.020, New P: -0.405
-Original Grad: 0.002, -lr * Pred Grad:  0.028, New P: 0.144
iter 4 loss: 0.562
Actual params: [-0.405 ,  0.1438]
-Original Grad: 0.002, -lr * Pred Grad:  0.036, New P: -0.369
-Original Grad: 0.003, -lr * Pred Grad:  0.044, New P: 0.188
iter 5 loss: 0.561
Actual params: [-0.3689,  0.1878]
-Original Grad: 0.003, -lr * Pred Grad:  0.059, New P: -0.310
-Original Grad: 0.003, -lr * Pred Grad:  0.058, New P: 0.246
iter 6 loss: 0.559
Actual params: [-0.3101,  0.2456]
-Original Grad: 0.003, -lr * Pred Grad:  0.056, New P: -0.254
-Original Grad: 0.002, -lr * Pred Grad:  0.045, New P: 0.290
iter 7 loss: 0.557
Actual params: [-0.2543,  0.2903]
-Original Grad: 0.005, -lr * Pred Grad:  0.119, New P: -0.136
-Original Grad: 0.003, -lr * Pred Grad:  0.078, New P: 0.368
iter 8 loss: 0.550
Actual params: [-0.1357,  0.3682]
-Original Grad: 0.006, -lr * Pred Grad:  0.165, New P: 0.029
-Original Grad: 0.003, -lr * Pred Grad:  0.083, New P: 0.452
iter 9 loss: 0.537
Actual params: [0.0292, 0.4515]
-Original Grad: 0.017, -lr * Pred Grad:  0.466, New P: 0.495
-Original Grad: 0.008, -lr * Pred Grad:  0.221, New P: 0.672
iter 10 loss: 0.441
Actual params: [0.4949, 0.6723]
-Original Grad: 0.116, -lr * Pred Grad:  1.042, New P: 1.537
-Original Grad: 0.041, -lr * Pred Grad:  0.398, New P: 1.070
iter 11 loss: 0.179
Actual params: [1.5371, 1.0699]
-Original Grad: -0.207, -lr * Pred Grad:  -0.426, New P: 1.111
-Original Grad: 0.112, -lr * Pred Grad:  0.635, New P: 1.705
iter 12 loss: 0.119
Actual params: [1.1114, 1.7048]
-Original Grad: 0.351, -lr * Pred Grad:  0.215, New P: 1.326
-Original Grad: 0.074, -lr * Pred Grad:  0.236, New P: 1.941
iter 13 loss: 0.109
Actual params: [1.3263, 1.941 ]
-Original Grad: -0.235, -lr * Pred Grad:  -0.028, New P: 1.298
-Original Grad: 0.282, -lr * Pred Grad:  0.163, New P: 2.104
iter 14 loss: 0.063
Actual params: [1.298 , 2.1036]
-Original Grad: 0.080, -lr * Pred Grad:  0.068, New P: 1.366
-Original Grad: 0.072, -lr * Pred Grad:  0.088, New P: 2.191
iter 15 loss: 0.059
Actual params: [1.3659, 2.1913]
-Original Grad: -0.153, -lr * Pred Grad:  -0.028, New P: 1.338
-Original Grad: 0.106, -lr * Pred Grad:  0.033, New P: 2.224
iter 16 loss: 0.063
Actual params: [1.3384, 2.2244]
-Original Grad: 0.176, -lr * Pred Grad:  0.079, New P: 1.418
-Original Grad: 0.057, -lr * Pred Grad:  0.091, New P: 2.316
iter 17 loss: 0.067
Actual params: [1.4175, 2.3158]
-Original Grad: 0.126, -lr * Pred Grad:  -0.008, New P: 1.409
-Original Grad: -0.235, -lr * Pred Grad:  -0.071, New P: 2.245
iter 18 loss: 0.060
Actual params: [1.4094, 2.2448]
-Original Grad: -0.262, -lr * Pred Grad:  -0.049, New P: 1.360
-Original Grad: 0.143, -lr * Pred Grad:  -0.000, New P: 2.245
iter 19 loss: 0.063
Actual params: [1.36  , 2.2445]
-Original Grad: 0.104, -lr * Pred Grad:  0.005, New P: 1.365
-Original Grad: -0.122, -lr * Pred Grad:  -0.023, New P: 2.222
iter 20 loss: 0.061
Actual params: [1.3653, 2.222 ]
-Original Grad: -0.103, -lr * Pred Grad:  -0.008, New P: 1.358
-Original Grad: 0.108, -lr * Pred Grad:  0.018, New P: 2.240
iter 21 loss: 0.063
Actual params: [1.3577, 2.2401]
-Original Grad: -0.021, -lr * Pred Grad:  0.012, New P: 1.370
-Original Grad: 0.068, -lr * Pred Grad:  0.025, New P: 2.265
iter 22 loss: 0.065
Actual params: [1.3696, 2.2653]
-Original Grad: 0.043, -lr * Pred Grad:  0.025, New P: 1.395
-Original Grad: 0.023, -lr * Pred Grad:  0.025, New P: 2.290
iter 23 loss: 0.066
Actual params: [1.3947, 2.2902]
-Original Grad: 0.035, -lr * Pred Grad:  0.019, New P: 1.414
-Original Grad: 0.012, -lr * Pred Grad:  0.017, New P: 2.307
iter 24 loss: 0.066
Actual params: [1.4137, 2.3067]
-Original Grad: 0.079, -lr * Pred Grad:  0.015, New P: 1.428
-Original Grad: -0.071, -lr * Pred Grad:  -0.005, New P: 2.302
iter 25 loss: 0.065
Actual params: [1.4282, 2.302 ]
-Original Grad: 0.156, -lr * Pred Grad:  -0.003, New P: 1.425
-Original Grad: -0.258, -lr * Pred Grad:  -0.040, New P: 2.262
iter 26 loss: 0.061
Actual params: [1.4254, 2.2617]
-Original Grad: -0.178, -lr * Pred Grad:  -0.051, New P: 1.375
-Original Grad: 0.082, -lr * Pred Grad:  -0.023, New P: 2.239
iter 27 loss: 0.061
Actual params: [1.3748, 2.2386]
-Original Grad: -0.014, -lr * Pred Grad:  0.016, New P: 1.391
-Original Grad: 0.072, -lr * Pred Grad:  0.022, New P: 2.261
iter 28 loss: 0.062
Actual params: [1.3907, 2.2606]
-Original Grad: -0.045, -lr * Pred Grad:  0.005, New P: 1.396
-Original Grad: 0.081, -lr * Pred Grad:  0.017, New P: 2.278
iter 29 loss: 0.064
Actual params: [1.3958, 2.2776]
-Original Grad: -0.050, -lr * Pred Grad:  -0.029, New P: 1.367
-Original Grad: -0.021, -lr * Pred Grad:  -0.024, New P: 2.254
iter 30 loss: 0.064
Actual params: [1.3672, 2.2539]
-Original Grad: 0.003, -lr * Pred Grad:  0.015, New P: 1.382
-Original Grad: 0.040, -lr * Pred Grad:  0.017, New P: 2.271
Target params: [1.3344, 1.5708]
iter 0 loss: 0.024
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.047, -lr * Pred Grad:  -0.162, New P: -0.634
-Original Grad: 0.043, -lr * Pred Grad:  0.245, New P: 0.249
iter 1 loss: 0.017
Actual params: [-0.6345,  0.2487]
-Original Grad: 0.002, -lr * Pred Grad:  0.065, New P: -0.569
-Original Grad: 0.019, -lr * Pred Grad:  0.168, New P: 0.416
iter 2 loss: 0.015
Actual params: [-0.5694,  0.4164]
-Original Grad: 0.008, -lr * Pred Grad:  0.103, New P: -0.466
-Original Grad: 0.021, -lr * Pred Grad:  0.185, New P: 0.601
iter 3 loss: 0.014
Actual params: [-0.4662,  0.6012]
-Original Grad: 0.018, -lr * Pred Grad:  0.144, New P: -0.322
-Original Grad: 0.016, -lr * Pred Grad:  0.152, New P: 0.753
iter 4 loss: 0.012
Actual params: [-0.3224,  0.7527]
-Original Grad: 0.017, -lr * Pred Grad:  0.123, New P: -0.199
-Original Grad: 0.021, -lr * Pred Grad:  0.160, New P: 0.913
iter 5 loss: 0.010
Actual params: [-0.1994,  0.9131]
-Original Grad: 0.024, -lr * Pred Grad:  0.123, New P: -0.077
-Original Grad: 0.003, -lr * Pred Grad:  0.043, New P: 0.957
iter 6 loss: 0.009
Actual params: [-0.0768,  0.9566]
-Original Grad: 0.013, -lr * Pred Grad:  0.074, New P: -0.003
-Original Grad: 0.020, -lr * Pred Grad:  0.118, New P: 1.074
iter 7 loss: 0.007
Actual params: [-0.0027,  1.0744]
-Original Grad: 0.030, -lr * Pred Grad:  0.091, New P: 0.088
-Original Grad: 0.019, -lr * Pred Grad:  0.095, New P: 1.169
iter 8 loss: 0.006
Actual params: [0.0878, 1.1693]
-Original Grad: -0.002, -lr * Pred Grad:  0.015, New P: 0.103
-Original Grad: 0.026, -lr * Pred Grad:  0.078, New P: 1.248
iter 9 loss: 0.005
Actual params: [0.103 , 1.2478]
-Original Grad: 0.026, -lr * Pred Grad:  0.052, New P: 0.155
-Original Grad: 0.003, -lr * Pred Grad:  0.020, New P: 1.267
iter 10 loss: 0.005
Actual params: [0.1551, 1.2674]
-Original Grad: 0.016, -lr * Pred Grad:  0.033, New P: 0.188
-Original Grad: -0.001, -lr * Pred Grad:  0.004, New P: 1.271
iter 11 loss: 0.004
Actual params: [0.1878, 1.271 ]
-Original Grad: 0.003, -lr * Pred Grad:  0.012, New P: 0.200
-Original Grad: 0.010, -lr * Pred Grad:  0.033, New P: 1.304
iter 12 loss: 0.004
Actual params: [0.1996, 1.3039]
-Original Grad: 0.014, -lr * Pred Grad:  0.030, New P: 0.229
-Original Grad: -0.001, -lr * Pred Grad:  0.003, New P: 1.307
iter 13 loss: 0.004
Actual params: [0.2292, 1.3073]
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: 0.230
-Original Grad: -0.008, -lr * Pred Grad:  -0.024, New P: 1.284
iter 14 loss: 0.004
Actual params: [0.23  , 1.2837]
-Original Grad: -0.000, -lr * Pred Grad:  0.004, New P: 0.234
-Original Grad: 0.009, -lr * Pred Grad:  0.024, New P: 1.307
iter 15 loss: 0.004
Actual params: [0.2336, 1.3074]
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: 0.232
-Original Grad: -0.012, -lr * Pred Grad:  -0.029, New P: 1.279
iter 16 loss: 0.004
Actual params: [0.2318, 1.2786]
-Original Grad: -0.018, -lr * Pred Grad:  -0.028, New P: 0.204
-Original Grad: 0.023, -lr * Pred Grad:  0.034, New P: 1.312
iter 17 loss: 0.004
Actual params: [0.204 , 1.3121]
-Original Grad: 0.006, -lr * Pred Grad:  0.010, New P: 0.214
-Original Grad: -0.006, -lr * Pred Grad:  -0.010, New P: 1.302
iter 18 loss: 0.004
Actual params: [0.2143, 1.3019]
-Original Grad: 0.013, -lr * Pred Grad:  0.021, New P: 0.235
-Original Grad: -0.002, -lr * Pred Grad:  -0.003, New P: 1.299
iter 19 loss: 0.004
Actual params: [0.2352, 1.2985]
-Original Grad: -0.006, -lr * Pred Grad:  -0.004, New P: 0.231
-Original Grad: -0.016, -lr * Pred Grad:  -0.025, New P: 1.273
iter 20 loss: 0.004
Actual params: [0.2313, 1.2734]
-Original Grad: 0.017, -lr * Pred Grad:  0.025, New P: 0.257
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: 1.266
iter 21 loss: 0.005
Actual params: [0.2567, 1.2658]
-Original Grad: -0.027, -lr * Pred Grad:  -0.027, New P: 0.230
-Original Grad: 0.020, -lr * Pred Grad:  0.030, New P: 1.296
iter 22 loss: 0.004
Actual params: [0.2302, 1.2959]
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: 0.231
-Original Grad: 0.008, -lr * Pred Grad:  0.011, New P: 1.306
iter 23 loss: 0.004
Actual params: [0.2313, 1.3064]
-Original Grad: -0.019, -lr * Pred Grad:  -0.016, New P: 0.215
-Original Grad: -0.015, -lr * Pred Grad:  -0.016, New P: 1.290
iter 24 loss: 0.004
Actual params: [0.2151, 1.2901]
-Original Grad: 0.012, -lr * Pred Grad:  0.012, New P: 0.227
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: 1.286
iter 25 loss: 0.004
Actual params: [0.2273, 1.2863]
-Original Grad: 0.005, -lr * Pred Grad:  0.005, New P: 0.232
-Original Grad: 0.012, -lr * Pred Grad:  0.015, New P: 1.302
iter 26 loss: 0.004
Actual params: [0.2324, 1.3016]
-Original Grad: 0.002, -lr * Pred Grad:  0.002, New P: 0.235
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: 1.301
iter 27 loss: 0.004
Actual params: [0.2348, 1.3008]
-Original Grad: -0.014, -lr * Pred Grad:  -0.015, New P: 0.220
-Original Grad: 0.008, -lr * Pred Grad:  0.010, New P: 1.311
iter 28 loss: 0.004
Actual params: [0.2197, 1.3108]
-Original Grad: 0.006, -lr * Pred Grad:  0.008, New P: 0.227
-Original Grad: -0.007, -lr * Pred Grad:  -0.009, New P: 1.302
iter 29 loss: 0.004
Actual params: [0.2275, 1.3017]
-Original Grad: 0.010, -lr * Pred Grad:  0.012, New P: 0.240
-Original Grad: -0.002, -lr * Pred Grad:  -0.003, New P: 1.298
iter 30 loss: 0.004
Actual params: [0.2397, 1.2983]
-Original Grad: 0.011, -lr * Pred Grad:  0.014, New P: 0.253
-Original Grad: -0.002, -lr * Pred Grad:  -0.005, New P: 1.294
Target params: [1.3344, 1.5708]
iter 0 loss: 0.370
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.261, -lr * Pred Grad:  -0.425, New P: -0.897
-Original Grad: -0.024, -lr * Pred Grad:  0.048, New P: 0.052
iter 1 loss: 0.313
Actual params: [-0.8974,  0.052 ]
-Original Grad: -0.239, -lr * Pred Grad:  -0.260, New P: -1.157
-Original Grad: -0.042, -lr * Pred Grad:  -0.083, New P: -0.031
iter 2 loss: 0.292
Actual params: [-1.157 , -0.0313]
-Original Grad: -0.077, -lr * Pred Grad:  -0.057, New P: -1.214
-Original Grad: -0.032, -lr * Pred Grad:  -0.248, New P: -0.280
iter 3 loss: 0.287
Actual params: [-1.2138, -0.2796]
-Original Grad: -0.031, -lr * Pred Grad:  -0.012, New P: -1.226
-Original Grad: -0.020, -lr * Pred Grad:  -0.196, New P: -0.475
iter 4 loss: 0.285
Actual params: [-1.2255, -0.4752]
-Original Grad: -0.013, -lr * Pred Grad:  -0.002, New P: -1.228
-Original Grad: -0.010, -lr * Pred Grad:  -0.115, New P: -0.590
iter 5 loss: 0.284
Actual params: [-1.2279, -0.5898]
-Original Grad: -0.008, -lr * Pred Grad:  -0.000, New P: -1.228
-Original Grad: -0.006, -lr * Pred Grad:  -0.080, New P: -0.670
iter 6 loss: 0.284
Actual params: [-1.2281, -0.6703]
-Original Grad: -0.005, -lr * Pred Grad:  0.000, New P: -1.228
-Original Grad: -0.004, -lr * Pred Grad:  -0.062, New P: -0.732
iter 7 loss: 0.284
Actual params: [-1.2278, -0.7323]
-Original Grad: -0.004, -lr * Pred Grad:  0.000, New P: -1.228
-Original Grad: -0.003, -lr * Pred Grad:  -0.050, New P: -0.782
iter 8 loss: 0.284
Actual params: [-1.2275, -0.7821]
-Original Grad: -0.003, -lr * Pred Grad:  0.001, New P: -1.227
-Original Grad: -0.003, -lr * Pred Grad:  -0.046, New P: -0.828
iter 9 loss: 0.284
Actual params: [-1.2267, -0.8281]
-Original Grad: -0.002, -lr * Pred Grad:  0.001, New P: -1.226
-Original Grad: -0.002, -lr * Pred Grad:  -0.035, New P: -0.863
iter 10 loss: 0.284
Actual params: [-1.2257, -0.863 ]
-Original Grad: -0.002, -lr * Pred Grad:  0.001, New P: -1.225
-Original Grad: -0.002, -lr * Pred Grad:  -0.039, New P: -0.902
iter 11 loss: 0.284
Actual params: [-1.2246, -0.9023]
-Original Grad: -0.001, -lr * Pred Grad:  0.001, New P: -1.223
-Original Grad: -0.001, -lr * Pred Grad:  -0.037, New P: -0.940
iter 12 loss: 0.284
Actual params: [-1.2234, -0.9396]
-Original Grad: -0.001, -lr * Pred Grad:  0.001, New P: -1.222
-Original Grad: -0.001, -lr * Pred Grad:  -0.030, New P: -0.970
iter 13 loss: 0.284
Actual params: [-1.2224, -0.9698]
-Original Grad: -0.001, -lr * Pred Grad:  0.001, New P: -1.221
-Original Grad: -0.001, -lr * Pred Grad:  -0.030, New P: -1.000
iter 14 loss: 0.284
Actual params: [-1.2213, -0.9999]
-Original Grad: -0.001, -lr * Pred Grad:  0.001, New P: -1.220
-Original Grad: -0.001, -lr * Pred Grad:  -0.028, New P: -1.028
iter 15 loss: 0.284
Actual params: [-1.2201, -1.0278]
-Original Grad: -0.001, -lr * Pred Grad:  0.001, New P: -1.219
-Original Grad: -0.001, -lr * Pred Grad:  -0.028, New P: -1.056
iter 16 loss: 0.284
Actual params: [-1.2188, -1.0557]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.218
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -1.078
iter 17 loss: 0.284
Actual params: [-1.2179, -1.0779]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.217
-Original Grad: -0.001, -lr * Pred Grad:  -0.026, New P: -1.104
iter 18 loss: 0.284
Actual params: [-1.2167, -1.1044]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.216
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.129
iter 19 loss: 0.284
Actual params: [-1.2155, -1.129 ]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.214
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.153
iter 20 loss: 0.284
Actual params: [-1.2143, -1.1528]
-Original Grad: -0.000, -lr * Pred Grad:  0.002, New P: -1.213
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.179
iter 21 loss: 0.284
Actual params: [-1.2128, -1.1791]
-Original Grad: -0.000, -lr * Pred Grad:  0.002, New P: -1.211
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.203
iter 22 loss: 0.284
Actual params: [-1.2112, -1.2031]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.210
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.223
iter 23 loss: 0.284
Actual params: [-1.21 , -1.223]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.209
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.242
iter 24 loss: 0.284
Actual params: [-1.2088, -1.2421]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.207
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.263
iter 25 loss: 0.284
Actual params: [-1.2074, -1.2632]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.206
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.284
iter 26 loss: 0.284
Actual params: [-1.206 , -1.2839]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.205
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.304
iter 27 loss: 0.284
Actual params: [-1.2048, -1.3038]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.203
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.322
iter 28 loss: 0.284
Actual params: [-1.2035, -1.322 ]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.202
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.340
iter 29 loss: 0.284
Actual params: [-1.2022, -1.3402]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.201
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.358
iter 30 loss: 0.284
Actual params: [-1.201 , -1.3577]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.200
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.373
Target params: [1.3344, 1.5708]
iter 0 loss: 0.137
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.089, -lr * Pred Grad:  0.350, New P: -0.122
-Original Grad: 0.082, -lr * Pred Grad:  0.326, New P: 0.330
iter 1 loss: 0.050
Actual params: [-0.1221,  0.3296]
-Original Grad: 0.166, -lr * Pred Grad:  0.391, New P: 0.269
-Original Grad: 0.070, -lr * Pred Grad:  -0.017, New P: 0.312
iter 2 loss: 0.098
Actual params: [0.269 , 0.3122]
-Original Grad: -0.713, -lr * Pred Grad:  -0.086, New P: 0.183
-Original Grad: 0.283, -lr * Pred Grad:  0.274, New P: 0.586
iter 3 loss: 0.043
Actual params: [0.183 , 0.5858]
-Original Grad: -0.279, -lr * Pred Grad:  0.010, New P: 0.193
-Original Grad: 0.197, -lr * Pred Grad:  0.141, New P: 0.727
iter 4 loss: 0.032
Actual params: [0.1925, 0.7267]
-Original Grad: -0.175, -lr * Pred Grad:  0.023, New P: 0.215
-Original Grad: 0.175, -lr * Pred Grad:  0.104, New P: 0.831
iter 5 loss: 0.029
Actual params: [0.2152, 0.831 ]
-Original Grad: 0.093, -lr * Pred Grad:  0.013, New P: 0.228
-Original Grad: -0.036, -lr * Pred Grad:  0.005, New P: 0.836
iter 6 loss: 0.029
Actual params: [0.2284, 0.8355]
-Original Grad: 0.134, -lr * Pred Grad:  0.012, New P: 0.241
-Original Grad: -0.068, -lr * Pred Grad:  -0.012, New P: 0.823
iter 7 loss: 0.030
Actual params: [0.2407, 0.8235]
-Original Grad: 0.137, -lr * Pred Grad:  0.013, New P: 0.253
-Original Grad: -0.071, -lr * Pred Grad:  -0.015, New P: 0.809
iter 8 loss: 0.032
Actual params: [0.2532, 0.8085]
-Original Grad: 0.025, -lr * Pred Grad:  0.022, New P: 0.275
-Original Grad: 0.020, -lr * Pred Grad:  0.040, New P: 0.849
iter 9 loss: 0.031
Actual params: [0.2754, 0.8486]
-Original Grad: 0.083, -lr * Pred Grad:  0.017, New P: 0.293
-Original Grad: -0.032, -lr * Pred Grad:  0.010, New P: 0.859
iter 10 loss: 0.031
Actual params: [0.2928, 0.8587]
-Original Grad: 0.057, -lr * Pred Grad:  0.027, New P: 0.320
-Original Grad: -0.000, -lr * Pred Grad:  0.037, New P: 0.895
iter 11 loss: 0.031
Actual params: [0.3202, 0.8952]
-Original Grad: -0.103, -lr * Pred Grad:  -0.021, New P: 0.299
-Original Grad: 0.043, -lr * Pred Grad:  -0.013, New P: 0.882
iter 12 loss: 0.030
Actual params: [0.2992, 0.8822]
-Original Grad: 0.080, -lr * Pred Grad:  0.003, New P: 0.302
-Original Grad: -0.052, -lr * Pred Grad:  -0.017, New P: 0.865
iter 13 loss: 0.031
Actual params: [0.302 , 0.8653]
-Original Grad: -0.075, -lr * Pred Grad:  0.031, New P: 0.333
-Original Grad: 0.097, -lr * Pred Grad:  0.074, New P: 0.939
iter 14 loss: 0.031
Actual params: [0.333 , 0.9391]
-Original Grad: 0.069, -lr * Pred Grad:  -0.002, New P: 0.331
-Original Grad: -0.053, -lr * Pred Grad:  -0.021, New P: 0.918
iter 15 loss: 0.031
Actual params: [0.3313, 0.9183]
-Original Grad: 0.076, -lr * Pred Grad:  0.000, New P: 0.332
-Original Grad: -0.055, -lr * Pred Grad:  -0.020, New P: 0.899
iter 16 loss: 0.032
Actual params: [0.3317, 0.8987]
-Original Grad: -0.157, -lr * Pred Grad:  -0.036, New P: 0.296
-Original Grad: 0.071, -lr * Pred Grad:  -0.027, New P: 0.871
iter 17 loss: 0.030
Actual params: [0.2956, 0.8714]
-Original Grad: 0.034, -lr * Pred Grad:  0.023, New P: 0.319
-Original Grad: -0.001, -lr * Pred Grad:  0.032, New P: 0.904
iter 18 loss: 0.031
Actual params: [0.319 , 0.9039]
-Original Grad: 0.039, -lr * Pred Grad:  -0.005, New P: 0.314
-Original Grad: -0.032, -lr * Pred Grad:  -0.019, New P: 0.885
iter 19 loss: 0.031
Actual params: [0.3136, 0.885 ]
-Original Grad: 0.070, -lr * Pred Grad:  0.003, New P: 0.317
-Original Grad: -0.046, -lr * Pred Grad:  -0.012, New P: 0.873
iter 20 loss: 0.032
Actual params: [0.3168, 0.873 ]
-Original Grad: 0.077, -lr * Pred Grad:  0.005, New P: 0.321
-Original Grad: -0.049, -lr * Pred Grad:  -0.012, New P: 0.861
iter 21 loss: 0.033
Actual params: [0.3215, 0.861 ]
-Original Grad: -0.080, -lr * Pred Grad:  0.055, New P: 0.376
-Original Grad: 0.106, -lr * Pred Grad:  0.107, New P: 0.968
iter 22 loss: 0.033
Actual params: [0.3765, 0.9681]
-Original Grad: 0.035, -lr * Pred Grad:  -0.019, New P: 0.357
-Original Grad: -0.041, -lr * Pred Grad:  -0.040, New P: 0.929
iter 23 loss: 0.033
Actual params: [0.357 , 0.9286]
-Original Grad: -0.017, -lr * Pred Grad:  -0.009, New P: 0.348
-Original Grad: 0.006, -lr * Pred Grad:  -0.010, New P: 0.918
iter 24 loss: 0.032
Actual params: [0.3479, 0.9183]
-Original Grad: -0.057, -lr * Pred Grad:  -0.026, New P: 0.322
-Original Grad: 0.024, -lr * Pred Grad:  -0.028, New P: 0.890
iter 25 loss: 0.031
Actual params: [0.3221, 0.8902]
-Original Grad: 0.074, -lr * Pred Grad:  -0.005, New P: 0.317
-Original Grad: -0.056, -lr * Pred Grad:  -0.027, New P: 0.863
iter 26 loss: 0.032
Actual params: [0.3166, 0.8628]
-Original Grad: -0.011, -lr * Pred Grad:  0.038, New P: 0.355
-Original Grad: 0.030, -lr * Pred Grad:  0.064, New P: 0.926
iter 27 loss: 0.033
Actual params: [0.3551, 0.9264]
-Original Grad: -0.018, -lr * Pred Grad:  -0.020, New P: 0.336
-Original Grad: 0.002, -lr * Pred Grad:  -0.027, New P: 0.900
iter 28 loss: 0.032
Actual params: [0.3356, 0.8998]
-Original Grad: -0.016, -lr * Pred Grad:  -0.038, New P: 0.297
-Original Grad: -0.011, -lr * Pred Grad:  -0.059, New P: 0.840
iter 29 loss: 0.032
Actual params: [0.2973, 0.8403]
-Original Grad: 0.039, -lr * Pred Grad:  0.057, New P: 0.354
-Original Grad: 0.007, -lr * Pred Grad:  0.083, New P: 0.923
iter 30 loss: 0.033
Actual params: [0.3545, 0.923 ]
-Original Grad: 0.029, -lr * Pred Grad:  -0.007, New P: 0.348
-Original Grad: -0.024, -lr * Pred Grad:  -0.018, New P: 0.905
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: -0.466
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.005
iter 1 loss: 0.363
Actual params: [-0.4656,  0.005 ]
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: -0.454
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.008
iter 2 loss: 0.363
Actual params: [-0.4542,  0.0075]
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: -0.447
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.009
iter 3 loss: 0.363
Actual params: [-0.4471,  0.0091]
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: -0.430
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.013
iter 4 loss: 0.363
Actual params: [-0.4303,  0.0129]
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: -0.418
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.015
iter 5 loss: 0.363
Actual params: [-0.4181,  0.0155]
-Original Grad: 0.001, -lr * Pred Grad:  0.020, New P: -0.398
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.020
iter 6 loss: 0.363
Actual params: [-0.3977,  0.0198]
-Original Grad: 0.001, -lr * Pred Grad:  0.025, New P: -0.373
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.025
iter 7 loss: 0.363
Actual params: [-0.3729,  0.0248]
-Original Grad: 0.001, -lr * Pred Grad:  0.033, New P: -0.340
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.031
iter 8 loss: 0.363
Actual params: [-0.3396,  0.0315]
-Original Grad: 0.002, -lr * Pred Grad:  0.043, New P: -0.297
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.040
iter 9 loss: 0.363
Actual params: [-0.297 ,  0.0401]
-Original Grad: 0.003, -lr * Pred Grad:  0.077, New P: -0.220
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.056
iter 10 loss: 0.363
Actual params: [-0.2202,  0.0555]
-Original Grad: 0.004, -lr * Pred Grad:  0.135, New P: -0.085
-Original Grad: 0.001, -lr * Pred Grad:  0.028, New P: 0.083
iter 11 loss: 0.361
Actual params: [-0.0853,  0.0831]
-Original Grad: 0.012, -lr * Pred Grad:  0.399, New P: 0.314
-Original Grad: 0.002, -lr * Pred Grad:  0.081, New P: 0.165
iter 12 loss: 0.309
Actual params: [0.3137, 0.1646]
-Original Grad: 0.132, -lr * Pred Grad:  0.765, New P: 1.078
-Original Grad: 0.046, -lr * Pred Grad:  0.241, New P: 0.405
iter 13 loss: 0.190
Actual params: [1.0784, 0.4053]
-Original Grad: -0.018, -lr * Pred Grad:  -0.110, New P: 0.968
-Original Grad: 0.518, -lr * Pred Grad:  0.371, New P: 0.777
iter 14 loss: 0.073
Actual params: [0.9681, 0.7765]
-Original Grad: 0.023, -lr * Pred Grad:  0.088, New P: 1.056
-Original Grad: 0.353, -lr * Pred Grad:  0.139, New P: 0.916
iter 15 loss: 0.038
Actual params: [1.0563, 0.9157]
-Original Grad: 0.114, -lr * Pred Grad:  0.243, New P: 1.299
-Original Grad: 0.219, -lr * Pred Grad:  0.032, New P: 0.948
iter 16 loss: 0.012
Actual params: [1.2993, 0.9478]
-Original Grad: 0.122, -lr * Pred Grad:  0.042, New P: 1.342
-Original Grad: 0.095, -lr * Pred Grad:  0.015, New P: 0.963
iter 17 loss: 0.010
Actual params: [1.3416, 0.9629]
-Original Grad: -0.022, -lr * Pred Grad:  -0.011, New P: 1.331
-Original Grad: 0.092, -lr * Pred Grad:  0.028, New P: 0.991
iter 18 loss: 0.010
Actual params: [1.3308, 0.9912]
-Original Grad: 0.049, -lr * Pred Grad:  0.006, New P: 1.337
-Original Grad: 0.070, -lr * Pred Grad:  0.017, New P: 1.009
iter 19 loss: 0.010
Actual params: [1.3365, 1.0085]
-Original Grad: 0.055, -lr * Pred Grad:  0.007, New P: 1.344
-Original Grad: 0.036, -lr * Pred Grad:  0.008, New P: 1.017
iter 20 loss: 0.010
Actual params: [1.3435, 1.0165]
-Original Grad: -0.037, -lr * Pred Grad:  -0.006, New P: 1.338
-Original Grad: 0.046, -lr * Pred Grad:  0.012, New P: 1.029
iter 21 loss: 0.010
Actual params: [1.3378, 1.0288]
-Original Grad: 0.026, -lr * Pred Grad:  0.003, New P: 1.341
-Original Grad: -0.010, -lr * Pred Grad:  -0.003, New P: 1.026
iter 22 loss: 0.010
Actual params: [1.3412, 1.0262]
-Original Grad: 0.001, -lr * Pred Grad:  -0.000, New P: 1.341
-Original Grad: -0.022, -lr * Pred Grad:  -0.006, New P: 1.020
iter 23 loss: 0.010
Actual params: [1.3411, 1.0199]
-Original Grad: 0.056, -lr * Pred Grad:  0.006, New P: 1.347
-Original Grad: 0.041, -lr * Pred Grad:  0.011, New P: 1.031
iter 24 loss: 0.010
Actual params: [1.347 , 1.0309]
-Original Grad: -0.059, -lr * Pred Grad:  -0.007, New P: 1.340
-Original Grad: 0.022, -lr * Pred Grad:  0.007, New P: 1.038
iter 25 loss: 0.010
Actual params: [1.3402, 1.0378]
-Original Grad: 0.018, -lr * Pred Grad:  0.002, New P: 1.342
-Original Grad: 0.004, -lr * Pred Grad:  0.002, New P: 1.039
iter 26 loss: 0.010
Actual params: [1.3422, 1.0394]
-Original Grad: 0.024, -lr * Pred Grad:  0.003, New P: 1.345
-Original Grad: 0.001, -lr * Pred Grad:  0.001, New P: 1.040
iter 27 loss: 0.010
Actual params: [1.3449, 1.0404]
-Original Grad: 0.084, -lr * Pred Grad:  0.008, New P: 1.353
-Original Grad: -0.019, -lr * Pred Grad:  -0.004, New P: 1.037
iter 28 loss: 0.010
Actual params: [1.3529, 1.0366]
-Original Grad: -0.029, -lr * Pred Grad:  -0.004, New P: 1.349
-Original Grad: -0.019, -lr * Pred Grad:  -0.008, New P: 1.028
iter 29 loss: 0.010
Actual params: [1.3491, 1.0284]
-Original Grad: -0.029, -lr * Pred Grad:  -0.003, New P: 1.346
-Original Grad: 0.008, -lr * Pred Grad:  0.002, New P: 1.030
iter 30 loss: 0.010
Actual params: [1.3463, 1.0302]
-Original Grad: -0.046, -lr * Pred Grad:  -0.005, New P: 1.341
-Original Grad: -0.007, -lr * Pred Grad:  -0.005, New P: 1.025
Target params: [1.3344, 1.5708]
iter 0 loss: 0.296
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.026, -lr * Pred Grad:  -0.258, New P: -0.730
-Original Grad: 0.045, -lr * Pred Grad:  0.446, New P: 0.450
iter 1 loss: 0.279
Actual params: [-0.7301,  0.4498]
-Original Grad: 0.047, -lr * Pred Grad:  0.450, New P: -0.280
-Original Grad: 0.068, -lr * Pred Grad:  0.586, New P: 1.036
iter 2 loss: 0.217
Actual params: [-0.28  ,  1.0362]
-Original Grad: 0.156, -lr * Pred Grad:  0.645, New P: 0.365
-Original Grad: 0.100, -lr * Pred Grad:  0.320, New P: 1.356
iter 3 loss: 0.079
Actual params: [0.3652, 1.3559]
-Original Grad: 0.271, -lr * Pred Grad:  0.191, New P: 0.556
-Original Grad: 0.132, -lr * Pred Grad:  0.171, New P: 1.526
iter 4 loss: 0.027
Actual params: [0.5561, 1.5264]
-Original Grad: 0.333, -lr * Pred Grad:  0.090, New P: 0.646
-Original Grad: 0.138, -lr * Pred Grad:  0.031, New P: 1.558
iter 5 loss: 0.015
Actual params: [0.6457, 1.5576]
-Original Grad: 0.187, -lr * Pred Grad:  0.049, New P: 0.695
-Original Grad: 0.073, -lr * Pred Grad:  -0.009, New P: 1.548
iter 6 loss: 0.011
Actual params: [0.6945, 1.5484]
-Original Grad: 0.170, -lr * Pred Grad:  0.010, New P: 0.704
-Original Grad: 0.088, -lr * Pred Grad:  0.052, New P: 1.601
iter 7 loss: 0.008
Actual params: [0.7041, 1.6006]
-Original Grad: 0.099, -lr * Pred Grad:  0.025, New P: 0.729
-Original Grad: 0.039, -lr * Pred Grad:  -0.016, New P: 1.585
iter 8 loss: 0.007
Actual params: [0.7289, 1.5846]
-Original Grad: 0.203, -lr * Pred Grad:  0.047, New P: 0.776
-Original Grad: 0.065, -lr * Pred Grad:  -0.056, New P: 1.528
iter 9 loss: 0.007
Actual params: [0.776 , 1.5284]
-Original Grad: 0.060, -lr * Pred Grad:  -0.022, New P: 0.754
-Original Grad: 0.069, -lr * Pred Grad:  0.070, New P: 1.598
iter 10 loss: 0.005
Actual params: [0.7543, 1.5985]
-Original Grad: 0.106, -lr * Pred Grad:  0.001, New P: 0.755
-Original Grad: 0.070, -lr * Pred Grad:  0.028, New P: 1.626
iter 11 loss: 0.005
Actual params: [0.7553, 1.6264]
-Original Grad: 0.157, -lr * Pred Grad:  0.018, New P: 0.773
-Original Grad: 0.056, -lr * Pred Grad:  -0.006, New P: 1.621
iter 12 loss: 0.004
Actual params: [0.7729, 1.6208]
-Original Grad: 0.109, -lr * Pred Grad:  0.011, New P: 0.783
-Original Grad: 0.038, -lr * Pred Grad:  -0.003, New P: 1.618
iter 13 loss: 0.004
Actual params: [0.7835, 1.618 ]
-Original Grad: 0.046, -lr * Pred Grad:  0.002, New P: 0.785
-Original Grad: 0.025, -lr * Pred Grad:  0.007, New P: 1.625
iter 14 loss: 0.004
Actual params: [0.7852, 1.6248]
-Original Grad: 0.093, -lr * Pred Grad:  0.003, New P: 0.789
-Original Grad: 0.052, -lr * Pred Grad:  0.012, New P: 1.637
iter 15 loss: 0.003
Actual params: [0.7887, 1.6371]
-Original Grad: 0.091, -lr * Pred Grad:  0.009, New P: 0.798
-Original Grad: 0.026, -lr * Pred Grad:  -0.005, New P: 1.632
iter 16 loss: 0.003
Actual params: [0.7978, 1.632 ]
-Original Grad: 0.024, -lr * Pred Grad:  0.000, New P: 0.798
-Original Grad: 0.015, -lr * Pred Grad:  0.005, New P: 1.637
iter 17 loss: 0.003
Actual params: [0.7979, 1.6369]
-Original Grad: 0.041, -lr * Pred Grad:  0.003, New P: 0.801
-Original Grad: 0.014, -lr * Pred Grad:  -0.001, New P: 1.636
iter 18 loss: 0.003
Actual params: [0.8012, 1.6364]
-Original Grad: 0.065, -lr * Pred Grad:  0.006, New P: 0.808
-Original Grad: 0.017, -lr * Pred Grad:  -0.004, New P: 1.632
iter 19 loss: 0.003
Actual params: [0.8075, 1.6324]
-Original Grad: 0.050, -lr * Pred Grad:  0.002, New P: 0.810
-Original Grad: 0.026, -lr * Pred Grad:  0.006, New P: 1.638
iter 20 loss: 0.003
Actual params: [0.8095, 1.6384]
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: 0.807
-Original Grad: 0.010, -lr * Pred Grad:  0.006, New P: 1.645
iter 21 loss: 0.003
Actual params: [0.8074, 1.6447]
-Original Grad: -0.011, -lr * Pred Grad:  -0.001, New P: 0.807
-Original Grad: -0.003, -lr * Pred Grad:  0.001, New P: 1.645
iter 22 loss: 0.003
Actual params: [0.8065, 1.6452]
-Original Grad: 0.066, -lr * Pred Grad:  0.006, New P: 0.812
-Original Grad: 0.017, -lr * Pred Grad:  -0.003, New P: 1.642
iter 23 loss: 0.003
Actual params: [0.8122, 1.6422]
-Original Grad: 0.008, -lr * Pred Grad:  -0.001, New P: 0.811
-Original Grad: 0.010, -lr * Pred Grad:  0.005, New P: 1.647
iter 24 loss: 0.003
Actual params: [0.8111, 1.6474]
-Original Grad: -0.033, -lr * Pred Grad:  -0.006, New P: 0.805
-Original Grad: 0.005, -lr * Pred Grad:  0.011, New P: 1.658
iter 25 loss: 0.003
Actual params: [0.8051, 1.658 ]
-Original Grad: -0.034, -lr * Pred Grad:  -0.002, New P: 0.803
-Original Grad: -0.015, -lr * Pred Grad:  -0.002, New P: 1.656
iter 26 loss: 0.003
Actual params: [0.8034, 1.6557]
-Original Grad: 0.003, -lr * Pred Grad:  -0.001, New P: 0.803
-Original Grad: 0.004, -lr * Pred Grad:  0.002, New P: 1.658
iter 27 loss: 0.003
Actual params: [0.8029, 1.6578]
-Original Grad: 0.021, -lr * Pred Grad:  0.001, New P: 0.804
-Original Grad: 0.010, -lr * Pred Grad:  0.002, New P: 1.660
iter 28 loss: 0.003
Actual params: [0.8038, 1.6599]
-Original Grad: 0.045, -lr * Pred Grad:  0.004, New P: 0.808
-Original Grad: 0.012, -lr * Pred Grad:  -0.002, New P: 1.658
iter 29 loss: 0.003
Actual params: [0.8079, 1.6583]
-Original Grad: 0.012, -lr * Pred Grad:  0.001, New P: 0.809
-Original Grad: 0.003, -lr * Pred Grad:  -0.001, New P: 1.658
iter 30 loss: 0.003
Actual params: [0.8091, 1.6576]
-Original Grad: -0.018, -lr * Pred Grad:  -0.000, New P: 0.809
-Original Grad: -0.011, -lr * Pred Grad:  -0.003, New P: 1.654
Target params: [1.3344, 1.5708]
iter 0 loss: 0.494
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.200, -lr * Pred Grad:  -0.452, New P: -0.924
-Original Grad: -0.121, -lr * Pred Grad:  -0.261, New P: -0.257
iter 1 loss: 0.436
Actual params: [-0.9239, -0.2574]
-Original Grad: -0.026, -lr * Pred Grad:  -0.042, New P: -0.966
-Original Grad: -0.021, -lr * Pred Grad:  -0.086, New P: -0.343
iter 2 loss: 0.435
Actual params: [-0.9659, -0.3429]
-Original Grad: -0.013, -lr * Pred Grad:  -0.020, New P: -0.986
-Original Grad: -0.011, -lr * Pred Grad:  -0.056, New P: -0.399
iter 3 loss: 0.434
Actual params: [-0.9857, -0.3991]
-Original Grad: -0.008, -lr * Pred Grad:  -0.014, New P: -1.000
-Original Grad: -0.007, -lr * Pred Grad:  -0.039, New P: -0.438
iter 4 loss: 0.434
Actual params: [-0.9999, -0.4381]
-Original Grad: -0.005, -lr * Pred Grad:  -0.007, New P: -1.007
-Original Grad: -0.005, -lr * Pred Grad:  -0.031, New P: -0.469
iter 5 loss: 0.434
Actual params: [-1.0073, -0.4687]
-Original Grad: -0.004, -lr * Pred Grad:  -0.006, New P: -1.013
-Original Grad: -0.004, -lr * Pred Grad:  -0.028, New P: -0.497
iter 6 loss: 0.434
Actual params: [-1.0129, -0.497 ]
-Original Grad: -0.004, -lr * Pred Grad:  -0.006, New P: -1.019
-Original Grad: -0.004, -lr * Pred Grad:  -0.029, New P: -0.526
iter 7 loss: 0.434
Actual params: [-1.019 , -0.5262]
-Original Grad: -0.002, -lr * Pred Grad:  -0.003, New P: -1.022
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.549
iter 8 loss: 0.433
Actual params: [-1.022 , -0.5492]
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: -1.026
-Original Grad: -0.002, -lr * Pred Grad:  -0.025, New P: -0.575
iter 9 loss: 0.433
Actual params: [-1.026 , -0.5745]
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: -1.030
-Original Grad: -0.002, -lr * Pred Grad:  -0.026, New P: -0.601
iter 10 loss: 0.433
Actual params: [-1.0298, -0.6008]
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: -1.032
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: -0.623
iter 11 loss: 0.433
Actual params: [-1.0316, -0.6228]
-Original Grad: -0.002, -lr * Pred Grad:  -0.003, New P: -1.035
-Original Grad: -0.002, -lr * Pred Grad:  -0.025, New P: -0.648
iter 12 loss: 0.433
Actual params: [-1.0347, -0.6475]
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: -1.037
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -0.670
iter 13 loss: 0.433
Actual params: [-1.037 , -0.6696]
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -1.038
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -0.691
iter 14 loss: 0.433
Actual params: [-1.0383, -0.6908]
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: -1.041
-Original Grad: -0.001, -lr * Pred Grad:  -0.024, New P: -0.715
iter 15 loss: 0.433
Actual params: [-1.0405, -0.715 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: -1.041
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -0.737
iter 16 loss: 0.433
Actual params: [-1.0415, -0.7374]
-Original Grad: -0.001, -lr * Pred Grad:  -0.000, New P: -1.042
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.756
iter 17 loss: 0.433
Actual params: [-1.0418, -0.7559]
-Original Grad: -0.001, -lr * Pred Grad:  0.000, New P: -1.042
-Original Grad: -0.001, -lr * Pred Grad:  -0.024, New P: -0.780
iter 18 loss: 0.433
Actual params: [-1.0416, -0.7804]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.041
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -0.801
iter 19 loss: 0.433
Actual params: [-1.0411, -0.8008]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.040
-Original Grad: -0.001, -lr * Pred Grad:  -0.023, New P: -0.824
iter 20 loss: 0.433
Actual params: [-1.0404, -0.824 ]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.040
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -0.844
iter 21 loss: 0.433
Actual params: [-1.0399, -0.8439]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.039
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -0.862
iter 22 loss: 0.433
Actual params: [-1.0393, -0.8621]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.038
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -0.883
iter 23 loss: 0.433
Actual params: [-1.0384, -0.8827]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.038
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -0.906
iter 24 loss: 0.433
Actual params: [-1.0375, -0.9055]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.037
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -0.925
iter 25 loss: 0.433
Actual params: [-1.0368, -0.9245]
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: -1.036
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -0.942
iter 26 loss: 0.433
Actual params: [-1.0364, -0.9422]
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: -1.036
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.956
iter 27 loss: 0.433
Actual params: [-1.0362, -0.9559]
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: -1.036
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.970
iter 28 loss: 0.433
Actual params: [-1.0359, -0.9699]
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: -1.036
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.982
iter 29 loss: 0.433
Actual params: [-1.0357, -0.9819]
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: -1.035
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.998
iter 30 loss: 0.433
Actual params: [-1.0353, -0.998 ]
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: -1.035
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.008
Target params: [1.3344, 1.5708]
iter 0 loss: 0.746
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.016, -lr * Pred Grad:  0.172, New P: -0.301
-Original Grad: 0.010, -lr * Pred Grad:  0.107, New P: 0.111
iter 1 loss: 0.737
Actual params: [-0.3008,  0.1107]
-Original Grad: 0.017, -lr * Pred Grad:  0.196, New P: -0.104
-Original Grad: 0.012, -lr * Pred Grad:  0.134, New P: 0.245
iter 2 loss: 0.719
Actual params: [-0.1044,  0.2447]
-Original Grad: 0.051, -lr * Pred Grad:  0.516, New P: 0.411
-Original Grad: 0.029, -lr * Pred Grad:  0.286, New P: 0.531
iter 3 loss: 0.592
Actual params: [0.4113, 0.531 ]
-Original Grad: 0.247, -lr * Pred Grad:  0.635, New P: 1.046
-Original Grad: 0.096, -lr * Pred Grad:  0.213, New P: 0.744
iter 4 loss: 0.285
Actual params: [1.0459, 0.7437]
-Original Grad: -0.094, -lr * Pred Grad:  -0.099, New P: 0.947
-Original Grad: 0.325, -lr * Pred Grad:  0.525, New P: 1.269
iter 5 loss: 0.177
Actual params: [0.9474, 1.2689]
-Original Grad: 1.251, -lr * Pred Grad:  0.101, New P: 1.049
-Original Grad: 0.044, -lr * Pred Grad:  0.058, New P: 1.326
iter 6 loss: 0.145
Actual params: [1.0488, 1.3265]
-Original Grad: 0.570, -lr * Pred Grad:  0.033, New P: 1.082
-Original Grad: 0.138, -lr * Pred Grad:  0.170, New P: 1.496
iter 7 loss: 0.117
Actual params: [1.082 , 1.4962]
-Original Grad: 0.621, -lr * Pred Grad:  0.031, New P: 1.113
-Original Grad: 0.069, -lr * Pred Grad:  0.068, New P: 1.565
iter 8 loss: 0.109
Actual params: [1.1126, 1.5646]
-Original Grad: 0.531, -lr * Pred Grad:  0.024, New P: 1.136
-Original Grad: 0.047, -lr * Pred Grad:  0.045, New P: 1.610
iter 9 loss: 0.104
Actual params: [1.1363, 1.6097]
-Original Grad: 0.674, -lr * Pred Grad:  0.023, New P: 1.160
-Original Grad: 0.086, -lr * Pred Grad:  0.084, New P: 1.694
iter 10 loss: 0.102
Actual params: [1.1597, 1.6937]
-Original Grad: 0.399, -lr * Pred Grad:  0.017, New P: 1.176
-Original Grad: -0.144, -lr * Pred Grad:  -0.090, New P: 1.604
iter 11 loss: 0.096
Actual params: [1.1764, 1.6038]
-Original Grad: 0.255, -lr * Pred Grad:  0.011, New P: 1.187
-Original Grad: -0.014, -lr * Pred Grad:  -0.004, New P: 1.599
iter 12 loss: 0.094
Actual params: [1.1873, 1.5994]
-Original Grad: 0.176, -lr * Pred Grad:  0.008, New P: 1.195
-Original Grad: 0.012, -lr * Pred Grad:  0.005, New P: 1.604
iter 13 loss: 0.093
Actual params: [1.1954, 1.6041]
-Original Grad: 0.267, -lr * Pred Grad:  0.011, New P: 1.207
-Original Grad: -0.112, -lr * Pred Grad:  -0.021, New P: 1.583
iter 14 loss: 0.092
Actual params: [1.2067, 1.5827]
-Original Grad: -0.060, -lr * Pred Grad:  0.001, New P: 1.208
-Original Grad: 0.255, -lr * Pred Grad:  0.052, New P: 1.634
iter 15 loss: 0.091
Actual params: [1.2078, 1.6343]
-Original Grad: 0.303, -lr * Pred Grad:  0.014, New P: 1.222
-Original Grad: -0.111, -lr * Pred Grad:  -0.016, New P: 1.618
iter 16 loss: 0.089
Actual params: [1.2222, 1.6184]
-Original Grad: 0.244, -lr * Pred Grad:  0.012, New P: 1.234
-Original Grad: -0.077, -lr * Pred Grad:  -0.007, New P: 1.612
iter 17 loss: 0.088
Actual params: [1.2341, 1.6118]
-Original Grad: -0.048, -lr * Pred Grad:  0.001, New P: 1.235
-Original Grad: 0.141, -lr * Pred Grad:  0.020, New P: 1.632
iter 18 loss: 0.087
Actual params: [1.2351, 1.6321]
-Original Grad: 0.168, -lr * Pred Grad:  0.009, New P: 1.244
-Original Grad: -0.058, -lr * Pred Grad:  -0.003, New P: 1.630
iter 19 loss: 0.087
Actual params: [1.2439, 1.6296]
-Original Grad: 0.102, -lr * Pred Grad:  0.010, New P: 1.254
-Original Grad: 0.105, -lr * Pred Grad:  0.018, New P: 1.648
iter 20 loss: 0.085
Actual params: [1.2542, 1.6477]
-Original Grad: 0.031, -lr * Pred Grad:  0.009, New P: 1.263
-Original Grad: 0.186, -lr * Pred Grad:  0.028, New P: 1.675
iter 21 loss: 0.084
Actual params: [1.2628, 1.6754]
-Original Grad: 0.086, -lr * Pred Grad:  0.010, New P: 1.272
-Original Grad: 0.086, -lr * Pred Grad:  0.016, New P: 1.691
iter 22 loss: 0.082
Actual params: [1.2724, 1.6914]
-Original Grad: 0.070, -lr * Pred Grad:  0.007, New P: 1.279
-Original Grad: 0.036, -lr * Pred Grad:  0.008, New P: 1.700
iter 23 loss: 0.082
Actual params: [1.2792, 1.6996]
-Original Grad: 0.118, -lr * Pred Grad:  0.006, New P: 1.286
-Original Grad: -0.064, -lr * Pred Grad:  -0.003, New P: 1.696
iter 24 loss: 0.081
Actual params: [1.2857, 1.6961]
-Original Grad: -0.079, -lr * Pred Grad:  -0.003, New P: 1.283
-Original Grad: 0.078, -lr * Pred Grad:  0.007, New P: 1.703
iter 25 loss: 0.081
Actual params: [1.2831, 1.703 ]
-Original Grad: 0.026, -lr * Pred Grad:  0.006, New P: 1.289
-Original Grad: 0.063, -lr * Pred Grad:  0.010, New P: 1.713
iter 26 loss: 0.080
Actual params: [1.2889, 1.7134]
-Original Grad: -0.071, -lr * Pred Grad:  -0.002, New P: 1.287
-Original Grad: 0.080, -lr * Pred Grad:  0.007, New P: 1.721
iter 27 loss: 0.080
Actual params: [1.2871, 1.7206]
-Original Grad: 0.029, -lr * Pred Grad:  0.006, New P: 1.293
-Original Grad: 0.045, -lr * Pred Grad:  0.009, New P: 1.729
iter 28 loss: 0.080
Actual params: [1.293 , 1.7294]
-Original Grad: 0.268, -lr * Pred Grad:  0.009, New P: 1.302
-Original Grad: -0.221, -lr * Pred Grad:  -0.013, New P: 1.716
iter 29 loss: 0.079
Actual params: [1.302 , 1.7161]
-Original Grad: 0.225, -lr * Pred Grad:  0.012, New P: 1.314
-Original Grad: -0.119, -lr * Pred Grad:  0.000, New P: 1.716
iter 30 loss: 0.078
Actual params: [1.3145, 1.7163]
-Original Grad: -0.054, -lr * Pred Grad:  -0.005, New P: 1.310
-Original Grad: 0.013, -lr * Pred Grad:  -0.003, New P: 1.714
Target params: [1.3344, 1.5708]
iter 0 loss: 0.103
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.038, -lr * Pred Grad:  -0.346, New P: -0.819
-Original Grad: 0.028, -lr * Pred Grad:  0.266, New P: 0.269
iter 1 loss: 0.097
Actual params: [-0.8187,  0.2692]
-Original Grad: -0.003, -lr * Pred Grad:  -0.031, New P: -0.850
-Original Grad: 0.002, -lr * Pred Grad:  0.022, New P: 0.291
iter 2 loss: 0.097
Actual params: [-0.8502,  0.2911]
-Original Grad: -0.003, -lr * Pred Grad:  -0.029, New P: -0.879
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 0.308
iter 3 loss: 0.097
Actual params: [-0.8794,  0.3076]
-Original Grad: -0.002, -lr * Pred Grad:  -0.025, New P: -0.904
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 0.324
iter 4 loss: 0.097
Actual params: [-0.9043,  0.3243]
-Original Grad: -0.002, -lr * Pred Grad:  -0.025, New P: -0.929
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.338
iter 5 loss: 0.097
Actual params: [-0.9294,  0.3378]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -0.949
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.354
iter 6 loss: 0.097
Actual params: [-0.9493,  0.3536]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -0.969
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.367
iter 7 loss: 0.097
Actual params: [-0.9691,  0.3668]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -0.989
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.382
iter 8 loss: 0.097
Actual params: [-0.9894,  0.3818]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -1.009
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.397
iter 9 loss: 0.097
Actual params: [-1.0089,  0.3969]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -1.029
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.413
iter 10 loss: 0.097
Actual params: [-1.0286,  0.4129]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -1.049
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 0.430
iter 11 loss: 0.097
Actual params: [-1.0489,  0.4301]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.062
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.441
iter 12 loss: 0.097
Actual params: [-1.062 ,  0.4409]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.076
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.453
iter 13 loss: 0.097
Actual params: [-1.0762,  0.4529]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.092
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.468
iter 14 loss: 0.097
Actual params: [-1.0918,  0.4677]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.106
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.481
iter 15 loss: 0.097
Actual params: [-1.1055,  0.4813]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.119
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.496
iter 16 loss: 0.097
Actual params: [-1.1187,  0.4957]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.129
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.507
iter 17 loss: 0.097
Actual params: [-1.1293,  0.5073]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.139
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.517
iter 18 loss: 0.097
Actual params: [-1.1387,  0.5174]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.148
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.528
iter 19 loss: 0.097
Actual params: [-1.1477,  0.5277]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.157
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.540
iter 20 loss: 0.097
Actual params: [-1.1568,  0.54  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.165
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.552
iter 21 loss: 0.097
Actual params: [-1.1651,  0.5517]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.172
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.562
iter 22 loss: 0.097
Actual params: [-1.1725,  0.5623]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.179
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.572
iter 23 loss: 0.097
Actual params: [-1.179 ,  0.5725]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.186
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.585
iter 24 loss: 0.097
Actual params: [-1.1857,  0.5851]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.191
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.598
iter 25 loss: 0.097
Actual params: [-1.1914,  0.5981]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.196
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.613
iter 26 loss: 0.097
Actual params: [-1.1963,  0.6133]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.200
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.626
iter 27 loss: 0.097
Actual params: [-1.2001,  0.6263]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -1.203
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.640
iter 28 loss: 0.097
Actual params: [-1.2029,  0.6401]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -1.204
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.655
iter 29 loss: 0.097
Actual params: [-1.2043,  0.6551]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -1.204
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.673
iter 30 loss: 0.097
Actual params: [-1.2038,  0.6732]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -1.200
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.693
Target params: [1.3344, 1.5708]
iter 0 loss: 0.185
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.040, -lr * Pred Grad:  0.407, New P: -0.065
-Original Grad: 0.017, -lr * Pred Grad:  0.168, New P: 0.172
iter 1 loss: 0.123
Actual params: [-0.065 ,  0.1715]
-Original Grad: 0.495, -lr * Pred Grad:  0.197, New P: 0.132
-Original Grad: 0.079, -lr * Pred Grad:  0.070, New P: 0.241
iter 2 loss: 0.059
Actual params: [0.132 , 0.2412]
-Original Grad: 0.310, -lr * Pred Grad:  0.068, New P: 0.200
-Original Grad: 0.060, -lr * Pred Grad:  0.137, New P: 0.379
iter 3 loss: 0.048
Actual params: [0.1995, 0.3787]
-Original Grad: 0.247, -lr * Pred Grad:  0.050, New P: 0.249
-Original Grad: 0.039, -lr * Pred Grad:  0.033, New P: 0.411
iter 4 loss: 0.047
Actual params: [0.249 , 0.4114]
-Original Grad: -0.117, -lr * Pred Grad:  -0.002, New P: 0.247
-Original Grad: 0.099, -lr * Pred Grad:  0.164, New P: 0.576
iter 5 loss: 0.042
Actual params: [0.2473, 0.5757]
-Original Grad: 0.361, -lr * Pred Grad:  0.034, New P: 0.282
-Original Grad: 0.002, -lr * Pred Grad:  0.041, New P: 0.617
iter 6 loss: 0.038
Actual params: [0.2815, 0.6171]
-Original Grad: 0.117, -lr * Pred Grad:  0.014, New P: 0.295
-Original Grad: 0.036, -lr * Pred Grad:  0.082, New P: 0.699
iter 7 loss: 0.036
Actual params: [0.2952, 0.6989]
-Original Grad: 0.171, -lr * Pred Grad:  0.013, New P: 0.308
-Original Grad: -0.012, -lr * Pred Grad:  -0.002, New P: 0.697
iter 8 loss: 0.035
Actual params: [0.3078, 0.697 ]
-Original Grad: 0.080, -lr * Pred Grad:  0.011, New P: 0.319
-Original Grad: 0.032, -lr * Pred Grad:  0.077, New P: 0.774
iter 9 loss: 0.033
Actual params: [0.3187, 0.7742]
-Original Grad: 0.206, -lr * Pred Grad:  0.017, New P: 0.336
-Original Grad: 0.010, -lr * Pred Grad:  0.049, New P: 0.823
iter 10 loss: 0.031
Actual params: [0.3357, 0.8235]
-Original Grad: 0.117, -lr * Pred Grad:  0.012, New P: 0.347
-Original Grad: 0.016, -lr * Pred Grad:  0.047, New P: 0.870
iter 11 loss: 0.030
Actual params: [0.3473, 0.8702]
-Original Grad: 0.089, -lr * Pred Grad:  0.014, New P: 0.361
-Original Grad: 0.060, -lr * Pred Grad:  0.091, New P: 0.961
iter 12 loss: 0.029
Actual params: [0.3613, 0.9613]
-Original Grad: 0.166, -lr * Pred Grad:  0.016, New P: 0.377
-Original Grad: 0.034, -lr * Pred Grad:  0.064, New P: 1.025
iter 13 loss: 0.028
Actual params: [0.3769, 1.0253]
-Original Grad: 0.112, -lr * Pred Grad:  0.007, New P: 0.383
-Original Grad: -0.017, -lr * Pred Grad:  -0.010, New P: 1.015
iter 14 loss: 0.027
Actual params: [0.3834, 1.0151]
-Original Grad: 0.196, -lr * Pred Grad:  0.013, New P: 0.397
-Original Grad: -0.018, -lr * Pred Grad:  0.001, New P: 1.016
iter 15 loss: 0.026
Actual params: [0.3967, 1.0157]
-Original Grad: 0.152, -lr * Pred Grad:  0.011, New P: 0.408
-Original Grad: -0.013, -lr * Pred Grad:  0.003, New P: 1.019
iter 16 loss: 0.025
Actual params: [0.4075, 1.0192]
-Original Grad: 0.137, -lr * Pred Grad:  0.015, New P: 0.423
-Original Grad: 0.026, -lr * Pred Grad:  0.058, New P: 1.077
iter 17 loss: 0.025
Actual params: [0.4228, 1.0769]
-Original Grad: 0.110, -lr * Pred Grad:  0.008, New P: 0.431
-Original Grad: -0.012, -lr * Pred Grad:  -0.002, New P: 1.075
iter 18 loss: 0.024
Actual params: [0.4313, 1.0745]
-Original Grad: 0.174, -lr * Pred Grad:  0.015, New P: 0.446
-Original Grad: -0.008, -lr * Pred Grad:  0.014, New P: 1.088
iter 19 loss: 0.023
Actual params: [0.4464, 1.0884]
-Original Grad: 0.083, -lr * Pred Grad:  0.008, New P: 0.454
-Original Grad: -0.003, -lr * Pred Grad:  0.009, New P: 1.097
iter 20 loss: 0.022
Actual params: [0.4541, 1.0973]
-Original Grad: 0.073, -lr * Pred Grad:  0.012, New P: 0.466
-Original Grad: 0.027, -lr * Pred Grad:  0.049, New P: 1.147
iter 21 loss: 0.021
Actual params: [0.4657, 1.1465]
-Original Grad: 0.130, -lr * Pred Grad:  0.011, New P: 0.477
-Original Grad: -0.016, -lr * Pred Grad:  -0.001, New P: 1.146
iter 22 loss: 0.021
Actual params: [0.4769, 1.1459]
-Original Grad: 0.089, -lr * Pred Grad:  0.009, New P: 0.486
-Original Grad: -0.006, -lr * Pred Grad:  0.008, New P: 1.154
iter 23 loss: 0.020
Actual params: [0.486, 1.154]
-Original Grad: 0.097, -lr * Pred Grad:  0.011, New P: 0.497
-Original Grad: -0.003, -lr * Pred Grad:  0.015, New P: 1.169
iter 24 loss: 0.019
Actual params: [0.497, 1.169]
-Original Grad: 0.075, -lr * Pred Grad:  0.008, New P: 0.505
-Original Grad: -0.007, -lr * Pred Grad:  0.007, New P: 1.176
iter 25 loss: 0.019
Actual params: [0.5052, 1.1761]
-Original Grad: 0.056, -lr * Pred Grad:  0.011, New P: 0.516
-Original Grad: 0.013, -lr * Pred Grad:  0.031, New P: 1.207
iter 26 loss: 0.018
Actual params: [0.5159, 1.2068]
-Original Grad: 0.106, -lr * Pred Grad:  0.012, New P: 0.528
-Original Grad: -0.013, -lr * Pred Grad:  0.008, New P: 1.215
iter 27 loss: 0.018
Actual params: [0.5279, 1.2152]
-Original Grad: 0.122, -lr * Pred Grad:  0.018, New P: 0.546
-Original Grad: -0.002, -lr * Pred Grad:  0.029, New P: 1.245
iter 28 loss: 0.016
Actual params: [0.5456, 1.2447]
-Original Grad: 0.123, -lr * Pred Grad:  0.011, New P: 0.556
-Original Grad: -0.029, -lr * Pred Grad:  -0.008, New P: 1.237
iter 29 loss: 0.016
Actual params: [0.5564, 1.237 ]
-Original Grad: 0.076, -lr * Pred Grad:  0.011, New P: 0.568
-Original Grad: -0.003, -lr * Pred Grad:  0.017, New P: 1.254
iter 30 loss: 0.015
Actual params: [0.5678, 1.2542]
-Original Grad: 0.090, -lr * Pred Grad:  0.011, New P: 0.579
-Original Grad: -0.014, -lr * Pred Grad:  0.007, New P: 1.261
