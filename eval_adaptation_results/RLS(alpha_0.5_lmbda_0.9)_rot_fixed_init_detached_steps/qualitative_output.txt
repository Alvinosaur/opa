Target params: [1.3344, 1.5708]
iter 0 loss: 0.077
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.046, -lr * Pred Grad:  -0.382, New P: -0.854
-Original Grad: -0.009, -lr * Pred Grad:  -0.061, New P: -0.058
iter 1 loss: 0.071
Actual params: [-0.8542, -0.0578]
-Original Grad: -0.007, -lr * Pred Grad:  -0.064, New P: -0.919
-Original Grad: 0.002, -lr * Pred Grad:  0.025, New P: -0.033
iter 2 loss: 0.071
Actual params: [-0.9187, -0.0329]
-Original Grad: -0.004, -lr * Pred Grad:  -0.038, New P: -0.957
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: -0.021
iter 3 loss: 0.071
Actual params: [-0.9565, -0.0208]
-Original Grad: -0.003, -lr * Pred Grad:  -0.030, New P: -0.987
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: -0.010
iter 4 loss: 0.071
Actual params: [-0.9869, -0.0101]
-Original Grad: -0.002, -lr * Pred Grad:  -0.029, New P: -1.016
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: -0.001
iter 5 loss: 0.071
Actual params: [-1.0161e+00, -6.9502e-04]
-Original Grad: -0.002, -lr * Pred Grad:  -0.028, New P: -1.044
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.008
iter 6 loss: 0.071
Actual params: [-1.0441,  0.0077]
-Original Grad: -0.002, -lr * Pred Grad:  -0.028, New P: -1.072
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.013
iter 7 loss: 0.071
Actual params: [-1.0723,  0.013 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -1.090
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.020
iter 8 loss: 0.071
Actual params: [-1.0899,  0.02  ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.024, New P: -1.114
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.025
iter 9 loss: 0.071
Actual params: [-1.1138,  0.0251]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -1.135
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.029
iter 10 loss: 0.071
Actual params: [-1.1349,  0.0289]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -1.155
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.035
iter 11 loss: 0.071
Actual params: [-1.1549,  0.0345]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -1.173
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.039
iter 12 loss: 0.071
Actual params: [-1.1726,  0.0386]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -1.193
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.042
iter 13 loss: 0.071
Actual params: [-1.1926,  0.0422]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.206
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.046
iter 14 loss: 0.071
Actual params: [-1.2056,  0.0461]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -1.224
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.051
iter 15 loss: 0.071
Actual params: [-1.2242,  0.051 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.240
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.055
iter 16 loss: 0.071
Actual params: [-1.2398,  0.0545]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.253
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.059
iter 17 loss: 0.071
Actual params: [-1.2529,  0.0586]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.270
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.063
iter 18 loss: 0.071
Actual params: [-1.2696,  0.0629]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.283
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.066
iter 19 loss: 0.071
Actual params: [-1.2827,  0.0661]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.295
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.069
iter 20 loss: 0.071
Actual params: [-1.2945,  0.0692]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.312
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.073
iter 21 loss: 0.071
Actual params: [-1.3115,  0.0732]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.326
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.078
iter 22 loss: 0.071
Actual params: [-1.326 ,  0.0776]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.339
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.082
iter 23 loss: 0.071
Actual params: [-1.3392,  0.0815]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.355
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.085
iter 24 loss: 0.071
Actual params: [-1.3555,  0.0846]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.365
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.087
iter 25 loss: 0.071
Actual params: [-1.3652,  0.0874]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.382
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.095
iter 26 loss: 0.071
Actual params: [-1.3821,  0.0952]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.393
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.099
iter 27 loss: 0.071
Actual params: [-1.393 ,  0.0988]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.405
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.104
iter 28 loss: 0.071
Actual params: [-1.4048,  0.1042]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.419
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.111
iter 29 loss: 0.071
Actual params: [-1.419 ,  0.1108]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.432
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.118
iter 30 loss: 0.071
Actual params: [-1.4319,  0.1177]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.445
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.125
Target params: [1.3344, 1.5708]
iter 0 loss: 0.443
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad:  0.017, New P: -0.455
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.016
iter 1 loss: 0.443
Actual params: [-0.4554,  0.0159]
-Original Grad: 0.002, -lr * Pred Grad:  0.026, New P: -0.430
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: 0.034
iter 2 loss: 0.443
Actual params: [-0.4297,  0.0337]
-Original Grad: 0.002, -lr * Pred Grad:  0.033, New P: -0.396
-Original Grad: 0.002, -lr * Pred Grad:  0.022, New P: 0.056
iter 3 loss: 0.443
Actual params: [-0.3963,  0.0557]
-Original Grad: 0.003, -lr * Pred Grad:  0.048, New P: -0.348
-Original Grad: 0.002, -lr * Pred Grad:  0.031, New P: 0.086
iter 4 loss: 0.443
Actual params: [-0.3483,  0.0864]
-Original Grad: 0.006, -lr * Pred Grad:  0.095, New P: -0.253
-Original Grad: 0.003, -lr * Pred Grad:  0.058, New P: 0.145
iter 5 loss: 0.442
Actual params: [-0.2531,  0.1446]
-Original Grad: 0.013, -lr * Pred Grad:  0.247, New P: -0.006
-Original Grad: 0.008, -lr * Pred Grad:  0.139, New P: 0.284
iter 6 loss: 0.426
Actual params: [-0.0063,  0.2838]
-Original Grad: 0.198, -lr * Pred Grad:  0.739, New P: 0.733
-Original Grad: 0.118, -lr * Pred Grad:  0.437, New P: 0.721
iter 7 loss: 0.151
Actual params: [0.7328, 0.7212]
-Original Grad: 0.192, -lr * Pred Grad:  0.032, New P: 0.765
-Original Grad: 0.326, -lr * Pred Grad:  0.397, New P: 1.118
iter 8 loss: 0.071
Actual params: [0.7652, 1.1182]
-Original Grad: 0.219, -lr * Pred Grad:  0.068, New P: 0.833
-Original Grad: 0.251, -lr * Pred Grad:  0.106, New P: 1.224
iter 9 loss: 0.039
Actual params: [0.833 , 1.2243]
-Original Grad: 0.113, -lr * Pred Grad:  0.017, New P: 0.850
-Original Grad: 0.161, -lr * Pred Grad:  0.065, New P: 1.289
iter 10 loss: 0.028
Actual params: [0.8495, 1.2888]
-Original Grad: 0.116, -lr * Pred Grad:  0.004, New P: 0.854
-Original Grad: 0.192, -lr * Pred Grad:  0.062, New P: 1.351
iter 11 loss: 0.020
Actual params: [0.8538, 1.3506]
-Original Grad: 0.191, -lr * Pred Grad:  0.018, New P: 0.872
-Original Grad: 0.135, -lr * Pred Grad:  0.025, New P: 1.376
iter 12 loss: 0.016
Actual params: [0.8721, 1.376 ]
-Original Grad: 0.050, -lr * Pred Grad:  -0.006, New P: 0.866
-Original Grad: 0.138, -lr * Pred Grad:  0.036, New P: 1.412
iter 13 loss: 0.014
Actual params: [0.8659, 1.4125]
-Original Grad: 0.110, -lr * Pred Grad:  0.009, New P: 0.875
-Original Grad: 0.121, -lr * Pred Grad:  0.021, New P: 1.434
iter 14 loss: 0.012
Actual params: [0.8746, 1.4339]
-Original Grad: 0.095, -lr * Pred Grad:  0.012, New P: 0.887
-Original Grad: 0.067, -lr * Pred Grad:  0.008, New P: 1.442
iter 15 loss: 0.011
Actual params: [0.8869, 1.4415]
-Original Grad: 0.027, -lr * Pred Grad:  -0.003, New P: 0.884
-Original Grad: 0.088, -lr * Pred Grad:  0.019, New P: 1.461
iter 16 loss: 0.010
Actual params: [0.8836, 1.4607]
-Original Grad: 0.041, -lr * Pred Grad:  0.003, New P: 0.886
-Original Grad: 0.068, -lr * Pred Grad:  0.012, New P: 1.473
iter 17 loss: 0.010
Actual params: [0.8862, 1.4726]
-Original Grad: 0.025, -lr * Pred Grad:  0.001, New P: 0.887
-Original Grad: 0.049, -lr * Pred Grad:  0.009, New P: 1.482
iter 18 loss: 0.010
Actual params: [0.8873, 1.4816]
-Original Grad: 0.097, -lr * Pred Grad:  0.020, New P: 0.907
-Original Grad: 0.042, -lr * Pred Grad:  -0.000, New P: 1.481
iter 19 loss: 0.009
Actual params: [0.9072, 1.4815]
-Original Grad: 0.042, -lr * Pred Grad:  0.006, New P: 0.913
-Original Grad: 0.047, -lr * Pred Grad:  0.007, New P: 1.489
iter 20 loss: 0.009
Actual params: [0.9132, 1.4887]
-Original Grad: 0.059, -lr * Pred Grad:  0.010, New P: 0.923
-Original Grad: 0.029, -lr * Pred Grad:  0.002, New P: 1.491
iter 21 loss: 0.009
Actual params: [0.9229, 1.4907]
-Original Grad: 0.071, -lr * Pred Grad:  0.009, New P: 0.932
-Original Grad: 0.032, -lr * Pred Grad:  0.003, New P: 1.494
iter 22 loss: 0.008
Actual params: [0.9322, 1.4937]
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: 0.931
-Original Grad: 0.017, -lr * Pred Grad:  0.004, New P: 1.498
iter 23 loss: 0.008
Actual params: [0.9311, 1.4978]
-Original Grad: 0.046, -lr * Pred Grad:  0.006, New P: 0.937
-Original Grad: 0.030, -lr * Pred Grad:  0.004, New P: 1.502
iter 24 loss: 0.008
Actual params: [0.9374, 1.5023]
-Original Grad: 0.041, -lr * Pred Grad:  0.007, New P: 0.944
-Original Grad: -0.014, -lr * Pred Grad:  -0.006, New P: 1.497
iter 25 loss: 0.008
Actual params: [0.9441, 1.4967]
-Original Grad: 0.076, -lr * Pred Grad:  0.008, New P: 0.952
-Original Grad: 0.003, -lr * Pred Grad:  -0.002, New P: 1.494
iter 26 loss: 0.008
Actual params: [0.9516, 1.4944]
-Original Grad: 0.092, -lr * Pred Grad:  0.007, New P: 0.959
-Original Grad: 0.019, -lr * Pred Grad:  0.002, New P: 1.496
iter 27 loss: 0.008
Actual params: [0.9587, 1.4965]
-Original Grad: 0.024, -lr * Pred Grad:  0.001, New P: 0.960
-Original Grad: 0.034, -lr * Pred Grad:  0.008, New P: 1.505
iter 28 loss: 0.008
Actual params: [0.9598, 1.5047]
-Original Grad: 0.003, -lr * Pred Grad:  0.001, New P: 0.961
-Original Grad: -0.017, -lr * Pred Grad:  -0.005, New P: 1.500
iter 29 loss: 0.008
Actual params: [0.9607, 1.5001]
-Original Grad: -0.016, -lr * Pred Grad:  -0.002, New P: 0.959
-Original Grad: -0.006, -lr * Pred Grad:  -0.001, New P: 1.499
iter 30 loss: 0.008
Actual params: [0.9592, 1.4991]
-Original Grad: -0.006, -lr * Pred Grad:  -0.001, New P: 0.958
-Original Grad: 0.015, -lr * Pred Grad:  0.004, New P: 1.503
Target params: [1.3344, 1.5708]
iter 0 loss: 0.725
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.016, -lr * Pred Grad:  0.176, New P: -0.296
-Original Grad: 0.019, -lr * Pred Grad:  0.203, New P: 0.206
iter 1 loss: 0.710
Actual params: [-0.2959,  0.2062]
-Original Grad: 0.046, -lr * Pred Grad:  0.400, New P: 0.104
-Original Grad: 0.061, -lr * Pred Grad:  0.532, New P: 0.739
iter 2 loss: 0.612
Actual params: [0.1042, 0.7386]
-Original Grad: 0.182, -lr * Pred Grad:  0.638, New P: 0.742
-Original Grad: 0.121, -lr * Pred Grad:  0.335, New P: 1.073
iter 3 loss: 0.281
Actual params: [0.7419, 1.0735]
-Original Grad: 0.713, -lr * Pred Grad:  0.260, New P: 1.002
-Original Grad: 0.148, -lr * Pred Grad:  -0.034, New P: 1.040
iter 4 loss: 0.216
Actual params: [1.0018, 1.0398]
-Original Grad: 0.177, -lr * Pred Grad:  -0.067, New P: 0.935
-Original Grad: 0.237, -lr * Pred Grad:  0.513, New P: 1.553
iter 5 loss: 0.148
Actual params: [0.935 , 1.5532]
-Original Grad: 0.987, -lr * Pred Grad:  0.089, New P: 1.024
-Original Grad: 0.134, -lr * Pred Grad:  -0.065, New P: 1.489
iter 6 loss: 0.128
Actual params: [1.024 , 1.4885]
-Original Grad: 0.892, -lr * Pred Grad:  0.043, New P: 1.068
-Original Grad: 0.146, -lr * Pred Grad:  0.026, New P: 1.515
iter 7 loss: 0.116
Actual params: [1.0675, 1.5147]
-Original Grad: 0.369, -lr * Pred Grad:  -0.011, New P: 1.057
-Original Grad: 0.149, -lr * Pred Grad:  0.187, New P: 1.702
iter 8 loss: 0.103
Actual params: [1.057 , 1.7016]
-Original Grad: 0.502, -lr * Pred Grad:  0.010, New P: 1.067
-Original Grad: 0.122, -lr * Pred Grad:  0.074, New P: 1.775
iter 9 loss: 0.100
Actual params: [1.0673, 1.7753]
-Original Grad: 0.194, -lr * Pred Grad:  0.022, New P: 1.089
-Original Grad: -0.020, -lr * Pred Grad:  -0.071, New P: 1.704
iter 10 loss: 0.095
Actual params: [1.089 , 1.7044]
-Original Grad: 0.280, -lr * Pred Grad:  -0.001, New P: 1.088
-Original Grad: 0.128, -lr * Pred Grad:  0.088, New P: 1.793
iter 11 loss: 0.094
Actual params: [1.088 , 1.7927]
-Original Grad: 0.579, -lr * Pred Grad:  0.033, New P: 1.121
-Original Grad: -0.045, -lr * Pred Grad:  -0.062, New P: 1.731
iter 12 loss: 0.086
Actual params: [1.1212, 1.7307]
-Original Grad: 0.363, -lr * Pred Grad:  0.011, New P: 1.132
-Original Grad: 0.118, -lr * Pred Grad:  0.032, New P: 1.763
iter 13 loss: 0.083
Actual params: [1.132, 1.763]
-Original Grad: 0.156, -lr * Pred Grad:  0.007, New P: 1.139
-Original Grad: 0.032, -lr * Pred Grad:  0.005, New P: 1.768
iter 14 loss: 0.082
Actual params: [1.1386, 1.7676]
-Original Grad: 0.403, -lr * Pred Grad:  0.021, New P: 1.159
-Original Grad: -0.029, -lr * Pred Grad:  -0.024, New P: 1.744
iter 15 loss: 0.079
Actual params: [1.1591, 1.7437]
-Original Grad: 0.309, -lr * Pred Grad:  0.010, New P: 1.169
-Original Grad: 0.122, -lr * Pred Grad:  0.029, New P: 1.772
iter 16 loss: 0.076
Actual params: [1.1691, 1.7724]
-Original Grad: 0.098, -lr * Pred Grad:  0.001, New P: 1.170
-Original Grad: 0.084, -lr * Pred Grad:  0.027, New P: 1.799
iter 17 loss: 0.075
Actual params: [1.1704, 1.7991]
-Original Grad: 0.154, -lr * Pred Grad:  0.004, New P: 1.175
-Original Grad: 0.091, -lr * Pred Grad:  0.029, New P: 1.828
iter 18 loss: 0.074
Actual params: [1.1747, 1.8277]
-Original Grad: 0.464, -lr * Pred Grad:  0.023, New P: 1.198
-Original Grad: 0.008, -lr * Pred Grad:  -0.014, New P: 1.814
iter 19 loss: 0.070
Actual params: [1.1981, 1.8137]
-Original Grad: 0.253, -lr * Pred Grad:  0.013, New P: 1.211
-Original Grad: -0.078, -lr * Pred Grad:  -0.019, New P: 1.795
iter 20 loss: 0.069
Actual params: [1.211, 1.795]
-Original Grad: 0.070, -lr * Pred Grad:  0.002, New P: 1.213
-Original Grad: 0.155, -lr * Pred Grad:  0.030, New P: 1.825
iter 21 loss: 0.068
Actual params: [1.2127, 1.8254]
-Original Grad: 0.231, -lr * Pred Grad:  0.010, New P: 1.223
-Original Grad: 0.130, -lr * Pred Grad:  0.024, New P: 1.849
iter 22 loss: 0.066
Actual params: [1.2228, 1.8491]
-Original Grad: 0.329, -lr * Pred Grad:  0.017, New P: 1.240
-Original Grad: -0.117, -lr * Pred Grad:  -0.021, New P: 1.828
iter 23 loss: 0.064
Actual params: [1.2401, 1.8284]
-Original Grad: 0.125, -lr * Pred Grad:  0.006, New P: 1.246
-Original Grad: 0.147, -lr * Pred Grad:  0.024, New P: 1.853
iter 24 loss: 0.063
Actual params: [1.2459, 1.8527]
-Original Grad: 0.103, -lr * Pred Grad:  0.006, New P: 1.252
-Original Grad: 0.064, -lr * Pred Grad:  0.010, New P: 1.863
iter 25 loss: 0.062
Actual params: [1.2516, 1.8629]
-Original Grad: 0.177, -lr * Pred Grad:  0.010, New P: 1.262
-Original Grad: 0.044, -lr * Pred Grad:  0.007, New P: 1.870
iter 26 loss: 0.061
Actual params: [1.2621, 1.8703]
-Original Grad: 0.135, -lr * Pred Grad:  0.008, New P: 1.270
-Original Grad: 0.134, -lr * Pred Grad:  0.022, New P: 1.892
iter 27 loss: 0.059
Actual params: [1.2702, 1.8924]
-Original Grad: 0.180, -lr * Pred Grad:  0.012, New P: 1.282
-Original Grad: 0.042, -lr * Pred Grad:  0.007, New P: 1.900
iter 28 loss: 0.058
Actual params: [1.2819, 1.8997]
-Original Grad: 0.149, -lr * Pred Grad:  0.009, New P: 1.291
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: 1.902
iter 29 loss: 0.057
Actual params: [1.291 , 1.9018]
-Original Grad: 0.210, -lr * Pred Grad:  0.013, New P: 1.304
-Original Grad: -0.026, -lr * Pred Grad:  0.001, New P: 1.903
iter 30 loss: 0.056
Actual params: [1.3039, 1.9025]
-Original Grad: 0.255, -lr * Pred Grad:  0.014, New P: 1.318
-Original Grad: -0.088, -lr * Pred Grad:  -0.003, New P: 1.899
Target params: [1.3344, 1.5708]
iter 0 loss: 0.228
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.003, -lr * Pred Grad:  0.030, New P: -0.442
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: 0.011
iter 1 loss: 0.228
Actual params: [-0.442 ,  0.0112]
-Original Grad: 0.004, -lr * Pred Grad:  0.044, New P: -0.398
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.023
iter 2 loss: 0.228
Actual params: [-0.398 ,  0.0228]
-Original Grad: 0.005, -lr * Pred Grad:  0.075, New P: -0.323
-Original Grad: 0.001, -lr * Pred Grad:  0.020, New P: 0.043
iter 3 loss: 0.228
Actual params: [-0.3232,  0.0428]
-Original Grad: 0.009, -lr * Pred Grad:  0.131, New P: -0.192
-Original Grad: 0.002, -lr * Pred Grad:  0.037, New P: 0.080
iter 4 loss: 0.226
Actual params: [-0.1921,  0.0802]
-Original Grad: 0.026, -lr * Pred Grad:  0.384, New P: 0.192
-Original Grad: 0.008, -lr * Pred Grad:  0.118, New P: 0.199
iter 5 loss: 0.151
Actual params: [0.1923, 0.1985]
-Original Grad: 0.208, -lr * Pred Grad:  0.367, New P: 0.559
-Original Grad: 0.066, -lr * Pred Grad:  0.078, New P: 0.276
iter 6 loss: 0.151
Actual params: [0.5588, 0.2761]
-Original Grad: -1.245, -lr * Pred Grad:  -0.024, New P: 0.535
-Original Grad: 0.426, -lr * Pred Grad:  0.298, New P: 0.575
iter 7 loss: 0.090
Actual params: [0.5348, 0.5745]
-Original Grad: -0.491, -lr * Pred Grad:  0.010, New P: 0.545
-Original Grad: 0.272, -lr * Pred Grad:  0.180, New P: 0.755
iter 8 loss: 0.072
Actual params: [0.5452, 0.7548]
-Original Grad: -0.395, -lr * Pred Grad:  0.016, New P: 0.561
-Original Grad: 0.264, -lr * Pred Grad:  0.150, New P: 0.905
iter 9 loss: 0.056
Actual params: [0.5611, 0.905 ]
-Original Grad: -0.033, -lr * Pred Grad:  0.028, New P: 0.589
-Original Grad: 0.138, -lr * Pred Grad:  0.101, New P: 1.006
iter 10 loss: 0.049
Actual params: [0.589 , 1.0062]
-Original Grad: 0.083, -lr * Pred Grad:  0.022, New P: 0.611
-Original Grad: 0.046, -lr * Pred Grad:  0.052, New P: 1.058
iter 11 loss: 0.045
Actual params: [0.6115, 1.0584]
-Original Grad: 0.039, -lr * Pred Grad:  0.022, New P: 0.634
-Original Grad: 0.074, -lr * Pred Grad:  0.058, New P: 1.117
iter 12 loss: 0.041
Actual params: [0.6336, 1.1168]
-Original Grad: 0.079, -lr * Pred Grad:  0.020, New P: 0.653
-Original Grad: 0.035, -lr * Pred Grad:  0.039, New P: 1.156
iter 13 loss: 0.038
Actual params: [0.6533, 1.156 ]
-Original Grad: 0.150, -lr * Pred Grad:  0.019, New P: 0.672
-Original Grad: -0.026, -lr * Pred Grad:  0.016, New P: 1.172
iter 14 loss: 0.037
Actual params: [0.6723, 1.1719]
-Original Grad: 0.091, -lr * Pred Grad:  0.021, New P: 0.693
-Original Grad: 0.025, -lr * Pred Grad:  0.037, New P: 1.208
iter 15 loss: 0.034
Actual params: [0.6931, 1.2085]
-Original Grad: 0.124, -lr * Pred Grad:  0.021, New P: 0.714
-Original Grad: -0.002, -lr * Pred Grad:  0.026, New P: 1.234
iter 16 loss: 0.031
Actual params: [0.7143, 1.2345]
-Original Grad: 0.109, -lr * Pred Grad:  0.018, New P: 0.732
-Original Grad: -0.008, -lr * Pred Grad:  0.019, New P: 1.254
iter 17 loss: 0.029
Actual params: [0.7324, 1.2538]
-Original Grad: 0.100, -lr * Pred Grad:  0.016, New P: 0.748
-Original Grad: -0.015, -lr * Pred Grad:  0.012, New P: 1.266
iter 18 loss: 0.028
Actual params: [0.7482, 1.2663]
-Original Grad: 0.137, -lr * Pred Grad:  0.022, New P: 0.770
-Original Grad: -0.024, -lr * Pred Grad:  0.015, New P: 1.282
iter 19 loss: 0.027
Actual params: [0.7701, 1.2816]
-Original Grad: 0.164, -lr * Pred Grad:  0.027, New P: 0.798
-Original Grad: -0.013, -lr * Pred Grad:  0.029, New P: 1.311
iter 20 loss: 0.025
Actual params: [0.7975, 1.3111]
-Original Grad: 0.092, -lr * Pred Grad:  0.014, New P: 0.812
-Original Grad: -0.012, -lr * Pred Grad:  0.012, New P: 1.323
iter 21 loss: 0.024
Actual params: [0.8119, 1.3229]
-Original Grad: 0.203, -lr * Pred Grad:  0.028, New P: 0.840
-Original Grad: -0.003, -lr * Pred Grad:  0.037, New P: 1.360
iter 22 loss: 0.022
Actual params: [0.8403, 1.3602]
-Original Grad: 0.097, -lr * Pred Grad:  0.014, New P: 0.854
-Original Grad: 0.007, -lr * Pred Grad:  0.028, New P: 1.388
iter 23 loss: 0.021
Actual params: [0.8543, 1.3881]
-Original Grad: 0.104, -lr * Pred Grad:  0.013, New P: 0.867
-Original Grad: -0.002, -lr * Pred Grad:  0.020, New P: 1.408
iter 24 loss: 0.020
Actual params: [0.8675, 1.4081]
-Original Grad: 0.059, -lr * Pred Grad:  0.014, New P: 0.882
-Original Grad: 0.032, -lr * Pred Grad:  0.050, New P: 1.458
iter 25 loss: 0.018
Actual params: [0.8818, 1.4576]
-Original Grad: 0.169, -lr * Pred Grad:  0.020, New P: 0.902
-Original Grad: 0.039, -lr * Pred Grad:  0.054, New P: 1.511
iter 26 loss: 0.017
Actual params: [0.9018, 1.5115]
-Original Grad: 0.075, -lr * Pred Grad:  0.009, New P: 0.911
-Original Grad: 0.014, -lr * Pred Grad:  0.019, New P: 1.531
iter 27 loss: 0.017
Actual params: [0.9107, 1.5309]
-Original Grad: 0.048, -lr * Pred Grad:  -0.001, New P: 0.909
-Original Grad: -0.065, -lr * Pred Grad:  -0.046, New P: 1.485
iter 28 loss: 0.016
Actual params: [0.9092, 1.4853]
-Original Grad: 0.030, -lr * Pred Grad:  0.005, New P: 0.915
-Original Grad: 0.022, -lr * Pred Grad:  0.018, New P: 1.504
iter 29 loss: 0.016
Actual params: [0.9147, 1.5036]
-Original Grad: 0.049, -lr * Pred Grad:  0.003, New P: 0.918
-Original Grad: -0.029, -lr * Pred Grad:  -0.015, New P: 1.489
iter 30 loss: 0.016
Actual params: [0.918 , 1.4887]
-Original Grad: 0.151, -lr * Pred Grad:  0.016, New P: 0.934
-Original Grad: -0.010, -lr * Pred Grad:  0.005, New P: 1.494
Target params: [1.3344, 1.5708]
iter 0 loss: 0.466
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.002, -lr * Pred Grad:  -0.019, New P: -0.492
-Original Grad: 0.001, -lr * Pred Grad:  0.006, New P: 0.010
iter 1 loss: 0.466
Actual params: [-0.4917,  0.0099]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.504
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.014
iter 2 loss: 0.466
Actual params: [-0.504 ,  0.0143]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.520
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.020
iter 3 loss: 0.466
Actual params: [-0.5201,  0.0198]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.536
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.026
iter 4 loss: 0.466
Actual params: [-0.5361,  0.0256]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.551
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.031
iter 5 loss: 0.466
Actual params: [-0.5514,  0.0312]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.563
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.036
iter 6 loss: 0.466
Actual params: [-0.5633,  0.0357]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.579
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.042
iter 7 loss: 0.466
Actual params: [-0.5786,  0.0417]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.593
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.047
iter 8 loss: 0.466
Actual params: [-0.5926,  0.0473]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: -0.606
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.052
iter 9 loss: 0.466
Actual params: [-0.6056,  0.0525]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.618
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.058
iter 10 loss: 0.466
Actual params: [-0.6182,  0.0576]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.629
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.062
iter 11 loss: 0.466
Actual params: [-0.6286,  0.062 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.641
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.067
iter 12 loss: 0.466
Actual params: [-0.6415,  0.0675]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.652
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.072
iter 13 loss: 0.466
Actual params: [-0.6522,  0.0722]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.663
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.077
iter 14 loss: 0.466
Actual params: [-0.663 ,  0.0771]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.675
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.083
iter 15 loss: 0.466
Actual params: [-0.6748,  0.0825]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.687
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.088
iter 16 loss: 0.466
Actual params: [-0.6873,  0.0884]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.699
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.094
iter 17 loss: 0.466
Actual params: [-0.6992,  0.0943]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.708
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.099
iter 18 loss: 0.466
Actual params: [-0.7081,  0.0987]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.722
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.106
iter 19 loss: 0.466
Actual params: [-0.722 ,  0.1059]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.733
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.112
iter 20 loss: 0.466
Actual params: [-0.7327,  0.1116]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.744
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.118
iter 21 loss: 0.466
Actual params: [-0.7438,  0.1177]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.754
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.123
iter 22 loss: 0.466
Actual params: [-0.7537,  0.1232]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.763
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.129
iter 23 loss: 0.466
Actual params: [-0.7631,  0.1286]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.771
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.133
iter 24 loss: 0.466
Actual params: [-0.7712,  0.1335]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.783
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.140
iter 25 loss: 0.466
Actual params: [-0.7825,  0.1405]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.792
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.146
iter 26 loss: 0.466
Actual params: [-0.7918,  0.1464]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.801
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.153
iter 27 loss: 0.466
Actual params: [-0.8013,  0.1528]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.812
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.160
iter 28 loss: 0.466
Actual params: [-0.8121,  0.1604]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.821
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.167
iter 29 loss: 0.466
Actual params: [-0.8211,  0.1669]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.829
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.173
iter 30 loss: 0.466
Actual params: [-0.8292,  0.1732]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.839
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.181
Target params: [1.3344, 1.5708]
iter 0 loss: 0.228
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.004, -lr * Pred Grad:  0.040, New P: -0.433
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.013
iter 1 loss: 0.228
Actual params: [-0.4326,  0.0134]
-Original Grad: 0.003, -lr * Pred Grad:  0.035, New P: -0.397
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.023
iter 2 loss: 0.228
Actual params: [-0.3972,  0.0231]
-Original Grad: 0.006, -lr * Pred Grad:  0.082, New P: -0.316
-Original Grad: 0.002, -lr * Pred Grad:  0.022, New P: 0.045
iter 3 loss: 0.228
Actual params: [-0.3156,  0.0452]
-Original Grad: 0.010, -lr * Pred Grad:  0.155, New P: -0.160
-Original Grad: 0.003, -lr * Pred Grad:  0.045, New P: 0.090
iter 4 loss: 0.225
Actual params: [-0.1601,  0.0902]
-Original Grad: 0.038, -lr * Pred Grad:  0.514, New P: 0.354
-Original Grad: 0.011, -lr * Pred Grad:  0.158, New P: 0.248
iter 5 loss: 0.088
Actual params: [0.3541, 0.2478]
-Original Grad: 0.258, -lr * Pred Grad:  0.223, New P: 0.577
-Original Grad: 0.087, -lr * Pred Grad:  0.134, New P: 0.382
iter 6 loss: 0.131
Actual params: [0.577 , 0.3815]
-Original Grad: -0.560, -lr * Pred Grad:  -0.082, New P: 0.495
-Original Grad: 0.516, -lr * Pred Grad:  0.263, New P: 0.644
iter 7 loss: 0.075
Actual params: [0.4949, 0.6442]
-Original Grad: -0.374, -lr * Pred Grad:  -0.056, New P: 0.439
-Original Grad: 0.204, -lr * Pred Grad:  0.041, New P: 0.685
iter 8 loss: 0.066
Actual params: [0.4389, 0.6848]
-Original Grad: -0.114, -lr * Pred Grad:  -0.003, New P: 0.436
-Original Grad: 0.081, -lr * Pred Grad:  0.039, New P: 0.724
iter 9 loss: 0.064
Actual params: [0.436 , 0.7236]
-Original Grad: -0.001, -lr * Pred Grad:  0.022, New P: 0.458
-Original Grad: 0.045, -lr * Pred Grad:  0.055, New P: 0.779
iter 10 loss: 0.062
Actual params: [0.4578, 0.7791]
-Original Grad: -0.040, -lr * Pred Grad:  0.037, New P: 0.495
-Original Grad: 0.101, -lr * Pred Grad:  0.100, New P: 0.879
iter 11 loss: 0.055
Actual params: [0.4946, 0.8788]
-Original Grad: -0.032, -lr * Pred Grad:  0.037, New P: 0.532
-Original Grad: 0.096, -lr * Pred Grad:  0.094, New P: 0.972
iter 12 loss: 0.050
Actual params: [0.5318, 0.9723]
-Original Grad: 0.090, -lr * Pred Grad:  0.047, New P: 0.579
-Original Grad: 0.034, -lr * Pred Grad:  0.080, New P: 1.053
iter 13 loss: 0.046
Actual params: [0.5789, 1.0527]
-Original Grad: 0.035, -lr * Pred Grad:  0.041, New P: 0.620
-Original Grad: 0.066, -lr * Pred Grad:  0.081, New P: 1.134
iter 14 loss: 0.040
Actual params: [0.6204, 1.134 ]
-Original Grad: 0.139, -lr * Pred Grad:  0.036, New P: 0.656
-Original Grad: -0.028, -lr * Pred Grad:  0.036, New P: 1.170
iter 15 loss: 0.038
Actual params: [0.6559, 1.1695]
-Original Grad: 0.184, -lr * Pred Grad:  0.046, New P: 0.702
-Original Grad: -0.036, -lr * Pred Grad:  0.046, New P: 1.215
iter 16 loss: 0.033
Actual params: [0.7019, 1.2155]
-Original Grad: 0.104, -lr * Pred Grad:  0.035, New P: 0.737
-Original Grad: 0.001, -lr * Pred Grad:  0.047, New P: 1.262
iter 17 loss: 0.029
Actual params: [0.7367, 1.262 ]
-Original Grad: 0.098, -lr * Pred Grad:  0.029, New P: 0.766
-Original Grad: -0.009, -lr * Pred Grad:  0.034, New P: 1.296
iter 18 loss: 0.026
Actual params: [0.7657, 1.296 ]
-Original Grad: 0.161, -lr * Pred Grad:  0.041, New P: 0.807
-Original Grad: -0.017, -lr * Pred Grad:  0.046, New P: 1.342
iter 19 loss: 0.023
Actual params: [0.807 , 1.3422]
-Original Grad: 0.071, -lr * Pred Grad:  0.012, New P: 0.819
-Original Grad: -0.001, -lr * Pred Grad:  0.016, New P: 1.359
iter 20 loss: 0.023
Actual params: [0.8187, 1.3586]
-Original Grad: -0.040, -lr * Pred Grad:  -0.005, New P: 0.814
-Original Grad: -0.003, -lr * Pred Grad:  -0.008, New P: 1.350
iter 21 loss: 0.023
Actual params: [0.8142, 1.3501]
-Original Grad: 0.098, -lr * Pred Grad:  0.011, New P: 0.825
-Original Grad: 0.007, -lr * Pred Grad:  0.020, New P: 1.371
iter 22 loss: 0.022
Actual params: [0.8255, 1.3706]
-Original Grad: -0.077, -lr * Pred Grad:  -0.006, New P: 0.820
-Original Grad: 0.008, -lr * Pred Grad:  -0.001, New P: 1.370
iter 23 loss: 0.022
Actual params: [0.82  , 1.3697]
-Original Grad: 0.247, -lr * Pred Grad:  0.017, New P: 0.837
-Original Grad: -0.018, -lr * Pred Grad:  0.009, New P: 1.379
iter 24 loss: 0.021
Actual params: [0.8374, 1.3788]
-Original Grad: 0.060, -lr * Pred Grad:  0.005, New P: 0.842
-Original Grad: -0.002, -lr * Pred Grad:  0.005, New P: 1.384
iter 25 loss: 0.021
Actual params: [0.8422, 1.3841]
-Original Grad: 0.031, -lr * Pred Grad:  0.006, New P: 0.848
-Original Grad: 0.020, -lr * Pred Grad:  0.029, New P: 1.413
iter 26 loss: 0.020
Actual params: [0.8478, 1.4131]
-Original Grad: -0.006, -lr * Pred Grad:  -0.000, New P: 0.848
-Original Grad: 0.002, -lr * Pred Grad:  0.002, New P: 1.415
iter 27 loss: 0.020
Actual params: [0.8475, 1.4153]
-Original Grad: -0.009, -lr * Pred Grad:  0.005, New P: 0.853
-Original Grad: 0.035, -lr * Pred Grad:  0.047, New P: 1.463
iter 28 loss: 0.019
Actual params: [0.853 , 1.4625]
-Original Grad: 0.093, -lr * Pred Grad:  0.003, New P: 0.856
-Original Grad: -0.032, -lr * Pred Grad:  -0.028, New P: 1.434
iter 29 loss: 0.019
Actual params: [0.8562, 1.4344]
-Original Grad: 0.085, -lr * Pred Grad:  0.012, New P: 0.868
-Original Grad: 0.018, -lr * Pred Grad:  0.042, New P: 1.476
iter 30 loss: 0.018
Actual params: [0.8683, 1.4761]
-Original Grad: 0.101, -lr * Pred Grad:  0.012, New P: 0.880
-Original Grad: 0.017, -lr * Pred Grad:  0.037, New P: 1.513
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.136, -lr * Pred Grad:  0.585, New P: 0.112
-Original Grad: 0.019, -lr * Pred Grad:  0.077, New P: 0.080
iter 1 loss: 0.045
Actual params: [0.1123, 0.0801]
-Original Grad: 0.126, -lr * Pred Grad:  0.166, New P: 0.279
-Original Grad: 0.028, -lr * Pred Grad:  0.134, New P: 0.214
iter 2 loss: 0.033
Actual params: [0.2786, 0.214 ]
-Original Grad: -0.114, -lr * Pred Grad:  0.015, New P: 0.294
-Original Grad: -0.060, -lr * Pred Grad:  -0.132, New P: 0.082
iter 3 loss: 0.034
Actual params: [0.294, 0.082]
-Original Grad: 0.061, -lr * Pred Grad:  -0.068, New P: 0.227
-Original Grad: 0.079, -lr * Pred Grad:  0.259, New P: 0.341
iter 4 loss: 0.035
Actual params: [0.2265, 0.3409]
-Original Grad: -0.031, -lr * Pred Grad:  0.044, New P: 0.271
-Original Grad: -0.067, -lr * Pred Grad:  -0.169, New P: 0.172
iter 5 loss: 0.032
Actual params: [0.2708, 0.1718]
-Original Grad: 0.091, -lr * Pred Grad:  0.012, New P: 0.283
-Original Grad: 0.024, -lr * Pred Grad:  -0.005, New P: 0.167
iter 6 loss: 0.032
Actual params: [0.2828, 0.167 ]
-Original Grad: 0.076, -lr * Pred Grad:  -0.002, New P: 0.281
-Original Grad: 0.035, -lr * Pred Grad:  0.039, New P: 0.206
iter 7 loss: 0.033
Actual params: [0.2812, 0.2063]
-Original Grad: 0.037, -lr * Pred Grad:  0.010, New P: 0.291
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: 0.183
iter 8 loss: 0.033
Actual params: [0.2912, 0.183 ]
-Original Grad: -0.171, -lr * Pred Grad:  -0.015, New P: 0.276
-Original Grad: -0.045, -lr * Pred Grad:  0.006, New P: 0.189
iter 9 loss: 0.033
Actual params: [0.2764, 0.1894]
-Original Grad: 0.048, -lr * Pred Grad:  0.005, New P: 0.282
-Original Grad: 0.011, -lr * Pred Grad:  -0.006, New P: 0.184
iter 10 loss: 0.033
Actual params: [0.2816, 0.1839]
-Original Grad: 0.114, -lr * Pred Grad:  0.007, New P: 0.288
-Original Grad: 0.036, -lr * Pred Grad:  0.009, New P: 0.193
iter 11 loss: 0.033
Actual params: [0.2884, 0.1928]
-Original Grad: -0.304, -lr * Pred Grad:  -0.009, New P: 0.279
-Original Grad: -0.108, -lr * Pred Grad:  -0.027, New P: 0.166
iter 12 loss: 0.032
Actual params: [0.279, 0.166]
-Original Grad: -0.038, -lr * Pred Grad:  -0.028, New P: 0.251
-Original Grad: 0.036, -lr * Pred Grad:  0.091, New P: 0.257
iter 13 loss: 0.033
Actual params: [0.2505, 0.2575]
-Original Grad: 0.048, -lr * Pred Grad:  0.012, New P: 0.262
-Original Grad: -0.002, -lr * Pred Grad:  -0.032, New P: 0.226
iter 14 loss: 0.033
Actual params: [0.2624, 0.2257]
-Original Grad: 0.066, -lr * Pred Grad:  0.001, New P: 0.263
-Original Grad: 0.024, -lr * Pred Grad:  0.010, New P: 0.235
iter 15 loss: 0.033
Actual params: [0.2633, 0.2354]
-Original Grad: -0.057, -lr * Pred Grad:  0.009, New P: 0.272
-Original Grad: -0.036, -lr * Pred Grad:  -0.040, New P: 0.195
iter 16 loss: 0.033
Actual params: [0.2721, 0.1954]
-Original Grad: -0.038, -lr * Pred Grad:  0.002, New P: 0.274
-Original Grad: -0.018, -lr * Pred Grad:  -0.015, New P: 0.180
iter 17 loss: 0.032
Actual params: [0.2745, 0.1805]
-Original Grad: -0.064, -lr * Pred Grad:  -0.003, New P: 0.271
-Original Grad: -0.018, -lr * Pred Grad:  0.001, New P: 0.181
iter 18 loss: 0.032
Actual params: [0.2713, 0.1815]
-Original Grad: 0.035, -lr * Pred Grad:  0.011, New P: 0.283
-Original Grad: -0.006, -lr * Pred Grad:  -0.034, New P: 0.148
iter 19 loss: 0.033
Actual params: [0.2826, 0.148 ]
-Original Grad: 0.154, -lr * Pred Grad:  -0.004, New P: 0.278
-Original Grad: 0.064, -lr * Pred Grad:  0.043, New P: 0.191
iter 20 loss: 0.033
Actual params: [0.2781, 0.1909]
-Original Grad: -0.033, -lr * Pred Grad:  -0.014, New P: 0.264
-Original Grad: 0.010, -lr * Pred Grad:  0.044, New P: 0.234
iter 21 loss: 0.033
Actual params: [0.2643, 0.2344]
-Original Grad: 0.049, -lr * Pred Grad:  0.023, New P: 0.288
-Original Grad: -0.021, -lr * Pred Grad:  -0.074, New P: 0.161
iter 22 loss: 0.032
Actual params: [0.2876, 0.1608]
-Original Grad: -0.041, -lr * Pred Grad:  -0.017, New P: 0.270
-Original Grad: 0.022, -lr * Pred Grad:  0.057, New P: 0.218
iter 23 loss: 0.033
Actual params: [0.2703, 0.2179]
-Original Grad: 0.067, -lr * Pred Grad:  0.006, New P: 0.277
-Original Grad: 0.012, -lr * Pred Grad:  -0.011, New P: 0.207
iter 24 loss: 0.033
Actual params: [0.2767, 0.2068]
-Original Grad: 0.048, -lr * Pred Grad:  0.008, New P: 0.284
-Original Grad: 0.003, -lr * Pred Grad:  -0.019, New P: 0.188
iter 25 loss: 0.033
Actual params: [0.2845, 0.1878]
-Original Grad: 0.068, -lr * Pred Grad:  0.008, New P: 0.293
-Original Grad: 0.011, -lr * Pred Grad:  -0.016, New P: 0.172
iter 26 loss: 0.033
Actual params: [0.2927, 0.1717]
-Original Grad: -0.022, -lr * Pred Grad:  -0.001, New P: 0.292
-Original Grad: -0.007, -lr * Pred Grad:  -0.002, New P: 0.170
iter 27 loss: 0.033
Actual params: [0.2921, 0.1702]
-Original Grad: -0.283, -lr * Pred Grad:  -0.003, New P: 0.289
-Original Grad: -0.099, -lr * Pred Grad:  -0.030, New P: 0.141
iter 28 loss: 0.033
Actual params: [0.2893, 0.1406]
-Original Grad: 0.169, -lr * Pred Grad:  -0.008, New P: 0.281
-Original Grad: 0.079, -lr * Pred Grad:  0.054, New P: 0.194
iter 29 loss: 0.033
Actual params: [0.2812, 0.1942]
-Original Grad: 0.023, -lr * Pred Grad:  -0.002, New P: 0.279
-Original Grad: 0.013, -lr * Pred Grad:  0.012, New P: 0.206
iter 30 loss: 0.033
Actual params: [0.2788, 0.2058]
-Original Grad: -0.068, -lr * Pred Grad:  0.003, New P: 0.281
-Original Grad: -0.030, -lr * Pred Grad:  -0.018, New P: 0.188
Target params: [1.3344, 1.5708]
iter 0 loss: 0.008
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.032, -lr * Pred Grad:  0.211, New P: -0.261
-Original Grad: -0.012, -lr * Pred Grad:  -0.077, New P: -0.074
iter 1 loss: 0.003
Actual params: [-0.2611, -0.0738]
-Original Grad: 0.034, -lr * Pred Grad:  0.058, New P: -0.203
-Original Grad: -0.012, -lr * Pred Grad:  -0.033, New P: -0.106
iter 2 loss: 0.002
Actual params: [-0.2034, -0.1063]
-Original Grad: 0.007, -lr * Pred Grad:  0.010, New P: -0.194
-Original Grad: -0.003, -lr * Pred Grad:  -0.013, New P: -0.119
iter 3 loss: 0.002
Actual params: [-0.1935, -0.1194]
-Original Grad: 0.001, -lr * Pred Grad:  -0.003, New P: -0.196
-Original Grad: -0.002, -lr * Pred Grad:  -0.017, New P: -0.137
iter 4 loss: 0.002
Actual params: [-0.1961, -0.1366]
-Original Grad: 0.011, -lr * Pred Grad:  0.024, New P: -0.172
-Original Grad: -0.001, -lr * Pred Grad:  0.023, New P: -0.114
iter 5 loss: 0.002
Actual params: [-0.1721, -0.1138]
-Original Grad: 0.004, -lr * Pred Grad:  0.005, New P: -0.167
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -0.122
iter 6 loss: 0.002
Actual params: [-0.1668, -0.1222]
-Original Grad: 0.002, -lr * Pred Grad:  0.001, New P: -0.166
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.134
iter 7 loss: 0.002
Actual params: [-0.1657, -0.1335]
-Original Grad: 0.003, -lr * Pred Grad:  0.005, New P: -0.161
-Original Grad: -0.001, -lr * Pred Grad:  -0.003, New P: -0.136
iter 8 loss: 0.002
Actual params: [-0.1607, -0.1361]
-Original Grad: 0.002, -lr * Pred Grad:  0.004, New P: -0.157
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.145
iter 9 loss: 0.002
Actual params: [-0.1567, -0.145 ]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: -0.156
-Original Grad: -0.002, -lr * Pred Grad:  -0.020, New P: -0.165
iter 10 loss: 0.002
Actual params: [-0.1564, -0.1646]
-Original Grad: 0.001, -lr * Pred Grad:  -0.002, New P: -0.159
-Original Grad: -0.002, -lr * Pred Grad:  -0.033, New P: -0.197
iter 11 loss: 0.002
Actual params: [-0.1587, -0.1972]
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: -0.156
-Original Grad: -0.002, -lr * Pred Grad:  -0.025, New P: -0.222
iter 12 loss: 0.002
Actual params: [-0.1559, -0.2223]
-Original Grad: 0.005, -lr * Pred Grad:  0.015, New P: -0.141
-Original Grad: 0.001, -lr * Pred Grad:  0.022, New P: -0.201
iter 13 loss: 0.002
Actual params: [-0.1408, -0.2005]
-Original Grad: 0.002, -lr * Pred Grad:  0.005, New P: -0.136
-Original Grad: -0.002, -lr * Pred Grad:  -0.025, New P: -0.225
iter 14 loss: 0.002
Actual params: [-0.1359, -0.2251]
-Original Grad: 0.003, -lr * Pred Grad:  0.011, New P: -0.125
-Original Grad: 0.001, -lr * Pred Grad:  0.019, New P: -0.206
iter 15 loss: 0.002
Actual params: [-0.1248, -0.2057]
-Original Grad: 0.003, -lr * Pred Grad:  0.012, New P: -0.113
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.205
iter 16 loss: 0.002
Actual params: [-0.113 , -0.2046]
-Original Grad: -0.004, -lr * Pred Grad:  -0.011, New P: -0.124
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.199
iter 17 loss: 0.002
Actual params: [-0.1243, -0.1985]
-Original Grad: 0.003, -lr * Pred Grad:  0.008, New P: -0.116
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.197
iter 18 loss: 0.002
Actual params: [-0.1158, -0.1966]
-Original Grad: -0.000, -lr * Pred Grad:  0.002, New P: -0.114
-Original Grad: -0.003, -lr * Pred Grad:  -0.044, New P: -0.240
iter 19 loss: 0.002
Actual params: [-0.1137, -0.2401]
-Original Grad: 0.001, -lr * Pred Grad:  0.005, New P: -0.109
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.239
iter 20 loss: 0.002
Actual params: [-0.1089, -0.239 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.002, New P: -0.107
-Original Grad: 0.004, -lr * Pred Grad:  0.040, New P: -0.199
iter 21 loss: 0.002
Actual params: [-0.1074, -0.1987]
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: -0.111
-Original Grad: -0.001, -lr * Pred Grad:  -0.004, New P: -0.203
iter 22 loss: 0.002
Actual params: [-0.1115, -0.2027]
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -0.112
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: -0.191
iter 23 loss: 0.002
Actual params: [-0.1121, -0.1913]
-Original Grad: -0.005, -lr * Pred Grad:  -0.010, New P: -0.122
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.202
iter 24 loss: 0.002
Actual params: [-0.1217, -0.2017]
-Original Grad: -0.003, -lr * Pred Grad:  -0.004, New P: -0.126
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.202
iter 25 loss: 0.002
Actual params: [-0.1262, -0.2019]
-Original Grad: 0.002, -lr * Pred Grad:  0.004, New P: -0.122
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.203
iter 26 loss: 0.002
Actual params: [-0.1219, -0.2031]
-Original Grad: -0.003, -lr * Pred Grad:  -0.006, New P: -0.127
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.199
iter 27 loss: 0.002
Actual params: [-0.1275, -0.1994]
-Original Grad: 0.003, -lr * Pred Grad:  0.006, New P: -0.122
-Original Grad: 0.001, -lr * Pred Grad:  0.004, New P: -0.195
iter 28 loss: 0.002
Actual params: [-0.122 , -0.1954]
-Original Grad: -0.004, -lr * Pred Grad:  -0.006, New P: -0.128
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.196
iter 29 loss: 0.002
Actual params: [-0.1284, -0.1961]
-Original Grad: 0.002, -lr * Pred Grad:  0.005, New P: -0.123
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.216
iter 30 loss: 0.002
Actual params: [-0.1231, -0.2156]
-Original Grad: -0.006, -lr * Pred Grad:  -0.007, New P: -0.130
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.238
Target params: [1.3344, 1.5708]
iter 0 loss: 0.537
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.027, New P: -0.500
-Original Grad: 0.029, -lr * Pred Grad:  0.306, New P: 0.310
iter 1 loss: 0.527
Actual params: [-0.4997,  0.3099]
-Original Grad: 0.006, -lr * Pred Grad:  0.078, New P: -0.421
-Original Grad: 0.013, -lr * Pred Grad:  0.156, New P: 0.466
iter 2 loss: 0.519
Actual params: [-0.4213,  0.4663]
-Original Grad: 0.010, -lr * Pred Grad:  0.130, New P: -0.291
-Original Grad: 0.012, -lr * Pred Grad:  0.155, New P: 0.621
iter 3 loss: 0.505
Actual params: [-0.2908,  0.621 ]
-Original Grad: 0.028, -lr * Pred Grad:  0.381, New P: 0.090
-Original Grad: 0.017, -lr * Pred Grad:  0.209, New P: 0.830
iter 4 loss: 0.455
Actual params: [0.0903, 0.8296]
-Original Grad: 0.071, -lr * Pred Grad:  0.786, New P: 0.876
-Original Grad: 0.007, -lr * Pred Grad:  0.030, New P: 0.859
iter 5 loss: 0.289
Actual params: [0.8759, 0.8595]
-Original Grad: 0.188, -lr * Pred Grad:  0.161, New P: 1.036
-Original Grad: 0.146, -lr * Pred Grad:  0.737, New P: 1.597
iter 6 loss: 0.129
Actual params: [1.0365, 1.5966]
-Original Grad: 0.409, -lr * Pred Grad:  0.227, New P: 1.263
-Original Grad: 0.101, -lr * Pred Grad:  -0.039, New P: 1.557
iter 7 loss: 0.109
Actual params: [1.2631, 1.5574]
-Original Grad: 0.188, -lr * Pred Grad:  -0.017, New P: 1.246
-Original Grad: 0.125, -lr * Pred Grad:  0.339, New P: 1.896
iter 8 loss: 0.087
Actual params: [1.2465, 1.8965]
-Original Grad: 0.265, -lr * Pred Grad:  0.137, New P: 1.384
-Original Grad: 0.024, -lr * Pred Grad:  -0.189, New P: 1.708
iter 9 loss: 0.097
Actual params: [1.3837, 1.7078]
-Original Grad: 0.053, -lr * Pred Grad:  -0.063, New P: 1.320
-Original Grad: 0.130, -lr * Pred Grad:  0.319, New P: 2.027
iter 10 loss: 0.081
Actual params: [1.3203, 2.0266]
-Original Grad: 0.233, -lr * Pred Grad:  0.094, New P: 1.414
-Original Grad: 0.012, -lr * Pred Grad:  -0.112, New P: 1.915
iter 11 loss: 0.081
Actual params: [1.4144, 1.9147]
-Original Grad: 0.040, -lr * Pred Grad:  -0.013, New P: 1.402
-Original Grad: 0.080, -lr * Pred Grad:  0.135, New P: 2.050
iter 12 loss: 0.079
Actual params: [1.4018, 2.0497]
-Original Grad: 0.078, -lr * Pred Grad:  0.024, New P: 1.425
-Original Grad: 0.018, -lr * Pred Grad:  0.006, New P: 2.056
iter 13 loss: 0.078
Actual params: [1.4254, 2.056 ]
-Original Grad: 0.083, -lr * Pred Grad:  0.021, New P: 1.447
-Original Grad: 0.033, -lr * Pred Grad:  0.033, New P: 2.089
iter 14 loss: 0.078
Actual params: [1.4468, 2.0894]
-Original Grad: 0.083, -lr * Pred Grad:  0.035, New P: 1.482
-Original Grad: 0.002, -lr * Pred Grad:  -0.030, New P: 2.059
iter 15 loss: 0.079
Actual params: [1.4815, 2.0595]
-Original Grad: 0.002, -lr * Pred Grad:  -0.005, New P: 1.476
-Original Grad: 0.019, -lr * Pred Grad:  0.034, New P: 2.094
iter 16 loss: 0.078
Actual params: [1.4764, 2.0936]
-Original Grad: 0.041, -lr * Pred Grad:  0.011, New P: 1.487
-Original Grad: 0.026, -lr * Pred Grad:  0.033, New P: 2.127
iter 17 loss: 0.077
Actual params: [1.4875, 2.1268]
-Original Grad: 0.056, -lr * Pred Grad:  0.025, New P: 1.513
-Original Grad: 0.002, -lr * Pred Grad:  -0.011, New P: 2.116
iter 18 loss: 0.079
Actual params: [1.5125, 2.116 ]
-Original Grad: -0.056, -lr * Pred Grad:  -0.028, New P: 1.485
-Original Grad: 0.052, -lr * Pred Grad:  0.081, New P: 2.197
iter 19 loss: 0.077
Actual params: [1.4848, 2.1973]
-Original Grad: 0.070, -lr * Pred Grad:  0.030, New P: 1.515
-Original Grad: -0.009, -lr * Pred Grad:  -0.020, New P: 2.177
iter 20 loss: 0.077
Actual params: [1.5151, 2.1774]
-Original Grad: -0.119, -lr * Pred Grad:  -0.036, New P: 1.479
-Original Grad: 0.065, -lr * Pred Grad:  0.070, New P: 2.248
iter 21 loss: 0.076
Actual params: [1.4786, 2.2476]
-Original Grad: 0.081, -lr * Pred Grad:  0.027, New P: 1.505
-Original Grad: -0.038, -lr * Pred Grad:  -0.042, New P: 2.205
iter 22 loss: 0.076
Actual params: [1.5052, 2.2053]
-Original Grad: 0.072, -lr * Pred Grad:  0.028, New P: 1.533
-Original Grad: -0.005, -lr * Pred Grad:  -0.001, New P: 2.205
iter 23 loss: 0.077
Actual params: [1.5331, 2.2046]
-Original Grad: 0.037, -lr * Pred Grad:  0.014, New P: 1.547
-Original Grad: -0.026, -lr * Pred Grad:  -0.036, New P: 2.168
iter 24 loss: 0.079
Actual params: [1.5466, 2.1684]
-Original Grad: -0.150, -lr * Pred Grad:  -0.043, New P: 1.504
-Original Grad: 0.064, -lr * Pred Grad:  0.054, New P: 2.222
iter 25 loss: 0.076
Actual params: [1.5041, 2.2223]
-Original Grad: 0.061, -lr * Pred Grad:  0.012, New P: 1.516
-Original Grad: -0.051, -lr * Pred Grad:  -0.062, New P: 2.161
iter 26 loss: 0.077
Actual params: [1.5158, 2.1608]
-Original Grad: -0.003, -lr * Pred Grad:  -0.007, New P: 1.509
-Original Grad: -0.017, -lr * Pred Grad:  -0.028, New P: 2.133
iter 27 loss: 0.078
Actual params: [1.5091, 2.1332]
-Original Grad: -0.041, -lr * Pred Grad:  -0.007, New P: 1.502
-Original Grad: 0.031, -lr * Pred Grad:  0.034, New P: 2.167
iter 28 loss: 0.077
Actual params: [1.5019, 2.1673]
-Original Grad: 0.080, -lr * Pred Grad:  0.035, New P: 1.537
-Original Grad: -0.002, -lr * Pred Grad:  0.023, New P: 2.190
iter 29 loss: 0.078
Actual params: [1.5373, 2.1901]
-Original Grad: -0.044, -lr * Pred Grad:  -0.018, New P: 1.520
-Original Grad: 0.006, -lr * Pred Grad:  -0.008, New P: 2.182
iter 30 loss: 0.077
Actual params: [1.5195, 2.1823]
-Original Grad: -0.153, -lr * Pred Grad:  -0.030, New P: 1.489
-Original Grad: 0.080, -lr * Pred Grad:  0.051, New P: 2.233
Target params: [1.3344, 1.5708]
iter 0 loss: 0.577
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.472
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.005
iter 1 loss: 0.576
Actual params: [-0.4717,  0.0051]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.468
-Original Grad: 0.001, -lr * Pred Grad:  0.006, New P: 0.011
iter 2 loss: 0.576
Actual params: [-0.4676,  0.0114]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.463
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.020
iter 3 loss: 0.576
Actual params: [-0.4633,  0.0199]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.459
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: 0.028
iter 4 loss: 0.576
Actual params: [-0.459 ,  0.0282]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.454
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.038
iter 5 loss: 0.576
Actual params: [-0.4543,  0.0378]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.451
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.046
iter 6 loss: 0.576
Actual params: [-0.4511,  0.0464]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.444
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.057
iter 7 loss: 0.576
Actual params: [-0.4437,  0.0573]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.437
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.069
iter 8 loss: 0.576
Actual params: [-0.4368,  0.0686]
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: -0.424
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: 0.087
iter 9 loss: 0.575
Actual params: [-0.424,  0.087]
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: -0.416
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.103
iter 10 loss: 0.575
Actual params: [-0.416 ,  0.1027]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.409
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.114
iter 11 loss: 0.575
Actual params: [-0.4094,  0.1142]
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: -0.398
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.131
iter 12 loss: 0.574
Actual params: [-0.3984,  0.1313]
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: -0.382
-Original Grad: 0.001, -lr * Pred Grad:  0.024, New P: 0.156
iter 13 loss: 0.574
Actual params: [-0.3816,  0.1558]
-Original Grad: 0.001, -lr * Pred Grad:  0.023, New P: -0.358
-Original Grad: 0.001, -lr * Pred Grad:  0.034, New P: 0.190
iter 14 loss: 0.572
Actual params: [-0.3582,  0.1895]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.356
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.201
iter 15 loss: 0.572
Actual params: [-0.3556,  0.2014]
-Original Grad: 0.002, -lr * Pred Grad:  0.085, New P: -0.271
-Original Grad: 0.002, -lr * Pred Grad:  0.131, New P: 0.333
iter 16 loss: 0.560
Actual params: [-0.2709,  0.3328]
-Original Grad: 0.002, -lr * Pred Grad:  0.096, New P: -0.175
-Original Grad: 0.002, -lr * Pred Grad:  0.125, New P: 0.458
iter 17 loss: 0.541
Actual params: [-0.175 ,  0.4578]
-Original Grad: 0.001, -lr * Pred Grad:  0.042, New P: -0.133
-Original Grad: 0.001, -lr * Pred Grad:  0.046, New P: 0.504
iter 18 loss: 0.532
Actual params: [-0.1329,  0.5038]
-Original Grad: 0.001, -lr * Pred Grad:  0.075, New P: -0.058
-Original Grad: 0.001, -lr * Pred Grad:  0.095, New P: 0.599
iter 19 loss: 0.514
Actual params: [-0.0577,  0.599 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.126, New P: 0.068
-Original Grad: 0.001, -lr * Pred Grad:  0.117, New P: 0.716
iter 20 loss: 0.481
Actual params: [0.0679, 0.716 ]
-Original Grad: 0.003, -lr * Pred Grad:  0.249, New P: 0.317
-Original Grad: 0.003, -lr * Pred Grad:  0.258, New P: 0.974
iter 21 loss: 0.408
Actual params: [0.3172, 0.9742]
-Original Grad: 0.020, -lr * Pred Grad:  1.429, New P: 1.746
-Original Grad: 0.008, -lr * Pred Grad:  0.535, New P: 1.509
iter 22 loss: 0.246
Actual params: [1.7458, 1.5094]
-Original Grad: 0.027, -lr * Pred Grad:  -0.002, New P: 1.743
-Original Grad: -0.307, -lr * Pred Grad:  -0.447, New P: 1.062
iter 23 loss: 0.124
Actual params: [1.7433, 1.062 ]
-Original Grad: 0.012, -lr * Pred Grad:  -0.399, New P: 1.345
-Original Grad: -0.267, -lr * Pred Grad:  -0.204, New P: 0.858
iter 24 loss: 0.100
Actual params: [1.3446, 0.8575]
-Original Grad: 0.115, -lr * Pred Grad:  1.186, New P: 2.530
-Original Grad: -0.085, -lr * Pred Grad:  0.072, New P: 0.929
iter 25 loss: 0.169
Actual params: [2.5304, 0.9292]
-Original Grad: -0.055, -lr * Pred Grad:  -0.469, New P: 2.062
-Original Grad: -0.116, -lr * Pred Grad:  -0.084, New P: 0.845
iter 26 loss: 0.128
Actual params: [2.0615, 0.8449]
-Original Grad: -0.011, -lr * Pred Grad:  -0.097, New P: 1.965
-Original Grad: -0.097, -lr * Pred Grad:  -0.048, New P: 0.797
iter 27 loss: 0.118
Actual params: [1.9646, 0.7974]
-Original Grad: -0.050, -lr * Pred Grad:  -0.261, New P: 1.703
-Original Grad: 0.051, -lr * Pred Grad:  0.017, New P: 0.814
iter 28 loss: 0.100
Actual params: [1.7032, 0.8142]
-Original Grad: 0.027, -lr * Pred Grad:  0.142, New P: 1.845
-Original Grad: -0.074, -lr * Pred Grad:  -0.033, New P: 0.781
iter 29 loss: 0.108
Actual params: [1.8454, 0.7815]
-Original Grad: -0.001, -lr * Pred Grad:  -0.013, New P: 1.832
-Original Grad: -0.087, -lr * Pred Grad:  -0.042, New P: 0.739
iter 30 loss: 0.106
Actual params: [1.8321, 0.7392]
-Original Grad: 0.015, -lr * Pred Grad:  0.088, New P: 1.920
-Original Grad: 0.002, -lr * Pred Grad:  0.001, New P: 0.740
Target params: [1.3344, 1.5708]
iter 0 loss: 0.486
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.022, -lr * Pred Grad:  0.227, New P: -0.246
-Original Grad: 0.036, -lr * Pred Grad:  0.373, New P: 0.376
iter 1 loss: 0.439
Actual params: [-0.2456,  0.3761]
-Original Grad: 0.109, -lr * Pred Grad:  0.524, New P: 0.278
-Original Grad: 0.110, -lr * Pred Grad:  0.530, New P: 0.906
iter 2 loss: 0.199
Actual params: [0.2781, 0.906 ]
-Original Grad: 0.202, -lr * Pred Grad:  0.337, New P: 0.616
-Original Grad: -0.011, -lr * Pred Grad:  -0.143, New P: 0.763
iter 3 loss: 0.146
Actual params: [0.6155, 0.7629]
-Original Grad: 0.288, -lr * Pred Grad:  0.247, New P: 0.862
-Original Grad: 0.075, -lr * Pred Grad:  0.263, New P: 1.026
iter 4 loss: 0.079
Actual params: [0.8624, 1.0263]
-Original Grad: 0.250, -lr * Pred Grad:  0.144, New P: 1.006
-Original Grad: -0.030, -lr * Pred Grad:  -0.043, New P: 0.983
iter 5 loss: 0.064
Actual params: [1.0062, 0.9829]
-Original Grad: 0.187, -lr * Pred Grad:  0.073, New P: 1.079
-Original Grad: -0.070, -lr * Pred Grad:  -0.103, New P: 0.879
iter 6 loss: 0.072
Actual params: [1.0795, 0.8794]
-Original Grad: 0.131, -lr * Pred Grad:  0.046, New P: 1.126
-Original Grad: 0.166, -lr * Pred Grad:  0.105, New P: 0.984
iter 7 loss: 0.046
Actual params: [1.1259, 0.9843]
-Original Grad: 0.077, -lr * Pred Grad:  0.037, New P: 1.163
-Original Grad: -0.010, -lr * Pred Grad:  -0.014, New P: 0.971
iter 8 loss: 0.042
Actual params: [1.1627, 0.9705]
-Original Grad: 0.066, -lr * Pred Grad:  0.036, New P: 1.199
-Original Grad: -0.068, -lr * Pred Grad:  -0.046, New P: 0.924
iter 9 loss: 0.044
Actual params: [1.1991, 0.9241]
-Original Grad: 0.073, -lr * Pred Grad:  0.019, New P: 1.218
-Original Grad: 0.137, -lr * Pred Grad:  0.047, New P: 0.971
iter 10 loss: 0.035
Actual params: [1.2184, 0.9715]
-Original Grad: 0.027, -lr * Pred Grad:  0.022, New P: 1.240
-Original Grad: -0.057, -lr * Pred Grad:  -0.028, New P: 0.943
iter 11 loss: 0.036
Actual params: [1.2402, 0.9431]
-Original Grad: 0.027, -lr * Pred Grad:  0.019, New P: 1.260
-Original Grad: -0.026, -lr * Pred Grad:  -0.015, New P: 0.928
iter 12 loss: 0.035
Actual params: [1.2597, 0.9279]
-Original Grad: 0.053, -lr * Pred Grad:  0.031, New P: 1.291
-Original Grad: 0.015, -lr * Pred Grad:  -0.002, New P: 0.926
iter 13 loss: 0.032
Actual params: [1.2908, 0.9256]
-Original Grad: 0.024, -lr * Pred Grad:  0.026, New P: 1.317
-Original Grad: -0.064, -lr * Pred Grad:  -0.031, New P: 0.895
iter 14 loss: 0.033
Actual params: [1.3167, 0.8947]
-Original Grad: 0.037, -lr * Pred Grad:  0.035, New P: 1.352
-Original Grad: -0.064, -lr * Pred Grad:  -0.034, New P: 0.861
iter 15 loss: 0.033
Actual params: [1.3517, 0.8611]
-Original Grad: 0.025, -lr * Pred Grad:  0.028, New P: 1.379
-Original Grad: -0.064, -lr * Pred Grad:  -0.034, New P: 0.828
iter 16 loss: 0.033
Actual params: [1.3793, 0.8275]
-Original Grad: 0.118, -lr * Pred Grad:  0.047, New P: 1.426
-Original Grad: 0.143, -lr * Pred Grad:  0.028, New P: 0.855
iter 17 loss: 0.026
Actual params: [1.4261, 0.8555]
-Original Grad: 0.046, -lr * Pred Grad:  0.027, New P: 1.453
-Original Grad: 0.029, -lr * Pred Grad:  -0.002, New P: 0.854
iter 18 loss: 0.025
Actual params: [1.4528, 0.8539]
-Original Grad: 0.019, -lr * Pred Grad:  0.032, New P: 1.485
-Original Grad: -0.059, -lr * Pred Grad:  -0.030, New P: 0.824
iter 19 loss: 0.024
Actual params: [1.4849, 0.8241]
-Original Grad: 0.029, -lr * Pred Grad:  0.032, New P: 1.517
-Original Grad: -0.025, -lr * Pred Grad:  -0.020, New P: 0.804
iter 20 loss: 0.023
Actual params: [1.517 , 0.8045]
-Original Grad: 0.052, -lr * Pred Grad:  0.039, New P: 1.556
-Original Grad: 0.014, -lr * Pred Grad:  -0.011, New P: 0.794
iter 21 loss: 0.022
Actual params: [1.5558, 0.7935]
-Original Grad: -0.009, -lr * Pred Grad:  0.010, New P: 1.566
-Original Grad: -0.050, -lr * Pred Grad:  -0.021, New P: 0.772
iter 22 loss: 0.022
Actual params: [1.5657, 0.7723]
-Original Grad: 0.021, -lr * Pred Grad:  0.022, New P: 1.588
-Original Grad: -0.005, -lr * Pred Grad:  -0.011, New P: 0.762
iter 23 loss: 0.022
Actual params: [1.5879, 0.7616]
-Original Grad: 0.042, -lr * Pred Grad:  0.029, New P: 1.617
-Original Grad: 0.036, -lr * Pred Grad:  0.001, New P: 0.763
iter 24 loss: 0.022
Actual params: [1.6167, 0.7625]
-Original Grad: 0.003, -lr * Pred Grad:  0.028, New P: 1.645
-Original Grad: -0.051, -lr * Pred Grad:  -0.031, New P: 0.732
iter 25 loss: 0.022
Actual params: [1.6445, 0.7318]
-Original Grad: 0.009, -lr * Pred Grad:  0.029, New P: 1.674
-Original Grad: -0.034, -lr * Pred Grad:  -0.026, New P: 0.706
iter 26 loss: 0.023
Actual params: [1.6739, 0.7062]
-Original Grad: -0.002, -lr * Pred Grad:  0.002, New P: 1.676
-Original Grad: -0.009, -lr * Pred Grad:  -0.005, New P: 0.702
iter 27 loss: 0.023
Actual params: [1.6758, 0.7017]
-Original Grad: -0.001, -lr * Pred Grad:  0.007, New P: 1.683
-Original Grad: -0.015, -lr * Pred Grad:  -0.010, New P: 0.692
iter 28 loss: 0.023
Actual params: [1.6832, 0.6917]
-Original Grad: 0.017, -lr * Pred Grad:  0.024, New P: 1.707
-Original Grad: -0.005, -lr * Pred Grad:  -0.014, New P: 0.678
iter 29 loss: 0.024
Actual params: [1.7069, 0.6777]
-Original Grad: -0.045, -lr * Pred Grad:  -0.032, New P: 1.675
-Original Grad: -0.014, -lr * Pred Grad:  0.009, New P: 0.687
iter 30 loss: 0.023
Actual params: [1.6749, 0.6865]
-Original Grad: 0.014, -lr * Pred Grad:  0.019, New P: 1.694
-Original Grad: -0.015, -lr * Pred Grad:  -0.016, New P: 0.670
Target params: [1.3344, 1.5708]
iter 0 loss: 0.170
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.208, -lr * Pred Grad:  0.622, New P: 0.150
-Original Grad: -0.033, -lr * Pred Grad:  -0.076, New P: -0.072
iter 1 loss: 0.081
Actual params: [ 0.1497, -0.0723]
-Original Grad: 0.238, -lr * Pred Grad:  0.231, New P: 0.381
-Original Grad: 0.081, -lr * Pred Grad:  0.288, New P: 0.216
iter 2 loss: 0.019
Actual params: [0.3806, 0.2155]
-Original Grad: 0.216, -lr * Pred Grad:  0.064, New P: 0.445
-Original Grad: 0.104, -lr * Pred Grad:  0.073, New P: 0.289
iter 3 loss: 0.009
Actual params: [0.4449, 0.2885]
-Original Grad: 0.126, -lr * Pred Grad:  0.044, New P: 0.489
-Original Grad: 0.047, -lr * Pred Grad:  -0.011, New P: 0.277
iter 4 loss: 0.006
Actual params: [0.4887, 0.2773]
-Original Grad: 0.092, -lr * Pred Grad:  0.039, New P: 0.527
-Original Grad: 0.027, -lr * Pred Grad:  -0.031, New P: 0.246
iter 5 loss: 0.005
Actual params: [0.5273, 0.2459]
-Original Grad: 0.049, -lr * Pred Grad:  -0.002, New P: 0.525
-Original Grad: 0.036, -lr * Pred Grad:  0.039, New P: 0.285
iter 6 loss: 0.005
Actual params: [0.5248, 0.2847]
-Original Grad: 0.018, -lr * Pred Grad:  0.009, New P: 0.534
-Original Grad: 0.001, -lr * Pred Grad:  -0.013, New P: 0.271
iter 7 loss: 0.005
Actual params: [0.5343, 0.2714]
-Original Grad: 0.040, -lr * Pred Grad:  0.002, New P: 0.537
-Original Grad: 0.027, -lr * Pred Grad:  0.014, New P: 0.285
iter 8 loss: 0.004
Actual params: [0.5367, 0.285 ]
-Original Grad: -0.026, -lr * Pred Grad:  0.012, New P: 0.549
-Original Grad: -0.043, -lr * Pred Grad:  -0.035, New P: 0.250
iter 9 loss: 0.005
Actual params: [0.5486, 0.2499]
-Original Grad: 0.030, -lr * Pred Grad:  -0.007, New P: 0.541
-Original Grad: 0.048, -lr * Pred Grad:  0.028, New P: 0.277
iter 10 loss: 0.004
Actual params: [0.5412, 0.2774]
-Original Grad: 0.015, -lr * Pred Grad:  -0.008, New P: 0.533
-Original Grad: 0.041, -lr * Pred Grad:  0.022, New P: 0.299
iter 11 loss: 0.004
Actual params: [0.5327, 0.2989]
-Original Grad: -0.013, -lr * Pred Grad:  -0.006, New P: 0.527
-Original Grad: 0.003, -lr * Pred Grad:  0.006, New P: 0.305
iter 12 loss: 0.005
Actual params: [0.5265, 0.305 ]
-Original Grad: 0.008, -lr * Pred Grad:  0.000, New P: 0.527
-Original Grad: 0.009, -lr * Pred Grad:  0.002, New P: 0.307
iter 13 loss: 0.004
Actual params: [0.527 , 0.3068]
-Original Grad: 0.006, -lr * Pred Grad:  0.003, New P: 0.530
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: 0.305
iter 14 loss: 0.004
Actual params: [0.5295, 0.3047]
-Original Grad: 0.041, -lr * Pred Grad:  0.013, New P: 0.542
-Original Grad: 0.006, -lr * Pred Grad:  -0.009, New P: 0.296
iter 15 loss: 0.004
Actual params: [0.5424, 0.2958]
-Original Grad: -0.008, -lr * Pred Grad:  -0.004, New P: 0.538
-Original Grad: 0.006, -lr * Pred Grad:  0.005, New P: 0.300
iter 16 loss: 0.004
Actual params: [0.5383, 0.3003]
-Original Grad: 0.036, -lr * Pred Grad:  0.012, New P: 0.550
-Original Grad: 0.003, -lr * Pred Grad:  -0.009, New P: 0.292
iter 17 loss: 0.004
Actual params: [0.5502, 0.2916]
-Original Grad: 0.021, -lr * Pred Grad:  0.009, New P: 0.559
-Original Grad: -0.007, -lr * Pred Grad:  -0.009, New P: 0.283
iter 18 loss: 0.004
Actual params: [0.559 , 0.2826]
-Original Grad: 0.024, -lr * Pred Grad:  0.005, New P: 0.564
-Original Grad: 0.011, -lr * Pred Grad:  -0.001, New P: 0.282
iter 19 loss: 0.004
Actual params: [0.5639, 0.2816]
-Original Grad: 0.041, -lr * Pred Grad:  0.007, New P: 0.571
-Original Grad: 0.014, -lr * Pred Grad:  -0.003, New P: 0.278
iter 20 loss: 0.004
Actual params: [0.5713, 0.2784]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.573
-Original Grad: -0.006, -lr * Pred Grad:  -0.002, New P: 0.276
iter 21 loss: 0.004
Actual params: [0.5726, 0.276 ]
-Original Grad: 0.031, -lr * Pred Grad:  0.001, New P: 0.574
-Original Grad: 0.035, -lr * Pred Grad:  0.007, New P: 0.283
iter 22 loss: 0.004
Actual params: [0.5736, 0.2828]
-Original Grad: 0.035, -lr * Pred Grad:  0.005, New P: 0.578
-Original Grad: 0.024, -lr * Pred Grad:  0.002, New P: 0.284
iter 23 loss: 0.004
Actual params: [0.5782, 0.2845]
-Original Grad: 0.032, -lr * Pred Grad:  0.005, New P: 0.584
-Original Grad: 0.015, -lr * Pred Grad:  -0.001, New P: 0.284
iter 24 loss: 0.004
Actual params: [0.5836, 0.2838]
-Original Grad: -0.006, -lr * Pred Grad:  0.001, New P: 0.585
-Original Grad: -0.015, -lr * Pred Grad:  -0.004, New P: 0.280
iter 25 loss: 0.004
Actual params: [0.5849, 0.2797]
-Original Grad: -0.008, -lr * Pred Grad:  0.002, New P: 0.587
-Original Grad: -0.021, -lr * Pred Grad:  -0.006, New P: 0.274
iter 26 loss: 0.004
Actual params: [0.587 , 0.2737]
-Original Grad: 0.031, -lr * Pred Grad:  -0.001, New P: 0.586
-Original Grad: 0.050, -lr * Pred Grad:  0.011, New P: 0.285
iter 27 loss: 0.004
Actual params: [0.5857, 0.2849]
-Original Grad: 0.010, -lr * Pred Grad:  0.007, New P: 0.593
-Original Grad: -0.025, -lr * Pred Grad:  -0.010, New P: 0.275
iter 28 loss: 0.004
Actual params: [0.5926, 0.2749]
-Original Grad: -0.023, -lr * Pred Grad:  0.001, New P: 0.593
-Original Grad: -0.036, -lr * Pred Grad:  -0.008, New P: 0.267
iter 29 loss: 0.004
Actual params: [0.5934, 0.2671]
-Original Grad: 0.035, -lr * Pred Grad:  0.008, New P: 0.602
-Original Grad: -0.001, -lr * Pred Grad:  -0.006, New P: 0.261
iter 30 loss: 0.004
Actual params: [0.6015, 0.2606]
-Original Grad: 0.015, -lr * Pred Grad:  0.002, New P: 0.604
-Original Grad: 0.008, -lr * Pred Grad:  0.000, New P: 0.261
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: -0.464
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.005
iter 1 loss: 0.363
Actual params: [-0.4639,  0.0054]
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.454
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.008
iter 2 loss: 0.363
Actual params: [-0.4542,  0.0076]
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: -0.440
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.011
iter 3 loss: 0.363
Actual params: [-0.4397,  0.0109]
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: -0.425
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.014
iter 4 loss: 0.363
Actual params: [-0.4246,  0.0142]
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: -0.407
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.018
iter 5 loss: 0.363
Actual params: [-0.4069,  0.0178]
-Original Grad: 0.001, -lr * Pred Grad:  0.024, New P: -0.383
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.023
iter 6 loss: 0.363
Actual params: [-0.383 ,  0.0228]
-Original Grad: 0.002, -lr * Pred Grad:  0.040, New P: -0.343
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.031
iter 7 loss: 0.363
Actual params: [-0.3428,  0.0314]
-Original Grad: 0.002, -lr * Pred Grad:  0.041, New P: -0.302
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.040
iter 8 loss: 0.363
Actual params: [-0.3017,  0.0396]
-Original Grad: 0.003, -lr * Pred Grad:  0.072, New P: -0.229
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.054
iter 9 loss: 0.363
Actual params: [-0.2293,  0.0541]
-Original Grad: 0.004, -lr * Pred Grad:  0.121, New P: -0.108
-Original Grad: 0.001, -lr * Pred Grad:  0.024, New P: 0.078
iter 10 loss: 0.362
Actual params: [-0.108 ,  0.0784]
-Original Grad: 0.010, -lr * Pred Grad:  0.303, New P: 0.195
-Original Grad: 0.002, -lr * Pred Grad:  0.062, New P: 0.140
iter 11 loss: 0.339
Actual params: [0.1947, 0.1401]
-Original Grad: 0.085, -lr * Pred Grad:  1.063, New P: 1.258
-Original Grad: 0.020, -lr * Pred Grad:  0.258, New P: 0.398
iter 12 loss: 0.164
Actual params: [1.2577, 0.3984]
-Original Grad: 0.135, -lr * Pred Grad:  -0.052, New P: 1.206
-Original Grad: 0.554, -lr * Pred Grad:  0.355, New P: 0.753
iter 13 loss: 0.047
Actual params: [1.2059, 0.7533]
-Original Grad: 0.209, -lr * Pred Grad:  0.178, New P: 1.384
-Original Grad: 0.480, -lr * Pred Grad:  0.045, New P: 0.798
iter 14 loss: 0.022
Actual params: [1.3837, 0.7984]
-Original Grad: 0.185, -lr * Pred Grad:  0.047, New P: 1.431
-Original Grad: 0.266, -lr * Pred Grad:  0.007, New P: 0.806
iter 15 loss: 0.019
Actual params: [1.4309, 0.8058]
-Original Grad: 0.183, -lr * Pred Grad:  0.020, New P: 1.451
-Original Grad: 0.223, -lr * Pred Grad:  0.003, New P: 0.809
iter 16 loss: 0.018
Actual params: [1.4507, 0.8087]
-Original Grad: -0.037, -lr * Pred Grad:  -0.043, New P: 1.408
-Original Grad: -0.002, -lr * Pred Grad:  0.032, New P: 0.841
iter 17 loss: 0.017
Actual params: [1.4078, 0.841 ]
-Original Grad: 0.019, -lr * Pred Grad:  -0.031, New P: 1.376
-Original Grad: 0.058, -lr * Pred Grad:  0.029, New P: 0.870
iter 18 loss: 0.016
Actual params: [1.3764, 0.8698]
-Original Grad: 0.103, -lr * Pred Grad:  -0.045, New P: 1.332
-Original Grad: 0.199, -lr * Pred Grad:  0.047, New P: 0.917
iter 19 loss: 0.013
Actual params: [1.3319, 0.9168]
-Original Grad: -0.000, -lr * Pred Grad:  -0.043, New P: 1.289
-Original Grad: 0.229, -lr * Pred Grad:  0.043, New P: 0.959
iter 20 loss: 0.012
Actual params: [1.2888, 0.9594]
-Original Grad: 0.085, -lr * Pred Grad:  0.004, New P: 1.293
-Original Grad: 0.091, -lr * Pred Grad:  0.003, New P: 0.962
iter 21 loss: 0.012
Actual params: [1.2931, 0.9622]
-Original Grad: 0.125, -lr * Pred Grad:  0.005, New P: 1.298
-Original Grad: 0.134, -lr * Pred Grad:  0.006, New P: 0.968
iter 22 loss: 0.011
Actual params: [1.2979, 0.9678]
-Original Grad: 0.117, -lr * Pred Grad:  0.003, New P: 1.301
-Original Grad: 0.143, -lr * Pred Grad:  0.008, New P: 0.976
iter 23 loss: 0.011
Actual params: [1.3006, 0.9759]
-Original Grad: 0.207, -lr * Pred Grad:  0.012, New P: 1.312
-Original Grad: 0.125, -lr * Pred Grad:  0.001, New P: 0.977
iter 24 loss: 0.011
Actual params: [1.3122, 0.9767]
-Original Grad: 0.124, -lr * Pred Grad:  0.003, New P: 1.315
-Original Grad: 0.119, -lr * Pred Grad:  0.007, New P: 0.983
iter 25 loss: 0.011
Actual params: [1.3154, 0.9834]
-Original Grad: 0.087, -lr * Pred Grad:  0.006, New P: 1.322
-Original Grad: 0.006, -lr * Pred Grad:  -0.004, New P: 0.980
iter 26 loss: 0.010
Actual params: [1.3218, 0.9796]
-Original Grad: 0.167, -lr * Pred Grad:  0.006, New P: 1.328
-Original Grad: 0.113, -lr * Pred Grad:  0.005, New P: 0.985
iter 27 loss: 0.010
Actual params: [1.3277, 0.9849]
-Original Grad: 0.007, -lr * Pred Grad:  0.001, New P: 1.328
-Original Grad: -0.001, -lr * Pred Grad:  -0.000, New P: 0.984
iter 28 loss: 0.010
Actual params: [1.3282, 0.9844]
-Original Grad: -0.036, -lr * Pred Grad:  -0.006, New P: 1.322
-Original Grad: 0.106, -lr * Pred Grad:  0.014, New P: 0.998
iter 29 loss: 0.010
Actual params: [1.3217, 0.9984]
-Original Grad: 0.099, -lr * Pred Grad:  0.006, New P: 1.327
-Original Grad: 0.016, -lr * Pred Grad:  -0.001, New P: 0.997
iter 30 loss: 0.010
Actual params: [1.3273, 0.9969]
-Original Grad: 0.125, -lr * Pred Grad:  0.008, New P: 1.336
-Original Grad: -0.036, -lr * Pred Grad:  -0.008, New P: 0.989
Target params: [1.3344, 1.5708]
iter 0 loss: 0.320
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.067, -lr * Pred Grad:  -0.609, New P: -1.082
-Original Grad: 0.003, -lr * Pred Grad:  0.024, New P: 0.028
iter 1 loss: 0.315
Actual params: [-1.0815,  0.0279]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.086
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.030
iter 2 loss: 0.315
Actual params: [-1.086 ,  0.0295]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.091
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.031
iter 3 loss: 0.315
Actual params: [-1.091 ,  0.0314]
-Original Grad: -0.001, -lr * Pred Grad:  -0.007, New P: -1.098
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.034
iter 4 loss: 0.315
Actual params: [-1.0981,  0.0344]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.104
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.037
iter 5 loss: 0.315
Actual params: [-1.1038,  0.0366]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.110
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.039
iter 6 loss: 0.315
Actual params: [-1.1099,  0.0387]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.116
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.041
iter 7 loss: 0.315
Actual params: [-1.1156,  0.0412]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.125
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.045
iter 8 loss: 0.315
Actual params: [-1.1246,  0.0448]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.131
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.048
iter 9 loss: 0.315
Actual params: [-1.1315,  0.0475]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.141
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.051
iter 10 loss: 0.315
Actual params: [-1.1413,  0.0511]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.150
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.055
iter 11 loss: 0.315
Actual params: [-1.1502,  0.0549]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.160
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.059
iter 12 loss: 0.315
Actual params: [-1.1603,  0.0591]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.171
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.063
iter 13 loss: 0.315
Actual params: [-1.1711,  0.0632]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.180
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.068
iter 14 loss: 0.315
Actual params: [-1.18  ,  0.0676]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.190
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.072
iter 15 loss: 0.315
Actual params: [-1.1902,  0.0722]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.201
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.077
iter 16 loss: 0.315
Actual params: [-1.201 ,  0.0766]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.209
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.080
iter 17 loss: 0.315
Actual params: [-1.2088,  0.0799]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.218
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.085
iter 18 loss: 0.315
Actual params: [-1.2176,  0.0849]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.227
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.090
iter 19 loss: 0.315
Actual params: [-1.2275,  0.0902]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.235
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.094
iter 20 loss: 0.315
Actual params: [-1.2348,  0.0944]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.246
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.101
iter 21 loss: 0.315
Actual params: [-1.2462,  0.1009]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.254
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.105
iter 22 loss: 0.315
Actual params: [-1.2539,  0.1053]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.262
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.109
iter 23 loss: 0.315
Actual params: [-1.262 ,  0.1093]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.269
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.114
iter 24 loss: 0.315
Actual params: [-1.2691,  0.1138]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.280
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.120
iter 25 loss: 0.315
Actual params: [-1.2804,  0.1203]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.289
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.125
iter 26 loss: 0.315
Actual params: [-1.2887,  0.1254]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.298
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.131
iter 27 loss: 0.315
Actual params: [-1.2983,  0.1308]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.304
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.135
iter 28 loss: 0.315
Actual params: [-1.3043,  0.1347]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.315
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.141
iter 29 loss: 0.315
Actual params: [-1.3152,  0.1411]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.332
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.151
iter 30 loss: 0.315
Actual params: [-1.332 ,  0.1512]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.342
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.157
Target params: [1.3344, 1.5708]
iter 0 loss: 0.583
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.035, -lr * Pred Grad:  0.352, New P: -0.120
-Original Grad: 0.010, -lr * Pred Grad:  0.099, New P: 0.103
iter 1 loss: 0.563
Actual params: [-0.1203,  0.1026]
-Original Grad: 0.033, -lr * Pred Grad:  0.329, New P: 0.209
-Original Grad: 0.023, -lr * Pred Grad:  0.249, New P: 0.351
iter 2 loss: 0.508
Actual params: [0.2091, 0.3513]
-Original Grad: 0.173, -lr * Pred Grad:  0.613, New P: 0.823
-Original Grad: 0.083, -lr * Pred Grad:  0.297, New P: 0.648
iter 3 loss: 0.277
Actual params: [0.8226, 0.648 ]
-Original Grad: 0.623, -lr * Pred Grad:  0.217, New P: 1.040
-Original Grad: 0.214, -lr * Pred Grad:  0.191, New P: 0.839
iter 4 loss: 0.198
Actual params: [1.0399, 0.8388]
-Original Grad: -0.169, -lr * Pred Grad:  -0.156, New P: 0.883
-Original Grad: 0.214, -lr * Pred Grad:  0.452, New P: 1.291
iter 5 loss: 0.144
Actual params: [0.8835, 1.2907]
-Original Grad: 0.725, -lr * Pred Grad:  0.146, New P: 1.030
-Original Grad: 0.021, -lr * Pred Grad:  -0.050, New P: 1.240
iter 6 loss: 0.108
Actual params: [1.0295, 1.2402]
-Original Grad: 0.645, -lr * Pred Grad:  0.074, New P: 1.103
-Original Grad: 0.053, -lr * Pred Grad:  0.027, New P: 1.267
iter 7 loss: 0.094
Actual params: [1.1031, 1.2675]
-Original Grad: 0.433, -lr * Pred Grad:  0.031, New P: 1.134
-Original Grad: 0.178, -lr * Pred Grad:  0.175, New P: 1.443
iter 8 loss: 0.072
Actual params: [1.1344, 1.4429]
-Original Grad: 0.309, -lr * Pred Grad:  0.022, New P: 1.157
-Original Grad: 0.118, -lr * Pred Grad:  0.100, New P: 1.543
iter 9 loss: 0.065
Actual params: [1.1567, 1.5427]
-Original Grad: 0.273, -lr * Pred Grad:  0.018, New P: 1.175
-Original Grad: 0.113, -lr * Pred Grad:  0.078, New P: 1.620
iter 10 loss: 0.061
Actual params: [1.1747, 1.6203]
-Original Grad: 0.310, -lr * Pred Grad:  0.026, New P: 1.201
-Original Grad: 0.031, -lr * Pred Grad:  0.005, New P: 1.625
iter 11 loss: 0.057
Actual params: [1.2008, 1.6253]
-Original Grad: 0.282, -lr * Pred Grad:  0.022, New P: 1.223
-Original Grad: 0.054, -lr * Pred Grad:  0.020, New P: 1.645
iter 12 loss: 0.054
Actual params: [1.2229, 1.6448]
-Original Grad: 0.303, -lr * Pred Grad:  0.023, New P: 1.246
-Original Grad: 0.074, -lr * Pred Grad:  0.025, New P: 1.669
iter 13 loss: 0.051
Actual params: [1.2456, 1.6694]
-Original Grad: 0.327, -lr * Pred Grad:  0.026, New P: 1.272
-Original Grad: 0.029, -lr * Pred Grad:  -0.003, New P: 1.666
iter 14 loss: 0.048
Actual params: [1.2716, 1.6662]
-Original Grad: 0.151, -lr * Pred Grad:  0.014, New P: 1.286
-Original Grad: -0.053, -lr * Pred Grad:  -0.024, New P: 1.642
iter 15 loss: 0.047
Actual params: [1.286 , 1.6424]
-Original Grad: 0.029, -lr * Pred Grad:  0.001, New P: 1.287
-Original Grad: 0.138, -lr * Pred Grad:  0.040, New P: 1.683
iter 16 loss: 0.046
Actual params: [1.2871, 1.6829]
-Original Grad: 0.080, -lr * Pred Grad:  0.007, New P: 1.294
-Original Grad: 0.021, -lr * Pred Grad:  0.006, New P: 1.689
iter 17 loss: 0.045
Actual params: [1.294 , 1.6888]
-Original Grad: 0.227, -lr * Pred Grad:  0.019, New P: 1.313
-Original Grad: -0.063, -lr * Pred Grad:  -0.014, New P: 1.675
iter 18 loss: 0.044
Actual params: [1.3135, 1.6748]
-Original Grad: 0.143, -lr * Pred Grad:  0.014, New P: 1.327
-Original Grad: 0.049, -lr * Pred Grad:  0.014, New P: 1.689
iter 19 loss: 0.043
Actual params: [1.3273, 1.689 ]
-Original Grad: -0.011, -lr * Pred Grad:  0.002, New P: 1.329
-Original Grad: 0.157, -lr * Pred Grad:  0.037, New P: 1.726
iter 20 loss: 0.042
Actual params: [1.3294, 1.7265]
-Original Grad: 0.033, -lr * Pred Grad:  0.002, New P: 1.331
-Original Grad: -0.052, -lr * Pred Grad:  -0.010, New P: 1.716
iter 21 loss: 0.042
Actual params: [1.331 , 1.7161]
-Original Grad: 0.107, -lr * Pred Grad:  0.011, New P: 1.342
-Original Grad: 0.015, -lr * Pred Grad:  0.008, New P: 1.724
iter 22 loss: 0.041
Actual params: [1.3423, 1.7236]
-Original Grad: -0.076, -lr * Pred Grad:  -0.000, New P: 1.342
-Original Grad: 0.162, -lr * Pred Grad:  0.031, New P: 1.755
iter 23 loss: 0.040
Actual params: [1.3421, 1.7547]
-Original Grad: 0.036, -lr * Pred Grad:  0.006, New P: 1.348
-Original Grad: 0.038, -lr * Pred Grad:  0.010, New P: 1.765
iter 24 loss: 0.040
Actual params: [1.3478, 1.7645]
-Original Grad: 0.135, -lr * Pred Grad:  0.008, New P: 1.356
-Original Grad: -0.113, -lr * Pred Grad:  -0.015, New P: 1.749
iter 25 loss: 0.039
Actual params: [1.356 , 1.7492]
-Original Grad: 0.010, -lr * Pred Grad:  0.008, New P: 1.364
-Original Grad: 0.106, -lr * Pred Grad:  0.022, New P: 1.771
iter 26 loss: 0.039
Actual params: [1.3641, 1.771 ]
-Original Grad: 0.122, -lr * Pred Grad:  0.015, New P: 1.379
-Original Grad: 0.012, -lr * Pred Grad:  0.011, New P: 1.782
iter 27 loss: 0.038
Actual params: [1.3794, 1.782 ]
-Original Grad: 0.062, -lr * Pred Grad:  0.011, New P: 1.390
-Original Grad: 0.043, -lr * Pred Grad:  0.014, New P: 1.796
iter 28 loss: 0.037
Actual params: [1.3902, 1.7957]
-Original Grad: -0.018, -lr * Pred Grad:  0.003, New P: 1.393
-Original Grad: 0.064, -lr * Pred Grad:  0.012, New P: 1.808
iter 29 loss: 0.036
Actual params: [1.3934, 1.8076]
-Original Grad: 0.137, -lr * Pred Grad:  0.010, New P: 1.403
-Original Grad: -0.081, -lr * Pred Grad:  -0.005, New P: 1.803
iter 30 loss: 0.036
Actual params: [1.4031, 1.8026]
-Original Grad: 0.060, -lr * Pred Grad:  0.010, New P: 1.413
-Original Grad: 0.026, -lr * Pred Grad:  0.011, New P: 1.813
Target params: [1.3344, 1.5708]
iter 0 loss: 0.018
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.004, -lr * Pred Grad:  0.046, New P: -0.426
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.019
iter 1 loss: 0.017
Actual params: [-0.4263, -0.0192]
-Original Grad: 0.004, -lr * Pred Grad:  0.052, New P: -0.375
-Original Grad: -0.002, -lr * Pred Grad:  -0.028, New P: -0.048
iter 2 loss: 0.016
Actual params: [-0.3745, -0.0476]
-Original Grad: 0.005, -lr * Pred Grad:  0.061, New P: -0.313
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -0.072
iter 3 loss: 0.015
Actual params: [-0.3131, -0.0716]
-Original Grad: 0.005, -lr * Pred Grad:  0.065, New P: -0.248
-Original Grad: -0.002, -lr * Pred Grad:  -0.026, New P: -0.097
iter 4 loss: 0.013
Actual params: [-0.2485, -0.0973]
-Original Grad: 0.005, -lr * Pred Grad:  0.066, New P: -0.182
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -0.121
iter 5 loss: 0.012
Actual params: [-0.1823, -0.1214]
-Original Grad: 0.005, -lr * Pred Grad:  0.074, New P: -0.108
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -0.141
iter 6 loss: 0.011
Actual params: [-0.1082, -0.1411]
-Original Grad: 0.005, -lr * Pred Grad:  0.080, New P: -0.028
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.161
iter 7 loss: 0.011
Actual params: [-0.0281, -0.1606]
-Original Grad: 0.008, -lr * Pred Grad:  0.126, New P: 0.098
-Original Grad: -0.002, -lr * Pred Grad:  -0.019, New P: -0.180
iter 8 loss: 0.016
Actual params: [ 0.0981, -0.18  ]
-Original Grad: -0.300, -lr * Pred Grad:  -0.063, New P: 0.035
-Original Grad: 0.080, -lr * Pred Grad:  0.026, New P: -0.154
iter 9 loss: 0.013
Actual params: [ 0.0351, -0.1542]
-Original Grad: -0.044, -lr * Pred Grad:  0.003, New P: 0.038
-Original Grad: 0.014, -lr * Pred Grad:  0.049, New P: -0.106
iter 10 loss: 0.012
Actual params: [ 0.0384, -0.1055]
-Original Grad: 0.007, -lr * Pred Grad:  0.002, New P: 0.041
-Original Grad: -0.002, -lr * Pred Grad:  0.002, New P: -0.103
iter 11 loss: 0.012
Actual params: [ 0.0408, -0.1035]
-Original Grad: -0.044, -lr * Pred Grad:  -0.002, New P: 0.039
-Original Grad: 0.013, -lr * Pred Grad:  0.030, New P: -0.073
iter 12 loss: 0.012
Actual params: [ 0.0386, -0.0734]
-Original Grad: 0.004, -lr * Pred Grad:  0.000, New P: 0.039
-Original Grad: -0.001, -lr * Pred Grad:  -0.004, New P: -0.077
iter 13 loss: 0.012
Actual params: [ 0.0386, -0.0773]
-Original Grad: -0.019, -lr * Pred Grad:  -0.000, New P: 0.039
-Original Grad: 0.006, -lr * Pred Grad:  0.018, New P: -0.060
iter 14 loss: 0.012
Actual params: [ 0.0386, -0.0595]
-Original Grad: -0.013, -lr * Pred Grad:  -0.004, New P: 0.035
-Original Grad: 0.003, -lr * Pred Grad:  -0.000, New P: -0.060
iter 15 loss: 0.012
Actual params: [ 0.0351, -0.06  ]
-Original Grad: -0.015, -lr * Pred Grad:  -0.003, New P: 0.032
-Original Grad: 0.004, -lr * Pred Grad:  0.004, New P: -0.056
iter 16 loss: 0.012
Actual params: [ 0.032 , -0.0557]
-Original Grad: -0.016, -lr * Pred Grad:  -0.004, New P: 0.028
-Original Grad: 0.004, -lr * Pred Grad:  0.002, New P: -0.054
iter 17 loss: 0.012
Actual params: [ 0.0281, -0.0535]
-Original Grad: -0.014, -lr * Pred Grad:  -0.005, New P: 0.023
-Original Grad: 0.004, -lr * Pred Grad:  -0.003, New P: -0.056
iter 18 loss: 0.012
Actual params: [ 0.0233, -0.0562]
-Original Grad: 0.007, -lr * Pred Grad:  -0.005, New P: 0.018
-Original Grad: -0.002, -lr * Pred Grad:  -0.027, New P: -0.083
iter 19 loss: 0.012
Actual params: [ 0.018 , -0.0834]
-Original Grad: -0.010, -lr * Pred Grad:  -0.009, New P: 0.009
-Original Grad: 0.002, -lr * Pred Grad:  -0.022, New P: -0.105
iter 20 loss: 0.012
Actual params: [ 0.0087, -0.1054]
-Original Grad: 0.006, -lr * Pred Grad:  -0.004, New P: 0.005
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: -0.127
iter 21 loss: 0.012
Actual params: [ 0.0049, -0.1269]
-Original Grad: -0.013, -lr * Pred Grad:  0.021, New P: 0.026
-Original Grad: 0.005, -lr * Pred Grad:  0.091, New P: -0.036
iter 22 loss: 0.012
Actual params: [ 0.026, -0.036]
-Original Grad: 0.009, -lr * Pred Grad:  -0.017, New P: 0.009
-Original Grad: -0.003, -lr * Pred Grad:  -0.074, New P: -0.110
iter 23 loss: 0.012
Actual params: [ 0.0086, -0.1098]
-Original Grad: -0.034, -lr * Pred Grad:  0.007, New P: 0.015
-Original Grad: 0.010, -lr * Pred Grad:  0.065, New P: -0.045
iter 24 loss: 0.012
Actual params: [ 0.0152, -0.0448]
-Original Grad: 0.006, -lr * Pred Grad:  -0.017, New P: -0.002
-Original Grad: -0.002, -lr * Pred Grad:  -0.067, New P: -0.111
iter 25 loss: 0.012
Actual params: [-0.0017, -0.1114]
-Original Grad: 0.008, -lr * Pred Grad:  0.001, New P: -0.000
-Original Grad: -0.002, -lr * Pred Grad:  -0.007, New P: -0.119
iter 26 loss: 0.012
Actual params: [-0.0003, -0.1188]
-Original Grad: 0.008, -lr * Pred Grad:  -0.006, New P: -0.006
-Original Grad: -0.003, -lr * Pred Grad:  -0.034, New P: -0.153
iter 27 loss: 0.012
Actual params: [-0.0061, -0.1527]
-Original Grad: -0.029, -lr * Pred Grad:  0.038, New P: 0.032
-Original Grad: 0.010, -lr * Pred Grad:  0.173, New P: 0.020
iter 28 loss: 0.012
Actual params: [0.0321, 0.0201]
-Original Grad: 0.008, -lr * Pred Grad:  -0.037, New P: -0.005
-Original Grad: -0.003, -lr * Pred Grad:  -0.139, New P: -0.119
iter 29 loss: 0.012
Actual params: [-0.0053, -0.1191]
-Original Grad: -0.009, -lr * Pred Grad:  0.036, New P: 0.030
-Original Grad: 0.004, -lr * Pred Grad:  0.134, New P: 0.015
iter 30 loss: 0.012
Actual params: [0.0304, 0.0146]
-Original Grad: 0.007, -lr * Pred Grad:  -0.042, New P: -0.011
-Original Grad: -0.003, -lr * Pred Grad:  -0.151, New P: -0.137
Target params: [1.3344, 1.5708]
iter 0 loss: 0.612
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.039, -lr * Pred Grad:  0.365, New P: -0.107
-Original Grad: 0.026, -lr * Pred Grad:  0.239, New P: 0.242
iter 1 loss: 0.582
Actual params: [-0.107 ,  0.2421]
-Original Grad: 0.089, -lr * Pred Grad:  0.543, New P: 0.436
-Original Grad: 0.045, -lr * Pred Grad:  0.262, New P: 0.504
iter 2 loss: 0.439
Actual params: [0.4355, 0.5042]
-Original Grad: 0.182, -lr * Pred Grad:  0.500, New P: 0.936
-Original Grad: 0.023, -lr * Pred Grad:  -0.107, New P: 0.397
iter 3 loss: 0.304
Actual params: [0.9358, 0.3974]
-Original Grad: 0.202, -lr * Pred Grad:  0.162, New P: 1.098
-Original Grad: 0.160, -lr * Pred Grad:  0.553, New P: 0.951
iter 4 loss: 0.194
Actual params: [1.098 , 0.9506]
-Original Grad: -0.169, -lr * Pred Grad:  -0.047, New P: 1.051
-Original Grad: 0.257, -lr * Pred Grad:  0.304, New P: 1.254
iter 5 loss: 0.168
Actual params: [1.0506, 1.2544]
-Original Grad: 0.121, -lr * Pred Grad:  0.148, New P: 1.199
-Original Grad: 0.117, -lr * Pred Grad:  0.177, New P: 1.432
iter 6 loss: 0.144
Actual params: [1.1987, 1.4315]
-Original Grad: 0.037, -lr * Pred Grad:  0.025, New P: 1.224
-Original Grad: 0.032, -lr * Pred Grad:  0.044, New P: 1.475
iter 7 loss: 0.142
Actual params: [1.2236, 1.4753]
-Original Grad: -0.035, -lr * Pred Grad:  -0.010, New P: 1.213
-Original Grad: 0.038, -lr * Pred Grad:  0.040, New P: 1.515
iter 8 loss: 0.144
Actual params: [1.2133, 1.515 ]
-Original Grad: -0.057, -lr * Pred Grad:  -0.006, New P: 1.207
-Original Grad: 0.071, -lr * Pred Grad:  0.079, New P: 1.594
iter 9 loss: 0.147
Actual params: [1.207 , 1.5941]
-Original Grad: -0.069, -lr * Pred Grad:  -0.025, New P: 1.182
-Original Grad: -0.034, -lr * Pred Grad:  -0.055, New P: 1.539
iter 10 loss: 0.147
Actual params: [1.1822, 1.5393]
-Original Grad: 0.218, -lr * Pred Grad:  0.046, New P: 1.228
-Original Grad: -0.051, -lr * Pred Grad:  -0.030, New P: 1.509
iter 11 loss: 0.143
Actual params: [1.2277, 1.5089]
-Original Grad: 0.136, -lr * Pred Grad:  0.039, New P: 1.267
-Original Grad: 0.045, -lr * Pred Grad:  0.091, New P: 1.600
iter 12 loss: 0.145
Actual params: [1.2667, 1.6003]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: 1.255
-Original Grad: -0.064, -lr * Pred Grad:  -0.094, New P: 1.506
iter 13 loss: 0.141
Actual params: [1.2546, 1.5059]
-Original Grad: -0.012, -lr * Pred Grad:  -0.003, New P: 1.251
-Original Grad: -0.006, -lr * Pred Grad:  -0.011, New P: 1.495
iter 14 loss: 0.141
Actual params: [1.2511, 1.495 ]
-Original Grad: 0.114, -lr * Pred Grad:  0.022, New P: 1.273
-Original Grad: -0.015, -lr * Pred Grad:  -0.003, New P: 1.492
iter 15 loss: 0.140
Actual params: [1.2726, 1.4919]
-Original Grad: -0.022, -lr * Pred Grad:  -0.011, New P: 1.262
-Original Grad: -0.030, -lr * Pred Grad:  -0.051, New P: 1.441
iter 16 loss: 0.140
Actual params: [1.2619, 1.441 ]
-Original Grad: -0.161, -lr * Pred Grad:  -0.022, New P: 1.240
-Original Grad: 0.056, -lr * Pred Grad:  0.049, New P: 1.490
iter 17 loss: 0.141
Actual params: [1.2397, 1.4898]
-Original Grad: 0.110, -lr * Pred Grad:  0.036, New P: 1.276
-Original Grad: 0.046, -lr * Pred Grad:  0.101, New P: 1.591
iter 18 loss: 0.144
Actual params: [1.2758, 1.5912]
-Original Grad: 0.055, -lr * Pred Grad:  0.011, New P: 1.287
-Original Grad: -0.002, -lr * Pred Grad:  0.011, New P: 1.602
iter 19 loss: 0.145
Actual params: [1.2866, 1.6021]
-Original Grad: -0.136, -lr * Pred Grad:  -0.031, New P: 1.256
-Original Grad: -0.043, -lr * Pred Grad:  -0.078, New P: 1.524
iter 20 loss: 0.141
Actual params: [1.2558, 1.524 ]
-Original Grad: 0.005, -lr * Pred Grad:  0.003, New P: 1.259
-Original Grad: 0.016, -lr * Pred Grad:  0.022, New P: 1.546
iter 21 loss: 0.142
Actual params: [1.2589, 1.5461]
-Original Grad: 0.072, -lr * Pred Grad:  0.000, New P: 1.259
-Original Grad: -0.080, -lr * Pred Grad:  -0.097, New P: 1.449
iter 22 loss: 0.140
Actual params: [1.259 , 1.4491]
-Original Grad: 0.071, -lr * Pred Grad:  0.008, New P: 1.267
-Original Grad: -0.011, -lr * Pred Grad:  -0.007, New P: 1.442
iter 23 loss: 0.140
Actual params: [1.2671, 1.4421]
-Original Grad: 0.011, -lr * Pred Grad:  0.003, New P: 1.270
-Original Grad: 0.012, -lr * Pred Grad:  0.020, New P: 1.462
iter 24 loss: 0.139
Actual params: [1.2702, 1.4622]
-Original Grad: 0.011, -lr * Pred Grad:  0.004, New P: 1.274
-Original Grad: 0.017, -lr * Pred Grad:  0.030, New P: 1.492
iter 25 loss: 0.140
Actual params: [1.274 , 1.4918]
-Original Grad: -0.045, -lr * Pred Grad:  -0.010, New P: 1.264
-Original Grad: -0.027, -lr * Pred Grad:  -0.052, New P: 1.439
iter 26 loss: 0.140
Actual params: [1.2637, 1.4394]
-Original Grad: -0.181, -lr * Pred Grad:  -0.022, New P: 1.241
-Original Grad: 0.030, -lr * Pred Grad:  0.024, New P: 1.464
iter 27 loss: 0.141
Actual params: [1.2414, 1.4635]
-Original Grad: 0.028, -lr * Pred Grad:  0.008, New P: 1.250
-Original Grad: 0.029, -lr * Pred Grad:  0.059, New P: 1.522
iter 28 loss: 0.142
Actual params: [1.2498, 1.5223]
-Original Grad: 0.097, -lr * Pred Grad:  0.021, New P: 1.271
-Original Grad: 0.038, -lr * Pred Grad:  0.088, New P: 1.610
iter 29 loss: 0.146
Actual params: [1.2711, 1.6104]
-Original Grad: -0.070, -lr * Pred Grad:  -0.014, New P: 1.257
-Original Grad: -0.024, -lr * Pred Grad:  -0.055, New P: 1.555
iter 30 loss: 0.143
Actual params: [1.2568, 1.5549]
-Original Grad: -0.088, -lr * Pred Grad:  -0.012, New P: 1.244
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: 1.542
Target params: [1.3344, 1.5708]
iter 0 loss: 0.082
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.043, -lr * Pred Grad:  -0.267, New P: -0.739
-Original Grad: 0.033, -lr * Pred Grad:  0.206, New P: 0.210
iter 1 loss: 0.072
Actual params: [-0.7394,  0.2099]
-Original Grad: -0.004, -lr * Pred Grad:  -0.021, New P: -0.761
-Original Grad: 0.004, -lr * Pred Grad:  0.028, New P: 0.238
iter 2 loss: 0.071
Actual params: [-0.7609,  0.2377]
-Original Grad: -0.001, -lr * Pred Grad:  -0.004, New P: -0.765
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.241
iter 3 loss: 0.071
Actual params: [-0.7647,  0.2406]
-Original Grad: -0.003, -lr * Pred Grad:  -0.019, New P: -0.783
-Original Grad: 0.003, -lr * Pred Grad:  0.027, New P: 0.267
iter 4 loss: 0.071
Actual params: [-0.7833,  0.2672]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.794
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.279
iter 5 loss: 0.071
Actual params: [-0.7937,  0.2789]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.798
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.282
iter 6 loss: 0.071
Actual params: [-0.7979,  0.2822]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.803
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.286
iter 7 loss: 0.071
Actual params: [-0.803 ,  0.2864]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.809
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.291
iter 8 loss: 0.071
Actual params: [-0.8089,  0.2909]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.820
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: 0.301
iter 9 loss: 0.071
Actual params: [-0.8202,  0.301 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.826
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.306
iter 10 loss: 0.070
Actual params: [-0.8261,  0.3057]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.833
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.311
iter 11 loss: 0.070
Actual params: [-0.833 ,  0.3112]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.840
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.317
iter 12 loss: 0.070
Actual params: [-0.8405,  0.3173]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.849
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.324
iter 13 loss: 0.070
Actual params: [-0.849 ,  0.3243]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.858
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.331
iter 14 loss: 0.070
Actual params: [-0.8576,  0.3313]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.866
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.338
iter 15 loss: 0.070
Actual params: [-0.8659,  0.3383]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -0.887
-Original Grad: 0.001, -lr * Pred Grad:  0.023, New P: 0.361
iter 16 loss: 0.070
Actual params: [-0.8869,  0.3614]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.895
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.368
iter 17 loss: 0.070
Actual params: [-0.8951,  0.3681]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.908
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.379
iter 18 loss: 0.070
Actual params: [-0.9075,  0.3787]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -0.927
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.397
iter 19 loss: 0.070
Actual params: [-0.9267,  0.3973]
-Original Grad: -0.001, -lr * Pred Grad:  -0.024, New P: -0.951
-Original Grad: 0.001, -lr * Pred Grad:  0.027, New P: 0.424
iter 20 loss: 0.070
Actual params: [-0.9508,  0.4238]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -0.972
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.447
iter 21 loss: 0.070
Actual params: [-0.9724,  0.4469]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.985
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.458
iter 22 loss: 0.070
Actual params: [-0.9846,  0.4581]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.997
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.469
iter 23 loss: 0.070
Actual params: [-0.9972,  0.4694]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.010
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.481
iter 24 loss: 0.070
Actual params: [-1.0099,  0.4807]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.022
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.492
iter 25 loss: 0.070
Actual params: [-1.0225,  0.4916]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.038
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.507
iter 26 loss: 0.070
Actual params: [-1.0384,  0.5073]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.058
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.527
iter 27 loss: 0.070
Actual params: [-1.0577,  0.5274]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.075
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.547
iter 28 loss: 0.070
Actual params: [-1.0755,  0.5467]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.088
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.560
iter 29 loss: 0.070
Actual params: [-1.0883,  0.5596]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.100
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.570
iter 30 loss: 0.070
Actual params: [-1.0996,  0.5701]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.114
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.585
Target params: [1.3344, 1.5708]
iter 0 loss: 0.049
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.021, -lr * Pred Grad:  -0.220, New P: -0.693
-Original Grad: 0.012, -lr * Pred Grad:  0.123, New P: 0.126
iter 1 loss: 0.047
Actual params: [-0.6926,  0.1262]
-Original Grad: -0.002, -lr * Pred Grad:  -0.019, New P: -0.712
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.138
iter 2 loss: 0.047
Actual params: [-0.7117,  0.1383]
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -0.736
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.154
iter 3 loss: 0.047
Actual params: [-0.7358,  0.1536]
-Original Grad: -0.001, -lr * Pred Grad:  -0.019, New P: -0.755
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.166
iter 4 loss: 0.046
Actual params: [-0.7551,  0.1657]
-Original Grad: -0.001, -lr * Pred Grad:  -0.022, New P: -0.777
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.179
iter 5 loss: 0.046
Actual params: [-0.7766,  0.1794]
-Original Grad: -0.001, -lr * Pred Grad:  -0.024, New P: -0.801
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.195
iter 6 loss: 0.046
Actual params: [-0.801 ,  0.1946]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.812
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.202
iter 7 loss: 0.046
Actual params: [-0.8122,  0.2018]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.828
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.211
iter 8 loss: 0.046
Actual params: [-0.8275,  0.2114]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.839
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.219
iter 9 loss: 0.046
Actual params: [-0.8388,  0.2189]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.853
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.228
iter 10 loss: 0.046
Actual params: [-0.853,  0.228]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.864
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.235
iter 11 loss: 0.046
Actual params: [-0.8641,  0.2354]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.875
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.243
iter 12 loss: 0.046
Actual params: [-0.8751,  0.2426]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.886
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.250
iter 13 loss: 0.046
Actual params: [-0.8857,  0.2497]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.896
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.257
iter 14 loss: 0.046
Actual params: [-0.8961,  0.2567]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.910
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.266
iter 15 loss: 0.046
Actual params: [-0.9102,  0.2659]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.921
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.273
iter 16 loss: 0.046
Actual params: [-0.921 ,  0.2732]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.932
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.281
iter 17 loss: 0.046
Actual params: [-0.9321,  0.2809]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.943
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.288
iter 18 loss: 0.046
Actual params: [-0.9431,  0.2884]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.951
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.294
iter 19 loss: 0.046
Actual params: [-0.9509,  0.2939]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.961
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.301
iter 20 loss: 0.046
Actual params: [-0.9608,  0.3008]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.972
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.309
iter 21 loss: 0.046
Actual params: [-0.9722,  0.3088]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.986
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.318
iter 22 loss: 0.046
Actual params: [-0.986 ,  0.3178]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.998
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.326
iter 23 loss: 0.046
Actual params: [-0.9979,  0.3258]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.007
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.332
iter 24 loss: 0.046
Actual params: [-1.0072,  0.3324]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.016
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.339
iter 25 loss: 0.046
Actual params: [-1.0165,  0.339 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.026
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.346
iter 26 loss: 0.046
Actual params: [-1.0261,  0.3457]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.036
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.353
iter 27 loss: 0.046
Actual params: [-1.0355,  0.3525]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.047
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.360
iter 28 loss: 0.046
Actual params: [-1.0467,  0.3601]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.055
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.366
iter 29 loss: 0.046
Actual params: [-1.0553,  0.3665]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.069
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.375
iter 30 loss: 0.046
Actual params: [-1.0692,  0.3752]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.080
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.382
Target params: [1.3344, 1.5708]
iter 0 loss: 0.194
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.038, New P: -0.510
-Original Grad: 0.003, -lr * Pred Grad:  0.031, New P: 0.035
iter 1 loss: 0.192
Actual params: [-0.5101,  0.0348]
-Original Grad: -0.003, -lr * Pred Grad:  -0.039, New P: -0.549
-Original Grad: 0.003, -lr * Pred Grad:  0.033, New P: 0.068
iter 2 loss: 0.189
Actual params: [-0.5488,  0.0681]
-Original Grad: -0.004, -lr * Pred Grad:  -0.060, New P: -0.609
-Original Grad: 0.004, -lr * Pred Grad:  0.052, New P: 0.120
iter 3 loss: 0.186
Actual params: [-0.6093,  0.1202]
-Original Grad: -0.002, -lr * Pred Grad:  -0.038, New P: -0.647
-Original Grad: 0.002, -lr * Pred Grad:  0.035, New P: 0.155
iter 4 loss: 0.184
Actual params: [-0.647 ,  0.1554]
-Original Grad: -0.002, -lr * Pred Grad:  -0.033, New P: -0.680
-Original Grad: 0.002, -lr * Pred Grad:  0.031, New P: 0.186
iter 5 loss: 0.183
Actual params: [-0.6802,  0.1861]
-Original Grad: -0.002, -lr * Pred Grad:  -0.039, New P: -0.719
-Original Grad: 0.002, -lr * Pred Grad:  0.038, New P: 0.224
iter 6 loss: 0.182
Actual params: [-0.7188,  0.2241]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -0.740
-Original Grad: 0.001, -lr * Pred Grad:  0.023, New P: 0.247
iter 7 loss: 0.181
Actual params: [-0.7402,  0.2468]
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -0.760
-Original Grad: 0.001, -lr * Pred Grad:  0.021, New P: 0.268
iter 8 loss: 0.181
Actual params: [-0.7598,  0.2682]
-Original Grad: -0.001, -lr * Pred Grad:  -0.037, New P: -0.796
-Original Grad: 0.001, -lr * Pred Grad:  0.038, New P: 0.306
iter 9 loss: 0.180
Actual params: [-0.7965,  0.3058]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.809
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.318
iter 10 loss: 0.180
Actual params: [-0.8087,  0.3182]
-Original Grad: -0.001, -lr * Pred Grad:  -0.028, New P: -0.836
-Original Grad: 0.001, -lr * Pred Grad:  0.031, New P: 0.349
iter 11 loss: 0.180
Actual params: [-0.8363,  0.3493]
-Original Grad: -0.001, -lr * Pred Grad:  -0.038, New P: -0.874
-Original Grad: 0.001, -lr * Pred Grad:  0.042, New P: 0.391
iter 12 loss: 0.179
Actual params: [-0.8741,  0.391 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.886
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.401
iter 13 loss: 0.179
Actual params: [-0.8862,  0.4006]
-Original Grad: -0.001, -lr * Pred Grad:  -0.027, New P: -0.913
-Original Grad: 0.001, -lr * Pred Grad:  0.029, New P: 0.430
iter 14 loss: 0.179
Actual params: [-0.9132,  0.4297]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.928
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.443
iter 15 loss: 0.179
Actual params: [-0.9281,  0.4429]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.944
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.458
iter 16 loss: 0.179
Actual params: [-0.9444,  0.4585]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -0.967
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.483
iter 17 loss: 0.179
Actual params: [-0.9669,  0.4829]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -0.995
-Original Grad: 0.000, -lr * Pred Grad:  0.033, New P: 0.516
iter 18 loss: 0.179
Actual params: [-0.9951,  0.5156]
-Original Grad: -0.001, -lr * Pred Grad:  -0.044, New P: -1.039
-Original Grad: 0.001, -lr * Pred Grad:  0.053, New P: 0.569
iter 19 loss: 0.178
Actual params: [-1.0387,  0.5686]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.055
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.582
iter 20 loss: 0.178
Actual params: [-1.0553,  0.5821]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.083
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: 0.612
iter 21 loss: 0.178
Actual params: [-1.0828,  0.6121]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.105
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.636
iter 22 loss: 0.178
Actual params: [-1.1055,  0.6361]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.130
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.657
iter 23 loss: 0.178
Actual params: [-1.1296,  0.6571]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.147
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.672
iter 24 loss: 0.178
Actual params: [-1.1467,  0.6721]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.171
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.697
iter 25 loss: 0.178
Actual params: [-1.1713,  0.6972]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.192
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.716
iter 26 loss: 0.178
Actual params: [-1.1922,  0.7164]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.220
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 0.742
iter 27 loss: 0.178
Actual params: [-1.22  ,  0.7421]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.237
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.758
iter 28 loss: 0.178
Actual params: [-1.2368,  0.7576]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.258
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.777
iter 29 loss: 0.178
Actual params: [-1.2582,  0.7774]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.276
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.795
iter 30 loss: 0.178
Actual params: [-1.2763,  0.7948]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.302
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.819
Target params: [1.3344, 1.5708]
iter 0 loss: 0.029
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.030, -lr * Pred Grad:  -0.163, New P: -0.635
-Original Grad: 0.006, -lr * Pred Grad:  0.031, New P: 0.034
iter 1 loss: 0.023
Actual params: [-0.6352,  0.0345]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.636
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.036
iter 2 loss: 0.023
Actual params: [-0.6359,  0.036 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.003, New P: -0.639
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.038
iter 3 loss: 0.023
Actual params: [-0.6392,  0.038 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.639
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.039
iter 4 loss: 0.023
Actual params: [-0.6394,  0.0389]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.640
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.041
iter 5 loss: 0.023
Actual params: [-0.64  ,  0.0406]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.641
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.042
iter 6 loss: 0.023
Actual params: [-0.6406,  0.0424]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.641
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.044
iter 7 loss: 0.023
Actual params: [-0.641 ,  0.0442]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.642
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.046
iter 8 loss: 0.023
Actual params: [-0.6416,  0.0461]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.643
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.049
iter 9 loss: 0.023
Actual params: [-0.6426,  0.0486]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.646
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.054
iter 10 loss: 0.023
Actual params: [-0.6461,  0.0544]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.647
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.057
iter 11 loss: 0.023
Actual params: [-0.6466,  0.0569]
-Original Grad: -0.001, -lr * Pred Grad:  -0.009, New P: -0.655
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.063
iter 12 loss: 0.022
Actual params: [-0.6551,  0.0635]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.656
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.066
iter 13 loss: 0.022
Actual params: [-0.6558,  0.0664]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.666
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.074
iter 14 loss: 0.022
Actual params: [-0.6657,  0.0736]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.667
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.077
iter 15 loss: 0.022
Actual params: [-0.6666,  0.0767]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.679
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.086
iter 16 loss: 0.022
Actual params: [-0.6786,  0.0863]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.680
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.092
iter 17 loss: 0.022
Actual params: [-0.6799,  0.0924]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.681
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.097
iter 18 loss: 0.022
Actual params: [-0.6811,  0.0974]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.684
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.105
iter 19 loss: 0.022
Actual params: [-0.6842,  0.1052]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.687
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.116
iter 20 loss: 0.021
Actual params: [-0.6867,  0.1161]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.688
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.124
iter 21 loss: 0.021
Actual params: [-0.6881,  0.124 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.692
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.135
iter 22 loss: 0.021
Actual params: [-0.6918,  0.1354]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.693
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.144
iter 23 loss: 0.021
Actual params: [-0.6928,  0.1443]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -0.719
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: 0.176
iter 24 loss: 0.021
Actual params: [-0.7188,  0.176 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -0.745
-Original Grad: 0.000, -lr * Pred Grad:  0.039, New P: 0.215
iter 25 loss: 0.020
Actual params: [-0.7455,  0.215 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.749
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.236
iter 26 loss: 0.020
Actual params: [-0.7494,  0.2358]
-Original Grad: -0.001, -lr * Pred Grad:  -0.056, New P: -0.805
-Original Grad: 0.001, -lr * Pred Grad:  0.095, New P: 0.331
iter 27 loss: 0.018
Actual params: [-0.8051,  0.3311]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -0.804
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.342
iter 28 loss: 0.018
Actual params: [-0.804 ,  0.3418]
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -0.803
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.355
iter 29 loss: 0.018
Actual params: [-0.8031,  0.355 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.803
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.368
iter 30 loss: 0.018
Actual params: [-0.8032,  0.3676]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.802
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.377
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.462
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.006
iter 1 loss: 0.363
Actual params: [-0.4621,  0.0058]
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.452
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.008
iter 2 loss: 0.363
Actual params: [-0.4519,  0.0081]
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: -0.440
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.011
iter 3 loss: 0.363
Actual params: [-0.4403,  0.0108]
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: -0.426
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.014
iter 4 loss: 0.363
Actual params: [-0.4256,  0.0141]
-Original Grad: 0.001, -lr * Pred Grad:  0.019, New P: -0.407
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.018
iter 5 loss: 0.363
Actual params: [-0.4071,  0.0179]
-Original Grad: 0.001, -lr * Pred Grad:  0.025, New P: -0.382
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.023
iter 6 loss: 0.363
Actual params: [-0.3822,  0.0231]
-Original Grad: 0.001, -lr * Pred Grad:  0.029, New P: -0.353
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.029
iter 7 loss: 0.363
Actual params: [-0.3528,  0.029 ]
-Original Grad: 0.001, -lr * Pred Grad:  0.031, New P: -0.322
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.035
iter 8 loss: 0.363
Actual params: [-0.3218,  0.0353]
-Original Grad: 0.002, -lr * Pred Grad:  0.056, New P: -0.266
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.046
iter 9 loss: 0.363
Actual params: [-0.2661,  0.0464]
-Original Grad: 0.004, -lr * Pred Grad:  0.107, New P: -0.159
-Original Grad: 0.001, -lr * Pred Grad:  0.022, New P: 0.068
iter 10 loss: 0.362
Actual params: [-0.1595,  0.068 ]
-Original Grad: 0.006, -lr * Pred Grad:  0.196, New P: 0.037
-Original Grad: 0.001, -lr * Pred Grad:  0.040, New P: 0.108
iter 11 loss: 0.357
Actual params: [0.037 , 0.1083]
-Original Grad: 0.029, -lr * Pred Grad:  0.857, New P: 0.894
-Original Grad: 0.006, -lr * Pred Grad:  0.191, New P: 0.300
iter 12 loss: 0.249
Actual params: [0.8936, 0.2995]
-Original Grad: 0.031, -lr * Pred Grad:  0.006, New P: 0.899
-Original Grad: 0.516, -lr * Pred Grad:  0.420, New P: 0.720
iter 13 loss: 0.094
Actual params: [0.8992, 0.7196]
-Original Grad: 0.040, -lr * Pred Grad:  0.299, New P: 1.198
-Original Grad: 0.379, -lr * Pred Grad:  0.140, New P: 0.859
iter 14 loss: 0.028
Actual params: [1.198 , 0.8593]
-Original Grad: 0.256, -lr * Pred Grad:  0.087, New P: 1.284
-Original Grad: 0.435, -lr * Pred Grad:  0.028, New P: 0.887
iter 15 loss: 0.018
Actual params: [1.2845, 0.8875]
-Original Grad: 0.110, -lr * Pred Grad:  -0.022, New P: 1.262
-Original Grad: 0.276, -lr * Pred Grad:  0.042, New P: 0.929
iter 16 loss: 0.015
Actual params: [1.262 , 0.9294]
-Original Grad: 0.070, -lr * Pred Grad:  0.013, New P: 1.275
-Original Grad: 0.061, -lr * Pred Grad:  0.001, New P: 0.931
iter 17 loss: 0.014
Actual params: [1.2748, 0.9306]
-Original Grad: 0.069, -lr * Pred Grad:  0.004, New P: 1.279
-Original Grad: 0.122, -lr * Pred Grad:  0.012, New P: 0.943
iter 18 loss: 0.013
Actual params: [1.2789, 0.9427]
-Original Grad: 0.266, -lr * Pred Grad:  0.030, New P: 1.309
-Original Grad: 0.142, -lr * Pred Grad:  0.001, New P: 0.944
iter 19 loss: 0.012
Actual params: [1.3089, 0.944 ]
-Original Grad: 0.196, -lr * Pred Grad:  0.015, New P: 1.324
-Original Grad: 0.116, -lr * Pred Grad:  0.005, New P: 0.949
iter 20 loss: 0.011
Actual params: [1.3242, 0.9488]
-Original Grad: 0.122, -lr * Pred Grad:  0.007, New P: 1.331
-Original Grad: 0.085, -lr * Pred Grad:  0.006, New P: 0.955
iter 21 loss: 0.011
Actual params: [1.3308, 0.9551]
-Original Grad: 0.021, -lr * Pred Grad:  -0.003, New P: 1.328
-Original Grad: 0.091, -lr * Pred Grad:  0.013, New P: 0.968
iter 22 loss: 0.011
Actual params: [1.3282, 0.9681]
-Original Grad: -0.012, -lr * Pred Grad:  -0.004, New P: 1.324
-Original Grad: 0.086, -lr * Pred Grad:  0.013, New P: 0.982
iter 23 loss: 0.010
Actual params: [1.3238, 0.9815]
-Original Grad: 0.023, -lr * Pred Grad:  -0.000, New P: 1.324
-Original Grad: 0.064, -lr * Pred Grad:  0.008, New P: 0.990
iter 24 loss: 0.010
Actual params: [1.3236, 0.99  ]
-Original Grad: 0.035, -lr * Pred Grad:  0.000, New P: 1.324
-Original Grad: 0.092, -lr * Pred Grad:  0.012, New P: 1.002
iter 25 loss: 0.010
Actual params: [1.3236, 1.002 ]
-Original Grad: 0.116, -lr * Pred Grad:  0.006, New P: 1.329
-Original Grad: 0.065, -lr * Pred Grad:  0.006, New P: 1.008
iter 26 loss: 0.010
Actual params: [1.3295, 1.0083]
-Original Grad: 0.023, -lr * Pred Grad:  -0.000, New P: 1.329
-Original Grad: 0.068, -lr * Pred Grad:  0.009, New P: 1.018
iter 27 loss: 0.010
Actual params: [1.3292, 1.0177]
-Original Grad: 0.048, -lr * Pred Grad:  0.004, New P: 1.333
-Original Grad: -0.027, -lr * Pred Grad:  -0.005, New P: 1.013
iter 28 loss: 0.010
Actual params: [1.3328, 1.0126]
-Original Grad: 0.067, -lr * Pred Grad:  0.004, New P: 1.337
-Original Grad: -0.009, -lr * Pred Grad:  -0.003, New P: 1.010
iter 29 loss: 0.010
Actual params: [1.3373, 1.01  ]
-Original Grad: 0.039, -lr * Pred Grad:  0.001, New P: 1.339
-Original Grad: 0.051, -lr * Pred Grad:  0.008, New P: 1.018
iter 30 loss: 0.010
Actual params: [1.3387, 1.0179]
-Original Grad: -0.041, -lr * Pred Grad:  -0.003, New P: 1.335
-Original Grad: 0.036, -lr * Pred Grad:  0.007, New P: 1.025
Target params: [1.3344, 1.5708]
iter 0 loss: 0.829
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.477
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.003
iter 1 loss: 0.829
Actual params: [-0.4771,  0.0032]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.472
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.004
iter 2 loss: 0.829
Actual params: [-0.4718,  0.0045]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.471
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.005
iter 3 loss: 0.829
Actual params: [-0.4708,  0.005 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.008, New P: -0.478
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.006
iter 4 loss: 0.829
Actual params: [-0.4784,  0.0058]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.490
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.005
iter 5 loss: 0.829
Actual params: [-0.4896,  0.0051]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.492
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.006
iter 6 loss: 0.829
Actual params: [-0.4917,  0.0059]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.496
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.006
iter 7 loss: 0.829
Actual params: [-0.4961,  0.0061]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.498
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.007
iter 8 loss: 0.829
Actual params: [-0.4975,  0.0066]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.503
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: 0.007
iter 9 loss: 0.829
Actual params: [-0.5028,  0.0065]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.498
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.008
iter 10 loss: 0.829
Actual params: [-0.4982,  0.0077]
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: -0.484
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.011
iter 11 loss: 0.829
Actual params: [-0.4845,  0.011 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.478
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.013
iter 12 loss: 0.829
Actual params: [-0.4783,  0.0128]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.484
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.013
iter 13 loss: 0.829
Actual params: [-0.4839,  0.0133]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.500
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.012
iter 14 loss: 0.829
Actual params: [-0.4999,  0.0123]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.500
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.014
iter 15 loss: 0.829
Actual params: [-0.4998,  0.0138]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.496
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.016
iter 16 loss: 0.829
Actual params: [-0.4962,  0.0165]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.493
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: 0.019
iter 17 loss: 0.829
Actual params: [-0.4933,  0.019 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: -0.476
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.024
iter 18 loss: 0.829
Actual params: [-0.4761,  0.024 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.486
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.025
iter 19 loss: 0.829
Actual params: [-0.4861,  0.0248]
-Original Grad: 0.001, -lr * Pred Grad:  0.048, New P: -0.438
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.040
iter 20 loss: 0.829
Actual params: [-0.4378,  0.04  ]
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: -0.403
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.051
iter 21 loss: 0.829
Actual params: [-0.403 ,  0.0512]
-Original Grad: -0.001, -lr * Pred Grad:  -0.054, New P: -0.457
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.051
iter 22 loss: 0.829
Actual params: [-0.4568,  0.0515]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.450
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.057
iter 23 loss: 0.829
Actual params: [-0.4501,  0.0566]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.445
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.063
iter 24 loss: 0.829
Actual params: [-0.4447,  0.0635]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.458
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.070
iter 25 loss: 0.829
Actual params: [-0.4579,  0.07  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -0.481
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.072
iter 26 loss: 0.829
Actual params: [-0.4814,  0.0722]
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: -0.470
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.082
iter 27 loss: 0.829
Actual params: [-0.4696,  0.0821]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -0.488
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.087
iter 28 loss: 0.829
Actual params: [-0.4881,  0.0869]
-Original Grad: 0.000, -lr * Pred Grad:  0.044, New P: -0.444
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.102
iter 29 loss: 0.829
Actual params: [-0.4441,  0.1024]
-Original Grad: 0.000, -lr * Pred Grad:  0.062, New P: -0.382
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 0.130
iter 30 loss: 0.829
Actual params: [-0.3817,  0.1299]
-Original Grad: 0.000, -lr * Pred Grad:  0.084, New P: -0.298
-Original Grad: 0.000, -lr * Pred Grad:  0.055, New P: 0.185
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.010, -lr * Pred Grad:  -0.113, New P: -0.585
-Original Grad: -0.006, -lr * Pred Grad:  -0.071, New P: -0.067
iter 1 loss: 0.118
Actual params: [-0.5851, -0.067 ]
-Original Grad: -0.005, -lr * Pred Grad:  -0.061, New P: -0.646
-Original Grad: -0.003, -lr * Pred Grad:  -0.036, New P: -0.103
iter 2 loss: 0.118
Actual params: [-0.6459, -0.1032]
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.669
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.120
iter 3 loss: 0.118
Actual params: [-0.6688, -0.1204]
-Original Grad: -0.002, -lr * Pred Grad:  -0.026, New P: -0.694
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.138
iter 4 loss: 0.118
Actual params: [-0.6944, -0.138 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.031, New P: -0.725
-Original Grad: -0.001, -lr * Pred Grad:  -0.020, New P: -0.158
iter 5 loss: 0.118
Actual params: [-0.7252, -0.1579]
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.737
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.167
iter 6 loss: 0.118
Actual params: [-0.7367, -0.167 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.753
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.179
iter 7 loss: 0.118
Actual params: [-0.7531, -0.1788]
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.771
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: -0.191
iter 8 loss: 0.118
Actual params: [-0.7706, -0.1913]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.783
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.201
iter 9 loss: 0.118
Actual params: [-0.7829, -0.2012]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.800
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.213
iter 10 loss: 0.118
Actual params: [-0.7996, -0.2132]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.813
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.223
iter 11 loss: 0.118
Actual params: [-0.8126, -0.2228]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.830
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.234
iter 12 loss: 0.118
Actual params: [-0.8299, -0.234 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.847
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.245
iter 13 loss: 0.118
Actual params: [-0.8468, -0.2446]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.858
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.254
iter 14 loss: 0.118
Actual params: [-0.8584, -0.2537]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.869
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.262
iter 15 loss: 0.118
Actual params: [-0.8686, -0.2617]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.879
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.270
iter 16 loss: 0.118
Actual params: [-0.8792, -0.2697]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.890
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.278
iter 17 loss: 0.118
Actual params: [-0.8901, -0.2783]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.905
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.289
iter 18 loss: 0.118
Actual params: [-0.9047, -0.2893]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.917
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.298
iter 19 loss: 0.118
Actual params: [-0.9165, -0.2985]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.930
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.308
iter 20 loss: 0.118
Actual params: [-0.9295, -0.3082]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.940
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.316
iter 21 loss: 0.118
Actual params: [-0.94  , -0.3159]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.954
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.326
iter 22 loss: 0.118
Actual params: [-0.9536, -0.326 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.965
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.335
iter 23 loss: 0.118
Actual params: [-0.965 , -0.3345]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.976
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.342
iter 24 loss: 0.118
Actual params: [-0.9755, -0.3422]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.988
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.351
iter 25 loss: 0.118
Actual params: [-0.9877, -0.3508]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.997
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.358
iter 26 loss: 0.118
Actual params: [-0.9969, -0.3576]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.009
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.366
iter 27 loss: 0.118
Actual params: [-1.0086, -0.3662]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.020
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.374
iter 28 loss: 0.118
Actual params: [-1.0198, -0.3741]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.034
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.384
iter 29 loss: 0.118
Actual params: [-1.0337, -0.3836]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.046
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.393
iter 30 loss: 0.118
Actual params: [-1.0463, -0.3926]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.059
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.402
Target params: [1.3344, 1.5708]
iter 0 loss: 0.081
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.008, -lr * Pred Grad:  -0.085, New P: -0.558
-Original Grad: 0.006, -lr * Pred Grad:  0.067, New P: 0.071
iter 1 loss: 0.080
Actual params: [-0.5577,  0.0705]
-Original Grad: -0.002, -lr * Pred Grad:  -0.019, New P: -0.576
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: 0.084
iter 2 loss: 0.080
Actual params: [-0.5763,  0.0838]
-Original Grad: -0.002, -lr * Pred Grad:  -0.023, New P: -0.599
-Original Grad: 0.001, -lr * Pred Grad:  0.018, New P: 0.102
iter 3 loss: 0.080
Actual params: [-0.5989,  0.1019]
-Original Grad: -0.001, -lr * Pred Grad:  -0.015, New P: -0.614
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.114
iter 4 loss: 0.080
Actual params: [-0.6136,  0.1141]
-Original Grad: -0.001, -lr * Pred Grad:  -0.014, New P: -0.628
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.129
iter 5 loss: 0.080
Actual params: [-0.6277,  0.1293]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.639
-Original Grad: 0.001, -lr * Pred Grad:  0.017, New P: 0.146
iter 6 loss: 0.080
Actual params: [-0.6392,  0.1458]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: -0.656
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.160
iter 7 loss: 0.080
Actual params: [-0.6558,  0.1599]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.664
-Original Grad: 0.001, -lr * Pred Grad:  0.012, New P: 0.172
iter 8 loss: 0.080
Actual params: [-0.6643,  0.1716]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.675
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.181
iter 9 loss: 0.080
Actual params: [-0.6755,  0.1812]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.685
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.191
iter 10 loss: 0.080
Actual params: [-0.6853,  0.1913]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.696
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.201
iter 11 loss: 0.080
Actual params: [-0.6957,  0.2014]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -0.697
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.211
iter 12 loss: 0.080
Actual params: [-0.6975,  0.2113]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.708
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.219
iter 13 loss: 0.080
Actual params: [-0.7078,  0.2191]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -0.709
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.225
iter 14 loss: 0.080
Actual params: [-0.7091,  0.2246]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.720
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.234
iter 15 loss: 0.080
Actual params: [-0.7205,  0.2339]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.737
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.246
iter 16 loss: 0.080
Actual params: [-0.7371,  0.2461]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.745
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.258
iter 17 loss: 0.080
Actual params: [-0.7453,  0.2581]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.754
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.268
iter 18 loss: 0.080
Actual params: [-0.7537,  0.268 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.760
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.274
iter 19 loss: 0.080
Actual params: [-0.7597,  0.2738]
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.760
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.280
iter 20 loss: 0.080
Actual params: [-0.7599,  0.2802]
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: -0.741
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.282
iter 21 loss: 0.080
Actual params: [-0.7405,  0.2822]
-Original Grad: 0.000, -lr * Pred Grad:  0.031, New P: -0.709
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.283
iter 22 loss: 0.080
Actual params: [-0.7091,  0.2826]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.717
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.296
iter 23 loss: 0.080
Actual params: [-0.7171,  0.2957]
-Original Grad: 0.000, -lr * Pred Grad:  0.048, New P: -0.669
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.294
iter 24 loss: 0.080
Actual params: [-0.6686,  0.2944]
-Original Grad: 0.000, -lr * Pred Grad:  0.044, New P: -0.624
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.312
iter 25 loss: 0.080
Actual params: [-0.6245,  0.3118]
-Original Grad: 0.000, -lr * Pred Grad:  0.047, New P: -0.578
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.323
iter 26 loss: 0.080
Actual params: [-0.5778,  0.323 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.088, New P: -0.665
-Original Grad: 0.000, -lr * Pred Grad:  0.065, New P: 0.388
iter 27 loss: 0.080
Actual params: [-0.6654,  0.3876]
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: -0.650
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 0.416
iter 28 loss: 0.080
Actual params: [-0.6501,  0.4159]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.647
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: 0.446
iter 29 loss: 0.080
Actual params: [-0.6474,  0.4458]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.653
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.469
iter 30 loss: 0.080
Actual params: [-0.6528,  0.4686]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -0.676
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.492
Target params: [1.3344, 1.5708]
iter 0 loss: 0.492
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.076, -lr * Pred Grad:  -0.092, New P: -0.564
-Original Grad: 0.138, -lr * Pred Grad:  0.419, New P: 0.423
iter 1 loss: 0.459
Actual params: [-0.5642,  0.4227]
-Original Grad: 0.115, -lr * Pred Grad:  0.119, New P: -0.445
-Original Grad: 0.119, -lr * Pred Grad:  0.200, New P: 0.623
iter 2 loss: 0.430
Actual params: [-0.4447,  0.6227]
-Original Grad: 0.178, -lr * Pred Grad:  0.130, New P: -0.314
-Original Grad: 0.144, -lr * Pred Grad:  0.193, New P: 0.815
iter 3 loss: 0.386
Actual params: [-0.3142,  0.8152]
-Original Grad: 0.300, -lr * Pred Grad:  0.290, New P: -0.024
-Original Grad: 0.141, -lr * Pred Grad:  -0.028, New P: 0.788
iter 4 loss: 0.308
Actual params: [-0.024 ,  0.7875]
-Original Grad: 0.207, -lr * Pred Grad:  0.168, New P: 0.144
-Original Grad: 1.474, -lr * Pred Grad:  0.058, New P: 0.845
iter 5 loss: 0.232
Actual params: [0.144 , 0.8451]
-Original Grad: 0.470, -lr * Pred Grad:  0.131, New P: 0.275
-Original Grad: 0.312, -lr * Pred Grad:  0.029, New P: 0.874
iter 6 loss: 0.189
Actual params: [0.2747, 0.8738]
-Original Grad: 0.405, -lr * Pred Grad:  0.094, New P: 0.368
-Original Grad: 0.277, -lr * Pred Grad:  0.021, New P: 0.895
iter 7 loss: 0.164
Actual params: [0.3682, 0.8946]
-Original Grad: 0.267, -lr * Pred Grad:  0.063, New P: 0.432
-Original Grad: 0.416, -lr * Pred Grad:  0.019, New P: 0.913
iter 8 loss: 0.148
Actual params: [0.4316, 0.9132]
-Original Grad: 0.198, -lr * Pred Grad:  0.049, New P: 0.481
-Original Grad: 0.502, -lr * Pred Grad:  0.019, New P: 0.932
iter 9 loss: 0.135
Actual params: [0.4811, 0.9318]
-Original Grad: 0.242, -lr * Pred Grad:  0.049, New P: 0.530
-Original Grad: 0.409, -lr * Pred Grad:  0.016, New P: 0.948
iter 10 loss: 0.123
Actual params: [0.5305, 0.9477]
-Original Grad: 0.164, -lr * Pred Grad:  0.038, New P: 0.569
-Original Grad: 0.561, -lr * Pred Grad:  0.019, New P: 0.967
iter 11 loss: 0.113
Actual params: [0.5685, 0.9666]
-Original Grad: 0.147, -lr * Pred Grad:  0.032, New P: 0.601
-Original Grad: 0.365, -lr * Pred Grad:  0.013, New P: 0.980
iter 12 loss: 0.107
Actual params: [0.601 , 0.9798]
-Original Grad: -0.046, -lr * Pred Grad:  -0.001, New P: 0.600
-Original Grad: 0.488, -lr * Pred Grad:  0.015, New P: 0.995
iter 13 loss: 0.103
Actual params: [0.6   , 0.9949]
-Original Grad: -0.009, -lr * Pred Grad:  0.006, New P: 0.606
-Original Grad: 0.506, -lr * Pred Grad:  0.017, New P: 1.012
iter 14 loss: 0.099
Actual params: [0.6063, 1.0118]
-Original Grad: -0.011, -lr * Pred Grad:  0.004, New P: 0.610
-Original Grad: 0.376, -lr * Pred Grad:  0.013, New P: 1.025
iter 15 loss: 0.095
Actual params: [0.6104, 1.0251]
-Original Grad: -0.016, -lr * Pred Grad:  0.003, New P: 0.614
-Original Grad: 0.384, -lr * Pred Grad:  0.014, New P: 1.039
iter 16 loss: 0.091
Actual params: [0.6137, 1.0394]
-Original Grad: 0.008, -lr * Pred Grad:  0.008, New P: 0.622
-Original Grad: 0.394, -lr * Pred Grad:  0.016, New P: 1.055
iter 17 loss: 0.087
Actual params: [0.6217, 1.0552]
-Original Grad: 0.095, -lr * Pred Grad:  0.023, New P: 0.645
-Original Grad: 0.323, -lr * Pred Grad:  0.015, New P: 1.070
iter 18 loss: 0.083
Actual params: [0.6452, 1.0702]
-Original Grad: 0.072, -lr * Pred Grad:  0.020, New P: 0.665
-Original Grad: 0.321, -lr * Pred Grad:  0.015, New P: 1.085
iter 19 loss: 0.080
Actual params: [0.6648, 1.0853]
-Original Grad: -0.083, -lr * Pred Grad:  -0.010, New P: 0.654
-Original Grad: 0.439, -lr * Pred Grad:  0.019, New P: 1.104
iter 20 loss: 0.076
Actual params: [0.6543, 1.1042]
-Original Grad: -0.025, -lr * Pred Grad:  0.002, New P: 0.656
-Original Grad: 0.434, -lr * Pred Grad:  0.020, New P: 1.124
iter 21 loss: 0.071
Actual params: [0.6563, 1.1245]
-Original Grad: 0.028, -lr * Pred Grad:  0.013, New P: 0.669
-Original Grad: 0.384, -lr * Pred Grad:  0.020, New P: 1.144
iter 22 loss: 0.067
Actual params: [0.6691, 1.144 ]
-Original Grad: -0.009, -lr * Pred Grad:  0.005, New P: 0.674
-Original Grad: 0.439, -lr * Pred Grad:  0.022, New P: 1.166
iter 23 loss: 0.063
Actual params: [0.6745, 1.1664]
-Original Grad: -0.014, -lr * Pred Grad:  0.001, New P: 0.676
-Original Grad: 0.261, -lr * Pred Grad:  0.014, New P: 1.180
iter 24 loss: 0.060
Actual params: [0.6757, 1.1801]
-Original Grad: 0.023, -lr * Pred Grad:  0.012, New P: 0.688
-Original Grad: 0.345, -lr * Pred Grad:  0.019, New P: 1.200
iter 25 loss: 0.057
Actual params: [0.6877, 1.1995]
-Original Grad: -0.030, -lr * Pred Grad:  -0.003, New P: 0.685
-Original Grad: 0.267, -lr * Pred Grad:  0.015, New P: 1.214
iter 26 loss: 0.054
Actual params: [0.6851, 1.2144]
-Original Grad: -0.107, -lr * Pred Grad:  -0.016, New P: 0.669
-Original Grad: 0.286, -lr * Pred Grad:  0.015, New P: 1.230
iter 27 loss: 0.051
Actual params: [0.6688, 1.2296]
-Original Grad: -0.099, -lr * Pred Grad:  -0.013, New P: 0.656
-Original Grad: 0.325, -lr * Pred Grad:  0.018, New P: 1.248
iter 28 loss: 0.048
Actual params: [0.6556, 1.2477]
-Original Grad: -0.162, -lr * Pred Grad:  -0.019, New P: 0.636
-Original Grad: 0.289, -lr * Pred Grad:  0.015, New P: 1.263
iter 29 loss: 0.046
Actual params: [0.6364, 1.2627]
-Original Grad: 0.005, -lr * Pred Grad:  0.007, New P: 0.644
-Original Grad: 0.285, -lr * Pred Grad:  0.019, New P: 1.282
iter 30 loss: 0.043
Actual params: [0.6438, 1.2818]
-Original Grad: -0.107, -lr * Pred Grad:  -0.011, New P: 0.633
-Original Grad: 0.244, -lr * Pred Grad:  0.014, New P: 1.296
Target params: [1.3344, 1.5708]
iter 0 loss: 0.239
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.472
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: 0.004
iter 1 loss: 0.239
Actual params: [-0.4719,  0.0036]
-Original Grad: 0.004, -lr * Pred Grad:  0.053, New P: -0.419
-Original Grad: -0.001, -lr * Pred Grad:  -0.016, New P: -0.013
iter 2 loss: 0.238
Actual params: [-0.4194, -0.0127]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.419
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.013
iter 3 loss: 0.238
Actual params: [-0.4187, -0.0125]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.418
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.012
iter 4 loss: 0.238
Actual params: [-0.4179, -0.0123]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.417
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.012
iter 5 loss: 0.238
Actual params: [-0.4173, -0.0121]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.417
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.012
iter 6 loss: 0.238
Actual params: [-0.4167, -0.012 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.415
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.012
iter 7 loss: 0.238
Actual params: [-0.4155, -0.0116]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.414
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.011
iter 8 loss: 0.238
Actual params: [-0.4145, -0.0113]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.413
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.011
iter 9 loss: 0.238
Actual params: [-0.4131, -0.0109]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.412
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.010
iter 10 loss: 0.238
Actual params: [-0.4118, -0.0105]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.410
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.010
iter 11 loss: 0.238
Actual params: [-0.4102, -0.01  ]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.409
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.010
iter 12 loss: 0.238
Actual params: [-0.409 , -0.0096]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.408
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -0.010
iter 13 loss: 0.238
Actual params: [-0.4079, -0.0096]
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.406
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.009
iter 14 loss: 0.238
Actual params: [-0.4064, -0.0094]
-Original Grad: 0.003, -lr * Pred Grad:  0.157, New P: -0.249
-Original Grad: -0.001, -lr * Pred Grad:  -0.049, New P: -0.058
iter 15 loss: 0.233
Actual params: [-0.2494, -0.0582]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.246
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.058
iter 16 loss: 0.233
Actual params: [-0.2457, -0.0579]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.241
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.057
iter 17 loss: 0.233
Actual params: [-0.2413, -0.0573]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.236
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.056
iter 18 loss: 0.233
Actual params: [-0.2358, -0.0565]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.229
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.055
iter 19 loss: 0.232
Actual params: [-0.2288, -0.0548]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.222
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.054
iter 20 loss: 0.232
Actual params: [-0.222 , -0.0539]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.215
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.053
iter 21 loss: 0.232
Actual params: [-0.2153, -0.0532]
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: -0.206
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.052
iter 22 loss: 0.232
Actual params: [-0.2059, -0.0516]
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: -0.199
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.051
iter 23 loss: 0.231
Actual params: [-0.1986, -0.0512]
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: -0.187
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.050
iter 24 loss: 0.231
Actual params: [-0.1874, -0.0497]
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: -0.177
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.049
iter 25 loss: 0.231
Actual params: [-0.1768, -0.0486]
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: -0.162
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.046
iter 26 loss: 0.230
Actual params: [-0.1615, -0.0464]
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: -0.154
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.046
iter 27 loss: 0.230
Actual params: [-0.1537, -0.0463]
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: -0.132
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.043
iter 28 loss: 0.229
Actual params: [-0.1324, -0.0431]
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: -0.111
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.040
iter 29 loss: 0.228
Actual params: [-0.111 , -0.0403]
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: -0.097
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -0.040
iter 30 loss: 0.227
Actual params: [-0.097, -0.04 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: -0.065
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.035
Target params: [1.3344, 1.5708]
iter 0 loss: 0.065
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.013, New P: -0.460
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.015
iter 1 loss: 0.064
Actual params: [-0.4597,  0.0146]
-Original Grad: 0.002, -lr * Pred Grad:  0.026, New P: -0.433
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 0.030
iter 2 loss: 0.063
Actual params: [-0.4333,  0.03  ]
-Original Grad: 0.005, -lr * Pred Grad:  0.065, New P: -0.368
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.039
iter 3 loss: 0.058
Actual params: [-0.368 ,  0.0388]
-Original Grad: 0.002, -lr * Pred Grad:  0.025, New P: -0.343
-Original Grad: 0.001, -lr * Pred Grad:  0.021, New P: 0.060
iter 4 loss: 0.056
Actual params: [-0.3428,  0.0595]
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.339
-Original Grad: 0.001, -lr * Pred Grad:  0.022, New P: 0.082
iter 5 loss: 0.056
Actual params: [-0.3387,  0.0815]
-Original Grad: 0.005, -lr * Pred Grad:  0.093, New P: -0.245
-Original Grad: 0.002, -lr * Pred Grad:  0.043, New P: 0.125
iter 6 loss: 0.048
Actual params: [-0.2455,  0.1247]
-Original Grad: 0.002, -lr * Pred Grad:  0.032, New P: -0.213
-Original Grad: 0.002, -lr * Pred Grad:  0.042, New P: 0.167
iter 7 loss: 0.046
Actual params: [-0.2132,  0.1668]
-Original Grad: 0.002, -lr * Pred Grad:  0.043, New P: -0.170
-Original Grad: 0.002, -lr * Pred Grad:  0.034, New P: 0.201
iter 8 loss: 0.043
Actual params: [-0.1702,  0.2008]
-Original Grad: 0.009, -lr * Pred Grad:  0.179, New P: 0.009
-Original Grad: 0.004, -lr * Pred Grad:  0.087, New P: 0.288
iter 9 loss: 0.031
Actual params: [0.0086, 0.2879]
-Original Grad: 0.006, -lr * Pred Grad:  0.128, New P: 0.136
-Original Grad: 0.004, -lr * Pred Grad:  0.088, New P: 0.376
iter 10 loss: 0.030
Actual params: [0.1361, 0.3757]
-Original Grad: 0.034, -lr * Pred Grad:  0.368, New P: 0.504
-Original Grad: 0.007, -lr * Pred Grad:  0.105, New P: 0.481
iter 11 loss: 0.087
Actual params: [0.5041, 0.4811]
-Original Grad: -0.419, -lr * Pred Grad:  -0.043, New P: 0.461
-Original Grad: 0.314, -lr * Pred Grad:  0.331, New P: 0.812
iter 12 loss: 0.036
Actual params: [0.4611, 0.8121]
-Original Grad: -0.344, -lr * Pred Grad:  -0.036, New P: 0.425
-Original Grad: 0.221, -lr * Pred Grad:  0.066, New P: 0.878
iter 13 loss: 0.028
Actual params: [0.4249, 0.8783]
-Original Grad: -0.036, -lr * Pred Grad:  0.065, New P: 0.490
-Original Grad: 0.042, -lr * Pred Grad:  0.124, New P: 1.002
iter 14 loss: 0.027
Actual params: [0.4897, 1.0024]
-Original Grad: 0.061, -lr * Pred Grad:  0.115, New P: 0.605
-Original Grad: 0.000, -lr * Pred Grad:  0.182, New P: 1.185
iter 15 loss: 0.025
Actual params: [0.605 , 1.1846]
-Original Grad: 0.052, -lr * Pred Grad:  0.081, New P: 0.686
-Original Grad: -0.001, -lr * Pred Grad:  0.128, New P: 1.312
iter 16 loss: 0.021
Actual params: [0.6864, 1.3125]
-Original Grad: 0.079, -lr * Pred Grad:  0.096, New P: 0.782
-Original Grad: -0.000, -lr * Pred Grad:  0.150, New P: 1.463
iter 17 loss: 0.018
Actual params: [0.7823, 1.4626]
-Original Grad: 0.069, -lr * Pred Grad:  0.072, New P: 0.854
-Original Grad: -0.003, -lr * Pred Grad:  0.111, New P: 1.573
iter 18 loss: 0.018
Actual params: [0.8539, 1.5732]
-Original Grad: 0.106, -lr * Pred Grad:  0.028, New P: 0.882
-Original Grad: -0.030, -lr * Pred Grad:  0.015, New P: 1.588
iter 19 loss: 0.018
Actual params: [0.8822, 1.5885]
-Original Grad: 0.077, -lr * Pred Grad:  0.006, New P: 0.888
-Original Grad: -0.052, -lr * Pred Grad:  -0.033, New P: 1.555
iter 20 loss: 0.017
Actual params: [0.8883, 1.5551]
-Original Grad: 0.062, -lr * Pred Grad:  0.002, New P: 0.890
-Original Grad: -0.065, -lr * Pred Grad:  -0.044, New P: 1.511
iter 21 loss: 0.015
Actual params: [0.8904, 1.5111]
-Original Grad: 0.087, -lr * Pred Grad:  0.016, New P: 0.907
-Original Grad: -0.022, -lr * Pred Grad:  -0.004, New P: 1.507
iter 22 loss: 0.015
Actual params: [0.9067, 1.507 ]
-Original Grad: 0.249, -lr * Pred Grad:  0.041, New P: 0.947
-Original Grad: -0.036, -lr * Pred Grad:  0.004, New P: 1.511
iter 23 loss: 0.013
Actual params: [0.9474, 1.5106]
-Original Grad: 0.156, -lr * Pred Grad:  0.017, New P: 0.965
-Original Grad: -0.062, -lr * Pred Grad:  -0.034, New P: 1.476
iter 24 loss: 0.012
Actual params: [0.9648, 1.4764]
-Original Grad: 0.074, -lr * Pred Grad:  0.007, New P: 0.971
-Original Grad: -0.053, -lr * Pred Grad:  -0.033, New P: 1.443
iter 25 loss: 0.012
Actual params: [0.9713, 1.443 ]
-Original Grad: 0.068, -lr * Pred Grad:  0.006, New P: 0.977
-Original Grad: -0.073, -lr * Pred Grad:  -0.042, New P: 1.401
iter 26 loss: 0.012
Actual params: [0.9771, 1.4011]
-Original Grad: 0.073, -lr * Pred Grad:  0.008, New P: 0.985
-Original Grad: -0.056, -lr * Pred Grad:  -0.029, New P: 1.372
iter 27 loss: 0.012
Actual params: [0.9852, 1.372 ]
-Original Grad: 0.183, -lr * Pred Grad:  0.024, New P: 1.009
-Original Grad: -0.013, -lr * Pred Grad:  -0.001, New P: 1.371
iter 28 loss: 0.012
Actual params: [1.0091, 1.3708]
-Original Grad: -0.007, -lr * Pred Grad:  -0.002, New P: 1.007
-Original Grad: -0.058, -lr * Pred Grad:  -0.032, New P: 1.339
iter 29 loss: 0.013
Actual params: [1.0071, 1.3387]
-Original Grad: 0.144, -lr * Pred Grad:  0.020, New P: 1.027
-Original Grad: -0.001, -lr * Pred Grad:  0.003, New P: 1.341
iter 30 loss: 0.013
Actual params: [1.0269, 1.3412]
-Original Grad: 0.060, -lr * Pred Grad:  0.008, New P: 1.035
-Original Grad: -0.043, -lr * Pred Grad:  -0.023, New P: 1.318
Target params: [1.3344, 1.5708]
iter 0 loss: 0.575
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.059, -lr * Pred Grad:  -0.503, New P: -0.975
-Original Grad: 0.055, -lr * Pred Grad:  0.460, New P: 0.463
iter 1 loss: 0.543
Actual params: [-0.975 ,  0.4635]
-Original Grad: 0.001, -lr * Pred Grad:  0.029, New P: -0.946
-Original Grad: 0.011, -lr * Pred Grad:  0.119, New P: 0.582
iter 2 loss: 0.541
Actual params: [-0.9458,  0.5822]
-Original Grad: 0.003, -lr * Pred Grad:  0.053, New P: -0.893
-Original Grad: 0.008, -lr * Pred Grad:  0.099, New P: 0.681
iter 3 loss: 0.540
Actual params: [-0.8933,  0.6814]
-Original Grad: 0.007, -lr * Pred Grad:  0.096, New P: -0.798
-Original Grad: 0.004, -lr * Pred Grad:  0.065, New P: 0.747
iter 4 loss: 0.539
Actual params: [-0.7977,  0.7468]
-Original Grad: 0.010, -lr * Pred Grad:  0.147, New P: -0.650
-Original Grad: 0.003, -lr * Pred Grad:  0.063, New P: 0.810
iter 5 loss: 0.534
Actual params: [-0.6504,  0.8096]
-Original Grad: 0.010, -lr * Pred Grad:  0.168, New P: -0.482
-Original Grad: 0.002, -lr * Pred Grad:  0.049, New P: 0.859
iter 6 loss: 0.526
Actual params: [-0.4822,  0.859 ]
-Original Grad: 0.020, -lr * Pred Grad:  0.329, New P: -0.153
-Original Grad: -0.003, -lr * Pred Grad:  -0.012, New P: 0.847
iter 7 loss: 0.497
Actual params: [-0.153 ,  0.8466]
-Original Grad: 0.056, -lr * Pred Grad:  0.808, New P: 0.655
-Original Grad: 0.003, -lr * Pred Grad:  0.177, New P: 1.024
iter 8 loss: 0.302
Actual params: [0.6549, 1.0236]
-Original Grad: 0.128, -lr * Pred Grad:  0.330, New P: 0.985
-Original Grad: 0.236, -lr * Pred Grad:  0.729, New P: 1.753
iter 9 loss: 0.102
Actual params: [0.9851, 1.7528]
-Original Grad: 0.292, -lr * Pred Grad:  0.131, New P: 1.116
-Original Grad: 0.288, -lr * Pred Grad:  0.277, New P: 2.030
iter 10 loss: 0.066
Actual params: [1.1158, 2.0298]
-Original Grad: 0.011, -lr * Pred Grad:  -0.079, New P: 1.037
-Original Grad: 0.216, -lr * Pred Grad:  0.216, New P: 2.245
iter 11 loss: 0.084
Actual params: [1.0369, 2.2454]
-Original Grad: 0.195, -lr * Pred Grad:  0.128, New P: 1.165
-Original Grad: -0.110, -lr * Pred Grad:  -0.106, New P: 2.140
iter 12 loss: 0.067
Actual params: [1.1645, 2.1397]
-Original Grad: 0.031, -lr * Pred Grad:  -0.017, New P: 1.147
-Original Grad: 0.203, -lr * Pred Grad:  0.096, New P: 2.236
iter 13 loss: 0.075
Actual params: [1.147 , 2.2359]
-Original Grad: 0.025, -lr * Pred Grad:  0.029, New P: 1.176
-Original Grad: -0.258, -lr * Pred Grad:  -0.083, New P: 2.153
iter 14 loss: 0.067
Actual params: [1.1762, 2.1526]
-Original Grad: 0.014, -lr * Pred Grad:  0.005, New P: 1.181
-Original Grad: 0.063, -lr * Pred Grad:  0.017, New P: 2.169
iter 15 loss: 0.068
Actual params: [1.181 , 2.1692]
-Original Grad: 0.065, -lr * Pred Grad:  0.033, New P: 1.214
-Original Grad: 0.049, -lr * Pred Grad:  0.009, New P: 2.179
iter 16 loss: 0.068
Actual params: [1.2143, 2.1786]
-Original Grad: 0.002, -lr * Pred Grad:  -0.001, New P: 1.214
-Original Grad: 0.064, -lr * Pred Grad:  0.015, New P: 2.193
iter 17 loss: 0.069
Actual params: [1.2136, 2.1932]
-Original Grad: 0.030, -lr * Pred Grad:  0.017, New P: 1.231
-Original Grad: 0.054, -lr * Pred Grad:  0.011, New P: 2.204
iter 18 loss: 0.070
Actual params: [1.2306, 2.2037]
-Original Grad: 0.041, -lr * Pred Grad:  0.023, New P: 1.254
-Original Grad: 0.097, -lr * Pred Grad:  0.019, New P: 2.222
iter 19 loss: 0.072
Actual params: [1.2539, 2.2224]
-Original Grad: -0.023, -lr * Pred Grad:  -0.017, New P: 1.237
-Original Grad: 0.015, -lr * Pred Grad:  0.003, New P: 2.226
iter 20 loss: 0.072
Actual params: [1.2371, 2.2258]
-Original Grad: -0.006, -lr * Pred Grad:  -0.003, New P: 1.234
-Original Grad: -0.143, -lr * Pred Grad:  -0.025, New P: 2.201
iter 21 loss: 0.070
Actual params: [1.2344, 2.201 ]
-Original Grad: 0.032, -lr * Pred Grad:  0.027, New P: 1.261
-Original Grad: -0.006, -lr * Pred Grad:  -0.002, New P: 2.199
iter 22 loss: 0.070
Actual params: [1.261 , 2.1994]
-Original Grad: -0.016, -lr * Pred Grad:  -0.015, New P: 1.246
-Original Grad: 0.127, -lr * Pred Grad:  0.021, New P: 2.220
iter 23 loss: 0.072
Actual params: [1.2455, 2.2203]
-Original Grad: -0.006, -lr * Pred Grad:  -0.005, New P: 1.241
-Original Grad: -0.102, -lr * Pred Grad:  -0.016, New P: 2.205
iter 24 loss: 0.070
Actual params: [1.2407, 2.2047]
-Original Grad: -0.017, -lr * Pred Grad:  -0.018, New P: 1.222
-Original Grad: 0.065, -lr * Pred Grad:  0.010, New P: 2.214
iter 25 loss: 0.071
Actual params: [1.2222, 2.2144]
-Original Grad: 0.033, -lr * Pred Grad:  0.036, New P: 1.258
-Original Grad: 0.100, -lr * Pred Grad:  0.015, New P: 2.229
iter 26 loss: 0.073
Actual params: [1.2579, 2.2293]
-Original Grad: -0.017, -lr * Pred Grad:  -0.020, New P: 1.238
-Original Grad: -0.093, -lr * Pred Grad:  -0.013, New P: 2.216
iter 27 loss: 0.071
Actual params: [1.2376, 2.216 ]
-Original Grad: -0.005, -lr * Pred Grad:  -0.007, New P: 1.231
-Original Grad: -0.039, -lr * Pred Grad:  -0.006, New P: 2.210
iter 28 loss: 0.071
Actual params: [1.231 , 2.2104]
-Original Grad: 0.033, -lr * Pred Grad:  0.044, New P: 1.275
-Original Grad: 0.100, -lr * Pred Grad:  0.014, New P: 2.225
iter 29 loss: 0.072
Actual params: [1.2747, 2.2245]
-Original Grad: -0.030, -lr * Pred Grad:  -0.041, New P: 1.234
-Original Grad: 0.070, -lr * Pred Grad:  0.009, New P: 2.234
iter 30 loss: 0.073
Actual params: [1.234 , 2.2336]
-Original Grad: -0.019, -lr * Pred Grad:  -0.029, New P: 1.205
-Original Grad: 0.019, -lr * Pred Grad:  0.002, New P: 2.235
Target params: [1.3344, 1.5708]
iter 0 loss: 0.309
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.483
-Original Grad: 0.001, -lr * Pred Grad:  0.008, New P: 0.011
iter 1 loss: 0.309
Actual params: [-0.4834,  0.0112]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.494
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.020
iter 2 loss: 0.309
Actual params: [-0.4938,  0.0202]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.499
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.024
iter 3 loss: 0.309
Actual params: [-0.4993,  0.0245]
-Original Grad: -0.001, -lr * Pred Grad:  -0.011, New P: -0.511
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.034
iter 4 loss: 0.309
Actual params: [-0.5108,  0.0339]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.519
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.043
iter 5 loss: 0.309
Actual params: [-0.5188,  0.0426]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.529
-Original Grad: 0.001, -lr * Pred Grad:  0.011, New P: 0.053
iter 6 loss: 0.309
Actual params: [-0.5288,  0.0535]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.533
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.060
iter 7 loss: 0.309
Actual params: [-0.5331,  0.0598]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.541
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.068
iter 8 loss: 0.309
Actual params: [-0.5408,  0.0676]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.548
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.077
iter 9 loss: 0.309
Actual params: [-0.5475,  0.077 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.555
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.089
iter 10 loss: 0.309
Actual params: [-0.555 ,  0.0893]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.559
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.100
iter 11 loss: 0.309
Actual params: [-0.5595,  0.0997]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.568
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.112
iter 12 loss: 0.309
Actual params: [-0.5676,  0.1115]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.571
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.128
iter 13 loss: 0.309
Actual params: [-0.5708,  0.1279]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.569
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.139
iter 14 loss: 0.309
Actual params: [-0.5687,  0.1387]
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.565
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.159
iter 15 loss: 0.309
Actual params: [-0.5655,  0.1593]
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.561
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.182
iter 16 loss: 0.309
Actual params: [-0.5608,  0.1819]
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: -0.545
-Original Grad: 0.001, -lr * Pred Grad:  0.032, New P: 0.214
iter 17 loss: 0.309
Actual params: [-0.5453,  0.2137]
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: -0.531
-Original Grad: 0.000, -lr * Pred Grad:  0.027, New P: 0.240
iter 18 loss: 0.309
Actual params: [-0.531 ,  0.2402]
-Original Grad: 0.001, -lr * Pred Grad:  0.059, New P: -0.472
-Original Grad: 0.001, -lr * Pred Grad:  0.078, New P: 0.318
iter 19 loss: 0.309
Actual params: [-0.4721,  0.3178]
-Original Grad: 0.003, -lr * Pred Grad:  0.244, New P: -0.228
-Original Grad: 0.003, -lr * Pred Grad:  0.230, New P: 0.548
iter 20 loss: 0.305
Actual params: [-0.2278,  0.5476]
-Original Grad: 0.053, -lr * Pred Grad:  1.288, New P: 1.060
-Original Grad: 0.038, -lr * Pred Grad:  1.020, New P: 1.568
iter 21 loss: 0.031
Actual params: [1.0603, 1.5678]
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 1.074
-Original Grad: 0.101, -lr * Pred Grad:  0.286, New P: 1.854
iter 22 loss: 0.022
Actual params: [1.074 , 1.8538]
-Original Grad: 0.055, -lr * Pred Grad:  0.056, New P: 1.130
-Original Grad: -0.086, -lr * Pred Grad:  -0.108, New P: 1.746
iter 23 loss: 0.023
Actual params: [1.1305, 1.7457]
-Original Grad: 0.011, -lr * Pred Grad:  0.005, New P: 1.135
-Original Grad: 0.046, -lr * Pred Grad:  0.050, New P: 1.796
iter 24 loss: 0.022
Actual params: [1.1353, 1.796 ]
-Original Grad: 0.036, -lr * Pred Grad:  0.031, New P: 1.167
-Original Grad: 0.016, -lr * Pred Grad:  0.015, New P: 1.811
iter 25 loss: 0.023
Actual params: [1.1666, 1.8113]
-Original Grad: 0.020, -lr * Pred Grad:  0.026, New P: 1.193
-Original Grad: -0.039, -lr * Pred Grad:  -0.041, New P: 1.771
iter 26 loss: 0.023
Actual params: [1.1925, 1.7706]
-Original Grad: 0.006, -lr * Pred Grad:  0.001, New P: 1.193
-Original Grad: 0.027, -lr * Pred Grad:  0.026, New P: 1.797
iter 27 loss: 0.023
Actual params: [1.1934, 1.7969]
-Original Grad: 0.006, -lr * Pred Grad:  0.003, New P: 1.196
-Original Grad: 0.018, -lr * Pred Grad:  0.015, New P: 1.812
iter 28 loss: 0.023
Actual params: [1.1965, 1.8122]
-Original Grad: -0.043, -lr * Pred Grad:  -0.013, New P: 1.184
-Original Grad: -0.116, -lr * Pred Grad:  -0.061, New P: 1.751
iter 29 loss: 0.023
Actual params: [1.1839, 1.7512]
-Original Grad: 0.029, -lr * Pred Grad:  0.021, New P: 1.205
-Original Grad: 0.038, -lr * Pred Grad:  0.016, New P: 1.767
iter 30 loss: 0.023
Actual params: [1.2048, 1.7674]
-Original Grad: 0.011, -lr * Pred Grad:  -0.003, New P: 1.202
-Original Grad: 0.045, -lr * Pred Grad:  0.029, New P: 1.796
