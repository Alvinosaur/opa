Target params: [1.3344, 1.5708]
iter 0 loss: 0.077
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.044, -lr * Pred Grad: 0.167, New P: -0.305
-Original Grad: -0.002, -lr * Pred Grad: 0.624, New P: 0.628
iter 1 loss: 0.076
Actual params: [-0.305 ,  0.6277]
-Original Grad: -0.016, -lr * Pred Grad: 0.127, New P: -0.178
-Original Grad: 0.037, -lr * Pred Grad: 0.328, New P: 0.955
iter 2 loss: 0.071
Actual params: [-0.1783,  0.9553]
-Original Grad: -0.000, -lr * Pred Grad: 0.105, New P: -0.073
-Original Grad: 0.034, -lr * Pred Grad: 0.076, New P: 1.031
iter 3 loss: 0.069
Actual params: [-0.0729,  1.0309]
-Original Grad: 0.011, -lr * Pred Grad: 0.121, New P: 0.048
-Original Grad: 0.028, -lr * Pred Grad: 0.071, New P: 1.102
iter 4 loss: 0.065
Actual params: [0.0478, 1.1022]
-Original Grad: 0.024, -lr * Pred Grad: 0.169, New P: 0.216
-Original Grad: 0.064, -lr * Pred Grad: 0.178, New P: 1.281
iter 5 loss: 0.054
Actual params: [0.2164, 1.2807]
-Original Grad: 0.062, -lr * Pred Grad: 0.327, New P: 0.544
-Original Grad: 0.064, -lr * Pred Grad: 0.223, New P: 1.504
iter 6 loss: 0.022
Actual params: [0.5436, 1.5037]
-Original Grad: 0.090, -lr * Pred Grad: 0.471, New P: 1.015
-Original Grad: 0.101, -lr * Pred Grad: 0.398, New P: 1.902
iter 7 loss: 0.013
Actual params: [1.0149, 1.9019]
-Original Grad: -0.011, -lr * Pred Grad: 0.161, New P: 1.176
-Original Grad: -0.150, -lr * Pred Grad: -0.303, New P: 1.599
iter 8 loss: 0.004
Actual params: [1.1757, 1.599 ]
-Original Grad: -0.023, -lr * Pred Grad: 0.054, New P: 1.230
-Original Grad: 0.094, -lr * Pred Grad: 0.101, New P: 1.700
iter 9 loss: 0.004
Actual params: [1.2299, 1.7004]
-Original Grad: -0.016, -lr * Pred Grad: -0.006, New P: 1.224
-Original Grad: 0.007, -lr * Pred Grad: 0.019, New P: 1.720
iter 10 loss: 0.004
Actual params: [1.2243, 1.7196]
-Original Grad: -0.013, -lr * Pred Grad: -0.039, New P: 1.185
-Original Grad: -0.012, -lr * Pred Grad: -0.024, New P: 1.696
iter 11 loss: 0.003
Actual params: [1.1853, 1.6957]
-Original Grad: -0.015, -lr * Pred Grad: -0.070, New P: 1.116
-Original Grad: -0.012, -lr * Pred Grad: -0.052, New P: 1.644
iter 12 loss: 0.003
Actual params: [1.1156, 1.6439]
-Original Grad: -0.011, -lr * Pred Grad: -0.084, New P: 1.032
-Original Grad: 0.031, -lr * Pred Grad: 0.038, New P: 1.682
iter 13 loss: 0.002
Actual params: [1.0318, 1.6822]
-Original Grad: 0.006, -lr * Pred Grad: -0.051, New P: 0.981
-Original Grad: -0.015, -lr * Pred Grad: -0.041, New P: 1.641
iter 14 loss: 0.002
Actual params: [0.9812, 1.641 ]
-Original Grad: 0.011, -lr * Pred Grad: -0.022, New P: 0.960
-Original Grad: 0.031, -lr * Pred Grad: 0.049, New P: 1.690
iter 15 loss: 0.002
Actual params: [0.9596, 1.6899]
-Original Grad: 0.011, -lr * Pred Grad: -0.003, New P: 0.956
-Original Grad: -0.002, -lr * Pred Grad: -0.008, New P: 1.682
iter 16 loss: 0.002
Actual params: [0.9562, 1.6816]
-Original Grad: 0.020, -lr * Pred Grad: 0.038, New P: 0.994
-Original Grad: -0.015, -lr * Pred Grad: -0.053, New P: 1.629
iter 17 loss: 0.002
Actual params: [0.9938, 1.629 ]
-Original Grad: 0.005, -lr * Pred Grad: 0.018, New P: 1.011
-Original Grad: 0.032, -lr * Pred Grad: 0.040, New P: 1.669
iter 18 loss: 0.002
Actual params: [1.0115, 1.6692]
-Original Grad: 0.002, -lr * Pred Grad: 0.006, New P: 1.017
-Original Grad: -0.009, -lr * Pred Grad: -0.028, New P: 1.641
iter 19 loss: 0.002
Actual params: [1.0174, 1.6408]
-Original Grad: 0.009, -lr * Pred Grad: 0.019, New P: 1.037
-Original Grad: 0.014, -lr * Pred Grad: 0.008, New P: 1.649
iter 20 loss: 0.002
Actual params: [1.0366, 1.6491]
-Original Grad: -0.005, -lr * Pred Grad: -0.016, New P: 1.021
-Original Grad: 0.037, -lr * Pred Grad: 0.079, New P: 1.728
iter 21 loss: 0.003
Actual params: [1.0209, 1.7278]
-Original Grad: -0.002, -lr * Pred Grad: -0.025, New P: 0.996
-Original Grad: -0.036, -lr * Pred Grad: -0.081, New P: 1.646
iter 22 loss: 0.002
Actual params: [0.9963, 1.6464]
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 0.972
-Original Grad: 0.016, -lr * Pred Grad: -0.005, New P: 1.641
iter 23 loss: 0.002
Actual params: [0.9724, 1.6412]
-Original Grad: 0.018, -lr * Pred Grad: 0.023, New P: 0.995
-Original Grad: 0.034, -lr * Pred Grad: 0.062, New P: 1.703
iter 24 loss: 0.002
Actual params: [0.9955, 1.7027]
-Original Grad: -0.001, -lr * Pred Grad: -0.008, New P: 0.987
-Original Grad: -0.031, -lr * Pred Grad: -0.074, New P: 1.629
iter 25 loss: 0.002
Actual params: [0.987 , 1.6289]
-Original Grad: 0.002, -lr * Pred Grad: -0.010, New P: 0.977
-Original Grad: 0.031, -lr * Pred Grad: 0.033, New P: 1.662
iter 26 loss: 0.002
Actual params: [0.9765, 1.6617]
-Original Grad: 0.016, -lr * Pred Grad: 0.026, New P: 1.003
-Original Grad: -0.018, -lr * Pred Grad: -0.056, New P: 1.606
iter 27 loss: 0.002
Actual params: [1.0026, 1.6055]
-Original Grad: 0.015, -lr * Pred Grad: 0.040, New P: 1.042
-Original Grad: 0.046, -lr * Pred Grad: 0.081, New P: 1.687
iter 28 loss: 0.002
Actual params: [1.0424, 1.6869]
-Original Grad: -0.005, -lr * Pred Grad: -0.003, New P: 1.039
-Original Grad: -0.012, -lr * Pred Grad: -0.026, New P: 1.661
iter 29 loss: 0.002
Actual params: [1.0394, 1.661 ]
-Original Grad: -0.006, -lr * Pred Grad: -0.027, New P: 1.012
-Original Grad: 0.015, -lr * Pred Grad: 0.015, New P: 1.676
iter 30 loss: 0.002
Actual params: [1.0124, 1.676 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.030, New P: 0.983
-Original Grad: -0.004, -lr * Pred Grad: -0.026, New P: 1.650
Target params: [1.3344, 1.5708]
iter 0 loss: 0.443
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad: 0.652, New P: 0.180
-Original Grad: 0.001, -lr * Pred Grad: 0.639, New P: 0.642
iter 1 loss: 0.345
Actual params: [0.1799, 0.6424]
-Original Grad: 0.247, -lr * Pred Grad: 1.981, New P: 2.161
-Original Grad: 0.322, -lr * Pred Grad: 1.820, New P: 2.462
iter 2 loss: 0.036
Actual params: [2.1608, 2.4623]
-Original Grad: 0.099, -lr * Pred Grad: 0.584, New P: 2.745
-Original Grad: -0.342, -lr * Pred Grad: -1.015, New P: 1.448
iter 3 loss: 0.142
Actual params: [2.7449, 1.4476]
-Original Grad: -0.174, -lr * Pred Grad: -0.674, New P: 2.070
-Original Grad: 0.574, -lr * Pred Grad: 0.812, New P: 2.260
iter 4 loss: 0.015
Actual params: [2.0704, 2.2601]
-Original Grad: 0.013, -lr * Pred Grad: 0.051, New P: 2.122
-Original Grad: -0.114, -lr * Pred Grad: -0.221, New P: 2.039
iter 5 loss: 0.010
Actual params: [2.1216, 2.0386]
-Original Grad: -0.042, -lr * Pred Grad: -0.147, New P: 1.975
-Original Grad: 0.059, -lr * Pred Grad: 0.127, New P: 2.166
iter 6 loss: 0.011
Actual params: [1.975 , 2.1661]
-Original Grad: 0.034, -lr * Pred Grad: 0.021, New P: 1.996
-Original Grad: -0.142, -lr * Pred Grad: -0.235, New P: 1.931
iter 7 loss: 0.008
Actual params: [1.9964, 1.9314]
-Original Grad: -0.028, -lr * Pred Grad: -0.085, New P: 1.911
-Original Grad: 0.065, -lr * Pred Grad: -0.005, New P: 1.927
iter 8 loss: 0.007
Actual params: [1.9111, 1.9265]
-Original Grad: -0.013, -lr * Pred Grad: -0.089, New P: 1.822
-Original Grad: 0.040, -lr * Pred Grad: 0.078, New P: 2.005
iter 9 loss: 0.009
Actual params: [1.8225, 2.0047]
-Original Grad: 0.028, -lr * Pred Grad: 0.008, New P: 1.830
-Original Grad: -0.073, -lr * Pred Grad: -0.142, New P: 1.862
iter 10 loss: 0.007
Actual params: [1.8302, 1.8624]
-Original Grad: -0.005, -lr * Pred Grad: -0.032, New P: 1.798
-Original Grad: 0.026, -lr * Pred Grad: -0.016, New P: 1.846
iter 11 loss: 0.007
Actual params: [1.7979, 1.846 ]
-Original Grad: -0.006, -lr * Pred Grad: -0.048, New P: 1.750
-Original Grad: 0.044, -lr * Pred Grad: 0.083, New P: 1.929
iter 12 loss: 0.008
Actual params: [1.7496, 1.9289]
-Original Grad: 0.014, -lr * Pred Grad: -0.004, New P: 1.745
-Original Grad: -0.032, -lr * Pred Grad: -0.068, New P: 1.861
iter 13 loss: 0.007
Actual params: [1.7455, 1.8607]
-Original Grad: -0.007, -lr * Pred Grad: -0.042, New P: 1.704
-Original Grad: 0.027, -lr * Pred Grad: 0.028, New P: 1.889
iter 14 loss: 0.008
Actual params: [1.7035, 1.8886]
-Original Grad: 0.005, -lr * Pred Grad: -0.025, New P: 1.679
-Original Grad: -0.007, -lr * Pred Grad: -0.031, New P: 1.858
iter 15 loss: 0.008
Actual params: [1.6787, 1.8578]
-Original Grad: 0.020, -lr * Pred Grad: 0.025, New P: 1.704
-Original Grad: -0.062, -lr * Pred Grad: -0.153, New P: 1.705
iter 16 loss: 0.010
Actual params: [1.7039, 1.7048]
-Original Grad: -0.014, -lr * Pred Grad: -0.046, New P: 1.658
-Original Grad: 0.088, -lr * Pred Grad: 0.124, New P: 1.828
iter 17 loss: 0.008
Actual params: [1.6583, 1.8285]
-Original Grad: 0.004, -lr * Pred Grad: -0.025, New P: 1.633
-Original Grad: -0.005, -lr * Pred Grad: 0.002, New P: 1.830
iter 18 loss: 0.008
Actual params: [1.6332, 1.8301]
-Original Grad: 0.008, -lr * Pred Grad: -0.009, New P: 1.624
-Original Grad: -0.009, -lr * Pred Grad: -0.029, New P: 1.801
iter 19 loss: 0.008
Actual params: [1.6243, 1.8007]
-Original Grad: 0.009, -lr * Pred Grad: 0.003, New P: 1.627
-Original Grad: -0.013, -lr * Pred Grad: -0.061, New P: 1.740
iter 20 loss: 0.008
Actual params: [1.6273, 1.7397]
-Original Grad: -0.009, -lr * Pred Grad: -0.040, New P: 1.588
-Original Grad: 0.053, -lr * Pred Grad: 0.091, New P: 1.831
iter 21 loss: 0.009
Actual params: [1.5876, 1.8308]
-Original Grad: 0.007, -lr * Pred Grad: -0.014, New P: 1.573
-Original Grad: 0.002, -lr * Pred Grad: 0.013, New P: 1.844
iter 22 loss: 0.010
Actual params: [1.5732, 1.8437]
-Original Grad: 0.025, -lr * Pred Grad: 0.045, New P: 1.618
-Original Grad: -0.055, -lr * Pred Grad: -0.131, New P: 1.713
iter 23 loss: 0.008
Actual params: [1.618, 1.713]
-Original Grad: -0.020, -lr * Pred Grad: -0.050, New P: 1.568
-Original Grad: 0.116, -lr * Pred Grad: 0.225, New P: 1.938
iter 24 loss: 0.015
Actual params: [1.5677, 1.938 ]
-Original Grad: 0.031, -lr * Pred Grad: 0.054, New P: 1.622
-Original Grad: -0.104, -lr * Pred Grad: -0.206, New P: 1.732
iter 25 loss: 0.008
Actual params: [1.6218, 1.7324]
-Original Grad: -0.007, -lr * Pred Grad: -0.011, New P: 1.611
-Original Grad: 0.052, -lr * Pred Grad: 0.035, New P: 1.767
iter 26 loss: 0.007
Actual params: [1.6106, 1.7671]
-Original Grad: -0.005, -lr * Pred Grad: -0.027, New P: 1.584
-Original Grad: 0.040, -lr * Pred Grad: 0.087, New P: 1.854
iter 27 loss: 0.010
Actual params: [1.584, 1.854]
-Original Grad: 0.016, -lr * Pred Grad: 0.018, New P: 1.602
-Original Grad: -0.024, -lr * Pred Grad: -0.048, New P: 1.806
iter 28 loss: 0.008
Actual params: [1.6024, 1.8064]
-Original Grad: 0.003, -lr * Pred Grad: 0.002, New P: 1.605
-Original Grad: -0.002, -lr * Pred Grad: -0.034, New P: 1.772
iter 29 loss: 0.007
Actual params: [1.6048, 1.7724]
-Original Grad: -0.001, -lr * Pred Grad: -0.012, New P: 1.593
-Original Grad: 0.026, -lr * Pred Grad: 0.028, New P: 1.800
iter 30 loss: 0.008
Actual params: [1.5928, 1.8   ]
-Original Grad: 0.012, -lr * Pred Grad: 0.014, New P: 1.606
-Original Grad: -0.020, -lr * Pred Grad: -0.058, New P: 1.742
Target params: [1.3344, 1.5708]
iter 0 loss: 0.725
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.016, -lr * Pred Grad: 0.803, New P: 0.330
-Original Grad: 0.038, -lr * Pred Grad: 0.814, New P: 0.817
iter 1 loss: 0.516
Actual params: [0.3302, 0.8171]
-Original Grad: 0.344, -lr * Pred Grad: 2.224, New P: 2.554
-Original Grad: 0.386, -lr * Pred Grad: 2.078, New P: 2.895
iter 2 loss: 0.739
Actual params: [2.5541, 2.895 ]
-Original Grad: 0.054, -lr * Pred Grad: -0.037, New P: 2.517
-Original Grad: -0.708, -lr * Pred Grad: -0.994, New P: 1.901
iter 3 loss: 0.435
Actual params: [2.5168, 1.9014]
-Original Grad: -0.134, -lr * Pred Grad: -0.272, New P: 2.244
-Original Grad: -0.229, -lr * Pred Grad: -0.234, New P: 1.668
iter 4 loss: 0.381
Actual params: [2.2444, 1.6677]
-Original Grad: -0.244, -lr * Pred Grad: -0.827, New P: 1.418
-Original Grad: -0.015, -lr * Pred Grad: -0.457, New P: 1.211
iter 5 loss: 0.235
Actual params: [1.4175, 1.2107]
-Original Grad: -0.230, -lr * Pred Grad: -1.060, New P: 0.357
-Original Grad: 0.389, -lr * Pred Grad: 0.613, New P: 1.824
iter 6 loss: 0.442
Actual params: [0.3575, 1.8239]
-Original Grad: 0.801, -lr * Pred Grad: 0.577, New P: 0.935
-Original Grad: -0.093, -lr * Pred Grad: -0.233, New P: 1.590
iter 7 loss: 0.146
Actual params: [0.935 , 1.5905]
-Original Grad: 0.715, -lr * Pred Grad: 0.855, New P: 1.790
-Original Grad: 0.201, -lr * Pred Grad: 0.476, New P: 2.066
iter 8 loss: 0.232
Actual params: [1.7901, 2.0661]
-Original Grad: -0.888, -lr * Pred Grad: -0.945, New P: 0.845
-Original Grad: 0.028, -lr * Pred Grad: 0.214, New P: 2.280
iter 9 loss: 0.295
Actual params: [0.8452, 2.2796]
-Original Grad: 0.622, -lr * Pred Grad: 0.346, New P: 1.191
-Original Grad: -0.730, -lr * Pred Grad: -0.411, New P: 1.869
iter 10 loss: 0.071
Actual params: [1.1908, 1.8689]
-Original Grad: 0.071, -lr * Pred Grad: 0.042, New P: 1.233
-Original Grad: 0.041, -lr * Pred Grad: -0.384, New P: 1.485
iter 11 loss: 0.128
Actual params: [1.2327, 1.4846]
-Original Grad: -0.331, -lr * Pred Grad: -0.424, New P: 0.809
-Original Grad: 0.576, -lr * Pred Grad: 1.012, New P: 2.496
iter 12 loss: 0.399
Actual params: [0.8089, 2.4965]
-Original Grad: 0.841, -lr * Pred Grad: 0.710, New P: 1.519
-Original Grad: -0.995, -lr * Pred Grad: -0.504, New P: 1.992
iter 13 loss: 0.073
Actual params: [1.5189, 1.992 ]
-Original Grad: -0.358, -lr * Pred Grad: -0.472, New P: 1.047
-Original Grad: 0.437, -lr * Pred Grad: 0.021, New P: 2.013
iter 14 loss: 0.119
Actual params: [1.0473, 2.0128]
-Original Grad: 0.240, -lr * Pred Grad: 0.131, New P: 1.178
-Original Grad: -0.219, -lr * Pred Grad: -0.355, New P: 1.657
iter 15 loss: 0.086
Actual params: [1.1783, 1.6573]
-Original Grad: -0.040, -lr * Pred Grad: -0.079, New P: 1.099
-Original Grad: 0.412, -lr * Pred Grad: 0.567, New P: 2.224
iter 16 loss: 0.135
Actual params: [1.0992, 2.2239]
-Original Grad: 0.233, -lr * Pred Grad: 0.364, New P: 1.463
-Original Grad: -0.184, -lr * Pred Grad: -0.337, New P: 1.887
iter 17 loss: 0.076
Actual params: [1.4629, 1.8866]
-Original Grad: -0.312, -lr * Pred Grad: -0.448, New P: 1.015
-Original Grad: 0.453, -lr * Pred Grad: 1.009, New P: 2.896
iter 18 loss: 0.784
Actual params: [1.015 , 2.8958]
-Original Grad: -0.126, -lr * Pred Grad: -0.567, New P: 0.448
-Original Grad: -2.204, -lr * Pred Grad: -0.486, New P: 2.410
iter 19 loss: 0.508
Actual params: [0.4484, 2.4102]
-Original Grad: 0.518, -lr * Pred Grad: 0.465, New P: 0.914
-Original Grad: -0.808, -lr * Pred Grad: -0.499, New P: 1.912
iter 20 loss: 0.163
Actual params: [0.9139, 1.9116]
-Original Grad: 0.327, -lr * Pred Grad: 0.562, New P: 1.475
-Original Grad: -0.147, -lr * Pred Grad: -0.591, New P: 1.321
iter 21 loss: 0.223
Actual params: [1.4755, 1.3209]
-Original Grad: -0.167, -lr * Pred Grad: -0.104, New P: 1.371
-Original Grad: 0.412, -lr * Pred Grad: -0.110, New P: 1.211
iter 22 loss: 0.225
Actual params: [1.3711, 1.2113]
-Original Grad: -0.281, -lr * Pred Grad: -0.590, New P: 0.781
-Original Grad: 0.455, -lr * Pred Grad: 0.941, New P: 2.153
iter 23 loss: 0.281
Actual params: [0.7812, 2.1526]
-Original Grad: 0.653, -lr * Pred Grad: 0.647, New P: 1.429
-Original Grad: -0.436, -lr * Pred Grad: -0.527, New P: 1.626
iter 24 loss: 0.143
Actual params: [1.4287, 1.6258]
-Original Grad: -0.333, -lr * Pred Grad: -0.470, New P: 0.959
-Original Grad: 0.676, -lr * Pred Grad: 0.851, New P: 2.477
iter 25 loss: 0.304
Actual params: [0.9591, 2.4767]
-Original Grad: 1.651, -lr * Pred Grad: 0.997, New P: 1.956
-Original Grad: -1.077, -lr * Pred Grad: -0.419, New P: 2.057
iter 26 loss: 0.335
Actual params: [1.9562, 2.0574]
-Original Grad: -0.530, -lr * Pred Grad: -0.704, New P: 1.252
-Original Grad: -0.210, -lr * Pred Grad: -0.442, New P: 1.616
iter 27 loss: 0.098
Actual params: [1.2518, 1.6156]
-Original Grad: -0.249, -lr * Pred Grad: -0.753, New P: 0.499
-Original Grad: 0.693, -lr * Pred Grad: 0.581, New P: 2.196
iter 28 loss: 0.418
Actual params: [0.499 , 2.1962]
-Original Grad: 0.850, -lr * Pred Grad: 0.340, New P: 0.839
-Original Grad: -0.505, -lr * Pred Grad: -0.434, New P: 1.762
iter 29 loss: 0.191
Actual params: [0.8387, 1.7621]
-Original Grad: 0.500, -lr * Pred Grad: 0.516, New P: 1.355
-Original Grad: -0.012, -lr * Pred Grad: -0.337, New P: 1.425
iter 30 loss: 0.181
Actual params: [1.3551, 1.4246]
-Original Grad: -0.199, -lr * Pred Grad: -0.120, New P: 1.235
-Original Grad: 0.337, -lr * Pred Grad: 0.377, New P: 1.801
Target params: [1.3344, 1.5708]
iter 0 loss: 0.228
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad: 0.657, New P: 0.184
-Original Grad: 0.001, -lr * Pred Grad: 0.637, New P: 0.640
iter 1 loss: 0.111
Actual params: [0.1844, 0.64  ]
-Original Grad: 0.307, -lr * Pred Grad: 2.123, New P: 2.307
-Original Grad: 0.148, -lr * Pred Grad: 0.984, New P: 1.624
iter 2 loss: 0.066
Actual params: [2.3071, 1.6245]
-Original Grad: -0.028, -lr * Pred Grad: -0.503, New P: 1.804
-Original Grad: -0.187, -lr * Pred Grad: -0.740, New P: 0.884
iter 3 loss: 0.030
Actual params: [1.8042, 0.8842]
-Original Grad: 0.047, -lr * Pred Grad: 0.561, New P: 2.365
-Original Grad: 0.129, -lr * Pred Grad: 0.176, New P: 1.061
iter 4 loss: 0.033
Actual params: [2.3651, 1.0605]
-Original Grad: -0.023, -lr * Pred Grad: 0.013, New P: 2.378
-Original Grad: -0.078, -lr * Pred Grad: -0.201, New P: 0.860
iter 5 loss: 0.029
Actual params: [2.3779, 0.8595]
-Original Grad: -0.016, -lr * Pred Grad: 0.058, New P: 2.436
-Original Grad: -0.027, -lr * Pred Grad: -0.118, New P: 0.741
iter 6 loss: 0.030
Actual params: [2.4363, 0.7412]
-Original Grad: -0.014, -lr * Pred Grad: 0.001, New P: 2.437
-Original Grad: 0.007, -lr * Pred Grad: -0.082, New P: 0.659
iter 7 loss: 0.032
Actual params: [2.437 , 0.6595]
-Original Grad: 0.000, -lr * Pred Grad: 0.009, New P: 2.446
-Original Grad: 0.060, -lr * Pred Grad: 0.098, New P: 0.757
iter 8 loss: 0.030
Actual params: [2.4461, 0.7571]
-Original Grad: -0.008, -lr * Pred Grad: -0.021, New P: 2.425
-Original Grad: 0.027, -lr * Pred Grad: 0.085, New P: 0.842
iter 9 loss: 0.030
Actual params: [2.4247, 0.8421]
-Original Grad: -0.015, -lr * Pred Grad: -0.057, New P: 2.367
-Original Grad: -0.013, -lr * Pred Grad: -0.020, New P: 0.822
iter 10 loss: 0.029
Actual params: [2.3674, 0.8223]
-Original Grad: -0.011, -lr * Pred Grad: -0.072, New P: 2.295
-Original Grad: -0.004, -lr * Pred Grad: -0.030, New P: 0.792
iter 11 loss: 0.028
Actual params: [2.2952, 0.7921]
-Original Grad: -0.003, -lr * Pred Grad: -0.066, New P: 2.229
-Original Grad: 0.041, -lr * Pred Grad: 0.074, New P: 0.866
iter 12 loss: 0.027
Actual params: [2.2294, 0.8662]
-Original Grad: -0.008, -lr * Pred Grad: -0.081, New P: 2.148
-Original Grad: 0.007, -lr * Pred Grad: 0.024, New P: 0.891
iter 13 loss: 0.027
Actual params: [2.1483, 0.8907]
-Original Grad: -0.011, -lr * Pred Grad: -0.097, New P: 2.051
-Original Grad: -0.011, -lr * Pred Grad: -0.032, New P: 0.859
iter 14 loss: 0.026
Actual params: [2.0513, 0.8587]
-Original Grad: -0.010, -lr * Pred Grad: -0.107, New P: 1.945
-Original Grad: 0.018, -lr * Pred Grad: 0.014, New P: 0.873
iter 15 loss: 0.026
Actual params: [1.9447, 0.8731]
-Original Grad: 0.026, -lr * Pred Grad: -0.014, New P: 1.931
-Original Grad: 0.096, -lr * Pred Grad: 0.269, New P: 1.142
iter 16 loss: 0.026
Actual params: [1.9308, 1.1422]
-Original Grad: -0.032, -lr * Pred Grad: -0.125, New P: 1.806
-Original Grad: -0.081, -lr * Pred Grad: -0.164, New P: 0.979
iter 17 loss: 0.024
Actual params: [1.8061, 0.9787]
-Original Grad: 0.006, -lr * Pred Grad: -0.073, New P: 1.733
-Original Grad: 0.050, -lr * Pred Grad: 0.064, New P: 1.043
iter 18 loss: 0.023
Actual params: [1.7331, 1.0429]
-Original Grad: 0.011, -lr * Pred Grad: -0.037, New P: 1.696
-Original Grad: 0.062, -lr * Pred Grad: 0.170, New P: 1.213
iter 19 loss: 0.021
Actual params: [1.6957, 1.2134]
-Original Grad: -0.039, -lr * Pred Grad: -0.158, New P: 1.538
-Original Grad: -0.062, -lr * Pred Grad: -0.122, New P: 1.092
iter 20 loss: 0.030
Actual params: [1.5381, 1.0917]
-Original Grad: 0.076, -lr * Pred Grad: 0.102, New P: 1.640
-Original Grad: 0.205, -lr * Pred Grad: 0.552, New P: 1.644
iter 21 loss: 0.036
Actual params: [1.6402, 1.6436]
-Original Grad: -0.047, -lr * Pred Grad: -0.118, New P: 1.523
-Original Grad: -0.150, -lr * Pred Grad: -0.314, New P: 1.329
iter 22 loss: 0.017
Actual params: [1.5226, 1.3294]
-Original Grad: -0.024, -lr * Pred Grad: -0.144, New P: 1.379
-Original Grad: -0.050, -lr * Pred Grad: -0.140, New P: 1.189
iter 23 loss: 0.027
Actual params: [1.379 , 1.1892]
-Original Grad: 0.031, -lr * Pred Grad: -0.016, New P: 1.363
-Original Grad: 0.273, -lr * Pred Grad: 0.652, New P: 1.841
iter 24 loss: 0.042
Actual params: [1.3628, 1.8408]
-Original Grad: -0.053, -lr * Pred Grad: -0.190, New P: 1.172
-Original Grad: -0.272, -lr * Pred Grad: -0.458, New P: 1.383
iter 25 loss: 0.012
Actual params: [1.1723, 1.3832]
-Original Grad: 0.006, -lr * Pred Grad: -0.115, New P: 1.058
-Original Grad: 0.049, -lr * Pred Grad: -0.063, New P: 1.320
iter 26 loss: 0.017
Actual params: [1.0575, 1.3201]
-Original Grad: 0.015, -lr * Pred Grad: -0.057, New P: 1.001
-Original Grad: 0.176, -lr * Pred Grad: 0.386, New P: 1.706
iter 27 loss: 0.021
Actual params: [1.0006, 1.7064]
-Original Grad: 0.008, -lr * Pred Grad: -0.041, New P: 0.960
-Original Grad: -0.156, -lr * Pred Grad: -0.286, New P: 1.421
iter 28 loss: 0.016
Actual params: [0.9596, 1.4208]
-Original Grad: 0.010, -lr * Pred Grad: -0.018, New P: 0.942
-Original Grad: 0.083, -lr * Pred Grad: 0.073, New P: 1.494
iter 29 loss: 0.015
Actual params: [0.9419, 1.4938]
-Original Grad: 0.008, -lr * Pred Grad: -0.006, New P: 0.936
-Original Grad: 0.011, -lr * Pred Grad: 0.016, New P: 1.510
iter 30 loss: 0.015
Actual params: [0.9362, 1.5098]
-Original Grad: 0.045, -lr * Pred Grad: 0.109, New P: 1.045
-Original Grad: -0.005, -lr * Pred Grad: -0.012, New P: 1.498
Target params: [1.3344, 1.5708]
iter 0 loss: 0.466
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad: 0.636, New P: 0.163
-Original Grad: 0.001, -lr * Pred Grad: 0.636, New P: 0.640
iter 1 loss: 0.394
Actual params: [0.1634, 0.6398]
-Original Grad: 0.664, -lr * Pred Grad: 2.461, New P: 2.625
-Original Grad: 0.773, -lr * Pred Grad: 2.681, New P: 3.321
iter 2 loss: 0.438
Actual params: [2.6246, 3.3206]
-Original Grad: -0.009, -lr * Pred Grad: -0.887, New P: 1.738
-Original Grad: -0.439, -lr * Pred Grad: -1.081, New P: 2.239
iter 3 loss: 0.135
Actual params: [1.7376, 2.2394]
-Original Grad: -0.068, -lr * Pred Grad: 0.075, New P: 1.813
-Original Grad: -0.390, -lr * Pred Grad: -0.232, New P: 2.008
iter 4 loss: 0.100
Actual params: [1.8125, 2.0076]
-Original Grad: -0.103, -lr * Pred Grad: -0.244, New P: 1.569
-Original Grad: -0.259, -lr * Pred Grad: -0.546, New P: 1.462
iter 5 loss: 0.032
Actual params: [1.5688, 1.4615]
-Original Grad: -0.117, -lr * Pred Grad: -0.398, New P: 1.171
-Original Grad: -0.249, -lr * Pred Grad: -0.512, New P: 0.950
iter 6 loss: 0.150
Actual params: [1.1712, 0.9497]
-Original Grad: 0.098, -lr * Pred Grad: -0.001, New P: 1.170
-Original Grad: 0.599, -lr * Pred Grad: 0.769, New P: 1.719
iter 7 loss: 0.025
Actual params: [1.1702, 1.7186]
-Original Grad: -0.038, -lr * Pred Grad: -0.144, New P: 1.026
-Original Grad: -0.074, -lr * Pred Grad: -0.277, New P: 1.442
iter 8 loss: 0.028
Actual params: [1.0262, 1.442 ]
-Original Grad: -0.006, -lr * Pred Grad: -0.118, New P: 0.908
-Original Grad: 0.114, -lr * Pred Grad: 0.230, New P: 1.672
iter 9 loss: 0.023
Actual params: [0.9081, 1.672 ]
-Original Grad: -0.016, -lr * Pred Grad: -0.137, New P: 0.771
-Original Grad: -0.057, -lr * Pred Grad: -0.105, New P: 1.567
iter 10 loss: 0.028
Actual params: [0.7715, 1.5671]
-Original Grad: 0.055, -lr * Pred Grad: 0.036, New P: 0.807
-Original Grad: -0.005, -lr * Pred Grad: -0.041, New P: 1.526
iter 11 loss: 0.027
Actual params: [0.8073, 1.5257]
-Original Grad: 0.002, -lr * Pred Grad: -0.008, New P: 0.800
-Original Grad: 0.016, -lr * Pred Grad: -0.010, New P: 1.516
iter 12 loss: 0.027
Actual params: [0.7996, 1.5155]
-Original Grad: 0.018, -lr * Pred Grad: 0.034, New P: 0.833
-Original Grad: 0.022, -lr * Pred Grad: 0.037, New P: 1.553
iter 13 loss: 0.026
Actual params: [0.8332, 1.5528]
-Original Grad: 0.003, -lr * Pred Grad: 0.012, New P: 0.845
-Original Grad: 0.019, -lr * Pred Grad: 0.044, New P: 1.596
iter 14 loss: 0.025
Actual params: [0.8451, 1.5965]
-Original Grad: 0.017, -lr * Pred Grad: 0.046, New P: 0.891
-Original Grad: -0.008, -lr * Pred Grad: -0.021, New P: 1.575
iter 15 loss: 0.025
Actual params: [0.8909, 1.575 ]
-Original Grad: 0.000, -lr * Pred Grad: 0.012, New P: 0.903
-Original Grad: 0.025, -lr * Pred Grad: 0.039, New P: 1.614
iter 16 loss: 0.024
Actual params: [0.9027, 1.6137]
-Original Grad: 0.043, -lr * Pred Grad: 0.120, New P: 1.023
-Original Grad: -0.040, -lr * Pred Grad: -0.101, New P: 1.513
iter 17 loss: 0.024
Actual params: [1.0225, 1.5131]
-Original Grad: 0.005, -lr * Pred Grad: 0.061, New P: 1.083
-Original Grad: 0.044, -lr * Pred Grad: 0.051, New P: 1.564
iter 18 loss: 0.022
Actual params: [1.0831, 1.5643]
-Original Grad: -0.011, -lr * Pred Grad: 0.000, New P: 1.083
-Original Grad: 0.001, -lr * Pred Grad: -0.005, New P: 1.559
iter 19 loss: 0.022
Actual params: [1.0834, 1.5595]
-Original Grad: -0.012, -lr * Pred Grad: -0.036, New P: 1.047
-Original Grad: -0.010, -lr * Pred Grad: -0.039, New P: 1.520
iter 20 loss: 0.023
Actual params: [1.0472, 1.5201]
-Original Grad: -0.017, -lr * Pred Grad: -0.079, New P: 0.968
-Original Grad: 0.036, -lr * Pred Grad: 0.055, New P: 1.575
iter 21 loss: 0.023
Actual params: [0.9684, 1.5754]
-Original Grad: -0.007, -lr * Pred Grad: -0.080, New P: 0.889
-Original Grad: -0.009, -lr * Pred Grad: -0.025, New P: 1.550
iter 22 loss: 0.026
Actual params: [0.8887, 1.5505]
-Original Grad: 0.039, -lr * Pred Grad: 0.040, New P: 0.929
-Original Grad: 0.051, -lr * Pred Grad: 0.112, New P: 1.662
iter 23 loss: 0.023
Actual params: [0.929 , 1.6623]
-Original Grad: 0.022, -lr * Pred Grad: 0.055, New P: 0.985
-Original Grad: -0.043, -lr * Pred Grad: -0.093, New P: 1.569
iter 24 loss: 0.023
Actual params: [0.9845, 1.5689]
-Original Grad: 0.006, -lr * Pred Grad: 0.036, New P: 1.020
-Original Grad: 0.002, -lr * Pred Grad: -0.039, New P: 1.530
iter 25 loss: 0.023
Actual params: [1.0203, 1.53  ]
-Original Grad: 0.021, -lr * Pred Grad: 0.072, New P: 1.093
-Original Grad: 0.035, -lr * Pred Grad: 0.047, New P: 1.577
iter 26 loss: 0.022
Actual params: [1.0925, 1.5765]
-Original Grad: -0.033, -lr * Pred Grad: -0.066, New P: 1.026
-Original Grad: -0.027, -lr * Pred Grad: -0.069, New P: 1.508
iter 27 loss: 0.024
Actual params: [1.0261, 1.5077]
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: 0.988
-Original Grad: 0.046, -lr * Pred Grad: 0.074, New P: 1.582
iter 28 loss: 0.023
Actual params: [0.988 , 1.5821]
-Original Grad: 0.012, -lr * Pred Grad: -0.006, New P: 0.982
-Original Grad: 0.005, -lr * Pred Grad: 0.016, New P: 1.598
iter 29 loss: 0.022
Actual params: [0.9817, 1.5984]
-Original Grad: 0.007, -lr * Pred Grad: -0.003, New P: 0.979
-Original Grad: -0.003, -lr * Pred Grad: -0.016, New P: 1.582
iter 30 loss: 0.023
Actual params: [0.9788, 1.5823]
-Original Grad: 0.035, -lr * Pred Grad: 0.083, New P: 1.062
-Original Grad: -0.001, -lr * Pred Grad: -0.027, New P: 1.555
Target params: [1.3344, 1.5708]
iter 0 loss: 0.228
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad: 0.664, New P: 0.192
-Original Grad: 0.001, -lr * Pred Grad: 0.638, New P: 0.642
iter 1 loss: 0.108
Actual params: [0.1918, 0.6418]
-Original Grad: 0.370, -lr * Pred Grad: 2.229, New P: 2.421
-Original Grad: 0.160, -lr * Pred Grad: 1.052, New P: 1.694
iter 2 loss: 0.076
Actual params: [2.4208, 1.6941]
-Original Grad: -0.024, -lr * Pred Grad: -0.593, New P: 1.828
-Original Grad: -0.213, -lr * Pred Grad: -0.790, New P: 0.904
iter 3 loss: 0.028
Actual params: [1.8278, 0.9041]
-Original Grad: 0.042, -lr * Pred Grad: 0.524, New P: 2.351
-Original Grad: 0.130, -lr * Pred Grad: 0.159, New P: 1.063
iter 4 loss: 0.033
Actual params: [2.3514, 1.0635]
-Original Grad: -0.019, -lr * Pred Grad: 0.031, New P: 2.382
-Original Grad: -0.061, -lr * Pred Grad: -0.181, New P: 0.883
iter 5 loss: 0.029
Actual params: [2.382 , 0.8825]
-Original Grad: -0.012, -lr * Pred Grad: 0.079, New P: 2.461
-Original Grad: -0.012, -lr * Pred Grad: -0.082, New P: 0.801
iter 6 loss: 0.030
Actual params: [2.4612, 0.8007]
-Original Grad: -0.013, -lr * Pred Grad: 0.017, New P: 2.478
-Original Grad: 0.007, -lr * Pred Grad: -0.060, New P: 0.741
iter 7 loss: 0.030
Actual params: [2.4779, 0.7409]
-Original Grad: -0.004, -lr * Pred Grad: 0.008, New P: 2.486
-Original Grad: 0.026, -lr * Pred Grad: 0.022, New P: 0.763
iter 8 loss: 0.030
Actual params: [2.4855, 0.7631]
-Original Grad: -0.006, -lr * Pred Grad: -0.014, New P: 2.471
-Original Grad: 0.017, -lr * Pred Grad: 0.031, New P: 0.795
iter 9 loss: 0.030
Actual params: [2.4714, 0.7946]
-Original Grad: -0.013, -lr * Pred Grad: -0.049, New P: 2.422
-Original Grad: -0.012, -lr * Pred Grad: -0.034, New P: 0.760
iter 10 loss: 0.030
Actual params: [2.4221, 0.7602]
-Original Grad: -0.009, -lr * Pred Grad: -0.063, New P: 2.359
-Original Grad: 0.024, -lr * Pred Grad: 0.030, New P: 0.790
iter 11 loss: 0.029
Actual params: [2.3594, 0.7903]
-Original Grad: -0.008, -lr * Pred Grad: -0.074, New P: 2.286
-Original Grad: 0.007, -lr * Pred Grad: 0.008, New P: 0.799
iter 12 loss: 0.028
Actual params: [2.2856, 0.7986]
-Original Grad: -0.004, -lr * Pred Grad: -0.072, New P: 2.214
-Original Grad: 0.027, -lr * Pred Grad: 0.057, New P: 0.856
iter 13 loss: 0.027
Actual params: [2.2138, 0.8558]
-Original Grad: -0.003, -lr * Pred Grad: -0.072, New P: 2.142
-Original Grad: 0.028, -lr * Pred Grad: 0.077, New P: 0.933
iter 14 loss: 0.027
Actual params: [2.1418, 0.9328]
-Original Grad: -0.011, -lr * Pred Grad: -0.094, New P: 2.048
-Original Grad: -0.014, -lr * Pred Grad: -0.028, New P: 0.904
iter 15 loss: 0.026
Actual params: [2.0476, 0.9045]
-Original Grad: -0.010, -lr * Pred Grad: -0.106, New P: 1.942
-Original Grad: 0.001, -lr * Pred Grad: -0.020, New P: 0.884
iter 16 loss: 0.026
Actual params: [1.9419, 0.884 ]
-Original Grad: 0.008, -lr * Pred Grad: -0.064, New P: 1.878
-Original Grad: 0.045, -lr * Pred Grad: 0.090, New P: 0.974
iter 17 loss: 0.024
Actual params: [1.878 , 0.9739]
-Original Grad: -0.003, -lr * Pred Grad: -0.073, New P: 1.805
-Original Grad: 0.033, -lr * Pred Grad: 0.106, New P: 1.080
iter 18 loss: 0.022
Actual params: [1.8052, 1.0797]
-Original Grad: -0.014, -lr * Pred Grad: -0.105, New P: 1.700
-Original Grad: -0.010, -lr * Pred Grad: -0.010, New P: 1.069
iter 19 loss: 0.023
Actual params: [1.6998, 1.0692]
-Original Grad: 0.036, -lr * Pred Grad: 0.020, New P: 1.720
-Original Grad: 0.114, -lr * Pred Grad: 0.325, New P: 1.395
iter 20 loss: 0.027
Actual params: [1.7199, 1.3946]
-Original Grad: -0.032, -lr * Pred Grad: -0.109, New P: 1.611
-Original Grad: -0.071, -lr * Pred Grad: -0.141, New P: 1.253
iter 21 loss: 0.019
Actual params: [1.6111, 1.2534]
-Original Grad: -0.024, -lr * Pred Grad: -0.147, New P: 1.464
-Original Grad: 0.021, -lr * Pred Grad: 0.014, New P: 1.268
iter 22 loss: 0.017
Actual params: [1.4644, 1.2678]
-Original Grad: 0.024, -lr * Pred Grad: -0.040, New P: 1.424
-Original Grad: 0.110, -lr * Pred Grad: 0.300, New P: 1.568
iter 23 loss: 0.023
Actual params: [1.4241, 1.568 ]
-Original Grad: -0.047, -lr * Pred Grad: -0.189, New P: 1.235
-Original Grad: -0.150, -lr * Pred Grad: -0.279, New P: 1.289
iter 24 loss: 0.017
Actual params: [1.2354, 1.2888]
-Original Grad: 0.005, -lr * Pred Grad: -0.119, New P: 1.117
-Original Grad: 0.159, -lr * Pred Grad: 0.254, New P: 1.543
iter 25 loss: 0.013
Actual params: [1.1166, 1.5427]
-Original Grad: -0.007, -lr * Pred Grad: -0.124, New P: 0.993
-Original Grad: -0.047, -lr * Pred Grad: -0.079, New P: 1.464
iter 26 loss: 0.014
Actual params: [0.993, 1.464]
-Original Grad: 0.038, -lr * Pred Grad: 0.006, New P: 0.999
-Original Grad: 0.026, -lr * Pred Grad: 0.042, New P: 1.506
iter 27 loss: 0.014
Actual params: [0.9991, 1.5065]
-Original Grad: 0.007, -lr * Pred Grad: -0.008, New P: 0.991
-Original Grad: 0.014, -lr * Pred Grad: 0.026, New P: 1.532
iter 28 loss: 0.014
Actual params: [0.9911, 1.5323]
-Original Grad: 0.036, -lr * Pred Grad: 0.085, New P: 1.076
-Original Grad: -0.042, -lr * Pred Grad: -0.100, New P: 1.433
iter 29 loss: 0.012
Actual params: [1.0757, 1.4325]
-Original Grad: 0.011, -lr * Pred Grad: 0.059, New P: 1.135
-Original Grad: 0.008, -lr * Pred Grad: -0.038, New P: 1.394
iter 30 loss: 0.012
Actual params: [1.135 , 1.3943]
-Original Grad: 0.015, -lr * Pred Grad: 0.073, New P: 1.208
-Original Grad: 0.084, -lr * Pred Grad: 0.189, New P: 1.584
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.054, -lr * Pred Grad: 1.154, New P: 0.682
-Original Grad: 0.012, -lr * Pred Grad: 0.690, New P: 0.693
iter 1 loss: 0.200
Actual params: [0.6816, 0.693 ]
-Original Grad: -0.099, -lr * Pred Grad: -0.677, New P: 0.005
-Original Grad: -0.450, -lr * Pred Grad: -0.681, New P: 0.012
iter 2 loss: 0.060
Actual params: [0.0047, 0.0119]
-Original Grad: 0.068, -lr * Pred Grad: 0.441, New P: 0.445
-Original Grad: 0.024, -lr * Pred Grad: -0.187, New P: -0.175
iter 3 loss: 0.042
Actual params: [ 0.4454, -0.1751]
-Original Grad: 0.019, -lr * Pred Grad: 0.132, New P: 0.577
-Original Grad: 0.076, -lr * Pred Grad: -0.084, New P: -0.259
iter 4 loss: 0.047
Actual params: [ 0.5773, -0.2586]
-Original Grad: -0.021, -lr * Pred Grad: 0.037, New P: 0.615
-Original Grad: -0.016, -lr * Pred Grad: -0.089, New P: -0.347
iter 5 loss: 0.048
Actual params: [ 0.6146, -0.3473]
-Original Grad: 0.001, -lr * Pred Grad: 0.047, New P: 0.661
-Original Grad: 0.052, -lr * Pred Grad: 0.044, New P: -0.303
iter 6 loss: 0.050
Actual params: [ 0.6612, -0.3028]
-Original Grad: 0.001, -lr * Pred Grad: 0.025, New P: 0.687
-Original Grad: -0.030, -lr * Pred Grad: -0.087, New P: -0.389
iter 7 loss: 0.050
Actual params: [ 0.6867, -0.3894]
-Original Grad: 0.012, -lr * Pred Grad: 0.048, New P: 0.734
-Original Grad: 0.079, -lr * Pred Grad: 0.148, New P: -0.242
iter 8 loss: 0.053
Actual params: [ 0.7342, -0.2418]
-Original Grad: -0.019, -lr * Pred Grad: -0.042, New P: 0.692
-Original Grad: -0.085, -lr * Pred Grad: -0.178, New P: -0.420
iter 9 loss: 0.050
Actual params: [ 0.6922, -0.4199]
-Original Grad: -0.006, -lr * Pred Grad: -0.045, New P: 0.647
-Original Grad: 0.057, -lr * Pred Grad: 0.049, New P: -0.371
iter 10 loss: 0.049
Actual params: [ 0.6474, -0.3713]
-Original Grad: 0.002, -lr * Pred Grad: -0.032, New P: 0.616
-Original Grad: 0.059, -lr * Pred Grad: 0.153, New P: -0.218
iter 11 loss: 0.050
Actual params: [ 0.6156, -0.2185]
-Original Grad: -0.072, -lr * Pred Grad: -0.243, New P: 0.372
-Original Grad: -0.098, -lr * Pred Grad: -0.196, New P: -0.414
iter 12 loss: 0.052
Actual params: [ 0.3721, -0.414 ]
-Original Grad: 0.026, -lr * Pred Grad: -0.085, New P: 0.287
-Original Grad: 0.072, -lr * Pred Grad: 0.075, New P: -0.339
iter 13 loss: 0.052
Actual params: [ 0.2874, -0.3393]
-Original Grad: 0.035, -lr * Pred Grad: 0.009, New P: 0.296
-Original Grad: 0.067, -lr * Pred Grad: 0.196, New P: -0.144
iter 14 loss: 0.041
Actual params: [ 0.2962, -0.1437]
-Original Grad: 0.043, -lr * Pred Grad: 0.096, New P: 0.393
-Original Grad: 0.092, -lr * Pred Grad: 0.366, New P: 0.222
iter 15 loss: 0.048
Actual params: [0.3927, 0.2219]
-Original Grad: -0.164, -lr * Pred Grad: -0.422, New P: -0.029
-Original Grad: -0.180, -lr * Pred Grad: -0.345, New P: -0.123
iter 16 loss: 0.070
Actual params: [-0.0289, -0.1227]
-Original Grad: 0.084, -lr * Pred Grad: -0.033, New P: -0.062
-Original Grad: 0.057, -lr * Pred Grad: -0.016, New P: -0.138
iter 17 loss: 0.075
Actual params: [-0.0616, -0.1382]
-Original Grad: 0.088, -lr * Pred Grad: 0.140, New P: 0.079
-Original Grad: 0.061, -lr * Pred Grad: 0.106, New P: -0.032
iter 18 loss: 0.052
Actual params: [ 0.0789, -0.0319]
-Original Grad: 0.080, -lr * Pred Grad: 0.262, New P: 0.341
-Original Grad: 0.031, -lr * Pred Grad: 0.113, New P: 0.081
iter 19 loss: 0.035
Actual params: [0.3408, 0.0814]
-Original Grad: 0.045, -lr * Pred Grad: 0.261, New P: 0.602
-Original Grad: 0.060, -lr * Pred Grad: 0.206, New P: 0.287
iter 20 loss: 0.098
Actual params: [0.602 , 0.2872]
-Original Grad: -0.147, -lr * Pred Grad: -0.293, New P: 0.309
-Original Grad: -0.216, -lr * Pred Grad: -0.341, New P: -0.054
iter 21 loss: 0.037
Actual params: [ 0.3087, -0.0539]
-Original Grad: 0.041, -lr * Pred Grad: -0.020, New P: 0.289
-Original Grad: 0.048, -lr * Pred Grad: -0.099, New P: -0.153
iter 22 loss: 0.041
Actual params: [ 0.2891, -0.1527]
-Original Grad: 0.028, -lr * Pred Grad: 0.025, New P: 0.314
-Original Grad: 0.064, -lr * Pred Grad: 0.073, New P: -0.079
iter 23 loss: 0.038
Actual params: [ 0.3138, -0.0795]
-Original Grad: 0.044, -lr * Pred Grad: 0.112, New P: 0.426
-Original Grad: 0.045, -lr * Pred Grad: 0.137, New P: 0.058
iter 24 loss: 0.042
Actual params: [0.4257, 0.0575]
-Original Grad: -0.040, -lr * Pred Grad: -0.065, New P: 0.361
-Original Grad: 0.010, -lr * Pred Grad: 0.053, New P: 0.111
iter 25 loss: 0.036
Actual params: [0.3607, 0.1107]
-Original Grad: 0.018, -lr * Pred Grad: 0.018, New P: 0.379
-Original Grad: 0.022, -lr * Pred Grad: 0.066, New P: 0.177
iter 26 loss: 0.041
Actual params: [0.3786, 0.1767]
-Original Grad: -0.106, -lr * Pred Grad: -0.303, New P: 0.075
-Original Grad: -0.092, -lr * Pred Grad: -0.191, New P: -0.015
iter 27 loss: 0.052
Actual params: [ 0.0751, -0.0147]
-Original Grad: 0.082, -lr * Pred Grad: 0.029, New P: 0.104
-Original Grad: 0.042, -lr * Pred Grad: -0.007, New P: -0.022
iter 28 loss: 0.049
Actual params: [ 0.1043, -0.0218]
-Original Grad: 0.087, -lr * Pred Grad: 0.183, New P: 0.287
-Original Grad: 0.088, -lr * Pred Grad: 0.219, New P: 0.197
iter 29 loss: 0.033
Actual params: [0.2869, 0.1971]
-Original Grad: -0.019, -lr * Pred Grad: 0.028, New P: 0.315
-Original Grad: -0.012, -lr * Pred Grad: 0.009, New P: 0.206
iter 30 loss: 0.035
Actual params: [0.3146, 0.2059]
-Original Grad: -0.052, -lr * Pred Grad: -0.125, New P: 0.189
-Original Grad: -0.050, -lr * Pred Grad: -0.114, New P: 0.092
Target params: [1.3344, 1.5708]
iter 0 loss: 0.008
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.015, -lr * Pred Grad: 0.797, New P: 0.325
-Original Grad: -0.009, -lr * Pred Grad: 0.592, New P: 0.595
iter 1 loss: 0.027
Actual params: [0.3248, 0.5952]
-Original Grad: -0.411, -lr * Pred Grad: -1.873, New P: -1.549
-Original Grad: 0.258, -lr * Pred Grad: 1.542, New P: 2.137
iter 2 loss: 0.010
Actual params: [-1.5486,  2.1368]
-Original Grad: -0.000, -lr * Pred Grad: -1.019, New P: -2.568
-Original Grad: -0.000, -lr * Pred Grad: -0.180, New P: 1.956
iter 3 loss: 0.010
Actual params: [-2.5677,  1.9564]
-Original Grad: 0.000, -lr * Pred Grad: -0.857, New P: -3.425
-Original Grad: -0.000, -lr * Pred Grad: -0.066, New P: 1.890
iter 4 loss: 0.010
Actual params: [-3.425 ,  1.8901]
-Original Grad: 0.000, -lr * Pred Grad: -0.721, New P: -4.146
-Original Grad: -0.000, -lr * Pred Grad: -0.023, New P: 1.868
iter 5 loss: 0.010
Actual params: [-4.1459,  1.8675]
-Original Grad: -0.000, -lr * Pred Grad: -0.653, New P: -4.799
-Original Grad: -0.000, -lr * Pred Grad: -0.036, New P: 1.831
iter 6 loss: 0.010
Actual params: [-4.7993,  1.8314]
-Original Grad: -0.000, -lr * Pred Grad: -0.582, New P: -5.381
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.799
iter 7 loss: 0.010
Actual params: [-5.3809,  1.7994]
-Original Grad: -0.000, -lr * Pred Grad: -0.511, New P: -5.892
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 1.770
iter 8 loss: 0.010
Actual params: [-5.8919,  1.7704]
-Original Grad: -0.000, -lr * Pred Grad: -0.443, New P: -6.334
-Original Grad: -0.000, -lr * Pred Grad: -0.026, New P: 1.744
iter 9 loss: 0.010
Actual params: [-6.3345,  1.7439]
-Original Grad: -0.000, -lr * Pred Grad: -0.378, New P: -6.712
-Original Grad: -0.000, -lr * Pred Grad: -0.026, New P: 1.718
iter 10 loss: 0.010
Actual params: [-6.7121,  1.7179]
-Original Grad: -0.000, -lr * Pred Grad: -0.318, New P: -7.030
-Original Grad: -0.000, -lr * Pred Grad: -0.026, New P: 1.691
iter 11 loss: 0.010
Actual params: [-7.0303,  1.6915]
-Original Grad: -0.000, -lr * Pred Grad: -0.265, New P: -7.295
-Original Grad: -0.000, -lr * Pred Grad: -0.027, New P: 1.664
iter 12 loss: 0.010
Actual params: [-7.2955,  1.6642]
-Original Grad: 0.000, -lr * Pred Grad: -0.220, New P: -7.515
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 1.636
iter 13 loss: 0.010
Actual params: [-7.5151,  1.636 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.182, New P: -7.697
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 1.607
iter 14 loss: 0.010
Actual params: [-7.6966,  1.607 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.151, New P: -7.847
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 1.577
iter 15 loss: 0.010
Actual params: [-7.8473,  1.5773]
-Original Grad: 0.000, -lr * Pred Grad: -0.126, New P: -7.973
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 1.547
iter 16 loss: 0.010
Actual params: [-7.9732,  1.5469]
-Original Grad: 0.000, -lr * Pred Grad: -0.107, New P: -8.080
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 1.516
iter 17 loss: 0.010
Actual params: [-8.0799,  1.516 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.092, New P: -8.172
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 1.485
iter 18 loss: 0.010
Actual params: [-8.1716,  1.4848]
-Original Grad: 0.000, -lr * Pred Grad: -0.080, New P: -8.252
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.453
iter 19 loss: 0.010
Actual params: [-8.2517,  1.4532]
-Original Grad: 0.000, -lr * Pred Grad: -0.071, New P: -8.323
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.421
iter 20 loss: 0.010
Actual params: [-8.323 ,  1.4213]
-Original Grad: 0.000, -lr * Pred Grad: -0.065, New P: -8.388
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.389
iter 21 loss: 0.010
Actual params: [-8.3875,  1.3893]
-Original Grad: 0.000, -lr * Pred Grad: -0.059, New P: -8.447
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.357
iter 22 loss: 0.010
Actual params: [-8.4469,  1.3571]
-Original Grad: 0.000, -lr * Pred Grad: -0.055, New P: -8.502
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.325
iter 23 loss: 0.010
Actual params: [-8.5024,  1.3248]
-Original Grad: 0.000, -lr * Pred Grad: -0.052, New P: -8.555
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.292
iter 24 loss: 0.010
Actual params: [-8.5548,  1.2925]
-Original Grad: 0.000, -lr * Pred Grad: -0.050, New P: -8.605
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.260
iter 25 loss: 0.010
Actual params: [-8.605 ,  1.2601]
-Original Grad: 0.000, -lr * Pred Grad: -0.048, New P: -8.654
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.228
iter 26 loss: 0.010
Actual params: [-8.6535,  1.2276]
-Original Grad: 0.000, -lr * Pred Grad: -0.047, New P: -8.701
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.195
iter 27 loss: 0.010
Actual params: [-8.7007,  1.1951]
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: -8.747
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.163
iter 28 loss: 0.010
Actual params: [-8.7469,  1.1626]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -8.792
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.130
iter 29 loss: 0.010
Actual params: [-8.7924,  1.1301]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -8.837
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.098
iter 30 loss: 0.010
Actual params: [-8.8374,  1.0976]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -8.882
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.065
Target params: [1.3344, 1.5708]
iter 0 loss: 0.537
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad: 0.611, New P: 0.139
-Original Grad: 0.072, -lr * Pred Grad: 0.971, New P: 0.975
iter 1 loss: 0.440
Actual params: [0.1388, 0.9747]
-Original Grad: 0.245, -lr * Pred Grad: 1.964, New P: 2.103
-Original Grad: 0.119, -lr * Pred Grad: 0.805, New P: 1.780
iter 2 loss: 0.233
Actual params: [2.1028, 1.7796]
-Original Grad: -0.222, -lr * Pred Grad: -1.198, New P: 0.905
-Original Grad: 0.245, -lr * Pred Grad: 1.260, New P: 3.039
iter 3 loss: 0.490
Actual params: [0.9053, 3.0394]
-Original Grad: -0.042, -lr * Pred Grad: -0.326, New P: 0.579
-Original Grad: -1.753, -lr * Pred Grad: -0.673, New P: 2.367
iter 4 loss: 0.284
Actual params: [0.5791, 2.3668]
-Original Grad: 0.343, -lr * Pred Grad: 0.576, New P: 1.155
-Original Grad: -0.319, -lr * Pred Grad: -0.436, New P: 1.930
iter 5 loss: 0.096
Actual params: [1.1551, 1.9304]
-Original Grad: 0.107, -lr * Pred Grad: 0.323, New P: 1.478
-Original Grad: 0.039, -lr * Pred Grad: -0.518, New P: 1.413
iter 6 loss: 0.168
Actual params: [1.4777, 1.4126]
-Original Grad: -0.306, -lr * Pred Grad: -0.539, New P: 0.938
-Original Grad: 0.638, -lr * Pred Grad: 0.860, New P: 2.272
iter 7 loss: 0.163
Actual params: [0.9383, 2.2722]
-Original Grad: 0.601, -lr * Pred Grad: 0.749, New P: 1.687
-Original Grad: -0.179, -lr * Pred Grad: -0.460, New P: 1.812
iter 8 loss: 0.141
Actual params: [1.6872, 1.8121]
-Original Grad: -0.306, -lr * Pred Grad: -0.483, New P: 1.204
-Original Grad: 0.403, -lr * Pred Grad: 0.712, New P: 2.524
iter 9 loss: 0.110
Actual params: [1.2042, 2.5242]
-Original Grad: 0.239, -lr * Pred Grad: 0.210, New P: 1.415
-Original Grad: -0.699, -lr * Pred Grad: -0.413, New P: 2.111
iter 10 loss: 0.078
Actual params: [1.4147, 2.1111]
-Original Grad: 0.009, -lr * Pred Grad: 0.023, New P: 1.438
-Original Grad: 0.065, -lr * Pred Grad: -0.291, New P: 1.820
iter 11 loss: 0.091
Actual params: [1.438 , 1.8201]
-Original Grad: -0.242, -lr * Pred Grad: -0.441, New P: 0.997
-Original Grad: 0.340, -lr * Pred Grad: 0.477, New P: 2.297
iter 12 loss: 0.142
Actual params: [0.9968, 2.2969]
-Original Grad: 0.364, -lr * Pred Grad: 0.397, New P: 1.394
-Original Grad: -0.146, -lr * Pred Grad: -0.288, New P: 2.009
iter 13 loss: 0.079
Actual params: [1.3936, 2.0091]
-Original Grad: -0.006, -lr * Pred Grad: 0.027, New P: 1.421
-Original Grad: 0.076, -lr * Pred Grad: 0.061, New P: 2.070
iter 14 loss: 0.078
Actual params: [1.4211, 2.0698]
-Original Grad: -0.017, -lr * Pred Grad: 0.042, New P: 1.463
-Original Grad: 0.043, -lr * Pred Grad: 0.090, New P: 2.159
iter 15 loss: 0.077
Actual params: [1.4634, 2.1593]
-Original Grad: -0.041, -lr * Pred Grad: -0.071, New P: 1.393
-Original Grad: 0.034, -lr * Pred Grad: 0.117, New P: 2.276
iter 16 loss: 0.079
Actual params: [1.3929, 2.2761]
-Original Grad: -0.010, -lr * Pred Grad: -0.061, New P: 1.332
-Original Grad: 0.092, -lr * Pred Grad: 0.322, New P: 2.598
iter 17 loss: 0.114
Actual params: [1.3316, 2.5983]
-Original Grad: 0.140, -lr * Pred Grad: 0.281, New P: 1.613
-Original Grad: -1.104, -lr * Pred Grad: -0.426, New P: 2.172
iter 18 loss: 0.088
Actual params: [1.6128, 2.1725]
-Original Grad: -0.288, -lr * Pred Grad: -0.548, New P: 1.065
-Original Grad: 0.307, -lr * Pred Grad: -0.248, New P: 1.925
iter 19 loss: 0.114
Actual params: [1.0649, 1.9249]
-Original Grad: 0.188, -lr * Pred Grad: 0.089, New P: 1.154
-Original Grad: 0.030, -lr * Pred Grad: -0.185, New P: 1.740
iter 20 loss: 0.102
Actual params: [1.1543, 1.7402]
-Original Grad: 0.123, -lr * Pred Grad: 0.207, New P: 1.361
-Original Grad: 0.164, -lr * Pred Grad: 0.233, New P: 1.973
iter 21 loss: 0.080
Actual params: [1.3608, 1.9732]
-Original Grad: 0.016, -lr * Pred Grad: 0.143, New P: 1.504
-Original Grad: 0.066, -lr * Pred Grad: 0.222, New P: 2.195
iter 22 loss: 0.076
Actual params: [1.5041, 2.1951]
-Original Grad: 0.000, -lr * Pred Grad: 0.099, New P: 1.603
-Original Grad: 0.126, -lr * Pred Grad: 0.505, New P: 2.700
iter 23 loss: 0.127
Actual params: [1.6028, 2.6999]
-Original Grad: 0.317, -lr * Pred Grad: 0.754, New P: 2.357
-Original Grad: -1.288, -lr * Pred Grad: -0.451, New P: 2.249
iter 24 loss: 0.259
Actual params: [2.3568, 2.2493]
-Original Grad: -0.188, -lr * Pred Grad: -0.319, New P: 2.038
-Original Grad: -0.050, -lr * Pred Grad: -0.462, New P: 1.787
iter 25 loss: 0.222
Actual params: [2.0377, 1.7875]
-Original Grad: -0.234, -lr * Pred Grad: -0.647, New P: 1.391
-Original Grad: 0.246, -lr * Pred Grad: -0.138, New P: 1.650
iter 26 loss: 0.106
Actual params: [1.3908, 1.6497]
-Original Grad: -0.242, -lr * Pred Grad: -0.973, New P: 0.417
-Original Grad: 0.423, -lr * Pred Grad: 1.041, New P: 2.691
iter 27 loss: 0.363
Actual params: [0.4173, 2.6905]
-Original Grad: 0.323, -lr * Pred Grad: 0.013, New P: 0.430
-Original Grad: -0.435, -lr * Pred Grad: -0.582, New P: 2.109
iter 28 loss: 0.298
Actual params: [0.4302, 2.1086]
-Original Grad: 0.398, -lr * Pred Grad: 0.461, New P: 0.892
-Original Grad: -0.133, -lr * Pred Grad: -0.290, New P: 1.819
iter 29 loss: 0.165
Actual params: [0.8915, 1.8186]
-Original Grad: 0.349, -lr * Pred Grad: 0.811, New P: 1.702
-Original Grad: 0.078, -lr * Pred Grad: -0.228, New P: 1.590
iter 30 loss: 0.177
Actual params: [1.702 , 1.5902]
-Original Grad: -0.304, -lr * Pred Grad: -0.459, New P: 1.243
-Original Grad: 0.458, -lr * Pred Grad: 1.188, New P: 2.778
Target params: [1.3344, 1.5708]
iter 0 loss: 0.577
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.005, -lr * Pred Grad: 0.700, New P: 0.227
-Original Grad: 0.012, -lr * Pred Grad: 0.692, New P: 0.695
iter 1 loss: 0.448
Actual params: [0.2272, 0.6953]
-Original Grad: 0.325, -lr * Pred Grad: 2.167, New P: 2.394
-Original Grad: 0.277, -lr * Pred Grad: 1.648, New P: 2.343
iter 2 loss: 0.518
Actual params: [2.3941, 2.343 ]
-Original Grad: -0.018, -lr * Pred Grad: -0.473, New P: 1.921
-Original Grad: -0.551, -lr * Pred Grad: -0.949, New P: 1.394
iter 3 loss: 0.200
Actual params: [1.9208, 1.3936]
-Original Grad: -0.103, -lr * Pred Grad: -0.158, New P: 1.763
-Original Grad: -0.467, -lr * Pred Grad: -0.325, New P: 1.068
iter 4 loss: 0.126
Actual params: [1.763 , 1.0682]
-Original Grad: -0.087, -lr * Pred Grad: -0.308, New P: 1.455
-Original Grad: -0.213, -lr * Pred Grad: -0.581, New P: 0.487
iter 5 loss: 0.149
Actual params: [1.4552, 0.4871]
-Original Grad: 0.657, -lr * Pred Grad: 0.985, New P: 2.440
-Original Grad: 1.151, -lr * Pred Grad: 1.544, New P: 2.031
iter 6 loss: 0.439
Actual params: [2.44 , 2.031]
-Original Grad: -0.031, -lr * Pred Grad: -0.168, New P: 2.271
-Original Grad: -0.502, -lr * Pred Grad: -0.725, New P: 1.306
iter 7 loss: 0.211
Actual params: [2.2715, 1.3064]
-Original Grad: -0.078, -lr * Pred Grad: -0.029, New P: 2.243
-Original Grad: -0.403, -lr * Pred Grad: -0.333, New P: 0.974
iter 8 loss: 0.153
Actual params: [2.2427, 0.9738]
-Original Grad: -0.082, -lr * Pred Grad: -0.173, New P: 2.069
-Original Grad: -0.281, -lr * Pred Grad: -0.558, New P: 0.415
iter 9 loss: 0.126
Actual params: [2.0693, 0.4155]
-Original Grad: -0.104, -lr * Pred Grad: -0.329, New P: 1.740
-Original Grad: 0.003, -lr * Pred Grad: -0.408, New P: 0.008
iter 10 loss: 0.153
Actual params: [1.74  , 0.0076]
-Original Grad: 0.003, -lr * Pred Grad: -0.209, New P: 1.531
-Original Grad: 0.410, -lr * Pred Grad: 0.605, New P: 0.612
iter 11 loss: 0.089
Actual params: [1.5314, 0.6121]
-Original Grad: 0.133, -lr * Pred Grad: 0.131, New P: 1.663
-Original Grad: 0.320, -lr * Pred Grad: 1.457, New P: 2.069
iter 12 loss: 0.413
Actual params: [1.6625, 2.0695]
-Original Grad: -0.086, -lr * Pred Grad: -0.180, New P: 1.482
-Original Grad: -0.587, -lr * Pred Grad: -0.780, New P: 1.290
iter 13 loss: 0.163
Actual params: [1.4821, 1.2896]
-Original Grad: -0.020, -lr * Pred Grad: -0.170, New P: 1.312
-Original Grad: -0.558, -lr * Pred Grad: -0.364, New P: 0.926
iter 14 loss: 0.107
Actual params: [1.3119, 0.9257]
-Original Grad: 0.100, -lr * Pred Grad: 0.127, New P: 1.439
-Original Grad: -0.095, -lr * Pred Grad: -0.555, New P: 0.371
iter 15 loss: 0.208
Actual params: [1.4393, 0.3707]
-Original Grad: 0.467, -lr * Pred Grad: 0.889, New P: 2.328
-Original Grad: 0.587, -lr * Pred Grad: 0.704, New P: 1.074
iter 16 loss: 0.173
Actual params: [2.3284, 1.0743]
-Original Grad: -0.083, -lr * Pred Grad: -0.111, New P: 2.217
-Original Grad: -0.224, -lr * Pred Grad: -0.443, New P: 0.631
iter 17 loss: 0.134
Actual params: [2.2174, 0.6311]
-Original Grad: -0.080, -lr * Pred Grad: -0.081, New P: 2.136
-Original Grad: 0.045, -lr * Pred Grad: -0.084, New P: 0.548
iter 18 loss: 0.128
Actual params: [2.1364, 0.5476]
-Original Grad: -0.079, -lr * Pred Grad: -0.218, New P: 1.918
-Original Grad: 0.133, -lr * Pred Grad: 0.197, New P: 0.744
iter 19 loss: 0.113
Actual params: [1.9182, 0.7444]
-Original Grad: -0.098, -lr * Pred Grad: -0.368, New P: 1.550
-Original Grad: -0.023, -lr * Pred Grad: -0.014, New P: 0.730
iter 20 loss: 0.090
Actual params: [1.5504, 0.7302]
-Original Grad: -0.029, -lr * Pred Grad: -0.322, New P: 1.228
-Original Grad: -0.202, -lr * Pred Grad: -0.314, New P: 0.417
iter 21 loss: 0.263
Actual params: [1.2283, 0.4166]
-Original Grad: 0.190, -lr * Pred Grad: 0.186, New P: 1.415
-Original Grad: 1.061, -lr * Pred Grad: 1.969, New P: 2.386
iter 22 loss: 0.499
Actual params: [1.4147, 2.3856]
-Original Grad: -0.062, -lr * Pred Grad: -0.121, New P: 1.293
-Original Grad: -0.803, -lr * Pred Grad: -0.791, New P: 1.595
iter 23 loss: 0.256
Actual params: [1.2935, 1.5946]
-Original Grad: -0.059, -lr * Pred Grad: -0.228, New P: 1.066
-Original Grad: -0.530, -lr * Pred Grad: -0.335, New P: 1.259
iter 24 loss: 0.200
Actual params: [1.0657, 1.2592]
-Original Grad: 0.234, -lr * Pred Grad: 0.398, New P: 1.463
-Original Grad: -0.132, -lr * Pred Grad: -0.557, New P: 0.702
iter 25 loss: 0.090
Actual params: [1.4635, 0.7025]
-Original Grad: 0.092, -lr * Pred Grad: 0.321, New P: 1.785
-Original Grad: 0.139, -lr * Pred Grad: -0.209, New P: 0.494
iter 26 loss: 0.099
Actual params: [1.7847, 0.4939]
-Original Grad: -0.179, -lr * Pred Grad: -0.295, New P: 1.490
-Original Grad: -0.052, -lr * Pred Grad: -0.261, New P: 0.233
iter 27 loss: 0.214
Actual params: [1.4897, 0.2329]
-Original Grad: 0.648, -lr * Pred Grad: 0.935, New P: 2.424
-Original Grad: 0.300, -lr * Pred Grad: 0.543, New P: 0.776
iter 28 loss: 0.153
Actual params: [2.4244, 0.7761]
-Original Grad: -0.105, -lr * Pred Grad: -0.268, New P: 2.157
-Original Grad: -0.035, -lr * Pred Grad: -0.061, New P: 0.715
iter 29 loss: 0.132
Actual params: [2.1566, 0.7155]
-Original Grad: -0.081, -lr * Pred Grad: -0.129, New P: 2.027
-Original Grad: -0.128, -lr * Pred Grad: -0.222, New P: 0.493
iter 30 loss: 0.120
Actual params: [2.0272, 0.4934]
-Original Grad: -0.109, -lr * Pred Grad: -0.295, New P: 1.732
-Original Grad: -0.006, -lr * Pred Grad: -0.167, New P: 0.327
Target params: [1.3344, 1.5708]
iter 0 loss: 0.486
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.020, -lr * Pred Grad: 0.844, New P: 0.371
-Original Grad: 0.069, -lr * Pred Grad: 0.958, New P: 0.962
iter 1 loss: 0.171
Actual params: [0.3715, 0.9619]
-Original Grad: 0.273, -lr * Pred Grad: 2.087, New P: 2.458
-Original Grad: -0.066, -lr * Pred Grad: -0.242, New P: 0.720
iter 2 loss: 0.129
Actual params: [2.4583, 0.7197]
-Original Grad: -0.131, -lr * Pred Grad: -0.948, New P: 1.510
-Original Grad: 0.101, -lr * Pred Grad: 0.291, New P: 1.011
iter 3 loss: 0.026
Actual params: [1.5104, 1.0111]
-Original Grad: -0.038, -lr * Pred Grad: -0.080, New P: 1.430
-Original Grad: -0.139, -lr * Pred Grad: -0.300, New P: 0.712
iter 4 loss: 0.036
Actual params: [1.43  , 0.7116]
-Original Grad: 0.230, -lr * Pred Grad: 0.573, New P: 2.003
-Original Grad: 0.197, -lr * Pred Grad: 0.331, New P: 1.043
iter 5 loss: 0.075
Actual params: [2.0028, 1.0428]
-Original Grad: -0.169, -lr * Pred Grad: -0.322, New P: 1.681
-Original Grad: -0.261, -lr * Pred Grad: -0.360, New P: 0.683
iter 6 loss: 0.023
Actual params: [1.6809, 0.6827]
-Original Grad: -0.027, -lr * Pred Grad: -0.168, New P: 1.513
-Original Grad: 0.034, -lr * Pred Grad: -0.146, New P: 0.537
iter 7 loss: 0.032
Actual params: [1.5133, 0.5367]
-Original Grad: 0.085, -lr * Pred Grad: 0.102, New P: 1.615
-Original Grad: 0.029, -lr * Pred Grad: -0.056, New P: 0.481
iter 8 loss: 0.029
Actual params: [1.6153, 0.4811]
-Original Grad: -0.039, -lr * Pred Grad: -0.076, New P: 1.539
-Original Grad: 0.147, -lr * Pred Grad: 0.384, New P: 0.865
iter 9 loss: 0.022
Actual params: [1.5388, 0.8646]
-Original Grad: 0.015, -lr * Pred Grad: 0.000, New P: 1.539
-Original Grad: 0.057, -lr * Pred Grad: 0.295, New P: 1.159
iter 10 loss: 0.041
Actual params: [1.539 , 1.1592]
-Original Grad: -0.060, -lr * Pred Grad: -0.177, New P: 1.362
-Original Grad: -0.240, -lr * Pred Grad: -0.385, New P: 0.774
iter 11 loss: 0.042
Actual params: [1.3623, 0.7739]
-Original Grad: 0.216, -lr * Pred Grad: 0.403, New P: 1.765
-Original Grad: 0.260, -lr * Pred Grad: 0.366, New P: 1.139
iter 12 loss: 0.056
Actual params: [1.7649, 1.1395]
-Original Grad: -0.112, -lr * Pred Grad: -0.175, New P: 1.590
-Original Grad: -0.275, -lr * Pred Grad: -0.376, New P: 0.764
iter 13 loss: 0.022
Actual params: [1.5896, 0.7639]
-Original Grad: 0.008, -lr * Pred Grad: -0.039, New P: 1.551
-Original Grad: 0.014, -lr * Pred Grad: -0.175, New P: 0.589
iter 14 loss: 0.027
Actual params: [1.5511, 0.5889]
-Original Grad: 0.066, -lr * Pred Grad: 0.128, New P: 1.679
-Original Grad: 0.033, -lr * Pred Grad: -0.072, New P: 0.517
iter 15 loss: 0.027
Actual params: [1.6786, 0.5171]
-Original Grad: -0.062, -lr * Pred Grad: -0.118, New P: 1.560
-Original Grad: 0.067, -lr * Pred Grad: 0.124, New P: 0.641
iter 16 loss: 0.025
Actual params: [1.5601, 0.6415]
-Original Grad: 0.035, -lr * Pred Grad: 0.033, New P: 1.593
-Original Grad: 0.050, -lr * Pred Grad: 0.166, New P: 0.807
iter 17 loss: 0.022
Actual params: [1.5935, 0.8075]
-Original Grad: -0.005, -lr * Pred Grad: -0.023, New P: 1.570
-Original Grad: -0.021, -lr * Pred Grad: -0.026, New P: 0.781
iter 18 loss: 0.022
Actual params: [1.5705, 0.7811]
-Original Grad: 0.013, -lr * Pred Grad: 0.015, New P: 1.585
-Original Grad: -0.017, -lr * Pred Grad: -0.056, New P: 0.725
iter 19 loss: 0.023
Actual params: [1.5853, 0.7253]
-Original Grad: 0.007, -lr * Pred Grad: 0.012, New P: 1.597
-Original Grad: 0.005, -lr * Pred Grad: -0.032, New P: 0.693
iter 20 loss: 0.023
Actual params: [1.5969, 0.6933]
-Original Grad: 0.010, -lr * Pred Grad: 0.021, New P: 1.618
-Original Grad: 0.040, -lr * Pred Grad: 0.069, New P: 0.763
iter 21 loss: 0.022
Actual params: [1.6182, 0.7626]
-Original Grad: -0.007, -lr * Pred Grad: -0.021, New P: 1.597
-Original Grad: -0.014, -lr * Pred Grad: -0.032, New P: 0.731
iter 22 loss: 0.022
Actual params: [1.5969, 0.7308]
-Original Grad: 0.016, -lr * Pred Grad: 0.023, New P: 1.620
-Original Grad: 0.032, -lr * Pred Grad: 0.057, New P: 0.788
iter 23 loss: 0.022
Actual params: [1.62  , 0.7876]
-Original Grad: -0.014, -lr * Pred Grad: -0.042, New P: 1.578
-Original Grad: -0.002, -lr * Pred Grad: -0.007, New P: 0.780
iter 24 loss: 0.022
Actual params: [1.5775, 0.7804]
-Original Grad: 0.007, -lr * Pred Grad: -0.013, New P: 1.565
-Original Grad: 0.017, -lr * Pred Grad: 0.025, New P: 0.805
iter 25 loss: 0.022
Actual params: [1.5648, 0.8054]
-Original Grad: 0.010, -lr * Pred Grad: 0.004, New P: 1.569
-Original Grad: -0.001, -lr * Pred Grad: -0.015, New P: 0.790
iter 26 loss: 0.022
Actual params: [1.5693, 0.7902]
-Original Grad: -0.014, -lr * Pred Grad: -0.053, New P: 1.516
-Original Grad: -0.043, -lr * Pred Grad: -0.116, New P: 0.674
iter 27 loss: 0.027
Actual params: [1.5161, 0.6742]
-Original Grad: 0.101, -lr * Pred Grad: 0.233, New P: 1.749
-Original Grad: 0.074, -lr * Pred Grad: 0.114, New P: 0.788
iter 28 loss: 0.027
Actual params: [1.7492, 0.7882]
-Original Grad: -0.065, -lr * Pred Grad: -0.099, New P: 1.650
-Original Grad: -0.029, -lr * Pred Grad: -0.061, New P: 0.727
iter 29 loss: 0.022
Actual params: [1.6498, 0.727 ]
-Original Grad: -0.027, -lr * Pred Grad: -0.118, New P: 1.532
-Original Grad: 0.017, -lr * Pred Grad: 0.011, New P: 0.738
iter 30 loss: 0.024
Actual params: [1.5318, 0.7385]
-Original Grad: 0.039, -lr * Pred Grad: 0.029, New P: 1.561
-Original Grad: 0.045, -lr * Pred Grad: 0.101, New P: 0.840
Target params: [1.3344, 1.5708]
iter 0 loss: 0.170
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.073, -lr * Pred Grad: 1.307, New P: 0.835
-Original Grad: -0.026, -lr * Pred Grad: 0.513, New P: 0.516
iter 1 loss: 0.036
Actual params: [0.8351, 0.5162]
-Original Grad: -0.115, -lr * Pred Grad: -0.812, New P: 0.023
-Original Grad: -0.263, -lr * Pred Grad: -0.597, New P: -0.081
iter 2 loss: 0.102
Actual params: [ 0.0229, -0.0805]
-Original Grad: 0.109, -lr * Pred Grad: 0.585, New P: 0.608
-Original Grad: 0.064, -lr * Pred Grad: -0.020, New P: -0.100
iter 3 loss: 0.020
Actual params: [ 0.6081, -0.1004]
-Original Grad: 0.052, -lr * Pred Grad: 0.272, New P: 0.880
-Original Grad: 0.114, -lr * Pred Grad: 0.194, New P: 0.093
iter 4 loss: 0.013
Actual params: [0.8805, 0.0933]
-Original Grad: -0.084, -lr * Pred Grad: -0.160, New P: 0.720
-Original Grad: -0.005, -lr * Pred Grad: 0.005, New P: 0.098
iter 5 loss: 0.006
Actual params: [0.7201, 0.098 ]
-Original Grad: -0.008, -lr * Pred Grad: -0.042, New P: 0.678
-Original Grad: 0.012, -lr * Pred Grad: 0.017, New P: 0.115
iter 6 loss: 0.006
Actual params: [0.6779, 0.1154]
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 0.648
-Original Grad: 0.049, -lr * Pred Grad: 0.112, New P: 0.227
iter 7 loss: 0.004
Actual params: [0.6477, 0.2273]
-Original Grad: -0.007, -lr * Pred Grad: -0.051, New P: 0.596
-Original Grad: -0.010, -lr * Pred Grad: -0.012, New P: 0.215
iter 8 loss: 0.004
Actual params: [0.5965, 0.2151]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: 0.560
-Original Grad: -0.002, -lr * Pred Grad: -0.021, New P: 0.194
iter 9 loss: 0.006
Actual params: [0.5597, 0.1936]
-Original Grad: 0.018, -lr * Pred Grad: 0.014, New P: 0.574
-Original Grad: 0.054, -lr * Pred Grad: 0.112, New P: 0.306
iter 10 loss: 0.004
Actual params: [0.5742, 0.306 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.011, New P: 0.563
-Original Grad: 0.010, -lr * Pred Grad: 0.045, New P: 0.351
iter 11 loss: 0.004
Actual params: [0.5627, 0.3507]
-Original Grad: -0.020, -lr * Pred Grad: -0.075, New P: 0.488
-Original Grad: -0.037, -lr * Pred Grad: -0.086, New P: 0.265
iter 12 loss: 0.007
Actual params: [0.4877, 0.2652]
-Original Grad: 0.043, -lr * Pred Grad: 0.068, New P: 0.556
-Original Grad: 0.061, -lr * Pred Grad: 0.103, New P: 0.368
iter 13 loss: 0.005
Actual params: [0.5559, 0.3679]
-Original Grad: -0.012, -lr * Pred Grad: -0.023, New P: 0.533
-Original Grad: -0.027, -lr * Pred Grad: -0.058, New P: 0.310
iter 14 loss: 0.004
Actual params: [0.5328, 0.31  ]
-Original Grad: 0.017, -lr * Pred Grad: 0.031, New P: 0.564
-Original Grad: 0.011, -lr * Pred Grad: -0.003, New P: 0.307
iter 15 loss: 0.004
Actual params: [0.5641, 0.307 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.003, New P: 0.562
-Original Grad: 0.004, -lr * Pred Grad: -0.016, New P: 0.291
iter 16 loss: 0.004
Actual params: [0.5615, 0.2913]
-Original Grad: 0.009, -lr * Pred Grad: 0.015, New P: 0.577
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: 0.292
iter 17 loss: 0.004
Actual params: [0.5768, 0.2919]
-Original Grad: 0.007, -lr * Pred Grad: 0.014, New P: 0.590
-Original Grad: -0.006, -lr * Pred Grad: -0.032, New P: 0.260
iter 18 loss: 0.004
Actual params: [0.5905, 0.2595]
-Original Grad: 0.009, -lr * Pred Grad: 0.022, New P: 0.613
-Original Grad: 0.013, -lr * Pred Grad: 0.002, New P: 0.262
iter 19 loss: 0.004
Actual params: [0.6128, 0.2616]
-Original Grad: -0.003, -lr * Pred Grad: -0.009, New P: 0.604
-Original Grad: 0.005, -lr * Pred Grad: -0.006, New P: 0.255
iter 20 loss: 0.004
Actual params: [0.6042, 0.2551]
-Original Grad: -0.005, -lr * Pred Grad: -0.030, New P: 0.574
-Original Grad: 0.001, -lr * Pred Grad: -0.019, New P: 0.236
iter 21 loss: 0.004
Actual params: [0.5743, 0.2364]
-Original Grad: 0.023, -lr * Pred Grad: 0.036, New P: 0.610
-Original Grad: 0.052, -lr * Pred Grad: 0.112, New P: 0.349
iter 22 loss: 0.005
Actual params: [0.6099, 0.3488]
-Original Grad: -0.025, -lr * Pred Grad: -0.069, New P: 0.541
-Original Grad: -0.034, -lr * Pred Grad: -0.071, New P: 0.278
iter 23 loss: 0.004
Actual params: [0.5409, 0.2779]
-Original Grad: 0.019, -lr * Pred Grad: 0.007, New P: 0.548
-Original Grad: 0.032, -lr * Pred Grad: 0.042, New P: 0.320
iter 24 loss: 0.004
Actual params: [0.5483, 0.3197]
-Original Grad: 0.000, -lr * Pred Grad: -0.018, New P: 0.531
-Original Grad: -0.009, -lr * Pred Grad: -0.033, New P: 0.287
iter 25 loss: 0.005
Actual params: [0.5307, 0.287 ]
-Original Grad: 0.012, -lr * Pred Grad: 0.012, New P: 0.543
-Original Grad: 0.003, -lr * Pred Grad: -0.018, New P: 0.269
iter 26 loss: 0.005
Actual params: [0.543 , 0.2689]
-Original Grad: 0.012, -lr * Pred Grad: 0.025, New P: 0.568
-Original Grad: 0.041, -lr * Pred Grad: 0.078, New P: 0.347
iter 27 loss: 0.004
Actual params: [0.5682, 0.3468]
-Original Grad: -0.003, -lr * Pred Grad: -0.008, New P: 0.560
-Original Grad: -0.034, -lr * Pred Grad: -0.078, New P: 0.269
iter 28 loss: 0.004
Actual params: [0.5597, 0.2686]
-Original Grad: 0.009, -lr * Pred Grad: 0.011, New P: 0.571
-Original Grad: -0.004, -lr * Pred Grad: -0.051, New P: 0.218
iter 29 loss: 0.005
Actual params: [0.5708, 0.218 ]
-Original Grad: 0.017, -lr * Pred Grad: 0.040, New P: 0.611
-Original Grad: 0.014, -lr * Pred Grad: -0.012, New P: 0.206
iter 30 loss: 0.004
Actual params: [0.611 , 0.2063]
-Original Grad: 0.011, -lr * Pred Grad: 0.041, New P: 0.652
-Original Grad: 0.030, -lr * Pred Grad: 0.054, New P: 0.261
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad: 0.651, New P: 0.179
-Original Grad: 0.000, -lr * Pred Grad: 0.635, New P: 0.639
iter 1 loss: 0.306
Actual params: [0.1792, 0.6385]
-Original Grad: 0.265, -lr * Pred Grad: 2.029, New P: 2.208
-Original Grad: 0.170, -lr * Pred Grad: 1.107, New P: 1.746
iter 2 loss: 0.179
Actual params: [2.2077, 1.7456]
-Original Grad: -0.043, -lr * Pred Grad: -0.510, New P: 1.698
-Original Grad: -0.207, -lr * Pred Grad: -0.796, New P: 0.950
iter 3 loss: 0.042
Actual params: [1.6979, 0.9499]
-Original Grad: -0.123, -lr * Pred Grad: -0.361, New P: 1.337
-Original Grad: 0.030, -lr * Pred Grad: 0.005, New P: 0.955
iter 4 loss: 0.011
Actual params: [1.3367, 0.955 ]
-Original Grad: -0.033, -lr * Pred Grad: -0.192, New P: 1.145
-Original Grad: 0.048, -lr * Pred Grad: -0.028, New P: 0.927
iter 5 loss: 0.026
Actual params: [1.1445, 0.927 ]
-Original Grad: 0.142, -lr * Pred Grad: 0.283, New P: 1.427
-Original Grad: 0.182, -lr * Pred Grad: 0.550, New P: 1.477
iter 6 loss: 0.036
Actual params: [1.4273, 1.4775]
-Original Grad: -0.104, -lr * Pred Grad: -0.200, New P: 1.227
-Original Grad: -0.337, -lr * Pred Grad: -0.437, New P: 1.041
iter 7 loss: 0.014
Actual params: [1.2272, 1.0405]
-Original Grad: 0.073, -lr * Pred Grad: 0.112, New P: 1.339
-Original Grad: 0.088, -lr * Pred Grad: -0.080, New P: 0.960
iter 8 loss: 0.011
Actual params: [1.3392, 0.9603]
-Original Grad: -0.025, -lr * Pred Grad: -0.047, New P: 1.292
-Original Grad: 0.047, -lr * Pred Grad: 0.005, New P: 0.965
iter 9 loss: 0.012
Actual params: [1.2925, 0.9648]
-Original Grad: 0.050, -lr * Pred Grad: 0.114, New P: 1.407
-Original Grad: 0.069, -lr * Pred Grad: 0.180, New P: 1.145
iter 10 loss: 0.011
Actual params: [1.4067, 1.1446]
-Original Grad: -0.023, -lr * Pred Grad: -0.023, New P: 1.383
-Original Grad: -0.002, -lr * Pred Grad: 0.028, New P: 1.172
iter 11 loss: 0.011
Actual params: [1.3834, 1.1721]
-Original Grad: -0.018, -lr * Pred Grad: -0.060, New P: 1.323
-Original Grad: -0.029, -lr * Pred Grad: -0.065, New P: 1.107
iter 12 loss: 0.010
Actual params: [1.3233, 1.1067]
-Original Grad: -0.002, -lr * Pred Grad: -0.050, New P: 1.273
-Original Grad: 0.006, -lr * Pred Grad: -0.030, New P: 1.077
iter 13 loss: 0.011
Actual params: [1.2733, 1.0768]
-Original Grad: 0.034, -lr * Pred Grad: 0.049, New P: 1.322
-Original Grad: 0.022, -lr * Pred Grad: 0.021, New P: 1.098
iter 14 loss: 0.010
Actual params: [1.322 , 1.0977]
-Original Grad: 0.011, -lr * Pred Grad: 0.036, New P: 1.358
-Original Grad: -0.001, -lr * Pred Grad: -0.013, New P: 1.084
iter 15 loss: 0.010
Actual params: [1.3576, 1.0843]
-Original Grad: 0.002, -lr * Pred Grad: 0.016, New P: 1.374
-Original Grad: 0.010, -lr * Pred Grad: 0.002, New P: 1.086
iter 16 loss: 0.010
Actual params: [1.3739, 1.0861]
-Original Grad: -0.020, -lr * Pred Grad: -0.056, New P: 1.318
-Original Grad: 0.006, -lr * Pred Grad: -0.005, New P: 1.081
iter 17 loss: 0.010
Actual params: [1.3175, 1.0811]
-Original Grad: 0.003, -lr * Pred Grad: -0.031, New P: 1.287
-Original Grad: -0.003, -lr * Pred Grad: -0.026, New P: 1.055
iter 18 loss: 0.011
Actual params: [1.2869, 1.0551]
-Original Grad: 0.031, -lr * Pred Grad: 0.051, New P: 1.338
-Original Grad: 0.017, -lr * Pred Grad: 0.014, New P: 1.069
iter 19 loss: 0.010
Actual params: [1.3377, 1.0691]
-Original Grad: 0.000, -lr * Pred Grad: 0.008, New P: 1.345
-Original Grad: 0.005, -lr * Pred Grad: -0.002, New P: 1.067
iter 20 loss: 0.010
Actual params: [1.3455, 1.0673]
-Original Grad: -0.023, -lr * Pred Grad: -0.069, New P: 1.277
-Original Grad: 0.000, -lr * Pred Grad: -0.017, New P: 1.050
iter 21 loss: 0.011
Actual params: [1.2767, 1.0502]
-Original Grad: 0.038, -lr * Pred Grad: 0.059, New P: 1.336
-Original Grad: 0.034, -lr * Pred Grad: 0.061, New P: 1.111
iter 22 loss: 0.010
Actual params: [1.336 , 1.1111]
-Original Grad: 0.002, -lr * Pred Grad: 0.012, New P: 1.348
-Original Grad: 0.029, -lr * Pred Grad: 0.079, New P: 1.191
iter 23 loss: 0.010
Actual params: [1.3481, 1.1906]
-Original Grad: 0.007, -lr * Pred Grad: 0.020, New P: 1.369
-Original Grad: 0.022, -lr * Pred Grad: 0.071, New P: 1.262
iter 24 loss: 0.012
Actual params: [1.3686, 1.2619]
-Original Grad: -0.044, -lr * Pred Grad: -0.127, New P: 1.242
-Original Grad: -0.065, -lr * Pred Grad: -0.142, New P: 1.120
iter 25 loss: 0.011
Actual params: [1.2421, 1.1196]
-Original Grad: 0.026, -lr * Pred Grad: -0.007, New P: 1.236
-Original Grad: 0.036, -lr * Pred Grad: 0.010, New P: 1.130
iter 26 loss: 0.011
Actual params: [1.2355, 1.1297]
-Original Grad: 0.019, -lr * Pred Grad: 0.022, New P: 1.257
-Original Grad: 0.049, -lr * Pred Grad: 0.110, New P: 1.239
iter 27 loss: 0.010
Actual params: [1.2572, 1.2392]
-Original Grad: 0.017, -lr * Pred Grad: 0.044, New P: 1.301
-Original Grad: -0.018, -lr * Pred Grad: -0.031, New P: 1.208
iter 28 loss: 0.010
Actual params: [1.3008, 1.2085]
-Original Grad: 0.000, -lr * Pred Grad: 0.013, New P: 1.313
-Original Grad: -0.002, -lr * Pred Grad: -0.027, New P: 1.182
iter 29 loss: 0.010
Actual params: [1.3133, 1.1818]
-Original Grad: 0.001, -lr * Pred Grad: 0.001, New P: 1.315
-Original Grad: -0.003, -lr * Pred Grad: -0.038, New P: 1.143
iter 30 loss: 0.010
Actual params: [1.3147, 1.1435]
-Original Grad: 0.034, -lr * Pred Grad: 0.088, New P: 1.402
-Original Grad: 0.005, -lr * Pred Grad: -0.021, New P: 1.123
Target params: [1.3344, 1.5708]
iter 0 loss: 0.320
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.025, -lr * Pred Grad: 0.378, New P: -0.094
-Original Grad: 0.010, -lr * Pred Grad: 0.681, New P: 0.684
iter 1 loss: 0.277
Actual params: [-0.0945,  0.6841]
-Original Grad: 0.125, -lr * Pred Grad: 1.399, New P: 1.304
-Original Grad: 0.036, -lr * Pred Grad: 0.323, New P: 1.007
iter 2 loss: 0.205
Actual params: [1.3045, 1.0071]
-Original Grad: 0.200, -lr * Pred Grad: 1.530, New P: 2.834
-Original Grad: 1.204, -lr * Pred Grad: 2.747, New P: 3.754
iter 3 loss: 0.362
Actual params: [2.8343, 3.754 ]
-Original Grad: 0.021, -lr * Pred Grad: 0.335, New P: 3.170
-Original Grad: -0.310, -lr * Pred Grad: -0.933, New P: 2.821
iter 4 loss: 0.239
Actual params: [3.1697, 2.8207]
-Original Grad: -0.050, -lr * Pred Grad: 0.055, New P: 3.224
-Original Grad: -0.194, -lr * Pred Grad: -0.177, New P: 2.644
iter 5 loss: 0.224
Actual params: [3.2245, 2.6437]
-Original Grad: -0.042, -lr * Pred Grad: -0.048, New P: 3.177
-Original Grad: -0.114, -lr * Pred Grad: -0.387, New P: 2.256
iter 6 loss: 0.193
Actual params: [3.1767, 2.2564]
-Original Grad: -0.087, -lr * Pred Grad: -0.258, New P: 2.918
-Original Grad: -0.066, -lr * Pred Grad: -0.266, New P: 1.991
iter 7 loss: 0.157
Actual params: [2.9182, 1.9909]
-Original Grad: -0.109, -lr * Pred Grad: -0.432, New P: 2.486
-Original Grad: 0.010, -lr * Pred Grad: -0.151, New P: 1.840
iter 8 loss: 0.095
Actual params: [2.4862, 1.8397]
-Original Grad: -0.062, -lr * Pred Grad: -0.446, New P: 2.041
-Original Grad: -0.016, -lr * Pred Grad: -0.118, New P: 1.722
iter 9 loss: 0.058
Actual params: [2.0405, 1.7221]
-Original Grad: -0.039, -lr * Pred Grad: -0.429, New P: 1.612
-Original Grad: -0.085, -lr * Pred Grad: -0.223, New P: 1.499
iter 10 loss: 0.024
Actual params: [1.6118, 1.4987]
-Original Grad: -0.011, -lr * Pred Grad: -0.366, New P: 1.245
-Original Grad: -0.012, -lr * Pred Grad: -0.156, New P: 1.343
iter 11 loss: 0.033
Actual params: [1.2453, 1.3428]
-Original Grad: 0.138, -lr * Pred Grad: 0.041, New P: 1.286
-Original Grad: 0.232, -lr * Pred Grad: 0.559, New P: 1.902
iter 12 loss: 0.025
Actual params: [1.2859, 1.9015]
-Original Grad: 0.044, -lr * Pred Grad: 0.058, New P: 1.344
-Original Grad: 0.040, -lr * Pred Grad: 0.267, New P: 2.168
iter 13 loss: 0.031
Actual params: [1.344 , 2.1681]
-Original Grad: -0.031, -lr * Pred Grad: -0.066, New P: 1.279
-Original Grad: -0.099, -lr * Pred Grad: -0.226, New P: 1.942
iter 14 loss: 0.024
Actual params: [1.2785, 1.9419]
-Original Grad: -0.007, -lr * Pred Grad: -0.064, New P: 1.214
-Original Grad: 0.014, -lr * Pred Grad: -0.030, New P: 1.912
iter 15 loss: 0.026
Actual params: [1.2142, 1.912 ]
-Original Grad: 0.021, -lr * Pred Grad: 0.007, New P: 1.221
-Original Grad: 0.046, -lr * Pred Grad: 0.066, New P: 1.978
iter 16 loss: 0.025
Actual params: [1.2215, 1.9781]
-Original Grad: 0.005, -lr * Pred Grad: -0.003, New P: 1.218
-Original Grad: 0.038, -lr * Pred Grad: 0.116, New P: 2.094
iter 17 loss: 0.025
Actual params: [1.2182, 2.094 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.015, New P: 1.203
-Original Grad: -0.013, -lr * Pred Grad: -0.016, New P: 2.078
iter 18 loss: 0.024
Actual params: [1.203 , 2.0784]
-Original Grad: -0.025, -lr * Pred Grad: -0.096, New P: 1.107
-Original Grad: -0.083, -lr * Pred Grad: -0.182, New P: 1.896
iter 19 loss: 0.028
Actual params: [1.107 , 1.8961]
-Original Grad: 0.013, -lr * Pred Grad: -0.029, New P: 1.078
-Original Grad: 0.053, -lr * Pred Grad: 0.009, New P: 1.905
iter 20 loss: 0.028
Actual params: [1.0778, 1.9055]
-Original Grad: -0.017, -lr * Pred Grad: -0.090, New P: 0.988
-Original Grad: 0.077, -lr * Pred Grad: 0.199, New P: 2.105
iter 21 loss: 0.021
Actual params: [0.9878, 2.1049]
-Original Grad: -0.014, -lr * Pred Grad: -0.110, New P: 0.878
-Original Grad: -0.050, -lr * Pred Grad: -0.095, New P: 2.009
iter 22 loss: 0.020
Actual params: [0.8781, 2.0095]
-Original Grad: -0.003, -lr * Pred Grad: -0.095, New P: 0.783
-Original Grad: 0.049, -lr * Pred Grad: 0.084, New P: 2.094
iter 23 loss: 0.025
Actual params: [0.7832, 2.0936]
-Original Grad: 0.017, -lr * Pred Grad: -0.033, New P: 0.750
-Original Grad: -0.024, -lr * Pred Grad: -0.059, New P: 2.035
iter 24 loss: 0.024
Actual params: [0.7504, 2.0346]
-Original Grad: 0.125, -lr * Pred Grad: 0.287, New P: 1.038
-Original Grad: -0.130, -lr * Pred Grad: -0.254, New P: 1.781
iter 25 loss: 0.037
Actual params: [1.0376, 1.7809]
-Original Grad: 0.024, -lr * Pred Grad: 0.173, New P: 1.210
-Original Grad: 0.113, -lr * Pred Grad: 0.082, New P: 1.863
iter 26 loss: 0.028
Actual params: [1.2102, 1.8626]
-Original Grad: 0.027, -lr * Pred Grad: 0.200, New P: 1.410
-Original Grad: 0.069, -lr * Pred Grad: 0.210, New P: 2.073
iter 27 loss: 0.027
Actual params: [1.4097, 2.0727]
-Original Grad: -0.017, -lr * Pred Grad: 0.054, New P: 1.464
-Original Grad: -0.062, -lr * Pred Grad: -0.122, New P: 1.950
iter 28 loss: 0.024
Actual params: [1.4642, 1.9503]
-Original Grad: -0.046, -lr * Pred Grad: -0.094, New P: 1.370
-Original Grad: -0.070, -lr * Pred Grad: -0.174, New P: 1.776
iter 29 loss: 0.022
Actual params: [1.37  , 1.7764]
-Original Grad: 0.049, -lr * Pred Grad: 0.089, New P: 1.459
-Original Grad: 0.006, -lr * Pred Grad: -0.102, New P: 1.674
iter 30 loss: 0.020
Actual params: [1.459 , 1.6745]
-Original Grad: 0.012, -lr * Pred Grad: 0.051, New P: 1.510
-Original Grad: -0.033, -lr * Pred Grad: -0.129, New P: 1.545
Target params: [1.3344, 1.5708]
iter 0 loss: 0.583
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.019, -lr * Pred Grad: 0.833, New P: 0.361
-Original Grad: 0.010, -lr * Pred Grad: 0.680, New P: 0.683
iter 1 loss: 0.439
Actual params: [0.361 , 0.6832]
-Original Grad: 0.397, -lr * Pred Grad: 2.306, New P: 2.667
-Original Grad: 0.147, -lr * Pred Grad: 0.982, New P: 1.665
iter 2 loss: 0.265
Actual params: [2.6669, 1.665 ]
-Original Grad: -0.380, -lr * Pred Grad: -1.459, New P: 1.208
-Original Grad: 0.539, -lr * Pred Grad: 2.377, New P: 4.042
iter 3 loss: 1.014
Actual params: [1.2083, 4.0425]
-Original Grad: -0.056, -lr * Pred Grad: -0.483, New P: 0.725
-Original Grad: -0.020, -lr * Pred Grad: -0.252, New P: 3.790
iter 4 loss: 0.875
Actual params: [0.7248, 3.7903]
-Original Grad: -0.299, -lr * Pred Grad: -1.066, New P: -0.342
-Original Grad: -0.264, -lr * Pred Grad: -0.431, New P: 3.359
iter 5 loss: 0.628
Actual params: [-0.3417,  3.3589]
-Original Grad: -0.108, -lr * Pred Grad: -1.115, New P: -1.457
-Original Grad: -0.199, -lr * Pred Grad: -0.372, New P: 2.987
iter 6 loss: 0.588
Actual params: [-1.4567,  2.987 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.983, New P: -2.439
-Original Grad: -0.000, -lr * Pred Grad: -0.290, New P: 2.697
iter 7 loss: 0.588
Actual params: [-2.4393,  2.6974]
-Original Grad: 0.000, -lr * Pred Grad: -0.872, New P: -3.311
-Original Grad: -0.000, -lr * Pred Grad: -0.150, New P: 2.548
iter 8 loss: 0.588
Actual params: [-3.3114,  2.5476]
-Original Grad: 0.000, -lr * Pred Grad: -0.802, New P: -4.114
-Original Grad: 0.000, -lr * Pred Grad: -0.094, New P: 2.453
iter 9 loss: 0.588
Actual params: [-4.1138,  2.4532]
-Original Grad: 0.000, -lr * Pred Grad: -0.750, New P: -4.864
-Original Grad: 0.000, -lr * Pred Grad: -0.065, New P: 2.388
iter 10 loss: 0.588
Actual params: [-4.8642,  2.3882]
-Original Grad: 0.000, -lr * Pred Grad: -0.697, New P: -5.561
-Original Grad: 0.000, -lr * Pred Grad: -0.051, New P: 2.337
iter 11 loss: 0.588
Actual params: [-5.561 ,  2.3373]
-Original Grad: 0.000, -lr * Pred Grad: -0.637, New P: -6.198
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: 2.295
iter 12 loss: 0.588
Actual params: [-6.1983,  2.2948]
-Original Grad: 0.000, -lr * Pred Grad: -0.573, New P: -6.772
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: 2.257
iter 13 loss: 0.588
Actual params: [-6.7716,  2.2568]
-Original Grad: 0.000, -lr * Pred Grad: -0.507, New P: -7.279
-Original Grad: 0.000, -lr * Pred Grad: -0.036, New P: 2.221
iter 14 loss: 0.588
Actual params: [-7.2787,  2.2213]
-Original Grad: 0.000, -lr * Pred Grad: -0.442, New P: -7.720
-Original Grad: 0.000, -lr * Pred Grad: -0.034, New P: 2.187
iter 15 loss: 0.588
Actual params: [-7.7203,  2.187 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.379, New P: -8.099
-Original Grad: 0.000, -lr * Pred Grad: -0.034, New P: 2.153
iter 16 loss: 0.588
Actual params: [-8.0991,  2.1533]
-Original Grad: 0.000, -lr * Pred Grad: -0.321, New P: -8.420
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 2.120
iter 17 loss: 0.588
Actual params: [-8.4198,  2.1198]
-Original Grad: 0.000, -lr * Pred Grad: -0.269, New P: -8.689
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 2.087
iter 18 loss: 0.588
Actual params: [-8.6886,  2.0865]
-Original Grad: 0.000, -lr * Pred Grad: -0.224, New P: -8.912
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 2.053
iter 19 loss: 0.588
Actual params: [-8.9122,  2.0533]
-Original Grad: 0.000, -lr * Pred Grad: -0.186, New P: -9.098
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 2.020
iter 20 loss: 0.588
Actual params: [-9.0979,  2.0202]
-Original Grad: 0.000, -lr * Pred Grad: -0.154, New P: -9.252
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.987
iter 21 loss: 0.588
Actual params: [-9.2524,  1.9871]
-Original Grad: 0.000, -lr * Pred Grad: -0.129, New P: -9.382
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.954
iter 22 loss: 0.588
Actual params: [-9.3818,  1.9541]
-Original Grad: 0.000, -lr * Pred Grad: -0.110, New P: -9.491
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.921
iter 23 loss: 0.588
Actual params: [-9.4915,  1.9212]
-Original Grad: 0.000, -lr * Pred Grad: -0.094, New P: -9.586
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.888
iter 24 loss: 0.588
Actual params: [-9.5857,  1.8883]
-Original Grad: 0.000, -lr * Pred Grad: -0.082, New P: -9.668
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.855
iter 25 loss: 0.588
Actual params: [-9.6678,  1.8555]
-Original Grad: 0.000, -lr * Pred Grad: -0.073, New P: -9.741
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.823
iter 26 loss: 0.588
Actual params: [-9.7408,  1.8228]
-Original Grad: 0.000, -lr * Pred Grad: -0.066, New P: -9.807
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.790
iter 27 loss: 0.588
Actual params: [-9.8067,  1.79  ]
-Original Grad: 0.000, -lr * Pred Grad: -0.060, New P: -9.867
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.757
iter 28 loss: 0.588
Actual params: [-9.8671,  1.7574]
-Original Grad: 0.000, -lr * Pred Grad: -0.056, New P: -9.923
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.725
iter 29 loss: 0.588
Actual params: [-9.9234,  1.7247]
-Original Grad: 0.000, -lr * Pred Grad: -0.053, New P: -9.977
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.692
iter 30 loss: 0.588
Actual params: [-9.9765,  1.6921]
-Original Grad: 0.000, -lr * Pred Grad: -0.051, New P: -10.027
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.659
Target params: [1.3344, 1.5708]
iter 0 loss: 0.018
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.015, -lr * Pred Grad: 0.795, New P: 0.323
-Original Grad: -0.007, -lr * Pred Grad: 0.599, New P: 0.603
iter 1 loss: 0.025
Actual params: [0.3227, 0.6027]
-Original Grad: -0.066, -lr * Pred Grad: -0.358, New P: -0.036
-Original Grad: 0.038, -lr * Pred Grad: 0.334, New P: 0.937
iter 2 loss: 0.018
Actual params: [-0.0358,  0.9369]
-Original Grad: 0.019, -lr * Pred Grad: 0.216, New P: 0.180
-Original Grad: -0.015, -lr * Pred Grad: -0.127, New P: 0.810
iter 3 loss: 0.013
Actual params: [0.1803, 0.8103]
-Original Grad: 0.004, -lr * Pred Grad: 0.055, New P: 0.235
-Original Grad: -0.004, -lr * Pred Grad: -0.030, New P: 0.781
iter 4 loss: 0.013
Actual params: [0.2348, 0.7807]
-Original Grad: 0.001, -lr * Pred Grad: 0.050, New P: 0.285
-Original Grad: -0.000, -lr * Pred Grad: -0.057, New P: 0.723
iter 5 loss: 0.017
Actual params: [0.2848, 0.7235]
-Original Grad: -0.089, -lr * Pred Grad: -0.329, New P: -0.044
-Original Grad: 0.048, -lr * Pred Grad: 0.071, New P: 0.794
iter 6 loss: 0.017
Actual params: [-0.0443,  0.7942]
-Original Grad: 0.021, -lr * Pred Grad: -0.101, New P: -0.145
-Original Grad: -0.015, -lr * Pred Grad: -0.037, New P: 0.757
iter 7 loss: 0.019
Actual params: [-0.1449,  0.7572]
-Original Grad: 0.016, -lr * Pred Grad: -0.048, New P: -0.193
-Original Grad: -0.012, -lr * Pred Grad: -0.054, New P: 0.703
iter 8 loss: 0.020
Actual params: [-0.1927,  0.7034]
-Original Grad: 0.026, -lr * Pred Grad: 0.023, New P: -0.169
-Original Grad: -0.018, -lr * Pred Grad: -0.081, New P: 0.622
iter 9 loss: 0.018
Actual params: [-0.1695,  0.6221]
-Original Grad: 0.018, -lr * Pred Grad: 0.046, New P: -0.124
-Original Grad: -0.017, -lr * Pred Grad: -0.088, New P: 0.534
iter 10 loss: 0.016
Actual params: [-0.1237,  0.5344]
-Original Grad: 0.014, -lr * Pred Grad: 0.055, New P: -0.068
-Original Grad: -0.018, -lr * Pred Grad: -0.094, New P: 0.440
iter 11 loss: 0.014
Actual params: [-0.0683,  0.4404]
-Original Grad: 0.014, -lr * Pred Grad: 0.064, New P: -0.004
-Original Grad: -0.017, -lr * Pred Grad: -0.096, New P: 0.344
iter 12 loss: 0.013
Actual params: [-0.0039,  0.3444]
-Original Grad: 0.015, -lr * Pred Grad: 0.071, New P: 0.067
-Original Grad: -0.021, -lr * Pred Grad: -0.105, New P: 0.240
iter 13 loss: 0.012
Actual params: [0.0673, 0.2395]
-Original Grad: -0.008, -lr * Pred Grad: 0.007, New P: 0.074
-Original Grad: -0.003, -lr * Pred Grad: -0.073, New P: 0.167
iter 14 loss: 0.012
Actual params: [0.0742, 0.1669]
-Original Grad: -0.007, -lr * Pred Grad: -0.020, New P: 0.054
-Original Grad: 0.003, -lr * Pred Grad: -0.046, New P: 0.121
iter 15 loss: 0.012
Actual params: [0.0541, 0.1211]
-Original Grad: -0.010, -lr * Pred Grad: -0.050, New P: 0.004
-Original Grad: 0.006, -lr * Pred Grad: -0.026, New P: 0.095
iter 16 loss: 0.011
Actual params: [0.0043, 0.095 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -0.037
-Original Grad: -0.003, -lr * Pred Grad: -0.038, New P: 0.057
iter 17 loss: 0.011
Actual params: [-0.0369,  0.0571]
-Original Grad: -0.002, -lr * Pred Grad: -0.047, New P: -0.084
-Original Grad: -0.003, -lr * Pred Grad: -0.041, New P: 0.016
iter 18 loss: 0.011
Actual params: [-0.084 ,  0.0162]
-Original Grad: 0.007, -lr * Pred Grad: -0.025, New P: -0.109
-Original Grad: -0.008, -lr * Pred Grad: -0.055, New P: -0.039
iter 19 loss: 0.011
Actual params: [-0.1087, -0.0389]
-Original Grad: 0.010, -lr * Pred Grad: -0.005, New P: -0.114
-Original Grad: -0.004, -lr * Pred Grad: -0.052, New P: -0.091
iter 20 loss: 0.011
Actual params: [-0.1137, -0.091 ]
-Original Grad: 0.009, -lr * Pred Grad: 0.005, New P: -0.109
-Original Grad: -0.005, -lr * Pred Grad: -0.054, New P: -0.145
iter 21 loss: 0.011
Actual params: [-0.1089, -0.145 ]
-Original Grad: 0.010, -lr * Pred Grad: 0.016, New P: -0.093
-Original Grad: -0.003, -lr * Pred Grad: -0.050, New P: -0.195
iter 22 loss: 0.011
Actual params: [-0.0929, -0.1949]
-Original Grad: 0.005, -lr * Pred Grad: 0.010, New P: -0.083
-Original Grad: -0.002, -lr * Pred Grad: -0.045, New P: -0.240
iter 23 loss: 0.011
Actual params: [-0.0832, -0.24  ]
-Original Grad: -0.005, -lr * Pred Grad: -0.021, New P: -0.104
-Original Grad: 0.002, -lr * Pred Grad: -0.035, New P: -0.275
iter 24 loss: 0.011
Actual params: [-0.104 , -0.2748]
-Original Grad: 0.006, -lr * Pred Grad: -0.006, New P: -0.110
-Original Grad: 0.002, -lr * Pred Grad: -0.030, New P: -0.305
iter 25 loss: 0.011
Actual params: [-0.1095, -0.3049]
-Original Grad: 0.009, -lr * Pred Grad: 0.007, New P: -0.103
-Original Grad: 0.003, -lr * Pred Grad: -0.024, New P: -0.329
iter 26 loss: 0.011
Actual params: [-0.1026, -0.3289]
-Original Grad: 0.006, -lr * Pred Grad: 0.008, New P: -0.095
-Original Grad: 0.003, -lr * Pred Grad: -0.022, New P: -0.351
iter 27 loss: 0.011
Actual params: [-0.095 , -0.3513]
-Original Grad: 0.003, -lr * Pred Grad: -0.000, New P: -0.095
-Original Grad: 0.004, -lr * Pred Grad: -0.018, New P: -0.369
iter 28 loss: 0.011
Actual params: [-0.0954, -0.3691]
-Original Grad: 0.002, -lr * Pred Grad: -0.006, New P: -0.102
-Original Grad: 0.004, -lr * Pred Grad: -0.017, New P: -0.386
iter 29 loss: 0.012
Actual params: [-0.1019, -0.3859]
-Original Grad: 0.005, -lr * Pred Grad: -0.001, New P: -0.103
-Original Grad: 0.005, -lr * Pred Grad: -0.014, New P: -0.400
iter 30 loss: 0.012
Actual params: [-0.103 , -0.3997]
-Original Grad: 0.000, -lr * Pred Grad: -0.014, New P: -0.117
-Original Grad: 0.003, -lr * Pred Grad: -0.018, New P: -0.418
Target params: [1.3344, 1.5708]
iter 0 loss: 0.612
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.027, -lr * Pred Grad: 0.913, New P: 0.440
-Original Grad: 0.037, -lr * Pred Grad: 0.808, New P: 0.811
iter 1 loss: 0.415
Actual params: [0.4403, 0.8111]
-Original Grad: 0.472, -lr * Pred Grad: 2.406, New P: 2.846
-Original Grad: 0.232, -lr * Pred Grad: 1.452, New P: 2.263
iter 2 loss: 0.623
Actual params: [2.8462, 2.2631]
-Original Grad: -0.002, -lr * Pred Grad: -0.641, New P: 2.206
-Original Grad: -0.199, -lr * Pred Grad: -0.852, New P: 1.411
iter 3 loss: 0.453
Actual params: [2.2056, 1.4115]
-Original Grad: -0.063, -lr * Pred Grad: 0.129, New P: 2.335
-Original Grad: -0.127, -lr * Pred Grad: -0.169, New P: 1.243
iter 4 loss: 0.453
Actual params: [2.335 , 1.2425]
-Original Grad: -0.048, -lr * Pred Grad: -0.084, New P: 2.251
-Original Grad: -0.065, -lr * Pred Grad: -0.341, New P: 0.901
iter 5 loss: 0.436
Actual params: [2.2509, 0.9011]
-Original Grad: -0.092, -lr * Pred Grad: -0.262, New P: 1.989
-Original Grad: 0.015, -lr * Pred Grad: -0.134, New P: 0.767
iter 6 loss: 0.400
Actual params: [1.9891, 0.7672]
-Original Grad: -0.144, -lr * Pred Grad: -0.511, New P: 1.478
-Original Grad: 0.083, -lr * Pred Grad: 0.093, New P: 0.860
iter 7 loss: 0.304
Actual params: [1.4776, 0.86  ]
-Original Grad: -0.152, -lr * Pred Grad: -0.724, New P: 0.753
-Original Grad: 0.270, -lr * Pred Grad: 0.986, New P: 1.846
iter 8 loss: 0.308
Actual params: [0.7532, 1.8462]
-Original Grad: 0.733, -lr * Pred Grad: 0.624, New P: 1.378
-Original Grad: -0.113, -lr * Pred Grad: -0.338, New P: 1.509
iter 9 loss: 0.154
Actual params: [1.3776, 1.5087]
-Original Grad: -0.216, -lr * Pred Grad: -0.360, New P: 1.018
-Original Grad: 0.096, -lr * Pred Grad: 0.222, New P: 1.731
iter 10 loss: 0.194
Actual params: [1.018 , 1.7308]
-Original Grad: 0.183, -lr * Pred Grad: 0.172, New P: 1.190
-Original Grad: -0.156, -lr * Pred Grad: -0.270, New P: 1.460
iter 11 loss: 0.145
Actual params: [1.1896, 1.4604]
-Original Grad: 0.009, -lr * Pred Grad: 0.033, New P: 1.222
-Original Grad: 0.024, -lr * Pred Grad: -0.077, New P: 1.383
iter 12 loss: 0.142
Actual params: [1.2224, 1.3835]
-Original Grad: 0.042, -lr * Pred Grad: 0.135, New P: 1.357
-Original Grad: 0.018, -lr * Pred Grad: -0.030, New P: 1.354
iter 13 loss: 0.163
Actual params: [1.3572, 1.3538]
-Original Grad: -0.190, -lr * Pred Grad: -0.367, New P: 0.990
-Original Grad: 0.175, -lr * Pred Grad: 0.517, New P: 1.871
iter 14 loss: 0.214
Actual params: [0.9898, 1.8712]
-Original Grad: 0.081, -lr * Pred Grad: -0.026, New P: 0.964
-Original Grad: -0.201, -lr * Pred Grad: -0.373, New P: 1.498
iter 15 loss: 0.194
Actual params: [0.9637, 1.4978]
-Original Grad: 0.275, -lr * Pred Grad: 0.479, New P: 1.442
-Original Grad: -0.130, -lr * Pred Grad: -0.265, New P: 1.233
iter 16 loss: 0.214
Actual params: [1.4424, 1.2331]
-Original Grad: -0.294, -lr * Pred Grad: -0.470, New P: 0.973
-Original Grad: 0.249, -lr * Pred Grad: 0.344, New P: 1.577
iter 17 loss: 0.196
Actual params: [0.9727, 1.577 ]
-Original Grad: 0.191, -lr * Pred Grad: 0.164, New P: 1.136
-Original Grad: -0.038, -lr * Pred Grad: -0.050, New P: 1.527
iter 18 loss: 0.153
Actual params: [1.1363, 1.5267]
-Original Grad: 0.182, -lr * Pred Grad: 0.353, New P: 1.489
-Original Grad: -0.036, -lr * Pred Grad: -0.082, New P: 1.445
iter 19 loss: 0.207
Actual params: [1.489 , 1.4446]
-Original Grad: -0.345, -lr * Pred Grad: -0.552, New P: 0.937
-Original Grad: 0.141, -lr * Pred Grad: 0.326, New P: 1.771
iter 20 loss: 0.225
Actual params: [0.9373, 1.7707]
-Original Grad: 0.610, -lr * Pred Grad: 0.637, New P: 1.575
-Original Grad: -0.159, -lr * Pred Grad: -0.291, New P: 1.480
iter 21 loss: 0.251
Actual params: [1.5745, 1.4798]
-Original Grad: -0.469, -lr * Pred Grad: -0.611, New P: 0.964
-Original Grad: 0.106, -lr * Pred Grad: 0.110, New P: 1.590
iter 22 loss: 0.200
Actual params: [0.9637, 1.5899]
-Original Grad: 0.144, -lr * Pred Grad: -0.123, New P: 0.840
-Original Grad: -0.030, -lr * Pred Grad: -0.070, New P: 1.520
iter 23 loss: 0.241
Actual params: [0.8402, 1.52  ]
-Original Grad: 0.188, -lr * Pred Grad: 0.146, New P: 0.986
-Original Grad: -0.025, -lr * Pred Grad: -0.082, New P: 1.438
iter 24 loss: 0.183
Actual params: [0.9858, 1.4383]
-Original Grad: 0.201, -lr * Pred Grad: 0.393, New P: 1.379
-Original Grad: 0.035, -lr * Pred Grad: 0.022, New P: 1.460
iter 25 loss: 0.158
Actual params: [1.3786, 1.46  ]
-Original Grad: -0.066, -lr * Pred Grad: 0.028, New P: 1.407
-Original Grad: 0.047, -lr * Pred Grad: 0.117, New P: 1.577
iter 26 loss: 0.159
Actual params: [1.4068, 1.5767]
-Original Grad: -0.317, -lr * Pred Grad: -0.658, New P: 0.748
-Original Grad: 0.054, -lr * Pred Grad: 0.185, New P: 1.761
iter 27 loss: 0.298
Actual params: [0.7485, 1.7614]
-Original Grad: 0.357, -lr * Pred Grad: 0.316, New P: 1.064
-Original Grad: -0.125, -lr * Pred Grad: -0.242, New P: 1.519
iter 28 loss: 0.167
Actual params: [1.0642, 1.5192]
-Original Grad: 0.122, -lr * Pred Grad: 0.220, New P: 1.284
-Original Grad: 0.003, -lr * Pred Grad: -0.097, New P: 1.422
iter 29 loss: 0.142
Actual params: [1.2843, 1.4218]
-Original Grad: -0.224, -lr * Pred Grad: -0.330, New P: 0.954
-Original Grad: 0.235, -lr * Pred Grad: 0.636, New P: 2.057
iter 30 loss: 0.256
Actual params: [0.9542, 2.0573]
-Original Grad: 0.131, -lr * Pred Grad: 0.129, New P: 1.083
-Original Grad: -0.230, -lr * Pred Grad: -0.434, New P: 1.624
Target params: [1.3344, 1.5708]
iter 0 loss: 0.082
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.039, -lr * Pred Grad: 0.225, New P: -0.248
-Original Grad: 0.060, -lr * Pred Grad: 0.917, New P: 0.920
iter 1 loss: 0.072
Actual params: [-0.2478,  0.92  ]
-Original Grad: -0.006, -lr * Pred Grad: 0.238, New P: -0.010
-Original Grad: 0.011, -lr * Pred Grad: 0.168, New P: 1.088
iter 2 loss: 0.071
Actual params: [-0.0101,  1.0885]
-Original Grad: -0.003, -lr * Pred Grad: 0.118, New P: 0.108
-Original Grad: 0.034, -lr * Pred Grad: 0.063, New P: 1.152
iter 3 loss: 0.069
Actual params: [0.108 , 1.1519]
-Original Grad: 0.001, -lr * Pred Grad: 0.082, New P: 0.190
-Original Grad: 0.052, -lr * Pred Grad: 0.156, New P: 1.308
iter 4 loss: 0.063
Actual params: [0.1901, 1.3084]
-Original Grad: 0.015, -lr * Pred Grad: 0.118, New P: 0.308
-Original Grad: 0.066, -lr * Pred Grad: 0.209, New P: 1.517
iter 5 loss: 0.053
Actual params: [0.3078, 1.517 ]
-Original Grad: 0.030, -lr * Pred Grad: 0.175, New P: 0.483
-Original Grad: 0.061, -lr * Pred Grad: 0.225, New P: 1.742
iter 6 loss: 0.037
Actual params: [0.4828, 1.7418]
-Original Grad: 0.036, -lr * Pred Grad: 0.212, New P: 0.694
-Original Grad: 0.050, -lr * Pred Grad: 0.200, New P: 1.942
iter 7 loss: 0.029
Actual params: [0.6945, 1.9415]
-Original Grad: 0.006, -lr * Pred Grad: 0.126, New P: 0.821
-Original Grad: -0.024, -lr * Pred Grad: -0.030, New P: 1.911
iter 8 loss: 0.027
Actual params: [0.8209, 1.9114]
-Original Grad: 0.025, -lr * Pred Grad: 0.161, New P: 0.982
-Original Grad: -0.067, -lr * Pred Grad: -0.145, New P: 1.766
iter 9 loss: 0.019
Actual params: [0.9817, 1.7665]
-Original Grad: 0.036, -lr * Pred Grad: 0.194, New P: 1.176
-Original Grad: 0.040, -lr * Pred Grad: -0.001, New P: 1.765
iter 10 loss: 0.016
Actual params: [1.1756, 1.7651]
-Original Grad: 0.011, -lr * Pred Grad: 0.131, New P: 1.306
-Original Grad: 0.051, -lr * Pred Grad: 0.118, New P: 1.883
iter 11 loss: 0.017
Actual params: [1.3061, 1.8831]
-Original Grad: 0.012, -lr * Pred Grad: 0.115, New P: 1.421
-Original Grad: -0.017, -lr * Pred Grad: -0.021, New P: 1.862
iter 12 loss: 0.015
Actual params: [1.4208, 1.862 ]
-Original Grad: 0.011, -lr * Pred Grad: 0.096, New P: 1.517
-Original Grad: 0.005, -lr * Pred Grad: -0.004, New P: 1.858
iter 13 loss: 0.014
Actual params: [1.5166, 1.8579]
-Original Grad: 0.005, -lr * Pred Grad: 0.067, New P: 1.584
-Original Grad: -0.011, -lr * Pred Grad: -0.047, New P: 1.811
iter 14 loss: 0.014
Actual params: [1.5836, 1.8109]
-Original Grad: -0.003, -lr * Pred Grad: 0.026, New P: 1.610
-Original Grad: 0.050, -lr * Pred Grad: 0.093, New P: 1.904
iter 15 loss: 0.014
Actual params: [1.6097, 1.9038]
-Original Grad: 0.003, -lr * Pred Grad: 0.019, New P: 1.629
-Original Grad: -0.004, -lr * Pred Grad: 0.000, New P: 1.904
iter 16 loss: 0.014
Actual params: [1.6289, 1.9041]
-Original Grad: 0.001, -lr * Pred Grad: 0.006, New P: 1.635
-Original Grad: 0.004, -lr * Pred Grad: -0.001, New P: 1.903
iter 17 loss: 0.014
Actual params: [1.6353, 1.9026]
-Original Grad: -0.005, -lr * Pred Grad: -0.019, New P: 1.616
-Original Grad: 0.022, -lr * Pred Grad: 0.035, New P: 1.938
iter 18 loss: 0.014
Actual params: [1.6164, 1.9379]
-Original Grad: 0.003, -lr * Pred Grad: -0.014, New P: 1.603
-Original Grad: -0.003, -lr * Pred Grad: -0.012, New P: 1.926
iter 19 loss: 0.014
Actual params: [1.6026, 1.9256]
-Original Grad: 0.009, -lr * Pred Grad: 0.002, New P: 1.605
-Original Grad: -0.021, -lr * Pred Grad: -0.069, New P: 1.857
iter 20 loss: 0.014
Actual params: [1.6049, 1.8569]
-Original Grad: -0.008, -lr * Pred Grad: -0.038, New P: 1.567
-Original Grad: 0.040, -lr * Pred Grad: 0.051, New P: 1.908
iter 21 loss: 0.014
Actual params: [1.5672, 1.9083]
-Original Grad: 0.006, -lr * Pred Grad: -0.018, New P: 1.549
-Original Grad: -0.023, -lr * Pred Grad: -0.059, New P: 1.849
iter 22 loss: 0.014
Actual params: [1.5493, 1.8488]
-Original Grad: -0.001, -lr * Pred Grad: -0.030, New P: 1.520
-Original Grad: 0.032, -lr * Pred Grad: 0.043, New P: 1.892
iter 23 loss: 0.014
Actual params: [1.5198, 1.8922]
-Original Grad: 0.003, -lr * Pred Grad: -0.023, New P: 1.497
-Original Grad: 0.009, -lr * Pred Grad: 0.016, New P: 1.908
iter 24 loss: 0.015
Actual params: [1.4969, 1.9084]
-Original Grad: 0.007, -lr * Pred Grad: -0.011, New P: 1.486
-Original Grad: 0.001, -lr * Pred Grad: -0.007, New P: 1.901
iter 25 loss: 0.015
Actual params: [1.4857, 1.901 ]
-Original Grad: 0.004, -lr * Pred Grad: -0.011, New P: 1.474
-Original Grad: 0.007, -lr * Pred Grad: -0.004, New P: 1.897
iter 26 loss: 0.015
Actual params: [1.4745, 1.8967]
-Original Grad: 0.004, -lr * Pred Grad: -0.011, New P: 1.464
-Original Grad: -0.003, -lr * Pred Grad: -0.027, New P: 1.870
iter 27 loss: 0.015
Actual params: [1.4636, 1.8701]
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: 1.464
-Original Grad: -0.001, -lr * Pred Grad: -0.029, New P: 1.841
iter 28 loss: 0.014
Actual params: [1.4643, 1.8407]
-Original Grad: 0.012, -lr * Pred Grad: 0.019, New P: 1.483
-Original Grad: 0.025, -lr * Pred Grad: 0.032, New P: 1.873
iter 29 loss: 0.014
Actual params: [1.4835, 1.8731]
-Original Grad: 0.006, -lr * Pred Grad: 0.014, New P: 1.497
-Original Grad: -0.005, -lr * Pred Grad: -0.022, New P: 1.851
iter 30 loss: 0.014
Actual params: [1.4974, 1.851 ]
-Original Grad: 0.003, -lr * Pred Grad: 0.004, New P: 1.501
-Original Grad: 0.020, -lr * Pred Grad: 0.026, New P: 1.877
Target params: [1.3344, 1.5708]
iter 0 loss: 0.049
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.020, -lr * Pred Grad: 0.434, New P: -0.039
-Original Grad: 0.020, -lr * Pred Grad: 0.730, New P: 0.733
iter 1 loss: 0.050
Actual params: [-0.0386,  0.733 ]
-Original Grad: -0.015, -lr * Pred Grad: 0.166, New P: 0.127
-Original Grad: 0.021, -lr * Pred Grad: 0.235, New P: 0.968
iter 2 loss: 0.051
Actual params: [0.1269, 0.9678]
-Original Grad: -0.039, -lr * Pred Grad: -0.161, New P: -0.034
-Original Grad: 0.064, -lr * Pred Grad: 0.210, New P: 1.178
iter 3 loss: 0.046
Actual params: [-0.0338,  1.1783]
-Original Grad: -0.004, -lr * Pred Grad: -0.033, New P: -0.067
-Original Grad: 0.011, -lr * Pred Grad: 0.020, New P: 1.198
iter 4 loss: 0.046
Actual params: [-0.0672,  1.1983]
-Original Grad: 0.000, -lr * Pred Grad: -0.034, New P: -0.101
-Original Grad: 0.009, -lr * Pred Grad: 0.012, New P: 1.210
iter 5 loss: 0.046
Actual params: [-0.1012,  1.2103]
-Original Grad: -0.002, -lr * Pred Grad: -0.039, New P: -0.140
-Original Grad: 0.008, -lr * Pred Grad: -0.003, New P: 1.207
iter 6 loss: 0.046
Actual params: [-0.1399,  1.2069]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: -0.172
-Original Grad: 0.005, -lr * Pred Grad: -0.010, New P: 1.197
iter 7 loss: 0.046
Actual params: [-0.1723,  1.1969]
-Original Grad: -0.001, -lr * Pred Grad: -0.037, New P: -0.209
-Original Grad: 0.003, -lr * Pred Grad: -0.013, New P: 1.183
iter 8 loss: 0.046
Actual params: [-0.2088,  1.1834]
-Original Grad: -0.000, -lr * Pred Grad: -0.035, New P: -0.244
-Original Grad: 0.002, -lr * Pred Grad: -0.016, New P: 1.168
iter 9 loss: 0.046
Actual params: [-0.2442,  1.1677]
-Original Grad: 0.000, -lr * Pred Grad: -0.034, New P: -0.279
-Original Grad: 0.001, -lr * Pred Grad: -0.020, New P: 1.148
iter 10 loss: 0.046
Actual params: [-0.2785,  1.1481]
-Original Grad: 0.000, -lr * Pred Grad: -0.036, New P: -0.314
-Original Grad: 0.001, -lr * Pred Grad: -0.022, New P: 1.127
iter 11 loss: 0.046
Actual params: [-0.314 ,  1.1265]
-Original Grad: -0.000, -lr * Pred Grad: -0.038, New P: -0.352
-Original Grad: 0.001, -lr * Pred Grad: -0.023, New P: 1.103
iter 12 loss: 0.046
Actual params: [-0.3515,  1.1035]
-Original Grad: 0.000, -lr * Pred Grad: -0.037, New P: -0.388
-Original Grad: 0.001, -lr * Pred Grad: -0.025, New P: 1.078
iter 13 loss: 0.046
Actual params: [-0.3884,  1.0782]
-Original Grad: -0.000, -lr * Pred Grad: -0.039, New P: -0.428
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 1.052
iter 14 loss: 0.046
Actual params: [-0.4276,  1.0516]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -0.468
-Original Grad: 0.000, -lr * Pred Grad: -0.028, New P: 1.024
iter 15 loss: 0.046
Actual params: [-0.4683,  1.024 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -0.510
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 0.995
iter 16 loss: 0.046
Actual params: [-0.5097,  0.9955]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.552
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 0.966
iter 17 loss: 0.046
Actual params: [-0.5523,  0.9665]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.595
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.936
iter 18 loss: 0.046
Actual params: [-0.595 ,  0.9365]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.638
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.906
iter 19 loss: 0.046
Actual params: [-0.6381,  0.9059]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.682
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.875
iter 20 loss: 0.046
Actual params: [-0.6815,  0.8749]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.725
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.844
iter 21 loss: 0.046
Actual params: [-0.725 ,  0.8436]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.769
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.812
iter 22 loss: 0.046
Actual params: [-0.7685,  0.8119]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.812
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.780
iter 23 loss: 0.046
Actual params: [-0.8121,  0.7801]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.856
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.748
iter 24 loss: 0.046
Actual params: [-0.8557,  0.7481]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.899
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.716
iter 25 loss: 0.046
Actual params: [-0.8993,  0.716 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.943
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.684
iter 26 loss: 0.046
Actual params: [-0.943 ,  0.6837]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.987
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.651
iter 27 loss: 0.046
Actual params: [-0.9866,  0.6515]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.030
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.619
iter 28 loss: 0.046
Actual params: [-1.0302,  0.6191]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.074
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.587
iter 29 loss: 0.046
Actual params: [-1.0739,  0.5867]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.117
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.554
iter 30 loss: 0.046
Actual params: [-1.1175,  0.5543]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.161
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.522
Target params: [1.3344, 1.5708]
iter 0 loss: 0.194
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.055, -lr * Pred Grad: 0.040, New P: -0.432
-Original Grad: 0.055, -lr * Pred Grad: 0.891, New P: 0.895
iter 1 loss: 0.179
Actual params: [-0.4319,  0.8947]
-Original Grad: -0.004, -lr * Pred Grad: 0.222, New P: -0.210
-Original Grad: 0.005, -lr * Pred Grad: 0.136, New P: 1.031
iter 2 loss: 0.181
Actual params: [-0.2102,  1.0312]
-Original Grad: -0.012, -lr * Pred Grad: 0.011, New P: -0.199
-Original Grad: 0.015, -lr * Pred Grad: -0.011, New P: 1.020
iter 3 loss: 0.181
Actual params: [-0.1988,  1.0198]
-Original Grad: -0.017, -lr * Pred Grad: -0.065, New P: -0.264
-Original Grad: 0.021, -lr * Pred Grad: 0.046, New P: 1.066
iter 4 loss: 0.180
Actual params: [-0.2642,  1.0657]
-Original Grad: -0.009, -lr * Pred Grad: -0.071, New P: -0.335
-Original Grad: 0.010, -lr * Pred Grad: 0.003, New P: 1.069
iter 5 loss: 0.179
Actual params: [-0.3349,  1.0688]
-Original Grad: -0.006, -lr * Pred Grad: -0.072, New P: -0.407
-Original Grad: 0.006, -lr * Pred Grad: -0.008, New P: 1.061
iter 6 loss: 0.179
Actual params: [-0.407 ,  1.0612]
-Original Grad: -0.005, -lr * Pred Grad: -0.074, New P: -0.481
-Original Grad: 0.005, -lr * Pred Grad: -0.012, New P: 1.049
iter 7 loss: 0.179
Actual params: [-0.4805,  1.049 ]
-Original Grad: -0.003, -lr * Pred Grad: -0.068, New P: -0.549
-Original Grad: 0.003, -lr * Pred Grad: -0.015, New P: 1.034
iter 8 loss: 0.178
Actual params: [-0.5489,  1.0336]
-Original Grad: -0.003, -lr * Pred Grad: -0.066, New P: -0.615
-Original Grad: 0.003, -lr * Pred Grad: -0.016, New P: 1.018
iter 9 loss: 0.178
Actual params: [-0.6152,  1.0175]
-Original Grad: -0.001, -lr * Pred Grad: -0.061, New P: -0.676
-Original Grad: 0.002, -lr * Pred Grad: -0.019, New P: 0.998
iter 10 loss: 0.178
Actual params: [-0.6764,  0.9982]
-Original Grad: -0.001, -lr * Pred Grad: -0.058, New P: -0.734
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: 0.978
iter 11 loss: 0.178
Actual params: [-0.7344,  0.978 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.056, New P: -0.791
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: 0.957
iter 12 loss: 0.178
Actual params: [-0.7909,  0.9575]
-Original Grad: -0.001, -lr * Pred Grad: -0.054, New P: -0.845
-Original Grad: 0.001, -lr * Pred Grad: -0.023, New P: 0.935
iter 13 loss: 0.178
Actual params: [-0.8452,  0.9349]
-Original Grad: -0.001, -lr * Pred Grad: -0.053, New P: -0.898
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 0.911
iter 14 loss: 0.178
Actual params: [-0.898 ,  0.9113]
-Original Grad: -0.001, -lr * Pred Grad: -0.052, New P: -0.950
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 0.887
iter 15 loss: 0.178
Actual params: [-0.9501,  0.8868]
-Original Grad: -0.001, -lr * Pred Grad: -0.051, New P: -1.001
-Original Grad: 0.001, -lr * Pred Grad: -0.026, New P: 0.861
iter 16 loss: 0.178
Actual params: [-1.0012,  0.8613]
-Original Grad: -0.001, -lr * Pred Grad: -0.050, New P: -1.051
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 0.835
iter 17 loss: 0.178
Actual params: [-1.0514,  0.8347]
-Original Grad: -0.001, -lr * Pred Grad: -0.050, New P: -1.101
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 0.808
iter 18 loss: 0.178
Actual params: [-1.1011,  0.8075]
-Original Grad: -0.000, -lr * Pred Grad: -0.049, New P: -1.150
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.780
iter 19 loss: 0.178
Actual params: [-1.1501,  0.7798]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.198
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.751
iter 20 loss: 0.178
Actual params: [-1.1985,  0.7514]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.247
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.723
iter 21 loss: 0.178
Actual params: [-1.2468,  0.7234]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.295
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.695
iter 22 loss: 0.178
Actual params: [-1.2946,  0.6948]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.342
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.666
iter 23 loss: 0.178
Actual params: [-1.3423,  0.6661]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.390
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.637
iter 24 loss: 0.178
Actual params: [-1.3899,  0.6375]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.437
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.609
iter 25 loss: 0.178
Actual params: [-1.4373,  0.6086]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.485
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.579
iter 26 loss: 0.178
Actual params: [-1.4846,  0.5792]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.532
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.550
iter 27 loss: 0.178
Actual params: [-1.5317,  0.5498]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.578
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.520
iter 28 loss: 0.178
Actual params: [-1.5782,  0.5196]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.625
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 0.489
iter 29 loss: 0.178
Actual params: [-1.6247,  0.4895]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.671
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.459
iter 30 loss: 0.178
Actual params: [-1.6711,  0.4591]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.718
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 0.429
Target params: [1.3344, 1.5708]
iter 0 loss: 0.029
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.033, -lr * Pred Grad: 0.285, New P: -0.188
-Original Grad: 0.007, -lr * Pred Grad: 0.665, New P: 0.669
iter 1 loss: 0.027
Actual params: [-0.1877,  0.6688]
-Original Grad: -0.025, -lr * Pred Grad: 0.048, New P: -0.140
-Original Grad: 0.053, -lr * Pred Grad: 0.421, New P: 1.090
iter 2 loss: 0.018
Actual params: [-0.1401,  1.0903]
-Original Grad: -0.012, -lr * Pred Grad: 0.014, New P: -0.126
-Original Grad: 0.029, -lr * Pred Grad: 0.044, New P: 1.134
iter 3 loss: 0.017
Actual params: [-0.1259,  1.1339]
-Original Grad: -0.009, -lr * Pred Grad: -0.035, New P: -0.161
-Original Grad: 0.024, -lr * Pred Grad: 0.055, New P: 1.189
iter 4 loss: 0.017
Actual params: [-0.1609,  1.1886]
-Original Grad: -0.008, -lr * Pred Grad: -0.055, New P: -0.216
-Original Grad: 0.017, -lr * Pred Grad: 0.032, New P: 1.221
iter 5 loss: 0.016
Actual params: [-0.2161,  1.2209]
-Original Grad: -0.005, -lr * Pred Grad: -0.057, New P: -0.273
-Original Grad: 0.010, -lr * Pred Grad: 0.012, New P: 1.233
iter 6 loss: 0.016
Actual params: [-0.2735,  1.2326]
-Original Grad: -0.003, -lr * Pred Grad: -0.057, New P: -0.330
-Original Grad: 0.006, -lr * Pred Grad: 0.000, New P: 1.233
iter 7 loss: 0.015
Actual params: [-0.33  ,  1.2326]
-Original Grad: -0.003, -lr * Pred Grad: -0.057, New P: -0.387
-Original Grad: 0.006, -lr * Pred Grad: -0.002, New P: 1.230
iter 8 loss: 0.015
Actual params: [-0.3866,  1.2305]
-Original Grad: -0.002, -lr * Pred Grad: -0.055, New P: -0.441
-Original Grad: 0.004, -lr * Pred Grad: -0.006, New P: 1.224
iter 9 loss: 0.015
Actual params: [-0.4415,  1.2242]
-Original Grad: -0.001, -lr * Pred Grad: -0.054, New P: -0.495
-Original Grad: 0.004, -lr * Pred Grad: -0.009, New P: 1.215
iter 10 loss: 0.015
Actual params: [-0.4951,  1.2154]
-Original Grad: -0.001, -lr * Pred Grad: -0.052, New P: -0.547
-Original Grad: 0.003, -lr * Pred Grad: -0.012, New P: 1.204
iter 11 loss: 0.015
Actual params: [-0.5474,  1.2039]
-Original Grad: -0.001, -lr * Pred Grad: -0.052, New P: -0.599
-Original Grad: 0.003, -lr * Pred Grad: -0.013, New P: 1.191
iter 12 loss: 0.015
Actual params: [-0.5991,  1.191 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.050, New P: -0.649
-Original Grad: 0.002, -lr * Pred Grad: -0.018, New P: 1.173
iter 13 loss: 0.015
Actual params: [-0.6487,  1.1728]
-Original Grad: -0.000, -lr * Pred Grad: -0.049, New P: -0.697
-Original Grad: 0.002, -lr * Pred Grad: -0.021, New P: 1.152
iter 14 loss: 0.015
Actual params: [-0.6974,  1.1519]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -0.745
-Original Grad: 0.001, -lr * Pred Grad: -0.023, New P: 1.129
iter 15 loss: 0.015
Actual params: [-0.7454,  1.1293]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -0.793
-Original Grad: 0.002, -lr * Pred Grad: -0.023, New P: 1.106
iter 16 loss: 0.015
Actual params: [-0.7932,  1.106 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -0.841
-Original Grad: 0.002, -lr * Pred Grad: -0.023, New P: 1.083
iter 17 loss: 0.015
Actual params: [-0.8414,  1.0834]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -0.889
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 1.059
iter 18 loss: 0.015
Actual params: [-0.8891,  1.0592]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -0.937
-Original Grad: 0.002, -lr * Pred Grad: -0.024, New P: 1.035
iter 19 loss: 0.015
Actual params: [-0.937 ,  1.0349]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -0.985
-Original Grad: 0.001, -lr * Pred Grad: -0.025, New P: 1.010
iter 20 loss: 0.015
Actual params: [-0.9847,  1.0097]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -1.033
-Original Grad: 0.001, -lr * Pred Grad: -0.025, New P: 0.984
iter 21 loss: 0.015
Actual params: [-1.0327,  0.9844]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.080
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 0.958
iter 22 loss: 0.015
Actual params: [-1.0803,  0.9578]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.128
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 0.931
iter 23 loss: 0.015
Actual params: [-1.1278,  0.9309]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.176
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 0.904
iter 24 loss: 0.015
Actual params: [-1.1755,  0.9039]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.223
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.876
iter 25 loss: 0.015
Actual params: [-1.2228,  0.8757]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.270
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.848
iter 26 loss: 0.015
Actual params: [-1.2702,  0.8475]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.317
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.819
iter 27 loss: 0.015
Actual params: [-1.3174,  0.819 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.364
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.790
iter 28 loss: 0.015
Actual params: [-1.3645,  0.7899]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.411
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.760
iter 29 loss: 0.015
Actual params: [-1.411,  0.76 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.458
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 0.730
iter 30 loss: 0.015
Actual params: [-1.4578,  0.7303]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.504
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.700
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad: 0.651, New P: 0.178
-Original Grad: 0.000, -lr * Pred Grad: 0.635, New P: 0.638
iter 1 loss: 0.307
Actual params: [0.1783, 0.6384]
-Original Grad: 0.293, -lr * Pred Grad: 2.092, New P: 2.271
-Original Grad: 0.193, -lr * Pred Grad: 1.234, New P: 1.872
iter 2 loss: 0.196
Actual params: [2.2705, 1.8724]
-Original Grad: -0.047, -lr * Pred Grad: -0.584, New P: 1.687
-Original Grad: -0.233, -lr * Pred Grad: -0.852, New P: 1.020
iter 3 loss: 0.038
Actual params: [1.6869, 1.0202]
-Original Grad: -0.087, -lr * Pred Grad: -0.145, New P: 1.542
-Original Grad: 0.047, -lr * Pred Grad: 0.027, New P: 1.047
iter 4 loss: 0.022
Actual params: [1.542 , 1.0474]
-Original Grad: -0.116, -lr * Pred Grad: -0.430, New P: 1.112
-Original Grad: 0.010, -lr * Pred Grad: -0.113, New P: 0.934
iter 5 loss: 0.029
Actual params: [1.1116, 0.9343]
-Original Grad: 0.159, -lr * Pred Grad: 0.237, New P: 1.349
-Original Grad: 0.209, -lr * Pred Grad: 0.566, New P: 1.501
iter 6 loss: 0.032
Actual params: [1.3487, 1.5007]
-Original Grad: -0.074, -lr * Pred Grad: -0.157, New P: 1.192
-Original Grad: -0.408, -lr * Pred Grad: -0.439, New P: 1.061
iter 7 loss: 0.015
Actual params: [1.1917, 1.0614]
-Original Grad: 0.047, -lr * Pred Grad: 0.054, New P: 1.246
-Original Grad: 0.086, -lr * Pred Grad: -0.138, New P: 0.923
iter 8 loss: 0.016
Actual params: [1.2457, 0.9232]
-Original Grad: 0.092, -lr * Pred Grad: 0.233, New P: 1.479
-Original Grad: 0.172, -lr * Pred Grad: 0.291, New P: 1.214
iter 9 loss: 0.016
Actual params: [1.4786, 1.214 ]
-Original Grad: -0.042, -lr * Pred Grad: -0.011, New P: 1.468
-Original Grad: -0.050, -lr * Pred Grad: -0.079, New P: 1.135
iter 10 loss: 0.014
Actual params: [1.4681, 1.1346]
-Original Grad: -0.057, -lr * Pred Grad: -0.144, New P: 1.324
-Original Grad: -0.013, -lr * Pred Grad: -0.048, New P: 1.087
iter 11 loss: 0.010
Actual params: [1.3237, 1.0866]
-Original Grad: 0.015, -lr * Pred Grad: -0.042, New P: 1.281
-Original Grad: 0.008, -lr * Pred Grad: -0.031, New P: 1.055
iter 12 loss: 0.011
Actual params: [1.2814, 1.0553]
-Original Grad: 0.042, -lr * Pred Grad: 0.063, New P: 1.345
-Original Grad: 0.034, -lr * Pred Grad: 0.056, New P: 1.111
iter 13 loss: 0.010
Actual params: [1.3446, 1.1108]
-Original Grad: 0.003, -lr * Pred Grad: 0.021, New P: 1.366
-Original Grad: -0.005, -lr * Pred Grad: -0.014, New P: 1.097
iter 14 loss: 0.010
Actual params: [1.366 , 1.0969]
-Original Grad: -0.004, -lr * Pred Grad: -0.006, New P: 1.360
-Original Grad: 0.006, -lr * Pred Grad: -0.005, New P: 1.092
iter 15 loss: 0.010
Actual params: [1.3599, 1.092 ]
-Original Grad: -0.017, -lr * Pred Grad: -0.062, New P: 1.298
-Original Grad: -0.024, -lr * Pred Grad: -0.078, New P: 1.014
iter 16 loss: 0.011
Actual params: [1.2978, 1.0141]
-Original Grad: 0.034, -lr * Pred Grad: 0.048, New P: 1.346
-Original Grad: 0.015, -lr * Pred Grad: -0.013, New P: 1.001
iter 17 loss: 0.010
Actual params: [1.3461, 1.0014]
-Original Grad: -0.007, -lr * Pred Grad: -0.017, New P: 1.329
-Original Grad: 0.039, -lr * Pred Grad: 0.075, New P: 1.076
iter 18 loss: 0.010
Actual params: [1.329, 1.076]
-Original Grad: 0.017, -lr * Pred Grad: 0.032, New P: 1.361
-Original Grad: -0.005, -lr * Pred Grad: -0.008, New P: 1.068
iter 19 loss: 0.010
Actual params: [1.3609, 1.068 ]
-Original Grad: 0.025, -lr * Pred Grad: 0.073, New P: 1.433
-Original Grad: -0.003, -lr * Pred Grad: -0.023, New P: 1.045
iter 20 loss: 0.012
Actual params: [1.4334, 1.0446]
-Original Grad: -0.040, -lr * Pred Grad: -0.087, New P: 1.346
-Original Grad: -0.013, -lr * Pred Grad: -0.058, New P: 0.986
iter 21 loss: 0.010
Actual params: [1.3464, 0.9862]
-Original Grad: -0.009, -lr * Pred Grad: -0.077, New P: 1.269
-Original Grad: 0.033, -lr * Pred Grad: 0.040, New P: 1.026
iter 22 loss: 0.012
Actual params: [1.2689, 1.0264]
-Original Grad: 0.044, -lr * Pred Grad: 0.060, New P: 1.329
-Original Grad: -0.003, -lr * Pred Grad: -0.014, New P: 1.012
iter 23 loss: 0.010
Actual params: [1.329 , 1.0124]
-Original Grad: 0.032, -lr * Pred Grad: 0.093, New P: 1.422
-Original Grad: 0.003, -lr * Pred Grad: -0.013, New P: 1.000
iter 24 loss: 0.012
Actual params: [1.4218, 0.9996]
-Original Grad: -0.056, -lr * Pred Grad: -0.122, New P: 1.300
-Original Grad: 0.024, -lr * Pred Grad: 0.035, New P: 1.035
iter 25 loss: 0.011
Actual params: [1.3001, 1.0349]
-Original Grad: 0.056, -lr * Pred Grad: 0.092, New P: 1.392
-Original Grad: 0.016, -lr * Pred Grad: 0.035, New P: 1.070
iter 26 loss: 0.011
Actual params: [1.3923, 1.0698]
-Original Grad: -0.026, -lr * Pred Grad: -0.055, New P: 1.338
-Original Grad: 0.012, -lr * Pred Grad: 0.027, New P: 1.096
iter 27 loss: 0.010
Actual params: [1.3376, 1.0964]
-Original Grad: 0.004, -lr * Pred Grad: -0.023, New P: 1.315
-Original Grad: -0.005, -lr * Pred Grad: -0.020, New P: 1.076
iter 28 loss: 0.010
Actual params: [1.3151, 1.0764]
-Original Grad: 0.012, -lr * Pred Grad: 0.004, New P: 1.319
-Original Grad: -0.012, -lr * Pred Grad: -0.051, New P: 1.025
iter 29 loss: 0.010
Actual params: [1.319 , 1.0254]
-Original Grad: 0.030, -lr * Pred Grad: 0.069, New P: 1.388
-Original Grad: 0.016, -lr * Pred Grad: -0.000, New P: 1.025
iter 30 loss: 0.010
Actual params: [1.3883, 1.0253]
-Original Grad: -0.040, -lr * Pred Grad: -0.093, New P: 1.295
-Original Grad: 0.029, -lr * Pred Grad: 0.055, New P: 1.080
Target params: [1.3344, 1.5708]
iter 0 loss: 0.829
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.002, -lr * Pred Grad: 0.627, New P: 0.155
-Original Grad: -0.000, -lr * Pred Grad: 0.633, New P: 0.637
iter 1 loss: 0.880
Actual params: [0.1549, 0.6365]
-Original Grad: -0.006, -lr * Pred Grad: 0.276, New P: 0.431
-Original Grad: 0.024, -lr * Pred Grad: 0.250, New P: 0.887
iter 2 loss: 0.693
Actual params: [0.4311, 0.8866]
-Original Grad: 1.281, -lr * Pred Grad: 2.418, New P: 2.849
-Original Grad: 0.382, -lr * Pred Grad: 1.707, New P: 2.594
iter 3 loss: 0.542
Actual params: [2.8494, 2.594 ]
-Original Grad: -0.189, -lr * Pred Grad: -1.177, New P: 1.672
-Original Grad: -0.604, -lr * Pred Grad: -0.873, New P: 1.721
iter 4 loss: 0.066
Actual params: [1.6721, 1.7212]
-Original Grad: 0.136, -lr * Pred Grad: 0.147, New P: 1.819
-Original Grad: 0.061, -lr * Pred Grad: -0.070, New P: 1.651
iter 5 loss: 0.055
Actual params: [1.8191, 1.6515]
-Original Grad: 0.020, -lr * Pred Grad: -0.005, New P: 1.814
-Original Grad: 0.004, -lr * Pred Grad: -0.246, New P: 1.405
iter 6 loss: 0.147
Actual params: [1.8143, 1.4053]
-Original Grad: -0.039, -lr * Pred Grad: -0.034, New P: 1.780
-Original Grad: 1.421, -lr * Pred Grad: 2.250, New P: 3.656
iter 7 loss: 0.747
Actual params: [1.7802, 3.6557]
-Original Grad: 0.003, -lr * Pred Grad: 0.002, New P: 1.782
-Original Grad: -0.658, -lr * Pred Grad: -0.902, New P: 2.754
iter 8 loss: 0.417
Actual params: [1.7822, 2.7539]
-Original Grad: -0.193, -lr * Pred Grad: -0.434, New P: 1.348
-Original Grad: -0.880, -lr * Pred Grad: -0.322, New P: 2.432
iter 9 loss: 0.219
Actual params: [1.3479, 2.4321]
-Original Grad: -0.144, -lr * Pred Grad: -0.646, New P: 0.702
-Original Grad: -0.726, -lr * Pred Grad: -0.592, New P: 1.841
iter 10 loss: 0.123
Actual params: [0.7022, 1.8406]
-Original Grad: 0.059, -lr * Pred Grad: -0.328, New P: 0.374
-Original Grad: -0.402, -lr * Pred Grad: -0.607, New P: 1.233
iter 11 loss: 0.572
Actual params: [0.3743, 1.2331]
-Original Grad: 1.371, -lr * Pred Grad: 1.066, New P: 1.440
-Original Grad: 1.122, -lr * Pred Grad: 0.889, New P: 2.122
iter 12 loss: 0.129
Actual params: [1.4401, 2.1218]
-Original Grad: -0.033, -lr * Pred Grad: -0.263, New P: 1.177
-Original Grad: -0.591, -lr * Pred Grad: -0.477, New P: 1.645
iter 13 loss: 0.157
Actual params: [1.1767, 1.6449]
-Original Grad: 0.054, -lr * Pred Grad: 0.136, New P: 1.313
-Original Grad: 0.050, -lr * Pred Grad: -0.319, New P: 1.326
iter 14 loss: 0.294
Actual params: [1.3127, 1.3255]
-Original Grad: 0.060, -lr * Pred Grad: 0.153, New P: 1.466
-Original Grad: 0.926, -lr * Pred Grad: 1.559, New P: 2.884
iter 15 loss: 0.404
Actual params: [1.4661, 2.8845]
-Original Grad: -0.201, -lr * Pred Grad: -0.296, New P: 1.170
-Original Grad: -0.742, -lr * Pred Grad: -0.673, New P: 2.211
iter 16 loss: 0.149
Actual params: [1.1705, 2.2112]
-Original Grad: 0.018, -lr * Pred Grad: -0.124, New P: 1.047
-Original Grad: -0.393, -lr * Pred Grad: -0.363, New P: 1.849
iter 17 loss: 0.139
Actual params: [1.0466, 1.8486]
-Original Grad: 0.007, -lr * Pred Grad: -0.088, New P: 0.958
-Original Grad: 0.019, -lr * Pred Grad: -0.474, New P: 1.375
iter 18 loss: 0.286
Actual params: [0.9584, 1.3747]
-Original Grad: -0.021, -lr * Pred Grad: -0.127, New P: 0.831
-Original Grad: 0.773, -lr * Pred Grad: 1.321, New P: 2.696
iter 19 loss: 0.288
Actual params: [0.831 , 2.6957]
-Original Grad: 0.005, -lr * Pred Grad: -0.088, New P: 0.743
-Original Grad: -0.500, -lr * Pred Grad: -0.657, New P: 2.039
iter 20 loss: 0.146
Actual params: [0.7427, 2.039 ]
-Original Grad: 0.143, -lr * Pred Grad: 0.261, New P: 1.004
-Original Grad: -0.540, -lr * Pred Grad: -0.378, New P: 1.661
iter 21 loss: 0.165
Actual params: [1.0042, 1.6614]
-Original Grad: -0.003, -lr * Pred Grad: 0.079, New P: 1.083
-Original Grad: 0.171, -lr * Pred Grad: -0.373, New P: 1.289
iter 22 loss: 0.336
Actual params: [1.0832, 1.2887]
-Original Grad: 0.192, -lr * Pred Grad: 0.544, New P: 1.628
-Original Grad: 0.803, -lr * Pred Grad: 1.603, New P: 2.892
iter 23 loss: 0.439
Actual params: [1.6276, 2.8919]
-Original Grad: -0.147, -lr * Pred Grad: -0.227, New P: 1.400
-Original Grad: -0.952, -lr * Pred Grad: -0.652, New P: 2.239
iter 24 loss: 0.161
Actual params: [1.4004, 2.2394]
-Original Grad: -0.074, -lr * Pred Grad: -0.264, New P: 1.136
-Original Grad: -0.746, -lr * Pred Grad: -0.419, New P: 1.820
iter 25 loss: 0.139
Actual params: [1.1361, 1.8204]
-Original Grad: 0.043, -lr * Pred Grad: -0.045, New P: 1.091
-Original Grad: -0.020, -lr * Pred Grad: -0.557, New P: 1.263
iter 26 loss: 0.348
Actual params: [1.0914, 1.2632]
-Original Grad: -0.006, -lr * Pred Grad: -0.079, New P: 1.012
-Original Grad: 0.479, -lr * Pred Grad: 0.335, New P: 1.598
iter 27 loss: 0.179
Actual params: [1.0122, 1.5984]
-Original Grad: 0.032, -lr * Pred Grad: 0.017, New P: 1.029
-Original Grad: 0.540, -lr * Pred Grad: 1.800, New P: 3.399
iter 28 loss: 0.566
Actual params: [1.0288, 3.3988]
-Original Grad: -0.164, -lr * Pred Grad: -0.438, New P: 0.591
-Original Grad: -0.888, -lr * Pred Grad: -0.747, New P: 2.652
iter 29 loss: 0.336
Actual params: [0.5911, 2.652 ]
-Original Grad: 0.277, -lr * Pred Grad: 0.353, New P: 0.944
-Original Grad: -0.501, -lr * Pred Grad: -0.362, New P: 2.290
iter 30 loss: 0.172
Actual params: [0.944 , 2.2899]
-Original Grad: -0.008, -lr * Pred Grad: 0.025, New P: 0.969
-Original Grad: -0.451, -lr * Pred Grad: -0.590, New P: 1.700
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.008, -lr * Pred Grad: 0.557, New P: 0.085
-Original Grad: -0.011, -lr * Pred Grad: 0.583, New P: 0.586
iter 1 loss: 0.127
Actual params: [0.0846, 0.5861]
-Original Grad: -0.012, -lr * Pred Grad: 0.210, New P: 0.294
-Original Grad: 0.199, -lr * Pred Grad: 1.254, New P: 1.840
iter 2 loss: 0.020
Actual params: [0.2943, 1.8401]
-Original Grad: 0.103, -lr * Pred Grad: 0.963, New P: 1.258
-Original Grad: 0.057, -lr * Pred Grad: 0.133, New P: 1.973
iter 3 loss: 0.014
Actual params: [1.2576, 1.9728]
-Original Grad: 0.026, -lr * Pred Grad: 0.305, New P: 1.562
-Original Grad: -0.164, -lr * Pred Grad: -0.457, New P: 1.516
iter 4 loss: 0.017
Actual params: [1.5622, 1.5156]
-Original Grad: -0.009, -lr * Pred Grad: 0.202, New P: 1.765
-Original Grad: 0.078, -lr * Pred Grad: 0.024, New P: 1.539
iter 5 loss: 0.018
Actual params: [1.7647, 1.5393]
-Original Grad: -0.020, -lr * Pred Grad: 0.038, New P: 1.802
-Original Grad: 0.126, -lr * Pred Grad: 0.306, New P: 1.845
iter 6 loss: 0.007
Actual params: [1.8023, 1.8454]
-Original Grad: -0.006, -lr * Pred Grad: 0.027, New P: 1.829
-Original Grad: 0.026, -lr * Pred Grad: 0.155, New P: 2.000
iter 7 loss: 0.006
Actual params: [1.8292, 2.    ]
-Original Grad: -0.001, -lr * Pred Grad: 0.011, New P: 1.840
-Original Grad: 0.006, -lr * Pred Grad: 0.054, New P: 2.054
iter 8 loss: 0.006
Actual params: [1.8397, 2.0543]
-Original Grad: -0.002, -lr * Pred Grad: -0.005, New P: 1.834
-Original Grad: 0.008, -lr * Pred Grad: 0.030, New P: 2.084
iter 9 loss: 0.006
Actual params: [1.8344, 2.0842]
-Original Grad: 0.006, -lr * Pred Grad: 0.007, New P: 1.841
-Original Grad: -0.013, -lr * Pred Grad: -0.033, New P: 2.051
iter 10 loss: 0.006
Actual params: [1.8412, 2.0509]
-Original Grad: 0.001, -lr * Pred Grad: -0.006, New P: 1.835
-Original Grad: 0.000, -lr * Pred Grad: -0.025, New P: 2.026
iter 11 loss: 0.006
Actual params: [1.8351, 2.0256]
-Original Grad: 0.003, -lr * Pred Grad: -0.004, New P: 1.831
-Original Grad: -0.003, -lr * Pred Grad: -0.033, New P: 1.992
iter 12 loss: 0.006
Actual params: [1.8308, 1.9922]
-Original Grad: -0.001, -lr * Pred Grad: -0.017, New P: 1.814
-Original Grad: 0.003, -lr * Pred Grad: -0.022, New P: 1.971
iter 13 loss: 0.006
Actual params: [1.8139, 1.9707]
-Original Grad: -0.004, -lr * Pred Grad: -0.032, New P: 1.782
-Original Grad: 0.015, -lr * Pred Grad: 0.010, New P: 1.981
iter 14 loss: 0.006
Actual params: [1.7818, 1.9809]
-Original Grad: -0.001, -lr * Pred Grad: -0.034, New P: 1.747
-Original Grad: 0.007, -lr * Pred Grad: 0.002, New P: 1.983
iter 15 loss: 0.006
Actual params: [1.7474, 1.983 ]
-Original Grad: 0.002, -lr * Pred Grad: -0.031, New P: 1.717
-Original Grad: 0.004, -lr * Pred Grad: -0.007, New P: 1.976
iter 16 loss: 0.006
Actual params: [1.7169, 1.9764]
-Original Grad: 0.005, -lr * Pred Grad: -0.020, New P: 1.697
-Original Grad: -0.004, -lr * Pred Grad: -0.029, New P: 1.947
iter 17 loss: 0.006
Actual params: [1.6969, 1.947 ]
-Original Grad: 0.009, -lr * Pred Grad: -0.004, New P: 1.693
-Original Grad: -0.007, -lr * Pred Grad: -0.045, New P: 1.902
iter 18 loss: 0.006
Actual params: [1.6934, 1.902 ]
-Original Grad: 0.002, -lr * Pred Grad: -0.012, New P: 1.681
-Original Grad: 0.007, -lr * Pred Grad: -0.021, New P: 1.881
iter 19 loss: 0.006
Actual params: [1.6811, 1.8812]
-Original Grad: 0.001, -lr * Pred Grad: -0.019, New P: 1.662
-Original Grad: 0.017, -lr * Pred Grad: 0.014, New P: 1.895
iter 20 loss: 0.006
Actual params: [1.6619, 1.895 ]
-Original Grad: 0.005, -lr * Pred Grad: -0.011, New P: 1.651
-Original Grad: 0.003, -lr * Pred Grad: -0.007, New P: 1.888
iter 21 loss: 0.006
Actual params: [1.6512, 1.8885]
-Original Grad: 0.003, -lr * Pred Grad: -0.014, New P: 1.638
-Original Grad: 0.005, -lr * Pred Grad: -0.008, New P: 1.880
iter 22 loss: 0.006
Actual params: [1.6376, 1.8801]
-Original Grad: 0.002, -lr * Pred Grad: -0.017, New P: 1.621
-Original Grad: 0.010, -lr * Pred Grad: 0.003, New P: 1.883
iter 23 loss: 0.006
Actual params: [1.6205, 1.8832]
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 1.614
-Original Grad: -0.005, -lr * Pred Grad: -0.030, New P: 1.853
iter 24 loss: 0.006
Actual params: [1.6136, 1.8528]
-Original Grad: 0.004, -lr * Pred Grad: -0.010, New P: 1.604
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 1.826
iter 25 loss: 0.006
Actual params: [1.6039, 1.8261]
-Original Grad: 0.003, -lr * Pred Grad: -0.012, New P: 1.592
-Original Grad: 0.010, -lr * Pred Grad: -0.005, New P: 1.821
iter 26 loss: 0.006
Actual params: [1.5919, 1.821 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.020, New P: 1.571
-Original Grad: 0.021, -lr * Pred Grad: 0.033, New P: 1.854
iter 27 loss: 0.006
Actual params: [1.5715, 1.8537]
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: 1.554
-Original Grad: 0.007, -lr * Pred Grad: 0.010, New P: 1.863
iter 28 loss: 0.006
Actual params: [1.5544, 1.8634]
-Original Grad: 0.006, -lr * Pred Grad: -0.007, New P: 1.547
-Original Grad: -0.002, -lr * Pred Grad: -0.018, New P: 1.845
iter 29 loss: 0.006
Actual params: [1.5472, 1.845 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.020, New P: 1.528
-Original Grad: 0.011, -lr * Pred Grad: 0.001, New P: 1.846
iter 30 loss: 0.006
Actual params: [1.5276, 1.8459]
-Original Grad: 0.005, -lr * Pred Grad: -0.012, New P: 1.516
-Original Grad: -0.000, -lr * Pred Grad: -0.020, New P: 1.826
Target params: [1.3344, 1.5708]
iter 0 loss: 0.081
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.005, -lr * Pred Grad: 0.595, New P: 0.123
-Original Grad: 0.007, -lr * Pred Grad: 0.665, New P: 0.669
iter 1 loss: 0.081
Actual params: [0.1231, 0.6688]
-Original Grad: -0.050, -lr * Pred Grad: -0.197, New P: -0.074
-Original Grad: 0.063, -lr * Pred Grad: 0.483, New P: 1.152
iter 2 loss: 0.075
Actual params: [-0.0738,  1.1519]
-Original Grad: 0.021, -lr * Pred Grad: 0.257, New P: 0.183
-Original Grad: 0.019, -lr * Pred Grad: -0.009, New P: 1.143
iter 3 loss: 0.064
Actual params: [0.1829, 1.1426]
-Original Grad: 0.009, -lr * Pred Grad: 0.109, New P: 0.292
-Original Grad: 0.050, -lr * Pred Grad: 0.135, New P: 1.278
iter 4 loss: 0.055
Actual params: [0.2919, 1.2775]
-Original Grad: -0.003, -lr * Pred Grad: 0.056, New P: 0.348
-Original Grad: 0.104, -lr * Pred Grad: 0.345, New P: 1.623
iter 5 loss: 0.034
Actual params: [0.3481, 1.6228]
-Original Grad: -0.027, -lr * Pred Grad: -0.078, New P: 0.270
-Original Grad: 0.062, -lr * Pred Grad: 0.268, New P: 1.891
iter 6 loss: 0.024
Actual params: [0.2699, 1.8912]
-Original Grad: 0.039, -lr * Pred Grad: 0.103, New P: 0.373
-Original Grad: 0.049, -lr * Pred Grad: 0.206, New P: 2.097
iter 7 loss: 0.015
Actual params: [0.3731, 2.0974]
-Original Grad: 0.053, -lr * Pred Grad: 0.195, New P: 0.568
-Original Grad: 0.022, -lr * Pred Grad: 0.101, New P: 2.199
iter 8 loss: 0.011
Actual params: [0.568 , 2.1987]
-Original Grad: -0.015, -lr * Pred Grad: 0.043, New P: 0.611
-Original Grad: 0.009, -lr * Pred Grad: 0.047, New P: 2.246
iter 9 loss: 0.012
Actual params: [0.6112, 2.2457]
-Original Grad: -0.012, -lr * Pred Grad: -0.000, New P: 0.611
-Original Grad: 0.020, -lr * Pred Grad: 0.057, New P: 2.303
iter 10 loss: 0.012
Actual params: [0.6109, 2.3032]
-Original Grad: -0.037, -lr * Pred Grad: -0.116, New P: 0.495
-Original Grad: 0.012, -lr * Pred Grad: 0.038, New P: 2.341
iter 11 loss: 0.011
Actual params: [0.4951, 2.3408]
-Original Grad: -0.007, -lr * Pred Grad: -0.091, New P: 0.404
-Original Grad: -0.013, -lr * Pred Grad: -0.031, New P: 2.310
iter 12 loss: 0.012
Actual params: [0.4037, 2.3099]
-Original Grad: 0.015, -lr * Pred Grad: -0.032, New P: 0.372
-Original Grad: -0.025, -lr * Pred Grad: -0.079, New P: 2.231
iter 13 loss: 0.013
Actual params: [0.3717, 2.231 ]
-Original Grad: 0.026, -lr * Pred Grad: 0.030, New P: 0.402
-Original Grad: 0.012, -lr * Pred Grad: -0.022, New P: 2.209
iter 14 loss: 0.012
Actual params: [0.4021, 2.2087]
-Original Grad: 0.022, -lr * Pred Grad: 0.058, New P: 0.460
-Original Grad: 0.006, -lr * Pred Grad: -0.014, New P: 2.195
iter 15 loss: 0.011
Actual params: [0.4603, 2.195 ]
-Original Grad: 0.011, -lr * Pred Grad: 0.051, New P: 0.511
-Original Grad: 0.002, -lr * Pred Grad: -0.016, New P: 2.179
iter 16 loss: 0.011
Actual params: [0.5112, 2.1788]
-Original Grad: -0.021, -lr * Pred Grad: -0.040, New P: 0.471
-Original Grad: 0.055, -lr * Pred Grad: 0.124, New P: 2.303
iter 17 loss: 0.010
Actual params: [0.4713, 2.3025]
-Original Grad: -0.009, -lr * Pred Grad: -0.054, New P: 0.417
-Original Grad: -0.010, -lr * Pred Grad: -0.009, New P: 2.293
iter 18 loss: 0.011
Actual params: [0.4173, 2.2935]
-Original Grad: 0.039, -lr * Pred Grad: 0.066, New P: 0.483
-Original Grad: 0.026, -lr * Pred Grad: 0.054, New P: 2.348
iter 19 loss: 0.011
Actual params: [0.4829, 2.3479]
-Original Grad: 0.026, -lr * Pred Grad: 0.085, New P: 0.568
-Original Grad: 0.015, -lr * Pred Grad: 0.037, New P: 2.385
iter 20 loss: 0.012
Actual params: [0.5676, 2.3846]
-Original Grad: -0.011, -lr * Pred Grad: 0.007, New P: 0.574
-Original Grad: -0.060, -lr * Pred Grad: -0.136, New P: 2.248
iter 21 loss: 0.011
Actual params: [0.5743, 2.2481]
-Original Grad: -0.025, -lr * Pred Grad: -0.071, New P: 0.503
-Original Grad: -0.024, -lr * Pred Grad: -0.119, New P: 2.129
iter 22 loss: 0.011
Actual params: [0.5035, 2.1291]
-Original Grad: -0.005, -lr * Pred Grad: -0.063, New P: 0.441
-Original Grad: 0.014, -lr * Pred Grad: -0.047, New P: 2.083
iter 23 loss: 0.013
Actual params: [0.4406, 2.0826]
-Original Grad: 0.034, -lr * Pred Grad: 0.041, New P: 0.481
-Original Grad: 0.032, -lr * Pred Grad: 0.040, New P: 2.123
iter 24 loss: 0.012
Actual params: [0.4815, 2.123 ]
-Original Grad: -0.016, -lr * Pred Grad: -0.048, New P: 0.434
-Original Grad: 0.056, -lr * Pred Grad: 0.152, New P: 2.275
iter 25 loss: 0.011
Actual params: [0.4338, 2.2749]
-Original Grad: 0.025, -lr * Pred Grad: 0.037, New P: 0.471
-Original Grad: 0.017, -lr * Pred Grad: 0.077, New P: 2.351
iter 26 loss: 0.011
Actual params: [0.4705, 2.3515]
-Original Grad: -0.002, -lr * Pred Grad: -0.005, New P: 0.466
-Original Grad: -0.025, -lr * Pred Grad: -0.052, New P: 2.300
iter 27 loss: 0.011
Actual params: [0.4655, 2.3   ]
-Original Grad: -0.003, -lr * Pred Grad: -0.021, New P: 0.444
-Original Grad: 0.008, -lr * Pred Grad: -0.014, New P: 2.286
iter 28 loss: 0.011
Actual params: [0.4442, 2.2856]
-Original Grad: 0.008, -lr * Pred Grad: -0.002, New P: 0.443
-Original Grad: 0.011, -lr * Pred Grad: -0.000, New P: 2.285
iter 29 loss: 0.011
Actual params: [0.4426, 2.2851]
-Original Grad: 0.015, -lr * Pred Grad: 0.027, New P: 0.469
-Original Grad: -0.004, -lr * Pred Grad: -0.026, New P: 2.259
iter 30 loss: 0.010
Actual params: [0.4694, 2.2588]
-Original Grad: 0.011, -lr * Pred Grad: 0.030, New P: 0.499
-Original Grad: -0.023, -lr * Pred Grad: -0.079, New P: 2.179
Target params: [1.3344, 1.5708]
iter 0 loss: 0.492
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.040, -lr * Pred Grad: 1.034, New P: 0.561
-Original Grad: 0.163, -lr * Pred Grad: 1.350, New P: 1.353
iter 1 loss: 0.035
Actual params: [0.5612, 1.3533]
-Original Grad: 0.034, -lr * Pred Grad: 0.652, New P: 1.213
-Original Grad: 0.184, -lr * Pred Grad: 1.116, New P: 2.469
iter 2 loss: 0.252
Actual params: [1.213 , 2.4689]
-Original Grad: 0.024, -lr * Pred Grad: 0.542, New P: 1.755
-Original Grad: -0.728, -lr * Pred Grad: -0.791, New P: 1.678
iter 3 loss: 0.034
Actual params: [1.7548, 1.6783]
-Original Grad: 0.034, -lr * Pred Grad: 0.448, New P: 2.202
-Original Grad: 0.023, -lr * Pred Grad: -0.210, New P: 1.468
iter 4 loss: 0.064
Actual params: [2.2024, 1.468 ]
-Original Grad: -0.042, -lr * Pred Grad: -0.048, New P: 2.154
-Original Grad: 0.386, -lr * Pred Grad: 0.552, New P: 2.020
iter 5 loss: 0.012
Actual params: [2.1544, 2.02  ]
-Original Grad: 0.006, -lr * Pred Grad: 0.087, New P: 2.242
-Original Grad: -0.001, -lr * Pred Grad: 0.028, New P: 2.048
iter 6 loss: 0.012
Actual params: [2.2418, 2.0477]
-Original Grad: 0.001, -lr * Pred Grad: 0.037, New P: 2.279
-Original Grad: 0.031, -lr * Pred Grad: 0.107, New P: 2.154
iter 7 loss: 0.014
Actual params: [2.2788, 2.1544]
-Original Grad: 0.013, -lr * Pred Grad: 0.064, New P: 2.343
-Original Grad: -0.041, -lr * Pred Grad: -0.082, New P: 2.073
iter 8 loss: 0.013
Actual params: [2.343 , 2.0728]
-Original Grad: -0.011, -lr * Pred Grad: -0.005, New P: 2.338
-Original Grad: 0.032, -lr * Pred Grad: 0.035, New P: 2.108
iter 9 loss: 0.013
Actual params: [2.3382, 2.1076]
-Original Grad: -0.001, -lr * Pred Grad: -0.005, New P: 2.333
-Original Grad: -0.001, -lr * Pred Grad: -0.012, New P: 2.096
iter 10 loss: 0.012
Actual params: [2.3333, 2.0958]
-Original Grad: -0.007, -lr * Pred Grad: -0.032, New P: 2.301
-Original Grad: -0.022, -lr * Pred Grad: -0.066, New P: 2.030
iter 11 loss: 0.013
Actual params: [2.3011, 2.0298]
-Original Grad: -0.012, -lr * Pred Grad: -0.062, New P: 2.239
-Original Grad: 0.076, -lr * Pred Grad: 0.151, New P: 2.181
iter 12 loss: 0.016
Actual params: [2.2388, 2.1806]
-Original Grad: 0.023, -lr * Pred Grad: 0.016, New P: 2.255
-Original Grad: -0.100, -lr * Pred Grad: -0.200, New P: 1.981
iter 13 loss: 0.013
Actual params: [2.2545, 1.981 ]
-Original Grad: -0.020, -lr * Pred Grad: -0.069, New P: 2.185
-Original Grad: 0.051, -lr * Pred Grad: 0.020, New P: 2.001
iter 14 loss: 0.012
Actual params: [2.1854, 2.001 ]
-Original Grad: 0.008, -lr * Pred Grad: -0.027, New P: 2.158
-Original Grad: 0.038, -lr * Pred Grad: 0.078, New P: 2.079
iter 15 loss: 0.013
Actual params: [2.1584, 2.0787]
-Original Grad: 0.029, -lr * Pred Grad: 0.046, New P: 2.204
-Original Grad: -0.035, -lr * Pred Grad: -0.076, New P: 2.003
iter 16 loss: 0.012
Actual params: [2.204 , 2.0031]
-Original Grad: 0.004, -lr * Pred Grad: 0.015, New P: 2.219
-Original Grad: 0.041, -lr * Pred Grad: 0.059, New P: 2.062
iter 17 loss: 0.012
Actual params: [2.2193, 2.062 ]
-Original Grad: 0.010, -lr * Pred Grad: 0.029, New P: 2.249
-Original Grad: 0.024, -lr * Pred Grad: 0.061, New P: 2.123
iter 18 loss: 0.013
Actual params: [2.2486, 2.123 ]
-Original Grad: 0.005, -lr * Pred Grad: 0.017, New P: 2.266
-Original Grad: -0.009, -lr * Pred Grad: -0.019, New P: 2.104
iter 19 loss: 0.012
Actual params: [2.266, 2.104]
-Original Grad: 0.004, -lr * Pred Grad: 0.010, New P: 2.276
-Original Grad: -0.013, -lr * Pred Grad: -0.051, New P: 2.053
iter 20 loss: 0.012
Actual params: [2.2764, 2.0528]
-Original Grad: -0.009, -lr * Pred Grad: -0.030, New P: 2.246
-Original Grad: 0.020, -lr * Pred Grad: 0.009, New P: 2.061
iter 21 loss: 0.012
Actual params: [2.2463, 2.0613]
-Original Grad: -0.004, -lr * Pred Grad: -0.040, New P: 2.207
-Original Grad: 0.020, -lr * Pred Grad: 0.035, New P: 2.096
iter 22 loss: 0.012
Actual params: [2.2067, 2.096 ]
-Original Grad: 0.011, -lr * Pred Grad: -0.007, New P: 2.200
-Original Grad: -0.052, -lr * Pred Grad: -0.123, New P: 1.973
iter 23 loss: 0.013
Actual params: [2.1995, 1.9728]
-Original Grad: -0.007, -lr * Pred Grad: -0.042, New P: 2.158
-Original Grad: 0.065, -lr * Pred Grad: 0.090, New P: 2.063
iter 24 loss: 0.012
Actual params: [2.1576, 2.0625]
-Original Grad: 0.004, -lr * Pred Grad: -0.026, New P: 2.132
-Original Grad: 0.005, -lr * Pred Grad: 0.019, New P: 2.082
iter 25 loss: 0.014
Actual params: [2.1317, 2.0819]
-Original Grad: 0.049, -lr * Pred Grad: 0.104, New P: 2.235
-Original Grad: -0.043, -lr * Pred Grad: -0.105, New P: 1.977
iter 26 loss: 0.013
Actual params: [2.2354, 1.9774]
-Original Grad: -0.017, -lr * Pred Grad: -0.015, New P: 2.220
-Original Grad: 0.096, -lr * Pred Grad: 0.188, New P: 2.166
iter 27 loss: 0.015
Actual params: [2.22  , 2.1656]
-Original Grad: 0.017, -lr * Pred Grad: 0.040, New P: 2.260
-Original Grad: -0.094, -lr * Pred Grad: -0.189, New P: 1.977
iter 28 loss: 0.013
Actual params: [2.2604, 1.9767]
-Original Grad: -0.013, -lr * Pred Grad: -0.031, New P: 2.229
-Original Grad: 0.034, -lr * Pred Grad: -0.003, New P: 1.973
iter 29 loss: 0.013
Actual params: [2.2289, 1.9732]
-Original Grad: -0.014, -lr * Pred Grad: -0.065, New P: 2.164
-Original Grad: 0.090, -lr * Pred Grad: 0.222, New P: 2.195
iter 30 loss: 0.020
Actual params: [2.164 , 2.1947]
-Original Grad: 0.028, -lr * Pred Grad: 0.028, New P: 2.192
-Original Grad: -0.122, -lr * Pred Grad: -0.235, New P: 1.960
Target params: [1.3344, 1.5708]
iter 0 loss: 0.239
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.011, -lr * Pred Grad: 0.760, New P: 0.288
-Original Grad: -0.005, -lr * Pred Grad: 0.609, New P: 0.612
iter 1 loss: 0.199
Actual params: [0.2881, 0.6122]
-Original Grad: 0.185, -lr * Pred Grad: 1.777, New P: 2.065
-Original Grad: -0.080, -lr * Pred Grad: -0.256, New P: 0.357
iter 2 loss: 0.096
Actual params: [2.065 , 0.3565]
-Original Grad: -0.059, -lr * Pred Grad: -0.452, New P: 1.613
-Original Grad: -0.106, -lr * Pred Grad: -0.226, New P: 0.131
iter 3 loss: 0.147
Actual params: [1.6126, 0.131 ]
-Original Grad: 0.226, -lr * Pred Grad: 1.137, New P: 2.750
-Original Grad: 0.551, -lr * Pred Grad: 1.464, New P: 1.595
iter 4 loss: 0.192
Actual params: [2.75  , 1.5951]
-Original Grad: -0.039, -lr * Pred Grad: -0.211, New P: 2.539
-Original Grad: -0.162, -lr * Pred Grad: -0.581, New P: 1.014
iter 5 loss: 0.154
Actual params: [2.5395, 1.0144]
-Original Grad: -0.057, -lr * Pred Grad: -0.032, New P: 2.507
-Original Grad: -0.054, -lr * Pred Grad: -0.071, New P: 0.943
iter 6 loss: 0.151
Actual params: [2.5071, 0.9434]
-Original Grad: -0.036, -lr * Pred Grad: -0.093, New P: 2.414
-Original Grad: -0.041, -lr * Pred Grad: -0.206, New P: 0.738
iter 7 loss: 0.141
Actual params: [2.4139, 0.7376]
-Original Grad: -0.055, -lr * Pred Grad: -0.188, New P: 2.225
-Original Grad: -0.084, -lr * Pred Grad: -0.214, New P: 0.524
iter 8 loss: 0.117
Actual params: [2.2255, 0.5237]
-Original Grad: -0.034, -lr * Pred Grad: -0.205, New P: 2.020
-Original Grad: -0.110, -lr * Pred Grad: -0.294, New P: 0.230
iter 9 loss: 0.091
Actual params: [2.0203, 0.2302]
-Original Grad: -0.030, -lr * Pred Grad: -0.223, New P: 1.798
-Original Grad: 0.033, -lr * Pred Grad: -0.121, New P: 0.109
iter 10 loss: 0.116
Actual params: [1.7978, 0.1092]
-Original Grad: 0.162, -lr * Pred Grad: 0.224, New P: 2.021
-Original Grad: 0.402, -lr * Pred Grad: 1.185, New P: 1.294
iter 11 loss: 0.151
Actual params: [2.0214, 1.294 ]
-Original Grad: -0.034, -lr * Pred Grad: -0.032, New P: 1.989
-Original Grad: -0.142, -lr * Pred Grad: -0.438, New P: 0.856
iter 12 loss: 0.127
Actual params: [1.9891, 0.8564]
-Original Grad: -0.023, -lr * Pred Grad: -0.065, New P: 1.924
-Original Grad: -0.078, -lr * Pred Grad: -0.113, New P: 0.743
iter 13 loss: 0.120
Actual params: [1.9245, 0.7431]
-Original Grad: -0.028, -lr * Pred Grad: -0.123, New P: 1.801
-Original Grad: -0.104, -lr * Pred Grad: -0.294, New P: 0.449
iter 14 loss: 0.091
Actual params: [1.8014, 0.4493]
-Original Grad: -0.044, -lr * Pred Grad: -0.211, New P: 1.590
-Original Grad: -0.204, -lr * Pred Grad: -0.375, New P: 0.074
iter 15 loss: 0.170
Actual params: [1.5902, 0.0743]
-Original Grad: 0.169, -lr * Pred Grad: 0.280, New P: 1.870
-Original Grad: 0.465, -lr * Pred Grad: 0.820, New P: 0.895
iter 16 loss: 0.124
Actual params: [1.8704, 0.8947]
-Original Grad: -0.035, -lr * Pred Grad: -0.021, New P: 1.850
-Original Grad: -0.072, -lr * Pred Grad: -0.196, New P: 0.699
iter 17 loss: 0.116
Actual params: [1.8498, 0.6988]
-Original Grad: -0.033, -lr * Pred Grad: -0.080, New P: 1.770
-Original Grad: -0.097, -lr * Pred Grad: -0.158, New P: 0.541
iter 18 loss: 0.100
Actual params: [1.7702, 0.5405]
-Original Grad: -0.030, -lr * Pred Grad: -0.134, New P: 1.636
-Original Grad: -0.173, -lr * Pred Grad: -0.344, New P: 0.197
iter 19 loss: 0.122
Actual params: [1.6362, 0.1966]
-Original Grad: 0.136, -lr * Pred Grad: 0.262, New P: 1.898
-Original Grad: 0.594, -lr * Pred Grad: 1.260, New P: 1.456
iter 20 loss: 0.160
Actual params: [1.8984, 1.4564]
-Original Grad: -0.042, -lr * Pred Grad: -0.034, New P: 1.864
-Original Grad: -0.172, -lr * Pred Grad: -0.530, New P: 0.926
iter 21 loss: 0.125
Actual params: [1.8643, 0.926 ]
-Original Grad: -0.038, -lr * Pred Grad: -0.106, New P: 1.758
-Original Grad: -0.070, -lr * Pred Grad: -0.094, New P: 0.832
iter 22 loss: 0.117
Actual params: [1.7582, 0.8322]
-Original Grad: -0.042, -lr * Pred Grad: -0.184, New P: 1.574
-Original Grad: -0.072, -lr * Pred Grad: -0.260, New P: 0.572
iter 23 loss: 0.095
Actual params: [1.5739, 0.5718]
-Original Grad: -0.032, -lr * Pred Grad: -0.216, New P: 1.358
-Original Grad: -0.176, -lr * Pred Grad: -0.334, New P: 0.238
iter 24 loss: 0.159
Actual params: [1.3583, 0.2383]
-Original Grad: 0.131, -lr * Pred Grad: 0.179, New P: 1.538
-Original Grad: 0.648, -lr * Pred Grad: 1.370, New P: 1.608
iter 25 loss: 0.148
Actual params: [1.5377, 1.6082]
-Original Grad: -0.094, -lr * Pred Grad: -0.206, New P: 1.332
-Original Grad: -0.277, -lr * Pred Grad: -0.699, New P: 0.909
iter 26 loss: 0.090
Actual params: [1.3317, 0.909 ]
-Original Grad: -0.030, -lr * Pred Grad: -0.210, New P: 1.121
-Original Grad: -0.054, -lr * Pred Grad: -0.092, New P: 0.817
iter 27 loss: 0.100
Actual params: [1.1213, 0.8167]
-Original Grad: 0.206, -lr * Pred Grad: 0.355, New P: 1.477
-Original Grad: -0.111, -lr * Pred Grad: -0.337, New P: 0.479
iter 28 loss: 0.081
Actual params: [1.4765, 0.4795]
-Original Grad: -0.051, -lr * Pred Grad: -0.041, New P: 1.435
-Original Grad: -0.184, -lr * Pred Grad: -0.363, New P: 0.117
iter 29 loss: 0.187
Actual params: [1.4351, 0.1168]
-Original Grad: 0.232, -lr * Pred Grad: 0.578, New P: 2.013
-Original Grad: 0.846, -lr * Pred Grad: 1.643, New P: 1.760
iter 30 loss: 0.194
Actual params: [2.0135, 1.7596]
-Original Grad: -0.023, -lr * Pred Grad: 0.082, New P: 2.095
-Original Grad: -0.225, -lr * Pred Grad: -0.732, New P: 1.027
Target params: [1.3344, 1.5708]
iter 0 loss: 0.065
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.058, -lr * Pred Grad: 1.190, New P: 0.717
-Original Grad: -0.025, -lr * Pred Grad: 0.513, New P: 0.516
iter 1 loss: 0.124
Actual params: [0.7173, 0.5164]
-Original Grad: -0.176, -lr * Pred Grad: -1.185, New P: -0.468
-Original Grad: 0.412, -lr * Pred Grad: 2.060, New P: 2.577
iter 2 loss: 0.074
Actual params: [-0.4676,  2.5768]
-Original Grad: -0.008, -lr * Pred Grad: -0.342, New P: -0.809
-Original Grad: -0.019, -lr * Pred Grad: -0.250, New P: 2.327
iter 3 loss: 0.072
Actual params: [-0.8091,  2.3267]
-Original Grad: -0.000, -lr * Pred Grad: -0.237, New P: -1.046
-Original Grad: -0.000, -lr * Pred Grad: -0.053, New P: 2.274
iter 4 loss: 0.072
Actual params: [-1.0457,  2.274 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.187, New P: -1.233
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 2.245
iter 5 loss: 0.072
Actual params: [-1.2328,  2.2448]
-Original Grad: -0.000, -lr * Pred Grad: -0.144, New P: -1.377
-Original Grad: -0.000, -lr * Pred Grad: -0.038, New P: 2.207
iter 6 loss: 0.072
Actual params: [-1.377 ,  2.2072]
-Original Grad: -0.000, -lr * Pred Grad: -0.113, New P: -1.490
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: 2.174
iter 7 loss: 0.072
Actual params: [-1.4904,  2.1736]
-Original Grad: 0.000, -lr * Pred Grad: -0.092, New P: -1.582
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 2.145
iter 8 loss: 0.072
Actual params: [-1.5823,  2.1453]
-Original Grad: -0.000, -lr * Pred Grad: -0.077, New P: -1.660
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 2.120
iter 9 loss: 0.072
Actual params: [-1.6596,  2.1204]
-Original Grad: 0.000, -lr * Pred Grad: -0.067, New P: -1.727
-Original Grad: -0.000, -lr * Pred Grad: -0.024, New P: 2.096
iter 10 loss: 0.072
Actual params: [-1.7268,  2.0962]
-Original Grad: 0.000, -lr * Pred Grad: -0.060, New P: -1.787
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 2.071
iter 11 loss: 0.072
Actual params: [-1.787 ,  2.0714]
-Original Grad: 0.000, -lr * Pred Grad: -0.055, New P: -1.842
-Original Grad: -0.000, -lr * Pred Grad: -0.026, New P: 2.045
iter 12 loss: 0.072
Actual params: [-1.8424,  2.0454]
-Original Grad: 0.000, -lr * Pred Grad: -0.052, New P: -1.894
-Original Grad: -0.000, -lr * Pred Grad: -0.027, New P: 2.018
iter 13 loss: 0.072
Actual params: [-1.8943,  2.0183]
-Original Grad: 0.000, -lr * Pred Grad: -0.049, New P: -1.944
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 1.990
iter 14 loss: 0.072
Actual params: [-1.9437,  1.9901]
-Original Grad: 0.000, -lr * Pred Grad: -0.048, New P: -1.991
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 1.961
iter 15 loss: 0.072
Actual params: [-1.9913,  1.961 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: -2.038
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 1.931
iter 16 loss: 0.072
Actual params: [-2.0376,  1.9311]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -2.083
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 1.901
iter 17 loss: 0.072
Actual params: [-2.0831,  1.9006]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -2.128
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 1.870
iter 18 loss: 0.072
Actual params: [-2.1278,  1.8695]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -2.172
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 1.838
iter 19 loss: 0.072
Actual params: [-2.1721,  1.8382]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -2.216
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.806
iter 20 loss: 0.072
Actual params: [-2.2161,  1.8065]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -2.260
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.775
iter 21 loss: 0.072
Actual params: [-2.2599,  1.7745]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -2.304
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.742
iter 22 loss: 0.072
Actual params: [-2.3035,  1.7425]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -2.347
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.710
iter 23 loss: 0.072
Actual params: [-2.3471,  1.7102]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.391
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.678
iter 24 loss: 0.072
Actual params: [-2.3905,  1.6779]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.434
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.646
iter 25 loss: 0.072
Actual params: [-2.4339,  1.6456]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.477
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.613
iter 26 loss: 0.072
Actual params: [-2.4774,  1.6131]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.521
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.581
iter 27 loss: 0.072
Actual params: [-2.5208,  1.5807]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.564
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.548
iter 28 loss: 0.072
Actual params: [-2.5641,  1.5482]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.608
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.516
iter 29 loss: 0.072
Actual params: [-2.6075,  1.5157]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.651
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.483
iter 30 loss: 0.072
Actual params: [-2.6509,  1.4832]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.694
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.451
Target params: [1.3344, 1.5708]
iter 0 loss: 0.575
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.064, -lr * Pred Grad: -0.056, New P: -0.528
-Original Grad: 0.197, -lr * Pred Grad: 1.471, New P: 1.475
iter 1 loss: 0.537
Actual params: [-0.5283,  1.4747]
-Original Grad: 0.045, -lr * Pred Grad: 0.672, New P: 0.144
-Original Grad: 0.009, -lr * Pred Grad: 0.081, New P: 1.556
iter 2 loss: 0.386
Actual params: [0.1435, 1.5559]
-Original Grad: 0.385, -lr * Pred Grad: 1.820, New P: 1.964
-Original Grad: 0.114, -lr * Pred Grad: 0.457, New P: 2.013
iter 3 loss: 0.116
Actual params: [1.9636, 2.0132]
-Original Grad: -0.226, -lr * Pred Grad: -1.039, New P: 0.925
-Original Grad: 0.402, -lr * Pred Grad: 1.803, New P: 3.816
iter 4 loss: 0.976
Actual params: [0.9246, 3.816 ]
-Original Grad: -0.448, -lr * Pred Grad: -1.326, New P: -0.401
-Original Grad: -1.329, -lr * Pred Grad: -0.722, New P: 3.094
iter 5 loss: 0.543
Actual params: [-0.4013,  3.0941]
-Original Grad: 0.013, -lr * Pred Grad: -0.890, New P: -1.291
-Original Grad: -0.020, -lr * Pred Grad: -0.279, New P: 2.815
iter 6 loss: 0.545
Actual params: [-1.2915,  2.8153]
-Original Grad: 0.000, -lr * Pred Grad: -0.691, New P: -1.982
-Original Grad: -0.000, -lr * Pred Grad: -0.416, New P: 2.399
iter 7 loss: 0.545
Actual params: [-1.9821,  2.3991]
-Original Grad: 0.000, -lr * Pred Grad: -0.589, New P: -2.572
-Original Grad: -0.000, -lr * Pred Grad: -0.236, New P: 2.163
iter 8 loss: 0.545
Actual params: [-2.5715,  2.1635]
-Original Grad: 0.000, -lr * Pred Grad: -0.533, New P: -3.104
-Original Grad: -0.000, -lr * Pred Grad: -0.182, New P: 1.981
iter 9 loss: 0.545
Actual params: [-3.104 ,  1.9811]
-Original Grad: 0.000, -lr * Pred Grad: -0.479, New P: -3.583
-Original Grad: -0.000, -lr * Pred Grad: -0.129, New P: 1.852
iter 10 loss: 0.545
Actual params: [-3.5835,  1.8519]
-Original Grad: 0.000, -lr * Pred Grad: -0.426, New P: -4.010
-Original Grad: -0.000, -lr * Pred Grad: -0.099, New P: 1.753
iter 11 loss: 0.545
Actual params: [-4.0096,  1.753 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.373, New P: -4.383
-Original Grad: -0.000, -lr * Pred Grad: -0.075, New P: 1.678
iter 12 loss: 0.545
Actual params: [-4.383,  1.678]
-Original Grad: 0.000, -lr * Pred Grad: -0.322, New P: -4.705
-Original Grad: -0.000, -lr * Pred Grad: -0.060, New P: 1.618
iter 13 loss: 0.545
Actual params: [-4.7054,  1.6181]
-Original Grad: 0.000, -lr * Pred Grad: -0.275, New P: -4.981
-Original Grad: -0.000, -lr * Pred Grad: -0.050, New P: 1.568
iter 14 loss: 0.545
Actual params: [-4.9805,  1.5681]
-Original Grad: 0.000, -lr * Pred Grad: -0.233, New P: -5.213
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: 1.524
iter 15 loss: 0.545
Actual params: [-5.2132,  1.5239]
-Original Grad: 0.000, -lr * Pred Grad: -0.196, New P: -5.409
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: 1.483
iter 16 loss: 0.545
Actual params: [-5.4089,  1.4832]
-Original Grad: 0.000, -lr * Pred Grad: -0.164, New P: -5.573
-Original Grad: -0.000, -lr * Pred Grad: -0.039, New P: 1.445
iter 17 loss: 0.545
Actual params: [-5.5734,  1.4446]
-Original Grad: 0.000, -lr * Pred Grad: -0.139, New P: -5.712
-Original Grad: 0.000, -lr * Pred Grad: -0.037, New P: 1.407
iter 18 loss: 0.545
Actual params: [-5.7121,  1.4073]
-Original Grad: 0.000, -lr * Pred Grad: -0.118, New P: -5.830
-Original Grad: 0.000, -lr * Pred Grad: -0.036, New P: 1.371
iter 19 loss: 0.545
Actual params: [-5.8298,  1.3711]
-Original Grad: 0.000, -lr * Pred Grad: -0.101, New P: -5.931
-Original Grad: 0.000, -lr * Pred Grad: -0.036, New P: 1.336
iter 20 loss: 0.545
Actual params: [-5.9309,  1.3355]
-Original Grad: 0.000, -lr * Pred Grad: -0.088, New P: -6.019
-Original Grad: 0.000, -lr * Pred Grad: -0.035, New P: 1.301
iter 21 loss: 0.545
Actual params: [-6.0189,  1.3005]
-Original Grad: -0.000, -lr * Pred Grad: -0.078, New P: -6.097
-Original Grad: 0.000, -lr * Pred Grad: -0.034, New P: 1.266
iter 22 loss: 0.545
Actual params: [-6.0966,  1.2661]
-Original Grad: -0.000, -lr * Pred Grad: -0.070, New P: -6.166
-Original Grad: 0.000, -lr * Pred Grad: -0.034, New P: 1.232
iter 23 loss: 0.545
Actual params: [-6.1663,  1.232 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.064, New P: -6.230
-Original Grad: 0.000, -lr * Pred Grad: -0.034, New P: 1.198
iter 24 loss: 0.545
Actual params: [-6.2299,  1.1982]
-Original Grad: 0.000, -lr * Pred Grad: -0.059, New P: -6.289
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.165
iter 25 loss: 0.545
Actual params: [-6.2887,  1.1647]
-Original Grad: -0.000, -lr * Pred Grad: -0.055, New P: -6.344
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.131
iter 26 loss: 0.545
Actual params: [-6.3438,  1.1315]
-Original Grad: -0.000, -lr * Pred Grad: -0.052, New P: -6.396
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.098
iter 27 loss: 0.545
Actual params: [-6.3961,  1.0983]
-Original Grad: -0.000, -lr * Pred Grad: -0.050, New P: -6.446
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.065
iter 28 loss: 0.545
Actual params: [-6.4462,  1.0654]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -6.495
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.032
iter 29 loss: 0.545
Actual params: [-6.4947,  1.0325]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -6.542
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.000
iter 30 loss: 0.545
Actual params: [-6.5419,  0.9997]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -6.588
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.967
Target params: [1.3344, 1.5708]
iter 0 loss: 0.309
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.000, -lr * Pred Grad: 0.641, New P: 0.169
-Original Grad: 0.000, -lr * Pred Grad: 0.636, New P: 0.639
iter 1 loss: 0.214
Actual params: [0.1685, 0.6393]
-Original Grad: 0.706, -lr * Pred Grad: 2.484, New P: 2.652
-Original Grad: 0.590, -lr * Pred Grad: 2.474, New P: 3.113
iter 2 loss: 0.268
Actual params: [2.6522, 3.1129]
-Original Grad: -0.005, -lr * Pred Grad: -0.906, New P: 1.747
-Original Grad: -0.168, -lr * Pred Grad: -0.673, New P: 2.440
iter 3 loss: 0.115
Actual params: [1.7466, 2.4404]
-Original Grad: -0.056, -lr * Pred Grad: 0.115, New P: 1.862
-Original Grad: -0.355, -lr * Pred Grad: -0.373, New P: 2.067
iter 4 loss: 0.066
Actual params: [1.8616, 2.0675]
-Original Grad: -0.038, -lr * Pred Grad: -0.059, New P: 1.803
-Original Grad: -0.188, -lr * Pred Grad: -0.470, New P: 1.597
iter 5 loss: 0.038
Actual params: [1.8029, 1.5974]
-Original Grad: -0.057, -lr * Pred Grad: -0.135, New P: 1.668
-Original Grad: -0.033, -lr * Pred Grad: -0.342, New P: 1.255
iter 6 loss: 0.033
Actual params: [1.668, 1.255]
-Original Grad: -0.009, -lr * Pred Grad: -0.087, New P: 1.581
-Original Grad: 0.105, -lr * Pred Grad: -0.010, New P: 1.245
iter 7 loss: 0.038
Actual params: [1.5806, 1.2451]
-Original Grad: 0.045, -lr * Pred Grad: 0.044, New P: 1.625
-Original Grad: 0.185, -lr * Pred Grad: 0.508, New P: 1.753
iter 8 loss: 0.033
Actual params: [1.625 , 1.7534]
-Original Grad: -0.024, -lr * Pred Grad: -0.055, New P: 1.570
-Original Grad: -0.043, -lr * Pred Grad: -0.072, New P: 1.681
iter 9 loss: 0.030
Actual params: [1.5695, 1.681 ]
-Original Grad: -0.028, -lr * Pred Grad: -0.111, New P: 1.459
-Original Grad: -0.054, -lr * Pred Grad: -0.107, New P: 1.574
iter 10 loss: 0.029
Actual params: [1.4587, 1.5737]
-Original Grad: -0.009, -lr * Pred Grad: -0.104, New P: 1.355
-Original Grad: 0.035, -lr * Pred Grad: -0.003, New P: 1.570
iter 11 loss: 0.028
Actual params: [1.3552, 1.5704]
-Original Grad: -0.013, -lr * Pred Grad: -0.118, New P: 1.237
-Original Grad: 0.057, -lr * Pred Grad: 0.135, New P: 1.705
iter 12 loss: 0.025
Actual params: [1.2374, 1.7054]
-Original Grad: -0.019, -lr * Pred Grad: -0.146, New P: 1.092
-Original Grad: 0.021, -lr * Pred Grad: 0.085, New P: 1.790
iter 13 loss: 0.022
Actual params: [1.0918, 1.7902]
-Original Grad: -0.011, -lr * Pred Grad: -0.142, New P: 0.949
-Original Grad: 0.019, -lr * Pred Grad: 0.068, New P: 1.858
iter 14 loss: 0.021
Actual params: [0.9494, 1.8579]
-Original Grad: -0.030, -lr * Pred Grad: -0.198, New P: 0.752
-Original Grad: -0.075, -lr * Pred Grad: -0.161, New P: 1.697
iter 15 loss: 0.020
Actual params: [0.7519, 1.6972]
-Original Grad: -0.016, -lr * Pred Grad: -0.196, New P: 0.556
-Original Grad: -0.016, -lr * Pred Grad: -0.110, New P: 1.587
iter 16 loss: 0.031
Actual params: [0.5559, 1.5868]
-Original Grad: 0.068, -lr * Pred Grad: 0.032, New P: 0.588
-Original Grad: 0.001, -lr * Pred Grad: -0.072, New P: 1.515
iter 17 loss: 0.030
Actual params: [0.5879, 1.5148]
-Original Grad: 0.069, -lr * Pred Grad: 0.158, New P: 0.746
-Original Grad: 0.035, -lr * Pred Grad: 0.036, New P: 1.551
iter 18 loss: 0.024
Actual params: [0.7463, 1.5511]
-Original Grad: -0.012, -lr * Pred Grad: 0.035, New P: 0.781
-Original Grad: 0.047, -lr * Pred Grad: 0.122, New P: 1.673
iter 19 loss: 0.021
Actual params: [0.7811, 1.6734]
-Original Grad: -0.001, -lr * Pred Grad: 0.024, New P: 0.805
-Original Grad: -0.006, -lr * Pred Grad: 0.003, New P: 1.676
iter 20 loss: 0.021
Actual params: [0.8048, 1.676 ]
-Original Grad: -0.038, -lr * Pred Grad: -0.107, New P: 0.698
-Original Grad: 0.014, -lr * Pred Grad: 0.025, New P: 1.701
iter 21 loss: 0.021
Actual params: [0.6979, 1.7007]
-Original Grad: 0.023, -lr * Pred Grad: 0.001, New P: 0.699
-Original Grad: -0.030, -lr * Pred Grad: -0.083, New P: 1.618
iter 22 loss: 0.022
Actual params: [0.6993, 1.6176]
-Original Grad: -0.009, -lr * Pred Grad: -0.052, New P: 0.648
-Original Grad: 0.009, -lr * Pred Grad: -0.027, New P: 1.591
iter 23 loss: 0.024
Actual params: [0.6476, 1.5909]
-Original Grad: 0.012, -lr * Pred Grad: -0.009, New P: 0.639
-Original Grad: 0.030, -lr * Pred Grad: 0.043, New P: 1.634
iter 24 loss: 0.024
Actual params: [0.6386, 1.6341]
-Original Grad: 0.031, -lr * Pred Grad: 0.063, New P: 0.702
-Original Grad: -0.004, -lr * Pred Grad: -0.015, New P: 1.619
iter 25 loss: 0.022
Actual params: [0.7018, 1.6189]
-Original Grad: 0.015, -lr * Pred Grad: 0.059, New P: 0.761
-Original Grad: 0.015, -lr * Pred Grad: 0.017, New P: 1.636
iter 26 loss: 0.021
Actual params: [0.7611, 1.6361]
-Original Grad: -0.041, -lr * Pred Grad: -0.096, New P: 0.665
-Original Grad: 0.024, -lr * Pred Grad: 0.049, New P: 1.685
iter 27 loss: 0.023
Actual params: [0.6654, 1.6854]
-Original Grad: 0.019, -lr * Pred Grad: -0.003, New P: 0.662
-Original Grad: 0.009, -lr * Pred Grad: 0.023, New P: 1.708
iter 28 loss: 0.023
Actual params: [0.6623, 1.7083]
-Original Grad: 0.021, -lr * Pred Grad: 0.035, New P: 0.697
-Original Grad: -0.028, -lr * Pred Grad: -0.075, New P: 1.633
iter 29 loss: 0.022
Actual params: [0.697 , 1.6335]
-Original Grad: 0.063, -lr * Pred Grad: 0.178, New P: 0.875
-Original Grad: 0.016, -lr * Pred Grad: -0.009, New P: 1.624
iter 30 loss: 0.026
Actual params: [0.8752, 1.6245]
-Original Grad: -0.055, -lr * Pred Grad: -0.084, New P: 0.791
-Original Grad: 0.084, -lr * Pred Grad: 0.213, New P: 1.837
