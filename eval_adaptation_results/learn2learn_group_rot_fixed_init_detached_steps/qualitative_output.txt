Target params: [1.3344, 1.5708]
iter 0 loss: 0.077
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.035, -lr * Pred Grad: 0.267, New P: -0.205
-Original Grad: -0.000, -lr * Pred Grad: 0.633, New P: 0.637
iter 1 loss: 0.079
Actual params: [-0.2052,  0.637 ]
-Original Grad: -0.022, -lr * Pred Grad: 0.082, New P: -0.123
-Original Grad: 0.047, -lr * Pred Grad: 0.385, New P: 1.022
iter 2 loss: 0.070
Actual params: [-0.1228,  1.0225]
-Original Grad: 0.011, -lr * Pred Grad: 0.200, New P: 0.077
-Original Grad: 0.031, -lr * Pred Grad: 0.060, New P: 1.083
iter 3 loss: 0.065
Actual params: [0.0774, 1.0829]
-Original Grad: 0.025, -lr * Pred Grad: 0.227, New P: 0.304
-Original Grad: 0.071, -lr * Pred Grad: 0.219, New P: 1.302
iter 4 loss: 0.049
Actual params: [0.3044, 1.3016]
-Original Grad: 0.082, -lr * Pred Grad: 0.489, New P: 0.794
-Original Grad: 0.085, -lr * Pred Grad: 0.308, New P: 1.609
iter 5 loss: 0.005
Actual params: [0.7938, 1.6094]
-Original Grad: 0.038, -lr * Pred Grad: 0.342, New P: 1.136
-Original Grad: 0.043, -lr * Pred Grad: 0.183, New P: 1.793
iter 6 loss: 0.006
Actual params: [1.1363, 1.7926]
-Original Grad: -0.007, -lr * Pred Grad: 0.166, New P: 1.302
-Original Grad: -0.087, -lr * Pred Grad: -0.176, New P: 1.617
iter 7 loss: 0.006
Actual params: [1.302, 1.617]
-Original Grad: -0.022, -lr * Pred Grad: 0.031, New P: 1.333
-Original Grad: 0.071, -lr * Pred Grad: 0.088, New P: 1.705
iter 8 loss: 0.005
Actual params: [1.3334, 1.7046]
-Original Grad: -0.011, -lr * Pred Grad: -0.000, New P: 1.333
-Original Grad: 0.022, -lr * Pred Grad: 0.064, New P: 1.769
iter 9 loss: 0.006
Actual params: [1.3334, 1.7687]
-Original Grad: -0.001, -lr * Pred Grad: -0.005, New P: 1.328
-Original Grad: -0.043, -lr * Pred Grad: -0.086, New P: 1.682
iter 10 loss: 0.005
Actual params: [1.3284, 1.6823]
-Original Grad: -0.013, -lr * Pred Grad: -0.050, New P: 1.278
-Original Grad: 0.035, -lr * Pred Grad: 0.035, New P: 1.717
iter 11 loss: 0.004
Actual params: [1.2784, 1.717 ]
-Original Grad: -0.003, -lr * Pred Grad: -0.047, New P: 1.232
-Original Grad: -0.030, -lr * Pred Grad: -0.077, New P: 1.640
iter 12 loss: 0.004
Actual params: [1.2317, 1.64  ]
-Original Grad: -0.021, -lr * Pred Grad: -0.103, New P: 1.129
-Original Grad: 0.053, -lr * Pred Grad: 0.089, New P: 1.728
iter 13 loss: 0.004
Actual params: [1.1291, 1.7285]
-Original Grad: -0.010, -lr * Pred Grad: -0.105, New P: 1.024
-Original Grad: -0.029, -lr * Pred Grad: -0.065, New P: 1.663
iter 14 loss: 0.002
Actual params: [1.0242, 1.6634]
-Original Grad: 0.007, -lr * Pred Grad: -0.064, New P: 0.960
-Original Grad: -0.012, -lr * Pred Grad: -0.059, New P: 1.604
iter 15 loss: 0.002
Actual params: [0.9604, 1.6042]
-Original Grad: 0.029, -lr * Pred Grad: 0.019, New P: 0.979
-Original Grad: 0.046, -lr * Pred Grad: 0.068, New P: 1.672
iter 16 loss: 0.002
Actual params: [0.9794, 1.6718]
-Original Grad: 0.006, -lr * Pred Grad: 0.002, New P: 0.981
-Original Grad: -0.012, -lr * Pred Grad: -0.028, New P: 1.644
iter 17 loss: 0.002
Actual params: [0.981 , 1.6438]
-Original Grad: 0.020, -lr * Pred Grad: 0.046, New P: 1.027
-Original Grad: 0.013, -lr * Pred Grad: 0.009, New P: 1.653
iter 18 loss: 0.002
Actual params: [1.0273, 1.653 ]
-Original Grad: -0.002, -lr * Pred Grad: 0.005, New P: 1.032
-Original Grad: 0.029, -lr * Pred Grad: 0.059, New P: 1.711
iter 19 loss: 0.003
Actual params: [1.0319, 1.7115]
-Original Grad: 0.001, -lr * Pred Grad: -0.003, New P: 1.029
-Original Grad: -0.029, -lr * Pred Grad: -0.069, New P: 1.643
iter 20 loss: 0.002
Actual params: [1.0292, 1.6426]
-Original Grad: 0.004, -lr * Pred Grad: -0.003, New P: 1.026
-Original Grad: 0.019, -lr * Pred Grad: 0.005, New P: 1.648
iter 21 loss: 0.002
Actual params: [1.026 , 1.6477]
-Original Grad: -0.002, -lr * Pred Grad: -0.020, New P: 1.006
-Original Grad: 0.030, -lr * Pred Grad: 0.056, New P: 1.704
iter 22 loss: 0.003
Actual params: [1.0063, 1.7042]
-Original Grad: -0.002, -lr * Pred Grad: -0.030, New P: 0.977
-Original Grad: -0.033, -lr * Pred Grad: -0.078, New P: 1.626
iter 23 loss: 0.002
Actual params: [0.9765, 1.6257]
-Original Grad: 0.004, -lr * Pred Grad: -0.019, New P: 0.958
-Original Grad: 0.033, -lr * Pred Grad: 0.034, New P: 1.660
iter 24 loss: 0.002
Actual params: [0.9577, 1.66  ]
-Original Grad: 0.018, -lr * Pred Grad: 0.023, New P: 0.981
-Original Grad: -0.015, -lr * Pred Grad: -0.049, New P: 1.611
iter 25 loss: 0.002
Actual params: [0.9807, 1.6111]
-Original Grad: 0.027, -lr * Pred Grad: 0.072, New P: 1.052
-Original Grad: 0.040, -lr * Pred Grad: 0.069, New P: 1.680
iter 26 loss: 0.002
Actual params: [1.0523, 1.6802]
-Original Grad: -0.006, -lr * Pred Grad: 0.009, New P: 1.061
-Original Grad: -0.005, -lr * Pred Grad: -0.013, New P: 1.667
iter 27 loss: 0.002
Actual params: [1.0611, 1.6674]
-Original Grad: -0.009, -lr * Pred Grad: -0.026, New P: 1.035
-Original Grad: 0.011, -lr * Pred Grad: 0.009, New P: 1.677
iter 28 loss: 0.002
Actual params: [1.0355, 1.6765]
-Original Grad: 0.007, -lr * Pred Grad: -0.003, New P: 1.033
-Original Grad: -0.006, -lr * Pred Grad: -0.032, New P: 1.645
iter 29 loss: 0.002
Actual params: [1.0328, 1.6446]
-Original Grad: 0.002, -lr * Pred Grad: -0.012, New P: 1.020
-Original Grad: 0.026, -lr * Pred Grad: 0.037, New P: 1.682
iter 30 loss: 0.002
Actual params: [1.0204, 1.6817]
-Original Grad: 0.004, -lr * Pred Grad: -0.009, New P: 1.011
-Original Grad: -0.017, -lr * Pred Grad: -0.048, New P: 1.633
Target params: [1.3344, 1.5708]
iter 0 loss: 0.443
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad: 0.651, New P: 0.179
-Original Grad: 0.001, -lr * Pred Grad: 0.638, New P: 0.642
iter 1 loss: 0.345
Actual params: [0.1788, 0.6417]
-Original Grad: 0.269, -lr * Pred Grad: 2.038, New P: 2.217
-Original Grad: 0.375, -lr * Pred Grad: 1.998, New P: 2.640
iter 2 loss: 0.078
Actual params: [2.2167, 2.6398]
-Original Grad: 0.107, -lr * Pred Grad: 0.580, New P: 2.797
-Original Grad: -0.748, -lr * Pred Grad: -0.946, New P: 1.694
iter 3 loss: 0.104
Actual params: [2.7968, 1.6942]
-Original Grad: -0.102, -lr * Pred Grad: -0.193, New P: 2.603
-Original Grad: 0.298, -lr * Pred Grad: 0.145, New P: 1.839
iter 4 loss: 0.062
Actual params: [2.6035, 1.8389]
-Original Grad: -0.063, -lr * Pred Grad: -0.139, New P: 2.464
-Original Grad: 0.264, -lr * Pred Grad: 0.647, New P: 2.485
iter 5 loss: 0.041
Actual params: [2.4641, 2.4854]
-Original Grad: -0.007, -lr * Pred Grad: -0.047, New P: 2.417
-Original Grad: -0.235, -lr * Pred Grad: -0.402, New P: 2.083
iter 6 loss: 0.028
Actual params: [2.4173, 2.0829]
-Original Grad: -0.067, -lr * Pred Grad: -0.220, New P: 2.197
-Original Grad: 0.085, -lr * Pred Grad: 0.024, New P: 2.107
iter 7 loss: 0.013
Actual params: [2.1975, 2.1073]
-Original Grad: -0.036, -lr * Pred Grad: -0.229, New P: 1.968
-Original Grad: 0.029, -lr * Pred Grad: 0.022, New P: 2.129
iter 8 loss: 0.010
Actual params: [1.9683, 2.1293]
-Original Grad: 0.032, -lr * Pred Grad: -0.077, New P: 1.891
-Original Grad: -0.118, -lr * Pred Grad: -0.213, New P: 1.916
iter 9 loss: 0.007
Actual params: [1.891 , 1.9161]
-Original Grad: -0.015, -lr * Pred Grad: -0.123, New P: 1.768
-Original Grad: 0.050, -lr * Pred Grad: -0.030, New P: 1.887
iter 10 loss: 0.007
Actual params: [1.7681, 1.8866]
-Original Grad: 0.003, -lr * Pred Grad: -0.090, New P: 1.678
-Original Grad: 0.004, -lr * Pred Grad: -0.027, New P: 1.860
iter 11 loss: 0.008
Actual params: [1.6784, 1.8599]
-Original Grad: 0.006, -lr * Pred Grad: -0.064, New P: 1.614
-Original Grad: 0.005, -lr * Pred Grad: -0.015, New P: 1.845
iter 12 loss: 0.009
Actual params: [1.6142, 1.8447]
-Original Grad: 0.006, -lr * Pred Grad: -0.046, New P: 1.568
-Original Grad: -0.015, -lr * Pred Grad: -0.061, New P: 1.784
iter 13 loss: 0.008
Actual params: [1.5681, 1.784 ]
-Original Grad: 0.009, -lr * Pred Grad: -0.024, New P: 1.544
-Original Grad: -0.014, -lr * Pred Grad: -0.074, New P: 1.710
iter 14 loss: 0.008
Actual params: [1.5444, 1.7097]
-Original Grad: -0.004, -lr * Pred Grad: -0.045, New P: 1.499
-Original Grad: 0.040, -lr * Pred Grad: 0.046, New P: 1.756
iter 15 loss: 0.008
Actual params: [1.4994, 1.7559]
-Original Grad: 0.004, -lr * Pred Grad: -0.032, New P: 1.468
-Original Grad: 0.000, -lr * Pred Grad: -0.006, New P: 1.750
iter 16 loss: 0.009
Actual params: [1.4675, 1.7501]
-Original Grad: 0.017, -lr * Pred Grad: 0.012, New P: 1.479
-Original Grad: -0.028, -lr * Pred Grad: -0.082, New P: 1.668
iter 17 loss: 0.008
Actual params: [1.4791, 1.6681]
-Original Grad: -0.020, -lr * Pred Grad: -0.070, New P: 1.409
-Original Grad: 0.091, -lr * Pred Grad: 0.187, New P: 1.855
iter 18 loss: 0.017
Actual params: [1.4088, 1.8552]
-Original Grad: 0.047, -lr * Pred Grad: 0.082, New P: 1.491
-Original Grad: -0.132, -lr * Pred Grad: -0.253, New P: 1.602
iter 19 loss: 0.012
Actual params: [1.4907, 1.602 ]
-Original Grad: -0.036, -lr * Pred Grad: -0.087, New P: 1.404
-Original Grad: 0.184, -lr * Pred Grad: 0.332, New P: 1.934
iter 20 loss: 0.024
Actual params: [1.4042, 1.9342]
-Original Grad: 0.069, -lr * Pred Grad: 0.144, New P: 1.548
-Original Grad: -0.147, -lr * Pred Grad: -0.275, New P: 1.659
iter 21 loss: 0.009
Actual params: [1.5481, 1.6592]
-Original Grad: -0.022, -lr * Pred Grad: -0.019, New P: 1.529
-Original Grad: 0.139, -lr * Pred Grad: 0.209, New P: 1.869
iter 22 loss: 0.012
Actual params: [1.5289, 1.8685]
-Original Grad: 0.026, -lr * Pred Grad: 0.069, New P: 1.598
-Original Grad: -0.072, -lr * Pred Grad: -0.143, New P: 1.726
iter 23 loss: 0.008
Actual params: [1.5978, 1.726 ]
-Original Grad: -0.005, -lr * Pred Grad: 0.005, New P: 1.603
-Original Grad: 0.047, -lr * Pred Grad: 0.055, New P: 1.781
iter 24 loss: 0.008
Actual params: [1.6028, 1.7812]
-Original Grad: 0.001, -lr * Pred Grad: 0.002, New P: 1.604
-Original Grad: 0.027, -lr * Pred Grad: 0.062, New P: 1.843
iter 25 loss: 0.009
Actual params: [1.6043, 1.8433]
-Original Grad: 0.007, -lr * Pred Grad: 0.011, New P: 1.615
-Original Grad: -0.001, -lr * Pred Grad: 0.005, New P: 1.848
iter 26 loss: 0.009
Actual params: [1.6151, 1.8482]
-Original Grad: 0.013, -lr * Pred Grad: 0.031, New P: 1.646
-Original Grad: -0.005, -lr * Pred Grad: -0.025, New P: 1.823
iter 27 loss: 0.008
Actual params: [1.6461, 1.8232]
-Original Grad: 0.009, -lr * Pred Grad: 0.030, New P: 1.676
-Original Grad: -0.018, -lr * Pred Grad: -0.068, New P: 1.755
iter 28 loss: 0.008
Actual params: [1.6765, 1.7553]
-Original Grad: -0.008, -lr * Pred Grad: -0.017, New P: 1.659
-Original Grad: 0.064, -lr * Pred Grad: 0.119, New P: 1.874
iter 29 loss: 0.009
Actual params: [1.6594, 1.8744]
-Original Grad: 0.020, -lr * Pred Grad: 0.038, New P: 1.697
-Original Grad: -0.044, -lr * Pred Grad: -0.094, New P: 1.780
iter 30 loss: 0.007
Actual params: [1.6974, 1.7804]
-Original Grad: -0.013, -lr * Pred Grad: -0.033, New P: 1.664
-Original Grad: 0.068, -lr * Pred Grad: 0.129, New P: 1.910
Target params: [1.3344, 1.5708]
iter 0 loss: 0.725
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.018, -lr * Pred Grad: 0.829, New P: 0.356
-Original Grad: 0.046, -lr * Pred Grad: 0.850, New P: 0.853
iter 1 loss: 0.495
Actual params: [0.3565, 0.853 ]
-Original Grad: 0.558, -lr * Pred Grad: 2.454, New P: 2.810
-Original Grad: 0.569, -lr * Pred Grad: 2.513, New P: 3.366
iter 2 loss: 0.885
Actual params: [2.81  , 3.3656]
-Original Grad: 0.107, -lr * Pred Grad: -0.169, New P: 2.641
-Original Grad: -0.428, -lr * Pred Grad: -1.091, New P: 2.275
iter 3 loss: 0.518
Actual params: [2.641 , 2.2747]
-Original Grad: -0.130, -lr * Pred Grad: -0.060, New P: 2.581
-Original Grad: -0.549, -lr * Pred Grad: -0.277, New P: 1.998
iter 4 loss: 0.454
Actual params: [2.5809, 1.9977]
-Original Grad: -0.116, -lr * Pred Grad: -0.293, New P: 2.288
-Original Grad: -0.254, -lr * Pred Grad: -0.575, New P: 1.423
iter 5 loss: 0.396
Actual params: [2.2875, 1.4225]
-Original Grad: -0.170, -lr * Pred Grad: -0.560, New P: 1.727
-Original Grad: 0.122, -lr * Pred Grad: -0.247, New P: 1.175
iter 6 loss: 0.300
Actual params: [1.7274, 1.1753]
-Original Grad: -0.252, -lr * Pred Grad: -0.935, New P: 0.792
-Original Grad: 0.526, -lr * Pred Grad: 1.231, New P: 2.406
iter 7 loss: 0.366
Actual params: [0.7921, 2.406 ]
-Original Grad: 0.580, -lr * Pred Grad: 0.383, New P: 1.175
-Original Grad: -0.741, -lr * Pred Grad: -0.585, New P: 1.821
iter 8 loss: 0.074
Actual params: [1.1752, 1.8212]
-Original Grad: 0.123, -lr * Pred Grad: 0.128, New P: 1.304
-Original Grad: 0.044, -lr * Pred Grad: -0.248, New P: 1.573
iter 9 loss: 0.120
Actual params: [1.3037, 1.5728]
-Original Grad: -0.384, -lr * Pred Grad: -0.591, New P: 0.713
-Original Grad: 0.694, -lr * Pred Grad: 1.294, New P: 2.867
iter 10 loss: 0.775
Actual params: [0.7131, 2.867 ]
-Original Grad: 0.679, -lr * Pred Grad: 0.625, New P: 1.338
-Original Grad: -1.810, -lr * Pred Grad: -0.555, New P: 2.312
iter 11 loss: 0.073
Actual params: [1.3381, 2.3118]
-Original Grad: 0.146, -lr * Pred Grad: 0.217, New P: 1.555
-Original Grad: -0.176, -lr * Pred Grad: -0.427, New P: 1.885
iter 12 loss: 0.113
Actual params: [1.5549, 1.885 ]
-Original Grad: -0.465, -lr * Pred Grad: -0.606, New P: 0.949
-Original Grad: 0.463, -lr * Pred Grad: -0.008, New P: 1.877
iter 13 loss: 0.142
Actual params: [0.9487, 1.8768]
-Original Grad: 0.409, -lr * Pred Grad: 0.356, New P: 1.305
-Original Grad: 0.010, -lr * Pred Grad: -0.120, New P: 1.757
iter 14 loss: 0.073
Actual params: [1.3047, 1.7567]
-Original Grad: -0.131, -lr * Pred Grad: -0.191, New P: 1.114
-Original Grad: 0.513, -lr * Pred Grad: 1.301, New P: 3.057
iter 15 loss: 0.915
Actual params: [1.1139, 3.0572]
-Original Grad: -0.356, -lr * Pred Grad: -0.749, New P: 0.365
-Original Grad: -0.910, -lr * Pred Grad: -0.570, New P: 2.487
iter 16 loss: 0.550
Actual params: [0.3649, 2.4875]
-Original Grad: 0.013, -lr * Pred Grad: -0.518, New P: -0.153
-Original Grad: -0.885, -lr * Pred Grad: -0.450, New P: 2.037
iter 17 loss: 0.697
Actual params: [-0.1528,  2.0373]
-Original Grad: 0.093, -lr * Pred Grad: -0.229, New P: -0.382
-Original Grad: -0.151, -lr * Pred Grad: -0.584, New P: 1.453
iter 18 loss: 0.680
Actual params: [-0.3822,  1.4533]
-Original Grad: 0.130, -lr * Pred Grad: 0.039, New P: -0.343
-Original Grad: -0.083, -lr * Pred Grad: -0.537, New P: 0.916
iter 19 loss: 0.675
Actual params: [-0.3433,  0.9161]
-Original Grad: 0.116, -lr * Pred Grad: 0.223, New P: -0.120
-Original Grad: 0.067, -lr * Pred Grad: -0.329, New P: 0.587
iter 20 loss: 0.667
Actual params: [-0.1202,  0.5875]
-Original Grad: 0.126, -lr * Pred Grad: 0.417, New P: 0.297
-Original Grad: 0.187, -lr * Pred Grad: 0.115, New P: 0.703
iter 21 loss: 0.558
Actual params: [0.297 , 0.7028]
-Original Grad: 0.320, -lr * Pred Grad: 0.947, New P: 1.244
-Original Grad: 0.446, -lr * Pred Grad: 1.439, New P: 2.142
iter 22 loss: 0.084
Actual params: [1.2439, 2.1419]
-Original Grad: 0.141, -lr * Pred Grad: 0.669, New P: 1.913
-Original Grad: -0.154, -lr * Pred Grad: -0.522, New P: 1.620
iter 23 loss: 0.289
Actual params: [1.9129, 1.6203]
-Original Grad: -0.413, -lr * Pred Grad: -0.766, New P: 1.146
-Original Grad: -0.060, -lr * Pred Grad: -0.089, New P: 1.531
iter 24 loss: 0.107
Actual params: [1.1464, 1.5312]
-Original Grad: -0.049, -lr * Pred Grad: -0.408, New P: 0.739
-Original Grad: 0.444, -lr * Pred Grad: 1.252, New P: 2.783
iter 25 loss: 0.707
Actual params: [0.7389, 2.7831]
-Original Grad: 0.951, -lr * Pred Grad: 0.850, New P: 1.589
-Original Grad: -2.536, -lr * Pred Grad: -0.564, New P: 2.220
iter 26 loss: 0.065
Actual params: [1.5892, 2.2196]
-Original Grad: -0.697, -lr * Pred Grad: -0.784, New P: 0.805
-Original Grad: 0.403, -lr * Pred Grad: -0.220, New P: 2.000
iter 27 loss: 0.234
Actual params: [0.8051, 2.    ]
-Original Grad: 0.395, -lr * Pred Grad: 0.094, New P: 0.899
-Original Grad: -0.314, -lr * Pred Grad: -0.471, New P: 1.529
iter 28 loss: 0.164
Actual params: [0.8991, 1.5286]
-Original Grad: 0.410, -lr * Pred Grad: 0.392, New P: 1.291
-Original Grad: 0.250, -lr * Pred Grad: -0.076, New P: 1.453
iter 29 loss: 0.154
Actual params: [1.2908, 1.4529]
-Original Grad: -0.419, -lr * Pred Grad: -0.452, New P: 0.838
-Original Grad: 0.588, -lr * Pred Grad: 1.528, New P: 2.981
iter 30 loss: 0.849
Actual params: [0.8383, 2.9807]
-Original Grad: 0.016, -lr * Pred Grad: -0.255, New P: 0.583
-Original Grad: -1.266, -lr * Pred Grad: -0.608, New P: 2.373
Target params: [1.3344, 1.5708]
iter 0 loss: 0.228
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad: 0.665, New P: 0.193
-Original Grad: 0.001, -lr * Pred Grad: 0.638, New P: 0.642
iter 1 loss: 0.108
Actual params: [0.1927, 0.642 ]
-Original Grad: 0.285, -lr * Pred Grad: 2.078, New P: 2.271
-Original Grad: 0.136, -lr * Pred Grad: 0.914, New P: 1.556
iter 2 loss: 0.059
Actual params: [2.271 , 1.5563]
-Original Grad: -0.040, -lr * Pred Grad: -0.527, New P: 1.744
-Original Grad: -0.199, -lr * Pred Grad: -0.735, New P: 0.821
iter 3 loss: 0.041
Actual params: [1.744 , 0.8208]
-Original Grad: 0.030, -lr * Pred Grad: 0.454, New P: 2.198
-Original Grad: 0.147, -lr * Pred Grad: 0.196, New P: 1.017
iter 4 loss: 0.029
Actual params: [2.1982, 1.0165]
-Original Grad: -0.019, -lr * Pred Grad: 0.023, New P: 2.221
-Original Grad: -0.059, -lr * Pred Grad: -0.162, New P: 0.855
iter 5 loss: 0.027
Actual params: [2.2211, 0.8547]
-Original Grad: -0.012, -lr * Pred Grad: 0.055, New P: 2.276
-Original Grad: 0.004, -lr * Pred Grad: -0.044, New P: 0.811
iter 6 loss: 0.028
Actual params: [2.2763, 0.8108]
-Original Grad: -0.002, -lr * Pred Grad: 0.031, New P: 2.307
-Original Grad: 0.026, -lr * Pred Grad: 0.002, New P: 0.813
iter 7 loss: 0.028
Actual params: [2.3075, 0.8128]
-Original Grad: -0.008, -lr * Pred Grad: -0.003, New P: 2.304
-Original Grad: 0.015, -lr * Pred Grad: 0.023, New P: 0.836
iter 8 loss: 0.028
Actual params: [2.304, 0.836]
-Original Grad: -0.006, -lr * Pred Grad: -0.020, New P: 2.284
-Original Grad: 0.015, -lr * Pred Grad: 0.029, New P: 0.866
iter 9 loss: 0.028
Actual params: [2.2845, 0.8655]
-Original Grad: -0.012, -lr * Pred Grad: -0.050, New P: 2.234
-Original Grad: -0.023, -lr * Pred Grad: -0.062, New P: 0.804
iter 10 loss: 0.028
Actual params: [2.2341, 0.8035]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: 2.197
-Original Grad: 0.049, -lr * Pred Grad: 0.083, New P: 0.887
iter 11 loss: 0.027
Actual params: [2.1972, 0.8865]
-Original Grad: -0.009, -lr * Pred Grad: -0.064, New P: 2.133
-Original Grad: 0.004, -lr * Pred Grad: 0.018, New P: 0.904
iter 12 loss: 0.026
Actual params: [2.1335, 0.9045]
-Original Grad: -0.012, -lr * Pred Grad: -0.085, New P: 2.048
-Original Grad: -0.006, -lr * Pred Grad: -0.020, New P: 0.884
iter 13 loss: 0.026
Actual params: [2.0484, 0.8843]
-Original Grad: -0.017, -lr * Pred Grad: -0.115, New P: 1.934
-Original Grad: 0.002, -lr * Pred Grad: -0.022, New P: 0.863
iter 14 loss: 0.027
Actual params: [1.9339, 0.8627]
-Original Grad: 0.034, -lr * Pred Grad: 0.004, New P: 1.938
-Original Grad: 0.108, -lr * Pred Grad: 0.287, New P: 1.149
iter 15 loss: 0.027
Actual params: [1.9381, 1.1494]
-Original Grad: -0.031, -lr * Pred Grad: -0.114, New P: 1.824
-Original Grad: -0.083, -lr * Pred Grad: -0.167, New P: 0.982
iter 16 loss: 0.024
Actual params: [1.8244, 0.9824]
-Original Grad: 0.014, -lr * Pred Grad: -0.041, New P: 1.784
-Original Grad: 0.070, -lr * Pred Grad: 0.116, New P: 1.098
iter 17 loss: 0.022
Actual params: [1.7838, 1.0984]
-Original Grad: -0.015, -lr * Pred Grad: -0.093, New P: 1.691
-Original Grad: -0.007, -lr * Pred Grad: -0.011, New P: 1.088
iter 18 loss: 0.022
Actual params: [1.6907, 1.0878]
-Original Grad: 0.024, -lr * Pred Grad: -0.004, New P: 1.687
-Original Grad: 0.082, -lr * Pred Grad: 0.221, New P: 1.309
iter 19 loss: 0.023
Actual params: [1.6865, 1.3089]
-Original Grad: -0.047, -lr * Pred Grad: -0.162, New P: 1.525
-Original Grad: -0.063, -lr * Pred Grad: -0.124, New P: 1.185
iter 20 loss: 0.021
Actual params: [1.5247, 1.1849]
-Original Grad: 0.028, -lr * Pred Grad: -0.031, New P: 1.494
-Original Grad: 0.159, -lr * Pred Grad: 0.401, New P: 1.586
iter 21 loss: 0.026
Actual params: [1.494 , 1.5859]
-Original Grad: -0.034, -lr * Pred Grad: -0.144, New P: 1.350
-Original Grad: -0.111, -lr * Pred Grad: -0.223, New P: 1.363
iter 22 loss: 0.014
Actual params: [1.3496, 1.3626]
-Original Grad: -0.023, -lr * Pred Grad: -0.170, New P: 1.180
-Original Grad: -0.044, -lr * Pred Grad: -0.130, New P: 1.233
iter 23 loss: 0.023
Actual params: [1.1798, 1.2328]
-Original Grad: 0.005, -lr * Pred Grad: -0.114, New P: 1.065
-Original Grad: 0.223, -lr * Pred Grad: 0.523, New P: 1.755
iter 24 loss: 0.025
Actual params: [1.0655, 1.7553]
-Original Grad: -0.005, -lr * Pred Grad: -0.114, New P: 0.951
-Original Grad: -0.211, -lr * Pred Grad: -0.382, New P: 1.373
iter 25 loss: 0.018
Actual params: [0.9512, 1.3732]
-Original Grad: 0.043, -lr * Pred Grad: 0.026, New P: 0.977
-Original Grad: 0.133, -lr * Pred Grad: 0.137, New P: 1.511
iter 26 loss: 0.014
Actual params: [0.9771, 1.5106]
-Original Grad: 0.014, -lr * Pred Grad: 0.021, New P: 0.998
-Original Grad: -0.041, -lr * Pred Grad: -0.094, New P: 1.417
iter 27 loss: 0.014
Actual params: [0.9977, 1.4169]
-Original Grad: 0.019, -lr * Pred Grad: 0.054, New P: 1.051
-Original Grad: 0.082, -lr * Pred Grad: 0.172, New P: 1.589
iter 28 loss: 0.015
Actual params: [1.0514, 1.5894]
-Original Grad: 0.006, -lr * Pred Grad: 0.034, New P: 1.085
-Original Grad: -0.061, -lr * Pred Grad: -0.119, New P: 1.470
iter 29 loss: 0.012
Actual params: [1.0855, 1.4699]
-Original Grad: -0.001, -lr * Pred Grad: 0.008, New P: 1.093
-Original Grad: 0.001, -lr * Pred Grad: -0.044, New P: 1.425
iter 30 loss: 0.012
Actual params: [1.0932, 1.4255]
-Original Grad: 0.012, -lr * Pred Grad: 0.030, New P: 1.124
-Original Grad: 0.034, -lr * Pred Grad: 0.037, New P: 1.462
Target params: [1.3344, 1.5708]
iter 0 loss: 0.466
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad: 0.634, New P: 0.162
-Original Grad: 0.001, -lr * Pred Grad: 0.637, New P: 0.640
iter 1 loss: 0.394
Actual params: [0.162 , 0.6403]
-Original Grad: 0.594, -lr * Pred Grad: 2.421, New P: 2.583
-Original Grad: 0.692, -lr * Pred Grad: 2.604, New P: 3.244
iter 2 loss: 0.421
Actual params: [2.5833, 3.2442]
-Original Grad: -0.012, -lr * Pred Grad: -0.833, New P: 1.751
-Original Grad: -0.417, -lr * Pred Grad: -1.071, New P: 2.173
iter 3 loss: 0.123
Actual params: [1.7506, 2.1734]
-Original Grad: -0.089, -lr * Pred Grad: 0.003, New P: 1.754
-Original Grad: -0.439, -lr * Pred Grad: -0.253, New P: 1.921
iter 4 loss: 0.084
Actual params: [1.7536, 1.9208]
-Original Grad: -0.095, -lr * Pred Grad: -0.248, New P: 1.505
-Original Grad: -0.145, -lr * Pred Grad: -0.533, New P: 1.387
iter 5 loss: 0.025
Actual params: [1.5052, 1.3874]
-Original Grad: -0.006, -lr * Pred Grad: -0.116, New P: 1.389
-Original Grad: 0.018, -lr * Pred Grad: -0.284, New P: 1.103
iter 6 loss: 0.079
Actual params: [1.3893, 1.1031]
-Original Grad: 0.144, -lr * Pred Grad: 0.244, New P: 1.634
-Original Grad: 0.360, -lr * Pred Grad: 0.725, New P: 1.829
iter 7 loss: 0.064
Actual params: [1.6336, 1.8286]
-Original Grad: -0.070, -lr * Pred Grad: -0.085, New P: 1.548
-Original Grad: -0.152, -lr * Pred Grad: -0.369, New P: 1.460
iter 8 loss: 0.030
Actual params: [1.5483, 1.4596]
-Original Grad: -0.048, -lr * Pred Grad: -0.149, New P: 1.399
-Original Grad: -0.132, -lr * Pred Grad: -0.223, New P: 1.237
iter 9 loss: 0.051
Actual params: [1.3989, 1.237 ]
-Original Grad: 0.078, -lr * Pred Grad: 0.105, New P: 1.504
-Original Grad: 0.304, -lr * Pred Grad: 0.530, New P: 1.767
iter 10 loss: 0.049
Actual params: [1.504 , 1.7667]
-Original Grad: -0.068, -lr * Pred Grad: -0.151, New P: 1.353
-Original Grad: -0.194, -lr * Pred Grad: -0.365, New P: 1.401
iter 11 loss: 0.024
Actual params: [1.3534, 1.4015]
-Original Grad: -0.003, -lr * Pred Grad: -0.098, New P: 1.255
-Original Grad: 0.095, -lr * Pred Grad: 0.074, New P: 1.475
iter 12 loss: 0.022
Actual params: [1.2553, 1.4754]
-Original Grad: -0.016, -lr * Pred Grad: -0.124, New P: 1.131
-Original Grad: -0.048, -lr * Pred Grad: -0.127, New P: 1.349
iter 13 loss: 0.035
Actual params: [1.1313, 1.3488]
-Original Grad: -0.006, -lr * Pred Grad: -0.112, New P: 1.019
-Original Grad: 0.232, -lr * Pred Grad: 0.624, New P: 1.973
iter 14 loss: 0.040
Actual params: [1.019, 1.973]
-Original Grad: 0.000, -lr * Pred Grad: -0.093, New P: 0.926
-Original Grad: -0.184, -lr * Pred Grad: -0.371, New P: 1.602
iter 15 loss: 0.023
Actual params: [0.9265, 1.6018]
-Original Grad: -0.000, -lr * Pred Grad: -0.082, New P: 0.844
-Original Grad: -0.008, -lr * Pred Grad: -0.088, New P: 1.513
iter 16 loss: 0.027
Actual params: [0.8442, 1.5134]
-Original Grad: -0.012, -lr * Pred Grad: -0.108, New P: 0.736
-Original Grad: 0.054, -lr * Pred Grad: 0.013, New P: 1.526
iter 17 loss: 0.031
Actual params: [0.7358, 1.5261]
-Original Grad: 0.148, -lr * Pred Grad: 0.302, New P: 1.038
-Original Grad: 0.024, -lr * Pred Grad: 0.061, New P: 1.587
iter 18 loss: 0.022
Actual params: [1.0378, 1.5872]
-Original Grad: -0.020, -lr * Pred Grad: 0.041, New P: 1.079
-Original Grad: 0.008, -lr * Pred Grad: 0.021, New P: 1.609
iter 19 loss: 0.022
Actual params: [1.0785, 1.6086]
-Original Grad: -0.029, -lr * Pred Grad: -0.032, New P: 1.047
-Original Grad: 0.010, -lr * Pred Grad: 0.017, New P: 1.626
iter 20 loss: 0.022
Actual params: [1.0466, 1.6259]
-Original Grad: 0.007, -lr * Pred Grad: 0.003, New P: 1.050
-Original Grad: -0.040, -lr * Pred Grad: -0.103, New P: 1.523
iter 21 loss: 0.023
Actual params: [1.0497, 1.523 ]
-Original Grad: 0.014, -lr * Pred Grad: 0.029, New P: 1.078
-Original Grad: 0.028, -lr * Pred Grad: 0.006, New P: 1.529
iter 22 loss: 0.022
Actual params: [1.0782, 1.5286]
-Original Grad: -0.004, -lr * Pred Grad: -0.009, New P: 1.069
-Original Grad: 0.008, -lr * Pred Grad: -0.001, New P: 1.528
iter 23 loss: 0.022
Actual params: [1.0689, 1.5277]
-Original Grad: -0.021, -lr * Pred Grad: -0.074, New P: 0.995
-Original Grad: -0.023, -lr * Pred Grad: -0.070, New P: 1.457
iter 24 loss: 0.028
Actual params: [0.9954, 1.4573]
-Original Grad: 0.015, -lr * Pred Grad: -0.010, New P: 0.986
-Original Grad: 0.124, -lr * Pred Grad: 0.300, New P: 1.757
iter 25 loss: 0.024
Actual params: [0.9855, 1.7575]
-Original Grad: -0.016, -lr * Pred Grad: -0.073, New P: 0.912
-Original Grad: -0.117, -lr * Pred Grad: -0.232, New P: 1.525
iter 26 loss: 0.026
Actual params: [0.9124, 1.5252]
-Original Grad: 0.006, -lr * Pred Grad: -0.038, New P: 0.874
-Original Grad: 0.063, -lr * Pred Grad: 0.058, New P: 1.583
iter 27 loss: 0.025
Actual params: [0.8741, 1.5828]
-Original Grad: -0.006, -lr * Pred Grad: -0.059, New P: 0.815
-Original Grad: 0.007, -lr * Pred Grad: 0.002, New P: 1.585
iter 28 loss: 0.026
Actual params: [0.8147, 1.5847]
-Original Grad: -0.005, -lr * Pred Grad: -0.067, New P: 0.747
-Original Grad: -0.011, -lr * Pred Grad: -0.034, New P: 1.551
iter 29 loss: 0.030
Actual params: [0.7475, 1.5506]
-Original Grad: 0.057, -lr * Pred Grad: 0.100, New P: 0.848
-Original Grad: -0.002, -lr * Pred Grad: -0.037, New P: 1.514
iter 30 loss: 0.027
Actual params: [0.8477, 1.514 ]
-Original Grad: -0.007, -lr * Pred Grad: 0.006, New P: 0.854
-Original Grad: 0.053, -lr * Pred Grad: 0.103, New P: 1.617
Target params: [1.3344, 1.5708]
iter 0 loss: 0.228
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad: 0.665, New P: 0.192
-Original Grad: 0.001, -lr * Pred Grad: 0.638, New P: 0.642
iter 1 loss: 0.108
Actual params: [0.1924, 0.6419]
-Original Grad: 0.340, -lr * Pred Grad: 2.184, New P: 2.377
-Original Grad: 0.155, -lr * Pred Grad: 1.028, New P: 1.670
iter 2 loss: 0.073
Actual params: [2.3765, 1.6704]
-Original Grad: -0.028, -lr * Pred Grad: -0.562, New P: 1.815
-Original Grad: -0.238, -lr * Pred Grad: -0.805, New P: 0.866
iter 3 loss: 0.031
Actual params: [1.815 , 0.8656]
-Original Grad: 0.091, -lr * Pred Grad: 0.714, New P: 2.529
-Original Grad: 0.210, -lr * Pred Grad: 0.282, New P: 1.148
iter 4 loss: 0.041
Actual params: [2.5289, 1.148 ]
-Original Grad: -0.037, -lr * Pred Grad: -0.055, New P: 2.474
-Original Grad: -0.120, -lr * Pred Grad: -0.247, New P: 0.901
iter 5 loss: 0.031
Actual params: [2.4742, 0.9011]
-Original Grad: -0.019, -lr * Pred Grad: 0.061, New P: 2.535
-Original Grad: -0.029, -lr * Pred Grad: -0.133, New P: 0.768
iter 6 loss: 0.031
Actual params: [2.5351, 0.7679]
-Original Grad: -0.008, -lr * Pred Grad: 0.018, New P: 2.553
-Original Grad: 0.012, -lr * Pred Grad: -0.090, New P: 0.678
iter 7 loss: 0.032
Actual params: [2.5531, 0.6777]
-Original Grad: -0.001, -lr * Pred Grad: 0.015, New P: 2.568
-Original Grad: 0.044, -lr * Pred Grad: 0.053, New P: 0.730
iter 8 loss: 0.032
Actual params: [2.5682, 0.7305]
-Original Grad: -0.017, -lr * Pred Grad: -0.038, New P: 2.530
-Original Grad: -0.004, -lr * Pred Grad: -0.013, New P: 0.718
iter 9 loss: 0.031
Actual params: [2.5302, 0.7176]
-Original Grad: -0.013, -lr * Pred Grad: -0.059, New P: 2.471
-Original Grad: 0.032, -lr * Pred Grad: 0.062, New P: 0.780
iter 10 loss: 0.030
Actual params: [2.4709, 0.7797]
-Original Grad: -0.014, -lr * Pred Grad: -0.082, New P: 2.389
-Original Grad: -0.003, -lr * Pred Grad: -0.006, New P: 0.773
iter 11 loss: 0.029
Actual params: [2.3892, 0.7733]
-Original Grad: -0.005, -lr * Pred Grad: -0.078, New P: 2.312
-Original Grad: 0.024, -lr * Pred Grad: 0.044, New P: 0.817
iter 12 loss: 0.028
Actual params: [2.3115, 0.8173]
-Original Grad: -0.003, -lr * Pred Grad: -0.075, New P: 2.237
-Original Grad: 0.030, -lr * Pred Grad: 0.076, New P: 0.893
iter 13 loss: 0.027
Actual params: [2.2366, 0.8933]
-Original Grad: -0.010, -lr * Pred Grad: -0.094, New P: 2.143
-Original Grad: -0.011, -lr * Pred Grad: -0.020, New P: 0.873
iter 14 loss: 0.027
Actual params: [2.1431, 0.873 ]
-Original Grad: -0.010, -lr * Pred Grad: -0.104, New P: 2.039
-Original Grad: -0.003, -lr * Pred Grad: -0.028, New P: 0.845
iter 15 loss: 0.026
Actual params: [2.0392, 0.8452]
-Original Grad: -0.005, -lr * Pred Grad: -0.098, New P: 1.941
-Original Grad: 0.027, -lr * Pred Grad: 0.037, New P: 0.882
iter 16 loss: 0.026
Actual params: [1.941 , 0.8824]
-Original Grad: 0.018, -lr * Pred Grad: -0.033, New P: 1.908
-Original Grad: 0.079, -lr * Pred Grad: 0.227, New P: 1.110
iter 17 loss: 0.025
Actual params: [1.9079, 1.1098]
-Original Grad: -0.022, -lr * Pred Grad: -0.109, New P: 1.799
-Original Grad: -0.055, -lr * Pred Grad: -0.107, New P: 1.002
iter 18 loss: 0.024
Actual params: [1.7989, 1.0025]
-Original Grad: 0.023, -lr * Pred Grad: -0.018, New P: 1.781
-Original Grad: 0.070, -lr * Pred Grad: 0.142, New P: 1.145
iter 19 loss: 0.023
Actual params: [1.7814, 1.1446]
-Original Grad: -0.022, -lr * Pred Grad: -0.100, New P: 1.681
-Original Grad: -0.030, -lr * Pred Grad: -0.059, New P: 1.086
iter 20 loss: 0.023
Actual params: [1.6813, 1.086 ]
-Original Grad: 0.037, -lr * Pred Grad: 0.030, New P: 1.711
-Original Grad: 0.132, -lr * Pred Grad: 0.358, New P: 1.444
iter 21 loss: 0.028
Actual params: [1.7113, 1.444 ]
-Original Grad: -0.036, -lr * Pred Grad: -0.116, New P: 1.595
-Original Grad: -0.095, -lr * Pred Grad: -0.192, New P: 1.252
iter 22 loss: 0.019
Actual params: [1.5954, 1.2516]
-Original Grad: -0.039, -lr * Pred Grad: -0.194, New P: 1.402
-Original Grad: 0.019, -lr * Pred Grad: -0.007, New P: 1.244
iter 23 loss: 0.019
Actual params: [1.4015, 1.2443]
-Original Grad: 0.018, -lr * Pred Grad: -0.086, New P: 1.316
-Original Grad: 0.180, -lr * Pred Grad: 0.526, New P: 1.771
iter 24 loss: 0.032
Actual params: [1.3157, 1.7707]
-Original Grad: -0.043, -lr * Pred Grad: -0.206, New P: 1.110
-Original Grad: -0.177, -lr * Pred Grad: -0.352, New P: 1.419
iter 25 loss: 0.012
Actual params: [1.1096, 1.4185]
-Original Grad: 0.006, -lr * Pred Grad: -0.133, New P: 0.977
-Original Grad: 0.032, -lr * Pred Grad: -0.030, New P: 1.388
iter 26 loss: 0.016
Actual params: [0.9769, 1.3882]
-Original Grad: 0.020, -lr * Pred Grad: -0.059, New P: 0.918
-Original Grad: 0.113, -lr * Pred Grad: 0.239, New P: 1.627
iter 27 loss: 0.019
Actual params: [0.9176, 1.6275]
-Original Grad: 0.029, -lr * Pred Grad: 0.013, New P: 0.930
-Original Grad: -0.111, -lr * Pred Grad: -0.207, New P: 1.421
iter 28 loss: 0.017
Actual params: [0.9304, 1.4209]
-Original Grad: 0.034, -lr * Pred Grad: 0.080, New P: 1.010
-Original Grad: 0.041, -lr * Pred Grad: 0.006, New P: 1.427
iter 29 loss: 0.014
Actual params: [1.0103, 1.4266]
-Original Grad: 0.028, -lr * Pred Grad: 0.111, New P: 1.121
-Original Grad: 0.076, -lr * Pred Grad: 0.181, New P: 1.607
iter 30 loss: 0.015
Actual params: [1.1208, 1.6071]
-Original Grad: -0.008, -lr * Pred Grad: 0.028, New P: 1.149
-Original Grad: -0.077, -lr * Pred Grad: -0.150, New P: 1.457
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.054, -lr * Pred Grad: 1.152, New P: 0.680
-Original Grad: 0.007, -lr * Pred Grad: 0.665, New P: 0.668
iter 1 loss: 0.193
Actual params: [0.6798, 0.6683]
-Original Grad: -0.043, -lr * Pred Grad: -0.176, New P: 0.503
-Original Grad: -0.352, -lr * Pred Grad: -0.682, New P: -0.014
iter 2 loss: 0.049
Actual params: [ 0.5034, -0.0139]
-Original Grad: -0.033, -lr * Pred Grad: -0.070, New P: 0.433
-Original Grad: 0.001, -lr * Pred Grad: -0.159, New P: -0.173
iter 3 loss: 0.042
Actual params: [ 0.4335, -0.1734]
-Original Grad: -0.019, -lr * Pred Grad: -0.091, New P: 0.342
-Original Grad: -0.004, -lr * Pred Grad: -0.218, New P: -0.392
iter 4 loss: 0.052
Actual params: [ 0.3425, -0.3917]
-Original Grad: 0.062, -lr * Pred Grad: 0.217, New P: 0.559
-Original Grad: 0.131, -lr * Pred Grad: 0.204, New P: -0.188
iter 5 loss: 0.047
Actual params: [ 0.5595, -0.1877]
-Original Grad: -0.080, -lr * Pred Grad: -0.231, New P: 0.329
-Original Grad: -0.041, -lr * Pred Grad: -0.094, New P: -0.281
iter 6 loss: 0.046
Actual params: [ 0.3285, -0.2814]
-Original Grad: 0.030, -lr * Pred Grad: -0.005, New P: 0.323
-Original Grad: 0.073, -lr * Pred Grad: 0.141, New P: -0.140
iter 7 loss: 0.040
Actual params: [ 0.3231, -0.1399]
-Original Grad: -0.013, -lr * Pred Grad: -0.077, New P: 0.246
-Original Grad: 0.027, -lr * Pred Grad: 0.091, New P: -0.049
iter 8 loss: 0.039
Actual params: [ 0.2461, -0.0485]
-Original Grad: 0.039, -lr * Pred Grad: 0.062, New P: 0.308
-Original Grad: 0.065, -lr * Pred Grad: 0.216, New P: 0.167
iter 9 loss: 0.033
Actual params: [0.3083, 0.1673]
-Original Grad: 0.032, -lr * Pred Grad: 0.103, New P: 0.411
-Original Grad: 0.022, -lr * Pred Grad: 0.108, New P: 0.275
iter 10 loss: 0.058
Actual params: [0.411 , 0.2752]
-Original Grad: -0.186, -lr * Pred Grad: -0.487, New P: -0.076
-Original Grad: -0.245, -lr * Pred Grad: -0.353, New P: -0.077
iter 11 loss: 0.074
Actual params: [-0.0758, -0.0774]
-Original Grad: 0.089, -lr * Pred Grad: -0.050, New P: -0.126
-Original Grad: 0.053, -lr * Pred Grad: -0.138, New P: -0.215
iter 12 loss: 0.087
Actual params: [-0.1257, -0.2151]
-Original Grad: 0.057, -lr * Pred Grad: 0.047, New P: -0.079
-Original Grad: 0.052, -lr * Pred Grad: 0.024, New P: -0.191
iter 13 loss: 0.080
Actual params: [-0.0786, -0.1914]
-Original Grad: 0.092, -lr * Pred Grad: 0.241, New P: 0.162
-Original Grad: 0.102, -lr * Pred Grad: 0.291, New P: 0.099
iter 14 loss: 0.039
Actual params: [0.1622, 0.0994]
-Original Grad: 0.055, -lr * Pred Grad: 0.262, New P: 0.424
-Original Grad: 0.020, -lr * Pred Grad: 0.121, New P: 0.220
iter 15 loss: 0.054
Actual params: [0.4239, 0.22  ]
-Original Grad: -0.185, -lr * Pred Grad: -0.392, New P: 0.032
-Original Grad: -0.185, -lr * Pred Grad: -0.309, New P: -0.089
iter 16 loss: 0.060
Actual params: [ 0.0318, -0.089 ]
-Original Grad: 0.083, -lr * Pred Grad: 0.029, New P: 0.061
-Original Grad: 0.058, -lr * Pred Grad: -0.066, New P: -0.155
iter 17 loss: 0.060
Actual params: [ 0.0607, -0.1547]
-Original Grad: 0.084, -lr * Pred Grad: 0.165, New P: 0.226
-Original Grad: 0.067, -lr * Pred Grad: 0.111, New P: -0.044
iter 18 loss: 0.039
Actual params: [ 0.2257, -0.0437]
-Original Grad: 0.066, -lr * Pred Grad: 0.242, New P: 0.468
-Original Grad: 0.079, -lr * Pred Grad: 0.267, New P: 0.224
iter 19 loss: 0.065
Actual params: [0.4679, 0.2238]
-Original Grad: -0.178, -lr * Pred Grad: -0.361, New P: 0.107
-Original Grad: -0.151, -lr * Pred Grad: -0.284, New P: -0.060
iter 20 loss: 0.050
Actual params: [ 0.107 , -0.0599]
-Original Grad: 0.085, -lr * Pred Grad: 0.035, New P: 0.142
-Original Grad: 0.067, -lr * Pred Grad: 0.019, New P: -0.041
iter 21 loss: 0.045
Actual params: [ 0.1417, -0.0412]
-Original Grad: 0.070, -lr * Pred Grad: 0.134, New P: 0.276
-Original Grad: 0.067, -lr * Pred Grad: 0.153, New P: 0.112
iter 22 loss: 0.033
Actual params: [0.2758, 0.1119]
-Original Grad: 0.038, -lr * Pred Grad: 0.159, New P: 0.435
-Original Grad: 0.023, -lr * Pred Grad: 0.102, New P: 0.214
iter 23 loss: 0.056
Actual params: [0.4352, 0.2144]
-Original Grad: -0.119, -lr * Pred Grad: -0.242, New P: 0.193
-Original Grad: -0.107, -lr * Pred Grad: -0.210, New P: 0.005
iter 24 loss: 0.039
Actual params: [0.1928, 0.0046]
-Original Grad: 0.076, -lr * Pred Grad: 0.078, New P: 0.271
-Original Grad: 0.060, -lr * Pred Grad: 0.023, New P: 0.027
iter 25 loss: 0.036
Actual params: [0.2707, 0.0271]
-Original Grad: 0.028, -lr * Pred Grad: 0.068, New P: 0.339
-Original Grad: 0.038, -lr * Pred Grad: 0.082, New P: 0.109
iter 26 loss: 0.035
Actual params: [0.3388, 0.1089]
-Original Grad: -0.111, -lr * Pred Grad: -0.269, New P: 0.070
-Original Grad: -0.009, -lr * Pred Grad: -0.012, New P: 0.097
iter 27 loss: 0.050
Actual params: [0.07  , 0.0973]
-Original Grad: 0.086, -lr * Pred Grad: 0.071, New P: 0.141
-Original Grad: 0.057, -lr * Pred Grad: 0.137, New P: 0.234
iter 28 loss: 0.040
Actual params: [0.1406, 0.2341]
-Original Grad: 0.067, -lr * Pred Grad: 0.158, New P: 0.298
-Original Grad: 0.004, -lr * Pred Grad: 0.031, New P: 0.265
iter 29 loss: 0.036
Actual params: [0.2981, 0.2654]
-Original Grad: -0.064, -lr * Pred Grad: -0.102, New P: 0.196
-Original Grad: -0.055, -lr * Pred Grad: -0.124, New P: 0.142
iter 30 loss: 0.036
Actual params: [0.1961, 0.1418]
-Original Grad: 0.031, -lr * Pred Grad: 0.039, New P: 0.235
-Original Grad: 0.030, -lr * Pred Grad: -0.000, New P: 0.142
Target params: [1.3344, 1.5708]
iter 0 loss: 0.008
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.015, -lr * Pred Grad: 0.800, New P: 0.328
-Original Grad: -0.011, -lr * Pred Grad: 0.582, New P: 0.585
iter 1 loss: 0.029
Actual params: [0.328 , 0.5854]
-Original Grad: -0.464, -lr * Pred Grad: -1.926, New P: -1.598
-Original Grad: 0.329, -lr * Pred Grad: 1.828, New P: 2.413
iter 2 loss: 0.010
Actual params: [-1.5977,  2.413 ]
-Original Grad: -0.000, -lr * Pred Grad: -1.124, New P: -2.722
-Original Grad: -0.000, -lr * Pred Grad: -0.188, New P: 2.225
iter 3 loss: 0.010
Actual params: [-2.7219,  2.225 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.989, New P: -3.711
-Original Grad: -0.000, -lr * Pred Grad: -0.062, New P: 2.163
iter 4 loss: 0.010
Actual params: [-3.7111,  2.1626]
-Original Grad: -0.000, -lr * Pred Grad: -0.827, New P: -4.538
-Original Grad: -0.000, -lr * Pred Grad: -0.023, New P: 2.139
iter 5 loss: 0.010
Actual params: [-4.5378,  2.1394]
-Original Grad: -0.000, -lr * Pred Grad: -0.762, New P: -5.299
-Original Grad: -0.000, -lr * Pred Grad: -0.037, New P: 2.103
iter 6 loss: 0.010
Actual params: [-5.2995,  2.1028]
-Original Grad: -0.000, -lr * Pred Grad: -0.694, New P: -5.993
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.070
iter 7 loss: 0.010
Actual params: [-5.9932,  2.0704]
-Original Grad: -0.000, -lr * Pred Grad: -0.625, New P: -6.618
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 2.042
iter 8 loss: 0.010
Actual params: [-6.6179,  2.0419]
-Original Grad: -0.000, -lr * Pred Grad: -0.554, New P: -7.172
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 2.017
iter 9 loss: 0.010
Actual params: [-7.172 ,  2.0166]
-Original Grad: -0.000, -lr * Pred Grad: -0.484, New P: -7.656
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 1.992
iter 10 loss: 0.010
Actual params: [-7.6556,  1.9918]
-Original Grad: -0.000, -lr * Pred Grad: -0.416, New P: -8.071
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 1.966
iter 11 loss: 0.010
Actual params: [-8.0712,  1.9664]
-Original Grad: -0.000, -lr * Pred Grad: -0.352, New P: -8.423
-Original Grad: -0.000, -lr * Pred Grad: -0.026, New P: 1.940
iter 12 loss: 0.010
Actual params: [-8.4232,  1.94  ]
-Original Grad: -0.000, -lr * Pred Grad: -0.294, New P: -8.718
-Original Grad: -0.000, -lr * Pred Grad: -0.027, New P: 1.913
iter 13 loss: 0.010
Actual params: [-8.7177,  1.9125]
-Original Grad: -0.000, -lr * Pred Grad: -0.244, New P: -8.962
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 1.884
iter 14 loss: 0.010
Actual params: [-8.9619,  1.8841]
-Original Grad: -0.000, -lr * Pred Grad: -0.202, New P: -9.164
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 1.855
iter 15 loss: 0.010
Actual params: [-9.1635,  1.8548]
-Original Grad: -0.000, -lr * Pred Grad: -0.167, New P: -9.330
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 1.825
iter 16 loss: 0.010
Actual params: [-9.3302,  1.8247]
-Original Grad: -0.000, -lr * Pred Grad: -0.139, New P: -9.469
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 1.794
iter 17 loss: 0.010
Actual params: [-9.4688,  1.7941]
-Original Grad: -0.000, -lr * Pred Grad: -0.116, New P: -9.585
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 1.763
iter 18 loss: 0.010
Actual params: [-9.5852,  1.763 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.099, New P: -9.684
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 1.732
iter 19 loss: 0.010
Actual params: [-9.6844,  1.7315]
-Original Grad: -0.000, -lr * Pred Grad: -0.086, New P: -9.770
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.700
iter 20 loss: 0.010
Actual params: [-9.7703,  1.6998]
-Original Grad: -0.000, -lr * Pred Grad: -0.076, New P: -9.846
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.668
iter 21 loss: 0.010
Actual params: [-9.8459,  1.6678]
-Original Grad: -0.000, -lr * Pred Grad: -0.068, New P: -9.914
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.636
iter 22 loss: 0.010
Actual params: [-9.9138,  1.6357]
-Original Grad: 0.000, -lr * Pred Grad: -0.062, New P: -9.976
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.603
iter 23 loss: 0.010
Actual params: [-9.9757,  1.6035]
-Original Grad: 0.000, -lr * Pred Grad: -0.057, New P: -10.033
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.571
iter 24 loss: 0.010
Actual params: [-10.033 ,   1.5712]
-Original Grad: 0.000, -lr * Pred Grad: -0.054, New P: -10.087
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.539
iter 25 loss: 0.010
Actual params: [-10.087 ,   1.5388]
-Original Grad: 0.000, -lr * Pred Grad: -0.051, New P: -10.138
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.506
iter 26 loss: 0.010
Actual params: [-10.1382,   1.5063]
-Original Grad: 0.000, -lr * Pred Grad: -0.049, New P: -10.188
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.474
iter 27 loss: 0.010
Actual params: [-10.1875,   1.4739]
-Original Grad: 0.000, -lr * Pred Grad: -0.048, New P: -10.235
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.441
iter 28 loss: 0.010
Actual params: [-10.2354,   1.4414]
-Original Grad: 0.000, -lr * Pred Grad: -0.047, New P: -10.282
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 1.409
iter 29 loss: 0.010
Actual params: [-10.282 ,   1.4089]
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: -10.328
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 1.376
iter 30 loss: 0.010
Actual params: [-10.3279,   1.3763]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -10.373
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.344
Target params: [1.3344, 1.5708]
iter 0 loss: 0.537
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad: 0.609, New P: 0.137
-Original Grad: 0.077, -lr * Pred Grad: 0.993, New P: 0.997
iter 1 loss: 0.439
Actual params: [0.1369, 0.9968]
-Original Grad: 0.249, -lr * Pred Grad: 1.976, New P: 2.113
-Original Grad: 0.102, -lr * Pred Grad: 0.702, New P: 1.699
iter 2 loss: 0.242
Actual params: [2.1134, 1.6991]
-Original Grad: -0.191, -lr * Pred Grad: -1.119, New P: 0.995
-Original Grad: 0.274, -lr * Pred Grad: 1.401, New P: 3.100
iter 3 loss: 0.550
Actual params: [0.9948, 3.1004]
-Original Grad: -0.762, -lr * Pred Grad: -1.897, New P: -0.902
-Original Grad: -2.248, -lr * Pred Grad: -0.697, New P: 2.403
iter 4 loss: 0.536
Actual params: [-0.9018,  2.4033]
-Original Grad: 0.003, -lr * Pred Grad: -1.385, New P: -2.287
-Original Grad: -0.001, -lr * Pred Grad: -0.373, New P: 2.031
iter 5 loss: 0.536
Actual params: [-2.2866,  2.0308]
-Original Grad: 0.000, -lr * Pred Grad: -1.295, New P: -3.581
-Original Grad: -0.000, -lr * Pred Grad: -0.480, New P: 1.551
iter 6 loss: 0.536
Actual params: [-3.5812,  1.5509]
-Original Grad: 0.000, -lr * Pred Grad: -1.141, New P: -4.723
-Original Grad: -0.000, -lr * Pred Grad: -0.316, New P: 1.235
iter 7 loss: 0.536
Actual params: [-4.7227,  1.2347]
-Original Grad: 0.000, -lr * Pred Grad: -1.079, New P: -5.802
-Original Grad: -0.000, -lr * Pred Grad: -0.238, New P: 0.996
iter 8 loss: 0.536
Actual params: [-5.8015,  0.9964]
-Original Grad: 0.000, -lr * Pred Grad: -1.036, New P: -6.837
-Original Grad: -0.000, -lr * Pred Grad: -0.175, New P: 0.821
iter 9 loss: 0.536
Actual params: [-6.8371,  0.8214]
-Original Grad: 0.000, -lr * Pred Grad: -1.000, New P: -7.837
-Original Grad: -0.000, -lr * Pred Grad: -0.133, New P: 0.688
iter 10 loss: 0.536
Actual params: [-7.8374,  0.6883]
-Original Grad: 0.000, -lr * Pred Grad: -0.958, New P: -8.795
-Original Grad: -0.000, -lr * Pred Grad: -0.100, New P: 0.588
iter 11 loss: 0.536
Actual params: [-8.7954,  0.5881]
-Original Grad: 0.000, -lr * Pred Grad: -0.906, New P: -9.702
-Original Grad: -0.000, -lr * Pred Grad: -0.077, New P: 0.511
iter 12 loss: 0.536
Actual params: [-9.7017,  0.5112]
-Original Grad: 0.000, -lr * Pred Grad: -0.846, New P: -10.547
-Original Grad: -0.000, -lr * Pred Grad: -0.061, New P: 0.450
iter 13 loss: 0.536
Actual params: [-10.5473,   0.45  ]
-Original Grad: 0.000, -lr * Pred Grad: -0.778, New P: -11.325
-Original Grad: -0.000, -lr * Pred Grad: -0.051, New P: 0.399
iter 14 loss: 0.536
Actual params: [-11.3253,   0.3987]
-Original Grad: 0.000, -lr * Pred Grad: -0.705, New P: -12.031
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: 0.353
iter 15 loss: 0.536
Actual params: [-12.0306,   0.3534]
-Original Grad: 0.000, -lr * Pred Grad: -0.630, New P: -12.660
-Original Grad: 0.000, -lr * Pred Grad: -0.042, New P: 0.312
iter 16 loss: 0.536
Actual params: [-12.6603,   0.3117]
-Original Grad: 0.000, -lr * Pred Grad: -0.553, New P: -13.214
-Original Grad: 0.000, -lr * Pred Grad: -0.039, New P: 0.272
iter 17 loss: 0.536
Actual params: [-13.2137,   0.2723]
-Original Grad: -0.000, -lr * Pred Grad: -0.479, New P: -13.692
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: 0.234
iter 18 loss: 0.536
Actual params: [-13.6924,   0.2344]
-Original Grad: -0.000, -lr * Pred Grad: -0.408, New P: -14.100
-Original Grad: -0.000, -lr * Pred Grad: -0.037, New P: 0.198
iter 19 loss: 0.536
Actual params: [-14.1002,   0.1976]
-Original Grad: -0.000, -lr * Pred Grad: -0.343, New P: -14.443
-Original Grad: -0.000, -lr * Pred Grad: -0.036, New P: 0.162
iter 20 loss: 0.536
Actual params: [-14.4427,   0.1616]
-Original Grad: -0.000, -lr * Pred Grad: -0.285, New P: -14.727
-Original Grad: -0.000, -lr * Pred Grad: -0.035, New P: 0.126
iter 21 loss: 0.536
Actual params: [-14.7273,   0.1263]
-Original Grad: -0.000, -lr * Pred Grad: -0.235, New P: -14.962
-Original Grad: -0.000, -lr * Pred Grad: -0.035, New P: 0.092
iter 22 loss: 0.536
Actual params: [-14.9619,   0.0916]
-Original Grad: -0.000, -lr * Pred Grad: -0.193, New P: -15.155
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: 0.057
iter 23 loss: 0.536
Actual params: [-15.1548,   0.0573]
-Original Grad: -0.000, -lr * Pred Grad: -0.159, New P: -15.314
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: 0.023
iter 24 loss: 0.536
Actual params: [-15.3139,   0.0234]
-Original Grad: -0.000, -lr * Pred Grad: -0.132, New P: -15.446
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: -0.010
iter 25 loss: 0.536
Actual params: [-1.5446e+01, -1.0201e-02]
-Original Grad: -0.000, -lr * Pred Grad: -0.111, New P: -15.557
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.044
iter 26 loss: 0.536
Actual params: [-15.5573,  -0.0436]
-Original Grad: -0.000, -lr * Pred Grad: -0.095, New P: -15.652
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.077
iter 27 loss: 0.536
Actual params: [-15.6523,  -0.0767]
-Original Grad: -0.000, -lr * Pred Grad: -0.083, New P: -15.735
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.110
iter 28 loss: 0.536
Actual params: [-15.7348,  -0.1097]
-Original Grad: -0.000, -lr * Pred Grad: -0.073, New P: -15.808
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.143
iter 29 loss: 0.536
Actual params: [-15.8079,  -0.1426]
-Original Grad: -0.000, -lr * Pred Grad: -0.066, New P: -15.874
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.175
iter 30 loss: 0.536
Actual params: [-15.8737,  -0.1755]
-Original Grad: -0.000, -lr * Pred Grad: -0.060, New P: -15.934
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.208
Target params: [1.3344, 1.5708]
iter 0 loss: 0.577
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.006, -lr * Pred Grad: 0.710, New P: 0.238
-Original Grad: 0.014, -lr * Pred Grad: 0.698, New P: 0.701
iter 1 loss: 0.445
Actual params: [0.2378, 0.7015]
-Original Grad: 0.286, -lr * Pred Grad: 2.091, New P: 2.329
-Original Grad: 0.202, -lr * Pred Grad: 1.290, New P: 1.991
iter 2 loss: 0.424
Actual params: [2.3291, 1.9911]
-Original Grad: -0.045, -lr * Pred Grad: -0.561, New P: 1.768
-Original Grad: -0.541, -lr * Pred Grad: -0.858, New P: 1.133
iter 3 loss: 0.136
Actual params: [1.768 , 1.1333]
-Original Grad: -0.087, -lr * Pred Grad: -0.147, New P: 1.621
-Original Grad: -0.254, -lr * Pred Grad: -0.312, New P: 0.821
iter 4 loss: 0.097
Actual params: [1.6213, 0.8209]
-Original Grad: -0.058, -lr * Pred Grad: -0.220, New P: 1.401
-Original Grad: 0.004, -lr * Pred Grad: -0.432, New P: 0.389
iter 5 loss: 0.217
Actual params: [1.4008, 0.3891]
-Original Grad: 0.379, -lr * Pred Grad: 0.740, New P: 2.141
-Original Grad: 0.729, -lr * Pred Grad: 1.464, New P: 1.853
iter 6 loss: 0.375
Actual params: [2.1413, 1.8532]
-Original Grad: -0.061, -lr * Pred Grad: -0.094, New P: 2.047
-Original Grad: -0.540, -lr * Pred Grad: -0.705, New P: 1.148
iter 7 loss: 0.163
Actual params: [2.0468, 1.1483]
-Original Grad: -0.100, -lr * Pred Grad: -0.162, New P: 1.885
-Original Grad: -0.516, -lr * Pred Grad: -0.358, New P: 0.790
iter 8 loss: 0.111
Actual params: [1.8851, 0.7904]
-Original Grad: -0.082, -lr * Pred Grad: -0.275, New P: 1.610
-Original Grad: -0.000, -lr * Pred Grad: -0.496, New P: 0.294
iter 9 loss: 0.138
Actual params: [1.6104, 0.2939]
-Original Grad: 0.644, -lr * Pred Grad: 0.831, New P: 2.442
-Original Grad: 0.662, -lr * Pred Grad: 1.077, New P: 1.371
iter 10 loss: 0.241
Actual params: [2.4416, 1.371 ]
-Original Grad: -0.194, -lr * Pred Grad: -0.358, New P: 2.084
-Original Grad: -1.089, -lr * Pred Grad: -0.510, New P: 0.861
iter 11 loss: 0.131
Actual params: [2.0837, 0.8609]
-Original Grad: -0.081, -lr * Pred Grad: -0.241, New P: 1.843
-Original Grad: -0.084, -lr * Pred Grad: -0.403, New P: 0.458
iter 12 loss: 0.105
Actual params: [1.8426, 0.458 ]
-Original Grad: -0.140, -lr * Pred Grad: -0.446, New P: 1.397
-Original Grad: 0.091, -lr * Pred Grad: -0.341, New P: 0.117
iter 13 loss: 0.292
Actual params: [1.3965, 0.1168]
-Original Grad: 0.838, -lr * Pred Grad: 0.784, New P: 2.181
-Original Grad: 0.456, -lr * Pred Grad: 0.850, New P: 0.967
iter 14 loss: 0.148
Actual params: [2.181 , 0.9671]
-Original Grad: -0.092, -lr * Pred Grad: -0.216, New P: 1.965
-Original Grad: -0.223, -lr * Pred Grad: -0.473, New P: 0.494
iter 15 loss: 0.115
Actual params: [1.965, 0.494]
-Original Grad: -0.093, -lr * Pred Grad: -0.158, New P: 1.807
-Original Grad: -0.030, -lr * Pred Grad: -0.136, New P: 0.358
iter 16 loss: 0.104
Actual params: [1.8072, 0.3585]
-Original Grad: -0.110, -lr * Pred Grad: -0.307, New P: 1.501
-Original Grad: 0.097, -lr * Pred Grad: 0.026, New P: 0.384
iter 17 loss: 0.170
Actual params: [1.5007, 0.3842]
-Original Grad: 0.799, -lr * Pred Grad: 0.828, New P: 2.329
-Original Grad: 1.280, -lr * Pred Grad: 2.446, New P: 2.831
iter 18 loss: 0.638
Actual params: [2.3285, 2.8307]
-Original Grad: 0.021, -lr * Pred Grad: -0.021, New P: 2.308
-Original Grad: -0.484, -lr * Pred Grad: -1.015, New P: 1.816
iter 19 loss: 0.374
Actual params: [2.3079, 1.8162]
-Original Grad: -0.059, -lr * Pred Grad: 0.052, New P: 2.360
-Original Grad: -0.623, -lr * Pred Grad: -0.256, New P: 1.560
iter 20 loss: 0.304
Actual params: [2.36, 1.56]
-Original Grad: -0.077, -lr * Pred Grad: -0.109, New P: 2.251
-Original Grad: -0.465, -lr * Pred Grad: -0.574, New P: 0.986
iter 21 loss: 0.155
Actual params: [2.251 , 0.9858]
-Original Grad: -0.086, -lr * Pred Grad: -0.240, New P: 2.011
-Original Grad: -0.269, -lr * Pred Grad: -0.565, New P: 0.421
iter 22 loss: 0.121
Actual params: [2.0107, 0.4206]
-Original Grad: -0.096, -lr * Pred Grad: -0.375, New P: 1.636
-Original Grad: 0.040, -lr * Pred Grad: -0.433, New P: -0.012
iter 23 loss: 0.167
Actual params: [ 1.6356, -0.0121]
-Original Grad: 0.233, -lr * Pred Grad: 0.237, New P: 1.873
-Original Grad: -0.076, -lr * Pred Grad: -0.384, New P: -0.396
iter 24 loss: 0.374
Actual params: [ 1.8725, -0.396 ]
-Original Grad: -0.356, -lr * Pred Grad: -0.619, New P: 1.254
-Original Grad: 2.712, -lr * Pred Grad: 2.390, New P: 1.994
iter 25 loss: 0.365
Actual params: [1.254 , 1.9937]
-Original Grad: -0.080, -lr * Pred Grad: -0.684, New P: 0.570
-Original Grad: -0.680, -lr * Pred Grad: -0.868, New P: 1.126
iter 26 loss: 0.364
Actual params: [0.5704, 1.1257]
-Original Grad: 0.149, -lr * Pred Grad: -0.201, New P: 0.369
-Original Grad: 0.039, -lr * Pred Grad: -0.126, New P: 0.999
iter 27 loss: 0.396
Actual params: [0.3689, 0.9994]
-Original Grad: 0.270, -lr * Pred Grad: 0.291, New P: 0.660
-Original Grad: 0.091, -lr * Pred Grad: -0.209, New P: 0.791
iter 28 loss: 0.379
Actual params: [0.66  , 0.7907]
-Original Grad: 0.026, -lr * Pred Grad: 0.132, New P: 0.792
-Original Grad: 0.198, -lr * Pred Grad: 0.423, New P: 1.214
iter 29 loss: 0.309
Actual params: [0.7919, 1.2138]
-Original Grad: 0.269, -lr * Pred Grad: 0.721, New P: 1.513
-Original Grad: 0.083, -lr * Pred Grad: 0.418, New P: 1.632
iter 30 loss: 0.273
Actual params: [1.5126, 1.6319]
-Original Grad: -0.032, -lr * Pred Grad: 0.090, New P: 1.602
-Original Grad: -0.649, -lr * Pred Grad: -0.445, New P: 1.187
Target params: [1.3344, 1.5708]
iter 0 loss: 0.486
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.019, -lr * Pred Grad: 0.834, New P: 0.361
-Original Grad: 0.067, -lr * Pred Grad: 0.946, New P: 0.950
iter 1 loss: 0.174
Actual params: [0.3613, 0.9499]
-Original Grad: 0.379, -lr * Pred Grad: 2.283, New P: 2.644
-Original Grad: -0.086, -lr * Pred Grad: -0.325, New P: 0.625
iter 2 loss: 0.157
Actual params: [2.6441, 0.6248]
-Original Grad: -0.155, -lr * Pred Grad: -1.111, New P: 1.534
-Original Grad: 0.169, -lr * Pred Grad: 0.509, New P: 1.134
iter 3 loss: 0.037
Actual params: [1.5335, 1.1339]
-Original Grad: -0.053, -lr * Pred Grad: -0.135, New P: 1.399
-Original Grad: -0.200, -lr * Pred Grad: -0.402, New P: 0.732
iter 4 loss: 0.040
Actual params: [1.3988, 0.7317]
-Original Grad: 0.205, -lr * Pred Grad: 0.423, New P: 1.821
-Original Grad: 0.188, -lr * Pred Grad: 0.238, New P: 0.970
iter 5 loss: 0.043
Actual params: [1.8214, 0.9699]
-Original Grad: -0.104, -lr * Pred Grad: -0.154, New P: 1.667
-Original Grad: -0.161, -lr * Pred Grad: -0.276, New P: 0.694
iter 6 loss: 0.023
Actual params: [1.667 , 0.6942]
-Original Grad: -0.021, -lr * Pred Grad: -0.070, New P: 1.597
-Original Grad: -0.012, -lr * Pred Grad: -0.146, New P: 0.548
iter 7 loss: 0.026
Actual params: [1.597, 0.548]
-Original Grad: 0.003, -lr * Pred Grad: -0.036, New P: 1.561
-Original Grad: 0.061, -lr * Pred Grad: 0.023, New P: 0.571
iter 8 loss: 0.027
Actual params: [1.5608, 0.5706]
-Original Grad: 0.085, -lr * Pred Grad: 0.177, New P: 1.738
-Original Grad: 0.100, -lr * Pred Grad: 0.287, New P: 0.858
iter 9 loss: 0.029
Actual params: [1.7379, 0.8578]
-Original Grad: -0.067, -lr * Pred Grad: -0.105, New P: 1.633
-Original Grad: -0.108, -lr * Pred Grad: -0.212, New P: 0.646
iter 10 loss: 0.023
Actual params: [1.6333, 0.6456]
-Original Grad: -0.020, -lr * Pred Grad: -0.101, New P: 1.532
-Original Grad: -0.005, -lr * Pred Grad: -0.080, New P: 0.566
iter 11 loss: 0.029
Actual params: [1.5321, 0.5657]
-Original Grad: 0.074, -lr * Pred Grad: 0.121, New P: 1.653
-Original Grad: 0.086, -lr * Pred Grad: 0.144, New P: 0.710
iter 12 loss: 0.023
Actual params: [1.6533, 0.7097]
-Original Grad: -0.008, -lr * Pred Grad: 0.012, New P: 1.665
-Original Grad: 0.020, -lr * Pred Grad: 0.087, New P: 0.797
iter 13 loss: 0.023
Actual params: [1.6653, 0.797 ]
-Original Grad: -0.024, -lr * Pred Grad: -0.058, New P: 1.607
-Original Grad: -0.056, -lr * Pred Grad: -0.118, New P: 0.679
iter 14 loss: 0.023
Actual params: [1.607 , 0.6789]
-Original Grad: 0.012, -lr * Pred Grad: -0.006, New P: 1.601
-Original Grad: -0.026, -lr * Pred Grad: -0.110, New P: 0.569
iter 15 loss: 0.025
Actual params: [1.6011, 0.5687]
-Original Grad: 0.012, -lr * Pred Grad: 0.011, New P: 1.612
-Original Grad: 0.049, -lr * Pred Grad: 0.042, New P: 0.611
iter 16 loss: 0.024
Actual params: [1.6118, 0.6108]
-Original Grad: 0.012, -lr * Pred Grad: 0.023, New P: 1.635
-Original Grad: 0.046, -lr * Pred Grad: 0.123, New P: 0.734
iter 17 loss: 0.022
Actual params: [1.6352, 0.7342]
-Original Grad: -0.010, -lr * Pred Grad: -0.027, New P: 1.608
-Original Grad: -0.025, -lr * Pred Grad: -0.047, New P: 0.687
iter 18 loss: 0.023
Actual params: [1.6081, 0.6875]
-Original Grad: -0.001, -lr * Pred Grad: -0.028, New P: 1.580
-Original Grad: 0.012, -lr * Pred Grad: 0.002, New P: 0.689
iter 19 loss: 0.023
Actual params: [1.5798, 0.6891]
-Original Grad: 0.020, -lr * Pred Grad: 0.025, New P: 1.605
-Original Grad: 0.045, -lr * Pred Grad: 0.097, New P: 0.786
iter 20 loss: 0.022
Actual params: [1.6047, 0.7864]
-Original Grad: -0.008, -lr * Pred Grad: -0.027, New P: 1.578
-Original Grad: -0.022, -lr * Pred Grad: -0.044, New P: 0.743
iter 21 loss: 0.022
Actual params: [1.578 , 0.7429]
-Original Grad: 0.020, -lr * Pred Grad: 0.032, New P: 1.610
-Original Grad: 0.033, -lr * Pred Grad: 0.057, New P: 0.800
iter 22 loss: 0.022
Actual params: [1.6096, 0.7998]
-Original Grad: -0.013, -lr * Pred Grad: -0.037, New P: 1.573
-Original Grad: -0.003, -lr * Pred Grad: -0.012, New P: 0.788
iter 23 loss: 0.022
Actual params: [1.5728, 0.7879]
-Original Grad: 0.010, -lr * Pred Grad: -0.001, New P: 1.572
-Original Grad: 0.014, -lr * Pred Grad: 0.018, New P: 0.806
iter 24 loss: 0.022
Actual params: [1.5719, 0.8063]
-Original Grad: 0.008, -lr * Pred Grad: 0.005, New P: 1.577
-Original Grad: -0.008, -lr * Pred Grad: -0.033, New P: 0.773
iter 25 loss: 0.022
Actual params: [1.5766, 0.7732]
-Original Grad: -0.022, -lr * Pred Grad: -0.076, New P: 1.501
-Original Grad: -0.031, -lr * Pred Grad: -0.096, New P: 0.677
iter 26 loss: 0.028
Actual params: [1.5008, 0.6774]
-Original Grad: 0.123, -lr * Pred Grad: 0.279, New P: 1.780
-Original Grad: 0.084, -lr * Pred Grad: 0.155, New P: 0.832
iter 27 loss: 0.030
Actual params: [1.7797, 0.8324]
-Original Grad: -0.082, -lr * Pred Grad: -0.134, New P: 1.645
-Original Grad: -0.071, -lr * Pred Grad: -0.147, New P: 0.686
iter 28 loss: 0.023
Actual params: [1.6454, 0.6856]
-Original Grad: -0.023, -lr * Pred Grad: -0.122, New P: 1.523
-Original Grad: 0.054, -lr * Pred Grad: 0.065, New P: 0.751
iter 29 loss: 0.024
Actual params: [1.5233, 0.7507]
-Original Grad: 0.045, -lr * Pred Grad: 0.040, New P: 1.563
-Original Grad: 0.046, -lr * Pred Grad: 0.124, New P: 0.874
iter 30 loss: 0.023
Actual params: [1.5628, 0.8745]
-Original Grad: -0.045, -lr * Pred Grad: -0.131, New P: 1.431
-Original Grad: -0.092, -lr * Pred Grad: -0.185, New P: 0.689
Target params: [1.3344, 1.5708]
iter 0 loss: 0.170
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.095, -lr * Pred Grad: 1.465, New P: 0.993
-Original Grad: -0.016, -lr * Pred Grad: 0.559, New P: 0.563
iter 1 loss: 0.066
Actual params: [0.9929, 0.5627]
-Original Grad: -0.098, -lr * Pred Grad: -0.704, New P: 0.289
-Original Grad: -0.260, -lr * Pred Grad: -0.611, New P: -0.048
iter 2 loss: 0.056
Actual params: [ 0.2889, -0.0481]
-Original Grad: 0.135, -lr * Pred Grad: 0.812, New P: 1.101
-Original Grad: 0.107, -lr * Pred Grad: 0.074, New P: 0.026
iter 3 loss: 0.026
Actual params: [1.1009, 0.0263]
-Original Grad: -0.050, -lr * Pred Grad: -0.206, New P: 0.895
-Original Grad: -0.013, -lr * Pred Grad: -0.111, New P: -0.085
iter 4 loss: 0.018
Actual params: [ 0.8948, -0.0847]
-Original Grad: -0.027, -lr * Pred Grad: -0.045, New P: 0.850
-Original Grad: 0.038, -lr * Pred Grad: 0.035, New P: -0.049
iter 5 loss: 0.014
Actual params: [ 0.8502, -0.0492]
-Original Grad: -0.037, -lr * Pred Grad: -0.147, New P: 0.703
-Original Grad: 0.080, -lr * Pred Grad: 0.199, New P: 0.150
iter 6 loss: 0.005
Actual params: [0.7031, 0.1497]
-Original Grad: -0.007, -lr * Pred Grad: -0.102, New P: 0.601
-Original Grad: 0.017, -lr * Pred Grad: 0.085, New P: 0.235
iter 7 loss: 0.004
Actual params: [0.6013, 0.2349]
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: 0.540
-Original Grad: -0.009, -lr * Pred Grad: -0.009, New P: 0.226
iter 8 loss: 0.005
Actual params: [0.5397, 0.2257]
-Original Grad: 0.015, -lr * Pred Grad: -0.013, New P: 0.527
-Original Grad: 0.039, -lr * Pred Grad: 0.083, New P: 0.309
iter 9 loss: 0.004
Actual params: [0.527 , 0.3087]
-Original Grad: 0.008, -lr * Pred Grad: -0.003, New P: 0.524
-Original Grad: 0.015, -lr * Pred Grad: 0.048, New P: 0.356
iter 10 loss: 0.004
Actual params: [0.5239, 0.3563]
-Original Grad: -0.018, -lr * Pred Grad: -0.068, New P: 0.456
-Original Grad: -0.027, -lr * Pred Grad: -0.063, New P: 0.293
iter 11 loss: 0.008
Actual params: [0.4559, 0.2935]
-Original Grad: 0.061, -lr * Pred Grad: 0.123, New P: 0.579
-Original Grad: 0.064, -lr * Pred Grad: 0.126, New P: 0.419
iter 12 loss: 0.006
Actual params: [0.5788, 0.419 ]
-Original Grad: -0.013, -lr * Pred Grad: -0.000, New P: 0.579
-Original Grad: -0.039, -lr * Pred Grad: -0.081, New P: 0.338
iter 13 loss: 0.004
Actual params: [0.5787, 0.3377]
-Original Grad: -0.004, -lr * Pred Grad: -0.010, New P: 0.568
-Original Grad: -0.025, -lr * Pred Grad: -0.089, New P: 0.248
iter 14 loss: 0.004
Actual params: [0.5683, 0.2482]
-Original Grad: 0.010, -lr * Pred Grad: 0.013, New P: 0.582
-Original Grad: 0.017, -lr * Pred Grad: -0.023, New P: 0.225
iter 15 loss: 0.004
Actual params: [0.5816, 0.2248]
-Original Grad: 0.013, -lr * Pred Grad: 0.031, New P: 0.613
-Original Grad: 0.030, -lr * Pred Grad: 0.047, New P: 0.272
iter 16 loss: 0.004
Actual params: [0.6129, 0.2723]
-Original Grad: 0.002, -lr * Pred Grad: 0.009, New P: 0.622
-Original Grad: -0.009, -lr * Pred Grad: -0.026, New P: 0.247
iter 17 loss: 0.004
Actual params: [0.6224, 0.2465]
-Original Grad: -0.005, -lr * Pred Grad: -0.019, New P: 0.603
-Original Grad: 0.000, -lr * Pred Grad: -0.023, New P: 0.223
iter 18 loss: 0.004
Actual params: [0.6032, 0.2232]
-Original Grad: -0.002, -lr * Pred Grad: -0.028, New P: 0.576
-Original Grad: 0.007, -lr * Pred Grad: -0.013, New P: 0.210
iter 19 loss: 0.005
Actual params: [0.5756, 0.2103]
-Original Grad: 0.011, -lr * Pred Grad: 0.003, New P: 0.578
-Original Grad: 0.024, -lr * Pred Grad: 0.036, New P: 0.247
iter 20 loss: 0.004
Actual params: [0.5784, 0.2467]
-Original Grad: 0.026, -lr * Pred Grad: 0.056, New P: 0.635
-Original Grad: 0.046, -lr * Pred Grad: 0.121, New P: 0.368
iter 21 loss: 0.006
Actual params: [0.6346, 0.3681]
-Original Grad: -0.034, -lr * Pred Grad: -0.083, New P: 0.551
-Original Grad: -0.056, -lr * Pred Grad: -0.118, New P: 0.250
iter 22 loss: 0.005
Actual params: [0.5512, 0.25  ]
-Original Grad: 0.018, -lr * Pred Grad: 0.000, New P: 0.551
-Original Grad: 0.035, -lr * Pred Grad: 0.027, New P: 0.277
iter 23 loss: 0.004
Actual params: [0.5512, 0.2768]
-Original Grad: 0.011, -lr * Pred Grad: 0.007, New P: 0.559
-Original Grad: 0.012, -lr * Pred Grad: 0.013, New P: 0.289
iter 24 loss: 0.004
Actual params: [0.5587, 0.2895]
-Original Grad: -0.001, -lr * Pred Grad: -0.013, New P: 0.546
-Original Grad: -0.007, -lr * Pred Grad: -0.028, New P: 0.262
iter 25 loss: 0.005
Actual params: [0.5455, 0.2619]
-Original Grad: 0.013, -lr * Pred Grad: 0.018, New P: 0.563
-Original Grad: 0.043, -lr * Pred Grad: 0.082, New P: 0.343
iter 26 loss: 0.004
Actual params: [0.5631, 0.3435]
-Original Grad: 0.000, -lr * Pred Grad: -0.006, New P: 0.557
-Original Grad: -0.030, -lr * Pred Grad: -0.068, New P: 0.276
iter 27 loss: 0.004
Actual params: [0.5575, 0.2755]
-Original Grad: 0.008, -lr * Pred Grad: 0.007, New P: 0.565
-Original Grad: -0.006, -lr * Pred Grad: -0.050, New P: 0.226
iter 28 loss: 0.005
Actual params: [0.5646, 0.226 ]
-Original Grad: 0.018, -lr * Pred Grad: 0.042, New P: 0.607
-Original Grad: 0.014, -lr * Pred Grad: -0.011, New P: 0.215
iter 29 loss: 0.004
Actual params: [0.6069, 0.2154]
-Original Grad: 0.011, -lr * Pred Grad: 0.039, New P: 0.646
-Original Grad: 0.028, -lr * Pred Grad: 0.048, New P: 0.264
iter 30 loss: 0.004
Actual params: [0.6462, 0.2635]
-Original Grad: -0.013, -lr * Pred Grad: -0.024, New P: 0.622
-Original Grad: -0.019, -lr * Pred Grad: -0.050, New P: 0.214
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad: 0.650, New P: 0.178
-Original Grad: 0.000, -lr * Pred Grad: 0.635, New P: 0.638
iter 1 loss: 0.307
Actual params: [0.178 , 0.6383]
-Original Grad: 0.294, -lr * Pred Grad: 2.095, New P: 2.273
-Original Grad: 0.188, -lr * Pred Grad: 1.209, New P: 1.847
iter 2 loss: 0.193
Actual params: [2.273 , 1.8472]
-Original Grad: -0.039, -lr * Pred Grad: -0.542, New P: 1.731
-Original Grad: -0.229, -lr * Pred Grad: -0.843, New P: 1.004
iter 3 loss: 0.043
Actual params: [1.7307, 1.0039]
-Original Grad: -0.123, -lr * Pred Grad: -0.321, New P: 1.410
-Original Grad: 0.064, -lr * Pred Grad: 0.051, New P: 1.055
iter 4 loss: 0.011
Actual params: [1.4101, 1.0553]
-Original Grad: -0.025, -lr * Pred Grad: -0.144, New P: 1.266
-Original Grad: -0.013, -lr * Pred Grad: -0.144, New P: 0.912
iter 5 loss: 0.016
Actual params: [1.2662, 0.9115]
-Original Grad: 0.122, -lr * Pred Grad: 0.252, New P: 1.518
-Original Grad: 0.363, -lr * Pred Grad: 1.054, New P: 1.966
iter 6 loss: 0.147
Actual params: [1.5179, 1.9658]
-Original Grad: -0.096, -lr * Pred Grad: -0.181, New P: 1.337
-Original Grad: -0.356, -lr * Pred Grad: -0.590, New P: 1.376
iter 7 loss: 0.016
Actual params: [1.3373, 1.3757]
-Original Grad: -0.040, -lr * Pred Grad: -0.192, New P: 1.145
-Original Grad: -0.181, -lr * Pred Grad: -0.264, New P: 1.111
iter 8 loss: 0.015
Actual params: [1.1453, 1.1113]
-Original Grad: 0.049, -lr * Pred Grad: 0.005, New P: 1.150
-Original Grad: 0.122, -lr * Pred Grad: -0.133, New P: 0.978
iter 9 loss: 0.022
Actual params: [1.1504, 0.9783]
-Original Grad: 0.094, -lr * Pred Grad: 0.203, New P: 1.354
-Original Grad: 0.131, -lr * Pred Grad: 0.291, New P: 1.270
iter 10 loss: 0.012
Actual params: [1.3537, 1.2695]
-Original Grad: -0.027, -lr * Pred Grad: 0.011, New P: 1.365
-Original Grad: -0.053, -lr * Pred Grad: -0.096, New P: 1.173
iter 11 loss: 0.010
Actual params: [1.3648, 1.1732]
-Original Grad: -0.025, -lr * Pred Grad: -0.050, New P: 1.315
-Original Grad: -0.014, -lr * Pred Grad: -0.055, New P: 1.118
iter 12 loss: 0.010
Actual params: [1.315 , 1.1181]
-Original Grad: 0.010, -lr * Pred Grad: -0.003, New P: 1.312
-Original Grad: 0.006, -lr * Pred Grad: -0.043, New P: 1.075
iter 13 loss: 0.010
Actual params: [1.3118, 1.0755]
-Original Grad: 0.017, -lr * Pred Grad: 0.027, New P: 1.339
-Original Grad: 0.004, -lr * Pred Grad: -0.023, New P: 1.053
iter 14 loss: 0.010
Actual params: [1.3389, 1.0527]
-Original Grad: 0.014, -lr * Pred Grad: 0.039, New P: 1.378
-Original Grad: -0.002, -lr * Pred Grad: -0.034, New P: 1.018
iter 15 loss: 0.010
Actual params: [1.3779, 1.0185]
-Original Grad: -0.026, -lr * Pred Grad: -0.063, New P: 1.315
-Original Grad: -0.011, -lr * Pred Grad: -0.058, New P: 0.961
iter 16 loss: 0.011
Actual params: [1.3152, 0.9606]
-Original Grad: 0.014, -lr * Pred Grad: -0.001, New P: 1.314
-Original Grad: 0.045, -lr * Pred Grad: 0.069, New P: 1.029
iter 17 loss: 0.010
Actual params: [1.3145, 1.0292]
-Original Grad: 0.026, -lr * Pred Grad: 0.052, New P: 1.366
-Original Grad: 0.009, -lr * Pred Grad: 0.025, New P: 1.054
iter 18 loss: 0.010
Actual params: [1.3664, 1.0543]
-Original Grad: -0.018, -lr * Pred Grad: -0.039, New P: 1.328
-Original Grad: -0.012, -lr * Pred Grad: -0.036, New P: 1.018
iter 19 loss: 0.010
Actual params: [1.3279, 1.0183]
-Original Grad: -0.012, -lr * Pred Grad: -0.063, New P: 1.265
-Original Grad: 0.011, -lr * Pred Grad: -0.006, New P: 1.013
iter 20 loss: 0.012
Actual params: [1.2648, 1.0127]
-Original Grad: 0.060, -lr * Pred Grad: 0.116, New P: 1.381
-Original Grad: 0.048, -lr * Pred Grad: 0.104, New P: 1.117
iter 21 loss: 0.011
Actual params: [1.3807, 1.1169]
-Original Grad: -0.014, -lr * Pred Grad: -0.005, New P: 1.376
-Original Grad: 0.013, -lr * Pred Grad: 0.051, New P: 1.167
iter 22 loss: 0.011
Actual params: [1.3755, 1.1675]
-Original Grad: 0.005, -lr * Pred Grad: 0.014, New P: 1.389
-Original Grad: 0.016, -lr * Pred Grad: 0.047, New P: 1.214
iter 23 loss: 0.012
Actual params: [1.3891, 1.2141]
-Original Grad: -0.047, -lr * Pred Grad: -0.140, New P: 1.249
-Original Grad: -0.057, -lr * Pred Grad: -0.131, New P: 1.083
iter 24 loss: 0.012
Actual params: [1.2491, 1.0831]
-Original Grad: 0.041, -lr * Pred Grad: 0.027, New P: 1.276
-Original Grad: 0.038, -lr * Pred Grad: 0.019, New P: 1.102
iter 25 loss: 0.011
Actual params: [1.2757, 1.1017]
-Original Grad: 0.025, -lr * Pred Grad: 0.052, New P: 1.328
-Original Grad: 0.029, -lr * Pred Grad: 0.058, New P: 1.160
iter 26 loss: 0.010
Actual params: [1.328 , 1.1599]
-Original Grad: 0.011, -lr * Pred Grad: 0.045, New P: 1.373
-Original Grad: -0.010, -lr * Pred Grad: -0.021, New P: 1.138
iter 27 loss: 0.010
Actual params: [1.3731, 1.1385]
-Original Grad: -0.023, -lr * Pred Grad: -0.050, New P: 1.323
-Original Grad: 0.011, -lr * Pred Grad: 0.003, New P: 1.142
iter 28 loss: 0.010
Actual params: [1.3231, 1.1419]
-Original Grad: 0.003, -lr * Pred Grad: -0.025, New P: 1.299
-Original Grad: -0.005, -lr * Pred Grad: -0.031, New P: 1.111
iter 29 loss: 0.010
Actual params: [1.2986, 1.1106]
-Original Grad: 0.044, -lr * Pred Grad: 0.091, New P: 1.390
-Original Grad: 0.017, -lr * Pred Grad: 0.013, New P: 1.124
iter 30 loss: 0.011
Actual params: [1.39  , 1.1239]
-Original Grad: -0.021, -lr * Pred Grad: -0.032, New P: 1.358
-Original Grad: -0.003, -lr * Pred Grad: -0.022, New P: 1.102
Target params: [1.3344, 1.5708]
iter 0 loss: 0.320
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.020, -lr * Pred Grad: 0.432, New P: -0.040
-Original Grad: 0.011, -lr * Pred Grad: 0.685, New P: 0.689
iter 1 loss: 0.266
Actual params: [-0.0405,  0.6888]
-Original Grad: 0.158, -lr * Pred Grad: 1.591, New P: 1.550
-Original Grad: 0.033, -lr * Pred Grad: 0.307, New P: 0.996
iter 2 loss: 0.153
Actual params: [1.5502, 0.996 ]
-Original Grad: 0.141, -lr * Pred Grad: 1.134, New P: 2.684
-Original Grad: 0.810, -lr * Pred Grad: 2.518, New P: 3.514
iter 3 loss: 0.326
Actual params: [2.6841, 3.5138]
-Original Grad: 0.004, -lr * Pred Grad: 0.388, New P: 3.072
-Original Grad: -0.271, -lr * Pred Grad: -0.874, New P: 2.640
iter 4 loss: 0.212
Actual params: [3.0718, 2.6396]
-Original Grad: -0.063, -lr * Pred Grad: -0.089, New P: 2.983
-Original Grad: -0.164, -lr * Pred Grad: -0.183, New P: 2.456
iter 5 loss: 0.185
Actual params: [2.9832, 2.4563]
-Original Grad: -0.077, -lr * Pred Grad: -0.227, New P: 2.757
-Original Grad: -0.123, -lr * Pred Grad: -0.382, New P: 2.074
iter 6 loss: 0.135
Actual params: [2.7566, 2.0739]
-Original Grad: -0.100, -lr * Pred Grad: -0.385, New P: 2.372
-Original Grad: -0.030, -lr * Pred Grad: -0.232, New P: 1.842
iter 7 loss: 0.084
Actual params: [2.3716, 1.8418]
-Original Grad: -0.046, -lr * Pred Grad: -0.356, New P: 2.016
-Original Grad: -0.066, -lr * Pred Grad: -0.246, New P: 1.596
iter 8 loss: 0.049
Actual params: [2.0161, 1.5958]
-Original Grad: -0.045, -lr * Pred Grad: -0.369, New P: 1.647
-Original Grad: -0.087, -lr * Pred Grad: -0.271, New P: 1.325
iter 9 loss: 0.017
Actual params: [1.6472, 1.3252]
-Original Grad: -0.008, -lr * Pred Grad: -0.303, New P: 1.344
-Original Grad: -0.077, -lr * Pred Grad: -0.282, New P: 1.043
iter 10 loss: 0.171
Actual params: [1.344 , 1.0432]
-Original Grad: 0.123, -lr * Pred Grad: 0.060, New P: 1.404
-Original Grad: 0.963, -lr * Pred Grad: 1.963, New P: 3.006
iter 11 loss: 0.200
Actual params: [1.4038, 3.0064]
-Original Grad: -0.056, -lr * Pred Grad: -0.172, New P: 1.232
-Original Grad: -0.255, -lr * Pred Grad: -0.813, New P: 2.194
iter 12 loss: 0.029
Actual params: [1.2319, 2.1938]
-Original Grad: -0.050, -lr * Pred Grad: -0.267, New P: 0.965
-Original Grad: -0.067, -lr * Pred Grad: -0.060, New P: 2.134
iter 13 loss: 0.022
Actual params: [0.9646, 2.1342]
-Original Grad: -0.021, -lr * Pred Grad: -0.252, New P: 0.712
-Original Grad: -0.051, -lr * Pred Grad: -0.255, New P: 1.879
iter 14 loss: 0.023
Actual params: [0.7123, 1.8792]
-Original Grad: 0.001, -lr * Pred Grad: -0.193, New P: 0.519
-Original Grad: -0.002, -lr * Pred Grad: -0.092, New P: 1.788
iter 15 loss: 0.060
Actual params: [0.5192, 1.7876]
-Original Grad: 0.261, -lr * Pred Grad: 0.456, New P: 0.975
-Original Grad: -0.137, -lr * Pred Grad: -0.281, New P: 1.507
iter 16 loss: 0.074
Actual params: [0.9749, 1.5068]
-Original Grad: 0.094, -lr * Pred Grad: 0.359, New P: 1.334
-Original Grad: 0.246, -lr * Pred Grad: 0.416, New P: 1.922
iter 17 loss: 0.023
Actual params: [1.334 , 1.9225]
-Original Grad: 0.009, -lr * Pred Grad: 0.272, New P: 1.606
-Original Grad: -0.005, -lr * Pred Grad: 0.063, New P: 1.985
iter 18 loss: 0.035
Actual params: [1.6063, 1.9853]
-Original Grad: -0.078, -lr * Pred Grad: -0.077, New P: 1.530
-Original Grad: -0.055, -lr * Pred Grad: -0.105, New P: 1.880
iter 19 loss: 0.025
Actual params: [1.5297, 1.8804]
-Original Grad: -0.081, -lr * Pred Grad: -0.241, New P: 1.289
-Original Grad: -0.048, -lr * Pred Grad: -0.150, New P: 1.731
iter 20 loss: 0.026
Actual params: [1.2892, 1.7308]
-Original Grad: 0.042, -lr * Pred Grad: -0.020, New P: 1.270
-Original Grad: 0.011, -lr * Pred Grad: -0.069, New P: 1.662
iter 21 loss: 0.027
Actual params: [1.2696, 1.662 ]
-Original Grad: 0.070, -lr * Pred Grad: 0.126, New P: 1.396
-Original Grad: 0.007, -lr * Pred Grad: -0.033, New P: 1.629
iter 22 loss: 0.020
Actual params: [1.3957, 1.6287]
-Original Grad: 0.005, -lr * Pred Grad: 0.058, New P: 1.454
-Original Grad: 0.004, -lr * Pred Grad: -0.023, New P: 1.606
iter 23 loss: 0.019
Actual params: [1.454 , 1.6061]
-Original Grad: 0.004, -lr * Pred Grad: 0.047, New P: 1.501
-Original Grad: -0.022, -lr * Pred Grad: -0.079, New P: 1.527
iter 24 loss: 0.020
Actual params: [1.5014, 1.5271]
-Original Grad: -0.016, -lr * Pred Grad: -0.024, New P: 1.477
-Original Grad: -0.028, -lr * Pred Grad: -0.110, New P: 1.417
iter 25 loss: 0.017
Actual params: [1.4771, 1.4168]
-Original Grad: -0.003, -lr * Pred Grad: -0.027, New P: 1.450
-Original Grad: -0.029, -lr * Pred Grad: -0.127, New P: 1.290
iter 26 loss: 0.018
Actual params: [1.4496, 1.2897]
-Original Grad: 0.017, -lr * Pred Grad: 0.017, New P: 1.467
-Original Grad: 0.162, -lr * Pred Grad: 0.369, New P: 1.658
iter 27 loss: 0.020
Actual params: [1.4667, 1.6582]
-Original Grad: -0.008, -lr * Pred Grad: -0.030, New P: 1.437
-Original Grad: 0.004, -lr * Pred Grad: 0.081, New P: 1.739
iter 28 loss: 0.020
Actual params: [1.4369, 1.7394]
-Original Grad: -0.027, -lr * Pred Grad: -0.103, New P: 1.334
-Original Grad: -0.014, -lr * Pred Grad: -0.015, New P: 1.724
iter 29 loss: 0.023
Actual params: [1.3341, 1.724 ]
-Original Grad: 0.035, -lr * Pred Grad: 0.024, New P: 1.359
-Original Grad: -0.012, -lr * Pred Grad: -0.047, New P: 1.677
iter 30 loss: 0.022
Actual params: [1.3586, 1.6765]
-Original Grad: 0.030, -lr * Pred Grad: 0.067, New P: 1.425
-Original Grad: -0.001, -lr * Pred Grad: -0.039, New P: 1.637
Target params: [1.3344, 1.5708]
iter 0 loss: 0.583
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.023, -lr * Pred Grad: 0.877, New P: 0.405
-Original Grad: 0.014, -lr * Pred Grad: 0.702, New P: 0.706
iter 1 loss: 0.417
Actual params: [0.4048, 0.7056]
-Original Grad: 0.642, -lr * Pred Grad: 2.518, New P: 2.923
-Original Grad: 0.267, -lr * Pred Grad: 1.608, New P: 2.314
iter 2 loss: 0.361
Actual params: [2.9231, 2.3138]
-Original Grad: -0.429, -lr * Pred Grad: -1.484, New P: 1.439
-Original Grad: -0.751, -lr * Pred Grad: -0.860, New P: 1.454
iter 3 loss: 0.073
Actual params: [1.4395, 1.4538]
-Original Grad: -0.120, -lr * Pred Grad: -0.611, New P: 0.828
-Original Grad: 0.366, -lr * Pred Grad: 0.188, New P: 1.642
iter 4 loss: 0.157
Actual params: [0.8285, 1.6417]
-Original Grad: 0.233, -lr * Pred Grad: 0.035, New P: 0.863
-Original Grad: 0.030, -lr * Pred Grad: -0.051, New P: 1.590
iter 5 loss: 0.146
Actual params: [0.8634, 1.5905]
-Original Grad: 0.267, -lr * Pred Grad: 0.349, New P: 1.213
-Original Grad: 0.009, -lr * Pred Grad: 0.014, New P: 1.605
iter 6 loss: 0.056
Actual params: [1.2128, 1.6048]
-Original Grad: 0.161, -lr * Pred Grad: 0.486, New P: 1.699
-Original Grad: 0.101, -lr * Pred Grad: 0.244, New P: 1.849
iter 7 loss: 0.057
Actual params: [1.6991, 1.8486]
-Original Grad: -0.204, -lr * Pred Grad: -0.263, New P: 1.437
-Original Grad: 0.283, -lr * Pred Grad: 1.207, New P: 3.055
iter 8 loss: 0.750
Actual params: [1.4365, 3.0554]
-Original Grad: -0.148, -lr * Pred Grad: -0.458, New P: 0.978
-Original Grad: -0.655, -lr * Pred Grad: -0.632, New P: 2.423
iter 9 loss: 0.294
Actual params: [0.9785, 2.4232]
-Original Grad: 0.398, -lr * Pred Grad: 0.497, New P: 1.476
-Original Grad: -1.339, -lr * Pred Grad: -0.448, New P: 1.975
iter 10 loss: 0.031
Actual params: [1.4759, 1.9749]
-Original Grad: 0.012, -lr * Pred Grad: 0.062, New P: 1.538
-Original Grad: 0.020, -lr * Pred Grad: -0.570, New P: 1.405
iter 11 loss: 0.109
Actual params: [1.538 , 1.4048]
-Original Grad: -0.306, -lr * Pred Grad: -0.561, New P: 0.977
-Original Grad: 0.683, -lr * Pred Grad: 0.587, New P: 1.992
iter 12 loss: 0.151
Actual params: [0.9773, 1.9916]
-Original Grad: 0.297, -lr * Pred Grad: 0.287, New P: 1.264
-Original Grad: -0.274, -lr * Pred Grad: -0.427, New P: 1.564
iter 13 loss: 0.052
Actual params: [1.2645, 1.5643]
-Original Grad: 0.049, -lr * Pred Grad: 0.105, New P: 1.369
-Original Grad: 0.067, -lr * Pred Grad: -0.163, New P: 1.401
iter 14 loss: 0.075
Actual params: [1.3695, 1.4009]
-Original Grad: -0.172, -lr * Pred Grad: -0.295, New P: 1.074
-Original Grad: 0.530, -lr * Pred Grad: 1.280, New P: 2.681
iter 15 loss: 0.473
Actual params: [1.0745, 2.6808]
-Original Grad: 0.631, -lr * Pred Grad: 0.812, New P: 1.886
-Original Grad: -2.214, -lr * Pred Grad: -0.555, New P: 2.126
iter 16 loss: 0.071
Actual params: [1.8863, 2.1259]
-Original Grad: -0.217, -lr * Pred Grad: -0.382, New P: 1.504
-Original Grad: 0.152, -lr * Pred Grad: -0.353, New P: 1.773
iter 17 loss: 0.037
Actual params: [1.5041, 1.7727]
-Original Grad: -0.050, -lr * Pred Grad: -0.204, New P: 1.300
-Original Grad: 0.157, -lr * Pred Grad: -0.267, New P: 1.506
iter 18 loss: 0.055
Actual params: [1.2997, 1.5058]
-Original Grad: -0.043, -lr * Pred Grad: -0.224, New P: 1.076
-Original Grad: 0.294, -lr * Pred Grad: 0.455, New P: 1.961
iter 19 loss: 0.110
Actual params: [1.0757, 1.9608]
-Original Grad: 0.253, -lr * Pred Grad: 0.332, New P: 1.407
-Original Grad: -0.233, -lr * Pred Grad: -0.368, New P: 1.592
iter 20 loss: 0.048
Actual params: [1.4074, 1.5923]
-Original Grad: -0.052, -lr * Pred Grad: -0.031, New P: 1.377
-Original Grad: 0.224, -lr * Pred Grad: 0.227, New P: 1.819
iter 21 loss: 0.037
Actual params: [1.3769, 1.8189]
-Original Grad: 0.025, -lr * Pred Grad: 0.087, New P: 1.464
-Original Grad: 0.026, -lr * Pred Grad: 0.109, New P: 1.928
iter 22 loss: 0.031
Actual params: [1.464 , 1.9278]
-Original Grad: 0.014, -lr * Pred Grad: 0.069, New P: 1.533
-Original Grad: 0.053, -lr * Pred Grad: 0.190, New P: 2.118
iter 23 loss: 0.029
Actual params: [1.5334, 2.1182]
-Original Grad: 0.140, -lr * Pred Grad: 0.379, New P: 1.912
-Original Grad: -0.087, -lr * Pred Grad: -0.172, New P: 1.947
iter 24 loss: 0.095
Actual params: [1.912 , 1.9466]
-Original Grad: -0.192, -lr * Pred Grad: -0.338, New P: 1.574
-Original Grad: 0.228, -lr * Pred Grad: 0.578, New P: 2.524
iter 25 loss: 0.105
Actual params: [1.5739, 2.5243]
-Original Grad: 0.185, -lr * Pred Grad: 0.277, New P: 1.851
-Original Grad: -0.999, -lr * Pred Grad: -0.431, New P: 2.093
iter 26 loss: 0.066
Actual params: [1.8507, 2.093 ]
-Original Grad: -0.207, -lr * Pred Grad: -0.394, New P: 1.457
-Original Grad: 0.133, -lr * Pred Grad: -0.342, New P: 1.751
iter 27 loss: 0.037
Actual params: [1.4568, 1.7509]
-Original Grad: -0.059, -lr * Pred Grad: -0.406, New P: 1.051
-Original Grad: 0.155, -lr * Pred Grad: -0.054, New P: 1.697
iter 28 loss: 0.092
Actual params: [1.0506, 1.6974]
-Original Grad: 0.320, -lr * Pred Grad: 0.371, New P: 1.422
-Original Grad: -0.170, -lr * Pred Grad: -0.318, New P: 1.379
iter 29 loss: 0.089
Actual params: [1.4217, 1.3793]
-Original Grad: -0.121, -lr * Pred Grad: -0.199, New P: 1.222
-Original Grad: 0.503, -lr * Pred Grad: 1.005, New P: 2.385
iter 30 loss: 0.131
Actual params: [1.2224, 2.3847]
-Original Grad: 0.361, -lr * Pred Grad: 0.610, New P: 1.832
-Original Grad: -0.557, -lr * Pred Grad: -0.548, New P: 1.836
Target params: [1.3344, 1.5708]
iter 0 loss: 0.018
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.014, -lr * Pred Grad: 0.785, New P: 0.313
-Original Grad: -0.006, -lr * Pred Grad: 0.605, New P: 0.609
iter 1 loss: 0.024
Actual params: [0.3129, 0.6085]
-Original Grad: -0.120, -lr * Pred Grad: -0.842, New P: -0.529
-Original Grad: 0.076, -lr * Pred Grad: 0.555, New P: 1.163
iter 2 loss: 0.027
Actual params: [-0.5293,  1.1632]
-Original Grad: 0.003, -lr * Pred Grad: -0.127, New P: -0.657
-Original Grad: -0.001, -lr * Pred Grad: -0.103, New P: 1.060
iter 3 loss: 0.027
Actual params: [-0.6567,  1.0605]
-Original Grad: 0.002, -lr * Pred Grad: -0.117, New P: -0.774
-Original Grad: -0.001, -lr * Pred Grad: -0.021, New P: 1.039
iter 4 loss: 0.027
Actual params: [-0.7736,  1.0394]
-Original Grad: 0.001, -lr * Pred Grad: -0.084, New P: -0.858
-Original Grad: -0.001, -lr * Pred Grad: -0.043, New P: 0.996
iter 5 loss: 0.027
Actual params: [-0.8578,  0.9959]
-Original Grad: 0.001, -lr * Pred Grad: -0.064, New P: -0.921
-Original Grad: -0.001, -lr * Pred Grad: -0.043, New P: 0.953
iter 6 loss: 0.027
Actual params: [-0.9215,  0.9534]
-Original Grad: 0.001, -lr * Pred Grad: -0.051, New P: -0.973
-Original Grad: -0.001, -lr * Pred Grad: -0.040, New P: 0.913
iter 7 loss: 0.027
Actual params: [-0.973,  0.913]
-Original Grad: 0.001, -lr * Pred Grad: -0.044, New P: -1.017
-Original Grad: -0.001, -lr * Pred Grad: -0.036, New P: 0.877
iter 8 loss: 0.027
Actual params: [-1.0171,  0.8766]
-Original Grad: 0.000, -lr * Pred Grad: -0.040, New P: -1.057
-Original Grad: -0.001, -lr * Pred Grad: -0.034, New P: 0.842
iter 9 loss: 0.027
Actual params: [-1.0574,  0.8424]
-Original Grad: 0.001, -lr * Pred Grad: -0.038, New P: -1.095
-Original Grad: -0.001, -lr * Pred Grad: -0.034, New P: 0.809
iter 10 loss: 0.027
Actual params: [-1.0955,  0.8089]
-Original Grad: 0.000, -lr * Pred Grad: -0.037, New P: -1.133
-Original Grad: -0.001, -lr * Pred Grad: -0.033, New P: 0.776
iter 11 loss: 0.027
Actual params: [-1.1327,  0.7759]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: -1.169
-Original Grad: -0.001, -lr * Pred Grad: -0.034, New P: 0.742
iter 12 loss: 0.027
Actual params: [-1.1693,  0.7422]
-Original Grad: 0.000, -lr * Pred Grad: -0.037, New P: -1.206
-Original Grad: -0.001, -lr * Pred Grad: -0.034, New P: 0.709
iter 13 loss: 0.027
Actual params: [-1.2058,  0.7085]
-Original Grad: 0.001, -lr * Pred Grad: -0.036, New P: -1.242
-Original Grad: -0.001, -lr * Pred Grad: -0.034, New P: 0.674
iter 14 loss: 0.027
Actual params: [-1.2422,  0.6741]
-Original Grad: 0.000, -lr * Pred Grad: -0.037, New P: -1.279
-Original Grad: -0.001, -lr * Pred Grad: -0.034, New P: 0.640
iter 15 loss: 0.027
Actual params: [-1.279 ,  0.6399]
-Original Grad: 0.000, -lr * Pred Grad: -0.037, New P: -1.316
-Original Grad: -0.001, -lr * Pred Grad: -0.034, New P: 0.606
iter 16 loss: 0.027
Actual params: [-1.3163,  0.6056]
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: -1.354
-Original Grad: -0.001, -lr * Pred Grad: -0.034, New P: 0.571
iter 17 loss: 0.027
Actual params: [-1.354 ,  0.5713]
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: -1.392
-Original Grad: -0.001, -lr * Pred Grad: -0.035, New P: 0.537
iter 18 loss: 0.027
Actual params: [-1.3922,  0.5367]
-Original Grad: 0.001, -lr * Pred Grad: -0.038, New P: -1.430
-Original Grad: -0.001, -lr * Pred Grad: -0.035, New P: 0.502
iter 19 loss: 0.027
Actual params: [-1.4301,  0.5016]
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: -1.468
-Original Grad: -0.001, -lr * Pred Grad: -0.035, New P: 0.466
iter 20 loss: 0.027
Actual params: [-1.4682,  0.4664]
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: -1.506
-Original Grad: -0.001, -lr * Pred Grad: -0.035, New P: 0.431
iter 21 loss: 0.027
Actual params: [-1.5064,  0.4311]
-Original Grad: 0.000, -lr * Pred Grad: -0.039, New P: -1.545
-Original Grad: -0.001, -lr * Pred Grad: -0.035, New P: 0.396
iter 22 loss: 0.027
Actual params: [-1.5449,  0.3962]
-Original Grad: 0.000, -lr * Pred Grad: -0.039, New P: -1.584
-Original Grad: -0.000, -lr * Pred Grad: -0.035, New P: 0.362
iter 23 loss: 0.027
Actual params: [-1.584 ,  0.3616]
-Original Grad: 0.000, -lr * Pred Grad: -0.039, New P: -1.623
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: 0.327
iter 24 loss: 0.027
Actual params: [-1.6234,  0.3274]
-Original Grad: 0.000, -lr * Pred Grad: -0.040, New P: -1.663
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: 0.293
iter 25 loss: 0.027
Actual params: [-1.663 ,  0.2933]
-Original Grad: 0.000, -lr * Pred Grad: -0.040, New P: -1.703
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: 0.259
iter 26 loss: 0.027
Actual params: [-1.7028,  0.2593]
-Original Grad: 0.000, -lr * Pred Grad: -0.040, New P: -1.743
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: 0.226
iter 27 loss: 0.027
Actual params: [-1.7431,  0.2255]
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: -1.784
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: 0.192
iter 28 loss: 0.027
Actual params: [-1.7837,  0.1919]
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: -1.824
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: 0.158
iter 29 loss: 0.027
Actual params: [-1.8245,  0.1584]
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: -1.866
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.125
iter 30 loss: 0.027
Actual params: [-1.8656,  0.1251]
-Original Grad: 0.000, -lr * Pred Grad: -0.041, New P: -1.907
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.092
Target params: [1.3344, 1.5708]
iter 0 loss: 0.612
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.033, -lr * Pred Grad: 0.973, New P: 0.501
-Original Grad: 0.044, -lr * Pred Grad: 0.842, New P: 0.845
iter 1 loss: 0.383
Actual params: [0.5005, 0.8452]
-Original Grad: 0.147, -lr * Pred Grad: 1.589, New P: 2.090
-Original Grad: 0.037, -lr * Pred Grad: 0.323, New P: 1.169
iter 2 loss: 0.420
Actual params: [2.0899, 1.1685]
-Original Grad: -0.088, -lr * Pred Grad: -0.571, New P: 1.519
-Original Grad: -0.062, -lr * Pred Grad: -0.308, New P: 0.860
iter 3 loss: 0.312
Actual params: [1.5185, 0.8605]
-Original Grad: -0.097, -lr * Pred Grad: -0.422, New P: 1.096
-Original Grad: 0.142, -lr * Pred Grad: 0.332, New P: 1.192
iter 4 loss: 0.163
Actual params: [1.0961, 1.1923]
-Original Grad: 0.061, -lr * Pred Grad: 0.107, New P: 1.203
-Original Grad: 0.114, -lr * Pred Grad: 0.471, New P: 1.663
iter 5 loss: 0.153
Actual params: [1.2026, 1.6635]
-Original Grad: 0.104, -lr * Pred Grad: 0.284, New P: 1.487
-Original Grad: -0.053, -lr * Pred Grad: -0.126, New P: 1.537
iter 6 loss: 0.199
Actual params: [1.4871, 1.5374]
-Original Grad: -0.463, -lr * Pred Grad: -0.924, New P: 0.563
-Original Grad: 0.170, -lr * Pred Grad: 0.471, New P: 2.008
iter 7 loss: 0.438
Actual params: [0.563 , 2.0085]
-Original Grad: 0.197, -lr * Pred Grad: -0.125, New P: 0.438
-Original Grad: -0.362, -lr * Pred Grad: -0.414, New P: 1.595
iter 8 loss: 0.404
Actual params: [0.4381, 1.5947]
-Original Grad: 0.492, -lr * Pred Grad: 0.553, New P: 0.991
-Original Grad: -0.237, -lr * Pred Grad: -0.391, New P: 1.204
iter 9 loss: 0.184
Actual params: [0.9911, 1.204 ]
-Original Grad: 0.202, -lr * Pred Grad: 0.494, New P: 1.485
-Original Grad: 0.146, -lr * Pred Grad: -0.133, New P: 1.071
iter 10 loss: 0.264
Actual params: [1.485 , 1.0714]
-Original Grad: -0.263, -lr * Pred Grad: -0.327, New P: 1.158
-Original Grad: 0.402, -lr * Pred Grad: 1.153, New P: 2.225
iter 11 loss: 0.498
Actual params: [1.158 , 2.2247]
-Original Grad: -0.373, -lr * Pred Grad: -0.916, New P: 0.242
-Original Grad: -3.291, -lr * Pred Grad: -0.564, New P: 1.661
iter 12 loss: 0.557
Actual params: [0.2419, 1.661 ]
-Original Grad: 0.384, -lr * Pred Grad: 0.182, New P: 0.424
-Original Grad: -0.158, -lr * Pred Grad: -0.450, New P: 1.211
iter 13 loss: 0.398
Actual params: [0.424 , 1.2111]
-Original Grad: 0.270, -lr * Pred Grad: 0.343, New P: 0.767
-Original Grad: 0.029, -lr * Pred Grad: -0.517, New P: 0.694
iter 14 loss: 0.308
Actual params: [0.7667, 0.6939]
-Original Grad: 0.196, -lr * Pred Grad: 0.564, New P: 1.331
-Original Grad: 0.174, -lr * Pred Grad: -0.176, New P: 0.518
iter 15 loss: 0.323
Actual params: [1.3308, 0.5176]
-Original Grad: -0.150, -lr * Pred Grad: -0.118, New P: 1.213
-Original Grad: 0.154, -lr * Pred Grad: 0.115, New P: 0.632
iter 16 loss: 0.281
Actual params: [1.2132, 0.6323]
-Original Grad: -0.223, -lr * Pred Grad: -0.522, New P: 0.692
-Original Grad: 0.325, -lr * Pred Grad: 1.017, New P: 1.649
iter 17 loss: 0.306
Actual params: [0.6915, 1.6488]
-Original Grad: 0.528, -lr * Pred Grad: 0.603, New P: 1.294
-Original Grad: -0.127, -lr * Pred Grad: -0.381, New P: 1.268
iter 18 loss: 0.159
Actual params: [1.2942, 1.2678]
-Original Grad: -0.198, -lr * Pred Grad: -0.314, New P: 0.980
-Original Grad: 0.241, -lr * Pred Grad: 0.591, New P: 1.858
iter 19 loss: 0.215
Actual params: [0.9802, 1.8584]
-Original Grad: 0.332, -lr * Pred Grad: 0.446, New P: 1.426
-Original Grad: -0.243, -lr * Pred Grad: -0.390, New P: 1.468
iter 20 loss: 0.174
Actual params: [1.4258, 1.4684]
-Original Grad: -0.336, -lr * Pred Grad: -0.469, New P: 0.957
-Original Grad: 0.219, -lr * Pred Grad: 0.305, New P: 1.773
iter 21 loss: 0.216
Actual params: [0.9566, 1.7735]
-Original Grad: 0.154, -lr * Pred Grad: 0.028, New P: 0.985
-Original Grad: 0.072, -lr * Pred Grad: 0.324, New P: 2.098
iter 22 loss: 0.261
Actual params: [0.9848, 2.0975]
-Original Grad: -0.127, -lr * Pred Grad: -0.292, New P: 0.693
-Original Grad: -0.277, -lr * Pred Grad: -0.378, New P: 1.720
iter 23 loss: 0.315
Actual params: [0.6927, 1.7196]
-Original Grad: 0.641, -lr * Pred Grad: 0.719, New P: 1.412
-Original Grad: -0.107, -lr * Pred Grad: -0.313, New P: 1.407
iter 24 loss: 0.175
Actual params: [1.4118, 1.4071]
-Original Grad: -0.138, -lr * Pred Grad: -0.235, New P: 1.177
-Original Grad: 0.100, -lr * Pred Grad: -0.059, New P: 1.348
iter 25 loss: 0.146
Actual params: [1.1773, 1.348 ]
-Original Grad: -0.008, -lr * Pred Grad: -0.030, New P: 1.147
-Original Grad: 0.047, -lr * Pred Grad: 0.075, New P: 1.423
iter 26 loss: 0.149
Actual params: [1.1474, 1.4228]
-Original Grad: 0.111, -lr * Pred Grad: 0.194, New P: 1.341
-Original Grad: 0.011, -lr * Pred Grad: 0.026, New P: 1.449
iter 27 loss: 0.150
Actual params: [1.341 , 1.4486]
-Original Grad: -0.260, -lr * Pred Grad: -0.450, New P: 0.891
-Original Grad: 0.252, -lr * Pred Grad: 0.871, New P: 2.319
iter 28 loss: 0.527
Actual params: [0.8914, 2.3194]
-Original Grad: 0.308, -lr * Pred Grad: 0.331, New P: 1.222
-Original Grad: -2.231, -lr * Pred Grad: -0.502, New P: 1.817
iter 29 loss: 0.177
Actual params: [1.2224, 1.8169]
-Original Grad: -0.104, -lr * Pred Grad: -0.164, New P: 1.059
-Original Grad: -0.370, -lr * Pred Grad: -0.490, New P: 1.327
iter 30 loss: 0.164
Actual params: [1.0587, 1.3272]
-Original Grad: 0.121, -lr * Pred Grad: 0.196, New P: 1.255
-Original Grad: 0.154, -lr * Pred Grad: -0.479, New P: 0.849
Target params: [1.3344, 1.5708]
iter 0 loss: 0.082
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.030, -lr * Pred Grad: 0.325, New P: -0.147
-Original Grad: 0.047, -lr * Pred Grad: 0.856, New P: 0.860
iter 1 loss: 0.074
Actual params: [-0.1471,  0.8599]
-Original Grad: -0.013, -lr * Pred Grad: 0.177, New P: 0.030
-Original Grad: 0.022, -lr * Pred Grad: 0.236, New P: 1.096
iter 2 loss: 0.071
Actual params: [0.0298, 1.0962]
-Original Grad: -0.003, -lr * Pred Grad: 0.122, New P: 0.152
-Original Grad: 0.043, -lr * Pred Grad: 0.104, New P: 1.200
iter 3 loss: 0.067
Actual params: [0.1518, 1.2004]
-Original Grad: 0.006, -lr * Pred Grad: 0.106, New P: 0.258
-Original Grad: 0.063, -lr * Pred Grad: 0.202, New P: 1.402
iter 4 loss: 0.058
Actual params: [0.2575, 1.4025]
-Original Grad: 0.022, -lr * Pred Grad: 0.157, New P: 0.414
-Original Grad: 0.063, -lr * Pred Grad: 0.215, New P: 1.618
iter 5 loss: 0.044
Actual params: [0.4145, 1.6178]
-Original Grad: 0.031, -lr * Pred Grad: 0.196, New P: 0.610
-Original Grad: 0.054, -lr * Pred Grad: 0.200, New P: 1.818
iter 6 loss: 0.030
Actual params: [0.6101, 1.818 ]
-Original Grad: 0.034, -lr * Pred Grad: 0.220, New P: 0.831
-Original Grad: 0.033, -lr * Pred Grad: 0.133, New P: 1.951
iter 7 loss: 0.029
Actual params: [0.8305, 1.9511]
-Original Grad: 0.021, -lr * Pred Grad: 0.186, New P: 1.016
-Original Grad: -0.087, -lr * Pred Grad: -0.167, New P: 1.784
iter 8 loss: 0.018
Actual params: [1.0161, 1.7839]
-Original Grad: 0.027, -lr * Pred Grad: 0.193, New P: 1.209
-Original Grad: 0.034, -lr * Pred Grad: -0.005, New P: 1.779
iter 9 loss: 0.015
Actual params: [1.2089, 1.7792]
-Original Grad: 0.009, -lr * Pred Grad: 0.129, New P: 1.338
-Original Grad: 0.044, -lr * Pred Grad: 0.087, New P: 1.866
iter 10 loss: 0.016
Actual params: [1.3378, 1.8657]
-Original Grad: 0.012, -lr * Pred Grad: 0.112, New P: 1.450
-Original Grad: -0.006, -lr * Pred Grad: 0.002, New P: 1.868
iter 11 loss: 0.015
Actual params: [1.45  , 1.8679]
-Original Grad: 0.013, -lr * Pred Grad: 0.101, New P: 1.551
-Original Grad: 0.007, -lr * Pred Grad: 0.007, New P: 1.875
iter 12 loss: 0.014
Actual params: [1.551 , 1.8746]
-Original Grad: 0.006, -lr * Pred Grad: 0.069, New P: 1.621
-Original Grad: -0.014, -lr * Pred Grad: -0.048, New P: 1.826
iter 13 loss: 0.014
Actual params: [1.6205, 1.8265]
-Original Grad: -0.005, -lr * Pred Grad: 0.024, New P: 1.644
-Original Grad: 0.049, -lr * Pred Grad: 0.091, New P: 1.917
iter 14 loss: 0.014
Actual params: [1.6441, 1.9175]
-Original Grad: 0.001, -lr * Pred Grad: 0.015, New P: 1.659
-Original Grad: 0.000, -lr * Pred Grad: 0.011, New P: 1.929
iter 15 loss: 0.014
Actual params: [1.6592, 1.9286]
-Original Grad: 0.004, -lr * Pred Grad: 0.012, New P: 1.671
-Original Grad: 0.006, -lr * Pred Grad: 0.007, New P: 1.935
iter 16 loss: 0.013
Actual params: [1.6708, 1.9353]
-Original Grad: 0.000, -lr * Pred Grad: -0.003, New P: 1.668
-Original Grad: 0.023, -lr * Pred Grad: 0.044, New P: 1.979
iter 17 loss: 0.014
Actual params: [1.6682, 1.9788]
-Original Grad: 0.005, -lr * Pred Grad: 0.002, New P: 1.670
-Original Grad: 0.006, -lr * Pred Grad: 0.013, New P: 1.992
iter 18 loss: 0.014
Actual params: [1.6704, 1.9919]
-Original Grad: 0.014, -lr * Pred Grad: 0.026, New P: 1.697
-Original Grad: -0.014, -lr * Pred Grad: -0.044, New P: 1.948
iter 19 loss: 0.013
Actual params: [1.6966, 1.9482]
-Original Grad: -0.003, -lr * Pred Grad: -0.009, New P: 1.688
-Original Grad: 0.031, -lr * Pred Grad: 0.044, New P: 1.992
iter 20 loss: 0.014
Actual params: [1.6875, 1.9922]
-Original Grad: 0.013, -lr * Pred Grad: 0.022, New P: 1.710
-Original Grad: -0.005, -lr * Pred Grad: -0.019, New P: 1.974
iter 21 loss: 0.013
Actual params: [1.7099, 1.9737]
-Original Grad: 0.004, -lr * Pred Grad: 0.008, New P: 1.718
-Original Grad: 0.031, -lr * Pred Grad: 0.057, New P: 2.031
iter 22 loss: 0.013
Actual params: [1.7176, 2.031 ]
-Original Grad: 0.006, -lr * Pred Grad: 0.011, New P: 1.729
-Original Grad: 0.026, -lr * Pred Grad: 0.071, New P: 2.102
iter 23 loss: 0.012
Actual params: [1.7287, 2.1023]
-Original Grad: 0.013, -lr * Pred Grad: 0.031, New P: 1.759
-Original Grad: 0.039, -lr * Pred Grad: 0.119, New P: 2.221
iter 24 loss: 0.009
Actual params: [1.7593, 2.2211]
-Original Grad: 0.003, -lr * Pred Grad: 0.014, New P: 1.773
-Original Grad: 0.043, -lr * Pred Grad: 0.148, New P: 2.370
iter 25 loss: 0.006
Actual params: [1.7729, 2.3696]
-Original Grad: -0.001, -lr * Pred Grad: -0.006, New P: 1.767
-Original Grad: 0.021, -lr * Pred Grad: 0.088, New P: 2.458
iter 26 loss: 0.005
Actual params: [1.767 , 2.4579]
-Original Grad: -0.001, -lr * Pred Grad: -0.016, New P: 1.751
-Original Grad: 0.006, -lr * Pred Grad: 0.032, New P: 2.490
iter 27 loss: 0.004
Actual params: [1.7507, 2.4896]
-Original Grad: -0.003, -lr * Pred Grad: -0.032, New P: 1.719
-Original Grad: 0.016, -lr * Pred Grad: 0.039, New P: 2.528
iter 28 loss: 0.004
Actual params: [1.7186, 2.5281]
-Original Grad: -0.002, -lr * Pred Grad: -0.037, New P: 1.681
-Original Grad: 0.006, -lr * Pred Grad: 0.012, New P: 2.540
iter 29 loss: 0.004
Actual params: [1.6814, 2.5405]
-Original Grad: -0.001, -lr * Pred Grad: -0.041, New P: 1.640
-Original Grad: 0.014, -lr * Pred Grad: 0.026, New P: 2.566
iter 30 loss: 0.005
Actual params: [1.6402, 2.5664]
-Original Grad: 0.000, -lr * Pred Grad: -0.040, New P: 1.600
-Original Grad: 0.004, -lr * Pred Grad: 0.002, New P: 2.568
Target params: [1.3344, 1.5708]
iter 0 loss: 0.049
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.014, -lr * Pred Grad: 0.493, New P: 0.021
-Original Grad: 0.015, -lr * Pred Grad: 0.707, New P: 0.710
iter 1 loss: 0.053
Actual params: [0.021 , 0.7104]
-Original Grad: -0.070, -lr * Pred Grad: -0.406, New P: -0.385
-Original Grad: 0.069, -lr * Pred Grad: 0.522, New P: 1.232
iter 2 loss: 0.046
Actual params: [-0.3852,  1.2324]
-Original Grad: -0.000, -lr * Pred Grad: 0.001, New P: -0.385
-Original Grad: 0.001, -lr * Pred Grad: -0.101, New P: 1.131
iter 3 loss: 0.046
Actual params: [-0.3847,  1.1313]
-Original Grad: -0.000, -lr * Pred Grad: -0.039, New P: -0.424
-Original Grad: 0.001, -lr * Pred Grad: -0.014, New P: 1.117
iter 4 loss: 0.046
Actual params: [-0.4239,  1.1168]
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: -0.454
-Original Grad: 0.001, -lr * Pred Grad: -0.038, New P: 1.079
iter 5 loss: 0.046
Actual params: [-0.4541,  1.0793]
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: -0.483
-Original Grad: 0.000, -lr * Pred Grad: -0.037, New P: 1.042
iter 6 loss: 0.046
Actual params: [-0.4832,  1.0423]
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: -0.513
-Original Grad: 0.000, -lr * Pred Grad: -0.035, New P: 1.007
iter 7 loss: 0.046
Actual params: [-0.5126,  1.0068]
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: -0.543
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.975
iter 8 loss: 0.046
Actual params: [-0.5429,  0.9753]
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: -0.575
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.946
iter 9 loss: 0.046
Actual params: [-0.5746,  0.9455]
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.608
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 0.917
iter 10 loss: 0.046
Actual params: [-0.6078,  0.9166]
-Original Grad: -0.000, -lr * Pred Grad: -0.035, New P: -0.643
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 0.888
iter 11 loss: 0.046
Actual params: [-0.6425,  0.8879]
-Original Grad: -0.000, -lr * Pred Grad: -0.036, New P: -0.678
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 0.859
iter 12 loss: 0.046
Actual params: [-0.6785,  0.8587]
-Original Grad: -0.000, -lr * Pred Grad: -0.037, New P: -0.716
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.829
iter 13 loss: 0.046
Actual params: [-0.7158,  0.8291]
-Original Grad: -0.000, -lr * Pred Grad: -0.038, New P: -0.754
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.799
iter 14 loss: 0.046
Actual params: [-0.7542,  0.799 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.039, New P: -0.793
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.769
iter 15 loss: 0.046
Actual params: [-0.7935,  0.7685]
-Original Grad: -0.000, -lr * Pred Grad: -0.040, New P: -0.834
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.738
iter 16 loss: 0.046
Actual params: [-0.8336,  0.7377]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -0.874
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.706
iter 17 loss: 0.046
Actual params: [-0.8744,  0.7064]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -0.916
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.675
iter 18 loss: 0.046
Actual params: [-0.9157,  0.6748]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -0.957
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.643
iter 19 loss: 0.046
Actual params: [-0.9574,  0.643 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -0.999
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.611
iter 20 loss: 0.046
Actual params: [-0.9995,  0.611 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -1.042
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.579
iter 21 loss: 0.046
Actual params: [-1.0419,  0.5789]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.084
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.547
iter 22 loss: 0.046
Actual params: [-1.0845,  0.5467]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.127
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.514
iter 23 loss: 0.046
Actual params: [-1.1273,  0.5144]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.170
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.482
iter 24 loss: 0.046
Actual params: [-1.1702,  0.482 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.213
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.450
iter 25 loss: 0.046
Actual params: [-1.2132,  0.4496]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.256
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.417
iter 26 loss: 0.046
Actual params: [-1.2564,  0.4172]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.300
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.385
iter 27 loss: 0.046
Actual params: [-1.2996,  0.3847]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.343
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.352
iter 28 loss: 0.046
Actual params: [-1.3429,  0.3522]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.386
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.320
iter 29 loss: 0.046
Actual params: [-1.3863,  0.3197]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.430
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.287
iter 30 loss: 0.046
Actual params: [-1.4296,  0.2872]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.473
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.255
Target params: [1.3344, 1.5708]
iter 0 loss: 0.194
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.038, -lr * Pred Grad: 0.229, New P: -0.243
-Original Grad: 0.042, -lr * Pred Grad: 0.833, New P: 0.836
iter 1 loss: 0.182
Actual params: [-0.2433,  0.8361]
-Original Grad: -0.019, -lr * Pred Grad: 0.103, New P: -0.141
-Original Grad: 0.019, -lr * Pred Grad: 0.220, New P: 1.056
iter 2 loss: 0.181
Actual params: [-0.1406,  1.0558]
-Original Grad: -0.020, -lr * Pred Grad: -0.049, New P: -0.190
-Original Grad: 0.027, -lr * Pred Grad: 0.037, New P: 1.092
iter 3 loss: 0.180
Actual params: [-0.1897,  1.0924]
-Original Grad: -0.012, -lr * Pred Grad: -0.063, New P: -0.253
-Original Grad: 0.015, -lr * Pred Grad: 0.034, New P: 1.126
iter 4 loss: 0.180
Actual params: [-0.253 ,  1.1261]
-Original Grad: -0.007, -lr * Pred Grad: -0.071, New P: -0.324
-Original Grad: 0.009, -lr * Pred Grad: 0.002, New P: 1.128
iter 5 loss: 0.179
Actual params: [-0.3244,  1.1279]
-Original Grad: -0.006, -lr * Pred Grad: -0.076, New P: -0.400
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: 1.121
iter 6 loss: 0.179
Actual params: [-0.4   ,  1.1211]
-Original Grad: -0.003, -lr * Pred Grad: -0.071, New P: -0.471
-Original Grad: 0.004, -lr * Pred Grad: -0.014, New P: 1.107
iter 7 loss: 0.179
Actual params: [-0.4706,  1.1071]
-Original Grad: -0.003, -lr * Pred Grad: -0.069, New P: -0.540
-Original Grad: 0.004, -lr * Pred Grad: -0.014, New P: 1.093
iter 8 loss: 0.178
Actual params: [-0.5396,  1.0928]
-Original Grad: -0.002, -lr * Pred Grad: -0.064, New P: -0.603
-Original Grad: 0.002, -lr * Pred Grad: -0.018, New P: 1.075
iter 9 loss: 0.178
Actual params: [-0.6031,  1.0746]
-Original Grad: -0.001, -lr * Pred Grad: -0.060, New P: -0.663
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: 1.055
iter 10 loss: 0.178
Actual params: [-0.6629,  1.0547]
-Original Grad: -0.001, -lr * Pred Grad: -0.058, New P: -0.721
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: 1.034
iter 11 loss: 0.178
Actual params: [-0.721 ,  1.0345]
-Original Grad: -0.001, -lr * Pred Grad: -0.055, New P: -0.777
-Original Grad: 0.001, -lr * Pred Grad: -0.022, New P: 1.012
iter 12 loss: 0.178
Actual params: [-0.7765,  1.0122]
-Original Grad: -0.001, -lr * Pred Grad: -0.054, New P: -0.830
-Original Grad: 0.001, -lr * Pred Grad: -0.023, New P: 0.989
iter 13 loss: 0.178
Actual params: [-0.83  ,  0.9891]
-Original Grad: -0.001, -lr * Pred Grad: -0.053, New P: -0.883
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 0.965
iter 14 loss: 0.178
Actual params: [-0.8826,  0.9651]
-Original Grad: -0.001, -lr * Pred Grad: -0.051, New P: -0.934
-Original Grad: 0.001, -lr * Pred Grad: -0.025, New P: 0.940
iter 15 loss: 0.178
Actual params: [-0.934,  0.94 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.050, New P: -0.984
-Original Grad: 0.001, -lr * Pred Grad: -0.026, New P: 0.914
iter 16 loss: 0.178
Actual params: [-0.9843,  0.9137]
-Original Grad: -0.000, -lr * Pred Grad: -0.050, New P: -1.034
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 0.887
iter 17 loss: 0.178
Actual params: [-1.0339,  0.8867]
-Original Grad: -0.000, -lr * Pred Grad: -0.049, New P: -1.083
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.859
iter 18 loss: 0.178
Actual params: [-1.0829,  0.859 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.131
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.831
iter 19 loss: 0.178
Actual params: [-1.1313,  0.8306]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.179
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.802
iter 20 loss: 0.178
Actual params: [-1.1794,  0.8023]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.227
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.773
iter 21 loss: 0.178
Actual params: [-1.2271,  0.7734]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.275
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.745
iter 22 loss: 0.178
Actual params: [-1.2745,  0.7445]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.322
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.716
iter 23 loss: 0.178
Actual params: [-1.3219,  0.7156]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.369
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.687
iter 24 loss: 0.178
Actual params: [-1.369 ,  0.6865]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.416
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.657
iter 25 loss: 0.178
Actual params: [-1.4159,  0.6571]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.463
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.628
iter 26 loss: 0.178
Actual params: [-1.4627,  0.6276]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.509
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.597
iter 27 loss: 0.178
Actual params: [-1.509 ,  0.5973]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.555
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 0.567
iter 28 loss: 0.178
Actual params: [-1.5553,  0.5671]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.601
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.537
iter 29 loss: 0.178
Actual params: [-1.6015,  0.5367]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.648
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 0.507
iter 30 loss: 0.178
Actual params: [-1.6478,  0.5066]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.694
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.476
Target params: [1.3344, 1.5708]
iter 0 loss: 0.029
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.034, -lr * Pred Grad: 0.280, New P: -0.192
-Original Grad: 0.006, -lr * Pred Grad: 0.662, New P: 0.666
iter 1 loss: 0.027
Actual params: [-0.1924,  0.6656]
-Original Grad: -0.038, -lr * Pred Grad: -0.091, New P: -0.284
-Original Grad: 0.082, -lr * Pred Grad: 0.594, New P: 1.260
iter 2 loss: 0.015
Actual params: [-0.2836,  1.2598]
-Original Grad: -0.003, -lr * Pred Grad: 0.045, New P: -0.239
-Original Grad: 0.008, -lr * Pred Grad: -0.070, New P: 1.189
iter 3 loss: 0.016
Actual params: [-0.2389,  1.1895]
-Original Grad: -0.005, -lr * Pred Grad: -0.022, New P: -0.261
-Original Grad: 0.012, -lr * Pred Grad: 0.010, New P: 1.200
iter 4 loss: 0.016
Actual params: [-0.261 ,  1.1998]
-Original Grad: -0.004, -lr * Pred Grad: -0.031, New P: -0.292
-Original Grad: 0.009, -lr * Pred Grad: -0.004, New P: 1.195
iter 5 loss: 0.016
Actual params: [-0.2916,  1.1955]
-Original Grad: -0.003, -lr * Pred Grad: -0.039, New P: -0.330
-Original Grad: 0.007, -lr * Pred Grad: -0.009, New P: 1.186
iter 6 loss: 0.016
Actual params: [-0.3301,  1.1863]
-Original Grad: -0.003, -lr * Pred Grad: -0.045, New P: -0.375
-Original Grad: 0.008, -lr * Pred Grad: -0.004, New P: 1.182
iter 7 loss: 0.015
Actual params: [-0.3748,  1.1822]
-Original Grad: -0.002, -lr * Pred Grad: -0.047, New P: -0.422
-Original Grad: 0.006, -lr * Pred Grad: -0.005, New P: 1.177
iter 8 loss: 0.015
Actual params: [-0.4215,  1.1774]
-Original Grad: -0.002, -lr * Pred Grad: -0.049, New P: -0.471
-Original Grad: 0.006, -lr * Pred Grad: -0.003, New P: 1.174
iter 9 loss: 0.015
Actual params: [-0.4708,  1.174 ]
-Original Grad: -0.002, -lr * Pred Grad: -0.050, New P: -0.521
-Original Grad: 0.005, -lr * Pred Grad: -0.006, New P: 1.168
iter 10 loss: 0.015
Actual params: [-0.5207,  1.1681]
-Original Grad: -0.001, -lr * Pred Grad: -0.050, New P: -0.571
-Original Grad: 0.005, -lr * Pred Grad: -0.008, New P: 1.160
iter 11 loss: 0.015
Actual params: [-0.5712,  1.1605]
-Original Grad: -0.001, -lr * Pred Grad: -0.049, New P: -0.620
-Original Grad: 0.002, -lr * Pred Grad: -0.015, New P: 1.146
iter 12 loss: 0.015
Actual params: [-0.6199,  1.1459]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -0.668
-Original Grad: 0.002, -lr * Pred Grad: -0.018, New P: 1.128
iter 13 loss: 0.015
Actual params: [-0.6681,  1.128 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -0.716
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: 1.108
iter 14 loss: 0.015
Actual params: [-0.7158,  1.1077]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -0.764
-Original Grad: 0.002, -lr * Pred Grad: -0.021, New P: 1.086
iter 15 loss: 0.015
Actual params: [-0.7635,  1.0864]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -0.812
-Original Grad: 0.003, -lr * Pred Grad: -0.021, New P: 1.066
iter 16 loss: 0.015
Actual params: [-0.8118,  1.0658]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -0.860
-Original Grad: 0.002, -lr * Pred Grad: -0.022, New P: 1.043
iter 17 loss: 0.015
Actual params: [-0.8599,  1.0434]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -0.908
-Original Grad: 0.002, -lr * Pred Grad: -0.023, New P: 1.021
iter 18 loss: 0.015
Actual params: [-0.9083,  1.0209]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -0.957
-Original Grad: 0.002, -lr * Pred Grad: -0.024, New P: 0.997
iter 19 loss: 0.015
Actual params: [-0.9566,  0.9972]
-Original Grad: -0.001, -lr * Pred Grad: -0.049, New P: -1.005
-Original Grad: 0.002, -lr * Pred Grad: -0.024, New P: 0.973
iter 20 loss: 0.015
Actual params: [-1.0052,  0.9734]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.053
-Original Grad: 0.001, -lr * Pred Grad: -0.025, New P: 0.948
iter 21 loss: 0.015
Actual params: [-1.0535,  0.9481]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.102
-Original Grad: 0.001, -lr * Pred Grad: -0.026, New P: 0.922
iter 22 loss: 0.015
Actual params: [-1.1017,  0.922 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -1.150
-Original Grad: 0.001, -lr * Pred Grad: -0.026, New P: 0.896
iter 23 loss: 0.015
Actual params: [-1.1499,  0.8958]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.198
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.868
iter 24 loss: 0.015
Actual params: [-1.1978,  0.8683]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -1.246
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 0.841
iter 25 loss: 0.015
Actual params: [-1.2458,  0.8409]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.294
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.813
iter 26 loss: 0.015
Actual params: [-1.2935,  0.813 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.341
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.785
iter 27 loss: 0.015
Actual params: [-1.3413,  0.7846]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.388
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 0.755
iter 28 loss: 0.015
Actual params: [-1.3884,  0.7552]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.436
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.726
iter 29 loss: 0.015
Actual params: [-1.4355,  0.7259]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.482
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.696
iter 30 loss: 0.015
Actual params: [-1.4824,  0.6961]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.529
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.666
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad: 0.651, New P: 0.179
-Original Grad: 0.000, -lr * Pred Grad: 0.635, New P: 0.638
iter 1 loss: 0.307
Actual params: [0.1788, 0.6385]
-Original Grad: 0.303, -lr * Pred Grad: 2.113, New P: 2.292
-Original Grad: 0.202, -lr * Pred Grad: 1.283, New P: 1.921
iter 2 loss: 0.204
Actual params: [2.2918, 1.9211]
-Original Grad: -0.037, -lr * Pred Grad: -0.543, New P: 1.748
-Original Grad: -0.208, -lr * Pred Grad: -0.832, New P: 1.089
iter 3 loss: 0.045
Actual params: [1.7484, 1.0887]
-Original Grad: -0.129, -lr * Pred Grad: -0.340, New P: 1.409
-Original Grad: -0.069, -lr * Pred Grad: -0.113, New P: 0.976
iter 4 loss: 0.012
Actual params: [1.4087, 0.9758]
-Original Grad: -0.043, -lr * Pred Grad: -0.215, New P: 1.194
-Original Grad: 0.025, -lr * Pred Grad: -0.178, New P: 0.798
iter 5 loss: 0.039
Actual params: [1.1939, 0.7977]
-Original Grad: 0.145, -lr * Pred Grad: 0.275, New P: 1.469
-Original Grad: 0.430, -lr * Pred Grad: 1.225, New P: 2.022
iter 6 loss: 0.153
Actual params: [1.4686, 2.0223]
-Original Grad: -0.097, -lr * Pred Grad: -0.179, New P: 1.289
-Original Grad: -0.390, -lr * Pred Grad: -0.659, New P: 1.363
iter 7 loss: 0.015
Actual params: [1.2892, 1.3634]
-Original Grad: -0.017, -lr * Pred Grad: -0.124, New P: 1.165
-Original Grad: -0.156, -lr * Pred Grad: -0.233, New P: 1.130
iter 8 loss: 0.013
Actual params: [1.1647, 1.1301]
-Original Grad: 0.030, -lr * Pred Grad: -0.009, New P: 1.156
-Original Grad: 0.098, -lr * Pred Grad: -0.170, New P: 0.960
iter 9 loss: 0.022
Actual params: [1.1556, 0.9604]
-Original Grad: 0.092, -lr * Pred Grad: 0.203, New P: 1.359
-Original Grad: 0.087, -lr * Pred Grad: 0.147, New P: 1.107
iter 10 loss: 0.010
Actual params: [1.3589, 1.107 ]
-Original Grad: -0.003, -lr * Pred Grad: 0.073, New P: 1.432
-Original Grad: -0.001, -lr * Pred Grad: 0.010, New P: 1.117
iter 11 loss: 0.012
Actual params: [1.4321, 1.1172]
-Original Grad: -0.038, -lr * Pred Grad: -0.054, New P: 1.378
-Original Grad: -0.006, -lr * Pred Grad: -0.020, New P: 1.097
iter 12 loss: 0.010
Actual params: [1.3783, 1.097 ]
-Original Grad: -0.007, -lr * Pred Grad: -0.047, New P: 1.331
-Original Grad: -0.013, -lr * Pred Grad: -0.059, New P: 1.038
iter 13 loss: 0.010
Actual params: [1.3314, 1.038 ]
-Original Grad: 0.016, -lr * Pred Grad: 0.004, New P: 1.335
-Original Grad: -0.020, -lr * Pred Grad: -0.086, New P: 0.952
iter 14 loss: 0.011
Actual params: [1.3353, 0.9523]
-Original Grad: -0.002, -lr * Pred Grad: -0.023, New P: 1.312
-Original Grad: 0.057, -lr * Pred Grad: 0.083, New P: 1.035
iter 15 loss: 0.010
Actual params: [1.3122, 1.0351]
-Original Grad: 0.024, -lr * Pred Grad: 0.041, New P: 1.353
-Original Grad: 0.014, -lr * Pred Grad: 0.042, New P: 1.077
iter 16 loss: 0.010
Actual params: [1.3532, 1.0773]
-Original Grad: -0.013, -lr * Pred Grad: -0.032, New P: 1.321
-Original Grad: 0.009, -lr * Pred Grad: 0.023, New P: 1.101
iter 17 loss: 0.010
Actual params: [1.3213, 1.1008]
-Original Grad: 0.012, -lr * Pred Grad: 0.008, New P: 1.330
-Original Grad: -0.000, -lr * Pred Grad: -0.011, New P: 1.090
iter 18 loss: 0.010
Actual params: [1.3296, 1.0895]
-Original Grad: 0.033, -lr * Pred Grad: 0.079, New P: 1.409
-Original Grad: 0.007, -lr * Pred Grad: -0.004, New P: 1.086
iter 19 loss: 0.011
Actual params: [1.4089, 1.086 ]
-Original Grad: -0.019, -lr * Pred Grad: -0.028, New P: 1.381
-Original Grad: -0.006, -lr * Pred Grad: -0.036, New P: 1.050
iter 20 loss: 0.010
Actual params: [1.3808, 1.0502]
-Original Grad: -0.018, -lr * Pred Grad: -0.071, New P: 1.309
-Original Grad: -0.002, -lr * Pred Grad: -0.037, New P: 1.014
iter 21 loss: 0.011
Actual params: [1.3093, 1.0135]
-Original Grad: 0.018, -lr * Pred Grad: -0.003, New P: 1.306
-Original Grad: -0.009, -lr * Pred Grad: -0.054, New P: 0.960
iter 22 loss: 0.011
Actual params: [1.3064, 0.9597]
-Original Grad: 0.052, -lr * Pred Grad: 0.120, New P: 1.427
-Original Grad: 0.026, -lr * Pred Grad: 0.023, New P: 0.983
iter 23 loss: 0.013
Actual params: [1.4268, 0.9828]
-Original Grad: -0.060, -lr * Pred Grad: -0.125, New P: 1.302
-Original Grad: 0.021, -lr * Pred Grad: 0.041, New P: 1.024
iter 24 loss: 0.011
Actual params: [1.3016, 1.024 ]
-Original Grad: 0.056, -lr * Pred Grad: 0.089, New P: 1.390
-Original Grad: 0.014, -lr * Pred Grad: 0.032, New P: 1.056
iter 25 loss: 0.011
Actual params: [1.3905, 1.0562]
-Original Grad: -0.025, -lr * Pred Grad: -0.056, New P: 1.335
-Original Grad: 0.002, -lr * Pred Grad: -0.001, New P: 1.055
iter 26 loss: 0.010
Actual params: [1.3346, 1.0548]
-Original Grad: 0.009, -lr * Pred Grad: -0.009, New P: 1.325
-Original Grad: -0.012, -lr * Pred Grad: -0.047, New P: 1.008
iter 27 loss: 0.010
Actual params: [1.3253, 1.0082]
-Original Grad: 0.015, -lr * Pred Grad: 0.019, New P: 1.344
-Original Grad: -0.018, -lr * Pred Grad: -0.075, New P: 0.933
iter 28 loss: 0.011
Actual params: [1.3439, 0.933 ]
-Original Grad: 0.014, -lr * Pred Grad: 0.034, New P: 1.378
-Original Grad: 0.171, -lr * Pred Grad: 0.456, New P: 1.389
iter 29 loss: 0.019
Actual params: [1.3783, 1.3891]
-Original Grad: -0.044, -lr * Pred Grad: -0.121, New P: 1.258
-Original Grad: -0.147, -lr * Pred Grad: -0.299, New P: 1.090
iter 30 loss: 0.011
Actual params: [1.2576, 1.0901]
-Original Grad: 0.033, -lr * Pred Grad: 0.019, New P: 1.277
-Original Grad: 0.011, -lr * Pred Grad: -0.055, New P: 1.035
Target params: [1.3344, 1.5708]
iter 0 loss: 0.829
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad: 0.639, New P: 0.167
-Original Grad: 0.000, -lr * Pred Grad: 0.634, New P: 0.637
iter 1 loss: 0.882
Actual params: [0.1665, 0.6373]
-Original Grad: -0.026, -lr * Pred Grad: 0.061, New P: 0.227
-Original Grad: 0.022, -lr * Pred Grad: 0.241, New P: 0.879
iter 2 loss: 0.862
Actual params: [0.2272, 0.8787]
-Original Grad: 0.382, -lr * Pred Grad: 1.794, New P: 2.021
-Original Grad: 0.274, -lr * Pred Grad: 1.283, New P: 2.162
iter 3 loss: 0.199
Actual params: [2.0212, 2.1619]
-Original Grad: -0.191, -lr * Pred Grad: -0.974, New P: 1.047
-Original Grad: -0.861, -lr * Pred Grad: -0.674, New P: 1.488
iter 4 loss: 0.208
Actual params: [1.0469, 1.4881]
-Original Grad: 0.113, -lr * Pred Grad: 0.242, New P: 1.289
-Original Grad: 0.342, -lr * Pred Grad: 0.051, New P: 1.539
iter 5 loss: 0.165
Actual params: [1.2893, 1.5392]
-Original Grad: 0.166, -lr * Pred Grad: 0.377, New P: 1.667
-Original Grad: 0.160, -lr * Pred Grad: 0.226, New P: 1.766
iter 6 loss: 0.068
Actual params: [1.6667, 1.7657]
-Original Grad: 0.089, -lr * Pred Grad: 0.430, New P: 2.096
-Original Grad: -0.045, -lr * Pred Grad: -0.066, New P: 1.700
iter 7 loss: 0.098
Actual params: [2.0962, 1.6999]
-Original Grad: -0.150, -lr * Pred Grad: -0.188, New P: 1.908
-Original Grad: -0.492, -lr * Pred Grad: -0.387, New P: 1.313
iter 8 loss: 0.284
Actual params: [1.908, 1.313]
-Original Grad: -0.093, -lr * Pred Grad: -0.281, New P: 1.627
-Original Grad: 0.991, -lr * Pred Grad: 1.452, New P: 2.765
iter 9 loss: 0.387
Actual params: [1.6269, 2.7648]
-Original Grad: -0.140, -lr * Pred Grad: -0.511, New P: 1.116
-Original Grad: -0.786, -lr * Pred Grad: -0.631, New P: 2.134
iter 10 loss: 0.139
Actual params: [1.1162, 2.1343]
-Original Grad: 0.030, -lr * Pred Grad: -0.273, New P: 0.843
-Original Grad: -0.235, -lr * Pred Grad: -0.351, New P: 1.783
iter 11 loss: 0.135
Actual params: [0.843 , 1.7833]
-Original Grad: 0.129, -lr * Pred Grad: 0.066, New P: 0.909
-Original Grad: -0.326, -lr * Pred Grad: -0.554, New P: 1.229
iter 12 loss: 0.358
Actual params: [0.9094, 1.229 ]
-Original Grad: -0.931, -lr * Pred Grad: -1.289, New P: -0.380
-Original Grad: 0.631, -lr * Pred Grad: 0.606, New P: 1.836
iter 13 loss: 0.783
Actual params: [-0.38  ,  1.8355]
-Original Grad: 0.091, -lr * Pred Grad: -0.807, New P: -1.187
-Original Grad: 0.091, -lr * Pred Grad: 0.268, New P: 2.104
iter 14 loss: 0.825
Actual params: [-1.1874,  2.1038]
-Original Grad: 0.016, -lr * Pred Grad: -0.867, New P: -2.055
-Original Grad: -0.002, -lr * Pred Grad: 0.018, New P: 2.122
iter 15 loss: 0.829
Actual params: [-2.0547,  2.1222]
-Original Grad: 0.000, -lr * Pred Grad: -0.821, New P: -2.876
-Original Grad: -0.000, -lr * Pred Grad: 0.007, New P: 2.129
iter 16 loss: 0.829
Actual params: [-2.8758,  2.1294]
-Original Grad: 0.000, -lr * Pred Grad: -0.802, New P: -3.678
-Original Grad: 0.000, -lr * Pred Grad: -0.015, New P: 2.114
iter 17 loss: 0.829
Actual params: [-3.6779,  2.1144]
-Original Grad: 0.000, -lr * Pred Grad: -0.771, New P: -4.449
-Original Grad: -0.000, -lr * Pred Grad: -0.020, New P: 2.094
iter 18 loss: 0.829
Actual params: [-4.4488,  2.0939]
-Original Grad: 0.000, -lr * Pred Grad: -0.730, New P: -5.179
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 2.068
iter 19 loss: 0.829
Actual params: [-5.1791,  2.0685]
-Original Grad: 0.000, -lr * Pred Grad: -0.679, New P: -5.858
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 2.041
iter 20 loss: 0.829
Actual params: [-5.8584,  2.0405]
-Original Grad: 0.000, -lr * Pred Grad: -0.621, New P: -6.480
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 2.011
iter 21 loss: 0.829
Actual params: [-6.4796,  2.0107]
-Original Grad: 0.000, -lr * Pred Grad: -0.559, New P: -7.038
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 1.980
iter 22 loss: 0.829
Actual params: [-7.0383,  1.9798]
-Original Grad: 0.000, -lr * Pred Grad: -0.495, New P: -7.533
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.948
iter 23 loss: 0.829
Actual params: [-7.533 ,  1.9482]
-Original Grad: 0.000, -lr * Pred Grad: -0.431, New P: -7.964
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.916
iter 24 loss: 0.829
Actual params: [-7.9645,  1.9164]
-Original Grad: 0.000, -lr * Pred Grad: -0.371, New P: -8.335
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.884
iter 25 loss: 0.829
Actual params: [-8.3354,  1.8843]
-Original Grad: 0.000, -lr * Pred Grad: -0.315, New P: -8.650
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.852
iter 26 loss: 0.829
Actual params: [-8.6504,  1.8521]
-Original Grad: 0.000, -lr * Pred Grad: -0.265, New P: -8.915
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.820
iter 27 loss: 0.829
Actual params: [-8.9151,  1.8198]
-Original Grad: 0.000, -lr * Pred Grad: -0.221, New P: -9.136
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.788
iter 28 loss: 0.829
Actual params: [-9.1362,  1.7875]
-Original Grad: 0.000, -lr * Pred Grad: -0.184, New P: -9.320
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.755
iter 29 loss: 0.829
Actual params: [-9.3204,  1.7552]
-Original Grad: 0.000, -lr * Pred Grad: -0.154, New P: -9.474
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.723
iter 30 loss: 0.829
Actual params: [-9.4741,  1.7228]
-Original Grad: 0.000, -lr * Pred Grad: -0.129, New P: -9.603
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.690
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.008, -lr * Pred Grad: 0.563, New P: 0.091
-Original Grad: -0.010, -lr * Pred Grad: 0.587, New P: 0.591
iter 1 loss: 0.127
Actual params: [0.0907, 0.5909]
-Original Grad: 0.000, -lr * Pred Grad: 0.337, New P: 0.427
-Original Grad: 0.105, -lr * Pred Grad: 0.728, New P: 1.319
iter 2 loss: 0.039
Actual params: [0.4273, 1.3187]
-Original Grad: 0.080, -lr * Pred Grad: 0.844, New P: 1.271
-Original Grad: 0.249, -lr * Pred Grad: 1.292, New P: 2.610
iter 3 loss: 0.117
Actual params: [1.271 , 2.6103]
-Original Grad: 0.062, -lr * Pred Grad: 0.578, New P: 1.849
-Original Grad: -0.306, -lr * Pred Grad: -0.877, New P: 1.734
iter 4 loss: 0.011
Actual params: [1.8491, 1.7337]
-Original Grad: -0.014, -lr * Pred Grad: 0.179, New P: 2.028
-Original Grad: 0.066, -lr * Pred Grad: 0.020, New P: 1.754
iter 5 loss: 0.013
Actual params: [2.0279, 1.7541]
-Original Grad: -0.029, -lr * Pred Grad: 0.019, New P: 2.047
-Original Grad: 0.080, -lr * Pred Grad: 0.041, New P: 1.795
iter 6 loss: 0.012
Actual params: [2.0469, 1.7953]
-Original Grad: -0.021, -lr * Pred Grad: -0.031, New P: 2.015
-Original Grad: 0.063, -lr * Pred Grad: 0.198, New P: 1.994
iter 7 loss: 0.007
Actual params: [2.0155, 1.9936]
-Original Grad: -0.009, -lr * Pred Grad: -0.038, New P: 1.977
-Original Grad: 0.025, -lr * Pred Grad: 0.119, New P: 2.112
iter 8 loss: 0.006
Actual params: [1.9774, 2.1122]
-Original Grad: -0.005, -lr * Pred Grad: -0.044, New P: 1.933
-Original Grad: 0.011, -lr * Pred Grad: 0.060, New P: 2.172
iter 9 loss: 0.006
Actual params: [1.9335, 2.1718]
-Original Grad: 0.003, -lr * Pred Grad: -0.029, New P: 1.905
-Original Grad: -0.007, -lr * Pred Grad: -0.009, New P: 2.162
iter 10 loss: 0.006
Actual params: [1.9049, 2.1625]
-Original Grad: 0.009, -lr * Pred Grad: -0.006, New P: 1.899
-Original Grad: -0.015, -lr * Pred Grad: -0.051, New P: 2.111
iter 11 loss: 0.006
Actual params: [1.8993, 2.111 ]
-Original Grad: 0.001, -lr * Pred Grad: -0.015, New P: 1.884
-Original Grad: -0.003, -lr * Pred Grad: -0.042, New P: 2.069
iter 12 loss: 0.006
Actual params: [1.884 , 2.0688]
-Original Grad: -0.000, -lr * Pred Grad: -0.022, New P: 1.862
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: 2.037
iter 13 loss: 0.006
Actual params: [1.8615, 2.0373]
-Original Grad: -0.001, -lr * Pred Grad: -0.029, New P: 1.833
-Original Grad: 0.005, -lr * Pred Grad: -0.017, New P: 2.020
iter 14 loss: 0.006
Actual params: [1.833 , 2.0203]
-Original Grad: 0.004, -lr * Pred Grad: -0.019, New P: 1.814
-Original Grad: -0.001, -lr * Pred Grad: -0.026, New P: 1.994
iter 15 loss: 0.006
Actual params: [1.8136, 1.9939]
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 1.785
-Original Grad: 0.006, -lr * Pred Grad: -0.014, New P: 1.980
iter 16 loss: 0.006
Actual params: [1.7848, 1.9797]
-Original Grad: -0.002, -lr * Pred Grad: -0.037, New P: 1.747
-Original Grad: 0.010, -lr * Pred Grad: 0.000, New P: 1.980
iter 17 loss: 0.006
Actual params: [1.7474, 1.9801]
-Original Grad: 0.003, -lr * Pred Grad: -0.028, New P: 1.720
-Original Grad: -0.003, -lr * Pred Grad: -0.025, New P: 1.955
iter 18 loss: 0.006
Actual params: [1.7198, 1.9549]
-Original Grad: 0.004, -lr * Pred Grad: -0.023, New P: 1.697
-Original Grad: -0.000, -lr * Pred Grad: -0.027, New P: 1.927
iter 19 loss: 0.006
Actual params: [1.6967, 1.9274]
-Original Grad: 0.006, -lr * Pred Grad: -0.012, New P: 1.684
-Original Grad: 0.002, -lr * Pred Grad: -0.025, New P: 1.902
iter 20 loss: 0.006
Actual params: [1.6842, 1.9025]
-Original Grad: 0.005, -lr * Pred Grad: -0.010, New P: 1.674
-Original Grad: 0.009, -lr * Pred Grad: -0.006, New P: 1.897
iter 21 loss: 0.006
Actual params: [1.6739, 1.8968]
-Original Grad: 0.004, -lr * Pred Grad: -0.011, New P: 1.663
-Original Grad: 0.004, -lr * Pred Grad: -0.011, New P: 1.886
iter 22 loss: 0.006
Actual params: [1.6629, 1.8857]
-Original Grad: 0.005, -lr * Pred Grad: -0.008, New P: 1.655
-Original Grad: 0.004, -lr * Pred Grad: -0.013, New P: 1.873
iter 23 loss: 0.006
Actual params: [1.6547, 1.8731]
-Original Grad: 0.001, -lr * Pred Grad: -0.016, New P: 1.639
-Original Grad: 0.008, -lr * Pred Grad: -0.003, New P: 1.870
iter 24 loss: 0.006
Actual params: [1.639 , 1.8698]
-Original Grad: 0.001, -lr * Pred Grad: -0.019, New P: 1.620
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: 1.871
iter 25 loss: 0.006
Actual params: [1.62  , 1.8708]
-Original Grad: 0.000, -lr * Pred Grad: -0.025, New P: 1.595
-Original Grad: 0.010, -lr * Pred Grad: 0.007, New P: 1.878
iter 26 loss: 0.006
Actual params: [1.5948, 1.8778]
-Original Grad: 0.005, -lr * Pred Grad: -0.017, New P: 1.578
-Original Grad: 0.001, -lr * Pred Grad: -0.014, New P: 1.864
iter 27 loss: 0.006
Actual params: [1.5783, 1.8637]
-Original Grad: 0.005, -lr * Pred Grad: -0.013, New P: 1.566
-Original Grad: -0.004, -lr * Pred Grad: -0.033, New P: 1.831
iter 28 loss: 0.006
Actual params: [1.5657, 1.8311]
-Original Grad: 0.004, -lr * Pred Grad: -0.011, New P: 1.555
-Original Grad: 0.010, -lr * Pred Grad: -0.006, New P: 1.825
iter 29 loss: 0.006
Actual params: [1.5549, 1.8249]
-Original Grad: -0.001, -lr * Pred Grad: -0.023, New P: 1.532
-Original Grad: 0.010, -lr * Pred Grad: 0.003, New P: 1.828
iter 30 loss: 0.006
Actual params: [1.532 , 1.8278]
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 1.508
-Original Grad: 0.016, -lr * Pred Grad: 0.022, New P: 1.850
Target params: [1.3344, 1.5708]
iter 0 loss: 0.081
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.008, -lr * Pred Grad: 0.565, New P: 0.093
-Original Grad: 0.009, -lr * Pred Grad: 0.676, New P: 0.679
iter 1 loss: 0.079
Actual params: [0.0925, 0.6792]
-Original Grad: 0.018, -lr * Pred Grad: 0.526, New P: 0.619
-Original Grad: 0.035, -lr * Pred Grad: 0.317, New P: 0.996
iter 2 loss: 0.181
Actual params: [0.6188, 0.9959]
-Original Grad: -0.808, -lr * Pred Grad: -1.983, New P: -1.365
-Original Grad: 0.422, -lr * Pred Grad: 1.868, New P: 2.864
iter 3 loss: 0.079
Actual params: [-1.3645,  2.864 ]
-Original Grad: 0.004, -lr * Pred Grad: -1.285, New P: -2.650
-Original Grad: 0.003, -lr * Pred Grad: -0.147, New P: 2.717
iter 4 loss: 0.080
Actual params: [-2.65  ,  2.7166]
-Original Grad: 0.000, -lr * Pred Grad: -1.362, New P: -4.012
-Original Grad: 0.000, -lr * Pred Grad: -0.056, New P: 2.661
iter 5 loss: 0.080
Actual params: [-4.0124,  2.661 ]
-Original Grad: 0.000, -lr * Pred Grad: -1.222, New P: -5.235
-Original Grad: 0.000, -lr * Pred Grad: -0.018, New P: 2.643
iter 6 loss: 0.080
Actual params: [-5.2347,  2.6433]
-Original Grad: 0.000, -lr * Pred Grad: -1.181, New P: -6.416
-Original Grad: 0.000, -lr * Pred Grad: -0.028, New P: 2.615
iter 7 loss: 0.080
Actual params: [-6.416 ,  2.6148]
-Original Grad: 0.000, -lr * Pred Grad: -1.136, New P: -7.552
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 2.590
iter 8 loss: 0.080
Actual params: [-7.5523,  2.5899]
-Original Grad: -0.000, -lr * Pred Grad: -1.101, New P: -8.654
-Original Grad: -0.000, -lr * Pred Grad: -0.022, New P: 2.568
iter 9 loss: 0.080
Actual params: [-8.6537,  2.5678]
-Original Grad: 0.000, -lr * Pred Grad: -1.060, New P: -9.713
-Original Grad: 0.000, -lr * Pred Grad: -0.021, New P: 2.547
iter 10 loss: 0.080
Actual params: [-9.7133,  2.5472]
-Original Grad: -0.000, -lr * Pred Grad: -1.009, New P: -10.723
-Original Grad: -0.000, -lr * Pred Grad: -0.021, New P: 2.526
iter 11 loss: 0.080
Actual params: [-10.7227,   2.5258]
-Original Grad: -0.000, -lr * Pred Grad: -0.950, New P: -11.673
-Original Grad: -0.000, -lr * Pred Grad: -0.023, New P: 2.503
iter 12 loss: 0.080
Actual params: [-11.6729,   2.5028]
-Original Grad: 0.000, -lr * Pred Grad: -0.883, New P: -12.556
-Original Grad: 0.000, -lr * Pred Grad: -0.025, New P: 2.478
iter 13 loss: 0.080
Actual params: [-12.5558,   2.4781]
-Original Grad: -0.000, -lr * Pred Grad: -0.809, New P: -13.365
-Original Grad: -0.000, -lr * Pred Grad: -0.026, New P: 2.452
iter 14 loss: 0.080
Actual params: [-13.3651,   2.4518]
-Original Grad: -0.000, -lr * Pred Grad: -0.731, New P: -14.096
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 2.424
iter 15 loss: 0.080
Actual params: [-14.0962,   2.4241]
-Original Grad: -0.000, -lr * Pred Grad: -0.650, New P: -14.746
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 2.395
iter 16 loss: 0.080
Actual params: [-14.7463,   2.3954]
-Original Grad: -0.000, -lr * Pred Grad: -0.569, New P: -15.315
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 2.366
iter 17 loss: 0.080
Actual params: [-15.3153,   2.3658]
-Original Grad: -0.000, -lr * Pred Grad: -0.490, New P: -15.805
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 2.336
iter 18 loss: 0.080
Actual params: [-15.8052,   2.3355]
-Original Grad: -0.000, -lr * Pred Grad: -0.415, New P: -16.220
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 2.305
iter 19 loss: 0.080
Actual params: [-16.2204,   2.3047]
-Original Grad: -0.000, -lr * Pred Grad: -0.347, New P: -16.567
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 2.273
iter 20 loss: 0.080
Actual params: [-16.5672,   2.2734]
-Original Grad: -0.000, -lr * Pred Grad: -0.286, New P: -16.854
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.242
iter 21 loss: 0.080
Actual params: [-16.8537,   2.2418]
-Original Grad: -0.000, -lr * Pred Grad: -0.235, New P: -17.088
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.210
iter 22 loss: 0.080
Actual params: [-17.0884,   2.21  ]
-Original Grad: -0.000, -lr * Pred Grad: -0.192, New P: -17.280
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.178
iter 23 loss: 0.080
Actual params: [-17.2804,   2.178 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.158, New P: -17.438
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.146
iter 24 loss: 0.080
Actual params: [-17.438 ,   2.1458]
-Original Grad: -0.000, -lr * Pred Grad: -0.131, New P: -17.569
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.114
iter 25 loss: 0.080
Actual params: [-17.5685,   2.1135]
-Original Grad: -0.000, -lr * Pred Grad: -0.110, New P: -17.678
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.081
iter 26 loss: 0.080
Actual params: [-17.6781,   2.0812]
-Original Grad: -0.000, -lr * Pred Grad: -0.094, New P: -17.772
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.049
iter 27 loss: 0.080
Actual params: [-17.7716,   2.0488]
-Original Grad: -0.000, -lr * Pred Grad: -0.081, New P: -17.853
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.016
iter 28 loss: 0.080
Actual params: [-17.8529,   2.0163]
-Original Grad: -0.000, -lr * Pred Grad: -0.072, New P: -17.925
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.984
iter 29 loss: 0.080
Actual params: [-17.9249,   1.9839]
-Original Grad: -0.000, -lr * Pred Grad: -0.065, New P: -17.990
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.951
iter 30 loss: 0.080
Actual params: [-17.9898,   1.9514]
-Original Grad: -0.000, -lr * Pred Grad: -0.060, New P: -18.049
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 1.919
Target params: [1.3344, 1.5708]
iter 0 loss: 0.492
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.026, -lr * Pred Grad: 0.899, New P: 0.427
-Original Grad: 0.121, -lr * Pred Grad: 1.182, New P: 1.186
iter 1 loss: 0.079
Actual params: [0.4271, 1.1856]
-Original Grad: 0.149, -lr * Pred Grad: 1.602, New P: 2.029
-Original Grad: 0.368, -lr * Pred Grad: 2.054, New P: 3.239
iter 2 loss: 0.570
Actual params: [2.0293, 3.2392]
-Original Grad: 0.012, -lr * Pred Grad: 0.201, New P: 2.230
-Original Grad: -0.555, -lr * Pred Grad: -1.071, New P: 2.168
iter 3 loss: 0.015
Actual params: [2.2305, 2.168 ]
-Original Grad: 0.028, -lr * Pred Grad: 0.557, New P: 2.787
-Original Grad: -0.081, -lr * Pred Grad: -0.120, New P: 2.048
iter 4 loss: 0.050
Actual params: [2.7873, 2.0476]
-Original Grad: -0.133, -lr * Pred Grad: -0.488, New P: 2.299
-Original Grad: 0.053, -lr * Pred Grad: -0.270, New P: 1.778
iter 5 loss: 0.029
Actual params: [2.2989, 1.778 ]
-Original Grad: -0.023, -lr * Pred Grad: -0.182, New P: 2.117
-Original Grad: 0.170, -lr * Pred Grad: 0.303, New P: 2.081
iter 6 loss: 0.014
Actual params: [2.1166, 2.0812]
-Original Grad: 0.036, -lr * Pred Grad: -0.013, New P: 2.103
-Original Grad: -0.053, -lr * Pred Grad: -0.098, New P: 1.984
iter 7 loss: 0.013
Actual params: [2.1034, 1.9835]
-Original Grad: 0.010, -lr * Pred Grad: -0.011, New P: 2.092
-Original Grad: 0.045, -lr * Pred Grad: 0.079, New P: 2.062
iter 8 loss: 0.014
Actual params: [2.0925, 2.0625]
-Original Grad: 0.032, -lr * Pred Grad: 0.070, New P: 2.163
-Original Grad: -0.040, -lr * Pred Grad: -0.096, New P: 1.967
iter 9 loss: 0.013
Actual params: [2.1626, 1.9666]
-Original Grad: -0.012, -lr * Pred Grad: -0.008, New P: 2.154
-Original Grad: 0.068, -lr * Pred Grad: 0.119, New P: 2.086
iter 10 loss: 0.013
Actual params: [2.1545, 2.0856]
-Original Grad: 0.024, -lr * Pred Grad: 0.061, New P: 2.216
-Original Grad: -0.023, -lr * Pred Grad: -0.044, New P: 2.041
iter 11 loss: 0.012
Actual params: [2.2157, 2.0411]
-Original Grad: 0.002, -lr * Pred Grad: 0.024, New P: 2.240
-Original Grad: 0.013, -lr * Pred Grad: 0.007, New P: 2.048
iter 12 loss: 0.012
Actual params: [2.2397, 2.0481]
-Original Grad: -0.010, -lr * Pred Grad: -0.019, New P: 2.220
-Original Grad: 0.003, -lr * Pred Grad: -0.013, New P: 2.035
iter 13 loss: 0.012
Actual params: [2.2202, 2.0347]
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: 2.221
-Original Grad: 0.027, -lr * Pred Grad: 0.049, New P: 2.083
iter 14 loss: 0.012
Actual params: [2.221 , 2.0835]
-Original Grad: 0.008, -lr * Pred Grad: 0.008, New P: 2.229
-Original Grad: -0.018, -lr * Pred Grad: -0.047, New P: 2.036
iter 15 loss: 0.012
Actual params: [2.2288, 2.0363]
-Original Grad: 0.002, -lr * Pred Grad: -0.003, New P: 2.226
-Original Grad: 0.026, -lr * Pred Grad: 0.032, New P: 2.068
iter 16 loss: 0.012
Actual params: [2.2258, 2.0682]
-Original Grad: 0.010, -lr * Pred Grad: 0.015, New P: 2.240
-Original Grad: 0.022, -lr * Pred Grad: 0.048, New P: 2.116
iter 17 loss: 0.013
Actual params: [2.2404, 2.1162]
-Original Grad: 0.003, -lr * Pred Grad: 0.004, New P: 2.244
-Original Grad: -0.006, -lr * Pred Grad: -0.016, New P: 2.100
iter 18 loss: 0.012
Actual params: [2.2441, 2.1001]
-Original Grad: 0.006, -lr * Pred Grad: 0.007, New P: 2.251
-Original Grad: -0.016, -lr * Pred Grad: -0.059, New P: 2.042
iter 19 loss: 0.012
Actual params: [2.2507, 2.0415]
-Original Grad: -0.006, -lr * Pred Grad: -0.028, New P: 2.223
-Original Grad: 0.021, -lr * Pred Grad: 0.009, New P: 2.050
iter 20 loss: 0.012
Actual params: [2.2229, 2.0503]
-Original Grad: -0.003, -lr * Pred Grad: -0.035, New P: 2.188
-Original Grad: 0.024, -lr * Pred Grad: 0.044, New P: 2.095
iter 21 loss: 0.013
Actual params: [2.1876, 2.0945]
-Original Grad: 0.013, -lr * Pred Grad: 0.001, New P: 2.189
-Original Grad: -0.056, -lr * Pred Grad: -0.130, New P: 1.964
iter 22 loss: 0.013
Actual params: [2.1885, 1.9643]
-Original Grad: -0.007, -lr * Pred Grad: -0.038, New P: 2.151
-Original Grad: 0.066, -lr * Pred Grad: 0.089, New P: 2.053
iter 23 loss: 0.012
Actual params: [2.1509, 2.0534]
-Original Grad: 0.004, -lr * Pred Grad: -0.023, New P: 2.128
-Original Grad: 0.011, -lr * Pred Grad: 0.035, New P: 2.088
iter 24 loss: 0.014
Actual params: [2.1281, 2.0882]
-Original Grad: 0.052, -lr * Pred Grad: 0.114, New P: 2.242
-Original Grad: -0.053, -lr * Pred Grad: -0.120, New P: 1.968
iter 25 loss: 0.013
Actual params: [2.2419, 1.968 ]
-Original Grad: -0.020, -lr * Pred Grad: -0.019, New P: 2.223
-Original Grad: 0.105, -lr * Pred Grad: 0.204, New P: 2.172
iter 26 loss: 0.016
Actual params: [2.2233, 2.1724]
-Original Grad: 0.018, -lr * Pred Grad: 0.042, New P: 2.265
-Original Grad: -0.099, -lr * Pred Grad: -0.197, New P: 1.975
iter 27 loss: 0.014
Actual params: [2.265, 1.975]
-Original Grad: -0.013, -lr * Pred Grad: -0.032, New P: 2.233
-Original Grad: 0.036, -lr * Pred Grad: -0.002, New P: 1.973
iter 28 loss: 0.013
Actual params: [2.2334, 1.9728]
-Original Grad: -0.015, -lr * Pred Grad: -0.068, New P: 2.166
-Original Grad: 0.092, -lr * Pred Grad: 0.227, New P: 2.200
iter 29 loss: 0.020
Actual params: [2.1658, 2.1998]
-Original Grad: 0.029, -lr * Pred Grad: 0.029, New P: 2.194
-Original Grad: -0.126, -lr * Pred Grad: -0.241, New P: 1.958
iter 30 loss: 0.013
Actual params: [2.1945, 1.9583]
-Original Grad: 0.002, -lr * Pred Grad: -0.003, New P: 2.191
-Original Grad: 0.061, -lr * Pred Grad: 0.028, New P: 1.987
Target params: [1.3344, 1.5708]
iter 0 loss: 0.239
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.013, -lr * Pred Grad: 0.776, New P: 0.303
-Original Grad: -0.006, -lr * Pred Grad: 0.605, New P: 0.608
iter 1 loss: 0.196
Actual params: [0.3035, 0.6085]
-Original Grad: 0.142, -lr * Pred Grad: 1.556, New P: 1.860
-Original Grad: -0.057, -lr * Pred Grad: -0.158, New P: 0.450
iter 2 loss: 0.094
Actual params: [1.8596, 0.4501]
-Original Grad: -0.049, -lr * Pred Grad: -0.299, New P: 1.560
-Original Grad: -0.212, -lr * Pred Grad: -0.367, New P: 0.083
iter 3 loss: 0.174
Actual params: [1.5603, 0.0826]
-Original Grad: 0.355, -lr * Pred Grad: 1.477, New P: 3.038
-Original Grad: 0.977, -lr * Pred Grad: 1.907, New P: 1.989
iter 4 loss: 0.232
Actual params: [3.0377, 1.9893]
-Original Grad: -0.012, -lr * Pred Grad: -0.295, New P: 2.742
-Original Grad: -0.182, -lr * Pred Grad: -0.712, New P: 1.277
iter 5 loss: 0.173
Actual params: [2.7425, 1.2775]
-Original Grad: -0.033, -lr * Pred Grad: 0.135, New P: 2.877
-Original Grad: -0.075, -lr * Pred Grad: -0.080, New P: 1.198
iter 6 loss: 0.176
Actual params: [2.8774, 1.1979]
-Original Grad: -0.044, -lr * Pred Grad: -0.050, New P: 2.827
-Original Grad: -0.062, -lr * Pred Grad: -0.246, New P: 0.952
iter 7 loss: 0.168
Actual params: [2.8273, 0.952 ]
-Original Grad: -0.039, -lr * Pred Grad: -0.094, New P: 2.733
-Original Grad: -0.062, -lr * Pred Grad: -0.191, New P: 0.761
iter 8 loss: 0.158
Actual params: [2.7331, 0.7609]
-Original Grad: -0.044, -lr * Pred Grad: -0.161, New P: 2.572
-Original Grad: -0.072, -lr * Pred Grad: -0.236, New P: 0.525
iter 9 loss: 0.134
Actual params: [2.5721, 0.5253]
-Original Grad: -0.053, -lr * Pred Grad: -0.240, New P: 2.332
-Original Grad: -0.165, -lr * Pred Grad: -0.344, New P: 0.181
iter 10 loss: 0.108
Actual params: [2.3323, 0.1809]
-Original Grad: -0.054, -lr * Pred Grad: -0.308, New P: 2.025
-Original Grad: 0.019, -lr * Pred Grad: -0.200, New P: -0.019
iter 11 loss: 0.103
Actual params: [ 2.0247, -0.0188]
-Original Grad: 0.035, -lr * Pred Grad: -0.136, New P: 1.889
-Original Grad: 0.032, -lr * Pred Grad: -0.053, New P: -0.072
iter 12 loss: 0.124
Actual params: [ 1.8887, -0.0716]
-Original Grad: 0.090, -lr * Pred Grad: 0.092, New P: 1.981
-Original Grad: 0.021, -lr * Pred Grad: -0.005, New P: -0.077
iter 13 loss: 0.109
Actual params: [ 1.9806, -0.0767]
-Original Grad: 0.188, -lr * Pred Grad: 0.458, New P: 2.438
-Original Grad: 0.101, -lr * Pred Grad: 0.259, New P: 0.183
iter 14 loss: 0.113
Actual params: [2.4384, 0.1827]
-Original Grad: -0.045, -lr * Pred Grad: 0.048, New P: 2.486
-Original Grad: 0.005, -lr * Pred Grad: 0.062, New P: 0.245
iter 15 loss: 0.116
Actual params: [2.4865, 0.2446]
-Original Grad: -0.045, -lr * Pred Grad: -0.045, New P: 2.442
-Original Grad: -0.026, -lr * Pred Grad: -0.054, New P: 0.191
iter 16 loss: 0.113
Actual params: [2.4416, 0.1909]
-Original Grad: -0.042, -lr * Pred Grad: -0.135, New P: 2.306
-Original Grad: 0.026, -lr * Pred Grad: 0.026, New P: 0.217
iter 17 loss: 0.106
Actual params: [2.3062, 0.217 ]
-Original Grad: -0.049, -lr * Pred Grad: -0.219, New P: 2.087
-Original Grad: -0.030, -lr * Pred Grad: -0.084, New P: 0.133
iter 18 loss: 0.096
Actual params: [2.0874, 0.1329]
-Original Grad: -0.024, -lr * Pred Grad: -0.215, New P: 1.873
-Original Grad: 0.081, -lr * Pred Grad: 0.163, New P: 0.296
iter 19 loss: 0.086
Actual params: [1.8728, 0.2955]
-Original Grad: -0.033, -lr * Pred Grad: -0.250, New P: 1.623
-Original Grad: 0.021, -lr * Pred Grad: 0.091, New P: 0.386
iter 20 loss: 0.080
Actual params: [1.6227, 0.3861]
-Original Grad: -0.024, -lr * Pred Grad: -0.258, New P: 1.365
-Original Grad: -0.048, -lr * Pred Grad: -0.101, New P: 0.285
iter 21 loss: 0.143
Actual params: [1.3648, 0.2849]
-Original Grad: 0.103, -lr * Pred Grad: 0.069, New P: 1.434
-Original Grad: 0.454, -lr * Pred Grad: 1.411, New P: 1.696
iter 22 loss: 0.151
Actual params: [1.434 , 1.6962]
-Original Grad: -0.062, -lr * Pred Grad: -0.180, New P: 1.254
-Original Grad: -0.247, -lr * Pred Grad: -0.682, New P: 1.015
iter 23 loss: 0.096
Actual params: [1.2538, 1.0147]
-Original Grad: 0.055, -lr * Pred Grad: 0.033, New P: 1.287
-Original Grad: -0.082, -lr * Pred Grad: -0.108, New P: 0.906
iter 24 loss: 0.090
Actual params: [1.2865, 0.9065]
-Original Grad: -0.027, -lr * Pred Grad: -0.095, New P: 1.191
-Original Grad: -0.040, -lr * Pred Grad: -0.262, New P: 0.644
iter 25 loss: 0.084
Actual params: [1.1913, 0.6443]
-Original Grad: 0.018, -lr * Pred Grad: -0.015, New P: 1.176
-Original Grad: -0.079, -lr * Pred Grad: -0.225, New P: 0.419
iter 26 loss: 0.114
Actual params: [1.1764, 0.4193]
-Original Grad: 0.250, -lr * Pred Grad: 0.571, New P: 1.748
-Original Grad: 1.226, -lr * Pred Grad: 2.186, New P: 2.606
iter 27 loss: 0.307
Actual params: [1.7479, 2.6057]
-Original Grad: 0.039, -lr * Pred Grad: 0.268, New P: 2.015
-Original Grad: -0.281, -lr * Pred Grad: -0.886, New P: 1.720
iter 28 loss: 0.190
Actual params: [2.0154, 1.7199]
-Original Grad: -0.028, -lr * Pred Grad: 0.134, New P: 2.149
-Original Grad: -0.225, -lr * Pred Grad: -0.166, New P: 1.554
iter 29 loss: 0.176
Actual params: [2.1489, 1.5543]
-Original Grad: -0.023, -lr * Pred Grad: 0.033, New P: 2.181
-Original Grad: -0.187, -lr * Pred Grad: -0.449, New P: 1.105
iter 30 loss: 0.144
Actual params: [2.1815, 1.1051]
-Original Grad: -0.029, -lr * Pred Grad: -0.052, New P: 2.130
-Original Grad: -0.116, -lr * Pred Grad: -0.365, New P: 0.740
Target params: [1.3344, 1.5708]
iter 0 loss: 0.065
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.050, -lr * Pred Grad: 1.124, New P: 0.651
-Original Grad: -0.016, -lr * Pred Grad: 0.558, New P: 0.561
iter 1 loss: 0.104
Actual params: [0.6512, 0.561 ]
-Original Grad: -0.187, -lr * Pred Grad: -1.241, New P: -0.590
-Original Grad: 0.376, -lr * Pred Grad: 1.974, New P: 2.535
iter 2 loss: 0.073
Actual params: [-0.59  ,  2.5354]
-Original Grad: -0.004, -lr * Pred Grad: -0.366, New P: -0.956
-Original Grad: -0.010, -lr * Pred Grad: -0.225, New P: 2.311
iter 3 loss: 0.072
Actual params: [-0.9563,  2.3107]
-Original Grad: -0.000, -lr * Pred Grad: -0.261, New P: -1.217
-Original Grad: -0.000, -lr * Pred Grad: -0.057, New P: 2.254
iter 4 loss: 0.072
Actual params: [-1.217 ,  2.2538]
-Original Grad: -0.000, -lr * Pred Grad: -0.208, New P: -1.425
-Original Grad: -0.000, -lr * Pred Grad: -0.027, New P: 2.227
iter 5 loss: 0.072
Actual params: [-1.4254,  2.227 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.161, New P: -1.587
-Original Grad: -0.000, -lr * Pred Grad: -0.037, New P: 2.190
iter 6 loss: 0.072
Actual params: [-1.5866,  2.1897]
-Original Grad: -0.000, -lr * Pred Grad: -0.127, New P: -1.714
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 2.157
iter 7 loss: 0.072
Actual params: [-1.7135,  2.1565]
-Original Grad: -0.000, -lr * Pred Grad: -0.102, New P: -1.816
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 2.128
iter 8 loss: 0.072
Actual params: [-1.8159,  2.1282]
-Original Grad: -0.000, -lr * Pred Grad: -0.085, New P: -1.901
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 2.103
iter 9 loss: 0.072
Actual params: [-1.9012,  2.1032]
-Original Grad: 0.000, -lr * Pred Grad: -0.073, New P: -1.975
-Original Grad: -0.000, -lr * Pred Grad: -0.024, New P: 2.079
iter 10 loss: 0.072
Actual params: [-1.9746,  2.0788]
-Original Grad: 0.000, -lr * Pred Grad: -0.065, New P: -2.039
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 2.054
iter 11 loss: 0.072
Actual params: [-2.0395,  2.0538]
-Original Grad: 0.000, -lr * Pred Grad: -0.059, New P: -2.098
-Original Grad: -0.000, -lr * Pred Grad: -0.026, New P: 2.028
iter 12 loss: 0.072
Actual params: [-2.0984,  2.0277]
-Original Grad: 0.000, -lr * Pred Grad: -0.055, New P: -2.153
-Original Grad: -0.000, -lr * Pred Grad: -0.027, New P: 2.000
iter 13 loss: 0.072
Actual params: [-2.153 ,  2.0005]
-Original Grad: 0.000, -lr * Pred Grad: -0.051, New P: -2.205
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 1.972
iter 14 loss: 0.072
Actual params: [-2.2045,  1.9722]
-Original Grad: 0.000, -lr * Pred Grad: -0.049, New P: -2.254
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 1.943
iter 15 loss: 0.072
Actual params: [-2.2537,  1.943 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.048, New P: -2.301
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 1.913
iter 16 loss: 0.072
Actual params: [-2.3013,  1.9131]
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: -2.348
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 1.883
iter 17 loss: 0.072
Actual params: [-2.3476,  1.8825]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -2.393
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 1.851
iter 18 loss: 0.072
Actual params: [-2.3931,  1.8515]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -2.438
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 1.820
iter 19 loss: 0.072
Actual params: [-2.438 ,  1.8201]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -2.482
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.788
iter 20 loss: 0.072
Actual params: [-2.4823,  1.7883]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -2.526
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.756
iter 21 loss: 0.072
Actual params: [-2.5264,  1.7564]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -2.570
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.724
iter 22 loss: 0.072
Actual params: [-2.5703,  1.7243]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -2.614
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.692
iter 23 loss: 0.072
Actual params: [-2.614 ,  1.6921]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -2.658
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.660
iter 24 loss: 0.072
Actual params: [-2.6575,  1.6598]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -2.701
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.627
iter 25 loss: 0.072
Actual params: [-2.7011,  1.6274]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.745
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.595
iter 26 loss: 0.072
Actual params: [-2.7445,  1.595 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.788
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.563
iter 27 loss: 0.072
Actual params: [-2.788 ,  1.5625]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.831
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.530
iter 28 loss: 0.072
Actual params: [-2.8314,  1.53  ]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.875
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.498
iter 29 loss: 0.072
Actual params: [-2.8748,  1.4975]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.918
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.465
iter 30 loss: 0.072
Actual params: [-2.9183,  1.465 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -2.962
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.432
Target params: [1.3344, 1.5708]
iter 0 loss: 0.575
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.056, -lr * Pred Grad: 0.028, New P: -0.445
-Original Grad: 0.165, -lr * Pred Grad: 1.357, New P: 1.360
iter 1 loss: 0.532
Actual params: [-0.4446,  1.3602]
-Original Grad: 0.068, -lr * Pred Grad: 0.891, New P: 0.447
-Original Grad: 0.005, -lr * Pred Grad: 0.073, New P: 1.433
iter 2 loss: 0.262
Actual params: [0.4467, 1.4332]
-Original Grad: 0.372, -lr * Pred Grad: 1.892, New P: 2.339
-Original Grad: 0.306, -lr * Pred Grad: 1.364, New P: 2.797
iter 3 loss: 0.130
Actual params: [2.3389, 2.7969]
-Original Grad: -0.038, -lr * Pred Grad: -0.424, New P: 1.915
-Original Grad: -0.031, -lr * Pred Grad: -0.240, New P: 2.557
iter 4 loss: 0.121
Actual params: [1.9153, 2.5571]
-Original Grad: 0.011, -lr * Pred Grad: 0.350, New P: 2.265
-Original Grad: -0.380, -lr * Pred Grad: -0.420, New P: 2.137
iter 5 loss: 0.143
Actual params: [2.2653, 2.1373]
-Original Grad: -0.137, -lr * Pred Grad: -0.332, New P: 1.934
-Original Grad: 0.282, -lr * Pred Grad: 0.138, New P: 2.275
iter 6 loss: 0.090
Actual params: [1.9337, 2.2754]
-Original Grad: -0.146, -lr * Pred Grad: -0.526, New P: 1.408
-Original Grad: -0.044, -lr * Pred Grad: -0.095, New P: 2.181
iter 7 loss: 0.067
Actual params: [1.4075, 2.1807]
-Original Grad: 0.016, -lr * Pred Grad: -0.269, New P: 1.138
-Original Grad: -0.050, -lr * Pred Grad: -0.128, New P: 2.053
iter 8 loss: 0.065
Actual params: [1.1383, 2.0527]
-Original Grad: -0.023, -lr * Pred Grad: -0.268, New P: 0.870
-Original Grad: 0.049, -lr * Pred Grad: 0.018, New P: 2.071
iter 9 loss: 0.096
Actual params: [0.8701, 2.0705]
-Original Grad: 0.272, -lr * Pred Grad: 0.364, New P: 1.234
-Original Grad: 0.061, -lr * Pred Grad: 0.158, New P: 2.228
iter 10 loss: 0.073
Actual params: [1.234 , 2.2285]
-Original Grad: -0.002, -lr * Pred Grad: 0.072, New P: 1.306
-Original Grad: -0.309, -lr * Pred Grad: -0.377, New P: 1.851
iter 11 loss: 0.089
Actual params: [1.3057, 1.8514]
-Original Grad: -0.021, -lr * Pred Grad: 0.041, New P: 1.347
-Original Grad: 0.392, -lr * Pred Grad: 0.567, New P: 2.419
iter 12 loss: 0.106
Actual params: [1.3472, 2.4189]
-Original Grad: 0.008, -lr * Pred Grad: 0.053, New P: 1.400
-Original Grad: -0.649, -lr * Pred Grad: -0.433, New P: 1.986
iter 13 loss: 0.071
Actual params: [1.4003, 1.9859]
-Original Grad: -0.006, -lr * Pred Grad: 0.007, New P: 1.407
-Original Grad: 0.195, -lr * Pred Grad: -0.159, New P: 1.826
iter 14 loss: 0.095
Actual params: [1.4075, 1.8265]
-Original Grad: -0.011, -lr * Pred Grad: -0.031, New P: 1.377
-Original Grad: 0.446, -lr * Pred Grad: 1.070, New P: 2.896
iter 15 loss: 0.240
Actual params: [1.3766, 2.8963]
-Original Grad: 0.123, -lr * Pred Grad: 0.294, New P: 1.671
-Original Grad: -0.317, -lr * Pred Grad: -0.595, New P: 2.301
iter 16 loss: 0.073
Actual params: [1.6707, 2.3011]
-Original Grad: 0.010, -lr * Pred Grad: 0.133, New P: 1.803
-Original Grad: -0.394, -lr * Pred Grad: -0.340, New P: 1.961
iter 17 loss: 0.094
Actual params: [1.8034, 1.9615]
-Original Grad: -0.168, -lr * Pred Grad: -0.376, New P: 1.428
-Original Grad: 0.328, -lr * Pred Grad: 0.054, New P: 2.015
iter 18 loss: 0.068
Actual params: [1.4275, 2.0152]
-Original Grad: -0.000, -lr * Pred Grad: -0.211, New P: 1.217
-Original Grad: 0.225, -lr * Pred Grad: 0.726, New P: 2.742
iter 19 loss: 0.205
Actual params: [1.2167, 2.7415]
-Original Grad: 0.027, -lr * Pred Grad: -0.091, New P: 1.126
-Original Grad: -0.613, -lr * Pred Grad: -0.485, New P: 2.256
iter 20 loss: 0.079
Actual params: [1.126 , 2.2562]
-Original Grad: 0.043, -lr * Pred Grad: 0.018, New P: 1.144
-Original Grad: -0.184, -lr * Pred Grad: -0.408, New P: 1.848
iter 21 loss: 0.085
Actual params: [1.1435, 1.8484]
-Original Grad: -0.026, -lr * Pred Grad: -0.089, New P: 1.055
-Original Grad: 0.330, -lr * Pred Grad: 0.143, New P: 1.991
iter 22 loss: 0.070
Actual params: [1.0549, 1.9915]
-Original Grad: 0.030, -lr * Pred Grad: 0.020, New P: 1.075
-Original Grad: 0.122, -lr * Pred Grad: 0.349, New P: 2.341
iter 23 loss: 0.096
Actual params: [1.0747, 2.3407]
-Original Grad: 0.035, -lr * Pred Grad: 0.082, New P: 1.156
-Original Grad: -0.334, -lr * Pred Grad: -0.408, New P: 1.933
iter 24 loss: 0.073
Actual params: [1.1564, 1.9325]
-Original Grad: -0.013, -lr * Pred Grad: -0.009, New P: 1.147
-Original Grad: 0.253, -lr * Pred Grad: 0.179, New P: 2.111
iter 25 loss: 0.066
Actual params: [1.1471, 2.1111]
-Original Grad: 0.010, -lr * Pred Grad: 0.020, New P: 1.167
-Original Grad: -0.195, -lr * Pred Grad: -0.315, New P: 1.796
iter 26 loss: 0.095
Actual params: [1.1674, 1.7961]
-Original Grad: -0.049, -lr * Pred Grad: -0.145, New P: 1.023
-Original Grad: 0.325, -lr * Pred Grad: 0.583, New P: 2.380
iter 27 loss: 0.109
Actual params: [1.0226, 2.3795]
-Original Grad: 0.093, -lr * Pred Grad: 0.165, New P: 1.188
-Original Grad: -0.382, -lr * Pred Grad: -0.443, New P: 1.936
iter 28 loss: 0.073
Actual params: [1.1878, 1.9365]
-Original Grad: 0.012, -lr * Pred Grad: 0.074, New P: 1.262
-Original Grad: 0.231, -lr * Pred Grad: 0.128, New P: 2.065
iter 29 loss: 0.065
Actual params: [1.2621, 2.0647]
-Original Grad: -0.014, -lr * Pred Grad: 0.008, New P: 1.270
-Original Grad: 0.204, -lr * Pred Grad: 0.691, New P: 2.756
iter 30 loss: 0.206
Actual params: [1.2702, 2.756 ]
-Original Grad: 0.064, -lr * Pred Grad: 0.184, New P: 1.455
-Original Grad: -0.712, -lr * Pred Grad: -0.463, New P: 2.293
Target params: [1.3344, 1.5708]
iter 0 loss: 0.309
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.000, -lr * Pred Grad: 0.642, New P: 0.169
-Original Grad: 0.000, -lr * Pred Grad: 0.636, New P: 0.639
iter 1 loss: 0.214
Actual params: [0.1692, 0.6392]
-Original Grad: 0.393, -lr * Pred Grad: 2.253, New P: 2.422
-Original Grad: 0.355, -lr * Pred Grad: 1.933, New P: 2.572
iter 2 loss: 0.175
Actual params: [2.4223, 2.572 ]
-Original Grad: -0.019, -lr * Pred Grad: -0.604, New P: 1.818
-Original Grad: -0.213, -lr * Pred Grad: -0.862, New P: 1.710
iter 3 loss: 0.041
Actual params: [1.8182, 1.7096]
-Original Grad: -0.025, -lr * Pred Grad: 0.251, New P: 2.069
-Original Grad: -0.026, -lr * Pred Grad: -0.057, New P: 1.652
iter 4 loss: 0.056
Actual params: [2.0695, 1.6523]
-Original Grad: -0.048, -lr * Pred Grad: -0.080, New P: 1.989
-Original Grad: -0.090, -lr * Pred Grad: -0.295, New P: 1.357
iter 5 loss: 0.045
Actual params: [1.989 , 1.3569]
-Original Grad: -0.034, -lr * Pred Grad: -0.084, New P: 1.905
-Original Grad: -0.038, -lr * Pred Grad: -0.192, New P: 1.165
iter 6 loss: 0.040
Actual params: [1.9047, 1.1646]
-Original Grad: -0.018, -lr * Pred Grad: -0.090, New P: 1.815
-Original Grad: -0.003, -lr * Pred Grad: -0.135, New P: 1.030
iter 7 loss: 0.043
Actual params: [1.8149, 1.0297]
-Original Grad: 0.065, -lr * Pred Grad: 0.105, New P: 1.920
-Original Grad: 0.110, -lr * Pred Grad: 0.192, New P: 1.221
iter 8 loss: 0.041
Actual params: [1.9203, 1.2212]
-Original Grad: -0.033, -lr * Pred Grad: -0.056, New P: 1.864
-Original Grad: -0.047, -lr * Pred Grad: -0.088, New P: 1.133
iter 9 loss: 0.039
Actual params: [1.864 , 1.1328]
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 1.835
-Original Grad: 0.007, -lr * Pred Grad: -0.018, New P: 1.115
iter 10 loss: 0.039
Actual params: [1.8349, 1.1146]
-Original Grad: -0.008, -lr * Pred Grad: -0.052, New P: 1.783
-Original Grad: 0.012, -lr * Pred Grad: -0.007, New P: 1.108
iter 11 loss: 0.039
Actual params: [1.7825, 1.1076]
-Original Grad: 0.023, -lr * Pred Grad: 0.018, New P: 1.800
-Original Grad: 0.077, -lr * Pred Grad: 0.196, New P: 1.304
iter 12 loss: 0.036
Actual params: [1.8002, 1.304 ]
-Original Grad: -0.036, -lr * Pred Grad: -0.109, New P: 1.692
-Original Grad: 0.042, -lr * Pred Grad: 0.172, New P: 1.476
iter 13 loss: 0.034
Actual params: [1.6916, 1.4762]
-Original Grad: -0.035, -lr * Pred Grad: -0.171, New P: 1.520
-Original Grad: -0.071, -lr * Pred Grad: -0.145, New P: 1.331
iter 14 loss: 0.033
Actual params: [1.5205, 1.3309]
-Original Grad: 0.033, -lr * Pred Grad: -0.031, New P: 1.490
-Original Grad: 0.028, -lr * Pred Grad: 0.006, New P: 1.337
iter 15 loss: 0.034
Actual params: [1.4897, 1.3368]
-Original Grad: 0.022, -lr * Pred Grad: 0.008, New P: 1.498
-Original Grad: 0.060, -lr * Pred Grad: 0.139, New P: 1.475
iter 16 loss: 0.032
Actual params: [1.4975, 1.4754]
-Original Grad: -0.001, -lr * Pred Grad: -0.019, New P: 1.478
-Original Grad: 0.040, -lr * Pred Grad: 0.150, New P: 1.625
iter 17 loss: 0.028
Actual params: [1.4784, 1.6252]
-Original Grad: -0.021, -lr * Pred Grad: -0.085, New P: 1.393
-Original Grad: -0.012, -lr * Pred Grad: -0.007, New P: 1.619
iter 18 loss: 0.027
Actual params: [1.3935, 1.6186]
-Original Grad: -0.053, -lr * Pred Grad: -0.217, New P: 1.176
-Original Grad: 0.011, -lr * Pred Grad: 0.017, New P: 1.636
iter 19 loss: 0.027
Actual params: [1.1763, 1.6361]
-Original Grad: -0.015, -lr * Pred Grad: -0.193, New P: 0.983
-Original Grad: 0.062, -lr * Pred Grad: 0.158, New P: 1.794
iter 20 loss: 0.022
Actual params: [0.9834, 1.7938]
-Original Grad: -0.015, -lr * Pred Grad: -0.190, New P: 0.794
-Original Grad: -0.012, -lr * Pred Grad: -0.004, New P: 1.790
iter 21 loss: 0.021
Actual params: [0.7935, 1.7897]
-Original Grad: -0.016, -lr * Pred Grad: -0.194, New P: 0.599
-Original Grad: -0.032, -lr * Pred Grad: -0.082, New P: 1.708
iter 22 loss: 0.027
Actual params: [0.5992, 1.7076]
-Original Grad: 0.044, -lr * Pred Grad: -0.033, New P: 0.567
-Original Grad: -0.038, -lr * Pred Grad: -0.129, New P: 1.578
iter 23 loss: 0.030
Actual params: [0.5666, 1.5782]
-Original Grad: 0.048, -lr * Pred Grad: 0.072, New P: 0.639
-Original Grad: 0.010, -lr * Pred Grad: -0.055, New P: 1.524
iter 24 loss: 0.026
Actual params: [0.6391, 1.5236]
-Original Grad: 0.053, -lr * Pred Grad: 0.168, New P: 0.807
-Original Grad: 0.047, -lr * Pred Grad: 0.074, New P: 1.597
iter 25 loss: 0.024
Actual params: [0.8071, 1.5972]
-Original Grad: -0.057, -lr * Pred Grad: -0.094, New P: 0.714
-Original Grad: 0.060, -lr * Pred Grad: 0.181, New P: 1.778
iter 26 loss: 0.022
Actual params: [0.7136, 1.7778]
-Original Grad: 0.009, -lr * Pred Grad: -0.016, New P: 0.697
-Original Grad: -0.003, -lr * Pred Grad: 0.025, New P: 1.803
iter 27 loss: 0.023
Actual params: [0.6971, 1.8025]
-Original Grad: 0.020, -lr * Pred Grad: 0.029, New P: 0.726
-Original Grad: -0.064, -lr * Pred Grad: -0.141, New P: 1.662
iter 28 loss: 0.021
Actual params: [0.726 , 1.6615]
-Original Grad: 0.038, -lr * Pred Grad: 0.105, New P: 0.831
-Original Grad: 0.004, -lr * Pred Grad: -0.068, New P: 1.594
iter 29 loss: 0.026
Actual params: [0.8313, 1.5936]
-Original Grad: -0.068, -lr * Pred Grad: -0.154, New P: 0.678
-Original Grad: 0.079, -lr * Pred Grad: 0.154, New P: 1.748
iter 30 loss: 0.023
Actual params: [0.6777, 1.748 ]
-Original Grad: 0.032, -lr * Pred Grad: 0.006, New P: 0.683
-Original Grad: -0.014, -lr * Pred Grad: -0.012, New P: 1.736
Target params: [1.3344, 1.5708]
iter 0 loss: 0.049
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.021, -lr * Pred Grad: 0.420, New P: -0.052
-Original Grad: 0.021, -lr * Pred Grad: 0.735, New P: 0.738
iter 1 loss: 0.050
Actual params: [-0.0525,  0.7382]
-Original Grad: -0.044, -lr * Pred Grad: -0.142, New P: -0.195
-Original Grad: 0.040, -lr * Pred Grad: 0.348, New P: 1.086
iter 2 loss: 0.046
Actual params: [-0.1949,  1.086 ]
-Original Grad: -0.001, -lr * Pred Grad: 0.070, New P: -0.124
-Original Grad: 0.002, -lr * Pred Grad: -0.074, New P: 1.012
iter 3 loss: 0.047
Actual params: [-0.1245,  1.0124]
-Original Grad: -0.009, -lr * Pred Grad: -0.035, New P: -0.160
-Original Grad: 0.008, -lr * Pred Grad: 0.008, New P: 1.020
iter 4 loss: 0.047
Actual params: [-0.1599,  1.02  ]
-Original Grad: -0.007, -lr * Pred Grad: -0.048, New P: -0.208
-Original Grad: 0.005, -lr * Pred Grad: -0.023, New P: 0.997
iter 5 loss: 0.047
Actual params: [-0.2076,  0.9965]
-Original Grad: -0.002, -lr * Pred Grad: -0.043, New P: -0.250
-Original Grad: 0.003, -lr * Pred Grad: -0.025, New P: 0.972
iter 6 loss: 0.047
Actual params: [-0.2502,  0.9718]
-Original Grad: -0.003, -lr * Pred Grad: -0.047, New P: -0.297
-Original Grad: 0.003, -lr * Pred Grad: -0.025, New P: 0.947
iter 7 loss: 0.047
Actual params: [-0.2969,  0.9469]
-Original Grad: -0.001, -lr * Pred Grad: -0.045, New P: -0.342
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 0.923
iter 8 loss: 0.046
Actual params: [-0.3416,  0.9226]
-Original Grad: -0.001, -lr * Pred Grad: -0.045, New P: -0.387
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 0.899
iter 9 loss: 0.046
Actual params: [-0.3868,  0.8987]
-Original Grad: -0.001, -lr * Pred Grad: -0.045, New P: -0.432
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 0.875
iter 10 loss: 0.046
Actual params: [-0.4319,  0.8746]
-Original Grad: -0.001, -lr * Pred Grad: -0.045, New P: -0.477
-Original Grad: 0.001, -lr * Pred Grad: -0.025, New P: 0.850
iter 11 loss: 0.046
Actual params: [-0.4772,  0.8496]
-Original Grad: -0.001, -lr * Pred Grad: -0.045, New P: -0.523
-Original Grad: 0.001, -lr * Pred Grad: -0.026, New P: 0.824
iter 12 loss: 0.046
Actual params: [-0.5225,  0.8236]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.567
-Original Grad: 0.000, -lr * Pred Grad: -0.028, New P: 0.796
iter 13 loss: 0.046
Actual params: [-0.5666,  0.7958]
-Original Grad: -0.001, -lr * Pred Grad: -0.045, New P: -0.612
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.768
iter 14 loss: 0.046
Actual params: [-0.6116,  0.7677]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.656
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 0.738
iter 15 loss: 0.046
Actual params: [-0.6557,  0.7383]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.700
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.708
iter 16 loss: 0.046
Actual params: [-0.6997,  0.7083]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.744
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.678
iter 17 loss: 0.046
Actual params: [-0.7438,  0.6779]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.788
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.647
iter 18 loss: 0.046
Actual params: [-0.7882,  0.6471]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.832
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.616
iter 19 loss: 0.046
Actual params: [-0.8324,  0.616 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.877
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.585
iter 20 loss: 0.046
Actual params: [-0.8765,  0.5846]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.920
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.553
iter 21 loss: 0.046
Actual params: [-0.9205,  0.5528]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.964
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.521
iter 22 loss: 0.046
Actual params: [-0.9643,  0.5208]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.008
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.489
iter 23 loss: 0.046
Actual params: [-1.0081,  0.4886]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.052
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.456
iter 24 loss: 0.046
Actual params: [-1.0518,  0.4564]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.096
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.424
iter 25 loss: 0.046
Actual params: [-1.0955,  0.4242]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.139
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.392
iter 26 loss: 0.046
Actual params: [-1.1392,  0.3918]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.183
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.359
iter 27 loss: 0.046
Actual params: [-1.1828,  0.3594]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.226
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.327
iter 28 loss: 0.046
Actual params: [-1.2264,  0.327 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.270
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.295
iter 29 loss: 0.046
Actual params: [-1.2701,  0.2946]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.314
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.262
iter 30 loss: 0.046
Actual params: [-1.3137,  0.2621]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.357
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.230
Target params: [1.3344, 1.5708]
iter 0 loss: 0.431
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad: 0.658, New P: 0.186
-Original Grad: -0.000, -lr * Pred Grad: 0.633, New P: 0.636
iter 1 loss: 0.329
Actual params: [0.186 , 0.6361]
-Original Grad: 0.332, -lr * Pred Grad: 2.169, New P: 2.355
-Original Grad: 0.197, -lr * Pred Grad: 1.252, New P: 1.889
iter 2 loss: 0.038
Actual params: [2.3549, 1.8885]
-Original Grad: -0.018, -lr * Pred Grad: -0.488, New P: 1.867
-Original Grad: -0.125, -lr * Pred Grad: -0.656, New P: 1.232
iter 3 loss: 0.022
Actual params: [1.8671, 1.2321]
-Original Grad: -0.058, -lr * Pred Grad: 0.082, New P: 1.949
-Original Grad: 0.035, -lr * Pred Grad: 0.043, New P: 1.275
iter 4 loss: 0.028
Actual params: [1.9491, 1.2751]
-Original Grad: -0.065, -lr * Pred Grad: -0.172, New P: 1.777
-Original Grad: 0.073, -lr * Pred Grad: 0.112, New P: 1.387
iter 5 loss: 0.014
Actual params: [1.777 , 1.3874]
-Original Grad: 0.006, -lr * Pred Grad: -0.025, New P: 1.752
-Original Grad: -0.008, -lr * Pred Grad: 0.002, New P: 1.389
iter 6 loss: 0.014
Actual params: [1.752 , 1.3891]
-Original Grad: 0.043, -lr * Pred Grad: 0.084, New P: 1.836
-Original Grad: 0.002, -lr * Pred Grad: -0.008, New P: 1.381
iter 7 loss: 0.015
Actual params: [1.8358, 1.3806]
-Original Grad: -0.027, -lr * Pred Grad: -0.042, New P: 1.794
-Original Grad: 0.012, -lr * Pred Grad: 0.007, New P: 1.387
iter 8 loss: 0.014
Actual params: [1.7938, 1.3872]
-Original Grad: -0.019, -lr * Pred Grad: -0.071, New P: 1.723
-Original Grad: 0.010, -lr * Pred Grad: 0.011, New P: 1.399
iter 9 loss: 0.016
Actual params: [1.7228, 1.3986]
-Original Grad: 0.045, -lr * Pred Grad: 0.070, New P: 1.792
-Original Grad: -0.015, -lr * Pred Grad: -0.047, New P: 1.352
iter 10 loss: 0.014
Actual params: [1.7925, 1.3519]
-Original Grad: -0.028, -lr * Pred Grad: -0.061, New P: 1.732
-Original Grad: 0.030, -lr * Pred Grad: 0.040, New P: 1.391
iter 11 loss: 0.015
Actual params: [1.7317, 1.3915]
-Original Grad: 0.011, -lr * Pred Grad: -0.006, New P: 1.726
-Original Grad: -0.013, -lr * Pred Grad: -0.037, New P: 1.354
iter 12 loss: 0.014
Actual params: [1.7257, 1.3542]
-Original Grad: -0.003, -lr * Pred Grad: -0.032, New P: 1.694
-Original Grad: 0.036, -lr * Pred Grad: 0.064, New P: 1.418
iter 13 loss: 0.018
Actual params: [1.6937, 1.4183]
-Original Grad: 0.028, -lr * Pred Grad: 0.047, New P: 1.741
-Original Grad: -0.030, -lr * Pred Grad: -0.072, New P: 1.347
iter 14 loss: 0.014
Actual params: [1.741 , 1.3467]
-Original Grad: 0.042, -lr * Pred Grad: 0.121, New P: 1.862
-Original Grad: -0.009, -lr * Pred Grad: -0.059, New P: 1.288
iter 15 loss: 0.019
Actual params: [1.8621, 1.288 ]
-Original Grad: -0.049, -lr * Pred Grad: -0.087, New P: 1.775
-Original Grad: 0.071, -lr * Pred Grad: 0.137, New P: 1.425
iter 16 loss: 0.014
Actual params: [1.7747, 1.4252]
-Original Grad: 0.023, -lr * Pred Grad: 0.020, New P: 1.794
-Original Grad: -0.026, -lr * Pred Grad: -0.046, New P: 1.380
iter 17 loss: 0.014
Actual params: [1.7944, 1.3796]
-Original Grad: -0.008, -lr * Pred Grad: -0.033, New P: 1.761
-Original Grad: 0.016, -lr * Pred Grad: 0.016, New P: 1.395
iter 18 loss: 0.014
Actual params: [1.7609, 1.3952]
-Original Grad: -0.006, -lr * Pred Grad: -0.045, New P: 1.716
-Original Grad: 0.046, -lr * Pred Grad: 0.106, New P: 1.501
iter 19 loss: 0.020
Actual params: [1.7158, 1.5008]
-Original Grad: 0.063, -lr * Pred Grad: 0.133, New P: 1.849
-Original Grad: -0.056, -lr * Pred Grad: -0.121, New P: 1.380
iter 20 loss: 0.016
Actual params: [1.8488, 1.3802]
-Original Grad: -0.010, -lr * Pred Grad: 0.015, New P: 1.863
-Original Grad: 0.054, -lr * Pred Grad: 0.072, New P: 1.453
iter 21 loss: 0.015
Actual params: [1.8635, 1.4525]
-Original Grad: -0.014, -lr * Pred Grad: -0.029, New P: 1.835
-Original Grad: 0.037, -lr * Pred Grad: 0.104, New P: 1.556
iter 22 loss: 0.014
Actual params: [1.8349, 1.5562]
-Original Grad: -0.002, -lr * Pred Grad: -0.030, New P: 1.805
-Original Grad: -0.012, -lr * Pred Grad: -0.014, New P: 1.542
iter 23 loss: 0.014
Actual params: [1.8045, 1.542 ]
-Original Grad: -0.018, -lr * Pred Grad: -0.084, New P: 1.721
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: 1.543
iter 24 loss: 0.021
Actual params: [1.7205, 1.5431]
-Original Grad: 0.042, -lr * Pred Grad: 0.054, New P: 1.774
-Original Grad: -0.017, -lr * Pred Grad: -0.059, New P: 1.484
iter 25 loss: 0.014
Actual params: [1.7742, 1.484 ]
-Original Grad: 0.051, -lr * Pred Grad: 0.140, New P: 1.914
-Original Grad: 0.013, -lr * Pred Grad: -0.007, New P: 1.477
iter 26 loss: 0.017
Actual params: [1.9145, 1.4767]
-Original Grad: -0.023, -lr * Pred Grad: -0.005, New P: 1.909
-Original Grad: 0.051, -lr * Pred Grad: 0.113, New P: 1.590
iter 27 loss: 0.015
Actual params: [1.9092, 1.5898]
-Original Grad: -0.000, -lr * Pred Grad: 0.003, New P: 1.912
-Original Grad: -0.025, -lr * Pred Grad: -0.049, New P: 1.541
iter 28 loss: 0.015
Actual params: [1.9123, 1.5407]
-Original Grad: 0.001, -lr * Pred Grad: -0.005, New P: 1.907
-Original Grad: -0.002, -lr * Pred Grad: -0.032, New P: 1.509
iter 29 loss: 0.015
Actual params: [1.9071, 1.5087]
-Original Grad: -0.019, -lr * Pred Grad: -0.070, New P: 1.838
-Original Grad: 0.023, -lr * Pred Grad: 0.020, New P: 1.529
iter 30 loss: 0.013
Actual params: [1.8375, 1.5291]
-Original Grad: 0.012, -lr * Pred Grad: -0.017, New P: 1.821
-Original Grad: -0.012, -lr * Pred Grad: -0.041, New P: 1.488
Target params: [1.3344, 1.5708]
iter 0 loss: 0.731
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.008, -lr * Pred Grad: 0.730, New P: 0.258
-Original Grad: -0.005, -lr * Pred Grad: 0.608, New P: 0.611
iter 1 loss: 0.625
Actual params: [0.258 , 0.6113]
-Original Grad: 0.528, -lr * Pred Grad: 2.406, New P: 2.664
-Original Grad: -0.008, -lr * Pred Grad: 0.075, New P: 0.687
iter 2 loss: 0.415
Actual params: [2.6639, 0.6868]
-Original Grad: -0.200, -lr * Pred Grad: -1.256, New P: 1.408
-Original Grad: 0.625, -lr * Pred Grad: 2.160, New P: 2.847
iter 3 loss: 0.708
Actual params: [1.4075, 2.8468]
-Original Grad: -0.068, -lr * Pred Grad: -0.212, New P: 1.195
-Original Grad: -0.816, -lr * Pred Grad: -0.905, New P: 1.942
iter 4 loss: 0.198
Actual params: [1.1952, 1.9418]
-Original Grad: 0.041, -lr * Pred Grad: -0.047, New P: 1.149
-Original Grad: -0.542, -lr * Pred Grad: -0.314, New P: 1.628
iter 5 loss: 0.109
Actual params: [1.1486, 1.6276]
-Original Grad: 0.243, -lr * Pred Grad: 0.453, New P: 1.601
-Original Grad: -0.763, -lr * Pred Grad: -0.592, New P: 1.036
iter 6 loss: 0.138
Actual params: [1.6014, 1.036 ]
-Original Grad: -0.164, -lr * Pred Grad: -0.236, New P: 1.365
-Original Grad: 0.462, -lr * Pred Grad: -0.136, New P: 0.900
iter 7 loss: 0.138
Actual params: [1.3651, 0.9001]
-Original Grad: -0.047, -lr * Pred Grad: -0.198, New P: 1.167
-Original Grad: 0.397, -lr * Pred Grad: 0.772, New P: 1.672
iter 8 loss: 0.120
Actual params: [1.1667, 1.6722]
-Original Grad: 0.310, -lr * Pred Grad: 0.503, New P: 1.670
-Original Grad: -0.665, -lr * Pred Grad: -0.464, New P: 1.208
iter 9 loss: 0.112
Actual params: [1.6701, 1.2081]
-Original Grad: -0.150, -lr * Pred Grad: -0.223, New P: 1.447
-Original Grad: 0.369, -lr * Pred Grad: 0.061, New P: 1.269
iter 10 loss: 0.077
Actual params: [1.4468, 1.2687]
-Original Grad: -0.061, -lr * Pred Grad: -0.221, New P: 1.225
-Original Grad: 0.265, -lr * Pred Grad: 0.691, New P: 1.960
iter 11 loss: 0.201
Actual params: [1.2254, 1.9597]
-Original Grad: 0.022, -lr * Pred Grad: -0.083, New P: 1.143
-Original Grad: -0.354, -lr * Pred Grad: -0.465, New P: 1.495
iter 12 loss: 0.077
Actual params: [1.1427, 1.4949]
-Original Grad: 0.224, -lr * Pred Grad: 0.401, New P: 1.544
-Original Grad: -0.404, -lr * Pred Grad: -0.405, New P: 1.090
iter 13 loss: 0.117
Actual params: [1.5437, 1.0899]
-Original Grad: -0.153, -lr * Pred Grad: -0.235, New P: 1.309
-Original Grad: 0.444, -lr * Pred Grad: 0.261, New P: 1.351
iter 14 loss: 0.067
Actual params: [1.3089, 1.3513]
-Original Grad: 0.069, -lr * Pred Grad: 0.076, New P: 1.385
-Original Grad: 0.171, -lr * Pred Grad: 0.610, New P: 1.961
iter 15 loss: 0.183
Actual params: [1.3853, 1.9609]
-Original Grad: 0.196, -lr * Pred Grad: 0.439, New P: 1.825
-Original Grad: -1.029, -lr * Pred Grad: -0.456, New P: 1.505
iter 16 loss: 0.077
Actual params: [1.8247, 1.5046]
-Original Grad: -0.094, -lr * Pred Grad: -0.078, New P: 1.747
-Original Grad: 0.261, -lr * Pred Grad: -0.238, New P: 1.267
iter 17 loss: 0.111
Actual params: [1.747 , 1.2666]
-Original Grad: -0.193, -lr * Pred Grad: -0.499, New P: 1.248
-Original Grad: 0.541, -lr * Pred Grad: 1.059, New P: 2.325
iter 18 loss: 0.396
Actual params: [1.248 , 2.3253]
-Original Grad: 0.038, -lr * Pred Grad: -0.199, New P: 1.049
-Original Grad: -1.280, -lr * Pred Grad: -0.503, New P: 1.822
iter 19 loss: 0.190
Actual params: [1.0489, 1.8224]
-Original Grad: 0.152, -lr * Pred Grad: 0.174, New P: 1.223
-Original Grad: -0.374, -lr * Pred Grad: -0.460, New P: 1.362
iter 20 loss: 0.071
Actual params: [1.2228, 1.3624]
-Original Grad: 0.033, -lr * Pred Grad: 0.115, New P: 1.338
-Original Grad: 0.180, -lr * Pred Grad: -0.398, New P: 0.964
iter 21 loss: 0.123
Actual params: [1.3381, 0.9643]
-Original Grad: -0.025, -lr * Pred Grad: 0.009, New P: 1.347
-Original Grad: 0.364, -lr * Pred Grad: 0.438, New P: 1.402
iter 22 loss: 0.061
Actual params: [1.347, 1.402]
-Original Grad: 0.032, -lr * Pred Grad: 0.099, New P: 1.446
-Original Grad: 0.137, -lr * Pred Grad: 0.447, New P: 1.849
iter 23 loss: 0.115
Actual params: [1.4465, 1.849 ]
-Original Grad: 0.223, -lr * Pred Grad: 0.578, New P: 2.025
-Original Grad: -0.868, -lr * Pred Grad: -0.441, New P: 1.408
iter 24 loss: 0.117
Actual params: [2.0248, 1.4077]
-Original Grad: -0.098, -lr * Pred Grad: -0.097, New P: 1.928
-Original Grad: 0.304, -lr * Pred Grad: -0.162, New P: 1.246
iter 25 loss: 0.144
Actual params: [1.9282, 1.2459]
-Original Grad: -0.121, -lr * Pred Grad: -0.312, New P: 1.616
-Original Grad: 0.467, -lr * Pred Grad: 1.055, New P: 2.301
iter 26 loss: 0.366
Actual params: [1.6161, 2.3006]
-Original Grad: -0.151, -lr * Pred Grad: -0.568, New P: 1.048
-Original Grad: -2.140, -lr * Pred Grad: -0.515, New P: 1.786
iter 27 loss: 0.181
Actual params: [1.0482, 1.7859]
-Original Grad: 0.207, -lr * Pred Grad: 0.119, New P: 1.167
-Original Grad: -0.698, -lr * Pred Grad: -0.494, New P: 1.292
iter 28 loss: 0.081
Actual params: [1.1668, 1.2919]
-Original Grad: 0.083, -lr * Pred Grad: 0.136, New P: 1.303
-Original Grad: 0.104, -lr * Pred Grad: -0.547, New P: 0.745
iter 29 loss: 0.171
Actual params: [1.3028, 0.7446]
-Original Grad: -0.056, -lr * Pred Grad: -0.062, New P: 1.240
-Original Grad: 0.499, -lr * Pred Grad: 0.294, New P: 1.038
iter 30 loss: 0.107
Actual params: [1.2403, 1.0382]
-Original Grad: -0.002, -lr * Pred Grad: -0.031, New P: 1.210
-Original Grad: 0.329, -lr * Pred Grad: 0.950, New P: 1.989
Target params: [1.3344, 1.5708]
iter 0 loss: 0.563
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.002, -lr * Pred Grad: 0.620, New P: 0.147
-Original Grad: 0.002, -lr * Pred Grad: 0.641, New P: 0.645
iter 1 loss: 0.570
Actual params: [0.1474, 0.6447]
-Original Grad: -0.074, -lr * Pred Grad: -0.442, New P: -0.295
-Original Grad: 0.151, -lr * Pred Grad: 1.004, New P: 1.648
iter 2 loss: 0.503
Actual params: [-0.295 ,  1.6483]
-Original Grad: 0.203, -lr * Pred Grad: 1.121, New P: 0.826
-Original Grad: 0.319, -lr * Pred Grad: 1.639, New P: 3.287
iter 3 loss: 0.393
Actual params: [0.8261, 3.2874]
-Original Grad: 0.015, -lr * Pred Grad: 0.070, New P: 0.896
-Original Grad: -0.641, -lr * Pred Grad: -0.906, New P: 2.381
iter 4 loss: 0.103
Actual params: [0.8957, 2.3813]
-Original Grad: -0.001, -lr * Pred Grad: 0.277, New P: 1.172
-Original Grad: -0.267, -lr * Pred Grad: -0.277, New P: 2.104
iter 5 loss: 0.060
Actual params: [1.1722, 2.1044]
-Original Grad: -0.055, -lr * Pred Grad: -0.092, New P: 1.080
-Original Grad: -0.252, -lr * Pred Grad: -0.548, New P: 1.556
iter 6 loss: 0.048
Actual params: [1.0803, 1.556 ]
-Original Grad: 0.010, -lr * Pred Grad: 0.040, New P: 1.120
-Original Grad: 0.337, -lr * Pred Grad: 0.167, New P: 1.723
iter 7 loss: 0.031
Actual params: [1.1201, 1.7226]
-Original Grad: 0.010, -lr * Pred Grad: 0.037, New P: 1.157
-Original Grad: -0.012, -lr * Pred Grad: -0.089, New P: 1.633
iter 8 loss: 0.037
Actual params: [1.1568, 1.6332]
-Original Grad: -0.001, -lr * Pred Grad: 0.013, New P: 1.170
-Original Grad: 0.091, -lr * Pred Grad: 0.156, New P: 1.789
iter 9 loss: 0.030
Actual params: [1.1696, 1.7891]
-Original Grad: 0.013, -lr * Pred Grad: 0.040, New P: 1.209
-Original Grad: 0.061, -lr * Pred Grad: 0.198, New P: 1.987
iter 10 loss: 0.050
Actual params: [1.2093, 1.9869]
-Original Grad: -0.090, -lr * Pred Grad: -0.244, New P: 0.966
-Original Grad: -0.211, -lr * Pred Grad: -0.330, New P: 1.657
iter 11 loss: 0.039
Actual params: [0.9656, 1.6572]
-Original Grad: 0.064, -lr * Pred Grad: 0.031, New P: 0.997
-Original Grad: 0.232, -lr * Pred Grad: 0.320, New P: 1.977
iter 12 loss: 0.042
Actual params: [0.9967, 1.9773]
-Original Grad: -0.009, -lr * Pred Grad: -0.051, New P: 0.946
-Original Grad: -0.116, -lr * Pred Grad: -0.222, New P: 1.756
iter 13 loss: 0.034
Actual params: [0.9455, 1.7558]
-Original Grad: -0.005, -lr * Pred Grad: -0.054, New P: 0.892
-Original Grad: 0.048, -lr * Pred Grad: 0.026, New P: 1.781
iter 14 loss: 0.034
Actual params: [0.892 , 1.7814]
-Original Grad: -0.022, -lr * Pred Grad: -0.110, New P: 0.782
-Original Grad: -0.008, -lr * Pred Grad: -0.046, New P: 1.735
iter 15 loss: 0.032
Actual params: [0.7822, 1.7355]
-Original Grad: 0.023, -lr * Pred Grad: -0.018, New P: 0.764
-Original Grad: -0.024, -lr * Pred Grad: -0.081, New P: 1.655
iter 16 loss: 0.033
Actual params: [0.7642, 1.6546]
-Original Grad: 0.185, -lr * Pred Grad: 0.427, New P: 1.191
-Original Grad: -0.140, -lr * Pred Grad: -0.282, New P: 1.373
iter 17 loss: 0.089
Actual params: [1.191, 1.373]
-Original Grad: 0.068, -lr * Pred Grad: 0.332, New P: 1.523
-Original Grad: 0.595, -lr * Pred Grad: 1.421, New P: 2.794
iter 18 loss: 0.271
Actual params: [1.5232, 2.7942]
-Original Grad: -0.049, -lr * Pred Grad: 0.058, New P: 1.581
-Original Grad: -0.660, -lr * Pred Grad: -0.667, New P: 2.128
iter 19 loss: 0.102
Actual params: [1.5808, 2.1276]
-Original Grad: -0.088, -lr * Pred Grad: -0.194, New P: 1.387
-Original Grad: -0.287, -lr * Pred Grad: -0.339, New P: 1.789
iter 20 loss: 0.029
Actual params: [1.3867, 1.789 ]
-Original Grad: -0.022, -lr * Pred Grad: -0.155, New P: 1.232
-Original Grad: -0.212, -lr * Pred Grad: -0.539, New P: 1.250
iter 21 loss: 0.113
Actual params: [1.2316, 1.2502]
-Original Grad: 0.343, -lr * Pred Grad: 0.611, New P: 1.843
-Original Grad: 0.420, -lr * Pred Grad: 0.358, New P: 1.609
iter 22 loss: 0.055
Actual params: [1.8425, 1.6085]
-Original Grad: -0.131, -lr * Pred Grad: -0.198, New P: 1.644
-Original Grad: -0.296, -lr * Pred Grad: -0.406, New P: 1.202
iter 23 loss: 0.034
Actual params: [1.6442, 1.2022]
-Original Grad: 0.011, -lr * Pred Grad: 0.007, New P: 1.652
-Original Grad: 0.130, -lr * Pred Grad: -0.066, New P: 1.136
iter 24 loss: 0.037
Actual params: [1.6517, 1.136 ]
-Original Grad: 0.101, -lr * Pred Grad: 0.227, New P: 1.878
-Original Grad: 0.111, -lr * Pred Grad: 0.182, New P: 1.318
iter 25 loss: 0.018
Actual params: [1.8785, 1.3177]
-Original Grad: -0.050, -lr * Pred Grad: -0.032, New P: 1.846
-Original Grad: -0.076, -lr * Pred Grad: -0.149, New P: 1.169
iter 26 loss: 0.021
Actual params: [1.846 , 1.1689]
-Original Grad: 0.003, -lr * Pred Grad: 0.015, New P: 1.861
-Original Grad: 0.168, -lr * Pred Grad: 0.378, New P: 1.547
iter 27 loss: 0.047
Actual params: [1.8607, 1.5468]
-Original Grad: -0.178, -lr * Pred Grad: -0.457, New P: 1.403
-Original Grad: -0.463, -lr * Pred Grad: -0.426, New P: 1.120
iter 28 loss: 0.070
Actual params: [1.4033, 1.1205]
-Original Grad: 0.169, -lr * Pred Grad: 0.123, New P: 1.526
-Original Grad: 0.129, -lr * Pred Grad: -0.158, New P: 0.962
iter 29 loss: 0.118
Actual params: [1.5264, 0.9622]
-Original Grad: 0.041, -lr * Pred Grad: 0.074, New P: 1.600
-Original Grad: 0.834, -lr * Pred Grad: 1.929, New P: 2.891
iter 30 loss: 0.308
Actual params: [1.6003, 2.8912]
-Original Grad: -0.054, -lr * Pred Grad: -0.096, New P: 1.504
-Original Grad: -0.727, -lr * Pred Grad: -0.808, New P: 2.083
Target params: [1.3344, 1.5708]
iter 0 loss: 0.495
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.000, -lr * Pred Grad: 0.648, New P: 0.176
-Original Grad: 0.001, -lr * Pred Grad: 0.637, New P: 0.640
iter 1 loss: 0.435
Actual params: [0.1761, 0.6404]
-Original Grad: 0.264, -lr * Pred Grad: 2.025, New P: 2.201
-Original Grad: 0.281, -lr * Pred Grad: 1.656, New P: 2.297
iter 2 loss: 0.046
Actual params: [2.2007, 2.2965]
-Original Grad: -0.144, -lr * Pred Grad: -0.984, New P: 1.217
-Original Grad: -0.037, -lr * Pred Grad: -0.344, New P: 1.952
iter 3 loss: 0.014
Actual params: [1.2172, 1.9524]
-Original Grad: 0.057, -lr * Pred Grad: 0.243, New P: 1.460
-Original Grad: -0.223, -lr * Pred Grad: -0.389, New P: 1.564
iter 4 loss: 0.044
Actual params: [1.4601, 1.5638]
-Original Grad: -0.171, -lr * Pred Grad: -0.480, New P: 0.980
-Original Grad: -0.045, -lr * Pred Grad: -0.276, New P: 1.288
iter 5 loss: 0.049
Actual params: [0.9798, 1.2877]
-Original Grad: 0.038, -lr * Pred Grad: -0.146, New P: 0.834
-Original Grad: 0.266, -lr * Pred Grad: 0.468, New P: 1.756
iter 6 loss: 0.021
Actual params: [0.8336, 1.7556]
-Original Grad: 0.160, -lr * Pred Grad: 0.249, New P: 1.083
-Original Grad: 0.045, -lr * Pred Grad: 0.253, New P: 2.008
iter 7 loss: 0.030
Actual params: [1.0828, 2.0082]
-Original Grad: 0.066, -lr * Pred Grad: 0.239, New P: 1.322
-Original Grad: -0.330, -lr * Pred Grad: -0.397, New P: 1.611
iter 8 loss: 0.028
Actual params: [1.3218, 1.6114]
-Original Grad: -0.139, -lr * Pred Grad: -0.234, New P: 1.088
-Original Grad: -0.017, -lr * Pred Grad: -0.263, New P: 1.348
iter 9 loss: 0.038
Actual params: [1.0876, 1.3482]
-Original Grad: 0.003, -lr * Pred Grad: -0.093, New P: 0.994
-Original Grad: 0.260, -lr * Pred Grad: 0.445, New P: 1.793
iter 10 loss: 0.011
Actual params: [0.9942, 1.7932]
-Original Grad: 0.064, -lr * Pred Grad: 0.089, New P: 1.083
-Original Grad: -0.126, -lr * Pred Grad: -0.252, New P: 1.541
iter 11 loss: 0.018
Actual params: [1.083 , 1.5414]
-Original Grad: 0.025, -lr * Pred Grad: 0.084, New P: 1.167
-Original Grad: 0.146, -lr * Pred Grad: 0.274, New P: 1.816
iter 12 loss: 0.006
Actual params: [1.167 , 1.8157]
-Original Grad: 0.019, -lr * Pred Grad: 0.096, New P: 1.263
-Original Grad: -0.058, -lr * Pred Grad: -0.101, New P: 1.715
iter 13 loss: 0.015
Actual params: [1.2626, 1.7145]
-Original Grad: -0.072, -lr * Pred Grad: -0.155, New P: 1.108
-Original Grad: 0.240, -lr * Pred Grad: 0.708, New P: 2.423
iter 14 loss: 0.133
Actual params: [1.1079, 2.4228]
-Original Grad: 0.174, -lr * Pred Grad: 0.363, New P: 1.471
-Original Grad: -0.587, -lr * Pred Grad: -0.471, New P: 1.952
iter 15 loss: 0.007
Actual params: [1.4707, 1.9521]
-Original Grad: -0.021, -lr * Pred Grad: 0.045, New P: 1.515
-Original Grad: 0.046, -lr * Pred Grad: -0.261, New P: 1.691
iter 16 loss: 0.042
Actual params: [1.5154, 1.6914]
-Original Grad: -0.140, -lr * Pred Grad: -0.330, New P: 1.186
-Original Grad: 0.310, -lr * Pred Grad: 0.476, New P: 2.167
iter 17 loss: 0.055
Actual params: [1.1857, 2.167 ]
-Original Grad: 0.094, -lr * Pred Grad: 0.071, New P: 1.257
-Original Grad: -0.449, -lr * Pred Grad: -0.434, New P: 1.733
iter 18 loss: 0.012
Actual params: [1.2567, 1.7326]
-Original Grad: -0.072, -lr * Pred Grad: -0.196, New P: 1.061
-Original Grad: 0.231, -lr * Pred Grad: 0.027, New P: 1.759
iter 19 loss: 0.007
Actual params: [1.0605, 1.7592]
-Original Grad: 0.017, -lr * Pred Grad: -0.084, New P: 0.977
-Original Grad: -0.019, -lr * Pred Grad: -0.094, New P: 1.665
iter 20 loss: 0.011
Actual params: [0.9768, 1.6652]
-Original Grad: 0.033, -lr * Pred Grad: 0.006, New P: 0.983
-Original Grad: 0.102, -lr * Pred Grad: 0.201, New P: 1.866
iter 21 loss: 0.017
Actual params: [0.9827, 1.8658]
-Original Grad: 0.085, -lr * Pred Grad: 0.200, New P: 1.182
-Original Grad: -0.221, -lr * Pred Grad: -0.335, New P: 1.531
iter 22 loss: 0.019
Actual params: [1.1824, 1.5311]
-Original Grad: -0.030, -lr * Pred Grad: -0.003, New P: 1.180
-Original Grad: 0.047, -lr * Pred Grad: -0.114, New P: 1.417
iter 23 loss: 0.029
Actual params: [1.1795, 1.4175]
-Original Grad: -0.011, -lr * Pred Grad: -0.020, New P: 1.159
-Original Grad: 0.210, -lr * Pred Grad: 0.511, New P: 1.929
iter 24 loss: 0.014
Actual params: [1.1594, 1.9288]
-Original Grad: 0.046, -lr * Pred Grad: 0.110, New P: 1.269
-Original Grad: -0.236, -lr * Pred Grad: -0.404, New P: 1.525
iter 25 loss: 0.023
Actual params: [1.269 , 1.5246]
-Original Grad: -0.062, -lr * Pred Grad: -0.136, New P: 1.133
-Original Grad: -0.016, -lr * Pred Grad: -0.163, New P: 1.362
iter 26 loss: 0.036
Actual params: [1.1334, 1.3617]
-Original Grad: 0.003, -lr * Pred Grad: -0.068, New P: 1.066
-Original Grad: 0.240, -lr * Pred Grad: 0.498, New P: 1.859
iter 27 loss: 0.012
Actual params: [1.0659, 1.8593]
-Original Grad: 0.049, -lr * Pred Grad: 0.070, New P: 1.136
-Original Grad: -0.178, -lr * Pred Grad: -0.343, New P: 1.517
iter 28 loss: 0.020
Actual params: [1.1362, 1.5165]
-Original Grad: -0.017, -lr * Pred Grad: -0.034, New P: 1.102
-Original Grad: 0.166, -lr * Pred Grad: 0.257, New P: 1.773
iter 29 loss: 0.006
Actual params: [1.1025, 1.7735]
-Original Grad: 0.020, -lr * Pred Grad: 0.033, New P: 1.135
-Original Grad: -0.035, -lr * Pred Grad: -0.052, New P: 1.722
iter 30 loss: 0.007
Actual params: [1.1355, 1.7217]
-Original Grad: -0.026, -lr * Pred Grad: -0.073, New P: 1.063
-Original Grad: 0.144, -lr * Pred Grad: 0.412, New P: 2.134
Target params: [1.3344, 1.5708]
iter 0 loss: 0.118
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.052, -lr * Pred Grad: 0.077, New P: -0.395
-Original Grad: -0.009, -lr * Pred Grad: 0.591, New P: 0.594
iter 1 loss: 0.112
Actual params: [-0.3949,  0.594 ]
-Original Grad: -0.038, -lr * Pred Grad: -0.120, New P: -0.515
-Original Grad: 0.061, -lr * Pred Grad: 0.466, New P: 1.060
iter 2 loss: 0.105
Actual params: [-0.5152,  1.0603]
-Original Grad: 0.001, -lr * Pred Grad: 0.036, New P: -0.479
-Original Grad: 0.002, -lr * Pred Grad: -0.076, New P: 0.984
iter 3 loss: 0.105
Actual params: [-0.4789,  0.984 ]
-Original Grad: 0.001, -lr * Pred Grad: 0.002, New P: -0.476
-Original Grad: 0.005, -lr * Pred Grad: -0.002, New P: 0.982
iter 4 loss: 0.105
Actual params: [-0.4765,  0.9816]
-Original Grad: 0.001, -lr * Pred Grad: -0.001, New P: -0.477
-Original Grad: 0.005, -lr * Pred Grad: -0.026, New P: 0.956
iter 5 loss: 0.106
Actual params: [-0.4775,  0.9557]
-Original Grad: 0.001, -lr * Pred Grad: -0.006, New P: -0.483
-Original Grad: 0.006, -lr * Pred Grad: -0.021, New P: 0.935
iter 6 loss: 0.106
Actual params: [-0.4833,  0.9349]
-Original Grad: 0.000, -lr * Pred Grad: -0.013, New P: -0.496
-Original Grad: 0.006, -lr * Pred Grad: -0.016, New P: 0.919
iter 7 loss: 0.106
Actual params: [-0.4964,  0.9193]
-Original Grad: -0.001, -lr * Pred Grad: -0.021, New P: -0.518
-Original Grad: 0.008, -lr * Pred Grad: -0.006, New P: 0.913
iter 8 loss: 0.106
Actual params: [-0.5178,  0.9134]
-Original Grad: -0.000, -lr * Pred Grad: -0.026, New P: -0.543
-Original Grad: 0.005, -lr * Pred Grad: -0.009, New P: 0.905
iter 9 loss: 0.106
Actual params: [-0.5435,  0.9046]
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: -0.573
-Original Grad: 0.004, -lr * Pred Grad: -0.010, New P: 0.895
iter 10 loss: 0.106
Actual params: [-0.5732,  0.8948]
-Original Grad: -0.001, -lr * Pred Grad: -0.034, New P: -0.608
-Original Grad: 0.005, -lr * Pred Grad: -0.009, New P: 0.885
iter 11 loss: 0.106
Actual params: [-0.6077,  0.8854]
-Original Grad: -0.000, -lr * Pred Grad: -0.036, New P: -0.644
-Original Grad: 0.003, -lr * Pred Grad: -0.013, New P: 0.872
iter 12 loss: 0.106
Actual params: [-0.6441,  0.8719]
-Original Grad: -0.001, -lr * Pred Grad: -0.039, New P: -0.683
-Original Grad: 0.003, -lr * Pred Grad: -0.015, New P: 0.857
iter 13 loss: 0.106
Actual params: [-0.6828,  0.8567]
-Original Grad: -0.000, -lr * Pred Grad: -0.040, New P: -0.723
-Original Grad: 0.002, -lr * Pred Grad: -0.019, New P: 0.838
iter 14 loss: 0.106
Actual params: [-0.723,  0.838]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -0.764
-Original Grad: 0.001, -lr * Pred Grad: -0.022, New P: 0.816
iter 15 loss: 0.106
Actual params: [-0.7644,  0.816 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.807
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 0.793
iter 16 loss: 0.106
Actual params: [-0.8069,  0.7925]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.850
-Original Grad: 0.001, -lr * Pred Grad: -0.025, New P: 0.767
iter 17 loss: 0.106
Actual params: [-0.8501,  0.7671]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.894
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 0.740
iter 18 loss: 0.106
Actual params: [-0.8937,  0.7399]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.938
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.712
iter 19 loss: 0.106
Actual params: [-0.9381,  0.712 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -0.983
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 0.683
iter 20 loss: 0.106
Actual params: [-0.9827,  0.6831]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.027
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.653
iter 21 loss: 0.106
Actual params: [-1.0274,  0.6532]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.072
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.623
iter 22 loss: 0.106
Actual params: [-1.0723,  0.623 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.117
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.592
iter 23 loss: 0.106
Actual params: [-1.1172,  0.5922]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.162
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.561
iter 24 loss: 0.106
Actual params: [-1.162,  0.561]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.207
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.530
iter 25 loss: 0.106
Actual params: [-1.2068,  0.5295]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.252
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.498
iter 26 loss: 0.106
Actual params: [-1.2515,  0.4979]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.296
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.466
iter 27 loss: 0.106
Actual params: [-1.2961,  0.466 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.341
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.434
iter 28 loss: 0.106
Actual params: [-1.3405,  0.434 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.385
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.402
iter 29 loss: 0.106
Actual params: [-1.3848,  0.4018]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.429
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.370
iter 30 loss: 0.106
Actual params: [-1.429 ,  0.3695]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.473
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.337
Target params: [1.3344, 1.5708]
iter 0 loss: 0.583
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad: 0.652, New P: 0.180
-Original Grad: 0.002, -lr * Pred Grad: 0.644, New P: 0.648
iter 1 loss: 0.449
Actual params: [0.18  , 0.6479]
-Original Grad: 0.267, -lr * Pred Grad: 2.032, New P: 2.212
-Original Grad: 0.619, -lr * Pred Grad: 2.517, New P: 3.165
iter 2 loss: 0.655
Actual params: [2.2122, 3.1652]
-Original Grad: -0.015, -lr * Pred Grad: -0.334, New P: 1.878
-Original Grad: -1.495, -lr * Pred Grad: -0.886, New P: 2.279
iter 3 loss: 0.039
Actual params: [1.8781, 2.2789]
-Original Grad: -0.051, -lr * Pred Grad: 0.089, New P: 1.967
-Original Grad: -0.253, -lr * Pred Grad: -0.300, New P: 1.979
iter 4 loss: 0.035
Actual params: [1.9667, 1.9785]
-Original Grad: -0.107, -lr * Pred Grad: -0.349, New P: 1.618
-Original Grad: 0.078, -lr * Pred Grad: -0.464, New P: 1.515
iter 5 loss: 0.063
Actual params: [1.6182, 1.5146]
-Original Grad: -0.069, -lr * Pred Grad: -0.354, New P: 1.264
-Original Grad: 0.404, -lr * Pred Grad: 0.555, New P: 2.070
iter 6 loss: 0.021
Actual params: [1.2638, 2.07  ]
-Original Grad: 0.077, -lr * Pred Grad: -0.001, New P: 1.263
-Original Grad: -0.214, -lr * Pred Grad: -0.391, New P: 1.679
iter 7 loss: 0.018
Actual params: [1.2628, 1.6789]
-Original Grad: -0.032, -lr * Pred Grad: -0.137, New P: 1.126
-Original Grad: 0.176, -lr * Pred Grad: 0.151, New P: 1.830
iter 8 loss: 0.012
Actual params: [1.1259, 1.8302]
-Original Grad: 0.033, -lr * Pred Grad: -0.008, New P: 1.118
-Original Grad: -0.063, -lr * Pred Grad: -0.140, New P: 1.690
iter 9 loss: 0.015
Actual params: [1.118 , 1.6901]
-Original Grad: -0.002, -lr * Pred Grad: -0.037, New P: 1.081
-Original Grad: 0.036, -lr * Pred Grad: 0.017, New P: 1.707
iter 10 loss: 0.014
Actual params: [1.081 , 1.7072]
-Original Grad: 0.002, -lr * Pred Grad: -0.030, New P: 1.051
-Original Grad: 0.094, -lr * Pred Grad: 0.245, New P: 1.952
iter 11 loss: 0.024
Actual params: [1.0507, 1.9522]
-Original Grad: 0.034, -lr * Pred Grad: 0.061, New P: 1.112
-Original Grad: -0.280, -lr * Pred Grad: -0.382, New P: 1.570
iter 12 loss: 0.023
Actual params: [1.1118, 1.5701]
-Original Grad: -0.014, -lr * Pred Grad: -0.027, New P: 1.085
-Original Grad: 0.197, -lr * Pred Grad: 0.136, New P: 1.706
iter 13 loss: 0.014
Actual params: [1.085 , 1.7063]
-Original Grad: -0.004, -lr * Pred Grad: -0.031, New P: 1.054
-Original Grad: 0.119, -lr * Pred Grad: 0.403, New P: 2.110
iter 14 loss: 0.049
Actual params: [1.054 , 2.1096]
-Original Grad: 0.022, -lr * Pred Grad: 0.031, New P: 1.086
-Original Grad: -0.391, -lr * Pred Grad: -0.434, New P: 1.675
iter 15 loss: 0.015
Actual params: [1.0855, 1.6752]
-Original Grad: -0.008, -lr * Pred Grad: -0.026, New P: 1.060
-Original Grad: 0.134, -lr * Pred Grad: -0.084, New P: 1.592
iter 16 loss: 0.021
Actual params: [1.0598, 1.5916]
-Original Grad: -0.010, -lr * Pred Grad: -0.054, New P: 1.006
-Original Grad: 0.125, -lr * Pred Grad: 0.225, New P: 1.817
iter 17 loss: 0.015
Actual params: [1.0058, 1.8166]
-Original Grad: 0.009, -lr * Pred Grad: -0.020, New P: 0.986
-Original Grad: -0.092, -lr * Pred Grad: -0.176, New P: 1.640
iter 18 loss: 0.018
Actual params: [0.9862, 1.6404]
-Original Grad: -0.001, -lr * Pred Grad: -0.035, New P: 0.952
-Original Grad: 0.162, -lr * Pred Grad: 0.343, New P: 1.984
iter 19 loss: 0.029
Actual params: [0.9515, 1.9838]
-Original Grad: -0.001, -lr * Pred Grad: -0.039, New P: 0.912
-Original Grad: -0.215, -lr * Pred Grad: -0.346, New P: 1.637
iter 20 loss: 0.020
Actual params: [0.9124, 1.6374]
-Original Grad: 0.024, -lr * Pred Grad: 0.030, New P: 0.942
-Original Grad: 0.141, -lr * Pred Grad: 0.114, New P: 1.752
iter 21 loss: 0.015
Actual params: [0.942 , 1.7517]
-Original Grad: 0.064, -lr * Pred Grad: 0.173, New P: 1.115
-Original Grad: 0.017, -lr * Pred Grad: 0.050, New P: 1.801
iter 22 loss: 0.012
Actual params: [1.1149, 1.8015]
-Original Grad: 0.020, -lr * Pred Grad: 0.129, New P: 1.244
-Original Grad: -0.012, -lr * Pred Grad: -0.020, New P: 1.781
iter 23 loss: 0.011
Actual params: [1.2438, 1.7813]
-Original Grad: -0.000, -lr * Pred Grad: 0.074, New P: 1.317
-Original Grad: 0.080, -lr * Pred Grad: 0.194, New P: 1.975
iter 24 loss: 0.010
Actual params: [1.3175, 1.9755]
-Original Grad: 0.026, -lr * Pred Grad: 0.119, New P: 1.437
-Original Grad: -0.035, -lr * Pred Grad: -0.059, New P: 1.916
iter 25 loss: 0.009
Actual params: [1.4367, 1.9161]
-Original Grad: -0.020, -lr * Pred Grad: -0.003, New P: 1.433
-Original Grad: 0.066, -lr * Pred Grad: 0.151, New P: 2.067
iter 26 loss: 0.013
Actual params: [1.4333, 2.0675]
-Original Grad: 0.012, -lr * Pred Grad: 0.038, New P: 1.471
-Original Grad: -0.120, -lr * Pred Grad: -0.233, New P: 1.834
iter 27 loss: 0.013
Actual params: [1.4712, 1.8345]
-Original Grad: -0.024, -lr * Pred Grad: -0.059, New P: 1.412
-Original Grad: 0.100, -lr * Pred Grad: 0.113, New P: 1.947
iter 28 loss: 0.009
Actual params: [1.4121, 1.947 ]
-Original Grad: -0.012, -lr * Pred Grad: -0.072, New P: 1.340
-Original Grad: 0.020, -lr * Pred Grad: 0.067, New P: 2.014
iter 29 loss: 0.011
Actual params: [1.3396, 2.0138]
-Original Grad: 0.030, -lr * Pred Grad: 0.025, New P: 1.364
-Original Grad: -0.108, -lr * Pred Grad: -0.209, New P: 1.804
iter 30 loss: 0.012
Actual params: [1.3643, 1.8043]
-Original Grad: -0.020, -lr * Pred Grad: -0.065, New P: 1.299
-Original Grad: 0.069, -lr * Pred Grad: 0.036, New P: 1.841
Target params: [1.3344, 1.5708]
iter 0 loss: 0.267
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.012, -lr * Pred Grad: 0.520, New P: 0.047
-Original Grad: -0.012, -lr * Pred Grad: 0.575, New P: 0.579
iter 1 loss: 0.260
Actual params: [0.0473, 0.5785]
-Original Grad: 0.022, -lr * Pred Grad: 0.565, New P: 0.613
-Original Grad: 0.331, -lr * Pred Grad: 1.830, New P: 2.409
iter 2 loss: 0.081
Actual params: [0.6128, 2.4088]
-Original Grad: -0.038, -lr * Pred Grad: -0.103, New P: 0.510
-Original Grad: -0.389, -lr * Pred Grad: -1.020, New P: 1.389
iter 3 loss: 0.064
Actual params: [0.5096, 1.389 ]
-Original Grad: 0.064, -lr * Pred Grad: 0.456, New P: 0.965
-Original Grad: 0.231, -lr * Pred Grad: 0.240, New P: 1.629
iter 4 loss: 0.018
Actual params: [0.9654, 1.6291]
-Original Grad: -0.044, -lr * Pred Grad: -0.110, New P: 0.855
-Original Grad: 0.133, -lr * Pred Grad: 0.380, New P: 2.009
iter 5 loss: 0.022
Actual params: [0.8551, 2.0087]
-Original Grad: -0.008, -lr * Pred Grad: -0.019, New P: 0.836
-Original Grad: -0.302, -lr * Pred Grad: -0.367, New P: 1.642
iter 6 loss: 0.018
Actual params: [0.8363, 1.6418]
-Original Grad: 0.023, -lr * Pred Grad: 0.062, New P: 0.898
-Original Grad: 0.074, -lr * Pred Grad: -0.102, New P: 1.540
iter 7 loss: 0.024
Actual params: [0.8981, 1.54  ]
-Original Grad: 0.030, -lr * Pred Grad: 0.110, New P: 1.009
-Original Grad: 0.154, -lr * Pred Grad: 0.311, New P: 1.851
iter 8 loss: 0.014
Actual params: [1.0086, 1.8506]
-Original Grad: -0.017, -lr * Pred Grad: -0.002, New P: 1.006
-Original Grad: -0.065, -lr * Pred Grad: -0.111, New P: 1.740
iter 9 loss: 0.014
Actual params: [1.0063, 1.7397]
-Original Grad: -0.002, -lr * Pred Grad: -0.003, New P: 1.004
-Original Grad: 0.007, -lr * Pred Grad: -0.012, New P: 1.728
iter 10 loss: 0.014
Actual params: [1.0036, 1.7277]
-Original Grad: -0.030, -lr * Pred Grad: -0.101, New P: 0.903
-Original Grad: 0.007, -lr * Pred Grad: -0.019, New P: 1.708
iter 11 loss: 0.014
Actual params: [0.9026, 1.7085]
-Original Grad: 0.000, -lr * Pred Grad: -0.066, New P: 0.837
-Original Grad: 0.021, -lr * Pred Grad: 0.031, New P: 1.739
iter 12 loss: 0.014
Actual params: [0.8371, 1.7394]
-Original Grad: 0.000, -lr * Pred Grad: -0.059, New P: 0.778
-Original Grad: 0.078, -lr * Pred Grad: 0.219, New P: 1.958
iter 13 loss: 0.020
Actual params: [0.7783, 1.9583]
-Original Grad: 0.006, -lr * Pred Grad: -0.038, New P: 0.740
-Original Grad: -0.164, -lr * Pred Grad: -0.292, New P: 1.666
iter 14 loss: 0.020
Actual params: [0.7404, 1.666 ]
-Original Grad: 0.040, -lr * Pred Grad: 0.071, New P: 0.812
-Original Grad: 0.072, -lr * Pred Grad: 0.006, New P: 1.672
iter 15 loss: 0.017
Actual params: [0.8117, 1.672 ]
-Original Grad: 0.017, -lr * Pred Grad: 0.064, New P: 0.876
-Original Grad: 0.013, -lr * Pred Grad: -0.001, New P: 1.671
iter 16 loss: 0.016
Actual params: [0.876 , 1.6715]
-Original Grad: -0.016, -lr * Pred Grad: -0.020, New P: 0.856
-Original Grad: 0.088, -lr * Pred Grad: 0.240, New P: 1.911
iter 17 loss: 0.016
Actual params: [0.8562, 1.9114]
-Original Grad: -0.021, -lr * Pred Grad: -0.077, New P: 0.779
-Original Grad: -0.064, -lr * Pred Grad: -0.127, New P: 1.784
iter 18 loss: 0.015
Actual params: [0.7793, 1.7843]
-Original Grad: 0.010, -lr * Pred Grad: -0.025, New P: 0.754
-Original Grad: -0.023, -lr * Pred Grad: -0.088, New P: 1.696
iter 19 loss: 0.018
Actual params: [0.754 , 1.6962]
-Original Grad: 0.025, -lr * Pred Grad: 0.036, New P: 0.790
-Original Grad: 0.047, -lr * Pred Grad: 0.042, New P: 1.739
iter 20 loss: 0.016
Actual params: [0.7895, 1.7387]
-Original Grad: 0.024, -lr * Pred Grad: 0.067, New P: 0.857
-Original Grad: 0.026, -lr * Pred Grad: 0.066, New P: 1.805
iter 21 loss: 0.013
Actual params: [0.8569, 1.805 ]
-Original Grad: 0.016, -lr * Pred Grad: 0.071, New P: 0.928
-Original Grad: -0.077, -lr * Pred Grad: -0.166, New P: 1.639
iter 22 loss: 0.017
Actual params: [0.9275, 1.6392]
-Original Grad: 0.002, -lr * Pred Grad: 0.037, New P: 0.964
-Original Grad: 0.083, -lr * Pred Grad: 0.109, New P: 1.749
iter 23 loss: 0.013
Actual params: [0.9643, 1.7486]
-Original Grad: -0.023, -lr * Pred Grad: -0.051, New P: 0.914
-Original Grad: -0.027, -lr * Pred Grad: -0.060, New P: 1.688
iter 24 loss: 0.015
Actual params: [0.9137, 1.6885]
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.881
-Original Grad: 0.077, -lr * Pred Grad: 0.176, New P: 1.864
iter 25 loss: 0.014
Actual params: [0.8813, 1.8641]
-Original Grad: -0.002, -lr * Pred Grad: -0.040, New P: 0.842
-Original Grad: -0.077, -lr * Pred Grad: -0.159, New P: 1.706
iter 26 loss: 0.015
Actual params: [0.8417, 1.7055]
-Original Grad: 0.004, -lr * Pred Grad: -0.027, New P: 0.815
-Original Grad: 0.047, -lr * Pred Grad: 0.043, New P: 1.749
iter 27 loss: 0.015
Actual params: [0.8145, 1.7489]
-Original Grad: 0.023, -lr * Pred Grad: 0.031, New P: 0.845
-Original Grad: -0.004, -lr * Pred Grad: -0.023, New P: 1.726
iter 28 loss: 0.014
Actual params: [0.8451, 1.7258]
-Original Grad: -0.004, -lr * Pred Grad: -0.012, New P: 0.833
-Original Grad: 0.019, -lr * Pred Grad: 0.027, New P: 1.753
iter 29 loss: 0.014
Actual params: [0.8326, 1.7532]
-Original Grad: 0.031, -lr * Pred Grad: 0.070, New P: 0.903
-Original Grad: -0.031, -lr * Pred Grad: -0.083, New P: 1.670
iter 30 loss: 0.016
Actual params: [0.9026, 1.6698]
-Original Grad: 0.001, -lr * Pred Grad: 0.022, New P: 0.925
-Original Grad: 0.042, -lr * Pred Grad: 0.055, New P: 1.725
Target params: [1.3344, 1.5708]
iter 0 loss: 0.048
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.051, -lr * Pred Grad: 0.083, New P: -0.389
-Original Grad: 0.063, -lr * Pred Grad: 0.929, New P: 0.932
iter 1 loss: 0.043
Actual params: [-0.3889,  0.9321]
-Original Grad: -0.003, -lr * Pred Grad: 0.251, New P: -0.138
-Original Grad: 0.020, -lr * Pred Grad: 0.214, New P: 1.146
iter 2 loss: 0.042
Actual params: [-0.1383,  1.1458]
-Original Grad: -0.016, -lr * Pred Grad: -0.005, New P: -0.144
-Original Grad: 0.074, -lr * Pred Grad: 0.250, New P: 1.396
iter 3 loss: 0.034
Actual params: [-0.1438,  1.3956]
-Original Grad: 0.004, -lr * Pred Grad: 0.060, New P: -0.084
-Original Grad: 0.044, -lr * Pred Grad: 0.144, New P: 1.540
iter 4 loss: 0.030
Actual params: [-0.0839,  1.5398]
-Original Grad: 0.035, -lr * Pred Grad: 0.184, New P: 0.100
-Original Grad: 0.037, -lr * Pred Grad: 0.114, New P: 1.653
iter 5 loss: 0.022
Actual params: [0.0997, 1.6534]
-Original Grad: 0.035, -lr * Pred Grad: 0.215, New P: 0.315
-Original Grad: 0.065, -lr * Pred Grad: 0.201, New P: 1.855
iter 6 loss: 0.010
Actual params: [0.3148, 1.8545]
-Original Grad: 0.029, -lr * Pred Grad: 0.217, New P: 0.532
-Original Grad: 0.051, -lr * Pred Grad: 0.194, New P: 2.049
iter 7 loss: 0.005
Actual params: [0.5319, 2.0487]
-Original Grad: -0.021, -lr * Pred Grad: 0.037, New P: 0.569
-Original Grad: -0.034, -lr * Pred Grad: -0.053, New P: 1.996
iter 8 loss: 0.005
Actual params: [0.5689, 1.9959]
-Original Grad: -0.011, -lr * Pred Grad: 0.003, New P: 0.572
-Original Grad: -0.004, -lr * Pred Grad: -0.027, New P: 1.969
iter 9 loss: 0.005
Actual params: [0.5722, 1.9687]
-Original Grad: -0.013, -lr * Pred Grad: -0.039, New P: 0.533
-Original Grad: 0.027, -lr * Pred Grad: 0.034, New P: 2.003
iter 10 loss: 0.004
Actual params: [0.5328, 2.0027]
-Original Grad: -0.010, -lr * Pred Grad: -0.060, New P: 0.473
-Original Grad: 0.002, -lr * Pred Grad: 0.005, New P: 2.008
iter 11 loss: 0.004
Actual params: [0.4729, 2.0076]
-Original Grad: 0.007, -lr * Pred Grad: -0.028, New P: 0.445
-Original Grad: 0.020, -lr * Pred Grad: 0.039, New P: 2.047
iter 12 loss: 0.004
Actual params: [0.4449, 2.0469]
-Original Grad: 0.004, -lr * Pred Grad: -0.024, New P: 0.421
-Original Grad: -0.003, -lr * Pred Grad: -0.010, New P: 2.037
iter 13 loss: 0.005
Actual params: [0.4207, 2.0366]
-Original Grad: 0.004, -lr * Pred Grad: -0.018, New P: 0.402
-Original Grad: 0.002, -lr * Pred Grad: -0.012, New P: 2.024
iter 14 loss: 0.005
Actual params: [0.4025, 2.0244]
-Original Grad: 0.030, -lr * Pred Grad: 0.058, New P: 0.460
-Original Grad: 0.023, -lr * Pred Grad: 0.036, New P: 2.060
iter 15 loss: 0.004
Actual params: [0.4602, 2.0602]
-Original Grad: -0.003, -lr * Pred Grad: 0.006, New P: 0.466
-Original Grad: -0.003, -lr * Pred Grad: -0.012, New P: 2.048
iter 16 loss: 0.004
Actual params: [0.4659, 2.048 ]
-Original Grad: -0.018, -lr * Pred Grad: -0.057, New P: 0.409
-Original Grad: -0.017, -lr * Pred Grad: -0.059, New P: 1.989
iter 17 loss: 0.005
Actual params: [0.4088, 1.989 ]
-Original Grad: 0.012, -lr * Pred Grad: -0.006, New P: 0.403
-Original Grad: 0.015, -lr * Pred Grad: -0.006, New P: 1.983
iter 18 loss: 0.005
Actual params: [0.4027, 1.9833]
-Original Grad: 0.029, -lr * Pred Grad: 0.057, New P: 0.460
-Original Grad: 0.018, -lr * Pred Grad: 0.024, New P: 2.007
iter 19 loss: 0.004
Actual params: [0.4599, 2.007 ]
-Original Grad: 0.008, -lr * Pred Grad: 0.037, New P: 0.497
-Original Grad: 0.006, -lr * Pred Grad: 0.007, New P: 2.014
iter 20 loss: 0.004
Actual params: [0.4967, 2.0136]
-Original Grad: 0.007, -lr * Pred Grad: 0.032, New P: 0.529
-Original Grad: -0.003, -lr * Pred Grad: -0.023, New P: 1.991
iter 21 loss: 0.004
Actual params: [0.529 , 1.9908]
-Original Grad: 0.005, -lr * Pred Grad: 0.024, New P: 0.553
-Original Grad: 0.003, -lr * Pred Grad: -0.019, New P: 1.972
iter 22 loss: 0.005
Actual params: [0.5526, 1.9719]
-Original Grad: -0.002, -lr * Pred Grad: -0.002, New P: 0.551
-Original Grad: 0.037, -lr * Pred Grad: 0.068, New P: 2.040
iter 23 loss: 0.005
Actual params: [0.5507, 2.0396]
-Original Grad: -0.002, -lr * Pred Grad: -0.016, New P: 0.535
-Original Grad: 0.015, -lr * Pred Grad: 0.043, New P: 2.082
iter 24 loss: 0.005
Actual params: [0.535 , 2.0825]
-Original Grad: -0.007, -lr * Pred Grad: -0.040, New P: 0.495
-Original Grad: -0.014, -lr * Pred Grad: -0.036, New P: 2.047
iter 25 loss: 0.004
Actual params: [0.4952, 2.0468]
-Original Grad: 0.006, -lr * Pred Grad: -0.020, New P: 0.475
-Original Grad: -0.011, -lr * Pred Grad: -0.054, New P: 1.993
iter 26 loss: 0.004
Actual params: [0.4752, 1.9926]
-Original Grad: -0.005, -lr * Pred Grad: -0.045, New P: 0.431
-Original Grad: 0.002, -lr * Pred Grad: -0.037, New P: 1.956
iter 27 loss: 0.005
Actual params: [0.4306, 1.9556]
-Original Grad: 0.039, -lr * Pred Grad: 0.070, New P: 0.501
-Original Grad: 0.038, -lr * Pred Grad: 0.061, New P: 2.016
iter 28 loss: 0.004
Actual params: [0.5008, 2.0164]
-Original Grad: -0.016, -lr * Pred Grad: -0.029, New P: 0.472
-Original Grad: -0.010, -lr * Pred Grad: -0.026, New P: 1.991
iter 29 loss: 0.004
Actual params: [0.4722, 1.9908]
-Original Grad: -0.004, -lr * Pred Grad: -0.031, New P: 0.441
-Original Grad: 0.002, -lr * Pred Grad: -0.019, New P: 1.972
iter 30 loss: 0.005
Actual params: [0.4407, 1.9722]
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.409
-Original Grad: 0.015, -lr * Pred Grad: 0.009, New P: 1.981
Target params: [1.3344, 1.5708]
iter 0 loss: 0.548
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.002, -lr * Pred Grad: 0.625, New P: 0.153
-Original Grad: 0.000, -lr * Pred Grad: 0.634, New P: 0.638
iter 1 loss: 0.536
Actual params: [0.1532, 0.6379]
-Original Grad: -0.057, -lr * Pred Grad: -0.266, New P: -0.113
-Original Grad: 0.650, -lr * Pred Grad: 2.554, New P: 3.192
iter 2 loss: 0.501
Actual params: [-0.1132,  3.1922]
-Original Grad: 0.101, -lr * Pred Grad: 0.772, New P: 0.659
-Original Grad: -0.184, -lr * Pred Grad: -0.697, New P: 2.495
iter 3 loss: 0.154
Actual params: [0.6586, 2.4954]
-Original Grad: 0.146, -lr * Pred Grad: 0.873, New P: 1.532
-Original Grad: -0.582, -lr * Pred Grad: -0.414, New P: 2.081
iter 4 loss: 0.083
Actual params: [1.5317, 2.0814]
-Original Grad: -0.051, -lr * Pred Grad: 0.013, New P: 1.544
-Original Grad: -0.426, -lr * Pred Grad: -0.555, New P: 1.527
iter 5 loss: 0.014
Actual params: [1.5444, 1.5268]
-Original Grad: 0.017, -lr * Pred Grad: 0.242, New P: 1.787
-Original Grad: -0.066, -lr * Pred Grad: -0.497, New P: 1.030
iter 6 loss: 0.058
Actual params: [1.7867, 1.0298]
-Original Grad: 0.053, -lr * Pred Grad: 0.284, New P: 2.071
-Original Grad: 0.593, -lr * Pred Grad: 0.901, New P: 1.931
iter 7 loss: 0.096
Actual params: [2.0708, 1.931 ]
-Original Grad: -0.110, -lr * Pred Grad: -0.221, New P: 1.850
-Original Grad: -0.392, -lr * Pred Grad: -0.538, New P: 1.393
iter 8 loss: 0.018
Actual params: [1.85  , 1.3931]
-Original Grad: -0.095, -lr * Pred Grad: -0.366, New P: 1.484
-Original Grad: -0.067, -lr * Pred Grad: -0.266, New P: 1.127
iter 9 loss: 0.084
Actual params: [1.4845, 1.1272]
-Original Grad: 0.138, -lr * Pred Grad: 0.143, New P: 1.628
-Original Grad: 0.973, -lr * Pred Grad: 1.800, New P: 2.928
iter 10 loss: 0.335
Actual params: [1.6276, 2.9276]
-Original Grad: -0.075, -lr * Pred Grad: -0.178, New P: 1.449
-Original Grad: -0.622, -lr * Pred Grad: -0.798, New P: 2.130
iter 11 loss: 0.089
Actual params: [1.4493, 2.1295]
-Original Grad: -0.066, -lr * Pred Grad: -0.291, New P: 1.159
-Original Grad: -0.496, -lr * Pred Grad: -0.323, New P: 1.807
iter 12 loss: 0.034
Actual params: [1.1586, 1.8066]
-Original Grad: 0.014, -lr * Pred Grad: -0.162, New P: 0.997
-Original Grad: -0.027, -lr * Pred Grad: -0.511, New P: 1.295
iter 13 loss: 0.085
Actual params: [0.9967, 1.2955]
-Original Grad: 0.121, -lr * Pred Grad: 0.164, New P: 1.161
-Original Grad: 0.443, -lr * Pred Grad: 0.543, New P: 1.838
iter 14 loss: 0.035
Actual params: [1.1607, 1.8384]
-Original Grad: 0.002, -lr * Pred Grad: 0.046, New P: 1.207
-Original Grad: -0.037, -lr * Pred Grad: -0.117, New P: 1.721
iter 15 loss: 0.032
Actual params: [1.2069, 1.7212]
-Original Grad: 0.031, -lr * Pred Grad: 0.124, New P: 1.331
-Original Grad: 0.045, -lr * Pred Grad: 0.089, New P: 1.810
iter 16 loss: 0.033
Actual params: [1.3313, 1.8101]
-Original Grad: 0.010, -lr * Pred Grad: 0.086, New P: 1.417
-Original Grad: -0.095, -lr * Pred Grad: -0.199, New P: 1.611
iter 17 loss: 0.021
Actual params: [1.4174, 1.6111]
-Original Grad: 0.025, -lr * Pred Grad: 0.122, New P: 1.539
-Original Grad: -0.074, -lr * Pred Grad: -0.218, New P: 1.393
iter 18 loss: 0.014
Actual params: [1.539 , 1.3932]
-Original Grad: 0.038, -lr * Pred Grad: 0.169, New P: 1.708
-Original Grad: -0.054, -lr * Pred Grad: -0.233, New P: 1.160
iter 19 loss: 0.027
Actual params: [1.7078, 1.1603]
-Original Grad: 0.074, -lr * Pred Grad: 0.292, New P: 2.000
-Original Grad: 0.457, -lr * Pred Grad: 1.184, New P: 2.345
iter 20 loss: 0.192
Actual params: [1.9999, 2.3447]
-Original Grad: -0.145, -lr * Pred Grad: -0.282, New P: 1.718
-Original Grad: -0.604, -lr * Pred Grad: -0.610, New P: 1.734
iter 21 loss: 0.030
Actual params: [1.7181, 1.7342]
-Original Grad: -0.033, -lr * Pred Grad: -0.230, New P: 1.489
-Original Grad: -0.184, -lr * Pred Grad: -0.333, New P: 1.402
iter 22 loss: 0.016
Actual params: [1.4886, 1.4016]
-Original Grad: 0.025, -lr * Pred Grad: -0.088, New P: 1.401
-Original Grad: -0.013, -lr * Pred Grad: -0.399, New P: 1.002
iter 23 loss: 0.183
Actual params: [1.401 , 1.0022]
-Original Grad: 0.131, -lr * Pred Grad: 0.231, New P: 1.632
-Original Grad: 1.115, -lr * Pred Grad: 1.943, New P: 2.945
iter 24 loss: 0.341
Actual params: [1.6323, 2.9454]
-Original Grad: -0.096, -lr * Pred Grad: -0.172, New P: 1.460
-Original Grad: -0.822, -lr * Pred Grad: -0.777, New P: 2.168
iter 25 loss: 0.099
Actual params: [1.4603, 2.1684]
-Original Grad: -0.055, -lr * Pred Grad: -0.245, New P: 1.215
-Original Grad: -0.470, -lr * Pred Grad: -0.337, New P: 1.831
iter 26 loss: 0.034
Actual params: [1.215 , 1.8311]
-Original Grad: 0.008, -lr * Pred Grad: -0.142, New P: 1.073
-Original Grad: -0.103, -lr * Pred Grad: -0.544, New P: 1.287
iter 27 loss: 0.075
Actual params: [1.0731, 1.2866]
-Original Grad: 0.115, -lr * Pred Grad: 0.171, New P: 1.245
-Original Grad: 0.416, -lr * Pred Grad: 0.336, New P: 1.623
iter 28 loss: 0.031
Actual params: [1.2446, 1.6228]
-Original Grad: 0.044, -lr * Pred Grad: 0.161, New P: 1.405
-Original Grad: 0.015, -lr * Pred Grad: -0.013, New P: 1.610
iter 29 loss: 0.021
Actual params: [1.4053, 1.6096]
-Original Grad: 0.080, -lr * Pred Grad: 0.318, New P: 1.723
-Original Grad: -0.042, -lr * Pred Grad: -0.104, New P: 1.505
iter 30 loss: 0.014
Actual params: [1.7231, 1.5053]
-Original Grad: -0.047, -lr * Pred Grad: 0.008, New P: 1.731
-Original Grad: -0.069, -lr * Pred Grad: -0.204, New P: 1.302
Target params: [1.3344, 1.5708]
iter 0 loss: 0.498
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad: 0.639, New P: 0.166
-Original Grad: 0.083, -lr * Pred Grad: 1.017, New P: 1.021
iter 1 loss: 0.263
Actual params: [0.1662, 1.0205]
-Original Grad: 0.298, -lr * Pred Grad: 2.101, New P: 2.268
-Original Grad: 0.331, -lr * Pred Grad: 1.911, New P: 2.931
iter 2 loss: 0.215
Actual params: [2.2675, 2.9312]
-Original Grad: 0.097, -lr * Pred Grad: 0.411, New P: 2.678
-Original Grad: -1.432, -lr * Pred Grad: -0.820, New P: 2.111
iter 3 loss: 0.121
Actual params: [2.6781, 2.111 ]
-Original Grad: -0.192, -lr * Pred Grad: -0.687, New P: 1.991
-Original Grad: 0.071, -lr * Pred Grad: -0.242, New P: 1.869
iter 4 loss: 0.058
Actual params: [1.9915, 1.8686]
-Original Grad: -0.147, -lr * Pred Grad: -0.595, New P: 1.397
-Original Grad: 0.228, -lr * Pred Grad: -0.111, New P: 1.757
iter 5 loss: 0.013
Actual params: [1.3967, 1.7571]
-Original Grad: -0.013, -lr * Pred Grad: -0.322, New P: 1.075
-Original Grad: 0.034, -lr * Pred Grad: -0.013, New P: 1.744
iter 6 loss: 0.009
Actual params: [1.0748, 1.7444]
-Original Grad: -0.013, -lr * Pred Grad: -0.266, New P: 0.809
-Original Grad: -0.078, -lr * Pred Grad: -0.203, New P: 1.541
iter 7 loss: 0.018
Actual params: [0.8092, 1.5415]
-Original Grad: 0.085, -lr * Pred Grad: -0.006, New P: 0.803
-Original Grad: 0.182, -lr * Pred Grad: 0.299, New P: 1.840
iter 8 loss: 0.013
Actual params: [0.8033, 1.8402]
-Original Grad: 0.054, -lr * Pred Grad: 0.077, New P: 0.880
-Original Grad: -0.122, -lr * Pred Grad: -0.233, New P: 1.607
iter 9 loss: 0.009
Actual params: [0.88  , 1.6074]
-Original Grad: 0.051, -lr * Pred Grad: 0.165, New P: 1.045
-Original Grad: 0.094, -lr * Pred Grad: 0.118, New P: 1.725
iter 10 loss: 0.007
Actual params: [1.0453, 1.725 ]
-Original Grad: -0.024, -lr * Pred Grad: 0.014, New P: 1.060
-Original Grad: -0.056, -lr * Pred Grad: -0.127, New P: 1.598
iter 11 loss: 0.007
Actual params: [1.0596, 1.5982]
-Original Grad: -0.008, -lr * Pred Grad: -0.003, New P: 1.057
-Original Grad: 0.093, -lr * Pred Grad: 0.179, New P: 1.777
iter 12 loss: 0.010
Actual params: [1.0571, 1.7773]
-Original Grad: -0.019, -lr * Pred Grad: -0.064, New P: 0.993
-Original Grad: -0.157, -lr * Pred Grad: -0.283, New P: 1.494
iter 13 loss: 0.014
Actual params: [0.9932, 1.4942]
-Original Grad: 0.003, -lr * Pred Grad: -0.037, New P: 0.956
-Original Grad: 0.140, -lr * Pred Grad: 0.164, New P: 1.658
iter 14 loss: 0.006
Actual params: [0.9562, 1.6578]
-Original Grad: 0.008, -lr * Pred Grad: -0.016, New P: 0.940
-Original Grad: 0.030, -lr * Pred Grad: 0.112, New P: 1.770
iter 15 loss: 0.008
Actual params: [0.9397, 1.7703]
-Original Grad: -0.014, -lr * Pred Grad: -0.066, New P: 0.874
-Original Grad: -0.097, -lr * Pred Grad: -0.191, New P: 1.580
iter 16 loss: 0.011
Actual params: [0.8735, 1.5795]
-Original Grad: 0.020, -lr * Pred Grad: 0.004, New P: 0.877
-Original Grad: 0.084, -lr * Pred Grad: 0.097, New P: 1.676
iter 17 loss: 0.007
Actual params: [0.8771, 1.6761]
-Original Grad: 0.039, -lr * Pred Grad: 0.087, New P: 0.964
-Original Grad: 0.026, -lr * Pred Grad: 0.078, New P: 1.754
iter 18 loss: 0.007
Actual params: [0.9639, 1.7538]
-Original Grad: -0.016, -lr * Pred Grad: -0.015, New P: 0.949
-Original Grad: -0.096, -lr * Pred Grad: -0.192, New P: 1.562
iter 19 loss: 0.009
Actual params: [0.9488, 1.5618]
-Original Grad: 0.017, -lr * Pred Grad: 0.038, New P: 0.987
-Original Grad: 0.122, -lr * Pred Grad: 0.193, New P: 1.755
iter 20 loss: 0.008
Actual params: [0.9867, 1.7547]
-Original Grad: -0.009, -lr * Pred Grad: -0.021, New P: 0.965
-Original Grad: -0.100, -lr * Pred Grad: -0.199, New P: 1.556
iter 21 loss: 0.009
Actual params: [0.9654, 1.556 ]
-Original Grad: 0.012, -lr * Pred Grad: 0.014, New P: 0.980
-Original Grad: 0.134, -lr * Pred Grad: 0.248, New P: 1.804
iter 22 loss: 0.010
Actual params: [0.9798, 1.8036]
-Original Grad: -0.018, -lr * Pred Grad: -0.058, New P: 0.922
-Original Grad: -0.140, -lr * Pred Grad: -0.261, New P: 1.543
iter 23 loss: 0.011
Actual params: [0.9217, 1.543 ]
-Original Grad: 0.027, -lr * Pred Grad: 0.034, New P: 0.956
-Original Grad: 0.125, -lr * Pred Grad: 0.167, New P: 1.711
iter 24 loss: 0.006
Actual params: [0.9556, 1.7105]
-Original Grad: -0.002, -lr * Pred Grad: -0.009, New P: 0.947
-Original Grad: -0.037, -lr * Pred Grad: -0.072, New P: 1.638
iter 25 loss: 0.006
Actual params: [0.9469, 1.638 ]
-Original Grad: 0.015, -lr * Pred Grad: 0.027, New P: 0.974
-Original Grad: 0.045, -lr * Pred Grad: 0.084, New P: 1.722
iter 26 loss: 0.006
Actual params: [0.9744, 1.7221]
-Original Grad: 0.012, -lr * Pred Grad: 0.033, New P: 1.008
-Original Grad: -0.049, -lr * Pred Grad: -0.113, New P: 1.609
iter 27 loss: 0.007
Actual params: [1.0078, 1.609 ]
-Original Grad: -0.002, -lr * Pred Grad: 0.002, New P: 1.009
-Original Grad: 0.062, -lr * Pred Grad: 0.096, New P: 1.705
iter 28 loss: 0.006
Actual params: [1.0094, 1.7047]
-Original Grad: -0.009, -lr * Pred Grad: -0.035, New P: 0.974
-Original Grad: -0.009, -lr * Pred Grad: -0.016, New P: 1.688
iter 29 loss: 0.006
Actual params: [0.9744, 1.6884]
-Original Grad: 0.009, -lr * Pred Grad: -0.005, New P: 0.970
-Original Grad: -0.000, -lr * Pred Grad: -0.016, New P: 1.673
iter 30 loss: 0.006
Actual params: [0.9698, 1.6726]
-Original Grad: -0.002, -lr * Pred Grad: -0.025, New P: 0.945
-Original Grad: 0.001, -lr * Pred Grad: -0.025, New P: 1.647
Target params: [1.3344, 1.5708]
iter 0 loss: 0.170
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.112, -lr * Pred Grad: 1.574, New P: 1.101
-Original Grad: -0.031, -lr * Pred Grad: 0.487, New P: 0.491
iter 1 loss: 0.066
Actual params: [1.1012, 0.4906]
-Original Grad: -0.073, -lr * Pred Grad: -0.536, New P: 0.565
-Original Grad: -0.245, -lr * Pred Grad: -0.573, New P: -0.083
iter 2 loss: 0.022
Actual params: [ 0.5651, -0.0828]
-Original Grad: 0.037, -lr * Pred Grad: 0.420, New P: 0.985
-Original Grad: 0.126, -lr * Pred Grad: 0.120, New P: 0.037
iter 3 loss: 0.020
Actual params: [0.9851, 0.0372]
-Original Grad: -0.047, -lr * Pred Grad: -0.170, New P: 0.816
-Original Grad: -0.040, -lr * Pred Grad: -0.151, New P: -0.114
iter 4 loss: 0.017
Actual params: [ 0.8155, -0.1142]
-Original Grad: -0.024, -lr * Pred Grad: -0.106, New P: 0.710
-Original Grad: 0.082, -lr * Pred Grad: 0.126, New P: 0.012
iter 5 loss: 0.009
Actual params: [0.7096, 0.0122]
-Original Grad: 0.003, -lr * Pred Grad: -0.049, New P: 0.661
-Original Grad: 0.047, -lr * Pred Grad: 0.137, New P: 0.149
iter 6 loss: 0.005
Actual params: [0.6605, 0.1493]
-Original Grad: -0.009, -lr * Pred Grad: -0.073, New P: 0.588
-Original Grad: 0.015, -lr * Pred Grad: 0.065, New P: 0.214
iter 7 loss: 0.004
Actual params: [0.588 , 0.2143]
-Original Grad: 0.004, -lr * Pred Grad: -0.041, New P: 0.547
-Original Grad: 0.011, -lr * Pred Grad: 0.034, New P: 0.248
iter 8 loss: 0.005
Actual params: [0.5469, 0.248 ]
-Original Grad: 0.011, -lr * Pred Grad: -0.008, New P: 0.539
-Original Grad: 0.032, -lr * Pred Grad: 0.081, New P: 0.329
iter 9 loss: 0.004
Actual params: [0.5388, 0.3289]
-Original Grad: 0.013, -lr * Pred Grad: 0.017, New P: 0.556
-Original Grad: 0.003, -lr * Pred Grad: 0.015, New P: 0.344
iter 10 loss: 0.004
Actual params: [0.5559, 0.3442]
-Original Grad: 0.003, -lr * Pred Grad: 0.004, New P: 0.559
-Original Grad: -0.013, -lr * Pred Grad: -0.039, New P: 0.305
iter 11 loss: 0.004
Actual params: [0.5595, 0.3053]
-Original Grad: 0.002, -lr * Pred Grad: -0.002, New P: 0.557
-Original Grad: 0.005, -lr * Pred Grad: -0.020, New P: 0.285
iter 12 loss: 0.004
Actual params: [0.5571, 0.2854]
-Original Grad: 0.009, -lr * Pred Grad: 0.014, New P: 0.571
-Original Grad: 0.008, -lr * Pred Grad: -0.007, New P: 0.279
iter 13 loss: 0.004
Actual params: [0.5709, 0.2787]
-Original Grad: 0.008, -lr * Pred Grad: 0.017, New P: 0.588
-Original Grad: 0.027, -lr * Pred Grad: 0.048, New P: 0.327
iter 14 loss: 0.004
Actual params: [0.5883, 0.3268]
-Original Grad: -0.012, -lr * Pred Grad: -0.036, New P: 0.552
-Original Grad: -0.023, -lr * Pred Grad: -0.059, New P: 0.267
iter 15 loss: 0.004
Actual params: [0.5518, 0.2673]
-Original Grad: 0.015, -lr * Pred Grad: 0.012, New P: 0.564
-Original Grad: 0.032, -lr * Pred Grad: 0.043, New P: 0.310
iter 16 loss: 0.004
Actual params: [0.5641, 0.3102]
-Original Grad: -0.003, -lr * Pred Grad: -0.019, New P: 0.545
-Original Grad: -0.007, -lr * Pred Grad: -0.024, New P: 0.286
iter 17 loss: 0.004
Actual params: [0.5447, 0.2858]
-Original Grad: 0.020, -lr * Pred Grad: 0.035, New P: 0.580
-Original Grad: 0.005, -lr * Pred Grad: -0.011, New P: 0.275
iter 18 loss: 0.004
Actual params: [0.58 , 0.275]
-Original Grad: -0.012, -lr * Pred Grad: -0.031, New P: 0.549
-Original Grad: -0.014, -lr * Pred Grad: -0.057, New P: 0.218
iter 19 loss: 0.005
Actual params: [0.5486, 0.2185]
-Original Grad: 0.013, -lr * Pred Grad: 0.012, New P: 0.561
-Original Grad: 0.026, -lr * Pred Grad: 0.024, New P: 0.243
iter 20 loss: 0.005
Actual params: [0.561 , 0.2427]
-Original Grad: 0.014, -lr * Pred Grad: 0.029, New P: 0.590
-Original Grad: 0.005, -lr * Pred Grad: -0.001, New P: 0.241
iter 21 loss: 0.004
Actual params: [0.59  , 0.2414]
-Original Grad: 0.013, -lr * Pred Grad: 0.040, New P: 0.630
-Original Grad: 0.006, -lr * Pred Grad: -0.002, New P: 0.240
iter 22 loss: 0.004
Actual params: [0.6303, 0.2396]
-Original Grad: -0.000, -lr * Pred Grad: 0.010, New P: 0.640
-Original Grad: -0.001, -lr * Pred Grad: -0.023, New P: 0.216
iter 23 loss: 0.004
Actual params: [0.6402, 0.2162]
-Original Grad: -0.009, -lr * Pred Grad: -0.027, New P: 0.613
-Original Grad: -0.010, -lr * Pred Grad: -0.051, New P: 0.165
iter 24 loss: 0.005
Actual params: [0.6129, 0.1651]
-Original Grad: 0.014, -lr * Pred Grad: 0.015, New P: 0.628
-Original Grad: 0.041, -lr * Pred Grad: 0.065, New P: 0.230
iter 25 loss: 0.004
Actual params: [0.628 , 0.2297]
-Original Grad: -0.000, -lr * Pred Grad: -0.010, New P: 0.618
-Original Grad: 0.009, -lr * Pred Grad: 0.024, New P: 0.254
iter 26 loss: 0.004
Actual params: [0.618 , 0.2536]
-Original Grad: 0.006, -lr * Pred Grad: 0.001, New P: 0.619
-Original Grad: -0.003, -lr * Pred Grad: -0.013, New P: 0.241
iter 27 loss: 0.004
Actual params: [0.6193, 0.2407]
-Original Grad: 0.005, -lr * Pred Grad: -0.000, New P: 0.619
-Original Grad: 0.014, -lr * Pred Grad: 0.012, New P: 0.252
iter 28 loss: 0.004
Actual params: [0.6189, 0.2524]
-Original Grad: -0.007, -lr * Pred Grad: -0.033, New P: 0.586
-Original Grad: -0.014, -lr * Pred Grad: -0.049, New P: 0.204
iter 29 loss: 0.005
Actual params: [0.586 , 0.2036]
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.558
-Original Grad: 0.019, -lr * Pred Grad: 0.011, New P: 0.215
iter 30 loss: 0.005
Actual params: [0.5576, 0.215 ]
-Original Grad: 0.046, -lr * Pred Grad: 0.096, New P: 0.654
-Original Grad: 0.050, -lr * Pred Grad: 0.120, New P: 0.335
Target params: [1.3344, 1.5708]
iter 0 loss: 0.514
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.054, -lr * Pred Grad: 1.156, New P: 0.683
-Original Grad: 0.019, -lr * Pred Grad: 0.724, New P: 0.728
iter 1 loss: 0.060
Actual params: [0.6834, 0.7278]
-Original Grad: 0.011, -lr * Pred Grad: 0.380, New P: 1.063
-Original Grad: 0.116, -lr * Pred Grad: 0.805, New P: 1.533
iter 2 loss: 0.234
Actual params: [1.0633, 1.5331]
-Original Grad: 0.075, -lr * Pred Grad: 0.965, New P: 2.029
-Original Grad: -0.389, -lr * Pred Grad: -0.761, New P: 0.772
iter 3 loss: 0.064
Actual params: [2.0286, 0.7719]
-Original Grad: -0.035, -lr * Pred Grad: -0.099, New P: 1.929
-Original Grad: 0.282, -lr * Pred Grad: 0.250, New P: 1.022
iter 4 loss: 0.038
Actual params: [1.9294, 1.0217]
-Original Grad: -0.008, -lr * Pred Grad: 0.100, New P: 2.029
-Original Grad: 0.096, -lr * Pred Grad: 0.277, New P: 1.299
iter 5 loss: 0.029
Actual params: [2.0291, 1.2986]
-Original Grad: -0.053, -lr * Pred Grad: -0.157, New P: 1.872
-Original Grad: 0.135, -lr * Pred Grad: 0.584, New P: 1.883
iter 6 loss: 0.319
Actual params: [1.8717, 1.8831]
-Original Grad: -0.009, -lr * Pred Grad: -0.088, New P: 1.784
-Original Grad: -1.154, -lr * Pred Grad: -0.440, New P: 1.443
iter 7 loss: 0.019
Actual params: [1.7841, 1.4427]
-Original Grad: 0.017, -lr * Pred Grad: -0.015, New P: 1.769
-Original Grad: -0.168, -lr * Pred Grad: -0.466, New P: 0.977
iter 8 loss: 0.038
Actual params: [1.7689, 0.9772]
-Original Grad: -0.010, -lr * Pred Grad: -0.057, New P: 1.712
-Original Grad: 0.102, -lr * Pred Grad: -0.360, New P: 0.618
iter 9 loss: 0.088
Actual params: [1.7119, 0.6176]
-Original Grad: 0.043, -lr * Pred Grad: 0.076, New P: 1.788
-Original Grad: 0.425, -lr * Pred Grad: 0.730, New P: 1.347
iter 10 loss: 0.014
Actual params: [1.7876, 1.3473]
-Original Grad: -0.021, -lr * Pred Grad: -0.041, New P: 1.747
-Original Grad: 0.040, -lr * Pred Grad: 0.162, New P: 1.510
iter 11 loss: 0.036
Actual params: [1.7469, 1.5096]
-Original Grad: 0.075, -lr * Pred Grad: 0.185, New P: 1.932
-Original Grad: -0.442, -lr * Pred Grad: -0.419, New P: 1.090
iter 12 loss: 0.035
Actual params: [1.9322, 1.0902]
-Original Grad: -0.048, -lr * Pred Grad: -0.070, New P: 1.862
-Original Grad: -0.036, -lr * Pred Grad: -0.359, New P: 0.732
iter 13 loss: 0.065
Actual params: [1.862 , 0.7317]
-Original Grad: -0.020, -lr * Pred Grad: -0.086, New P: 1.776
-Original Grad: 0.254, -lr * Pred Grad: 0.264, New P: 0.996
iter 14 loss: 0.037
Actual params: [1.7759, 0.9956]
-Original Grad: -0.005, -lr * Pred Grad: -0.076, New P: 1.700
-Original Grad: 0.114, -lr * Pred Grad: 0.442, New P: 1.437
iter 15 loss: 0.022
Actual params: [1.7002, 1.4371]
-Original Grad: 0.021, -lr * Pred Grad: -0.005, New P: 1.696
-Original Grad: -0.227, -lr * Pred Grad: -0.396, New P: 1.041
iter 16 loss: 0.034
Actual params: [1.6956, 1.0412]
-Original Grad: 0.032, -lr * Pred Grad: 0.063, New P: 1.758
-Original Grad: 0.172, -lr * Pred Grad: 0.181, New P: 1.222
iter 17 loss: 0.021
Actual params: [1.7584, 1.2223]
-Original Grad: -0.006, -lr * Pred Grad: -0.001, New P: 1.758
-Original Grad: 0.122, -lr * Pred Grad: 0.441, New P: 1.664
iter 18 loss: 0.109
Actual params: [1.7579, 1.6636]
-Original Grad: 0.147, -lr * Pred Grad: 0.386, New P: 2.144
-Original Grad: -0.639, -lr * Pred Grad: -0.434, New P: 1.230
iter 19 loss: 0.047
Actual params: [2.1443, 1.2301]
-Original Grad: -0.119, -lr * Pred Grad: -0.198, New P: 1.946
-Original Grad: 0.173, -lr * Pred Grad: -0.194, New P: 1.036
iter 20 loss: 0.038
Actual params: [1.946 , 1.0363]
-Original Grad: -0.023, -lr * Pred Grad: -0.142, New P: 1.804
-Original Grad: 0.092, -lr * Pred Grad: 0.014, New P: 1.051
iter 21 loss: 0.033
Actual params: [1.8042, 1.0507]
-Original Grad: -0.009, -lr * Pred Grad: -0.122, New P: 1.683
-Original Grad: 0.128, -lr * Pred Grad: 0.336, New P: 1.387
iter 22 loss: 0.015
Actual params: [1.6825, 1.3867]
-Original Grad: 0.058, -lr * Pred Grad: 0.058, New P: 1.740
-Original Grad: -0.230, -lr * Pred Grad: -0.358, New P: 1.028
iter 23 loss: 0.034
Actual params: [1.7402, 1.0283]
-Original Grad: -0.028, -lr * Pred Grad: -0.075, New P: 1.665
-Original Grad: 0.056, -lr * Pred Grad: -0.085, New P: 0.943
iter 24 loss: 0.043
Actual params: [1.6649, 0.9431]
-Original Grad: 0.045, -lr * Pred Grad: 0.073, New P: 1.737
-Original Grad: 0.180, -lr * Pred Grad: 0.423, New P: 1.366
iter 25 loss: 0.013
Actual params: [1.7375, 1.3663]
-Original Grad: -0.005, -lr * Pred Grad: -0.001, New P: 1.736
-Original Grad: -0.020, -lr * Pred Grad: 0.013, New P: 1.379
iter 26 loss: 0.014
Actual params: [1.7364, 1.3793]
-Original Grad: 0.007, -lr * Pred Grad: 0.015, New P: 1.751
-Original Grad: -0.083, -lr * Pred Grad: -0.162, New P: 1.217
iter 27 loss: 0.021
Actual params: [1.7513, 1.2169]
-Original Grad: -0.030, -lr * Pred Grad: -0.092, New P: 1.660
-Original Grad: 0.134, -lr * Pred Grad: 0.241, New P: 1.458
iter 28 loss: 0.030
Actual params: [1.6597, 1.4582]
-Original Grad: 0.083, -lr * Pred Grad: 0.168, New P: 1.828
-Original Grad: -0.364, -lr * Pred Grad: -0.403, New P: 1.055
iter 29 loss: 0.033
Actual params: [1.8281, 1.0553]
-Original Grad: -0.005, -lr * Pred Grad: 0.037, New P: 1.865
-Original Grad: 0.077, -lr * Pred Grad: -0.179, New P: 0.876
iter 30 loss: 0.047
Actual params: [1.8649, 0.8758]
-Original Grad: 0.003, -lr * Pred Grad: 0.037, New P: 1.902
-Original Grad: 0.050, -lr * Pred Grad: -0.026, New P: 0.850
Target params: [1.3344, 1.5708]
iter 0 loss: 0.542
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.031, -lr * Pred Grad: 0.955, New P: 0.483
-Original Grad: 0.024, -lr * Pred Grad: 0.747, New P: 0.750
iter 1 loss: 0.225
Actual params: [0.4828, 0.7505]
-Original Grad: 0.215, -lr * Pred Grad: 1.917, New P: 2.400
-Original Grad: 0.229, -lr * Pred Grad: 1.433, New P: 2.183
iter 2 loss: 0.317
Actual params: [2.4001, 2.1833]
-Original Grad: -0.011, -lr * Pred Grad: -0.161, New P: 2.239
-Original Grad: -1.307, -lr * Pred Grad: -0.725, New P: 1.458
iter 3 loss: 0.194
Actual params: [2.2392, 1.4579]
-Original Grad: -0.247, -lr * Pred Grad: -1.041, New P: 1.198
-Original Grad: 0.415, -lr * Pred Grad: -0.010, New P: 1.447
iter 4 loss: 0.061
Actual params: [1.1979, 1.4475]
-Original Grad: 0.079, -lr * Pred Grad: 0.001, New P: 1.199
-Original Grad: -0.068, -lr * Pred Grad: -0.305, New P: 1.142
iter 5 loss: 0.066
Actual params: [1.1986, 1.1424]
-Original Grad: -0.068, -lr * Pred Grad: -0.275, New P: 0.924
-Original Grad: 0.065, -lr * Pred Grad: -0.070, New P: 1.072
iter 6 loss: 0.070
Actual params: [0.9236, 1.0723]
-Original Grad: -0.014, -lr * Pred Grad: -0.231, New P: 0.693
-Original Grad: -0.010, -lr * Pred Grad: -0.109, New P: 0.963
iter 7 loss: 0.110
Actual params: [0.693 , 0.9635]
-Original Grad: 0.129, -lr * Pred Grad: 0.154, New P: 0.847
-Original Grad: 0.120, -lr * Pred Grad: 0.221, New P: 1.185
iter 8 loss: 0.073
Actual params: [0.8465, 1.1849]
-Original Grad: 0.028, -lr * Pred Grad: 0.100, New P: 0.946
-Original Grad: 0.009, -lr * Pred Grad: 0.063, New P: 1.248
iter 9 loss: 0.074
Actual params: [0.9465, 1.2484]
-Original Grad: 0.055, -lr * Pred Grad: 0.222, New P: 1.169
-Original Grad: -0.096, -lr * Pred Grad: -0.192, New P: 1.056
iter 10 loss: 0.066
Actual params: [1.1687, 1.0562]
-Original Grad: -0.029, -lr * Pred Grad: 0.022, New P: 1.191
-Original Grad: 0.015, -lr * Pred Grad: -0.075, New P: 0.982
iter 11 loss: 0.069
Actual params: [1.1907, 0.9816]
-Original Grad: -0.026, -lr * Pred Grad: -0.045, New P: 1.146
-Original Grad: 0.075, -lr * Pred Grad: 0.134, New P: 1.115
iter 12 loss: 0.065
Actual params: [1.1459, 1.1155]
-Original Grad: -0.030, -lr * Pred Grad: -0.113, New P: 1.033
-Original Grad: 0.028, -lr * Pred Grad: 0.104, New P: 1.220
iter 13 loss: 0.066
Actual params: [1.0333, 1.2196]
-Original Grad: -0.032, -lr * Pred Grad: -0.165, New P: 0.868
-Original Grad: -0.063, -lr * Pred Grad: -0.134, New P: 1.086
iter 14 loss: 0.072
Actual params: [0.8682, 1.0857]
-Original Grad: -0.028, -lr * Pred Grad: -0.195, New P: 0.673
-Original Grad: 0.100, -lr * Pred Grad: 0.190, New P: 1.276
iter 15 loss: 0.090
Actual params: [0.6731, 1.2758]
-Original Grad: 0.128, -lr * Pred Grad: 0.187, New P: 0.860
-Original Grad: 0.070, -lr * Pred Grad: 0.268, New P: 1.544
iter 16 loss: 0.117
Actual params: [0.8596, 1.5436]
-Original Grad: 0.018, -lr * Pred Grad: 0.090, New P: 0.949
-Original Grad: -0.219, -lr * Pred Grad: -0.357, New P: 1.186
iter 17 loss: 0.071
Actual params: [0.9495, 1.1862]
-Original Grad: 0.087, -lr * Pred Grad: 0.303, New P: 1.253
-Original Grad: -0.083, -lr * Pred Grad: -0.274, New P: 0.913
iter 18 loss: 0.074
Actual params: [1.2526, 0.9125]
-Original Grad: -0.040, -lr * Pred Grad: 0.014, New P: 1.267
-Original Grad: 0.152, -lr * Pred Grad: 0.126, New P: 1.038
iter 19 loss: 0.069
Actual params: [1.2666, 1.0381]
-Original Grad: -0.026, -lr * Pred Grad: -0.039, New P: 1.228
-Original Grad: 0.046, -lr * Pred Grad: 0.154, New P: 1.192
iter 20 loss: 0.067
Actual params: [1.2275, 1.1916]
-Original Grad: -0.013, -lr * Pred Grad: -0.059, New P: 1.168
-Original Grad: -0.012, -lr * Pred Grad: -0.005, New P: 1.186
iter 21 loss: 0.065
Actual params: [1.1681, 1.1861]
-Original Grad: -0.030, -lr * Pred Grad: -0.129, New P: 1.039
-Original Grad: 0.004, -lr * Pred Grad: -0.001, New P: 1.186
iter 22 loss: 0.065
Actual params: [1.039 , 1.1856]
-Original Grad: 0.011, -lr * Pred Grad: -0.060, New P: 0.979
-Original Grad: 0.009, -lr * Pred Grad: 0.000, New P: 1.186
iter 23 loss: 0.069
Actual params: [0.9792, 1.186 ]
-Original Grad: 0.013, -lr * Pred Grad: -0.025, New P: 0.954
-Original Grad: 0.049, -lr * Pred Grad: 0.115, New P: 1.301
iter 24 loss: 0.078
Actual params: [0.9543, 1.3013]
-Original Grad: 0.066, -lr * Pred Grad: 0.142, New P: 1.096
-Original Grad: -0.135, -lr * Pred Grad: -0.254, New P: 1.047
iter 25 loss: 0.066
Actual params: [1.0965, 1.0471]
-Original Grad: 0.012, -lr * Pred Grad: 0.083, New P: 1.180
-Original Grad: 0.081, -lr * Pred Grad: 0.038, New P: 1.085
iter 26 loss: 0.065
Actual params: [1.1795, 1.0851]
-Original Grad: -0.066, -lr * Pred Grad: -0.145, New P: 1.035
-Original Grad: 0.048, -lr * Pred Grad: 0.116, New P: 1.201
iter 27 loss: 0.066
Actual params: [1.0348, 1.2007]
-Original Grad: 0.021, -lr * Pred Grad: -0.018, New P: 1.017
-Original Grad: 0.009, -lr * Pred Grad: 0.045, New P: 1.246
iter 28 loss: 0.068
Actual params: [1.0167, 1.2461]
-Original Grad: 0.050, -lr * Pred Grad: 0.099, New P: 1.116
-Original Grad: -0.041, -lr * Pred Grad: -0.096, New P: 1.150
iter 29 loss: 0.064
Actual params: [1.1161, 1.15  ]
-Original Grad: -0.001, -lr * Pred Grad: 0.030, New P: 1.146
-Original Grad: 0.045, -lr * Pred Grad: 0.054, New P: 1.204
iter 30 loss: 0.064
Actual params: [1.1464, 1.2039]
-Original Grad: 0.017, -lr * Pred Grad: 0.065, New P: 1.212
-Original Grad: 0.012, -lr * Pred Grad: 0.028, New P: 1.231
Target params: [1.3344, 1.5708]
iter 0 loss: 0.206
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.122, -lr * Pred Grad: 1.628, New P: 1.155
-Original Grad: 0.174, -lr * Pred Grad: 1.390, New P: 1.393
iter 1 loss: 0.021
Actual params: [1.1553, 1.3935]
-Original Grad: -0.001, -lr * Pred Grad: 0.086, New P: 1.242
-Original Grad: 0.005, -lr * Pred Grad: 0.072, New P: 1.466
iter 2 loss: 0.022
Actual params: [1.2417, 1.4656]
-Original Grad: -0.020, -lr * Pred Grad: 0.232, New P: 1.474
-Original Grad: -0.079, -lr * Pred Grad: -0.312, New P: 1.154
iter 3 loss: 0.033
Actual params: [1.4735, 1.154 ]
-Original Grad: 0.028, -lr * Pred Grad: 0.307, New P: 1.781
-Original Grad: 0.221, -lr * Pred Grad: 0.538, New P: 1.692
iter 4 loss: 0.040
Actual params: [1.7808, 1.692 ]
-Original Grad: -0.060, -lr * Pred Grad: -0.154, New P: 1.627
-Original Grad: -0.133, -lr * Pred Grad: -0.280, New P: 1.412
iter 5 loss: 0.023
Actual params: [1.6272, 1.4117]
-Original Grad: -0.005, -lr * Pred Grad: -0.039, New P: 1.588
-Original Grad: -0.043, -lr * Pred Grad: -0.125, New P: 1.287
iter 6 loss: 0.023
Actual params: [1.5878, 1.2872]
-Original Grad: 0.003, -lr * Pred Grad: -0.025, New P: 1.563
-Original Grad: -0.013, -lr * Pred Grad: -0.148, New P: 1.140
iter 7 loss: 0.030
Actual params: [1.5628, 1.1396]
-Original Grad: 0.048, -lr * Pred Grad: 0.114, New P: 1.677
-Original Grad: 0.095, -lr * Pred Grad: 0.154, New P: 1.294
iter 8 loss: 0.022
Actual params: [1.6767, 1.2939]
-Original Grad: -0.001, -lr * Pred Grad: 0.038, New P: 1.714
-Original Grad: -0.025, -lr * Pred Grad: -0.038, New P: 1.256
iter 9 loss: 0.021
Actual params: [1.7143, 1.2562]
-Original Grad: -0.014, -lr * Pred Grad: -0.017, New P: 1.697
-Original Grad: -0.051, -lr * Pred Grad: -0.123, New P: 1.133
iter 10 loss: 0.024
Actual params: [1.6972, 1.1331]
-Original Grad: 0.024, -lr * Pred Grad: 0.057, New P: 1.754
-Original Grad: 0.051, -lr * Pred Grad: 0.040, New P: 1.173
iter 11 loss: 0.022
Actual params: [1.7538, 1.1734]
-Original Grad: -0.001, -lr * Pred Grad: 0.012, New P: 1.766
-Original Grad: -0.014, -lr * Pred Grad: -0.040, New P: 1.133
iter 12 loss: 0.022
Actual params: [1.7657, 1.1332]
-Original Grad: -0.014, -lr * Pred Grad: -0.038, New P: 1.727
-Original Grad: -0.001, -lr * Pred Grad: -0.030, New P: 1.103
iter 13 loss: 0.024
Actual params: [1.7273, 1.103 ]
-Original Grad: 0.031, -lr * Pred Grad: 0.057, New P: 1.784
-Original Grad: 0.052, -lr * Pred Grad: 0.102, New P: 1.205
iter 14 loss: 0.021
Actual params: [1.7842, 1.2049]
-Original Grad: -0.020, -lr * Pred Grad: -0.045, New P: 1.739
-Original Grad: -0.052, -lr * Pred Grad: -0.112, New P: 1.093
iter 15 loss: 0.024
Actual params: [1.7391, 1.0927]
-Original Grad: 0.037, -lr * Pred Grad: 0.075, New P: 1.814
-Original Grad: 0.058, -lr * Pred Grad: 0.087, New P: 1.180
iter 16 loss: 0.022
Actual params: [1.8139, 1.1798]
-Original Grad: -0.025, -lr * Pred Grad: -0.054, New P: 1.760
-Original Grad: -0.045, -lr * Pred Grad: -0.105, New P: 1.074
iter 17 loss: 0.024
Actual params: [1.7603, 1.0745]
-Original Grad: 0.021, -lr * Pred Grad: 0.027, New P: 1.787
-Original Grad: 0.063, -lr * Pred Grad: 0.106, New P: 1.180
iter 18 loss: 0.021
Actual params: [1.7874, 1.1801]
-Original Grad: 0.005, -lr * Pred Grad: 0.009, New P: 1.797
-Original Grad: 0.016, -lr * Pred Grad: 0.053, New P: 1.233
iter 19 loss: 0.022
Actual params: [1.7967, 1.2334]
-Original Grad: -0.060, -lr * Pred Grad: -0.177, New P: 1.620
-Original Grad: -0.084, -lr * Pred Grad: -0.176, New P: 1.057
iter 20 loss: 0.033
Actual params: [1.6196, 1.0572]
-Original Grad: 0.064, -lr * Pred Grad: 0.067, New P: 1.686
-Original Grad: 0.146, -lr * Pred Grad: 0.278, New P: 1.335
iter 21 loss: 0.022
Actual params: [1.6861, 1.3354]
-Original Grad: -0.029, -lr * Pred Grad: -0.081, New P: 1.605
-Original Grad: -0.054, -lr * Pred Grad: -0.099, New P: 1.237
iter 22 loss: 0.023
Actual params: [1.6047, 1.2366]
-Original Grad: 0.022, -lr * Pred Grad: 0.007, New P: 1.612
-Original Grad: 0.029, -lr * Pred Grad: 0.043, New P: 1.280
iter 23 loss: 0.022
Actual params: [1.612 , 1.2796]
-Original Grad: 0.018, -lr * Pred Grad: 0.032, New P: 1.643
-Original Grad: 0.002, -lr * Pred Grad: -0.009, New P: 1.270
iter 24 loss: 0.022
Actual params: [1.6435, 1.2703]
-Original Grad: 0.009, -lr * Pred Grad: 0.027, New P: 1.671
-Original Grad: -0.004, -lr * Pred Grad: -0.026, New P: 1.245
iter 25 loss: 0.022
Actual params: [1.6709, 1.2447]
-Original Grad: -0.003, -lr * Pred Grad: -0.003, New P: 1.668
-Original Grad: -0.013, -lr * Pred Grad: -0.058, New P: 1.187
iter 26 loss: 0.023
Actual params: [1.6677, 1.1867]
-Original Grad: 0.011, -lr * Pred Grad: 0.021, New P: 1.688
-Original Grad: 0.037, -lr * Pred Grad: 0.049, New P: 1.236
iter 27 loss: 0.022
Actual params: [1.6883, 1.2359]
-Original Grad: 0.004, -lr * Pred Grad: 0.007, New P: 1.696
-Original Grad: 0.004, -lr * Pred Grad: 0.007, New P: 1.243
iter 28 loss: 0.022
Actual params: [1.6957, 1.2432]
-Original Grad: -0.004, -lr * Pred Grad: -0.019, New P: 1.677
-Original Grad: -0.025, -lr * Pred Grad: -0.070, New P: 1.173
iter 29 loss: 0.023
Actual params: [1.6772, 1.1732]
-Original Grad: 0.028, -lr * Pred Grad: 0.057, New P: 1.734
-Original Grad: 0.039, -lr * Pred Grad: 0.049, New P: 1.222
iter 30 loss: 0.021
Actual params: [1.7337, 1.2224]
-Original Grad: -0.015, -lr * Pred Grad: -0.030, New P: 1.704
-Original Grad: -0.019, -lr * Pred Grad: -0.051, New P: 1.171
Target params: [1.3344, 1.5708]
iter 0 loss: 0.778
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.002, -lr * Pred Grad: 0.628, New P: 0.155
-Original Grad: 0.001, -lr * Pred Grad: 0.636, New P: 0.640
iter 1 loss: 0.813
Actual params: [0.1552, 0.64  ]
-Original Grad: -0.097, -lr * Pred Grad: -0.648, New P: -0.493
-Original Grad: -0.014, -lr * Pred Grad: 0.042, New P: 0.682
iter 2 loss: 0.778
Actual params: [-0.4931,  0.682 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.087, New P: -0.581
-Original Grad: -0.000, -lr * Pred Grad: -0.036, New P: 0.646
iter 3 loss: 0.778
Actual params: [-0.5806,  0.6455]
-Original Grad: -0.000, -lr * Pred Grad: -0.095, New P: -0.675
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.613
iter 4 loss: 0.778
Actual params: [-0.6751,  0.6129]
-Original Grad: -0.000, -lr * Pred Grad: -0.070, New P: -0.745
-Original Grad: -0.000, -lr * Pred Grad: -0.049, New P: 0.564
iter 5 loss: 0.778
Actual params: [-0.7454,  0.5636]
-Original Grad: -0.000, -lr * Pred Grad: -0.057, New P: -0.802
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: 0.518
iter 6 loss: 0.778
Actual params: [-0.8025,  0.518 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.049, New P: -0.851
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: 0.477
iter 7 loss: 0.778
Actual params: [-0.8512,  0.477 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.895
-Original Grad: -0.000, -lr * Pred Grad: -0.036, New P: 0.441
iter 8 loss: 0.778
Actual params: [-0.8953,  0.4407]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -0.937
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: 0.407
iter 9 loss: 0.778
Actual params: [-0.9371,  0.4072]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -0.978
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.375
iter 10 loss: 0.778
Actual params: [-0.9777,  0.3751]
-Original Grad: -0.000, -lr * Pred Grad: -0.040, New P: -1.018
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.344
iter 11 loss: 0.778
Actual params: [-1.0178,  0.3438]
-Original Grad: -0.000, -lr * Pred Grad: -0.040, New P: -1.058
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.313
iter 12 loss: 0.778
Actual params: [-1.0579,  0.3126]
-Original Grad: -0.000, -lr * Pred Grad: -0.040, New P: -1.098
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.281
iter 13 loss: 0.778
Actual params: [-1.0982,  0.2813]
-Original Grad: -0.000, -lr * Pred Grad: -0.040, New P: -1.139
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.250
iter 14 loss: 0.778
Actual params: [-1.1386,  0.2499]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -1.179
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.218
iter 15 loss: 0.778
Actual params: [-1.1794,  0.2183]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -1.221
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.186
iter 16 loss: 0.778
Actual params: [-1.2206,  0.1864]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -1.262
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.154
iter 17 loss: 0.778
Actual params: [-1.262 ,  0.1544]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -1.304
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.122
iter 18 loss: 0.778
Actual params: [-1.3037,  0.1223]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -1.346
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.090
iter 19 loss: 0.778
Actual params: [-1.3457,  0.09  ]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -1.388
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.058
iter 20 loss: 0.778
Actual params: [-1.3879,  0.0577]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -1.430
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.025
iter 21 loss: 0.778
Actual params: [-1.4303,  0.0252]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.473
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -0.007
iter 22 loss: 0.778
Actual params: [-1.473 , -0.0072]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.516
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: -0.040
iter 23 loss: 0.778
Actual params: [-1.5157, -0.0397]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.559
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.072
iter 24 loss: 0.778
Actual params: [-1.5586, -0.0722]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.602
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.105
iter 25 loss: 0.778
Actual params: [-1.6016, -0.1047]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.645
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.137
iter 26 loss: 0.778
Actual params: [-1.6447, -0.1373]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.688
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.170
iter 27 loss: 0.778
Actual params: [-1.6879, -0.1698]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.731
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.202
iter 28 loss: 0.778
Actual params: [-1.7311, -0.2024]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.774
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.235
iter 29 loss: 0.778
Actual params: [-1.7744, -0.2349]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.818
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.267
iter 30 loss: 0.778
Actual params: [-1.8177, -0.2675]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.861
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.300
Target params: [1.3344, 1.5708]
iter 0 loss: 0.440
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.007, -lr * Pred Grad: 0.712, New P: 0.239
-Original Grad: 0.006, -lr * Pred Grad: 0.662, New P: 0.665
iter 1 loss: 0.101
Actual params: [0.2394, 0.6652]
-Original Grad: 0.341, -lr * Pred Grad: 2.197, New P: 2.437
-Original Grad: 0.462, -lr * Pred Grad: 2.243, New P: 2.909
iter 2 loss: 0.602
Actual params: [2.4365, 2.9085]
-Original Grad: 0.024, -lr * Pred Grad: -0.243, New P: 2.193
-Original Grad: -0.082, -lr * Pred Grad: -0.466, New P: 2.442
iter 3 loss: 0.559
Actual params: [2.1934, 2.4423]
-Original Grad: 0.057, -lr * Pred Grad: 0.759, New P: 2.952
-Original Grad: -0.213, -lr * Pred Grad: -0.356, New P: 2.086
iter 4 loss: 0.468
Actual params: [2.9521, 2.0862]
-Original Grad: -0.050, -lr * Pred Grad: -0.088, New P: 2.864
-Original Grad: -0.304, -lr * Pred Grad: -0.432, New P: 1.654
iter 5 loss: 0.147
Actual params: [2.864 , 1.6543]
-Original Grad: 0.067, -lr * Pred Grad: 0.323, New P: 3.187
-Original Grad: -0.614, -lr * Pred Grad: -0.534, New P: 1.120
iter 6 loss: 0.144
Actual params: [3.187, 1.12 ]
-Original Grad: -0.166, -lr * Pred Grad: -0.339, New P: 2.848
-Original Grad: 0.059, -lr * Pred Grad: -0.444, New P: 0.676
iter 7 loss: 0.099
Actual params: [2.8477, 0.6758]
-Original Grad: -0.147, -lr * Pred Grad: -0.542, New P: 2.306
-Original Grad: 0.214, -lr * Pred Grad: 0.072, New P: 0.748
iter 8 loss: 0.035
Actual params: [2.3058, 0.7477]
-Original Grad: -0.025, -lr * Pred Grad: -0.412, New P: 1.893
-Original Grad: 0.149, -lr * Pred Grad: 0.350, New P: 1.098
iter 9 loss: 0.015
Actual params: [1.8934, 1.0982]
-Original Grad: 0.072, -lr * Pred Grad: -0.139, New P: 1.754
-Original Grad: -0.175, -lr * Pred Grad: -0.320, New P: 0.778
iter 10 loss: 0.016
Actual params: [1.7545, 0.7784]
-Original Grad: -0.012, -lr * Pred Grad: -0.179, New P: 1.576
-Original Grad: 0.073, -lr * Pred Grad: -0.010, New P: 0.768
iter 11 loss: 0.022
Actual params: [1.5759, 0.7679]
-Original Grad: 0.042, -lr * Pred Grad: -0.043, New P: 1.533
-Original Grad: 0.088, -lr * Pred Grad: 0.183, New P: 0.951
iter 12 loss: 0.025
Actual params: [1.5329, 0.9506]
-Original Grad: 0.080, -lr * Pred Grad: 0.144, New P: 1.677
-Original Grad: -0.241, -lr * Pred Grad: -0.352, New P: 0.598
iter 13 loss: 0.030
Actual params: [1.6767, 0.5985]
-Original Grad: 0.038, -lr * Pred Grad: 0.155, New P: 1.832
-Original Grad: 0.138, -lr * Pred Grad: 0.040, New P: 0.638
iter 14 loss: 0.030
Actual params: [1.8316, 0.6381]
-Original Grad: -0.013, -lr * Pred Grad: 0.046, New P: 1.877
-Original Grad: 0.132, -lr * Pred Grad: 0.386, New P: 1.024
iter 15 loss: 0.010
Actual params: [1.8772, 1.0242]
-Original Grad: 0.005, -lr * Pred Grad: 0.047, New P: 1.924
-Original Grad: -0.026, -lr * Pred Grad: -0.015, New P: 1.010
iter 16 loss: 0.008
Actual params: [1.9242, 1.0096]
-Original Grad: 0.023, -lr * Pred Grad: 0.086, New P: 2.010
-Original Grad: -0.066, -lr * Pred Grad: -0.136, New P: 0.873
iter 17 loss: 0.015
Actual params: [2.0098, 0.8734]
-Original Grad: -0.018, -lr * Pred Grad: -0.017, New P: 1.993
-Original Grad: 0.091, -lr * Pred Grad: 0.136, New P: 1.009
iter 18 loss: 0.008
Actual params: [1.9929, 1.0092]
-Original Grad: -0.007, -lr * Pred Grad: -0.030, New P: 1.963
-Original Grad: 0.043, -lr * Pred Grad: 0.153, New P: 1.162
iter 19 loss: 0.017
Actual params: [1.963 , 1.1623]
-Original Grad: 0.054, -lr * Pred Grad: 0.121, New P: 2.084
-Original Grad: -0.137, -lr * Pred Grad: -0.253, New P: 0.909
iter 20 loss: 0.015
Actual params: [2.0836, 0.9088]
-Original Grad: -0.026, -lr * Pred Grad: -0.031, New P: 2.052
-Original Grad: 0.124, -lr * Pred Grad: 0.151, New P: 1.059
iter 21 loss: 0.008
Actual params: [2.0525, 1.0594]
-Original Grad: -0.003, -lr * Pred Grad: -0.024, New P: 2.028
-Original Grad: -0.005, -lr * Pred Grad: 0.008, New P: 1.067
iter 22 loss: 0.008
Actual params: [2.0282, 1.0674]
-Original Grad: -0.010, -lr * Pred Grad: -0.055, New P: 1.973
-Original Grad: 0.002, -lr * Pred Grad: 0.002, New P: 1.069
iter 23 loss: 0.009
Actual params: [1.9731, 1.0694]
-Original Grad: 0.033, -lr * Pred Grad: 0.048, New P: 2.021
-Original Grad: -0.106, -lr * Pred Grad: -0.222, New P: 0.847
iter 24 loss: 0.017
Actual params: [2.0212, 0.847 ]
-Original Grad: -0.018, -lr * Pred Grad: -0.047, New P: 1.974
-Original Grad: 0.105, -lr * Pred Grad: 0.110, New P: 0.957
iter 25 loss: 0.009
Actual params: [1.9742, 0.9567]
-Original Grad: -0.009, -lr * Pred Grad: -0.060, New P: 1.914
-Original Grad: 0.070, -lr * Pred Grad: 0.230, New P: 1.187
iter 26 loss: 0.024
Actual params: [1.9142, 1.1866]
-Original Grad: 0.065, -lr * Pred Grad: 0.129, New P: 2.043
-Original Grad: -0.194, -lr * Pred Grad: -0.326, New P: 0.860
iter 27 loss: 0.017
Actual params: [2.0429, 0.8601]
-Original Grad: -0.025, -lr * Pred Grad: -0.031, New P: 2.012
-Original Grad: 0.105, -lr * Pred Grad: 0.042, New P: 0.902
iter 28 loss: 0.013
Actual params: [2.0124, 0.9023]
-Original Grad: -0.013, -lr * Pred Grad: -0.052, New P: 1.960
-Original Grad: 0.081, -lr * Pred Grad: 0.214, New P: 1.116
iter 29 loss: 0.013
Actual params: [1.9599, 1.1162]
-Original Grad: 0.057, -lr * Pred Grad: 0.117, New P: 2.077
-Original Grad: -0.118, -lr * Pred Grad: -0.224, New P: 0.892
iter 30 loss: 0.016
Actual params: [2.0774, 0.8917]
-Original Grad: -0.022, -lr * Pred Grad: -0.026, New P: 2.051
-Original Grad: 0.105, -lr * Pred Grad: 0.143, New P: 1.035
Target params: [1.3344, 1.5708]
iter 0 loss: 0.586
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.012, -lr * Pred Grad: 0.514, New P: 0.042
-Original Grad: 0.021, -lr * Pred Grad: 0.731, New P: 0.735
iter 1 loss: 0.574
Actual params: [0.0417, 0.7348]
-Original Grad: 0.138, -lr * Pred Grad: 1.504, New P: 1.546
-Original Grad: 0.022, -lr * Pred Grad: 0.242, New P: 0.977
iter 2 loss: 0.039
Actual params: [1.5456, 0.9765]
-Original Grad: 0.088, -lr * Pred Grad: 0.805, New P: 2.351
-Original Grad: 0.258, -lr * Pred Grad: 1.216, New P: 2.193
iter 3 loss: 0.301
Actual params: [2.3508, 2.1928]
-Original Grad: -0.087, -lr * Pred Grad: -0.276, New P: 2.075
-Original Grad: -0.452, -lr * Pred Grad: -0.777, New P: 1.416
iter 4 loss: 0.147
Actual params: [2.0753, 1.4157]
-Original Grad: -0.182, -lr * Pred Grad: -0.754, New P: 1.321
-Original Grad: -0.294, -lr * Pred Grad: -0.314, New P: 1.102
iter 5 loss: 0.051
Actual params: [1.3212, 1.1019]
-Original Grad: 0.110, -lr * Pred Grad: 0.071, New P: 1.392
-Original Grad: 0.499, -lr * Pred Grad: 0.517, New P: 1.619
iter 6 loss: 0.033
Actual params: [1.3919, 1.6187]
-Original Grad: -0.129, -lr * Pred Grad: -0.378, New P: 1.014
-Original Grad: -0.388, -lr * Pred Grad: -0.447, New P: 1.172
iter 7 loss: 0.073
Actual params: [1.0137, 1.1722]
-Original Grad: 0.004, -lr * Pred Grad: -0.261, New P: 0.752
-Original Grad: 0.442, -lr * Pred Grad: 0.514, New P: 1.686
iter 8 loss: 0.080
Actual params: [0.7524, 1.6857]
-Original Grad: 0.361, -lr * Pred Grad: 0.534, New P: 1.287
-Original Grad: -0.325, -lr * Pred Grad: -0.392, New P: 1.294
iter 9 loss: 0.022
Actual params: [1.2869, 1.2936]
-Original Grad: 0.066, -lr * Pred Grad: 0.245, New P: 1.532
-Original Grad: 0.270, -lr * Pred Grad: 0.253, New P: 1.546
iter 10 loss: 0.043
Actual params: [1.5315, 1.5461]
-Original Grad: -0.186, -lr * Pred Grad: -0.287, New P: 1.245
-Original Grad: -0.407, -lr * Pred Grad: -0.384, New P: 1.163
iter 11 loss: 0.051
Actual params: [1.2445, 1.1625]
-Original Grad: 0.061, -lr * Pred Grad: 0.054, New P: 1.299
-Original Grad: 0.391, -lr * Pred Grad: 0.389, New P: 1.551
iter 12 loss: 0.016
Actual params: [1.2986, 1.5511]
-Original Grad: -0.043, -lr * Pred Grad: -0.117, New P: 1.182
-Original Grad: -0.151, -lr * Pred Grad: -0.271, New P: 1.280
iter 13 loss: 0.035
Actual params: [1.1817, 1.2798]
-Original Grad: 0.089, -lr * Pred Grad: 0.151, New P: 1.332
-Original Grad: 0.298, -lr * Pred Grad: 0.634, New P: 1.914
iter 14 loss: 0.090
Actual params: [1.3324, 1.9138]
-Original Grad: -0.277, -lr * Pred Grad: -0.591, New P: 0.741
-Original Grad: -0.758, -lr * Pred Grad: -0.424, New P: 1.489
iter 15 loss: 0.074
Actual params: [0.7409, 1.4894]
-Original Grad: 0.190, -lr * Pred Grad: 0.055, New P: 0.796
-Original Grad: -0.079, -lr * Pred Grad: -0.405, New P: 1.084
iter 16 loss: 0.096
Actual params: [0.7962, 1.0842]
-Original Grad: 0.027, -lr * Pred Grad: -0.012, New P: 0.785
-Original Grad: 0.338, -lr * Pred Grad: 0.220, New P: 1.305
iter 17 loss: 0.066
Actual params: [0.7845, 1.3045]
-Original Grad: 0.058, -lr * Pred Grad: 0.125, New P: 0.910
-Original Grad: 0.228, -lr * Pred Grad: 0.819, New P: 2.124
iter 18 loss: 0.090
Actual params: [0.9099, 2.1238]
-Original Grad: 0.050, -lr * Pred Grad: 0.180, New P: 1.090
-Original Grad: -0.503, -lr * Pred Grad: -0.534, New P: 1.590
iter 19 loss: 0.019
Actual params: [1.09  , 1.5896]
-Original Grad: -0.000, -lr * Pred Grad: 0.094, New P: 1.184
-Original Grad: -0.125, -lr * Pred Grad: -0.337, New P: 1.253
iter 20 loss: 0.040
Actual params: [1.1843, 1.2525]
-Original Grad: 0.096, -lr * Pred Grad: 0.326, New P: 1.510
-Original Grad: 0.345, -lr * Pred Grad: 0.331, New P: 1.583
iter 21 loss: 0.045
Actual params: [1.5101, 1.5832]
-Original Grad: -0.211, -lr * Pred Grad: -0.431, New P: 1.079
-Original Grad: -0.472, -lr * Pred Grad: -0.425, New P: 1.158
iter 22 loss: 0.072
Actual params: [1.0794, 1.1582]
-Original Grad: 0.057, -lr * Pred Grad: -0.082, New P: 0.997
-Original Grad: 0.455, -lr * Pred Grad: 0.419, New P: 1.577
iter 23 loss: 0.027
Actual params: [0.9969, 1.5769]
-Original Grad: 0.085, -lr * Pred Grad: 0.105, New P: 1.102
-Original Grad: -0.074, -lr * Pred Grad: -0.155, New P: 1.422
iter 24 loss: 0.023
Actual params: [1.1017, 1.4222]
-Original Grad: 0.037, -lr * Pred Grad: 0.119, New P: 1.221
-Original Grad: 0.153, -lr * Pred Grad: 0.346, New P: 1.768
iter 25 loss: 0.042
Actual params: [1.2207, 1.7677]
-Original Grad: -0.104, -lr * Pred Grad: -0.218, New P: 1.003
-Original Grad: -0.495, -lr * Pred Grad: -0.404, New P: 1.364
iter 26 loss: 0.038
Actual params: [1.0029, 1.3637]
-Original Grad: 0.051, -lr * Pred Grad: 0.019, New P: 1.022
-Original Grad: 0.189, -lr * Pred Grad: -0.096, New P: 1.267
iter 27 loss: 0.051
Actual params: [1.0216, 1.2673]
-Original Grad: 0.003, -lr * Pred Grad: -0.019, New P: 1.003
-Original Grad: 0.251, -lr * Pred Grad: 0.616, New P: 1.883
iter 28 loss: 0.047
Actual params: [1.0027, 1.883 ]
-Original Grad: -0.049, -lr * Pred Grad: -0.159, New P: 0.844
-Original Grad: -0.302, -lr * Pred Grad: -0.449, New P: 1.434
iter 29 loss: 0.049
Actual params: [0.8438, 1.4337]
-Original Grad: 0.118, -lr * Pred Grad: 0.207, New P: 1.051
-Original Grad: 0.101, -lr * Pred Grad: -0.022, New P: 1.412
iter 30 loss: 0.028
Actual params: [1.0512, 1.4117]
-Original Grad: 0.037, -lr * Pred Grad: 0.155, New P: 1.207
-Original Grad: 0.167, -lr * Pred Grad: 0.400, New P: 1.812
Target params: [1.3344, 1.5708]
iter 0 loss: 0.293
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.025, -lr * Pred Grad: 0.377, New P: -0.096
-Original Grad: 0.052, -lr * Pred Grad: 0.880, New P: 0.883
iter 1 loss: 0.215
Actual params: [-0.0958,  0.8834]
-Original Grad: 0.149, -lr * Pred Grad: 1.533, New P: 1.438
-Original Grad: 0.229, -lr * Pred Grad: 1.443, New P: 2.326
iter 2 loss: 0.040
Actual params: [1.4376, 2.326 ]
-Original Grad: 0.004, -lr * Pred Grad: 0.065, New P: 1.502
-Original Grad: -0.290, -lr * Pred Grad: -0.955, New P: 1.371
iter 3 loss: 0.045
Actual params: [1.5023, 1.371 ]
-Original Grad: -0.050, -lr * Pred Grad: -0.001, New P: 1.501
-Original Grad: 0.186, -lr * Pred Grad: 0.217, New P: 1.588
iter 4 loss: 0.022
Actual params: [1.5012, 1.5876]
-Original Grad: -0.073, -lr * Pred Grad: -0.271, New P: 1.230
-Original Grad: 0.187, -lr * Pred Grad: 0.617, New P: 2.204
iter 5 loss: 0.033
Actual params: [1.2301, 2.2041]
-Original Grad: 0.002, -lr * Pred Grad: -0.084, New P: 1.146
-Original Grad: -0.129, -lr * Pred Grad: -0.285, New P: 1.919
iter 6 loss: 0.014
Actual params: [1.1458, 1.9187]
-Original Grad: -0.034, -lr * Pred Grad: -0.177, New P: 0.969
-Original Grad: -0.113, -lr * Pred Grad: -0.192, New P: 1.727
iter 7 loss: 0.003
Actual params: [0.9687, 1.7266]
-Original Grad: -0.009, -lr * Pred Grad: -0.145, New P: 0.824
-Original Grad: -0.035, -lr * Pred Grad: -0.230, New P: 1.497
iter 8 loss: 0.013
Actual params: [0.824, 1.497]
-Original Grad: 0.069, -lr * Pred Grad: 0.079, New P: 0.903
-Original Grad: 0.157, -lr * Pred Grad: 0.258, New P: 1.755
iter 9 loss: 0.003
Actual params: [0.9035, 1.755 ]
-Original Grad: 0.005, -lr * Pred Grad: 0.021, New P: 0.925
-Original Grad: -0.014, -lr * Pred Grad: 0.010, New P: 1.765
iter 10 loss: 0.004
Actual params: [0.9249, 1.7648]
-Original Grad: -0.004, -lr * Pred Grad: -0.004, New P: 0.921
-Original Grad: -0.037, -lr * Pred Grad: -0.078, New P: 1.686
iter 11 loss: 0.003
Actual params: [0.9214, 1.6864]
-Original Grad: -0.012, -lr * Pred Grad: -0.047, New P: 0.875
-Original Grad: 0.004, -lr * Pred Grad: -0.044, New P: 1.642
iter 12 loss: 0.004
Actual params: [0.8745, 1.6421]
-Original Grad: 0.029, -lr * Pred Grad: 0.046, New P: 0.920
-Original Grad: 0.034, -lr * Pred Grad: 0.047, New P: 1.689
iter 13 loss: 0.003
Actual params: [0.9202, 1.6894]
-Original Grad: -0.012, -lr * Pred Grad: -0.031, New P: 0.889
-Original Grad: 0.005, -lr * Pred Grad: 0.010, New P: 1.700
iter 14 loss: 0.003
Actual params: [0.8895, 1.6995]
-Original Grad: 0.010, -lr * Pred Grad: 0.005, New P: 0.894
-Original Grad: -0.016, -lr * Pred Grad: -0.049, New P: 1.651
iter 15 loss: 0.003
Actual params: [0.8944, 1.6507]
-Original Grad: 0.012, -lr * Pred Grad: 0.021, New P: 0.916
-Original Grad: 0.028, -lr * Pred Grad: 0.031, New P: 1.682
iter 16 loss: 0.003
Actual params: [0.9158, 1.6816]
-Original Grad: 0.009, -lr * Pred Grad: 0.025, New P: 0.940
-Original Grad: -0.000, -lr * Pred Grad: -0.011, New P: 1.671
iter 17 loss: 0.003
Actual params: [0.9404, 1.671 ]
-Original Grad: -0.021, -lr * Pred Grad: -0.060, New P: 0.880
-Original Grad: -0.019, -lr * Pred Grad: -0.065, New P: 1.606
iter 18 loss: 0.005
Actual params: [0.8803, 1.6064]
-Original Grad: 0.015, -lr * Pred Grad: 0.002, New P: 0.882
-Original Grad: 0.082, -lr * Pred Grad: 0.172, New P: 1.778
iter 19 loss: 0.004
Actual params: [0.8821, 1.7783]
-Original Grad: 0.009, -lr * Pred Grad: 0.006, New P: 0.888
-Original Grad: -0.047, -lr * Pred Grad: -0.094, New P: 1.684
iter 20 loss: 0.003
Actual params: [0.8877, 1.6842]
-Original Grad: 0.012, -lr * Pred Grad: 0.023, New P: 0.911
-Original Grad: 0.008, -lr * Pred Grad: -0.019, New P: 1.665
iter 21 loss: 0.003
Actual params: [0.911 , 1.6651]
-Original Grad: -0.006, -lr * Pred Grad: -0.017, New P: 0.894
-Original Grad: 0.017, -lr * Pred Grad: 0.008, New P: 1.673
iter 22 loss: 0.003
Actual params: [0.8939, 1.6732]
-Original Grad: 0.010, -lr * Pred Grad: 0.008, New P: 0.902
-Original Grad: 0.021, -lr * Pred Grad: 0.040, New P: 1.713
iter 23 loss: 0.003
Actual params: [0.9022, 1.7129]
-Original Grad: -0.006, -lr * Pred Grad: -0.028, New P: 0.874
-Original Grad: -0.001, -lr * Pred Grad: -0.008, New P: 1.705
iter 24 loss: 0.003
Actual params: [0.874 , 1.7047]
-Original Grad: 0.026, -lr * Pred Grad: 0.045, New P: 0.919
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: 1.703
iter 25 loss: 0.003
Actual params: [0.9193, 1.7032]
-Original Grad: 0.008, -lr * Pred Grad: 0.029, New P: 0.948
-Original Grad: -0.002, -lr * Pred Grad: -0.025, New P: 1.678
iter 26 loss: 0.003
Actual params: [0.9484, 1.6778]
-Original Grad: -0.012, -lr * Pred Grad: -0.028, New P: 0.921
-Original Grad: 0.013, -lr * Pred Grad: 0.005, New P: 1.683
iter 27 loss: 0.003
Actual params: [0.9205, 1.6825]
-Original Grad: -0.012, -lr * Pred Grad: -0.057, New P: 0.863
-Original Grad: 0.003, -lr * Pred Grad: -0.010, New P: 1.672
iter 28 loss: 0.004
Actual params: [0.8632, 1.6722]
-Original Grad: 0.024, -lr * Pred Grad: 0.023, New P: 0.886
-Original Grad: 0.012, -lr * Pred Grad: 0.008, New P: 1.680
iter 29 loss: 0.003
Actual params: [0.8863, 1.6797]
-Original Grad: 0.021, -lr * Pred Grad: 0.050, New P: 0.936
-Original Grad: 0.013, -lr * Pred Grad: 0.016, New P: 1.696
iter 30 loss: 0.003
Actual params: [0.9358, 1.6959]
-Original Grad: -0.017, -lr * Pred Grad: -0.035, New P: 0.900
-Original Grad: 0.007, -lr * Pred Grad: 0.007, New P: 1.702
Target params: [1.3344, 1.5708]
iter 0 loss: 0.267
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.011, -lr * Pred Grad: 0.524, New P: 0.052
-Original Grad: -0.013, -lr * Pred Grad: 0.571, New P: 0.575
iter 1 loss: 0.260
Actual params: [0.052 , 0.5745]
-Original Grad: 0.024, -lr * Pred Grad: 0.585, New P: 0.637
-Original Grad: 0.263, -lr * Pred Grad: 1.558, New P: 2.133
iter 2 loss: 0.037
Actual params: [0.6365, 2.1327]
-Original Grad: 0.004, -lr * Pred Grad: 0.260, New P: 0.897
-Original Grad: -0.126, -lr * Pred Grad: -0.660, New P: 1.472
iter 3 loss: 0.031
Actual params: [0.8966, 1.4722]
-Original Grad: -0.020, -lr * Pred Grad: 0.033, New P: 0.930
-Original Grad: 0.195, -lr * Pred Grad: 0.403, New P: 1.875
iter 4 loss: 0.014
Actual params: [0.9299, 1.8751]
-Original Grad: 0.006, -lr * Pred Grad: 0.088, New P: 1.018
-Original Grad: -0.111, -lr * Pred Grad: -0.229, New P: 1.646
iter 5 loss: 0.018
Actual params: [1.018 , 1.6463]
-Original Grad: -0.024, -lr * Pred Grad: -0.061, New P: 0.957
-Original Grad: 0.071, -lr * Pred Grad: 0.080, New P: 1.727
iter 6 loss: 0.014
Actual params: [0.9565, 1.7266]
-Original Grad: 0.005, -lr * Pred Grad: -0.006, New P: 0.951
-Original Grad: 0.033, -lr * Pred Grad: 0.077, New P: 1.804
iter 7 loss: 0.013
Actual params: [0.9508, 1.8041]
-Original Grad: -0.020, -lr * Pred Grad: -0.083, New P: 0.868
-Original Grad: -0.028, -lr * Pred Grad: -0.047, New P: 1.757
iter 8 loss: 0.013
Actual params: [0.8678, 1.7573]
-Original Grad: 0.013, -lr * Pred Grad: -0.015, New P: 0.853
-Original Grad: 0.036, -lr * Pred Grad: 0.057, New P: 1.815
iter 9 loss: 0.013
Actual params: [0.853 , 1.8145]
-Original Grad: 0.018, -lr * Pred Grad: 0.024, New P: 0.877
-Original Grad: -0.017, -lr * Pred Grad: -0.039, New P: 1.775
iter 10 loss: 0.013
Actual params: [0.8767, 1.7753]
-Original Grad: 0.001, -lr * Pred Grad: -0.001, New P: 0.875
-Original Grad: 0.004, -lr * Pred Grad: -0.016, New P: 1.759
iter 11 loss: 0.013
Actual params: [0.8754, 1.7591]
-Original Grad: -0.010, -lr * Pred Grad: -0.039, New P: 0.836
-Original Grad: 0.041, -lr * Pred Grad: 0.079, New P: 1.838
iter 12 loss: 0.014
Actual params: [0.8359, 1.8382]
-Original Grad: 0.015, -lr * Pred Grad: 0.011, New P: 0.847
-Original Grad: -0.106, -lr * Pred Grad: -0.211, New P: 1.628
iter 13 loss: 0.018
Actual params: [0.8469, 1.6276]
-Original Grad: 0.019, -lr * Pred Grad: 0.041, New P: 0.888
-Original Grad: 0.107, -lr * Pred Grad: 0.137, New P: 1.765
iter 14 loss: 0.013
Actual params: [0.8881, 1.7648]
-Original Grad: 0.016, -lr * Pred Grad: 0.054, New P: 0.942
-Original Grad: -0.032, -lr * Pred Grad: -0.063, New P: 1.702
iter 15 loss: 0.014
Actual params: [0.9418, 1.7021]
-Original Grad: 0.004, -lr * Pred Grad: 0.031, New P: 0.972
-Original Grad: 0.044, -lr * Pred Grad: 0.084, New P: 1.786
iter 16 loss: 0.013
Actual params: [0.9725, 1.7862]
-Original Grad: 0.010, -lr * Pred Grad: 0.040, New P: 1.013
-Original Grad: -0.045, -lr * Pred Grad: -0.105, New P: 1.682
iter 17 loss: 0.016
Actual params: [1.0127, 1.6816]
-Original Grad: -0.016, -lr * Pred Grad: -0.035, New P: 0.978
-Original Grad: 0.018, -lr * Pred Grad: -0.010, New P: 1.672
iter 18 loss: 0.016
Actual params: [0.9779, 1.6716]
-Original Grad: -0.023, -lr * Pred Grad: -0.093, New P: 0.885
-Original Grad: 0.065, -lr * Pred Grad: 0.147, New P: 1.819
iter 19 loss: 0.013
Actual params: [0.8847, 1.8186]
-Original Grad: 0.005, -lr * Pred Grad: -0.051, New P: 0.834
-Original Grad: -0.070, -lr * Pred Grad: -0.144, New P: 1.674
iter 20 loss: 0.016
Actual params: [0.8336, 1.6742]
-Original Grad: 0.017, -lr * Pred Grad: -0.004, New P: 0.829
-Original Grad: 0.057, -lr * Pred Grad: 0.071, New P: 1.745
iter 21 loss: 0.014
Actual params: [0.8291, 1.7452]
-Original Grad: 0.015, -lr * Pred Grad: 0.017, New P: 0.846
-Original Grad: 0.059, -lr * Pred Grad: 0.169, New P: 1.914
iter 22 loss: 0.016
Actual params: [0.8464, 1.9141]
-Original Grad: 0.015, -lr * Pred Grad: 0.036, New P: 0.882
-Original Grad: -0.121, -lr * Pred Grad: -0.232, New P: 1.682
iter 23 loss: 0.015
Actual params: [0.8824, 1.6822]
-Original Grad: 0.013, -lr * Pred Grad: 0.044, New P: 0.927
-Original Grad: 0.034, -lr * Pred Grad: -0.036, New P: 1.646
iter 24 loss: 0.017
Actual params: [0.9268, 1.6459]
-Original Grad: -0.020, -lr * Pred Grad: -0.045, New P: 0.882
-Original Grad: 0.120, -lr * Pred Grad: 0.296, New P: 1.942
iter 25 loss: 0.017
Actual params: [0.8816, 1.9415]
-Original Grad: -0.006, -lr * Pred Grad: -0.048, New P: 0.833
-Original Grad: -0.163, -lr * Pred Grad: -0.298, New P: 1.644
iter 26 loss: 0.018
Actual params: [0.8333, 1.6438]
-Original Grad: -0.007, -lr * Pred Grad: -0.062, New P: 0.771
-Original Grad: 0.120, -lr * Pred Grad: 0.130, New P: 1.774
iter 27 loss: 0.016
Actual params: [0.771 , 1.7737]
-Original Grad: 0.025, -lr * Pred Grad: 0.018, New P: 0.789
-Original Grad: 0.001, -lr * Pred Grad: 0.012, New P: 1.785
iter 28 loss: 0.015
Actual params: [0.7886, 1.7853]
-Original Grad: 0.022, -lr * Pred Grad: 0.048, New P: 0.836
-Original Grad: -0.000, -lr * Pred Grad: -0.002, New P: 1.783
iter 29 loss: 0.014
Actual params: [0.8362, 1.7833]
-Original Grad: 0.011, -lr * Pred Grad: 0.044, New P: 0.880
-Original Grad: -0.042, -lr * Pred Grad: -0.114, New P: 1.669
iter 30 loss: 0.016
Actual params: [0.8799, 1.6691]
-Original Grad: 0.022, -lr * Pred Grad: 0.079, New P: 0.959
-Original Grad: 0.018, -lr * Pred Grad: -0.025, New P: 1.644
Target params: [1.3344, 1.5708]
iter 0 loss: 0.432
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.016, -lr * Pred Grad: 0.470, New P: -0.002
-Original Grad: -0.005, -lr * Pred Grad: 0.612, New P: 0.616
iter 1 loss: 0.287
Actual params: [-0.0019,  0.6156]
-Original Grad: 0.763, -lr * Pred Grad: 2.453, New P: 2.451
-Original Grad: 0.782, -lr * Pred Grad: 2.679, New P: 3.295
iter 2 loss: 0.411
Actual params: [2.4509, 3.2946]
-Original Grad: -0.005, -lr * Pred Grad: -0.930, New P: 1.521
-Original Grad: -0.333, -lr * Pred Grad: -0.965, New P: 2.330
iter 3 loss: 0.138
Actual params: [1.5208, 2.3295]
-Original Grad: -0.175, -lr * Pred Grad: -0.270, New P: 1.250
-Original Grad: -0.463, -lr * Pred Grad: -0.301, New P: 2.029
iter 4 loss: 0.039
Actual params: [1.2505, 2.0287]
-Original Grad: -0.091, -lr * Pred Grad: -0.321, New P: 0.929
-Original Grad: -0.280, -lr * Pred Grad: -0.549, New P: 1.479
iter 5 loss: 0.013
Actual params: [0.9295, 1.4794]
-Original Grad: -0.040, -lr * Pred Grad: -0.270, New P: 0.659
-Original Grad: 0.090, -lr * Pred Grad: -0.264, New P: 1.215
iter 6 loss: 0.024
Actual params: [0.6591, 1.2149]
-Original Grad: -0.029, -lr * Pred Grad: -0.254, New P: 0.405
-Original Grad: 0.183, -lr * Pred Grad: 0.224, New P: 1.439
iter 7 loss: 0.051
Actual params: [0.4047, 1.4391]
-Original Grad: 0.240, -lr * Pred Grad: 0.316, New P: 0.721
-Original Grad: -0.221, -lr * Pred Grad: -0.351, New P: 1.088
iter 8 loss: 0.050
Actual params: [0.7209, 1.0883]
-Original Grad: -0.059, -lr * Pred Grad: -0.063, New P: 0.658
-Original Grad: 0.357, -lr * Pred Grad: 0.588, New P: 1.676
iter 9 loss: 0.016
Actual params: [0.6582, 1.6765]
-Original Grad: 0.147, -lr * Pred Grad: 0.359, New P: 1.017
-Original Grad: -0.116, -lr * Pred Grad: -0.240, New P: 1.436
iter 10 loss: 0.023
Actual params: [1.017 , 1.4364]
-Original Grad: -0.053, -lr * Pred Grad: -0.017, New P: 1.000
-Original Grad: 0.173, -lr * Pred Grad: 0.391, New P: 1.827
iter 11 loss: 0.012
Actual params: [0.9997, 1.8275]
-Original Grad: -0.030, -lr * Pred Grad: -0.045, New P: 0.955
-Original Grad: -0.084, -lr * Pred Grad: -0.150, New P: 1.678
iter 12 loss: 0.006
Actual params: [0.9548, 1.6778]
-Original Grad: -0.018, -lr * Pred Grad: -0.074, New P: 0.881
-Original Grad: -0.014, -lr * Pred Grad: -0.058, New P: 1.620
iter 13 loss: 0.005
Actual params: [0.881 , 1.6199]
-Original Grad: -0.013, -lr * Pred Grad: -0.088, New P: 0.793
-Original Grad: -0.013, -lr * Pred Grad: -0.090, New P: 1.530
iter 14 loss: 0.004
Actual params: [0.7929, 1.5296]
-Original Grad: -0.014, -lr * Pred Grad: -0.107, New P: 0.686
-Original Grad: 0.032, -lr * Pred Grad: 0.025, New P: 1.555
iter 15 loss: 0.009
Actual params: [0.6863, 1.5551]
-Original Grad: 0.053, -lr * Pred Grad: 0.058, New P: 0.744
-Original Grad: -0.019, -lr * Pred Grad: -0.059, New P: 1.496
iter 16 loss: 0.006
Actual params: [0.7445, 1.4963]
-Original Grad: 0.011, -lr * Pred Grad: 0.031, New P: 0.775
-Original Grad: 0.018, -lr * Pred Grad: 0.006, New P: 1.502
iter 17 loss: 0.005
Actual params: [0.7751, 1.5022]
-Original Grad: -0.014, -lr * Pred Grad: -0.032, New P: 0.744
-Original Grad: 0.050, -lr * Pred Grad: 0.115, New P: 1.618
iter 18 loss: 0.006
Actual params: [0.7435, 1.6176]
-Original Grad: 0.034, -lr * Pred Grad: 0.070, New P: 0.814
-Original Grad: -0.034, -lr * Pred Grad: -0.070, New P: 1.547
iter 19 loss: 0.004
Actual params: [0.8137, 1.5473]
-Original Grad: -0.005, -lr * Pred Grad: 0.003, New P: 0.817
-Original Grad: 0.006, -lr * Pred Grad: -0.020, New P: 1.527
iter 20 loss: 0.005
Actual params: [0.8167, 1.5269]
-Original Grad: -0.026, -lr * Pred Grad: -0.078, New P: 0.738
-Original Grad: 0.055, -lr * Pred Grad: 0.113, New P: 1.640
iter 21 loss: 0.007
Actual params: [0.7384, 1.6396]
-Original Grad: 0.041, -lr * Pred Grad: 0.062, New P: 0.801
-Original Grad: -0.054, -lr * Pred Grad: -0.116, New P: 1.524
iter 22 loss: 0.005
Actual params: [0.8007, 1.5241]
-Original Grad: -0.008, -lr * Pred Grad: -0.015, New P: 0.786
-Original Grad: 0.033, -lr * Pred Grad: 0.022, New P: 1.546
iter 23 loss: 0.004
Actual params: [0.7855, 1.5464]
-Original Grad: -0.014, -lr * Pred Grad: -0.054, New P: 0.732
-Original Grad: 0.013, -lr * Pred Grad: 0.014, New P: 1.560
iter 24 loss: 0.006
Actual params: [0.732 , 1.5601]
-Original Grad: 0.012, -lr * Pred Grad: -0.009, New P: 0.723
-Original Grad: -0.003, -lr * Pred Grad: -0.018, New P: 1.542
iter 25 loss: 0.006
Actual params: [0.7232, 1.5422]
-Original Grad: 0.037, -lr * Pred Grad: 0.079, New P: 0.803
-Original Grad: -0.007, -lr * Pred Grad: -0.041, New P: 1.502
iter 26 loss: 0.005
Actual params: [0.8026, 1.5016]
-Original Grad: -0.026, -lr * Pred Grad: -0.049, New P: 0.753
-Original Grad: 0.063, -lr * Pred Grad: 0.131, New P: 1.633
iter 27 loss: 0.006
Actual params: [0.7531, 1.6329]
-Original Grad: 0.023, -lr * Pred Grad: 0.035, New P: 0.789
-Original Grad: -0.074, -lr * Pred Grad: -0.156, New P: 1.477
iter 28 loss: 0.006
Actual params: [0.7886, 1.4774]
-Original Grad: -0.019, -lr * Pred Grad: -0.055, New P: 0.734
-Original Grad: 0.082, -lr * Pred Grad: 0.128, New P: 1.605
iter 29 loss: 0.006
Actual params: [0.7336, 1.605 ]
-Original Grad: 0.039, -lr * Pred Grad: 0.071, New P: 0.804
-Original Grad: -0.033, -lr * Pred Grad: -0.071, New P: 1.534
iter 30 loss: 0.004
Actual params: [0.8045, 1.5345]
-Original Grad: -0.018, -lr * Pred Grad: -0.037, New P: 0.768
-Original Grad: 0.036, -lr * Pred Grad: 0.056, New P: 1.591
Target params: [1.3344, 1.5708]
iter 0 loss: 0.292
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.004, -lr * Pred Grad: 0.682, New P: 0.210
-Original Grad: 0.021, -lr * Pred Grad: 0.731, New P: 0.735
iter 1 loss: 0.198
Actual params: [0.2096, 0.7349]
-Original Grad: 0.195, -lr * Pred Grad: 1.808, New P: 2.017
-Original Grad: 0.231, -lr * Pred Grad: 1.443, New P: 2.177
iter 2 loss: 0.037
Actual params: [2.0174, 2.1775]
-Original Grad: -0.024, -lr * Pred Grad: -0.232, New P: 1.785
-Original Grad: 0.043, -lr * Pred Grad: 0.021, New P: 2.198
iter 3 loss: 0.031
Actual params: [1.7851, 2.1982]
-Original Grad: -0.011, -lr * Pred Grad: 0.262, New P: 2.047
-Original Grad: 0.004, -lr * Pred Grad: -0.066, New P: 2.132
iter 4 loss: 0.040
Actual params: [2.0472, 2.132 ]
-Original Grad: -0.045, -lr * Pred Grad: -0.100, New P: 1.948
-Original Grad: 0.116, -lr * Pred Grad: 0.319, New P: 2.451
iter 5 loss: 0.041
Actual params: [1.9476, 2.4511]
-Original Grad: 0.045, -lr * Pred Grad: 0.166, New P: 2.113
-Original Grad: -0.223, -lr * Pred Grad: -0.326, New P: 2.125
iter 6 loss: 0.044
Actual params: [2.1134, 2.1255]
-Original Grad: -0.046, -lr * Pred Grad: -0.087, New P: 2.026
-Original Grad: 0.134, -lr * Pred Grad: 0.067, New P: 2.193
iter 7 loss: 0.037
Actual params: [2.0259, 2.1928]
-Original Grad: -0.029, -lr * Pred Grad: -0.114, New P: 1.912
-Original Grad: 0.093, -lr * Pred Grad: 0.262, New P: 2.455
iter 8 loss: 0.043
Actual params: [1.9116, 2.4546]
-Original Grad: 0.084, -lr * Pred Grad: 0.155, New P: 2.067
-Original Grad: -0.217, -lr * Pred Grad: -0.319, New P: 2.136
iter 9 loss: 0.041
Actual params: [2.0667, 2.1355]
-Original Grad: -0.042, -lr * Pred Grad: -0.068, New P: 1.999
-Original Grad: 0.119, -lr * Pred Grad: 0.054, New P: 2.189
iter 10 loss: 0.036
Actual params: [1.9986, 2.1893]
-Original Grad: -0.021, -lr * Pred Grad: -0.091, New P: 1.908
-Original Grad: 0.058, -lr * Pred Grad: 0.150, New P: 2.339
iter 11 loss: 0.035
Actual params: [1.9076, 2.3394]
-Original Grad: 0.009, -lr * Pred Grad: -0.039, New P: 1.869
-Original Grad: -0.063, -lr * Pred Grad: -0.118, New P: 2.222
iter 12 loss: 0.033
Actual params: [1.869 , 2.2216]
-Original Grad: -0.021, -lr * Pred Grad: -0.102, New P: 1.767
-Original Grad: 0.032, -lr * Pred Grad: 0.021, New P: 2.243
iter 13 loss: 0.032
Actual params: [1.7669, 2.243 ]
-Original Grad: -0.004, -lr * Pred Grad: -0.087, New P: 1.680
-Original Grad: -0.042, -lr * Pred Grad: -0.111, New P: 2.132
iter 14 loss: 0.030
Actual params: [1.6798, 2.1318]
-Original Grad: -0.020, -lr * Pred Grad: -0.130, New P: 1.550
-Original Grad: 0.015, -lr * Pred Grad: -0.024, New P: 2.108
iter 15 loss: 0.031
Actual params: [1.5498, 2.1082]
-Original Grad: 0.027, -lr * Pred Grad: -0.024, New P: 1.526
-Original Grad: -0.068, -lr * Pred Grad: -0.172, New P: 1.936
iter 16 loss: 0.031
Actual params: [1.5262, 1.9365]
-Original Grad: -0.030, -lr * Pred Grad: -0.127, New P: 1.399
-Original Grad: 0.058, -lr * Pred Grad: 0.037, New P: 1.973
iter 17 loss: 0.031
Actual params: [1.3988, 1.9734]
-Original Grad: 0.017, -lr * Pred Grad: -0.044, New P: 1.355
-Original Grad: -0.009, -lr * Pred Grad: -0.035, New P: 1.938
iter 18 loss: 0.031
Actual params: [1.3547, 1.9381]
-Original Grad: 0.015, -lr * Pred Grad: -0.013, New P: 1.342
-Original Grad: -0.011, -lr * Pred Grad: -0.053, New P: 1.885
iter 19 loss: 0.030
Actual params: [1.3417, 1.8848]
-Original Grad: 0.020, -lr * Pred Grad: 0.027, New P: 1.369
-Original Grad: -0.052, -lr * Pred Grad: -0.150, New P: 1.735
iter 20 loss: 0.025
Actual params: [1.369 , 1.7351]
-Original Grad: 0.005, -lr * Pred Grad: 0.013, New P: 1.382
-Original Grad: -0.036, -lr * Pred Grad: -0.155, New P: 1.581
iter 21 loss: 0.022
Actual params: [1.3824, 1.5805]
-Original Grad: 0.016, -lr * Pred Grad: 0.044, New P: 1.426
-Original Grad: -0.029, -lr * Pred Grad: -0.150, New P: 1.431
iter 22 loss: 0.021
Actual params: [1.4263, 1.4308]
-Original Grad: 0.004, -lr * Pred Grad: 0.023, New P: 1.449
-Original Grad: 0.003, -lr * Pred Grad: -0.083, New P: 1.348
iter 23 loss: 0.022
Actual params: [1.4495, 1.3478]
-Original Grad: -0.006, -lr * Pred Grad: -0.014, New P: 1.436
-Original Grad: 0.047, -lr * Pred Grad: 0.056, New P: 1.404
iter 24 loss: 0.021
Actual params: [1.4356, 1.4041]
-Original Grad: -0.010, -lr * Pred Grad: -0.046, New P: 1.389
-Original Grad: 0.063, -lr * Pred Grad: 0.179, New P: 1.584
iter 25 loss: 0.022
Actual params: [1.3895, 1.5835]
-Original Grad: 0.007, -lr * Pred Grad: -0.019, New P: 1.371
-Original Grad: -0.022, -lr * Pred Grad: -0.032, New P: 1.552
iter 26 loss: 0.022
Actual params: [1.3706, 1.5519]
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 1.340
-Original Grad: -0.023, -lr * Pred Grad: -0.070, New P: 1.482
iter 27 loss: 0.021
Actual params: [1.3401, 1.4823]
-Original Grad: -0.017, -lr * Pred Grad: -0.083, New P: 1.257
-Original Grad: -0.021, -lr * Pred Grad: -0.096, New P: 1.386
iter 28 loss: 0.019
Actual params: [1.2573, 1.3861]
-Original Grad: -0.030, -lr * Pred Grad: -0.150, New P: 1.108
-Original Grad: 0.024, -lr * Pred Grad: -0.004, New P: 1.382
iter 29 loss: 0.016
Actual params: [1.1076, 1.382 ]
-Original Grad: -0.024, -lr * Pred Grad: -0.176, New P: 0.932
-Original Grad: 0.029, -lr * Pred Grad: 0.051, New P: 1.433
iter 30 loss: 0.014
Actual params: [0.9318, 1.4326]
-Original Grad: -0.024, -lr * Pred Grad: -0.199, New P: 0.733
-Original Grad: -0.013, -lr * Pred Grad: -0.034, New P: 1.398
Target params: [1.3344, 1.5708]
iter 0 loss: 0.062
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.027, -lr * Pred Grad: 0.353, New P: -0.119
-Original Grad: 0.050, -lr * Pred Grad: 0.869, New P: 0.873
iter 1 loss: 0.059
Actual params: [-0.1189,  0.8725]
-Original Grad: -0.014, -lr * Pred Grad: 0.170, New P: 0.051
-Original Grad: 0.035, -lr * Pred Grad: 0.308, New P: 1.181
iter 2 loss: 0.053
Actual params: [0.0509, 1.1806]
-Original Grad: 0.003, -lr * Pred Grad: 0.175, New P: 0.226
-Original Grad: 0.063, -lr * Pred Grad: 0.201, New P: 1.381
iter 3 loss: 0.044
Actual params: [0.2256, 1.3815]
-Original Grad: 0.017, -lr * Pred Grad: 0.188, New P: 0.414
-Original Grad: 0.067, -lr * Pred Grad: 0.227, New P: 1.609
iter 4 loss: 0.029
Actual params: [0.4139, 1.6087]
-Original Grad: 0.032, -lr * Pred Grad: 0.239, New P: 0.653
-Original Grad: 0.069, -lr * Pred Grad: 0.245, New P: 1.854
iter 5 loss: 0.016
Actual params: [0.6533, 1.8536]
-Original Grad: 0.020, -lr * Pred Grad: 0.193, New P: 0.847
-Original Grad: 0.025, -lr * Pred Grad: 0.103, New P: 1.957
iter 6 loss: 0.017
Actual params: [0.8467, 1.9566]
-Original Grad: -0.000, -lr * Pred Grad: 0.103, New P: 0.950
-Original Grad: -0.127, -lr * Pred Grad: -0.226, New P: 1.731
iter 7 loss: 0.010
Actual params: [0.95  , 1.7311]
-Original Grad: 0.013, -lr * Pred Grad: 0.109, New P: 1.059
-Original Grad: 0.011, -lr * Pred Grad: -0.099, New P: 1.632
iter 8 loss: 0.011
Actual params: [1.059 , 1.6325]
-Original Grad: -0.011, -lr * Pred Grad: 0.020, New P: 1.079
-Original Grad: 0.072, -lr * Pred Grad: 0.103, New P: 1.735
iter 9 loss: 0.009
Actual params: [1.0789, 1.7354]
-Original Grad: -0.010, -lr * Pred Grad: -0.015, New P: 1.064
-Original Grad: -0.006, -lr * Pred Grad: 0.005, New P: 1.741
iter 10 loss: 0.009
Actual params: [1.0642, 1.7408]
-Original Grad: -0.000, -lr * Pred Grad: -0.014, New P: 1.051
-Original Grad: 0.003, -lr * Pred Grad: -0.000, New P: 1.741
iter 11 loss: 0.009
Actual params: [1.0506, 1.7406]
-Original Grad: -0.016, -lr * Pred Grad: -0.065, New P: 0.985
-Original Grad: 0.018, -lr * Pred Grad: 0.026, New P: 1.766
iter 12 loss: 0.009
Actual params: [0.9854, 1.7664]
-Original Grad: 0.022, -lr * Pred Grad: 0.014, New P: 0.999
-Original Grad: 0.007, -lr * Pred Grad: 0.010, New P: 1.777
iter 13 loss: 0.009
Actual params: [0.9991, 1.7769]
-Original Grad: 0.005, -lr * Pred Grad: 0.001, New P: 1.000
-Original Grad: -0.026, -lr * Pred Grad: -0.072, New P: 1.705
iter 14 loss: 0.009
Actual params: [1.0001, 1.7046]
-Original Grad: 0.010, -lr * Pred Grad: 0.017, New P: 1.017
-Original Grad: 0.030, -lr * Pred Grad: 0.026, New P: 1.731
iter 15 loss: 0.009
Actual params: [1.0167, 1.7307]
-Original Grad: -0.009, -lr * Pred Grad: -0.030, New P: 0.987
-Original Grad: 0.016, -lr * Pred Grad: 0.030, New P: 1.761
iter 16 loss: 0.009
Actual params: [0.9872, 1.761 ]
-Original Grad: 0.017, -lr * Pred Grad: 0.022, New P: 1.009
-Original Grad: -0.026, -lr * Pred Grad: -0.068, New P: 1.693
iter 17 loss: 0.009
Actual params: [1.0091, 1.6932]
-Original Grad: 0.009, -lr * Pred Grad: 0.020, New P: 1.030
-Original Grad: 0.026, -lr * Pred Grad: 0.020, New P: 1.713
iter 18 loss: 0.009
Actual params: [1.0295, 1.7132]
-Original Grad: 0.012, -lr * Pred Grad: 0.034, New P: 1.063
-Original Grad: 0.028, -lr * Pred Grad: 0.059, New P: 1.773
iter 19 loss: 0.009
Actual params: [1.0634, 1.7727]
-Original Grad: -0.005, -lr * Pred Grad: -0.007, New P: 1.056
-Original Grad: -0.027, -lr * Pred Grad: -0.064, New P: 1.708
iter 20 loss: 0.009
Actual params: [1.056 , 1.7083]
-Original Grad: -0.001, -lr * Pred Grad: -0.016, New P: 1.040
-Original Grad: 0.023, -lr * Pred Grad: 0.018, New P: 1.726
iter 21 loss: 0.009
Actual params: [1.0398, 1.7264]
-Original Grad: -0.003, -lr * Pred Grad: -0.030, New P: 1.010
-Original Grad: 0.014, -lr * Pred Grad: 0.020, New P: 1.747
iter 22 loss: 0.009
Actual params: [1.0101, 1.7465]
-Original Grad: 0.023, -lr * Pred Grad: 0.033, New P: 1.043
-Original Grad: 0.003, -lr * Pred Grad: -0.001, New P: 1.746
iter 23 loss: 0.009
Actual params: [1.0429, 1.7458]
-Original Grad: -0.005, -lr * Pred Grad: -0.015, New P: 1.028
-Original Grad: 0.015, -lr * Pred Grad: 0.020, New P: 1.766
iter 24 loss: 0.009
Actual params: [1.028 , 1.7656]
-Original Grad: 0.018, -lr * Pred Grad: 0.034, New P: 1.062
-Original Grad: -0.007, -lr * Pred Grad: -0.029, New P: 1.736
iter 25 loss: 0.009
Actual params: [1.0616, 1.7362]
-Original Grad: -0.011, -lr * Pred Grad: -0.030, New P: 1.031
-Original Grad: 0.021, -lr * Pred Grad: 0.024, New P: 1.760
iter 26 loss: 0.009
Actual params: [1.0313, 1.7602]
-Original Grad: 0.000, -lr * Pred Grad: -0.025, New P: 1.007
-Original Grad: -0.024, -lr * Pred Grad: -0.068, New P: 1.692
iter 27 loss: 0.009
Actual params: [1.0066, 1.6917]
-Original Grad: -0.008, -lr * Pred Grad: -0.052, New P: 0.954
-Original Grad: 0.027, -lr * Pred Grad: 0.024, New P: 1.716
iter 28 loss: 0.010
Actual params: [0.9542, 1.7156]
-Original Grad: 0.012, -lr * Pred Grad: -0.010, New P: 0.944
-Original Grad: 0.010, -lr * Pred Grad: 0.010, New P: 1.726
iter 29 loss: 0.010
Actual params: [0.9437, 1.7256]
-Original Grad: 0.014, -lr * Pred Grad: 0.012, New P: 0.956
-Original Grad: -0.003, -lr * Pred Grad: -0.021, New P: 1.705
iter 30 loss: 0.010
Actual params: [0.9558, 1.7045]
-Original Grad: 0.006, -lr * Pred Grad: 0.009, New P: 0.965
-Original Grad: 0.020, -lr * Pred Grad: 0.025, New P: 1.729
Target params: [1.3344, 1.5708]
iter 0 loss: 0.481
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.013, -lr * Pred Grad: 0.781, New P: 0.309
-Original Grad: -0.008, -lr * Pred Grad: 0.596, New P: 0.599
iter 1 loss: 0.339
Actual params: [0.309 , 0.5995]
-Original Grad: 0.582, -lr * Pred Grad: 2.457, New P: 2.766
-Original Grad: 0.064, -lr * Pred Grad: 0.487, New P: 1.087
iter 2 loss: 0.175
Actual params: [2.7662, 1.0869]
-Original Grad: -0.188, -lr * Pred Grad: -1.257, New P: 1.509
-Original Grad: 0.442, -lr * Pred Grad: 1.999, New P: 3.085
iter 3 loss: 0.721
Actual params: [1.509 , 3.0854]
-Original Grad: -0.122, -lr * Pred Grad: -0.346, New P: 1.163
-Original Grad: -1.207, -lr * Pred Grad: -0.810, New P: 2.275
iter 4 loss: 0.319
Actual params: [1.1632, 2.275 ]
-Original Grad: -0.905, -lr * Pred Grad: -1.566, New P: -0.403
-Original Grad: -0.631, -lr * Pred Grad: -0.383, New P: 1.892
iter 5 loss: 0.478
Actual params: [-0.403 ,  1.8923]
-Original Grad: 0.030, -lr * Pred Grad: -1.072, New P: -1.475
-Original Grad: 0.033, -lr * Pred Grad: -0.550, New P: 1.343
iter 6 loss: 0.482
Actual params: [-1.4751,  1.3426]
-Original Grad: 0.000, -lr * Pred Grad: -1.127, New P: -2.603
-Original Grad: 0.000, -lr * Pred Grad: -0.383, New P: 0.960
iter 7 loss: 0.482
Actual params: [-2.6025,  0.9597]
-Original Grad: 0.000, -lr * Pred Grad: -1.058, New P: -3.661
-Original Grad: 0.000, -lr * Pred Grad: -0.285, New P: 0.675
iter 8 loss: 0.482
Actual params: [-3.6609,  0.6749]
-Original Grad: 0.000, -lr * Pred Grad: -1.040, New P: -4.701
-Original Grad: 0.000, -lr * Pred Grad: -0.208, New P: 0.467
iter 9 loss: 0.482
Actual params: [-4.7008,  0.4671]
-Original Grad: 0.000, -lr * Pred Grad: -1.015, New P: -5.716
-Original Grad: 0.000, -lr * Pred Grad: -0.157, New P: 0.310
iter 10 loss: 0.482
Actual params: [-5.7161,  0.3099]
-Original Grad: 0.000, -lr * Pred Grad: -0.989, New P: -6.705
-Original Grad: 0.000, -lr * Pred Grad: -0.118, New P: 0.192
iter 11 loss: 0.482
Actual params: [-6.7054,  0.1922]
-Original Grad: 0.000, -lr * Pred Grad: -0.953, New P: -7.658
-Original Grad: 0.000, -lr * Pred Grad: -0.089, New P: 0.103
iter 12 loss: 0.482
Actual params: [-7.658 ,  0.1028]
-Original Grad: -0.000, -lr * Pred Grad: -0.905, New P: -8.563
-Original Grad: -0.000, -lr * Pred Grad: -0.070, New P: 0.033
iter 13 loss: 0.482
Actual params: [-8.563 ,  0.0331]
-Original Grad: -0.000, -lr * Pred Grad: -0.848, New P: -9.411
-Original Grad: -0.000, -lr * Pred Grad: -0.057, New P: -0.024
iter 14 loss: 0.482
Actual params: [-9.411 , -0.0237]
-Original Grad: -0.000, -lr * Pred Grad: -0.783, New P: -10.194
-Original Grad: -0.000, -lr * Pred Grad: -0.049, New P: -0.072
iter 15 loss: 0.482
Actual params: [-10.1944,  -0.0725]
-Original Grad: -0.000, -lr * Pred Grad: -0.713, New P: -10.908
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.116
iter 16 loss: 0.482
Actual params: [-10.9079,  -0.1163]
-Original Grad: -0.000, -lr * Pred Grad: -0.640, New P: -11.548
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -0.157
iter 17 loss: 0.482
Actual params: [-11.548 ,  -0.1572]
-Original Grad: 0.000, -lr * Pred Grad: -0.565, New P: -12.113
-Original Grad: -0.000, -lr * Pred Grad: -0.039, New P: -0.196
iter 18 loss: 0.482
Actual params: [-12.1134,  -0.1961]
-Original Grad: -0.000, -lr * Pred Grad: -0.492, New P: -12.605
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: -0.234
iter 19 loss: 0.482
Actual params: [-12.6052,  -0.2337]
-Original Grad: -0.000, -lr * Pred Grad: -0.421, New P: -13.026
-Original Grad: 0.000, -lr * Pred Grad: -0.037, New P: -0.270
iter 20 loss: 0.482
Actual params: [-13.0263,  -0.2702]
-Original Grad: -0.000, -lr * Pred Grad: -0.356, New P: -13.382
-Original Grad: 0.000, -lr * Pred Grad: -0.036, New P: -0.306
iter 21 loss: 0.482
Actual params: [-13.382,  -0.306]
-Original Grad: -0.000, -lr * Pred Grad: -0.297, New P: -13.679
-Original Grad: 0.000, -lr * Pred Grad: -0.035, New P: -0.341
iter 22 loss: 0.482
Actual params: [-13.6787,  -0.3411]
-Original Grad: -0.000, -lr * Pred Grad: -0.246, New P: -13.924
-Original Grad: 0.000, -lr * Pred Grad: -0.035, New P: -0.376
iter 23 loss: 0.482
Actual params: [-13.9243,  -0.3757]
-Original Grad: -0.000, -lr * Pred Grad: -0.202, New P: -14.127
-Original Grad: 0.000, -lr * Pred Grad: -0.034, New P: -0.410
iter 24 loss: 0.482
Actual params: [-14.1266,  -0.4098]
-Original Grad: -0.000, -lr * Pred Grad: -0.167, New P: -14.294
-Original Grad: 0.000, -lr * Pred Grad: -0.034, New P: -0.444
iter 25 loss: 0.482
Actual params: [-14.2936,  -0.4436]
-Original Grad: -0.000, -lr * Pred Grad: -0.139, New P: -14.432
-Original Grad: 0.000, -lr * Pred Grad: -0.034, New P: -0.477
iter 26 loss: 0.482
Actual params: [-14.4322,  -0.4771]
-Original Grad: -0.000, -lr * Pred Grad: -0.116, New P: -14.549
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.510
iter 27 loss: 0.482
Actual params: [-14.5485,  -0.5104]
-Original Grad: -0.000, -lr * Pred Grad: -0.099, New P: -14.648
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.543
iter 28 loss: 0.482
Actual params: [-14.6476,  -0.5435]
-Original Grad: -0.000, -lr * Pred Grad: -0.086, New P: -14.733
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.576
iter 29 loss: 0.482
Actual params: [-14.7333,  -0.5765]
-Original Grad: -0.000, -lr * Pred Grad: -0.075, New P: -14.809
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.609
iter 30 loss: 0.482
Actual params: [-14.8088,  -0.6093]
-Original Grad: -0.000, -lr * Pred Grad: -0.068, New P: -14.876
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: -0.642
Target params: [1.3344, 1.5708]
iter 0 loss: 0.212
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.038, -lr * Pred Grad: 1.017, New P: 0.545
-Original Grad: -0.007, -lr * Pred Grad: 0.599, New P: 0.602
iter 1 loss: 0.087
Actual params: [0.5446, 0.6023]
-Original Grad: -0.118, -lr * Pred Grad: -0.822, New P: -0.278
-Original Grad: 0.366, -lr * Pred Grad: 1.956, New P: 2.559
iter 2 loss: 0.225
Actual params: [-0.2778,  2.5586]
-Original Grad: 0.002, -lr * Pred Grad: -0.089, New P: -0.367
-Original Grad: -0.006, -lr * Pred Grad: -0.214, New P: 2.345
iter 3 loss: 0.224
Actual params: [-0.3672,  2.3449]
-Original Grad: 0.006, -lr * Pred Grad: -0.069, New P: -0.436
-Original Grad: -0.009, -lr * Pred Grad: -0.084, New P: 2.261
iter 4 loss: 0.224
Actual params: [-0.4365,  2.2612]
-Original Grad: 0.005, -lr * Pred Grad: -0.035, New P: -0.472
-Original Grad: -0.007, -lr * Pred Grad: -0.046, New P: 2.215
iter 5 loss: 0.224
Actual params: [-0.4717,  2.215 ]
-Original Grad: 0.005, -lr * Pred Grad: -0.014, New P: -0.485
-Original Grad: -0.008, -lr * Pred Grad: -0.062, New P: 2.153
iter 6 loss: 0.224
Actual params: [-0.4855,  2.153 ]
-Original Grad: 0.008, -lr * Pred Grad: 0.008, New P: -0.478
-Original Grad: -0.011, -lr * Pred Grad: -0.066, New P: 2.087
iter 7 loss: 0.224
Actual params: [-0.4778,  2.0874]
-Original Grad: 0.012, -lr * Pred Grad: 0.031, New P: -0.447
-Original Grad: -0.015, -lr * Pred Grad: -0.075, New P: 2.013
iter 8 loss: 0.222
Actual params: [-0.4472,  2.0127]
-Original Grad: 0.017, -lr * Pred Grad: 0.058, New P: -0.389
-Original Grad: -0.021, -lr * Pred Grad: -0.089, New P: 1.923
iter 9 loss: 0.219
Actual params: [-0.3889,  1.9234]
-Original Grad: 0.032, -lr * Pred Grad: 0.122, New P: -0.267
-Original Grad: -0.039, -lr * Pred Grad: -0.131, New P: 1.792
iter 10 loss: 0.207
Actual params: [-0.2673,  1.7922]
-Original Grad: 0.090, -lr * Pred Grad: 0.320, New P: 0.053
-Original Grad: -0.092, -lr * Pred Grad: -0.233, New P: 1.559
iter 11 loss: 0.159
Actual params: [0.0527, 1.5595]
-Original Grad: 0.122, -lr * Pred Grad: 0.497, New P: 0.549
-Original Grad: -0.057, -lr * Pred Grad: -0.231, New P: 1.328
iter 12 loss: 0.046
Actual params: [0.5495, 1.3281]
-Original Grad: 0.150, -lr * Pred Grad: 0.669, New P: 1.219
-Original Grad: -0.155, -lr * Pred Grad: -0.347, New P: 0.981
iter 13 loss: 0.055
Actual params: [1.2187, 0.9808]
-Original Grad: 0.019, -lr * Pred Grad: 0.326, New P: 1.544
-Original Grad: 0.129, -lr * Pred Grad: 0.024, New P: 1.005
iter 14 loss: 0.060
Actual params: [1.5443, 1.0052]
-Original Grad: -0.010, -lr * Pred Grad: 0.191, New P: 1.736
-Original Grad: 0.170, -lr * Pred Grad: 0.517, New P: 1.522
iter 15 loss: 0.038
Actual params: [1.7356, 1.522 ]
-Original Grad: -0.045, -lr * Pred Grad: -0.008, New P: 1.728
-Original Grad: 0.003, -lr * Pred Grad: 0.090, New P: 1.612
iter 16 loss: 0.040
Actual params: [1.7281, 1.6119]
-Original Grad: -0.068, -lr * Pred Grad: -0.166, New P: 1.562
-Original Grad: -0.079, -lr * Pred Grad: -0.163, New P: 1.449
iter 17 loss: 0.032
Actual params: [1.5623, 1.4488]
-Original Grad: -0.043, -lr * Pred Grad: -0.202, New P: 1.361
-Original Grad: 0.005, -lr * Pred Grad: -0.062, New P: 1.387
iter 18 loss: 0.031
Actual params: [1.3608, 1.3866]
-Original Grad: -0.018, -lr * Pred Grad: -0.182, New P: 1.179
-Original Grad: 0.160, -lr * Pred Grad: 0.417, New P: 1.803
iter 19 loss: 0.035
Actual params: [1.1788, 1.8035]
-Original Grad: -0.007, -lr * Pred Grad: -0.158, New P: 1.021
-Original Grad: -0.192, -lr * Pred Grad: -0.347, New P: 1.456
iter 20 loss: 0.018
Actual params: [1.0208, 1.4561]
-Original Grad: -0.036, -lr * Pred Grad: -0.223, New P: 0.798
-Original Grad: 0.049, -lr * Pred Grad: -0.034, New P: 1.422
iter 21 loss: 0.020
Actual params: [0.7981, 1.4216]
-Original Grad: 0.085, -lr * Pred Grad: 0.054, New P: 0.852
-Original Grad: -0.058, -lr * Pred Grad: -0.178, New P: 1.244
iter 22 loss: 0.017
Actual params: [0.8522, 1.2439]
-Original Grad: 0.086, -lr * Pred Grad: 0.201, New P: 1.053
-Original Grad: 0.040, -lr * Pred Grad: -0.001, New P: 1.242
iter 23 loss: 0.027
Actual params: [1.0527, 1.2425]
-Original Grad: -0.054, -lr * Pred Grad: -0.062, New P: 0.990
-Original Grad: 0.158, -lr * Pred Grad: 0.465, New P: 1.707
iter 24 loss: 0.024
Actual params: [0.9903, 1.7072]
-Original Grad: 0.009, -lr * Pred Grad: 0.004, New P: 0.995
-Original Grad: -0.186, -lr * Pred Grad: -0.354, New P: 1.353
iter 25 loss: 0.019
Actual params: [0.9948, 1.3531]
-Original Grad: -0.048, -lr * Pred Grad: -0.153, New P: 0.842
-Original Grad: 0.052, -lr * Pred Grad: -0.014, New P: 1.339
iter 26 loss: 0.016
Actual params: [0.842 , 1.3388]
-Original Grad: 0.052, -lr * Pred Grad: 0.047, New P: 0.889
-Original Grad: -0.027, -lr * Pred Grad: -0.116, New P: 1.223
iter 27 loss: 0.018
Actual params: [0.8888, 1.2231]
-Original Grad: -0.025, -lr * Pred Grad: -0.079, New P: 0.809
-Original Grad: 0.076, -lr * Pred Grad: 0.133, New P: 1.356
iter 28 loss: 0.018
Actual params: [0.8094, 1.3565]
-Original Grad: 0.050, -lr * Pred Grad: 0.085, New P: 0.894
-Original Grad: 0.014, -lr * Pred Grad: 0.058, New P: 1.415
iter 29 loss: 0.015
Actual params: [0.894 , 1.4146]
-Original Grad: 0.050, -lr * Pred Grad: 0.156, New P: 1.050
-Original Grad: -0.006, -lr * Pred Grad: -0.007, New P: 1.408
iter 30 loss: 0.020
Actual params: [1.0498, 1.4078]
-Original Grad: -0.005, -lr * Pred Grad: 0.060, New P: 1.110
-Original Grad: 0.067, -lr * Pred Grad: 0.166, New P: 1.574
Target params: [1.3344, 1.5708]
iter 0 loss: 0.057
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.006, -lr * Pred Grad: 0.579, New P: 0.106
-Original Grad: 0.009, -lr * Pred Grad: 0.676, New P: 0.679
iter 1 loss: 0.054
Actual params: [0.1062, 0.6793]
-Original Grad: 0.027, -lr * Pred Grad: 0.613, New P: 0.719
-Original Grad: 0.040, -lr * Pred Grad: 0.347, New P: 1.026
iter 2 loss: 0.021
Actual params: [0.7188, 1.026 ]
-Original Grad: 0.046, -lr * Pred Grad: 0.631, New P: 1.350
-Original Grad: 0.095, -lr * Pred Grad: 0.386, New P: 1.412
iter 3 loss: 0.028
Actual params: [1.35  , 1.4125]
-Original Grad: -0.001, -lr * Pred Grad: 0.206, New P: 1.556
-Original Grad: -0.131, -lr * Pred Grad: -0.382, New P: 1.031
iter 4 loss: 0.022
Actual params: [1.5565, 1.0307]
-Original Grad: -0.022, -lr * Pred Grad: 0.043, New P: 1.600
-Original Grad: 0.154, -lr * Pred Grad: 0.245, New P: 1.276
iter 5 loss: 0.020
Actual params: [1.5997, 1.2756]
-Original Grad: 0.017, -lr * Pred Grad: 0.128, New P: 1.728
-Original Grad: -0.017, -lr * Pred Grad: -0.021, New P: 1.254
iter 6 loss: 0.019
Actual params: [1.7279, 1.2545]
-Original Grad: 0.002, -lr * Pred Grad: 0.060, New P: 1.787
-Original Grad: 0.020, -lr * Pred Grad: 0.046, New P: 1.301
iter 7 loss: 0.019
Actual params: [1.7875, 1.3006]
-Original Grad: 0.004, -lr * Pred Grad: 0.051, New P: 1.838
-Original Grad: -0.019, -lr * Pred Grad: -0.052, New P: 1.249
iter 8 loss: 0.020
Actual params: [1.8384, 1.2487]
-Original Grad: 0.001, -lr * Pred Grad: 0.028, New P: 1.867
-Original Grad: -0.020, -lr * Pred Grad: -0.074, New P: 1.175
iter 9 loss: 0.022
Actual params: [1.8668, 1.1751]
-Original Grad: -0.011, -lr * Pred Grad: -0.022, New P: 1.845
-Original Grad: 0.061, -lr * Pred Grad: 0.101, New P: 1.276
iter 10 loss: 0.020
Actual params: [1.8452, 1.2758]
-Original Grad: -0.006, -lr * Pred Grad: -0.035, New P: 1.811
-Original Grad: 0.034, -lr * Pred Grad: 0.113, New P: 1.389
iter 11 loss: 0.019
Actual params: [1.8107, 1.389 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.035, New P: 1.776
-Original Grad: 0.009, -lr * Pred Grad: 0.046, New P: 1.435
iter 12 loss: 0.020
Actual params: [1.7761, 1.4347]
-Original Grad: 0.012, -lr * Pred Grad: 0.000, New P: 1.776
-Original Grad: -0.043, -lr * Pred Grad: -0.098, New P: 1.336
iter 13 loss: 0.019
Actual params: [1.7762, 1.3363]
-Original Grad: -0.009, -lr * Pred Grad: -0.042, New P: 1.734
-Original Grad: 0.034, -lr * Pred Grad: 0.026, New P: 1.362
iter 14 loss: 0.019
Actual params: [1.7343, 1.362 ]
-Original Grad: -0.005, -lr * Pred Grad: -0.050, New P: 1.684
-Original Grad: 0.029, -lr * Pred Grad: 0.063, New P: 1.425
iter 15 loss: 0.021
Actual params: [1.6841, 1.4251]
-Original Grad: 0.017, -lr * Pred Grad: 0.002, New P: 1.686
-Original Grad: -0.043, -lr * Pred Grad: -0.098, New P: 1.327
iter 16 loss: 0.020
Actual params: [1.6863, 1.3274]
-Original Grad: 0.017, -lr * Pred Grad: 0.029, New P: 1.715
-Original Grad: -0.012, -lr * Pred Grad: -0.077, New P: 1.250
iter 17 loss: 0.019
Actual params: [1.7152, 1.2503]
-Original Grad: -0.004, -lr * Pred Grad: -0.012, New P: 1.704
-Original Grad: 0.006, -lr * Pred Grad: -0.041, New P: 1.209
iter 18 loss: 0.020
Actual params: [1.7035, 1.2091]
-Original Grad: -0.007, -lr * Pred Grad: -0.037, New P: 1.666
-Original Grad: 0.033, -lr * Pred Grad: 0.046, New P: 1.255
iter 19 loss: 0.019
Actual params: [1.6664, 1.255 ]
-Original Grad: -0.005, -lr * Pred Grad: -0.048, New P: 1.618
-Original Grad: 0.028, -lr * Pred Grad: 0.072, New P: 1.326
iter 20 loss: 0.020
Actual params: [1.6184, 1.3265]
-Original Grad: 0.034, -lr * Pred Grad: 0.051, New P: 1.670
-Original Grad: -0.048, -lr * Pred Grad: -0.108, New P: 1.219
iter 21 loss: 0.020
Actual params: [1.6695, 1.2186]
-Original Grad: -0.011, -lr * Pred Grad: -0.025, New P: 1.645
-Original Grad: 0.048, -lr * Pred Grad: 0.060, New P: 1.279
iter 22 loss: 0.019
Actual params: [1.6445, 1.2788]
-Original Grad: 0.002, -lr * Pred Grad: -0.016, New P: 1.628
-Original Grad: 0.039, -lr * Pred Grad: 0.106, New P: 1.384
iter 23 loss: 0.021
Actual params: [1.6283, 1.3844]
-Original Grad: 0.024, -lr * Pred Grad: 0.043, New P: 1.671
-Original Grad: -0.002, -lr * Pred Grad: 0.011, New P: 1.395
iter 24 loss: 0.020
Actual params: [1.6714, 1.3954]
-Original Grad: 0.008, -lr * Pred Grad: 0.028, New P: 1.700
-Original Grad: -0.010, -lr * Pred Grad: -0.034, New P: 1.361
iter 25 loss: 0.020
Actual params: [1.6999, 1.361 ]
-Original Grad: 0.008, -lr * Pred Grad: 0.030, New P: 1.730
-Original Grad: 0.011, -lr * Pred Grad: -0.005, New P: 1.356
iter 26 loss: 0.019
Actual params: [1.7303, 1.3556]
-Original Grad: 0.018, -lr * Pred Grad: 0.059, New P: 1.790
-Original Grad: -0.050, -lr * Pred Grad: -0.130, New P: 1.225
iter 27 loss: 0.020
Actual params: [1.7896, 1.2254]
-Original Grad: -0.010, -lr * Pred Grad: -0.008, New P: 1.782
-Original Grad: 0.065, -lr * Pred Grad: 0.084, New P: 1.310
iter 28 loss: 0.019
Actual params: [1.7816, 1.3096]
-Original Grad: -0.010, -lr * Pred Grad: -0.039, New P: 1.743
-Original Grad: 0.038, -lr * Pred Grad: 0.114, New P: 1.424
iter 29 loss: 0.020
Actual params: [1.7427, 1.4237]
-Original Grad: 0.015, -lr * Pred Grad: 0.008, New P: 1.751
-Original Grad: -0.037, -lr * Pred Grad: -0.074, New P: 1.349
iter 30 loss: 0.019
Actual params: [1.7511, 1.3495]
-Original Grad: 0.004, -lr * Pred Grad: -0.002, New P: 1.749
-Original Grad: -0.021, -lr * Pred Grad: -0.083, New P: 1.266
Target params: [1.3344, 1.5708]
iter 0 loss: 0.432
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.014, -lr * Pred Grad: 0.500, New P: 0.028
-Original Grad: -0.003, -lr * Pred Grad: 0.617, New P: 0.621
iter 1 loss: 0.267
Actual params: [0.0275, 0.621 ]
-Original Grad: 0.662, -lr * Pred Grad: 2.415, New P: 2.443
-Original Grad: 0.680, -lr * Pred Grad: 2.583, New P: 3.204
iter 2 loss: 0.393
Actual params: [2.4429, 3.2037]
-Original Grad: -0.014, -lr * Pred Grad: -0.891, New P: 1.552
-Original Grad: -0.382, -lr * Pred Grad: -1.041, New P: 2.163
iter 3 loss: 0.108
Actual params: [1.5517, 2.1626]
-Original Grad: -0.208, -lr * Pred Grad: -0.414, New P: 1.137
-Original Grad: -0.473, -lr * Pred Grad: -0.274, New P: 1.888
iter 4 loss: 0.020
Actual params: [1.1372, 1.8883]
-Original Grad: -0.035, -lr * Pred Grad: -0.221, New P: 0.916
-Original Grad: -0.257, -lr * Pred Grad: -0.558, New P: 1.330
iter 5 loss: 0.030
Actual params: [0.9162, 1.3299]
-Original Grad: -0.093, -lr * Pred Grad: -0.361, New P: 0.555
-Original Grad: 0.200, -lr * Pred Grad: -0.095, New P: 1.235
iter 6 loss: 0.023
Actual params: [0.555 , 1.2354]
-Original Grad: -0.015, -lr * Pred Grad: -0.280, New P: 0.275
-Original Grad: 0.146, -lr * Pred Grad: 0.242, New P: 1.477
iter 7 loss: 0.101
Actual params: [0.2747, 1.4775]
-Original Grad: 0.408, -lr * Pred Grad: 0.536, New P: 0.811
-Original Grad: -0.440, -lr * Pred Grad: -0.412, New P: 1.066
iter 8 loss: 0.074
Actual params: [0.8107, 1.0657]
-Original Grad: -0.142, -lr * Pred Grad: -0.216, New P: 0.595
-Original Grad: 0.532, -lr * Pred Grad: 0.634, New P: 1.699
iter 9 loss: 0.026
Actual params: [0.5949, 1.6993]
-Original Grad: 0.094, -lr * Pred Grad: 0.171, New P: 0.765
-Original Grad: -0.115, -lr * Pred Grad: -0.264, New P: 1.435
iter 10 loss: 0.008
Actual params: [0.7654, 1.4353]
-Original Grad: -0.039, -lr * Pred Grad: -0.046, New P: 0.719
-Original Grad: 0.117, -lr * Pred Grad: 0.212, New P: 1.647
iter 11 loss: 0.008
Actual params: [0.7191, 1.6473]
-Original Grad: 0.048, -lr * Pred Grad: 0.111, New P: 0.830
-Original Grad: -0.055, -lr * Pred Grad: -0.100, New P: 1.547
iter 12 loss: 0.004
Actual params: [0.8301, 1.5472]
-Original Grad: -0.016, -lr * Pred Grad: 0.002, New P: 0.832
-Original Grad: 0.027, -lr * Pred Grad: 0.028, New P: 1.576
iter 13 loss: 0.004
Actual params: [0.8316, 1.5756]
-Original Grad: -0.006, -lr * Pred Grad: -0.009, New P: 0.822
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: 1.576
iter 14 loss: 0.004
Actual params: [0.8223, 1.5761]
-Original Grad: -0.003, -lr * Pred Grad: -0.022, New P: 0.800
-Original Grad: -0.012, -lr * Pred Grad: -0.042, New P: 1.534
iter 15 loss: 0.004
Actual params: [0.8004, 1.5342]
-Original Grad: -0.003, -lr * Pred Grad: -0.033, New P: 0.767
-Original Grad: 0.031, -lr * Pred Grad: 0.043, New P: 1.577
iter 16 loss: 0.004
Actual params: [0.7671, 1.577 ]
-Original Grad: 0.022, -lr * Pred Grad: 0.025, New P: 0.792
-Original Grad: 0.001, -lr * Pred Grad: -0.002, New P: 1.575
iter 17 loss: 0.004
Actual params: [0.7924, 1.575 ]
-Original Grad: -0.003, -lr * Pred Grad: -0.013, New P: 0.780
-Original Grad: -0.000, -lr * Pred Grad: -0.016, New P: 1.559
iter 18 loss: 0.004
Actual params: [0.7795, 1.5588]
-Original Grad: 0.006, -lr * Pred Grad: -0.001, New P: 0.779
-Original Grad: 0.008, -lr * Pred Grad: -0.007, New P: 1.552
iter 19 loss: 0.004
Actual params: [0.7789, 1.5521]
-Original Grad: 0.013, -lr * Pred Grad: 0.021, New P: 0.800
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: 1.553
iter 20 loss: 0.004
Actual params: [0.7996, 1.5527]
-Original Grad: 0.011, -lr * Pred Grad: 0.027, New P: 0.827
-Original Grad: 0.003, -lr * Pred Grad: -0.010, New P: 1.543
iter 21 loss: 0.004
Actual params: [0.8268, 1.5428]
-Original Grad: -0.029, -lr * Pred Grad: -0.079, New P: 0.748
-Original Grad: 0.037, -lr * Pred Grad: 0.073, New P: 1.616
iter 22 loss: 0.006
Actual params: [0.7478, 1.6162]
-Original Grad: 0.015, -lr * Pred Grad: -0.010, New P: 0.737
-Original Grad: -0.031, -lr * Pred Grad: -0.073, New P: 1.543
iter 23 loss: 0.006
Actual params: [0.7374, 1.5432]
-Original Grad: 0.031, -lr * Pred Grad: 0.060, New P: 0.797
-Original Grad: 0.003, -lr * Pred Grad: -0.033, New P: 1.510
iter 24 loss: 0.005
Actual params: [0.7969, 1.5104]
-Original Grad: -0.019, -lr * Pred Grad: -0.038, New P: 0.759
-Original Grad: 0.050, -lr * Pred Grad: 0.093, New P: 1.603
iter 25 loss: 0.005
Actual params: [0.7586, 1.6031]
-Original Grad: 0.011, -lr * Pred Grad: 0.004, New P: 0.762
-Original Grad: -0.016, -lr * Pred Grad: -0.032, New P: 1.571
iter 26 loss: 0.005
Actual params: [0.7625, 1.5709]
-Original Grad: 0.020, -lr * Pred Grad: 0.042, New P: 0.804
-Original Grad: -0.008, -lr * Pred Grad: -0.041, New P: 1.530
iter 27 loss: 0.004
Actual params: [0.804 , 1.5302]
-Original Grad: -0.022, -lr * Pred Grad: -0.055, New P: 0.749
-Original Grad: 0.034, -lr * Pred Grad: 0.048, New P: 1.579
iter 28 loss: 0.005
Actual params: [0.7489, 1.5787]
-Original Grad: 0.006, -lr * Pred Grad: -0.021, New P: 0.728
-Original Grad: -0.001, -lr * Pred Grad: -0.005, New P: 1.573
iter 29 loss: 0.006
Actual params: [0.7277, 1.5733]
-Original Grad: 0.043, -lr * Pred Grad: 0.090, New P: 0.817
-Original Grad: -0.025, -lr * Pred Grad: -0.074, New P: 1.500
iter 30 loss: 0.006
Actual params: [0.8173, 1.4998]
-Original Grad: -0.023, -lr * Pred Grad: -0.038, New P: 0.779
-Original Grad: 0.047, -lr * Pred Grad: 0.068, New P: 1.568
Target params: [1.3344, 1.5708]
iter 0 loss: 0.008
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.012, -lr * Pred Grad: 0.763, New P: 0.291
-Original Grad: -0.007, -lr * Pred Grad: 0.600, New P: 0.604
iter 1 loss: 0.018
Actual params: [0.2908, 0.6036]
-Original Grad: -0.292, -lr * Pred Grad: -1.670, New P: -1.380
-Original Grad: 0.186, -lr * Pred Grad: 1.192, New P: 1.796
iter 2 loss: 0.010
Actual params: [-1.3796,  1.7957]
-Original Grad: 0.000, -lr * Pred Grad: -0.734, New P: -2.114
-Original Grad: 0.000, -lr * Pred Grad: -0.158, New P: 1.638
iter 3 loss: 0.010
Actual params: [-2.114 ,  1.6382]
-Original Grad: 0.000, -lr * Pred Grad: -0.559, New P: -2.673
-Original Grad: 0.000, -lr * Pred Grad: -0.057, New P: 1.581
iter 4 loss: 0.010
Actual params: [-2.6734,  1.5811]
-Original Grad: 0.000, -lr * Pred Grad: -0.476, New P: -3.149
-Original Grad: 0.000, -lr * Pred Grad: -0.025, New P: 1.556
iter 5 loss: 0.010
Actual params: [-3.1492,  1.5562]
-Original Grad: 0.000, -lr * Pred Grad: -0.403, New P: -3.553
-Original Grad: 0.000, -lr * Pred Grad: -0.036, New P: 1.520
iter 6 loss: 0.010
Actual params: [-3.5526,  1.5198]
-Original Grad: 0.000, -lr * Pred Grad: -0.336, New P: -3.889
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 1.487
iter 7 loss: 0.010
Actual params: [-3.8889,  1.4872]
-Original Grad: 0.000, -lr * Pred Grad: -0.278, New P: -4.167
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 1.457
iter 8 loss: 0.010
Actual params: [-4.167 ,  1.4571]
-Original Grad: 0.000, -lr * Pred Grad: -0.229, New P: -4.396
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 1.429
iter 9 loss: 0.010
Actual params: [-4.396 ,  1.4292]
-Original Grad: 0.000, -lr * Pred Grad: -0.188, New P: -4.584
-Original Grad: 0.000, -lr * Pred Grad: -0.027, New P: 1.402
iter 10 loss: 0.010
Actual params: [-4.5842,  1.4018]
-Original Grad: 0.000, -lr * Pred Grad: -0.155, New P: -4.740
-Original Grad: 0.000, -lr * Pred Grad: -0.028, New P: 1.374
iter 11 loss: 0.010
Actual params: [-4.7397,  1.3742]
-Original Grad: 0.000, -lr * Pred Grad: -0.129, New P: -4.869
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 1.346
iter 12 loss: 0.010
Actual params: [-4.8691,  1.346 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.109, New P: -4.978
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 1.317
iter 13 loss: 0.010
Actual params: [-4.9781,  1.3171]
-Original Grad: 0.000, -lr * Pred Grad: -0.093, New P: -5.071
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 1.287
iter 14 loss: 0.010
Actual params: [-5.0715,  1.2875]
-Original Grad: 0.000, -lr * Pred Grad: -0.081, New P: -5.153
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 1.257
iter 15 loss: 0.010
Actual params: [-5.1528,  1.2573]
-Original Grad: 0.000, -lr * Pred Grad: -0.072, New P: -5.225
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 1.227
iter 16 loss: 0.010
Actual params: [-5.2249,  1.2266]
-Original Grad: 0.000, -lr * Pred Grad: -0.065, New P: -5.290
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 1.195
iter 17 loss: 0.010
Actual params: [-5.29  ,  1.1954]
-Original Grad: 0.000, -lr * Pred Grad: -0.060, New P: -5.350
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 1.164
iter 18 loss: 0.010
Actual params: [-5.3498,  1.1639]
-Original Grad: 0.000, -lr * Pred Grad: -0.056, New P: -5.406
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.132
iter 19 loss: 0.010
Actual params: [-5.4055,  1.1322]
-Original Grad: 0.000, -lr * Pred Grad: -0.053, New P: -5.458
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.100
iter 20 loss: 0.010
Actual params: [-5.4582,  1.1002]
-Original Grad: 0.000, -lr * Pred Grad: -0.050, New P: -5.509
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.068
iter 21 loss: 0.010
Actual params: [-5.5086,  1.0681]
-Original Grad: 0.000, -lr * Pred Grad: -0.049, New P: -5.557
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.036
iter 22 loss: 0.010
Actual params: [-5.5572,  1.0358]
-Original Grad: 0.000, -lr * Pred Grad: -0.047, New P: -5.604
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.003
iter 23 loss: 0.010
Actual params: [-5.6044,  1.0035]
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: -5.651
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.971
iter 24 loss: 0.010
Actual params: [-5.6507,  0.9711]
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: -5.696
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.939
iter 25 loss: 0.010
Actual params: [-5.6962,  0.9387]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -5.741
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.906
iter 26 loss: 0.010
Actual params: [-5.7412,  0.9062]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -5.786
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.874
iter 27 loss: 0.010
Actual params: [-5.7857,  0.8737]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -5.830
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.841
iter 28 loss: 0.010
Actual params: [-5.83  ,  0.8412]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -5.874
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.809
iter 29 loss: 0.010
Actual params: [-5.874 ,  0.8087]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -5.918
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.776
iter 30 loss: 0.010
Actual params: [-5.9179,  0.7761]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -5.962
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.744
Target params: [1.3344, 1.5708]
iter 0 loss: 0.293
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.022, -lr * Pred Grad: 0.414, New P: -0.058
-Original Grad: 0.046, -lr * Pred Grad: 0.850, New P: 0.854
iter 1 loss: 0.214
Actual params: [-0.0579,  0.8538]
-Original Grad: 0.102, -lr * Pred Grad: 1.252, New P: 1.194
-Original Grad: 0.173, -lr * Pred Grad: 1.137, New P: 1.990
iter 2 loss: 0.019
Actual params: [1.1938, 1.9903]
-Original Grad: -0.002, -lr * Pred Grad: 0.126, New P: 1.320
-Original Grad: -0.154, -lr * Pred Grad: -0.731, New P: 1.259
iter 3 loss: 0.047
Actual params: [1.3196, 1.2591]
-Original Grad: -0.091, -lr * Pred Grad: -0.356, New P: 0.964
-Original Grad: 0.352, -lr * Pred Grad: 0.729, New P: 1.989
iter 4 loss: 0.015
Actual params: [0.964 , 1.9886]
-Original Grad: -0.006, -lr * Pred Grad: -0.070, New P: 0.894
-Original Grad: -0.168, -lr * Pred Grad: -0.352, New P: 1.637
iter 5 loss: 0.004
Actual params: [0.8942, 1.6371]
-Original Grad: 0.019, -lr * Pred Grad: 0.005, New P: 0.899
-Original Grad: 0.066, -lr * Pred Grad: 0.062, New P: 1.700
iter 6 loss: 0.003
Actual params: [0.8995, 1.6995]
-Original Grad: -0.004, -lr * Pred Grad: -0.032, New P: 0.867
-Original Grad: -0.007, -lr * Pred Grad: -0.048, New P: 1.651
iter 7 loss: 0.004
Actual params: [0.8674, 1.6515]
-Original Grad: 0.028, -lr * Pred Grad: 0.059, New P: 0.926
-Original Grad: 0.043, -lr * Pred Grad: 0.086, New P: 1.737
iter 8 loss: 0.003
Actual params: [0.9259, 1.7373]
-Original Grad: -0.003, -lr * Pred Grad: 0.006, New P: 0.932
-Original Grad: -0.043, -lr * Pred Grad: -0.087, New P: 1.650
iter 9 loss: 0.003
Actual params: [0.9321, 1.6502]
-Original Grad: -0.012, -lr * Pred Grad: -0.035, New P: 0.897
-Original Grad: 0.024, -lr * Pred Grad: 0.012, New P: 1.662
iter 10 loss: 0.003
Actual params: [0.8971, 1.6623]
-Original Grad: 0.008, -lr * Pred Grad: -0.004, New P: 0.894
-Original Grad: 0.040, -lr * Pred Grad: 0.089, New P: 1.752
iter 11 loss: 0.003
Actual params: [0.8935, 1.7516]
-Original Grad: 0.008, -lr * Pred Grad: 0.006, New P: 0.900
-Original Grad: -0.023, -lr * Pred Grad: -0.043, New P: 1.708
iter 12 loss: 0.003
Actual params: [0.8999, 1.7083]
-Original Grad: 0.016, -lr * Pred Grad: 0.037, New P: 0.937
-Original Grad: -0.001, -lr * Pred Grad: -0.027, New P: 1.681
iter 13 loss: 0.003
Actual params: [0.9369, 1.6809]
-Original Grad: -0.007, -lr * Pred Grad: -0.014, New P: 0.923
-Original Grad: 0.023, -lr * Pred Grad: 0.024, New P: 1.705
iter 14 loss: 0.003
Actual params: [0.9234, 1.7053]
-Original Grad: -0.010, -lr * Pred Grad: -0.043, New P: 0.880
-Original Grad: 0.003, -lr * Pred Grad: -0.002, New P: 1.703
iter 15 loss: 0.003
Actual params: [0.8802, 1.7029]
-Original Grad: 0.015, -lr * Pred Grad: 0.006, New P: 0.886
-Original Grad: -0.007, -lr * Pred Grad: -0.033, New P: 1.670
iter 16 loss: 0.003
Actual params: [0.8864, 1.6696]
-Original Grad: 0.020, -lr * Pred Grad: 0.042, New P: 0.928
-Original Grad: 0.007, -lr * Pred Grad: -0.013, New P: 1.657
iter 17 loss: 0.003
Actual params: [0.9281, 1.6567]
-Original Grad: 0.011, -lr * Pred Grad: 0.039, New P: 0.967
-Original Grad: 0.004, -lr * Pred Grad: -0.014, New P: 1.642
iter 18 loss: 0.003
Actual params: [0.9668, 1.6424]
-Original Grad: -0.001, -lr * Pred Grad: 0.010, New P: 0.976
-Original Grad: 0.015, -lr * Pred Grad: 0.013, New P: 1.655
iter 19 loss: 0.003
Actual params: [0.9765, 1.6554]
-Original Grad: 0.003, -lr * Pred Grad: 0.006, New P: 0.983
-Original Grad: 0.004, -lr * Pred Grad: -0.004, New P: 1.652
iter 20 loss: 0.003
Actual params: [0.9828, 1.6516]
-Original Grad: 0.002, -lr * Pred Grad: -0.002, New P: 0.981
-Original Grad: 0.013, -lr * Pred Grad: 0.015, New P: 1.666
iter 21 loss: 0.003
Actual params: [0.9808, 1.6664]
-Original Grad: -0.010, -lr * Pred Grad: -0.042, New P: 0.938
-Original Grad: -0.001, -lr * Pred Grad: -0.015, New P: 1.651
iter 22 loss: 0.003
Actual params: [0.9383, 1.6512]
-Original Grad: -0.005, -lr * Pred Grad: -0.051, New P: 0.888
-Original Grad: 0.031, -lr * Pred Grad: 0.056, New P: 1.707
iter 23 loss: 0.003
Actual params: [0.8876, 1.7074]
-Original Grad: 0.017, -lr * Pred Grad: 0.003, New P: 0.890
-Original Grad: 0.006, -lr * Pred Grad: 0.015, New P: 1.722
iter 24 loss: 0.003
Actual params: [0.8903, 1.7221]
-Original Grad: 0.017, -lr * Pred Grad: 0.028, New P: 0.918
-Original Grad: 0.000, -lr * Pred Grad: -0.010, New P: 1.712
iter 25 loss: 0.003
Actual params: [0.9179, 1.7124]
-Original Grad: 0.006, -lr * Pred Grad: 0.017, New P: 0.935
-Original Grad: -0.007, -lr * Pred Grad: -0.037, New P: 1.675
iter 26 loss: 0.003
Actual params: [0.9351, 1.6749]
-Original Grad: -0.003, -lr * Pred Grad: -0.010, New P: 0.925
-Original Grad: 0.018, -lr * Pred Grad: 0.012, New P: 1.686
iter 27 loss: 0.003
Actual params: [0.9255, 1.6865]
-Original Grad: -0.005, -lr * Pred Grad: -0.030, New P: 0.895
-Original Grad: -0.004, -lr * Pred Grad: -0.025, New P: 1.661
iter 28 loss: 0.003
Actual params: [0.8954, 1.6613]
-Original Grad: -0.005, -lr * Pred Grad: -0.045, New P: 0.850
-Original Grad: 0.015, -lr * Pred Grad: 0.012, New P: 1.673
iter 29 loss: 0.004
Actual params: [0.8504, 1.6729]
-Original Grad: 0.025, -lr * Pred Grad: 0.031, New P: 0.881
-Original Grad: 0.023, -lr * Pred Grad: 0.044, New P: 1.717
iter 30 loss: 0.003
Actual params: [0.8812, 1.7173]
-Original Grad: 0.003, -lr * Pred Grad: 0.003, New P: 0.885
-Original Grad: -0.003, -lr * Pred Grad: -0.012, New P: 1.705
Target params: [1.3344, 1.5708]
iter 0 loss: 0.103
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.023, -lr * Pred Grad: 0.397, New P: -0.076
-Original Grad: 0.030, -lr * Pred Grad: 0.774, New P: 0.778
iter 1 loss: 0.096
Actual params: [-0.0756,  0.7776]
-Original Grad: 0.005, -lr * Pred Grad: 0.375, New P: 0.299
-Original Grad: 0.095, -lr * Pred Grad: 0.680, New P: 1.457
iter 2 loss: 0.042
Actual params: [0.2993, 1.4574]
-Original Grad: 0.118, -lr * Pred Grad: 1.054, New P: 1.353
-Original Grad: 0.159, -lr * Pred Grad: 0.777, New P: 2.235
iter 3 loss: 0.032
Actual params: [1.3528, 2.2347]
-Original Grad: 0.120, -lr * Pred Grad: 0.929, New P: 2.282
-Original Grad: -0.392, -lr * Pred Grad: -0.706, New P: 1.529
iter 4 loss: 0.024
Actual params: [2.2818, 1.5292]
-Original Grad: -0.019, -lr * Pred Grad: 0.185, New P: 2.467
-Original Grad: 0.129, -lr * Pred Grad: -0.007, New P: 1.522
iter 5 loss: 0.027
Actual params: [2.4666, 1.5225]
-Original Grad: -0.014, -lr * Pred Grad: 0.152, New P: 2.619
-Original Grad: 0.115, -lr * Pred Grad: 0.158, New P: 1.681
iter 6 loss: 0.022
Actual params: [2.6188, 1.6809]
-Original Grad: -0.013, -lr * Pred Grad: 0.049, New P: 2.668
-Original Grad: 0.108, -lr * Pred Grad: 0.401, New P: 2.082
iter 7 loss: 0.024
Actual params: [2.6677, 2.0815]
-Original Grad: -0.039, -lr * Pred Grad: -0.079, New P: 2.588
-Original Grad: 0.019, -lr * Pred Grad: 0.145, New P: 2.227
iter 8 loss: 0.019
Actual params: [2.5884, 2.2266]
-Original Grad: -0.044, -lr * Pred Grad: -0.161, New P: 2.428
-Original Grad: 0.048, -lr * Pred Grad: 0.186, New P: 2.412
iter 9 loss: 0.013
Actual params: [2.4279, 2.4123]
-Original Grad: 0.001, -lr * Pred Grad: -0.096, New P: 2.332
-Original Grad: 0.004, -lr * Pred Grad: 0.047, New P: 2.459
iter 10 loss: 0.014
Actual params: [2.3322, 2.4589]
-Original Grad: 0.009, -lr * Pred Grad: -0.056, New P: 2.276
-Original Grad: -0.032, -lr * Pred Grad: -0.066, New P: 2.392
iter 11 loss: 0.014
Actual params: [2.2762, 2.3924]
-Original Grad: 0.010, -lr * Pred Grad: -0.029, New P: 2.247
-Original Grad: -0.040, -lr * Pred Grad: -0.120, New P: 2.273
iter 12 loss: 0.013
Actual params: [2.2474, 2.2726]
-Original Grad: 0.005, -lr * Pred Grad: -0.022, New P: 2.225
-Original Grad: -0.028, -lr * Pred Grad: -0.125, New P: 2.147
iter 13 loss: 0.013
Actual params: [2.225 , 2.1473]
-Original Grad: -0.004, -lr * Pred Grad: -0.041, New P: 2.184
-Original Grad: 0.010, -lr * Pred Grad: -0.052, New P: 2.095
iter 14 loss: 0.013
Actual params: [2.1837, 2.0954]
-Original Grad: -0.003, -lr * Pred Grad: -0.048, New P: 2.136
-Original Grad: 0.014, -lr * Pred Grad: -0.008, New P: 2.087
iter 15 loss: 0.013
Actual params: [2.1361, 2.087 ]
-Original Grad: 0.002, -lr * Pred Grad: -0.039, New P: 2.097
-Original Grad: -0.004, -lr * Pred Grad: -0.032, New P: 2.055
iter 16 loss: 0.013
Actual params: [2.097 , 2.0552]
-Original Grad: -0.004, -lr * Pred Grad: -0.051, New P: 2.046
-Original Grad: 0.005, -lr * Pred Grad: -0.020, New P: 2.036
iter 17 loss: 0.013
Actual params: [2.0456, 2.0356]
-Original Grad: -0.007, -lr * Pred Grad: -0.066, New P: 1.979
-Original Grad: 0.024, -lr * Pred Grad: 0.032, New P: 2.068
iter 18 loss: 0.012
Actual params: [1.9792, 2.068 ]
-Original Grad: -0.004, -lr * Pred Grad: -0.070, New P: 1.909
-Original Grad: 0.017, -lr * Pred Grad: 0.036, New P: 2.104
iter 19 loss: 0.012
Actual params: [1.9093, 2.1036]
-Original Grad: -0.001, -lr * Pred Grad: -0.065, New P: 1.844
-Original Grad: -0.005, -lr * Pred Grad: -0.019, New P: 2.085
iter 20 loss: 0.012
Actual params: [1.8442, 2.085 ]
-Original Grad: -0.004, -lr * Pred Grad: -0.070, New P: 1.775
-Original Grad: -0.001, -lr * Pred Grad: -0.025, New P: 2.060
iter 21 loss: 0.012
Actual params: [1.7747, 2.0602]
-Original Grad: -0.008, -lr * Pred Grad: -0.084, New P: 1.691
-Original Grad: -0.005, -lr * Pred Grad: -0.041, New P: 2.019
iter 22 loss: 0.011
Actual params: [1.6906, 2.0195]
-Original Grad: -0.009, -lr * Pred Grad: -0.097, New P: 1.594
-Original Grad: -0.011, -lr * Pred Grad: -0.060, New P: 1.960
iter 23 loss: 0.011
Actual params: [1.5941, 1.9597]
-Original Grad: -0.016, -lr * Pred Grad: -0.125, New P: 1.469
-Original Grad: 0.020, -lr * Pred Grad: 0.003, New P: 1.963
iter 24 loss: 0.011
Actual params: [1.4688, 1.963 ]
-Original Grad: 0.005, -lr * Pred Grad: -0.085, New P: 1.384
-Original Grad: -0.005, -lr * Pred Grad: -0.033, New P: 1.930
iter 25 loss: 0.012
Actual params: [1.3843, 1.9295]
-Original Grad: 0.015, -lr * Pred Grad: -0.034, New P: 1.351
-Original Grad: 0.013, -lr * Pred Grad: 0.003, New P: 1.932
iter 26 loss: 0.012
Actual params: [1.3506, 1.9322]
-Original Grad: 0.002, -lr * Pred Grad: -0.039, New P: 1.311
-Original Grad: 0.019, -lr * Pred Grad: 0.027, New P: 1.960
iter 27 loss: 0.013
Actual params: [1.3112, 1.9596]
-Original Grad: 0.034, -lr * Pred Grad: 0.056, New P: 1.367
-Original Grad: -0.015, -lr * Pred Grad: -0.045, New P: 1.914
iter 28 loss: 0.012
Actual params: [1.3667, 1.9143]
-Original Grad: 0.011, -lr * Pred Grad: 0.039, New P: 1.406
-Original Grad: -0.002, -lr * Pred Grad: -0.039, New P: 1.875
iter 29 loss: 0.011
Actual params: [1.4062, 1.8752]
-Original Grad: 0.010, -lr * Pred Grad: 0.043, New P: 1.449
-Original Grad: -0.014, -lr * Pred Grad: -0.068, New P: 1.808
iter 30 loss: 0.011
Actual params: [1.4489, 1.8076]
-Original Grad: 0.000, -lr * Pred Grad: 0.015, New P: 1.464
-Original Grad: 0.001, -lr * Pred Grad: -0.044, New P: 1.763
Target params: [1.3344, 1.5708]
iter 0 loss: 0.017
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.009, -lr * Pred Grad: 0.549, New P: 0.077
-Original Grad: 0.002, -lr * Pred Grad: 0.644, New P: 0.647
iter 1 loss: 0.036
Actual params: [0.0771, 0.6471]
-Original Grad: -0.062, -lr * Pred Grad: -0.319, New P: -0.242
-Original Grad: 0.068, -lr * Pred Grad: 0.514, New P: 1.161
iter 2 loss: 0.017
Actual params: [-0.2418,  1.1611]
-Original Grad: -0.004, -lr * Pred Grad: 0.013, New P: -0.229
-Original Grad: 0.015, -lr * Pred Grad: -0.026, New P: 1.135
iter 3 loss: 0.017
Actual params: [-0.2291,  1.1354]
-Original Grad: -0.007, -lr * Pred Grad: -0.059, New P: -0.288
-Original Grad: 0.026, -lr * Pred Grad: 0.055, New P: 1.190
iter 4 loss: 0.016
Actual params: [-0.2879,  1.1904]
-Original Grad: -0.005, -lr * Pred Grad: -0.057, New P: -0.345
-Original Grad: 0.018, -lr * Pred Grad: 0.034, New P: 1.224
iter 5 loss: 0.016
Actual params: [-0.3453,  1.224 ]
-Original Grad: -0.002, -lr * Pred Grad: -0.052, New P: -0.397
-Original Grad: 0.008, -lr * Pred Grad: 0.006, New P: 1.230
iter 6 loss: 0.016
Actual params: [-0.3974,  1.2304]
-Original Grad: -0.002, -lr * Pred Grad: -0.050, New P: -0.447
-Original Grad: 0.006, -lr * Pred Grad: -0.002, New P: 1.228
iter 7 loss: 0.015
Actual params: [-0.4473,  1.2282]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: -0.495
-Original Grad: 0.004, -lr * Pred Grad: -0.009, New P: 1.220
iter 8 loss: 0.015
Actual params: [-0.4952,  1.2196]
-Original Grad: -0.001, -lr * Pred Grad: -0.046, New P: -0.541
-Original Grad: 0.002, -lr * Pred Grad: -0.015, New P: 1.205
iter 9 loss: 0.015
Actual params: [-0.541,  1.205]
-Original Grad: -0.001, -lr * Pred Grad: -0.046, New P: -0.587
-Original Grad: 0.002, -lr * Pred Grad: -0.016, New P: 1.189
iter 10 loss: 0.015
Actual params: [-0.5869,  1.1887]
-Original Grad: -0.001, -lr * Pred Grad: -0.046, New P: -0.633
-Original Grad: 0.002, -lr * Pred Grad: -0.019, New P: 1.170
iter 11 loss: 0.015
Actual params: [-0.633 ,  1.1697]
-Original Grad: -0.001, -lr * Pred Grad: -0.046, New P: -0.679
-Original Grad: 0.001, -lr * Pred Grad: -0.021, New P: 1.148
iter 12 loss: 0.015
Actual params: [-0.6791,  1.1482]
-Original Grad: -0.001, -lr * Pred Grad: -0.046, New P: -0.725
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 1.125
iter 13 loss: 0.015
Actual params: [-0.725 ,  1.1245]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -0.771
-Original Grad: 0.000, -lr * Pred Grad: -0.026, New P: 1.099
iter 14 loss: 0.015
Actual params: [-0.7706,  1.0985]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -0.816
-Original Grad: 0.000, -lr * Pred Grad: -0.027, New P: 1.071
iter 15 loss: 0.015
Actual params: [-0.8163,  1.0712]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -0.862
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 1.043
iter 16 loss: 0.015
Actual params: [-0.8616,  1.0427]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -0.907
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 1.013
iter 17 loss: 0.015
Actual params: [-0.9069,  1.0135]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -0.952
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.983
iter 18 loss: 0.015
Actual params: [-0.9519,  0.9832]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -0.997
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.953
iter 19 loss: 0.015
Actual params: [-0.9968,  0.9526]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.041
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.921
iter 20 loss: 0.015
Actual params: [-1.0413,  0.9213]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.086
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.890
iter 21 loss: 0.015
Actual params: [-1.0856,  0.8895]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.130
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.858
iter 22 loss: 0.015
Actual params: [-1.1298,  0.8575]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -1.174
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.825
iter 23 loss: 0.015
Actual params: [-1.1737,  0.8253]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.217
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.793
iter 24 loss: 0.015
Actual params: [-1.2175,  0.793 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -1.261
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.761
iter 25 loss: 0.015
Actual params: [-1.2611,  0.7607]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.305
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.728
iter 26 loss: 0.015
Actual params: [-1.3047,  0.7283]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -1.348
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.696
iter 27 loss: 0.015
Actual params: [-1.3482,  0.6958]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.392
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.663
iter 28 loss: 0.015
Actual params: [-1.3918,  0.6634]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.435
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.631
iter 29 loss: 0.015
Actual params: [-1.4353,  0.6309]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.479
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.598
iter 30 loss: 0.015
Actual params: [-1.4788,  0.5984]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.522
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.566
Target params: [1.3344, 1.5708]
iter 0 loss: 0.095
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.085, -lr * Pred Grad: -0.294, New P: -0.766
-Original Grad: 0.151, -lr * Pred Grad: 1.302, New P: 1.305
iter 1 loss: 0.080
Actual params: [-0.7662,  1.3051]
-Original Grad: 0.005, -lr * Pred Grad: 0.210, New P: -0.556
-Original Grad: 0.000, -lr * Pred Grad: 0.057, New P: 1.362
iter 2 loss: 0.078
Actual params: [-0.556 ,  1.3618]
-Original Grad: 0.020, -lr * Pred Grad: 0.194, New P: -0.362
-Original Grad: 0.003, -lr * Pred Grad: -0.059, New P: 1.303
iter 3 loss: 0.073
Actual params: [-0.3621,  1.3032]
-Original Grad: 0.026, -lr * Pred Grad: 0.248, New P: -0.114
-Original Grad: 0.018, -lr * Pred Grad: 0.037, New P: 1.340
iter 4 loss: 0.066
Actual params: [-0.1145,  1.3404]
-Original Grad: 0.023, -lr * Pred Grad: 0.230, New P: 0.115
-Original Grad: 0.094, -lr * Pred Grad: 0.236, New P: 1.576
iter 5 loss: 0.046
Actual params: [0.1151, 1.5763]
-Original Grad: 0.060, -lr * Pred Grad: 0.375, New P: 0.490
-Original Grad: 0.095, -lr * Pred Grad: 0.364, New P: 1.941
iter 6 loss: 0.011
Actual params: [0.49  , 1.9407]
-Original Grad: 0.014, -lr * Pred Grad: 0.213, New P: 0.703
-Original Grad: 0.039, -lr * Pred Grad: 0.199, New P: 2.139
iter 7 loss: 0.013
Actual params: [0.7026, 2.1394]
-Original Grad: -0.064, -lr * Pred Grad: -0.106, New P: 0.597
-Original Grad: -0.126, -lr * Pred Grad: -0.239, New P: 1.901
iter 8 loss: 0.011
Actual params: [0.5967, 1.9008]
-Original Grad: 0.005, -lr * Pred Grad: -0.007, New P: 0.590
-Original Grad: 0.037, -lr * Pred Grad: -0.024, New P: 1.876
iter 9 loss: 0.012
Actual params: [0.5901, 1.8764]
-Original Grad: 0.006, -lr * Pred Grad: -0.001, New P: 0.589
-Original Grad: 0.057, -lr * Pred Grad: 0.106, New P: 1.982
iter 10 loss: 0.009
Actual params: [0.5891, 1.9823]
-Original Grad: -0.006, -lr * Pred Grad: -0.030, New P: 0.559
-Original Grad: 0.021, -lr * Pred Grad: 0.083, New P: 2.065
iter 11 loss: 0.008
Actual params: [0.559 , 2.0651]
-Original Grad: -0.013, -lr * Pred Grad: -0.065, New P: 0.494
-Original Grad: -0.015, -lr * Pred Grad: -0.023, New P: 2.042
iter 12 loss: 0.009
Actual params: [0.4943, 2.0418]
-Original Grad: 0.009, -lr * Pred Grad: -0.025, New P: 0.470
-Original Grad: 0.062, -lr * Pred Grad: 0.145, New P: 2.187
iter 13 loss: 0.008
Actual params: [0.4696, 2.1866]
-Original Grad: 0.032, -lr * Pred Grad: 0.057, New P: 0.526
-Original Grad: 0.004, -lr * Pred Grad: 0.039, New P: 2.225
iter 14 loss: 0.009
Actual params: [0.5264, 2.2252]
-Original Grad: -0.009, -lr * Pred Grad: -0.013, New P: 0.514
-Original Grad: -0.042, -lr * Pred Grad: -0.094, New P: 2.131
iter 15 loss: 0.008
Actual params: [0.5137, 2.1314]
-Original Grad: -0.018, -lr * Pred Grad: -0.066, New P: 0.448
-Original Grad: -0.005, -lr * Pred Grad: -0.063, New P: 2.069
iter 16 loss: 0.009
Actual params: [0.4481, 2.0688]
-Original Grad: 0.017, -lr * Pred Grad: 0.001, New P: 0.449
-Original Grad: 0.016, -lr * Pred Grad: -0.010, New P: 2.059
iter 17 loss: 0.009
Actual params: [0.4488, 2.0588]
-Original Grad: 0.011, -lr * Pred Grad: 0.011, New P: 0.459
-Original Grad: 0.037, -lr * Pred Grad: 0.074, New P: 2.133
iter 18 loss: 0.008
Actual params: [0.4594, 2.1327]
-Original Grad: 0.013, -lr * Pred Grad: 0.028, New P: 0.487
-Original Grad: -0.006, -lr * Pred Grad: -0.010, New P: 2.122
iter 19 loss: 0.008
Actual params: [0.487 , 2.1222]
-Original Grad: 0.013, -lr * Pred Grad: 0.039, New P: 0.526
-Original Grad: 0.004, -lr * Pred Grad: -0.006, New P: 2.116
iter 20 loss: 0.008
Actual params: [0.5257, 2.1158]
-Original Grad: -0.013, -lr * Pred Grad: -0.029, New P: 0.497
-Original Grad: 0.008, -lr * Pred Grad: -0.002, New P: 2.114
iter 21 loss: 0.008
Actual params: [0.497 , 2.1135]
-Original Grad: -0.004, -lr * Pred Grad: -0.033, New P: 0.463
-Original Grad: -0.017, -lr * Pred Grad: -0.059, New P: 2.055
iter 22 loss: 0.009
Actual params: [0.4635, 2.0545]
-Original Grad: 0.013, -lr * Pred Grad: 0.002, New P: 0.466
-Original Grad: 0.021, -lr * Pred Grad: 0.010, New P: 2.064
iter 23 loss: 0.009
Actual params: [0.4659, 2.0642]
-Original Grad: 0.042, -lr * Pred Grad: 0.102, New P: 0.568
-Original Grad: 0.041, -lr * Pred Grad: 0.091, New P: 2.155
iter 24 loss: 0.009
Actual params: [0.5678, 2.1554]
-Original Grad: -0.036, -lr * Pred Grad: -0.068, New P: 0.500
-Original Grad: -0.057, -lr * Pred Grad: -0.124, New P: 2.031
iter 25 loss: 0.009
Actual params: [0.5002, 2.031 ]
-Original Grad: 0.031, -lr * Pred Grad: 0.052, New P: 0.552
-Original Grad: 0.031, -lr * Pred Grad: 0.012, New P: 2.043
iter 26 loss: 0.008
Actual params: [0.5525, 2.0426]
-Original Grad: 0.002, -lr * Pred Grad: 0.010, New P: 0.562
-Original Grad: 0.008, -lr * Pred Grad: -0.004, New P: 2.039
iter 27 loss: 0.008
Actual params: [0.5623, 2.0391]
-Original Grad: -0.001, -lr * Pred Grad: -0.004, New P: 0.558
-Original Grad: 0.000, -lr * Pred Grad: -0.016, New P: 2.023
iter 28 loss: 0.008
Actual params: [0.5579, 2.0231]
-Original Grad: 0.000, -lr * Pred Grad: -0.013, New P: 0.545
-Original Grad: 0.004, -lr * Pred Grad: -0.015, New P: 2.008
iter 29 loss: 0.009
Actual params: [0.5453, 2.0081]
-Original Grad: 0.011, -lr * Pred Grad: 0.012, New P: 0.557
-Original Grad: 0.025, -lr * Pred Grad: 0.037, New P: 2.045
iter 30 loss: 0.008
Actual params: [0.5572, 2.0454]
-Original Grad: 0.009, -lr * Pred Grad: 0.016, New P: 0.573
-Original Grad: 0.009, -lr * Pred Grad: 0.016, New P: 2.062
Target params: [1.3344, 1.5708]
iter 0 loss: 0.432
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.015, -lr * Pred Grad: 0.486, New P: 0.013
-Original Grad: -0.005, -lr * Pred Grad: 0.610, New P: 0.613
iter 1 loss: 0.279
Actual params: [0.0132, 0.6133]
-Original Grad: 0.832, -lr * Pred Grad: 2.486, New P: 2.500
-Original Grad: 0.741, -lr * Pred Grad: 2.642, New P: 3.256
iter 2 loss: 0.404
Actual params: [2.4996, 3.2556]
-Original Grad: -0.008, -lr * Pred Grad: -0.975, New P: 1.525
-Original Grad: -0.372, -lr * Pred Grad: -1.022, New P: 2.233
iter 3 loss: 0.117
Actual params: [1.5248, 2.2333]
-Original Grad: -0.206, -lr * Pred Grad: -0.358, New P: 1.166
-Original Grad: -0.403, -lr * Pred Grad: -0.262, New P: 1.972
iter 4 loss: 0.028
Actual params: [1.1663, 1.9716]
-Original Grad: -0.025, -lr * Pred Grad: -0.183, New P: 0.983
-Original Grad: -0.290, -lr * Pred Grad: -0.547, New P: 1.425
iter 5 loss: 0.022
Actual params: [0.9832, 1.4251]
-Original Grad: -0.095, -lr * Pred Grad: -0.332, New P: 0.651
-Original Grad: 0.206, -lr * Pred Grad: -0.079, New P: 1.346
iter 6 loss: 0.014
Actual params: [0.651 , 1.3459]
-Original Grad: 0.043, -lr * Pred Grad: -0.116, New P: 0.535
-Original Grad: 0.120, -lr * Pred Grad: 0.185, New P: 1.531
iter 7 loss: 0.029
Actual params: [0.5346, 1.5307]
-Original Grad: 0.120, -lr * Pred Grad: 0.152, New P: 0.687
-Original Grad: -0.052, -lr * Pred Grad: -0.120, New P: 1.411
iter 8 loss: 0.010
Actual params: [0.6868, 1.4107]
-Original Grad: 0.020, -lr * Pred Grad: 0.094, New P: 0.781
-Original Grad: 0.063, -lr * Pred Grad: 0.086, New P: 1.496
iter 9 loss: 0.005
Actual params: [0.781 , 1.4963]
-Original Grad: -0.030, -lr * Pred Grad: -0.016, New P: 0.765
-Original Grad: 0.044, -lr * Pred Grad: 0.123, New P: 1.619
iter 10 loss: 0.005
Actual params: [0.7653, 1.6194]
-Original Grad: 0.042, -lr * Pred Grad: 0.112, New P: 0.877
-Original Grad: -0.059, -lr * Pred Grad: -0.119, New P: 1.501
iter 11 loss: 0.008
Actual params: [0.8773, 1.5007]
-Original Grad: -0.040, -lr * Pred Grad: -0.068, New P: 0.810
-Original Grad: 0.074, -lr * Pred Grad: 0.125, New P: 1.626
iter 12 loss: 0.004
Actual params: [0.8096, 1.626 ]
-Original Grad: 0.009, -lr * Pred Grad: -0.009, New P: 0.800
-Original Grad: -0.030, -lr * Pred Grad: -0.063, New P: 1.563
iter 13 loss: 0.004
Actual params: [0.8004, 1.5634]
-Original Grad: -0.008, -lr * Pred Grad: -0.044, New P: 0.756
-Original Grad: 0.010, -lr * Pred Grad: -0.006, New P: 1.558
iter 14 loss: 0.005
Actual params: [0.7562, 1.5578]
-Original Grad: 0.024, -lr * Pred Grad: 0.028, New P: 0.784
-Original Grad: -0.002, -lr * Pred Grad: -0.032, New P: 1.526
iter 15 loss: 0.005
Actual params: [0.7844, 1.5261]
-Original Grad: -0.006, -lr * Pred Grad: -0.022, New P: 0.763
-Original Grad: 0.028, -lr * Pred Grad: 0.042, New P: 1.569
iter 16 loss: 0.005
Actual params: [0.7628, 1.5685]
-Original Grad: 0.019, -lr * Pred Grad: 0.031, New P: 0.794
-Original Grad: -0.007, -lr * Pred Grad: -0.025, New P: 1.543
iter 17 loss: 0.004
Actual params: [0.7942, 1.5435]
-Original Grad: -0.004, -lr * Pred Grad: -0.012, New P: 0.782
-Original Grad: 0.018, -lr * Pred Grad: 0.021, New P: 1.565
iter 18 loss: 0.004
Actual params: [0.7822, 1.5649]
-Original Grad: -0.002, -lr * Pred Grad: -0.023, New P: 0.760
-Original Grad: 0.019, -lr * Pred Grad: 0.037, New P: 1.602
iter 19 loss: 0.005
Actual params: [0.7596, 1.6016]
-Original Grad: 0.040, -lr * Pred Grad: 0.085, New P: 0.845
-Original Grad: -0.033, -lr * Pred Grad: -0.082, New P: 1.519
iter 20 loss: 0.006
Actual params: [0.8445, 1.5191]
-Original Grad: -0.034, -lr * Pred Grad: -0.071, New P: 0.774
-Original Grad: 0.060, -lr * Pred Grad: 0.103, New P: 1.622
iter 21 loss: 0.005
Actual params: [0.774 , 1.6217]
-Original Grad: 0.012, -lr * Pred Grad: -0.007, New P: 0.767
-Original Grad: -0.051, -lr * Pred Grad: -0.113, New P: 1.509
iter 22 loss: 0.005
Actual params: [0.7668, 1.5088]
-Original Grad: 0.012, -lr * Pred Grad: 0.009, New P: 0.775
-Original Grad: 0.027, -lr * Pred Grad: 0.009, New P: 1.518
iter 23 loss: 0.005
Actual params: [0.7754, 1.5182]
-Original Grad: -0.002, -lr * Pred Grad: -0.016, New P: 0.759
-Original Grad: 0.034, -lr * Pred Grad: 0.065, New P: 1.583
iter 24 loss: 0.005
Actual params: [0.7592, 1.5831]
-Original Grad: 0.027, -lr * Pred Grad: 0.055, New P: 0.815
-Original Grad: -0.019, -lr * Pred Grad: -0.043, New P: 1.540
iter 25 loss: 0.004
Actual params: [0.8146, 1.5398]
-Original Grad: -0.008, -lr * Pred Grad: -0.010, New P: 0.804
-Original Grad: 0.030, -lr * Pred Grad: 0.044, New P: 1.584
iter 26 loss: 0.004
Actual params: [0.8042, 1.5837]
-Original Grad: -0.004, -lr * Pred Grad: -0.025, New P: 0.780
-Original Grad: -0.001, -lr * Pred Grad: -0.009, New P: 1.575
iter 27 loss: 0.004
Actual params: [0.7796, 1.5749]
-Original Grad: 0.023, -lr * Pred Grad: 0.039, New P: 0.818
-Original Grad: -0.010, -lr * Pred Grad: -0.042, New P: 1.533
iter 28 loss: 0.005
Actual params: [0.8183, 1.5331]
-Original Grad: -0.006, -lr * Pred Grad: -0.013, New P: 0.805
-Original Grad: 0.028, -lr * Pred Grad: 0.034, New P: 1.567
iter 29 loss: 0.004
Actual params: [0.8052, 1.5673]
-Original Grad: -0.010, -lr * Pred Grad: -0.046, New P: 0.759
-Original Grad: 0.019, -lr * Pred Grad: 0.041, New P: 1.609
iter 30 loss: 0.005
Actual params: [0.7593, 1.6087]
-Original Grad: 0.073, -lr * Pred Grad: 0.165, New P: 0.924
-Original Grad: -0.030, -lr * Pred Grad: -0.075, New P: 1.533
Target params: [1.3344, 1.5708]
iter 0 loss: 0.274
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.018, -lr * Pred Grad: 0.448, New P: -0.024
-Original Grad: 0.512, -lr * Pred Grad: 2.189, New P: 2.192
iter 1 loss: 0.225
Actual params: [-0.0245,  2.1925]
-Original Grad: 0.126, -lr * Pred Grad: 1.416, New P: 1.392
-Original Grad: -0.110, -lr * Pred Grad: -0.436, New P: 1.756
iter 2 loss: 0.059
Actual params: [1.3919, 1.7565]
-Original Grad: -0.015, -lr * Pred Grad: -0.029, New P: 1.363
-Original Grad: -0.199, -lr * Pred Grad: -0.347, New P: 1.409
iter 3 loss: 0.021
Actual params: [1.3632, 1.4091]
-Original Grad: 0.011, -lr * Pred Grad: 0.348, New P: 1.711
-Original Grad: -0.044, -lr * Pred Grad: -0.254, New P: 1.155
iter 4 loss: 0.006
Actual params: [1.7108, 1.1553]
-Original Grad: 0.017, -lr * Pred Grad: 0.201, New P: 1.912
-Original Grad: -0.013, -lr * Pred Grad: -0.178, New P: 0.978
iter 5 loss: 0.006
Actual params: [1.9118, 0.9776]
-Original Grad: 0.003, -lr * Pred Grad: 0.133, New P: 2.045
-Original Grad: 0.030, -lr * Pred Grad: -0.042, New P: 0.936
iter 6 loss: 0.007
Actual params: [2.0453, 0.9356]
-Original Grad: -0.003, -lr * Pred Grad: 0.072, New P: 2.117
-Original Grad: 0.039, -lr * Pred Grad: 0.051, New P: 0.987
iter 7 loss: 0.008
Actual params: [2.1172, 0.9868]
-Original Grad: -0.015, -lr * Pred Grad: -0.000, New P: 2.117
-Original Grad: -0.008, -lr * Pred Grad: -0.024, New P: 0.963
iter 8 loss: 0.008
Actual params: [2.1171, 0.9632]
-Original Grad: -0.014, -lr * Pred Grad: -0.036, New P: 2.081
-Original Grad: 0.007, -lr * Pred Grad: -0.007, New P: 0.956
iter 9 loss: 0.007
Actual params: [2.0809, 0.9563]
-Original Grad: -0.010, -lr * Pred Grad: -0.054, New P: 2.027
-Original Grad: 0.028, -lr * Pred Grad: 0.047, New P: 1.003
iter 10 loss: 0.006
Actual params: [2.0266, 1.0029]
-Original Grad: -0.010, -lr * Pred Grad: -0.070, New P: 1.956
-Original Grad: -0.010, -lr * Pred Grad: -0.028, New P: 0.975
iter 11 loss: 0.006
Actual params: [1.9564, 0.9747]
-Original Grad: 0.005, -lr * Pred Grad: -0.043, New P: 1.913
-Original Grad: 0.044, -lr * Pred Grad: 0.088, New P: 1.063
iter 12 loss: 0.005
Actual params: [1.9131, 1.0632]
-Original Grad: -0.012, -lr * Pred Grad: -0.079, New P: 1.834
-Original Grad: -0.001, -lr * Pred Grad: 0.005, New P: 1.068
iter 13 loss: 0.005
Actual params: [1.8336, 1.0679]
-Original Grad: 0.022, -lr * Pred Grad: -0.001, New P: 1.833
-Original Grad: 0.033, -lr * Pred Grad: 0.076, New P: 1.144
iter 14 loss: 0.006
Actual params: [1.8325, 1.1436]
-Original Grad: -0.001, -lr * Pred Grad: -0.027, New P: 1.806
-Original Grad: -0.012, -lr * Pred Grad: -0.025, New P: 1.118
iter 15 loss: 0.005
Actual params: [1.8059, 1.1185]
-Original Grad: 0.004, -lr * Pred Grad: -0.019, New P: 1.787
-Original Grad: 0.014, -lr * Pred Grad: 0.012, New P: 1.131
iter 16 loss: 0.005
Actual params: [1.7873, 1.1308]
-Original Grad: 0.006, -lr * Pred Grad: -0.009, New P: 1.779
-Original Grad: -0.009, -lr * Pred Grad: -0.039, New P: 1.092
iter 17 loss: 0.005
Actual params: [1.7786, 1.0918]
-Original Grad: 0.002, -lr * Pred Grad: -0.017, New P: 1.762
-Original Grad: 0.018, -lr * Pred Grad: 0.012, New P: 1.104
iter 18 loss: 0.006
Actual params: [1.7619, 1.1043]
-Original Grad: 0.026, -lr * Pred Grad: 0.049, New P: 1.811
-Original Grad: 0.027, -lr * Pred Grad: 0.053, New P: 1.158
iter 19 loss: 0.006
Actual params: [1.8112, 1.1576]
-Original Grad: 0.005, -lr * Pred Grad: 0.024, New P: 1.835
-Original Grad: -0.023, -lr * Pred Grad: -0.056, New P: 1.102
iter 20 loss: 0.005
Actual params: [1.8348, 1.1017]
-Original Grad: 0.008, -lr * Pred Grad: 0.029, New P: 1.863
-Original Grad: 0.017, -lr * Pred Grad: 0.005, New P: 1.106
iter 21 loss: 0.005
Actual params: [1.8634, 1.1064]
-Original Grad: -0.011, -lr * Pred Grad: -0.027, New P: 1.837
-Original Grad: -0.035, -lr * Pred Grad: -0.101, New P: 1.005
iter 22 loss: 0.006
Actual params: [1.8368, 1.0055]
-Original Grad: 0.021, -lr * Pred Grad: 0.038, New P: 1.875
-Original Grad: 0.058, -lr * Pred Grad: 0.085, New P: 1.091
iter 23 loss: 0.005
Actual params: [1.8748, 1.0907]
-Original Grad: -0.005, -lr * Pred Grad: -0.012, New P: 1.863
-Original Grad: -0.012, -lr * Pred Grad: -0.026, New P: 1.065
iter 24 loss: 0.005
Actual params: [1.8626, 1.0649]
-Original Grad: 0.014, -lr * Pred Grad: 0.025, New P: 1.888
-Original Grad: 0.014, -lr * Pred Grad: 0.015, New P: 1.080
iter 25 loss: 0.005
Actual params: [1.8876, 1.0801]
-Original Grad: -0.014, -lr * Pred Grad: -0.043, New P: 1.845
-Original Grad: -0.004, -lr * Pred Grad: -0.026, New P: 1.054
iter 26 loss: 0.005
Actual params: [1.845 , 1.0544]
-Original Grad: 0.003, -lr * Pred Grad: -0.024, New P: 1.821
-Original Grad: 0.013, -lr * Pred Grad: 0.007, New P: 1.061
iter 27 loss: 0.005
Actual params: [1.8212, 1.061 ]
-Original Grad: 0.015, -lr * Pred Grad: 0.013, New P: 1.834
-Original Grad: 0.041, -lr * Pred Grad: 0.091, New P: 1.152
iter 28 loss: 0.006
Actual params: [1.8343, 1.1518]
-Original Grad: -0.006, -lr * Pred Grad: -0.026, New P: 1.808
-Original Grad: -0.030, -lr * Pred Grad: -0.065, New P: 1.086
iter 29 loss: 0.005
Actual params: [1.8082, 1.0864]
-Original Grad: 0.021, -lr * Pred Grad: 0.032, New P: 1.840
-Original Grad: 0.028, -lr * Pred Grad: 0.033, New P: 1.119
iter 30 loss: 0.005
Actual params: [1.8404, 1.1191]
-Original Grad: 0.001, -lr * Pred Grad: 0.002, New P: 1.842
-Original Grad: -0.036, -lr * Pred Grad: -0.097, New P: 1.022
Target params: [1.3344, 1.5708]
iter 0 loss: 0.365
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.019, -lr * Pred Grad: 0.443, New P: -0.030
-Original Grad: -0.024, -lr * Pred Grad: 0.519, New P: 0.522
iter 1 loss: 0.402
Actual params: [-0.0298,  0.5221]
-Original Grad: -0.052, -lr * Pred Grad: -0.223, New P: -0.252
-Original Grad: 0.038, -lr * Pred Grad: 0.324, New P: 0.846
iter 2 loss: 0.364
Actual params: [-0.2524,  0.8461]
-Original Grad: -0.014, -lr * Pred Grad: -0.052, New P: -0.304
-Original Grad: 0.086, -lr * Pred Grad: 0.342, New P: 1.188
iter 3 loss: 0.344
Actual params: [-0.3042,  1.1877]
-Original Grad: 0.023, -lr * Pred Grad: 0.092, New P: -0.212
-Original Grad: 0.071, -lr * Pred Grad: 0.263, New P: 1.451
iter 4 loss: 0.328
Actual params: [-0.212 ,  1.4511]
-Original Grad: 0.046, -lr * Pred Grad: 0.222, New P: 0.010
-Original Grad: 0.027, -lr * Pred Grad: 0.089, New P: 1.540
iter 5 loss: 0.302
Actual params: [0.0096, 1.5398]
-Original Grad: 0.107, -lr * Pred Grad: 0.505, New P: 0.515
-Original Grad: 0.015, -lr * Pred Grad: 0.050, New P: 1.590
iter 6 loss: 0.147
Actual params: [0.5149, 1.5899]
-Original Grad: 0.072, -lr * Pred Grad: 0.472, New P: 0.987
-Original Grad: 0.117, -lr * Pred Grad: 0.357, New P: 1.946
iter 7 loss: 0.042
Actual params: [0.9873, 1.9464]
-Original Grad: 0.070, -lr * Pred Grad: 0.503, New P: 1.490
-Original Grad: 0.050, -lr * Pred Grad: 0.251, New P: 2.197
iter 8 loss: 0.041
Actual params: [1.4904, 2.1972]
-Original Grad: -0.085, -lr * Pred Grad: -0.089, New P: 1.402
-Original Grad: -0.133, -lr * Pred Grad: -0.256, New P: 1.942
iter 9 loss: 0.018
Actual params: [1.4019, 1.9416]
-Original Grad: 0.031, -lr * Pred Grad: 0.136, New P: 1.538
-Original Grad: -0.002, -lr * Pred Grad: -0.095, New P: 1.847
iter 10 loss: 0.017
Actual params: [1.5376, 1.8466]
-Original Grad: -0.041, -lr * Pred Grad: -0.070, New P: 1.468
-Original Grad: -0.095, -lr * Pred Grad: -0.238, New P: 1.608
iter 11 loss: 0.010
Actual params: [1.468 , 1.6085]
-Original Grad: 0.046, -lr * Pred Grad: 0.102, New P: 1.570
-Original Grad: 0.003, -lr * Pred Grad: -0.124, New P: 1.484
iter 12 loss: 0.006
Actual params: [1.5698, 1.4842]
-Original Grad: 0.014, -lr * Pred Grad: 0.069, New P: 1.639
-Original Grad: -0.002, -lr * Pred Grad: -0.083, New P: 1.401
iter 13 loss: 0.006
Actual params: [1.6387, 1.4012]
-Original Grad: 0.015, -lr * Pred Grad: 0.080, New P: 1.719
-Original Grad: 0.042, -lr * Pred Grad: 0.045, New P: 1.446
iter 14 loss: 0.009
Actual params: [1.7187, 1.446 ]
-Original Grad: -0.044, -lr * Pred Grad: -0.085, New P: 1.633
-Original Grad: -0.066, -lr * Pred Grad: -0.154, New P: 1.292
iter 15 loss: 0.015
Actual params: [1.6333, 1.2921]
-Original Grad: 0.018, -lr * Pred Grad: 0.005, New P: 1.638
-Original Grad: 0.131, -lr * Pred Grad: 0.256, New P: 1.548
iter 16 loss: 0.008
Actual params: [1.6381, 1.5481]
-Original Grad: -0.033, -lr * Pred Grad: -0.109, New P: 1.529
-Original Grad: -0.056, -lr * Pred Grad: -0.106, New P: 1.443
iter 17 loss: 0.009
Actual params: [1.529 , 1.4426]
-Original Grad: 0.050, -lr * Pred Grad: 0.063, New P: 1.592
-Original Grad: 0.037, -lr * Pred Grad: 0.057, New P: 1.500
iter 18 loss: 0.006
Actual params: [1.5919, 1.4999]
-Original Grad: -0.005, -lr * Pred Grad: -0.011, New P: 1.581
-Original Grad: -0.031, -lr * Pred Grad: -0.085, New P: 1.415
iter 19 loss: 0.007
Actual params: [1.581 , 1.4152]
-Original Grad: 0.014, -lr * Pred Grad: 0.027, New P: 1.608
-Original Grad: 0.037, -lr * Pred Grad: 0.046, New P: 1.461
iter 20 loss: 0.006
Actual params: [1.6077, 1.4614]
-Original Grad: 0.009, -lr * Pred Grad: 0.024, New P: 1.632
-Original Grad: -0.010, -lr * Pred Grad: -0.034, New P: 1.428
iter 21 loss: 0.006
Actual params: [1.6318, 1.4277]
-Original Grad: -0.001, -lr * Pred Grad: -0.002, New P: 1.630
-Original Grad: -0.006, -lr * Pred Grad: -0.040, New P: 1.388
iter 22 loss: 0.007
Actual params: [1.6301, 1.3878]
-Original Grad: 0.015, -lr * Pred Grad: 0.032, New P: 1.662
-Original Grad: 0.039, -lr * Pred Grad: 0.062, New P: 1.449
iter 23 loss: 0.006
Actual params: [1.6625, 1.4494]
-Original Grad: -0.026, -lr * Pred Grad: -0.070, New P: 1.592
-Original Grad: -0.039, -lr * Pred Grad: -0.094, New P: 1.355
iter 24 loss: 0.011
Actual params: [1.5921, 1.3554]
-Original Grad: 0.040, -lr * Pred Grad: 0.064, New P: 1.657
-Original Grad: 0.086, -lr * Pred Grad: 0.174, New P: 1.529
iter 25 loss: 0.008
Actual params: [1.6565, 1.529 ]
-Original Grad: -0.039, -lr * Pred Grad: -0.100, New P: 1.557
-Original Grad: -0.061, -lr * Pred Grad: -0.127, New P: 1.402
iter 26 loss: 0.009
Actual params: [1.5565, 1.4021]
-Original Grad: 0.026, -lr * Pred Grad: 0.013, New P: 1.569
-Original Grad: 0.062, -lr * Pred Grad: 0.099, New P: 1.501
iter 27 loss: 0.006
Actual params: [1.569 , 1.5014]
-Original Grad: 0.010, -lr * Pred Grad: 0.010, New P: 1.579
-Original Grad: -0.011, -lr * Pred Grad: -0.024, New P: 1.477
iter 28 loss: 0.006
Actual params: [1.579 , 1.4769]
-Original Grad: 0.003, -lr * Pred Grad: -0.001, New P: 1.578
-Original Grad: -0.013, -lr * Pred Grad: -0.048, New P: 1.429
iter 29 loss: 0.007
Actual params: [1.5783, 1.4286]
-Original Grad: 0.013, -lr * Pred Grad: 0.025, New P: 1.603
-Original Grad: 0.033, -lr * Pred Grad: 0.039, New P: 1.468
iter 30 loss: 0.006
Actual params: [1.6029, 1.468 ]
-Original Grad: 0.002, -lr * Pred Grad: 0.005, New P: 1.607
-Original Grad: -0.002, -lr * Pred Grad: -0.011, New P: 1.457
Target params: [1.3344, 1.5708]
iter 0 loss: 0.104
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.036, -lr * Pred Grad: 0.999, New P: 0.527
-Original Grad: 0.064, -lr * Pred Grad: 0.934, New P: 0.938
iter 1 loss: 0.050
Actual params: [0.5267, 0.9377]
-Original Grad: -0.026, -lr * Pred Grad: 0.029, New P: 0.556
-Original Grad: 0.036, -lr * Pred Grad: 0.308, New P: 1.246
iter 2 loss: 0.050
Actual params: [0.5556, 1.2461]
-Original Grad: 0.043, -lr * Pred Grad: 0.584, New P: 1.139
-Original Grad: -0.040, -lr * Pred Grad: -0.243, New P: 1.003
iter 3 loss: 0.019
Actual params: [1.1393, 1.0032]
-Original Grad: 0.065, -lr * Pred Grad: 0.527, New P: 1.667
-Original Grad: 0.102, -lr * Pred Grad: 0.244, New P: 1.248
iter 4 loss: 0.020
Actual params: [1.6667, 1.2475]
-Original Grad: -0.019, -lr * Pred Grad: 0.117, New P: 1.783
-Original Grad: -0.092, -lr * Pred Grad: -0.196, New P: 1.051
iter 5 loss: 0.016
Actual params: [1.7833, 1.051 ]
-Original Grad: -0.014, -lr * Pred Grad: 0.067, New P: 1.850
-Original Grad: -0.053, -lr * Pred Grad: -0.161, New P: 0.890
iter 6 loss: 0.014
Actual params: [1.85  , 0.8896]
-Original Grad: -0.010, -lr * Pred Grad: 0.014, New P: 1.864
-Original Grad: -0.011, -lr * Pred Grad: -0.142, New P: 0.748
iter 7 loss: 0.014
Actual params: [1.8636, 0.7481]
-Original Grad: 0.000, -lr * Pred Grad: 0.013, New P: 1.876
-Original Grad: 0.012, -lr * Pred Grad: -0.052, New P: 0.696
iter 8 loss: 0.015
Actual params: [1.8764, 0.6959]
-Original Grad: 0.007, -lr * Pred Grad: 0.022, New P: 1.898
-Original Grad: 0.026, -lr * Pred Grad: 0.017, New P: 0.713
iter 9 loss: 0.015
Actual params: [1.8982, 0.7129]
-Original Grad: 0.001, -lr * Pred Grad: 0.006, New P: 1.904
-Original Grad: 0.015, -lr * Pred Grad: 0.023, New P: 0.736
iter 10 loss: 0.014
Actual params: [1.9043, 0.7358]
-Original Grad: -0.009, -lr * Pred Grad: -0.031, New P: 1.873
-Original Grad: 0.022, -lr * Pred Grad: 0.046, New P: 0.782
iter 11 loss: 0.014
Actual params: [1.8731, 0.782 ]
-Original Grad: 0.002, -lr * Pred Grad: -0.020, New P: 1.853
-Original Grad: 0.020, -lr * Pred Grad: 0.051, New P: 0.833
iter 12 loss: 0.014
Actual params: [1.8534, 0.8332]
-Original Grad: -0.009, -lr * Pred Grad: -0.052, New P: 1.801
-Original Grad: -0.011, -lr * Pred Grad: -0.027, New P: 0.806
iter 13 loss: 0.014
Actual params: [1.801 , 0.8061]
-Original Grad: 0.003, -lr * Pred Grad: -0.036, New P: 1.765
-Original Grad: 0.024, -lr * Pred Grad: 0.036, New P: 0.842
iter 14 loss: 0.014
Actual params: [1.7649, 0.8422]
-Original Grad: 0.002, -lr * Pred Grad: -0.034, New P: 1.731
-Original Grad: 0.006, -lr * Pred Grad: 0.008, New P: 0.850
iter 15 loss: 0.014
Actual params: [1.7313, 0.8504]
-Original Grad: -0.002, -lr * Pred Grad: -0.043, New P: 1.688
-Original Grad: 0.015, -lr * Pred Grad: 0.025, New P: 0.875
iter 16 loss: 0.014
Actual params: [1.6884, 0.8752]
-Original Grad: -0.005, -lr * Pred Grad: -0.055, New P: 1.634
-Original Grad: 0.010, -lr * Pred Grad: 0.016, New P: 0.891
iter 17 loss: 0.013
Actual params: [1.6336, 0.8908]
-Original Grad: -0.001, -lr * Pred Grad: -0.053, New P: 1.580
-Original Grad: -0.004, -lr * Pred Grad: -0.022, New P: 0.868
iter 18 loss: 0.014
Actual params: [1.5802, 0.8683]
-Original Grad: 0.005, -lr * Pred Grad: -0.037, New P: 1.543
-Original Grad: 0.033, -lr * Pred Grad: 0.058, New P: 0.927
iter 19 loss: 0.013
Actual params: [1.5431, 0.9268]
-Original Grad: -0.002, -lr * Pred Grad: -0.046, New P: 1.497
-Original Grad: 0.023, -lr * Pred Grad: 0.063, New P: 0.990
iter 20 loss: 0.013
Actual params: [1.4975, 0.9898]
-Original Grad: -0.004, -lr * Pred Grad: -0.056, New P: 1.442
-Original Grad: 0.015, -lr * Pred Grad: 0.047, New P: 1.036
iter 21 loss: 0.013
Actual params: [1.4417, 1.0363]
-Original Grad: -0.007, -lr * Pred Grad: -0.070, New P: 1.372
-Original Grad: -0.022, -lr * Pred Grad: -0.054, New P: 0.983
iter 22 loss: 0.013
Actual params: [1.3717, 0.9826]
-Original Grad: 0.005, -lr * Pred Grad: -0.046, New P: 1.326
-Original Grad: 0.034, -lr * Pred Grad: 0.050, New P: 1.033
iter 23 loss: 0.013
Actual params: [1.3257, 1.0329]
-Original Grad: 0.003, -lr * Pred Grad: -0.040, New P: 1.286
-Original Grad: 0.013, -lr * Pred Grad: 0.029, New P: 1.062
iter 24 loss: 0.013
Actual params: [1.2855, 1.0621]
-Original Grad: 0.014, -lr * Pred Grad: -0.002, New P: 1.284
-Original Grad: -0.003, -lr * Pred Grad: -0.013, New P: 1.049
iter 25 loss: 0.013
Actual params: [1.2838, 1.0494]
-Original Grad: 0.006, -lr * Pred Grad: -0.004, New P: 1.280
-Original Grad: 0.013, -lr * Pred Grad: 0.011, New P: 1.060
iter 26 loss: 0.013
Actual params: [1.2799, 1.0601]
-Original Grad: 0.015, -lr * Pred Grad: 0.027, New P: 1.307
-Original Grad: 0.004, -lr * Pred Grad: -0.005, New P: 1.055
iter 27 loss: 0.013
Actual params: [1.3073, 1.0551]
-Original Grad: 0.002, -lr * Pred Grad: 0.004, New P: 1.311
-Original Grad: 0.006, -lr * Pred Grad: -0.004, New P: 1.051
iter 28 loss: 0.013
Actual params: [1.3114, 1.0514]
-Original Grad: 0.005, -lr * Pred Grad: 0.005, New P: 1.316
-Original Grad: 0.017, -lr * Pred Grad: 0.023, New P: 1.074
iter 29 loss: 0.013
Actual params: [1.3165, 1.074 ]
-Original Grad: 0.004, -lr * Pred Grad: 0.003, New P: 1.319
-Original Grad: 0.001, -lr * Pred Grad: -0.008, New P: 1.066
iter 30 loss: 0.013
Actual params: [1.3195, 1.0662]
-Original Grad: 0.002, -lr * Pred Grad: -0.005, New P: 1.315
-Original Grad: 0.006, -lr * Pred Grad: -0.005, New P: 1.061
Target params: [1.3344, 1.5708]
iter 0 loss: 0.044
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.006, -lr * Pred Grad: 0.586, New P: 0.114
-Original Grad: 0.000, -lr * Pred Grad: 0.634, New P: 0.638
iter 1 loss: 0.057
Actual params: [0.1139, 0.638 ]
-Original Grad: -0.028, -lr * Pred Grad: 0.041, New P: 0.155
-Original Grad: 0.032, -lr * Pred Grad: 0.296, New P: 0.934
iter 2 loss: 0.050
Actual params: [0.1548, 0.9336]
-Original Grad: -0.016, -lr * Pred Grad: 0.030, New P: 0.185
-Original Grad: 0.049, -lr * Pred Grad: 0.146, New P: 1.079
iter 3 loss: 0.046
Actual params: [0.1845, 1.0793]
-Original Grad: -0.005, -lr * Pred Grad: 0.008, New P: 0.192
-Original Grad: 0.030, -lr * Pred Grad: 0.081, New P: 1.160
iter 4 loss: 0.043
Actual params: [0.1921, 1.1602]
-Original Grad: -0.002, -lr * Pred Grad: -0.008, New P: 0.184
-Original Grad: 0.036, -lr * Pred Grad: 0.097, New P: 1.257
iter 5 loss: 0.041
Actual params: [0.1842, 1.2568]
-Original Grad: 0.004, -lr * Pred Grad: 0.003, New P: 0.187
-Original Grad: 0.034, -lr * Pred Grad: 0.098, New P: 1.355
iter 6 loss: 0.038
Actual params: [0.187 , 1.3547]
-Original Grad: 0.011, -lr * Pred Grad: 0.028, New P: 0.216
-Original Grad: 0.038, -lr * Pred Grad: 0.119, New P: 1.474
iter 7 loss: 0.036
Actual params: [0.2155, 1.4736]
-Original Grad: 0.016, -lr * Pred Grad: 0.055, New P: 0.270
-Original Grad: 0.029, -lr * Pred Grad: 0.103, New P: 1.577
iter 8 loss: 0.032
Actual params: [0.2702, 1.5768]
-Original Grad: 0.026, -lr * Pred Grad: 0.101, New P: 0.371
-Original Grad: 0.026, -lr * Pred Grad: 0.093, New P: 1.670
iter 9 loss: 0.027
Actual params: [0.371 , 1.6695]
-Original Grad: 0.026, -lr * Pred Grad: 0.125, New P: 0.496
-Original Grad: 0.025, -lr * Pred Grad: 0.085, New P: 1.754
iter 10 loss: 0.020
Actual params: [0.496 , 1.7545]
-Original Grad: 0.037, -lr * Pred Grad: 0.176, New P: 0.672
-Original Grad: 0.034, -lr * Pred Grad: 0.111, New P: 1.865
iter 11 loss: 0.015
Actual params: [0.6722, 1.8652]
-Original Grad: 0.012, -lr * Pred Grad: 0.122, New P: 0.795
-Original Grad: 0.005, -lr * Pred Grad: 0.033, New P: 1.898
iter 12 loss: 0.015
Actual params: [0.7946, 1.898 ]
-Original Grad: 0.019, -lr * Pred Grad: 0.128, New P: 0.923
-Original Grad: -0.057, -lr * Pred Grad: -0.125, New P: 1.773
iter 13 loss: 0.008
Actual params: [0.9225, 1.7731]
-Original Grad: 0.043, -lr * Pred Grad: 0.191, New P: 1.114
-Original Grad: -0.007, -lr * Pred Grad: -0.080, New P: 1.693
iter 14 loss: 0.006
Actual params: [1.114 , 1.6932]
-Original Grad: -0.016, -lr * Pred Grad: 0.046, New P: 1.160
-Original Grad: 0.051, -lr * Pred Grad: 0.068, New P: 1.761
iter 15 loss: 0.005
Actual params: [1.1596, 1.7609]
-Original Grad: -0.009, -lr * Pred Grad: 0.011, New P: 1.171
-Original Grad: 0.035, -lr * Pred Grad: 0.102, New P: 1.863
iter 16 loss: 0.004
Actual params: [1.1711, 1.863 ]
-Original Grad: -0.008, -lr * Pred Grad: -0.019, New P: 1.152
-Original Grad: 0.012, -lr * Pred Grad: 0.050, New P: 1.913
iter 17 loss: 0.006
Actual params: [1.1523, 1.9126]
-Original Grad: 0.009, -lr * Pred Grad: 0.007, New P: 1.159
-Original Grad: -0.067, -lr * Pred Grad: -0.147, New P: 1.766
iter 18 loss: 0.005
Actual params: [1.1593, 1.7656]
-Original Grad: -0.015, -lr * Pred Grad: -0.053, New P: 1.107
-Original Grad: 0.048, -lr * Pred Grad: 0.032, New P: 1.797
iter 19 loss: 0.004
Actual params: [1.1067, 1.7974]
-Original Grad: -0.005, -lr * Pred Grad: -0.055, New P: 1.052
-Original Grad: -0.003, -lr * Pred Grad: -0.020, New P: 1.778
iter 20 loss: 0.005
Actual params: [1.0518, 1.7777]
-Original Grad: 0.019, -lr * Pred Grad: 0.004, New P: 1.056
-Original Grad: 0.005, -lr * Pred Grad: -0.009, New P: 1.768
iter 21 loss: 0.005
Actual params: [1.0559, 1.7684]
-Original Grad: 0.006, -lr * Pred Grad: -0.003, New P: 1.053
-Original Grad: 0.025, -lr * Pred Grad: 0.039, New P: 1.808
iter 22 loss: 0.005
Actual params: [1.0529, 1.8076]
-Original Grad: 0.021, -lr * Pred Grad: 0.043, New P: 1.096
-Original Grad: -0.033, -lr * Pred Grad: -0.083, New P: 1.725
iter 23 loss: 0.005
Actual params: [1.0962, 1.7249]
-Original Grad: -0.009, -lr * Pred Grad: -0.019, New P: 1.077
-Original Grad: 0.046, -lr * Pred Grad: 0.065, New P: 1.790
iter 24 loss: 0.004
Actual params: [1.0775, 1.7902]
-Original Grad: -0.005, -lr * Pred Grad: -0.032, New P: 1.046
-Original Grad: 0.016, -lr * Pred Grad: 0.042, New P: 1.833
iter 25 loss: 0.005
Actual params: [1.0459, 1.8326]
-Original Grad: 0.018, -lr * Pred Grad: 0.018, New P: 1.064
-Original Grad: -0.045, -lr * Pred Grad: -0.105, New P: 1.727
iter 26 loss: 0.005
Actual params: [1.0643, 1.7274]
-Original Grad: -0.009, -lr * Pred Grad: -0.032, New P: 1.032
-Original Grad: 0.037, -lr * Pred Grad: 0.030, New P: 1.757
iter 27 loss: 0.005
Actual params: [1.0318, 1.7569]
-Original Grad: 0.018, -lr * Pred Grad: 0.023, New P: 1.055
-Original Grad: -0.009, -lr * Pred Grad: -0.037, New P: 1.720
iter 28 loss: 0.005
Actual params: [1.0545, 1.7201]
-Original Grad: 0.007, -lr * Pred Grad: 0.013, New P: 1.068
-Original Grad: 0.029, -lr * Pred Grad: 0.046, New P: 1.766
iter 29 loss: 0.005
Actual params: [1.0675, 1.7656]
-Original Grad: -0.005, -lr * Pred Grad: -0.018, New P: 1.050
-Original Grad: 0.027, -lr * Pred Grad: 0.067, New P: 1.833
iter 30 loss: 0.005
Actual params: [1.0498, 1.8328]
-Original Grad: 0.019, -lr * Pred Grad: 0.034, New P: 1.084
-Original Grad: -0.018, -lr * Pred Grad: -0.040, New P: 1.792
Target params: [1.3344, 1.5708]
iter 0 loss: 0.170
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.090, -lr * Pred Grad: 1.429, New P: 0.956
-Original Grad: -0.020, -lr * Pred Grad: 0.538, New P: 0.541
iter 1 loss: 0.058
Actual params: [0.9562, 0.541 ]
-Original Grad: -0.144, -lr * Pred Grad: -0.999, New P: -0.043
-Original Grad: -0.204, -lr * Pred Grad: -0.545, New P: -0.004
iter 2 loss: 0.110
Actual params: [-0.0426, -0.0044]
-Original Grad: 0.154, -lr * Pred Grad: 0.634, New P: 0.591
-Original Grad: 0.044, -lr * Pred Grad: -0.001, New P: -0.005
iter 3 loss: 0.014
Actual params: [ 0.5913, -0.0053]
-Original Grad: 0.054, -lr * Pred Grad: 0.233, New P: 0.824
-Original Grad: 0.117, -lr * Pred Grad: 0.224, New P: 0.219
iter 4 loss: 0.011
Actual params: [0.824 , 0.2186]
-Original Grad: -0.073, -lr * Pred Grad: -0.073, New P: 0.751
-Original Grad: -0.050, -lr * Pred Grad: -0.108, New P: 0.111
iter 5 loss: 0.006
Actual params: [0.7515, 0.1106]
-Original Grad: -0.024, -lr * Pred Grad: -0.060, New P: 0.692
-Original Grad: -0.004, -lr * Pred Grad: -0.054, New P: 0.057
iter 6 loss: 0.007
Actual params: [0.6918, 0.057 ]
-Original Grad: 0.007, -lr * Pred Grad: -0.008, New P: 0.684
-Original Grad: 0.061, -lr * Pred Grad: 0.096, New P: 0.153
iter 7 loss: 0.005
Actual params: [0.6842, 0.1533]
-Original Grad: 0.000, -lr * Pred Grad: -0.017, New P: 0.668
-Original Grad: 0.035, -lr * Pred Grad: 0.111, New P: 0.264
iter 8 loss: 0.005
Actual params: [0.6676, 0.264 ]
-Original Grad: -0.014, -lr * Pred Grad: -0.060, New P: 0.608
-Original Grad: -0.031, -lr * Pred Grad: -0.060, New P: 0.204
iter 9 loss: 0.004
Actual params: [0.6077, 0.2038]
-Original Grad: -0.001, -lr * Pred Grad: -0.048, New P: 0.559
-Original Grad: 0.003, -lr * Pred Grad: -0.025, New P: 0.179
iter 10 loss: 0.006
Actual params: [0.5595, 0.1792]
-Original Grad: 0.031, -lr * Pred Grad: 0.043, New P: 0.603
-Original Grad: 0.054, -lr * Pred Grad: 0.109, New P: 0.288
iter 11 loss: 0.004
Actual params: [0.603 , 0.2885]
-Original Grad: -0.023, -lr * Pred Grad: -0.062, New P: 0.541
-Original Grad: -0.021, -lr * Pred Grad: -0.039, New P: 0.249
iter 12 loss: 0.005
Actual params: [0.5409, 0.2493]
-Original Grad: 0.001, -lr * Pred Grad: -0.040, New P: 0.501
-Original Grad: -0.000, -lr * Pred Grad: -0.024, New P: 0.225
iter 13 loss: 0.007
Actual params: [0.5007, 0.2252]
-Original Grad: 0.032, -lr * Pred Grad: 0.048, New P: 0.549
-Original Grad: 0.051, -lr * Pred Grad: 0.102, New P: 0.327
iter 14 loss: 0.004
Actual params: [0.5486, 0.3269]
-Original Grad: -0.001, -lr * Pred Grad: 0.003, New P: 0.551
-Original Grad: 0.001, -lr * Pred Grad: 0.017, New P: 0.344
iter 15 loss: 0.004
Actual params: [0.5511, 0.3436]
-Original Grad: -0.006, -lr * Pred Grad: -0.024, New P: 0.527
-Original Grad: -0.029, -lr * Pred Grad: -0.074, New P: 0.269
iter 16 loss: 0.005
Actual params: [0.5272, 0.2691]
-Original Grad: 0.020, -lr * Pred Grad: 0.033, New P: 0.560
-Original Grad: 0.021, -lr * Pred Grad: 0.003, New P: 0.272
iter 17 loss: 0.004
Actual params: [0.5604, 0.272 ]
-Original Grad: 0.010, -lr * Pred Grad: 0.030, New P: 0.590
-Original Grad: 0.016, -lr * Pred Grad: 0.020, New P: 0.292
iter 18 loss: 0.004
Actual params: [0.5903, 0.2919]
-Original Grad: -0.002, -lr * Pred Grad: 0.001, New P: 0.592
-Original Grad: -0.011, -lr * Pred Grad: -0.038, New P: 0.254
iter 19 loss: 0.004
Actual params: [0.5916, 0.2541]
-Original Grad: 0.009, -lr * Pred Grad: 0.019, New P: 0.610
-Original Grad: 0.015, -lr * Pred Grad: 0.006, New P: 0.260
iter 20 loss: 0.004
Actual params: [0.6103, 0.2597]
-Original Grad: -0.003, -lr * Pred Grad: -0.011, New P: 0.599
-Original Grad: -0.017, -lr * Pred Grad: -0.058, New P: 0.202
iter 21 loss: 0.005
Actual params: [0.5994, 0.2015]
-Original Grad: 0.002, -lr * Pred Grad: -0.011, New P: 0.588
-Original Grad: 0.029, -lr * Pred Grad: 0.032, New P: 0.234
iter 22 loss: 0.004
Actual params: [0.5881, 0.2336]
-Original Grad: 0.013, -lr * Pred Grad: 0.017, New P: 0.605
-Original Grad: 0.052, -lr * Pred Grad: 0.133, New P: 0.367
iter 23 loss: 0.005
Actual params: [0.6048, 0.3669]
-Original Grad: -0.026, -lr * Pred Grad: -0.081, New P: 0.524
-Original Grad: -0.054, -lr * Pred Grad: -0.113, New P: 0.254
iter 24 loss: 0.005
Actual params: [0.5242, 0.2543]
-Original Grad: 0.035, -lr * Pred Grad: 0.043, New P: 0.567
-Original Grad: 0.049, -lr * Pred Grad: 0.067, New P: 0.321
iter 25 loss: 0.004
Actual params: [0.5675, 0.321 ]
-Original Grad: 0.003, -lr * Pred Grad: 0.005, New P: 0.573
-Original Grad: -0.022, -lr * Pred Grad: -0.058, New P: 0.263
iter 26 loss: 0.004
Actual params: [0.5728, 0.263 ]
-Original Grad: 0.025, -lr * Pred Grad: 0.065, New P: 0.638
-Original Grad: 0.035, -lr * Pred Grad: 0.055, New P: 0.318
iter 27 loss: 0.005
Actual params: [0.6376, 0.3182]
-Original Grad: -0.019, -lr * Pred Grad: -0.034, New P: 0.603
-Original Grad: -0.045, -lr * Pred Grad: -0.109, New P: 0.209
iter 28 loss: 0.004
Actual params: [0.6031, 0.2093]
-Original Grad: 0.009, -lr * Pred Grad: 0.004, New P: 0.607
-Original Grad: 0.023, -lr * Pred Grad: -0.003, New P: 0.206
iter 29 loss: 0.004
Actual params: [0.6068, 0.206 ]
-Original Grad: 0.010, -lr * Pred Grad: 0.014, New P: 0.621
-Original Grad: 0.005, -lr * Pred Grad: -0.014, New P: 0.192
iter 30 loss: 0.004
Actual params: [0.621, 0.192]
-Original Grad: 0.018, -lr * Pred Grad: 0.044, New P: 0.665
-Original Grad: 0.023, -lr * Pred Grad: 0.037, New P: 0.229
Target params: [1.3344, 1.5708]
iter 0 loss: 0.591
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.043, -lr * Pred Grad: 1.060, New P: 0.588
-Original Grad: -0.003, -lr * Pred Grad: 0.620, New P: 0.624
iter 1 loss: 0.274
Actual params: [0.5879, 0.6235]
-Original Grad: 0.106, -lr * Pred Grad: 1.298, New P: 1.886
-Original Grad: -0.035, -lr * Pred Grad: -0.061, New P: 0.562
iter 2 loss: 0.358
Actual params: [1.8862, 0.5622]
-Original Grad: -0.126, -lr * Pred Grad: -0.767, New P: 1.119
-Original Grad: -0.023, -lr * Pred Grad: -0.089, New P: 0.473
iter 3 loss: 0.163
Actual params: [1.1189, 0.4728]
-Original Grad: 0.016, -lr * Pred Grad: 0.077, New P: 1.196
-Original Grad: 0.153, -lr * Pred Grad: 0.396, New P: 0.869
iter 4 loss: 0.121
Actual params: [1.1962, 0.8693]
-Original Grad: 0.011, -lr * Pred Grad: 0.014, New P: 1.211
-Original Grad: 0.069, -lr * Pred Grad: 0.304, New P: 1.173
iter 5 loss: 0.108
Actual params: [1.2105, 1.1731]
-Original Grad: 0.051, -lr * Pred Grad: 0.185, New P: 1.396
-Original Grad: 0.066, -lr * Pred Grad: 0.277, New P: 1.450
iter 6 loss: 0.126
Actual params: [1.3959, 1.4501]
-Original Grad: -0.081, -lr * Pred Grad: -0.178, New P: 1.218
-Original Grad: -0.619, -lr * Pred Grad: -0.425, New P: 1.025
iter 7 loss: 0.113
Actual params: [1.2177, 1.0247]
-Original Grad: 0.123, -lr * Pred Grad: 0.278, New P: 1.496
-Original Grad: 0.001, -lr * Pred Grad: -0.369, New P: 0.656
iter 8 loss: 0.205
Actual params: [1.4957, 0.6561]
-Original Grad: -0.268, -lr * Pred Grad: -0.585, New P: 0.911
-Original Grad: 0.297, -lr * Pred Grad: 0.329, New P: 0.985
iter 9 loss: 0.165
Actual params: [0.9107, 0.9849]
-Original Grad: 0.058, -lr * Pred Grad: -0.227, New P: 0.683
-Original Grad: -0.047, -lr * Pred Grad: -0.106, New P: 0.879
iter 10 loss: 0.230
Actual params: [0.6833, 0.8793]
-Original Grad: 0.313, -lr * Pred Grad: 0.433, New P: 1.117
-Original Grad: -0.027, -lr * Pred Grad: -0.083, New P: 0.796
iter 11 loss: 0.130
Actual params: [1.1166, 0.796 ]
-Original Grad: 0.031, -lr * Pred Grad: 0.161, New P: 1.278
-Original Grad: -0.006, -lr * Pred Grad: -0.088, New P: 0.708
iter 12 loss: 0.136
Actual params: [1.2778, 0.7082]
-Original Grad: 0.039, -lr * Pred Grad: 0.271, New P: 1.549
-Original Grad: 0.069, -lr * Pred Grad: 0.116, New P: 0.824
iter 13 loss: 0.194
Actual params: [1.5493, 0.8238]
-Original Grad: -0.264, -lr * Pred Grad: -0.538, New P: 1.012
-Original Grad: 0.201, -lr * Pred Grad: 0.753, New P: 1.577
iter 14 loss: 0.227
Actual params: [1.0115, 1.5768]
-Original Grad: -0.075, -lr * Pred Grad: -0.513, New P: 0.498
-Original Grad: -1.883, -lr * Pred Grad: -0.486, New P: 1.091
iter 15 loss: 0.330
Actual params: [0.4981, 1.0912]
-Original Grad: 0.221, -lr * Pred Grad: 0.148, New P: 0.646
-Original Grad: -0.146, -lr * Pred Grad: -0.474, New P: 0.617
iter 16 loss: 0.252
Actual params: [0.6463, 0.6168]
-Original Grad: 0.208, -lr * Pred Grad: 0.386, New P: 1.032
-Original Grad: 0.044, -lr * Pred Grad: -0.474, New P: 0.143
iter 17 loss: 0.222
Actual params: [1.0319, 0.143 ]
-Original Grad: 0.083, -lr * Pred Grad: 0.389, New P: 1.421
-Original Grad: 0.209, -lr * Pred Grad: -0.012, New P: 0.131
iter 18 loss: 0.303
Actual params: [1.4211, 0.1307]
-Original Grad: -0.082, -lr * Pred Grad: -0.007, New P: 1.414
-Original Grad: 0.209, -lr * Pred Grad: 0.430, New P: 0.561
iter 19 loss: 0.196
Actual params: [1.4139, 0.5611]
-Original Grad: -0.291, -lr * Pred Grad: -0.705, New P: 0.709
-Original Grad: 0.458, -lr * Pred Grad: 1.813, New P: 2.375
iter 20 loss: 0.701
Actual params: [0.7086, 2.3746]
-Original Grad: 0.332, -lr * Pred Grad: 0.304, New P: 1.013
-Original Grad: -0.285, -lr * Pred Grad: -0.823, New P: 1.551
iter 21 loss: 0.194
Actual params: [1.0129, 1.5515]
-Original Grad: 0.234, -lr * Pred Grad: 0.420, New P: 1.433
-Original Grad: -0.817, -lr * Pred Grad: -0.348, New P: 1.203
iter 22 loss: 0.100
Actual params: [1.4326, 1.2032]
-Original Grad: -0.009, -lr * Pred Grad: 0.206, New P: 1.639
-Original Grad: 0.054, -lr * Pred Grad: -0.504, New P: 0.699
iter 23 loss: 0.251
Actual params: [1.6387, 0.6988]
-Original Grad: -0.225, -lr * Pred Grad: -0.402, New P: 1.237
-Original Grad: 0.173, -lr * Pred Grad: -0.039, New P: 0.660
iter 24 loss: 0.139
Actual params: [1.2371, 0.6597]
-Original Grad: -0.028, -lr * Pred Grad: -0.273, New P: 0.964
-Original Grad: 0.114, -lr * Pred Grad: 0.188, New P: 0.848
iter 25 loss: 0.149
Actual params: [0.964 , 0.8478]
-Original Grad: 0.159, -lr * Pred Grad: 0.167, New P: 1.131
-Original Grad: 0.030, -lr * Pred Grad: 0.087, New P: 0.934
iter 26 loss: 0.124
Actual params: [1.1312, 0.9344]
-Original Grad: 0.040, -lr * Pred Grad: 0.113, New P: 1.244
-Original Grad: 0.081, -lr * Pred Grad: 0.250, New P: 1.185
iter 27 loss: 0.104
Actual params: [1.2443, 1.1848]
-Original Grad: 0.091, -lr * Pred Grad: 0.301, New P: 1.545
-Original Grad: 0.016, -lr * Pred Grad: 0.095, New P: 1.279
iter 28 loss: 0.124
Actual params: [1.545 , 1.2794]
-Original Grad: -0.339, -lr * Pred Grad: -0.655, New P: 0.890
-Original Grad: 0.178, -lr * Pred Grad: 0.669, New P: 1.949
iter 29 loss: 0.555
Actual params: [0.8898, 1.9486]
-Original Grad: 0.331, -lr * Pred Grad: 0.302, New P: 1.192
-Original Grad: -0.721, -lr * Pred Grad: -0.477, New P: 1.472
iter 30 loss: 0.110
Actual params: [1.1918, 1.4716]
-Original Grad: 0.100, -lr * Pred Grad: 0.187, New P: 1.379
-Original Grad: -0.294, -lr * Pred Grad: -0.455, New P: 1.017
Target params: [1.3344, 1.5708]
iter 0 loss: 0.746
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad: 0.662, New P: 0.190
-Original Grad: -0.000, -lr * Pred Grad: 0.631, New P: 0.635
iter 1 loss: 0.717
Actual params: [0.1899, 0.6349]
-Original Grad: 0.161, -lr * Pred Grad: 1.651, New P: 1.840
-Original Grad: 0.008, -lr * Pred Grad: 0.163, New P: 0.798
iter 2 loss: 0.431
Actual params: [1.8404, 0.7976]
-Original Grad: -0.197, -lr * Pred Grad: -1.112, New P: 0.728
-Original Grad: 0.532, -lr * Pred Grad: 2.051, New P: 2.848
iter 3 loss: 0.720
Actual params: [0.7284, 2.8484]
-Original Grad: 0.593, -lr * Pred Grad: 1.075, New P: 1.804
-Original Grad: -1.790, -lr * Pred Grad: -0.763, New P: 2.086
iter 4 loss: 0.261
Actual params: [1.8035, 2.0857]
-Original Grad: -0.610, -lr * Pred Grad: -0.983, New P: 0.820
-Original Grad: -0.418, -lr * Pred Grad: -0.384, New P: 1.702
iter 5 loss: 0.219
Actual params: [0.8203, 1.7016]
-Original Grad: 0.444, -lr * Pred Grad: 0.232, New P: 1.052
-Original Grad: -0.219, -lr * Pred Grad: -0.589, New P: 1.113
iter 6 loss: 0.224
Actual params: [1.0521, 1.1128]
-Original Grad: 0.160, -lr * Pred Grad: 0.146, New P: 1.198
-Original Grad: 0.464, -lr * Pred Grad: 0.160, New P: 1.273
iter 7 loss: 0.166
Actual params: [1.1977, 1.273 ]
-Original Grad: -0.044, -lr * Pred Grad: 0.041, New P: 1.238
-Original Grad: 0.338, -lr * Pred Grad: 0.944, New P: 2.217
iter 8 loss: 0.087
Actual params: [1.2383, 2.2168]
-Original Grad: 0.186, -lr * Pred Grad: 0.417, New P: 1.656
-Original Grad: 0.132, -lr * Pred Grad: 0.657, New P: 2.873
iter 9 loss: 0.940
Actual params: [1.6557, 2.8734]
-Original Grad: -0.173, -lr * Pred Grad: -0.231, New P: 1.425
-Original Grad: -2.416, -lr * Pred Grad: -0.554, New P: 2.319
iter 10 loss: 0.053
Actual params: [1.4251, 2.319 ]
-Original Grad: -0.112, -lr * Pred Grad: -0.359, New P: 1.067
-Original Grad: -0.200, -lr * Pred Grad: -0.480, New P: 1.839
iter 11 loss: 0.124
Actual params: [1.0666, 1.8392]
-Original Grad: 0.370, -lr * Pred Grad: 0.482, New P: 1.548
-Original Grad: -0.072, -lr * Pred Grad: -0.555, New P: 1.284
iter 12 loss: 0.234
Actual params: [1.5483, 1.284 ]
-Original Grad: -0.382, -lr * Pred Grad: -0.566, New P: 0.982
-Original Grad: 0.352, -lr * Pred Grad: 0.068, New P: 1.352
iter 13 loss: 0.204
Actual params: [0.9818, 1.3524]
-Original Grad: 0.419, -lr * Pred Grad: 0.364, New P: 1.346
-Original Grad: 0.450, -lr * Pred Grad: 1.259, New P: 2.611
iter 14 loss: 0.290
Actual params: [1.3461, 2.6114]
-Original Grad: 0.354, -lr * Pred Grad: 0.572, New P: 1.918
-Original Grad: -2.398, -lr * Pred Grad: -0.565, New P: 2.046
iter 15 loss: 0.302
Actual params: [1.9185, 2.0461]
-Original Grad: -0.348, -lr * Pred Grad: -0.437, New P: 1.481
-Original Grad: -0.235, -lr * Pred Grad: -0.448, New P: 1.598
iter 16 loss: 0.157
Actual params: [1.4811, 1.5982]
-Original Grad: -0.379, -lr * Pred Grad: -0.931, New P: 0.550
-Original Grad: 1.308, -lr * Pred Grad: 1.005, New P: 2.604
iter 17 loss: 0.503
Actual params: [0.5503, 2.6036]
-Original Grad: 0.196, -lr * Pred Grad: -0.236, New P: 0.315
-Original Grad: -1.068, -lr * Pred Grad: -0.480, New P: 2.124
iter 18 loss: 0.594
Actual params: [0.3147, 2.1237]
-Original Grad: 0.679, -lr * Pred Grad: 0.624, New P: 0.938
-Original Grad: 0.009, -lr * Pred Grad: -0.395, New P: 1.729
iter 19 loss: 0.182
Actual params: [0.9385, 1.729 ]
-Original Grad: 0.620, -lr * Pred Grad: 1.001, New P: 1.939
-Original Grad: -0.417, -lr * Pred Grad: -0.548, New P: 1.181
iter 20 loss: 0.360
Actual params: [1.9394, 1.1812]
-Original Grad: -0.161, -lr * Pred Grad: -0.189, New P: 1.751
-Original Grad: 0.390, -lr * Pred Grad: 0.017, New P: 1.199
iter 21 loss: 0.316
Actual params: [1.7508, 1.1986]
-Original Grad: -0.286, -lr * Pred Grad: -0.481, New P: 1.270
-Original Grad: 0.442, -lr * Pred Grad: 1.196, New P: 2.395
iter 22 loss: 0.075
Actual params: [1.2697, 2.395 ]
-Original Grad: 0.476, -lr * Pred Grad: 0.518, New P: 1.788
-Original Grad: -0.808, -lr * Pred Grad: -0.567, New P: 1.828
iter 23 loss: 0.211
Actual params: [1.7881, 1.8283]
-Original Grad: -0.580, -lr * Pred Grad: -0.688, New P: 1.100
-Original Grad: -0.174, -lr * Pred Grad: -0.376, New P: 1.452
iter 24 loss: 0.145
Actual params: [1.0999, 1.4523]
-Original Grad: 0.073, -lr * Pred Grad: -0.349, New P: 0.751
-Original Grad: 0.462, -lr * Pred Grad: 0.291, New P: 1.744
iter 25 loss: 0.244
Actual params: [0.7507, 1.7436]
-Original Grad: 0.522, -lr * Pred Grad: 0.417, New P: 1.168
-Original Grad: -0.224, -lr * Pred Grad: -0.356, New P: 1.388
iter 26 loss: 0.152
Actual params: [1.1677, 1.3875]
-Original Grad: 0.064, -lr * Pred Grad: 0.130, New P: 1.298
-Original Grad: 0.244, -lr * Pred Grad: 0.243, New P: 1.631
iter 27 loss: 0.084
Actual params: [1.2981, 1.631 ]
-Original Grad: -0.128, -lr * Pred Grad: -0.117, New P: 1.181
-Original Grad: 0.840, -lr * Pred Grad: 2.308, New P: 3.939
iter 28 loss: 1.238
Actual params: [1.181 , 3.9386]
-Original Grad: -0.156, -lr * Pred Grad: -0.396, New P: 0.785
-Original Grad: -0.259, -lr * Pred Grad: -0.755, New P: 3.184
iter 29 loss: 0.927
Actual params: [0.7846, 3.1841]
-Original Grad: -0.210, -lr * Pred Grad: -0.737, New P: 0.048
-Original Grad: -0.698, -lr * Pred Grad: -0.352, New P: 2.833
iter 30 loss: 0.772
Actual params: [0.0478, 2.8326]
-Original Grad: -0.136, -lr * Pred Grad: -0.874, New P: -0.826
-Original Grad: -0.642, -lr * Pred Grad: -0.568, New P: 2.264
Target params: [1.3344, 1.5708]
iter 0 loss: 0.065
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.059, -lr * Pred Grad: 1.194, New P: 0.722
-Original Grad: -0.019, -lr * Pred Grad: 0.545, New P: 0.548
iter 1 loss: 0.118
Actual params: [0.722 , 0.5485]
-Original Grad: -0.122, -lr * Pred Grad: -0.852, New P: -0.130
-Original Grad: 0.375, -lr * Pred Grad: 1.968, New P: 2.516
iter 2 loss: 0.083
Actual params: [-0.1301,  2.5162]
-Original Grad: -0.032, -lr * Pred Grad: -0.308, New P: -0.438
-Original Grad: -0.074, -lr * Pred Grad: -0.461, New P: 2.055
iter 3 loss: 0.072
Actual params: [-0.4377,  2.0552]
-Original Grad: 0.000, -lr * Pred Grad: -0.159, New P: -0.596
-Original Grad: -0.000, -lr * Pred Grad: -0.050, New P: 2.005
iter 4 loss: 0.072
Actual params: [-0.5965,  2.0054]
-Original Grad: 0.000, -lr * Pred Grad: -0.127, New P: -0.723
-Original Grad: -0.000, -lr * Pred Grad: -0.055, New P: 1.951
iter 5 loss: 0.072
Actual params: [-0.7233,  1.9508]
-Original Grad: 0.000, -lr * Pred Grad: -0.098, New P: -0.821
-Original Grad: 0.000, -lr * Pred Grad: -0.039, New P: 1.911
iter 6 loss: 0.072
Actual params: [-0.8212,  1.9114]
-Original Grad: 0.000, -lr * Pred Grad: -0.078, New P: -0.900
-Original Grad: -0.000, -lr * Pred Grad: -0.036, New P: 1.876
iter 7 loss: 0.072
Actual params: [-0.8996,  1.8756]
-Original Grad: 0.000, -lr * Pred Grad: -0.066, New P: -0.965
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 1.846
iter 8 loss: 0.072
Actual params: [-0.9651,  1.8464]
-Original Grad: 0.000, -lr * Pred Grad: -0.057, New P: -1.022
-Original Grad: 0.000, -lr * Pred Grad: -0.027, New P: 1.820
iter 9 loss: 0.072
Actual params: [-1.0224,  1.8196]
-Original Grad: 0.000, -lr * Pred Grad: -0.052, New P: -1.074
-Original Grad: 0.000, -lr * Pred Grad: -0.026, New P: 1.794
iter 10 loss: 0.072
Actual params: [-1.0745,  1.7935]
-Original Grad: 0.000, -lr * Pred Grad: -0.049, New P: -1.123
-Original Grad: 0.000, -lr * Pred Grad: -0.027, New P: 1.767
iter 11 loss: 0.072
Actual params: [-1.1232,  1.767 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.047, New P: -1.170
-Original Grad: 0.000, -lr * Pred Grad: -0.027, New P: 1.740
iter 12 loss: 0.072
Actual params: [-1.1698,  1.7396]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -1.215
-Original Grad: 0.000, -lr * Pred Grad: -0.028, New P: 1.711
iter 13 loss: 0.072
Actual params: [-1.215 ,  1.7114]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -1.259
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 1.682
iter 14 loss: 0.072
Actual params: [-1.2593,  1.6823]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -1.303
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 1.653
iter 15 loss: 0.072
Actual params: [-1.3031,  1.6525]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.346
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 1.622
iter 16 loss: 0.072
Actual params: [-1.3464,  1.6221]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.390
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 1.591
iter 17 loss: 0.072
Actual params: [-1.3896,  1.5912]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.433
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 1.560
iter 18 loss: 0.072
Actual params: [-1.4327,  1.5598]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.476
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.528
iter 19 loss: 0.072
Actual params: [-1.4757,  1.5282]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.519
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 1.496
iter 20 loss: 0.072
Actual params: [-1.5187,  1.4963]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.562
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.464
iter 21 loss: 0.072
Actual params: [-1.5617,  1.4643]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.605
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.432
iter 22 loss: 0.072
Actual params: [-1.6048,  1.4321]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.648
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.400
iter 23 loss: 0.072
Actual params: [-1.6479,  1.3998]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.691
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.367
iter 24 loss: 0.072
Actual params: [-1.691 ,  1.3674]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.734
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.335
iter 25 loss: 0.072
Actual params: [-1.7342,  1.335 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.777
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.303
iter 26 loss: 0.072
Actual params: [-1.7775,  1.3026]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.821
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.270
iter 27 loss: 0.072
Actual params: [-1.8207,  1.2701]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.864
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 1.238
iter 28 loss: 0.072
Actual params: [-1.864 ,  1.2376]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.907
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 1.205
iter 29 loss: 0.072
Actual params: [-1.9074,  1.205 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.951
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 1.173
iter 30 loss: 0.072
Actual params: [-1.9507,  1.1725]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -1.994
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 1.140
Target params: [1.3344, 1.5708]
iter 0 loss: 0.514
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.044, -lr * Pred Grad: 1.072, New P: 0.600
-Original Grad: 0.012, -lr * Pred Grad: 0.692, New P: 0.695
iter 1 loss: 0.070
Actual params: [0.5999, 0.6955]
-Original Grad: 0.097, -lr * Pred Grad: 1.224, New P: 1.824
-Original Grad: 0.372, -lr * Pred Grad: 2.007, New P: 2.702
iter 2 loss: 0.637
Actual params: [1.8237, 2.7021]
-Original Grad: -0.034, -lr * Pred Grad: -0.056, New P: 1.768
-Original Grad: -0.339, -lr * Pred Grad: -1.032, New P: 1.670
iter 3 loss: 0.112
Actual params: [1.768 , 1.6702]
-Original Grad: 0.233, -lr * Pred Grad: 1.338, New P: 3.106
-Original Grad: -1.318, -lr * Pred Grad: -0.368, New P: 1.302
iter 4 loss: 0.376
Actual params: [3.1059, 1.3021]
-Original Grad: -0.108, -lr * Pred Grad: -0.515, New P: 2.591
-Original Grad: -0.282, -lr * Pred Grad: -0.603, New P: 0.699
iter 5 loss: 0.129
Actual params: [2.5912, 0.6989]
-Original Grad: -0.059, -lr * Pred Grad: -0.148, New P: 2.443
-Original Grad: 0.242, -lr * Pred Grad: -0.299, New P: 0.400
iter 6 loss: 0.195
Actual params: [2.443 , 0.4003]
-Original Grad: -0.052, -lr * Pred Grad: -0.213, New P: 2.230
-Original Grad: 0.381, -lr * Pred Grad: 0.607, New P: 1.008
iter 7 loss: 0.058
Actual params: [2.2301, 1.0077]
-Original Grad: -0.067, -lr * Pred Grad: -0.302, New P: 1.928
-Original Grad: 0.050, -lr * Pred Grad: 0.130, New P: 1.138
iter 8 loss: 0.035
Actual params: [1.928 , 1.1379]
-Original Grad: -0.036, -lr * Pred Grad: -0.298, New P: 1.630
-Original Grad: -0.036, -lr * Pred Grad: -0.076, New P: 1.062
iter 9 loss: 0.032
Actual params: [1.6301, 1.0623]
-Original Grad: 0.007, -lr * Pred Grad: -0.202, New P: 1.428
-Original Grad: 0.214, -lr * Pred Grad: 0.597, New P: 1.659
iter 10 loss: 0.201
Actual params: [1.4278, 1.6594]
-Original Grad: 0.223, -lr * Pred Grad: 0.340, New P: 1.768
-Original Grad: -1.008, -lr * Pred Grad: -0.433, New P: 1.227
iter 11 loss: 0.021
Actual params: [1.7675, 1.2266]
-Original Grad: -0.049, -lr * Pred Grad: -0.034, New P: 1.733
-Original Grad: 0.172, -lr * Pred Grad: -0.312, New P: 0.915
iter 12 loss: 0.045
Actual params: [1.7332, 0.915 ]
-Original Grad: 0.006, -lr * Pred Grad: 0.042, New P: 1.775
-Original Grad: 0.150, -lr * Pred Grad: -0.033, New P: 0.882
iter 13 loss: 0.048
Actual params: [1.7749, 0.8816]
-Original Grad: 0.002, -lr * Pred Grad: 0.020, New P: 1.795
-Original Grad: 0.127, -lr * Pred Grad: 0.257, New P: 1.139
iter 14 loss: 0.028
Actual params: [1.7947, 1.1387]
-Original Grad: -0.006, -lr * Pred Grad: -0.011, New P: 1.784
-Original Grad: 0.082, -lr * Pred Grad: 0.300, New P: 1.438
iter 15 loss: 0.018
Actual params: [1.7841, 1.4384]
-Original Grad: 0.062, -lr * Pred Grad: 0.157, New P: 1.941
-Original Grad: -0.235, -lr * Pred Grad: -0.366, New P: 1.073
iter 16 loss: 0.036
Actual params: [1.9409, 1.0726]
-Original Grad: -0.030, -lr * Pred Grad: -0.024, New P: 1.917
-Original Grad: 0.130, -lr * Pred Grad: 0.050, New P: 1.123
iter 17 loss: 0.034
Actual params: [1.9172, 1.1226]
-Original Grad: -0.040, -lr * Pred Grad: -0.122, New P: 1.796
-Original Grad: -0.048, -lr * Pred Grad: -0.129, New P: 0.994
iter 18 loss: 0.037
Actual params: [1.7956, 0.994 ]
-Original Grad: -0.012, -lr * Pred Grad: -0.112, New P: 1.684
-Original Grad: 0.135, -lr * Pred Grad: 0.293, New P: 1.287
iter 19 loss: 0.014
Actual params: [1.6837, 1.2868]
-Original Grad: -0.004, -lr * Pred Grad: -0.098, New P: 1.585
-Original Grad: 0.162, -lr * Pred Grad: 0.723, New P: 2.010
iter 20 loss: 0.404
Actual params: [1.5852, 2.0099]
-Original Grad: 0.096, -lr * Pred Grad: 0.170, New P: 1.755
-Original Grad: -0.540, -lr * Pred Grad: -0.529, New P: 1.481
iter 21 loss: 0.027
Actual params: [1.7554, 1.4813]
-Original Grad: 0.039, -lr * Pred Grad: 0.156, New P: 1.912
-Original Grad: -0.302, -lr * Pred Grad: -0.422, New P: 1.059
iter 22 loss: 0.035
Actual params: [1.9116, 1.059 ]
-Original Grad: -0.036, -lr * Pred Grad: -0.015, New P: 1.896
-Original Grad: 0.042, -lr * Pred Grad: -0.387, New P: 0.672
iter 23 loss: 0.075
Actual params: [1.8964, 0.6724]
-Original Grad: 0.006, -lr * Pred Grad: 0.021, New P: 1.918
-Original Grad: 0.395, -lr * Pred Grad: 0.710, New P: 1.382
iter 24 loss: 0.018
Actual params: [1.9175, 1.3821]
-Original Grad: -0.024, -lr * Pred Grad: -0.068, New P: 1.849
-Original Grad: 0.004, -lr * Pred Grad: 0.057, New P: 1.439
iter 25 loss: 0.018
Actual params: [1.8492, 1.4387]
-Original Grad: -0.042, -lr * Pred Grad: -0.166, New P: 1.683
-Original Grad: -0.092, -lr * Pred Grad: -0.175, New P: 1.263
iter 26 loss: 0.015
Actual params: [1.6829, 1.2634]
-Original Grad: -0.014, -lr * Pred Grad: -0.153, New P: 1.530
-Original Grad: 0.103, -lr * Pred Grad: 0.148, New P: 1.412
iter 27 loss: 0.039
Actual params: [1.5298, 1.4117]
-Original Grad: 0.197, -lr * Pred Grad: 0.367, New P: 1.897
-Original Grad: -0.623, -lr * Pred Grad: -0.408, New P: 1.004
iter 28 loss: 0.038
Actual params: [1.8969, 1.0037]
-Original Grad: -0.006, -lr * Pred Grad: 0.087, New P: 1.984
-Original Grad: 0.097, -lr * Pred Grad: -0.320, New P: 0.684
iter 29 loss: 0.078
Actual params: [1.9842, 0.6838]
-Original Grad: -0.010, -lr * Pred Grad: 0.066, New P: 2.050
-Original Grad: 0.226, -lr * Pred Grad: 0.251, New P: 0.934
iter 30 loss: 0.047
Actual params: [2.0504, 0.9345]
-Original Grad: -0.029, -lr * Pred Grad: -0.044, New P: 2.006
-Original Grad: 0.027, -lr * Pred Grad: 0.090, New P: 1.025
Target params: [1.3344, 1.5708]
iter 0 loss: 0.274
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.067, -lr * Pred Grad: 1.257, New P: 0.785
-Original Grad: 0.474, -lr * Pred Grad: 2.134, New P: 2.137
iter 1 loss: 0.100
Actual params: [0.7845, 2.1373]
-Original Grad: 0.013, -lr * Pred Grad: 0.377, New P: 1.162
-Original Grad: -0.345, -lr * Pred Grad: -0.988, New P: 1.150
iter 2 loss: 0.036
Actual params: [1.1617, 1.1495]
-Original Grad: -0.018, -lr * Pred Grad: 0.188, New P: 1.350
-Original Grad: 0.147, -lr * Pred Grad: 0.202, New P: 1.351
iter 3 loss: 0.021
Actual params: [1.3496, 1.3511]
-Original Grad: 0.031, -lr * Pred Grad: 0.349, New P: 1.699
-Original Grad: 0.071, -lr * Pred Grad: 0.152, New P: 1.503
iter 4 loss: 0.026
Actual params: [1.6986, 1.5031]
-Original Grad: -0.016, -lr * Pred Grad: 0.040, New P: 1.739
-Original Grad: -0.153, -lr * Pred Grad: -0.239, New P: 1.264
iter 5 loss: 0.008
Actual params: [1.7391, 1.2643]
-Original Grad: -0.001, -lr * Pred Grad: 0.062, New P: 1.801
-Original Grad: -0.059, -lr * Pred Grad: -0.217, New P: 1.047
iter 6 loss: 0.006
Actual params: [1.8013, 1.047 ]
-Original Grad: 0.018, -lr * Pred Grad: 0.092, New P: 1.893
-Original Grad: 0.028, -lr * Pred Grad: -0.095, New P: 0.952
iter 7 loss: 0.007
Actual params: [1.893 , 0.9521]
-Original Grad: 0.013, -lr * Pred Grad: 0.083, New P: 1.976
-Original Grad: 0.082, -lr * Pred Grad: 0.148, New P: 1.101
iter 8 loss: 0.007
Actual params: [1.9762, 1.1005]
-Original Grad: -0.011, -lr * Pred Grad: 0.011, New P: 1.987
-Original Grad: -0.045, -lr * Pred Grad: -0.089, New P: 1.012
iter 9 loss: 0.006
Actual params: [1.9868, 1.0116]
-Original Grad: -0.004, -lr * Pred Grad: -0.005, New P: 1.982
-Original Grad: 0.003, -lr * Pred Grad: -0.033, New P: 0.978
iter 10 loss: 0.006
Actual params: [1.9816, 0.9783]
-Original Grad: -0.002, -lr * Pred Grad: -0.017, New P: 1.965
-Original Grad: 0.033, -lr * Pred Grad: 0.040, New P: 1.019
iter 11 loss: 0.006
Actual params: [1.9649, 1.0188]
-Original Grad: -0.008, -lr * Pred Grad: -0.044, New P: 1.921
-Original Grad: 0.001, -lr * Pred Grad: -0.002, New P: 1.017
iter 12 loss: 0.006
Actual params: [1.9208, 1.017 ]
-Original Grad: 0.002, -lr * Pred Grad: -0.031, New P: 1.890
-Original Grad: 0.017, -lr * Pred Grad: 0.026, New P: 1.043
iter 13 loss: 0.005
Actual params: [1.89  , 1.0426]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: 1.858
-Original Grad: 0.042, -lr * Pred Grad: 0.105, New P: 1.148
iter 14 loss: 0.006
Actual params: [1.8577, 1.1481]
-Original Grad: -0.008, -lr * Pred Grad: -0.058, New P: 1.799
-Original Grad: -0.043, -lr * Pred Grad: -0.092, New P: 1.056
iter 15 loss: 0.006
Actual params: [1.7994, 1.0564]
-Original Grad: 0.014, -lr * Pred Grad: -0.008, New P: 1.791
-Original Grad: 0.036, -lr * Pred Grad: 0.043, New P: 1.100
iter 16 loss: 0.005
Actual params: [1.7909, 1.0996]
-Original Grad: 0.006, -lr * Pred Grad: -0.008, New P: 1.783
-Original Grad: 0.008, -lr * Pred Grad: 0.011, New P: 1.110
iter 17 loss: 0.005
Actual params: [1.7833, 1.1103]
-Original Grad: 0.015, -lr * Pred Grad: 0.023, New P: 1.806
-Original Grad: 0.014, -lr * Pred Grad: 0.024, New P: 1.135
iter 18 loss: 0.005
Actual params: [1.8058, 1.1348]
-Original Grad: 0.002, -lr * Pred Grad: 0.003, New P: 1.809
-Original Grad: 0.004, -lr * Pred Grad: -0.000, New P: 1.134
iter 19 loss: 0.005
Actual params: [1.8089, 1.1343]
-Original Grad: -0.003, -lr * Pred Grad: -0.019, New P: 1.790
-Original Grad: -0.025, -lr * Pred Grad: -0.074, New P: 1.060
iter 20 loss: 0.006
Actual params: [1.7896, 1.0604]
-Original Grad: 0.019, -lr * Pred Grad: 0.032, New P: 1.822
-Original Grad: 0.026, -lr * Pred Grad: 0.016, New P: 1.076
iter 21 loss: 0.005
Actual params: [1.8215, 1.076 ]
-Original Grad: 0.005, -lr * Pred Grad: 0.015, New P: 1.836
-Original Grad: 0.016, -lr * Pred Grad: 0.024, New P: 1.100
iter 22 loss: 0.005
Actual params: [1.8363, 1.0997]
-Original Grad: -0.003, -lr * Pred Grad: -0.010, New P: 1.827
-Original Grad: -0.003, -lr * Pred Grad: -0.016, New P: 1.084
iter 23 loss: 0.005
Actual params: [1.8265, 1.0835]
-Original Grad: 0.009, -lr * Pred Grad: 0.008, New P: 1.835
-Original Grad: -0.005, -lr * Pred Grad: -0.034, New P: 1.049
iter 24 loss: 0.005
Actual params: [1.8348, 1.0495]
-Original Grad: 0.011, -lr * Pred Grad: 0.020, New P: 1.855
-Original Grad: 0.025, -lr * Pred Grad: 0.029, New P: 1.078
iter 25 loss: 0.005
Actual params: [1.8551, 1.0782]
-Original Grad: -0.003, -lr * Pred Grad: -0.011, New P: 1.845
-Original Grad: -0.020, -lr * Pred Grad: -0.059, New P: 1.019
iter 26 loss: 0.006
Actual params: [1.8445, 1.0194]
-Original Grad: 0.002, -lr * Pred Grad: -0.011, New P: 1.833
-Original Grad: 0.017, -lr * Pred Grad: 0.003, New P: 1.023
iter 27 loss: 0.006
Actual params: [1.8331, 1.0227]
-Original Grad: 0.022, -lr * Pred Grad: 0.041, New P: 1.874
-Original Grad: 0.078, -lr * Pred Grad: 0.200, New P: 1.223
iter 28 loss: 0.009
Actual params: [1.8743, 1.2225]
-Original Grad: -0.026, -lr * Pred Grad: -0.069, New P: 1.805
-Original Grad: -0.105, -lr * Pred Grad: -0.209, New P: 1.013
iter 29 loss: 0.007
Actual params: [1.8052, 1.0132]
-Original Grad: 0.027, -lr * Pred Grad: 0.032, New P: 1.838
-Original Grad: 0.058, -lr * Pred Grad: 0.041, New P: 1.055
iter 30 loss: 0.005
Actual params: [1.8376, 1.0547]
-Original Grad: 0.005, -lr * Pred Grad: 0.008, New P: 1.846
-Original Grad: 0.012, -lr * Pred Grad: 0.015, New P: 1.070
Target params: [1.3344, 1.5708]
iter 0 loss: 0.187
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad: 0.653, New P: 0.180
-Original Grad: 0.000, -lr * Pred Grad: 0.635, New P: 0.639
iter 1 loss: 0.103
Actual params: [0.1805, 0.6388]
-Original Grad: 0.231, -lr * Pred Grad: 1.933, New P: 2.114
-Original Grad: 0.121, -lr * Pred Grad: 0.827, New P: 1.466
iter 2 loss: 0.038
Actual params: [2.114 , 1.4657]
-Original Grad: -0.018, -lr * Pred Grad: -0.278, New P: 1.836
-Original Grad: -0.111, -lr * Pred Grad: -0.553, New P: 0.913
iter 3 loss: 0.024
Actual params: [1.8356, 0.913 ]
-Original Grad: 0.034, -lr * Pred Grad: 0.545, New P: 2.381
-Original Grad: 0.004, -lr * Pred Grad: -0.015, New P: 0.898
iter 4 loss: 0.019
Actual params: [2.3811, 0.8982]
-Original Grad: -0.009, -lr * Pred Grad: 0.080, New P: 2.461
-Original Grad: -0.032, -lr * Pred Grad: -0.165, New P: 0.733
iter 5 loss: 0.018
Actual params: [2.4608, 0.7328]
-Original Grad: 0.001, -lr * Pred Grad: 0.130, New P: 2.591
-Original Grad: 0.005, -lr * Pred Grad: -0.064, New P: 0.669
iter 6 loss: 0.018
Actual params: [2.5906, 0.6693]
-Original Grad: -0.005, -lr * Pred Grad: 0.056, New P: 2.647
-Original Grad: 0.002, -lr * Pred Grad: -0.053, New P: 0.617
iter 7 loss: 0.018
Actual params: [2.6468, 0.6167]
-Original Grad: -0.001, -lr * Pred Grad: 0.038, New P: 2.685
-Original Grad: 0.010, -lr * Pred Grad: -0.016, New P: 0.600
iter 8 loss: 0.018
Actual params: [2.6848, 0.6005]
-Original Grad: -0.014, -lr * Pred Grad: -0.017, New P: 2.668
-Original Grad: -0.004, -lr * Pred Grad: -0.037, New P: 0.564
iter 9 loss: 0.018
Actual params: [2.6682, 0.5637]
-Original Grad: 0.004, -lr * Pred Grad: 0.000, New P: 2.668
-Original Grad: 0.016, -lr * Pred Grad: 0.006, New P: 0.570
iter 10 loss: 0.018
Actual params: [2.6685, 0.5698]
-Original Grad: -0.015, -lr * Pred Grad: -0.053, New P: 2.616
-Original Grad: 0.005, -lr * Pred Grad: -0.006, New P: 0.564
iter 11 loss: 0.018
Actual params: [2.6158, 0.5637]
-Original Grad: -0.007, -lr * Pred Grad: -0.059, New P: 2.557
-Original Grad: 0.017, -lr * Pred Grad: 0.022, New P: 0.586
iter 12 loss: 0.018
Actual params: [2.5567, 0.5856]
-Original Grad: 0.003, -lr * Pred Grad: -0.043, New P: 2.513
-Original Grad: 0.023, -lr * Pred Grad: 0.049, New P: 0.634
iter 13 loss: 0.018
Actual params: [2.5135, 0.6343]
-Original Grad: -0.003, -lr * Pred Grad: -0.052, New P: 2.462
-Original Grad: 0.008, -lr * Pred Grad: 0.020, New P: 0.654
iter 14 loss: 0.018
Actual params: [2.4615, 0.654 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.049, New P: 2.413
-Original Grad: 0.005, -lr * Pred Grad: 0.003, New P: 0.657
iter 15 loss: 0.018
Actual params: [2.413 , 0.6568]
-Original Grad: -0.004, -lr * Pred Grad: -0.059, New P: 2.354
-Original Grad: -0.000, -lr * Pred Grad: -0.018, New P: 0.639
iter 16 loss: 0.019
Actual params: [2.3537, 0.6391]
-Original Grad: 0.012, -lr * Pred Grad: -0.018, New P: 2.336
-Original Grad: 0.062, -lr * Pred Grad: 0.143, New P: 0.782
iter 17 loss: 0.018
Actual params: [2.3355, 0.7817]
-Original Grad: 0.002, -lr * Pred Grad: -0.026, New P: 2.309
-Original Grad: 0.010, -lr * Pred Grad: 0.053, New P: 0.835
iter 18 loss: 0.018
Actual params: [2.3091, 0.8349]
-Original Grad: -0.006, -lr * Pred Grad: -0.049, New P: 2.260
-Original Grad: -0.031, -lr * Pred Grad: -0.070, New P: 0.765
iter 19 loss: 0.018
Actual params: [2.2604, 0.7654]
-Original Grad: 0.007, -lr * Pred Grad: -0.024, New P: 2.236
-Original Grad: 0.027, -lr * Pred Grad: 0.023, New P: 0.788
iter 20 loss: 0.018
Actual params: [2.2364, 0.7883]
-Original Grad: 0.015, -lr * Pred Grad: 0.010, New P: 2.247
-Original Grad: 0.040, -lr * Pred Grad: 0.094, New P: 0.882
iter 21 loss: 0.018
Actual params: [2.2467, 0.8823]
-Original Grad: 0.002, -lr * Pred Grad: -0.008, New P: 2.239
-Original Grad: 0.011, -lr * Pred Grad: 0.044, New P: 0.926
iter 22 loss: 0.019
Actual params: [2.2391, 0.9264]
-Original Grad: -0.001, -lr * Pred Grad: -0.019, New P: 2.220
-Original Grad: -0.008, -lr * Pred Grad: -0.020, New P: 0.907
iter 23 loss: 0.019
Actual params: [2.2201, 0.9066]
-Original Grad: -0.005, -lr * Pred Grad: -0.040, New P: 2.180
-Original Grad: -0.016, -lr * Pred Grad: -0.059, New P: 0.847
iter 24 loss: 0.018
Actual params: [2.1802, 0.8475]
-Original Grad: 0.006, -lr * Pred Grad: -0.021, New P: 2.159
-Original Grad: 0.003, -lr * Pred Grad: -0.034, New P: 0.813
iter 25 loss: 0.019
Actual params: [2.1594, 0.8132]
-Original Grad: 0.018, -lr * Pred Grad: 0.021, New P: 2.181
-Original Grad: 0.053, -lr * Pred Grad: 0.105, New P: 0.918
iter 26 loss: 0.019
Actual params: [2.1805, 0.9183]
-Original Grad: -0.001, -lr * Pred Grad: -0.010, New P: 2.170
-Original Grad: -0.013, -lr * Pred Grad: -0.021, New P: 0.897
iter 27 loss: 0.018
Actual params: [2.1704, 0.8971]
-Original Grad: -0.001, -lr * Pred Grad: -0.019, New P: 2.151
-Original Grad: -0.012, -lr * Pred Grad: -0.045, New P: 0.852
iter 28 loss: 0.019
Actual params: [2.1511, 0.8521]
-Original Grad: -0.008, -lr * Pred Grad: -0.048, New P: 2.103
-Original Grad: -0.031, -lr * Pred Grad: -0.105, New P: 0.747
iter 29 loss: 0.021
Actual params: [2.1028, 0.7471]
-Original Grad: 0.010, -lr * Pred Grad: -0.014, New P: 2.089
-Original Grad: 0.027, -lr * Pred Grad: 0.000, New P: 0.747
iter 30 loss: 0.021
Actual params: [2.089 , 0.7475]
-Original Grad: 0.034, -lr * Pred Grad: 0.068, New P: 2.157
-Original Grad: 0.092, -lr * Pred Grad: 0.245, New P: 0.992
Target params: [1.3344, 1.5708]
iter 0 loss: 0.422
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.054, -lr * Pred Grad: 1.159, New P: 0.687
-Original Grad: 0.137, -lr * Pred Grad: 1.248, New P: 1.252
iter 1 loss: 0.049
Actual params: [0.6866, 1.2518]
-Original Grad: 0.114, -lr * Pred Grad: 1.338, New P: 2.025
-Original Grad: 0.175, -lr * Pred Grad: 1.086, New P: 2.338
iter 2 loss: 0.085
Actual params: [2.0249, 2.338 ]
-Original Grad: 0.034, -lr * Pred Grad: 0.558, New P: 2.583
-Original Grad: -0.705, -lr * Pred Grad: -0.786, New P: 1.552
iter 3 loss: 0.101
Actual params: [2.5829, 1.5519]
-Original Grad: -0.062, -lr * Pred Grad: -0.151, New P: 2.432
-Original Grad: 0.134, -lr * Pred Grad: -0.106, New P: 1.446
iter 4 loss: 0.099
Actual params: [2.4323, 1.446 ]
-Original Grad: -0.088, -lr * Pred Grad: -0.329, New P: 2.103
-Original Grad: 0.232, -lr * Pred Grad: 0.284, New P: 1.730
iter 5 loss: 0.036
Actual params: [2.1032, 1.7298]
-Original Grad: -0.067, -lr * Pred Grad: -0.343, New P: 1.760
-Original Grad: 0.190, -lr * Pred Grad: 0.782, New P: 2.512
iter 6 loss: 0.286
Actual params: [1.7604, 2.5118]
-Original Grad: 0.203, -lr * Pred Grad: 0.334, New P: 2.095
-Original Grad: -1.499, -lr * Pred Grad: -0.476, New P: 2.036
iter 7 loss: 0.019
Actual params: [2.0949, 2.0355]
-Original Grad: -0.036, -lr * Pred Grad: -0.033, New P: 2.062
-Original Grad: -0.013, -lr * Pred Grad: -0.430, New P: 1.605
iter 8 loss: 0.046
Actual params: [2.0623, 1.6053]
-Original Grad: -0.095, -lr * Pred Grad: -0.243, New P: 1.819
-Original Grad: 0.239, -lr * Pred Grad: -0.163, New P: 1.442
iter 9 loss: 0.043
Actual params: [1.8193, 1.4421]
-Original Grad: -0.091, -lr * Pred Grad: -0.393, New P: 1.427
-Original Grad: 0.242, -lr * Pred Grad: 0.425, New P: 1.867
iter 10 loss: 0.069
Actual params: [1.4265, 1.8668]
-Original Grad: 0.013, -lr * Pred Grad: -0.230, New P: 1.197
-Original Grad: -0.253, -lr * Pred Grad: -0.380, New P: 1.486
iter 11 loss: 0.040
Actual params: [1.1968, 1.4865]
-Original Grad: -0.024, -lr * Pred Grad: -0.251, New P: 0.946
-Original Grad: -0.113, -lr * Pred Grad: -0.314, New P: 1.172
iter 12 loss: 0.038
Actual params: [0.9457, 1.1722]
-Original Grad: -0.008, -lr * Pred Grad: -0.222, New P: 0.723
-Original Grad: 0.180, -lr * Pred Grad: 0.078, New P: 1.250
iter 13 loss: 0.045
Actual params: [0.7233, 1.2499]
-Original Grad: 0.073, -lr * Pred Grad: 0.011, New P: 0.735
-Original Grad: 0.100, -lr * Pred Grad: 0.305, New P: 1.555
iter 14 loss: 0.040
Actual params: [0.7346, 1.5548]
-Original Grad: 0.015, -lr * Pred Grad: -0.002, New P: 0.733
-Original Grad: -0.034, -lr * Pred Grad: -0.051, New P: 1.504
iter 15 loss: 0.039
Actual params: [0.7327, 1.5038]
-Original Grad: 0.088, -lr * Pred Grad: 0.229, New P: 0.961
-Original Grad: 0.017, -lr * Pred Grad: 0.029, New P: 1.533
iter 16 loss: 0.037
Actual params: [0.9613, 1.5332]
-Original Grad: -0.019, -lr * Pred Grad: 0.034, New P: 0.996
-Original Grad: -0.082, -lr * Pred Grad: -0.187, New P: 1.346
iter 17 loss: 0.032
Actual params: [0.9955, 1.346 ]
-Original Grad: -0.049, -lr * Pred Grad: -0.115, New P: 0.881
-Original Grad: 0.068, -lr * Pred Grad: 0.053, New P: 1.399
iter 18 loss: 0.032
Actual params: [0.8808, 1.3987]
-Original Grad: 0.047, -lr * Pred Grad: 0.073, New P: 0.954
-Original Grad: 0.003, -lr * Pred Grad: -0.002, New P: 1.397
iter 19 loss: 0.032
Actual params: [0.9537, 1.3968]
-Original Grad: 0.003, -lr * Pred Grad: 0.017, New P: 0.971
-Original Grad: -0.019, -lr * Pred Grad: -0.059, New P: 1.337
iter 20 loss: 0.032
Actual params: [0.9709, 1.3373]
-Original Grad: 0.007, -lr * Pred Grad: 0.025, New P: 0.996
-Original Grad: -0.008, -lr * Pred Grad: -0.061, New P: 1.276
iter 21 loss: 0.033
Actual params: [0.9963, 1.2763]
-Original Grad: -0.006, -lr * Pred Grad: -0.015, New P: 0.981
-Original Grad: 0.089, -lr * Pred Grad: 0.192, New P: 1.468
iter 22 loss: 0.034
Actual params: [0.9814, 1.4679]
-Original Grad: -0.011, -lr * Pred Grad: -0.048, New P: 0.933
-Original Grad: -0.012, -lr * Pred Grad: -0.001, New P: 1.467
iter 23 loss: 0.033
Actual params: [0.933 , 1.4668]
-Original Grad: -0.063, -lr * Pred Grad: -0.219, New P: 0.714
-Original Grad: -0.080, -lr * Pred Grad: -0.171, New P: 1.296
iter 24 loss: 0.043
Actual params: [0.714 , 1.2956]
-Original Grad: 0.030, -lr * Pred Grad: -0.059, New P: 0.655
-Original Grad: 0.110, -lr * Pred Grad: 0.165, New P: 1.461
iter 25 loss: 0.044
Actual params: [0.6547, 1.461 ]
-Original Grad: 0.152, -lr * Pred Grad: 0.312, New P: 0.966
-Original Grad: 0.113, -lr * Pred Grad: 0.428, New P: 1.889
iter 26 loss: 0.078
Actual params: [0.9665, 1.8891]
-Original Grad: -0.047, -lr * Pred Grad: -0.022, New P: 0.945
-Original Grad: -0.300, -lr * Pred Grad: -0.434, New P: 1.455
iter 27 loss: 0.033
Actual params: [0.9448, 1.4548]
-Original Grad: -0.006, -lr * Pred Grad: 0.003, New P: 0.947
-Original Grad: -0.030, -lr * Pred Grad: -0.236, New P: 1.219
iter 28 loss: 0.035
Actual params: [0.9473, 1.219 ]
-Original Grad: -0.009, -lr * Pred Grad: -0.033, New P: 0.914
-Original Grad: 0.078, -lr * Pred Grad: -0.035, New P: 1.184
iter 29 loss: 0.037
Actual params: [0.9143, 1.1844]
-Original Grad: 0.034, -lr * Pred Grad: 0.069, New P: 0.983
-Original Grad: 0.169, -lr * Pred Grad: 0.484, New P: 1.669
iter 30 loss: 0.047
Actual params: [0.983 , 1.6689]
-Original Grad: -0.103, -lr * Pred Grad: -0.267, New P: 0.716
-Original Grad: -0.149, -lr * Pred Grad: -0.306, New P: 1.363
Target params: [1.3344, 1.5708]
iter 0 loss: 0.565
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad: 0.655, New P: 0.182
-Original Grad: 0.002, -lr * Pred Grad: 0.642, New P: 0.645
iter 1 loss: 0.503
Actual params: [0.1822, 0.6452]
-Original Grad: 0.262, -lr * Pred Grad: 2.021, New P: 2.203
-Original Grad: 0.356, -lr * Pred Grad: 1.939, New P: 2.585
iter 2 loss: 0.261
Actual params: [2.2035, 2.5846]
-Original Grad: -0.678, -lr * Pred Grad: -1.675, New P: 0.529
-Original Grad: -1.417, -lr * Pred Grad: -0.798, New P: 1.787
iter 3 loss: 0.193
Actual params: [0.5287, 1.787 ]
-Original Grad: 0.458, -lr * Pred Grad: 0.230, New P: 0.759
-Original Grad: 0.149, -lr * Pred Grad: -0.201, New P: 1.586
iter 4 loss: 0.129
Actual params: [0.759 , 1.5855]
-Original Grad: 0.406, -lr * Pred Grad: 0.386, New P: 1.145
-Original Grad: 0.285, -lr * Pred Grad: 0.078, New P: 1.664
iter 5 loss: 0.043
Actual params: [1.1453, 1.6637]
-Original Grad: 0.062, -lr * Pred Grad: 0.300, New P: 1.445
-Original Grad: 0.239, -lr * Pred Grad: 0.730, New P: 2.394
iter 6 loss: 0.035
Actual params: [1.4451, 2.394 ]
-Original Grad: 0.194, -lr * Pred Grad: 0.655, New P: 2.100
-Original Grad: -0.613, -lr * Pred Grad: -0.472, New P: 1.922
iter 7 loss: 0.157
Actual params: [2.1   , 1.9222]
-Original Grad: -0.346, -lr * Pred Grad: -0.539, New P: 1.562
-Original Grad: 0.238, -lr * Pred Grad: -0.057, New P: 1.865
iter 8 loss: 0.058
Actual params: [1.5615, 1.8649]
-Original Grad: -0.126, -lr * Pred Grad: -0.531, New P: 1.031
-Original Grad: 0.280, -lr * Pred Grad: 0.662, New P: 2.527
iter 9 loss: 0.199
Actual params: [1.0309, 2.5273]
-Original Grad: 0.091, -lr * Pred Grad: -0.155, New P: 0.876
-Original Grad: -1.226, -lr * Pred Grad: -0.441, New P: 2.086
iter 10 loss: 0.111
Actual params: [0.8762, 2.0865]
-Original Grad: 0.312, -lr * Pred Grad: 0.397, New P: 1.273
-Original Grad: -0.427, -lr * Pred Grad: -0.495, New P: 1.592
iter 11 loss: 0.054
Actual params: [1.2731, 1.5917]
-Original Grad: -0.146, -lr * Pred Grad: -0.198, New P: 1.075
-Original Grad: 0.395, -lr * Pred Grad: -0.165, New P: 1.426
iter 12 loss: 0.070
Actual params: [1.075 , 1.4263]
-Original Grad: 0.002, -lr * Pred Grad: -0.073, New P: 1.002
-Original Grad: 0.467, -lr * Pred Grad: 1.020, New P: 2.446
iter 13 loss: 0.172
Actual params: [1.0016, 2.4461]
-Original Grad: 0.182, -lr * Pred Grad: 0.339, New P: 1.341
-Original Grad: -1.250, -lr * Pred Grad: -0.495, New P: 1.951
iter 14 loss: 0.032
Actual params: [1.341 , 1.9511]
-Original Grad: -0.126, -lr * Pred Grad: -0.187, New P: 1.154
-Original Grad: 0.174, -lr * Pred Grad: -0.317, New P: 1.634
iter 15 loss: 0.045
Actual params: [1.1541, 1.634 ]
-Original Grad: -0.030, -lr * Pred Grad: -0.158, New P: 0.996
-Original Grad: 0.244, -lr * Pred Grad: 0.025, New P: 1.659
iter 16 loss: 0.057
Actual params: [0.9961, 1.6589]
-Original Grad: 0.118, -lr * Pred Grad: 0.178, New P: 1.174
-Original Grad: 0.161, -lr * Pred Grad: 0.364, New P: 2.023
iter 17 loss: 0.031
Actual params: [1.1738, 2.023 ]
-Original Grad: 0.115, -lr * Pred Grad: 0.321, New P: 1.495
-Original Grad: -0.151, -lr * Pred Grad: -0.281, New P: 1.742
iter 18 loss: 0.065
Actual params: [1.4949, 1.7422]
-Original Grad: -0.201, -lr * Pred Grad: -0.362, New P: 1.133
-Original Grad: 0.382, -lr * Pred Grad: 0.861, New P: 2.604
iter 19 loss: 0.216
Actual params: [1.133 , 2.6037]
-Original Grad: 0.057, -lr * Pred Grad: -0.037, New P: 1.096
-Original Grad: -1.578, -lr * Pred Grad: -0.460, New P: 2.144
iter 20 loss: 0.060
Actual params: [1.0961, 2.144 ]
-Original Grad: 0.257, -lr * Pred Grad: 0.479, New P: 1.575
-Original Grad: -0.421, -lr * Pred Grad: -0.485, New P: 1.659
iter 21 loss: 0.088
Actual params: [1.5751, 1.659 ]
-Original Grad: -0.193, -lr * Pred Grad: -0.305, New P: 1.270
-Original Grad: 0.515, -lr * Pred Grad: -0.059, New P: 1.600
iter 22 loss: 0.053
Actual params: [1.2697, 1.5998]
-Original Grad: -0.120, -lr * Pred Grad: -0.450, New P: 0.820
-Original Grad: 0.378, -lr * Pred Grad: 0.816, New P: 2.416
iter 23 loss: 0.213
Actual params: [0.8201, 2.4162]
-Original Grad: 0.104, -lr * Pred Grad: -0.031, New P: 0.789
-Original Grad: -0.960, -lr * Pred Grad: -0.459, New P: 1.957
iter 24 loss: 0.124
Actual params: [0.7894, 1.957 ]
-Original Grad: 0.386, -lr * Pred Grad: 0.618, New P: 1.407
-Original Grad: -0.131, -lr * Pred Grad: -0.433, New P: 1.524
iter 25 loss: 0.083
Actual params: [1.4069, 1.5243]
-Original Grad: -0.259, -lr * Pred Grad: -0.405, New P: 1.002
-Original Grad: 0.654, -lr * Pred Grad: 0.629, New P: 2.153
iter 26 loss: 0.085
Actual params: [1.0019, 2.1533]
-Original Grad: 0.215, -lr * Pred Grad: 0.287, New P: 1.289
-Original Grad: -0.473, -lr * Pred Grad: -0.449, New P: 1.704
iter 27 loss: 0.045
Actual params: [1.2888, 1.7042]
-Original Grad: -0.167, -lr * Pred Grad: -0.288, New P: 1.001
-Original Grad: 0.248, -lr * Pred Grad: -0.014, New P: 1.690
iter 28 loss: 0.055
Actual params: [1.0006, 1.6899]
-Original Grad: 0.183, -lr * Pred Grad: 0.236, New P: 1.236
-Original Grad: 0.167, -lr * Pred Grad: 0.338, New P: 2.028
iter 29 loss: 0.026
Actual params: [1.2361, 2.0283]
-Original Grad: 0.051, -lr * Pred Grad: 0.155, New P: 1.391
-Original Grad: -0.048, -lr * Pred Grad: -0.070, New P: 1.958
iter 30 loss: 0.035
Actual params: [1.3908, 1.9583]
-Original Grad: -0.108, -lr * Pred Grad: -0.159, New P: 1.232
-Original Grad: 0.183, -lr * Pred Grad: 0.526, New P: 2.484
Target params: [1.3344, 1.5708]
iter 0 loss: 0.548
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.002, -lr * Pred Grad: 0.622, New P: 0.150
-Original Grad: 0.000, -lr * Pred Grad: 0.634, New P: 0.638
iter 1 loss: 0.536
Actual params: [0.1501, 0.6377]
-Original Grad: 0.168, -lr * Pred Grad: 1.681, New P: 1.831
-Original Grad: 0.723, -lr * Pred Grad: 2.634, New P: 3.272
iter 2 loss: 0.463
Actual params: [1.8307, 3.2721]
-Original Grad: -0.046, -lr * Pred Grad: -0.339, New P: 1.492
-Original Grad: -0.542, -lr * Pred Grad: -1.113, New P: 2.159
iter 3 loss: 0.098
Actual params: [1.4918, 2.1592]
-Original Grad: -0.055, -lr * Pred Grad: -0.090, New P: 1.402
-Original Grad: -0.457, -lr * Pred Grad: -0.235, New P: 1.924
iter 4 loss: 0.048
Actual params: [1.4018, 1.924 ]
-Original Grad: -0.022, -lr * Pred Grad: -0.081, New P: 1.321
-Original Grad: -0.236, -lr * Pred Grad: -0.564, New P: 1.360
iter 5 loss: 0.030
Actual params: [1.3207, 1.3598]
-Original Grad: 0.112, -lr * Pred Grad: 0.323, New P: 1.644
-Original Grad: 0.059, -lr * Pred Grad: -0.299, New P: 1.061
iter 6 loss: 0.077
Actual params: [1.6441, 1.0607]
-Original Grad: 0.085, -lr * Pred Grad: 0.350, New P: 1.994
-Original Grad: 0.735, -lr * Pred Grad: 1.607, New P: 2.668
iter 7 loss: 0.290
Actual params: [1.9941, 2.6676]
-Original Grad: -0.084, -lr * Pred Grad: -0.078, New P: 1.916
-Original Grad: -0.549, -lr * Pred Grad: -0.759, New P: 1.908
iter 8 loss: 0.071
Actual params: [1.9158, 1.9082]
-Original Grad: -0.096, -lr * Pred Grad: -0.285, New P: 1.631
-Original Grad: -0.363, -lr * Pred Grad: -0.313, New P: 1.595
iter 9 loss: 0.017
Actual params: [1.6307, 1.5948]
-Original Grad: 0.006, -lr * Pred Grad: -0.137, New P: 1.494
-Original Grad: -0.092, -lr * Pred Grad: -0.510, New P: 1.085
iter 10 loss: 0.108
Actual params: [1.4937, 1.0846]
-Original Grad: 0.288, -lr * Pred Grad: 0.517, New P: 2.011
-Original Grad: 1.375, -lr * Pred Grad: 1.906, New P: 2.991
iter 11 loss: 0.387
Actual params: [2.0111, 2.9909]
-Original Grad: -0.058, -lr * Pred Grad: -0.024, New P: 1.987
-Original Grad: -0.517, -lr * Pred Grad: -0.849, New P: 2.142
iter 12 loss: 0.135
Actual params: [1.9869, 2.1416]
-Original Grad: -0.110, -lr * Pred Grad: -0.234, New P: 1.753
-Original Grad: -0.483, -lr * Pred Grad: -0.299, New P: 1.843
iter 13 loss: 0.046
Actual params: [1.7532, 1.8429]
-Original Grad: -0.034, -lr * Pred Grad: -0.210, New P: 1.543
-Original Grad: -0.217, -lr * Pred Grad: -0.556, New P: 1.287
iter 14 loss: 0.015
Actual params: [1.543 , 1.2867]
-Original Grad: 0.103, -lr * Pred Grad: 0.113, New P: 1.656
-Original Grad: 0.152, -lr * Pred Grad: -0.198, New P: 1.089
iter 15 loss: 0.061
Actual params: [1.6559, 1.0888]
-Original Grad: 0.425, -lr * Pred Grad: 0.806, New P: 2.462
-Original Grad: 1.086, -lr * Pred Grad: 2.119, New P: 3.208
iter 16 loss: 0.463
Actual params: [2.4622, 3.2079]
-Original Grad: -0.020, -lr * Pred Grad: 0.082, New P: 2.544
-Original Grad: -0.485, -lr * Pred Grad: -0.941, New P: 2.267
iter 17 loss: 0.231
Actual params: [2.5444, 2.2666]
-Original Grad: -0.081, -lr * Pred Grad: -0.015, New P: 2.529
-Original Grad: -0.508, -lr * Pred Grad: -0.269, New P: 1.998
iter 18 loss: 0.166
Actual params: [2.5293, 1.9977]
-Original Grad: -0.082, -lr * Pred Grad: -0.184, New P: 2.345
-Original Grad: -0.331, -lr * Pred Grad: -0.567, New P: 1.431
iter 19 loss: 0.082
Actual params: [2.345 , 1.4306]
-Original Grad: -0.100, -lr * Pred Grad: -0.348, New P: 1.997
-Original Grad: -0.026, -lr * Pred Grad: -0.442, New P: 0.989
iter 20 loss: 0.074
Actual params: [1.9967, 0.9888]
-Original Grad: -0.078, -lr * Pred Grad: -0.427, New P: 1.570
-Original Grad: 0.746, -lr * Pred Grad: 1.318, New P: 2.307
iter 21 loss: 0.141
Actual params: [1.5697, 2.3069]
-Original Grad: -0.057, -lr * Pred Grad: -0.456, New P: 1.113
-Original Grad: -0.535, -lr * Pred Grad: -0.655, New P: 1.652
iter 22 loss: 0.036
Actual params: [1.1134, 1.6522]
-Original Grad: 0.023, -lr * Pred Grad: -0.296, New P: 0.817
-Original Grad: 0.042, -lr * Pred Grad: -0.163, New P: 1.489
iter 23 loss: 0.039
Actual params: [0.817 , 1.4888]
-Original Grad: -0.234, -lr * Pred Grad: -0.789, New P: 0.028
-Original Grad: 0.222, -lr * Pred Grad: 0.203, New P: 1.691
iter 24 loss: 0.227
Actual params: [0.0278, 1.6913]
-Original Grad: 0.504, -lr * Pred Grad: 0.381, New P: 0.409
-Original Grad: 0.101, -lr * Pred Grad: 0.386, New P: 2.077
iter 25 loss: 0.110
Actual params: [0.4088, 2.0769]
-Original Grad: 0.274, -lr * Pred Grad: 0.462, New P: 0.871
-Original Grad: -0.479, -lr * Pred Grad: -0.429, New P: 1.648
iter 26 loss: 0.031
Actual params: [0.8707, 1.6476]
-Original Grad: -0.084, -lr * Pred Grad: 0.060, New P: 0.931
-Original Grad: 0.089, -lr * Pred Grad: -0.219, New P: 1.428
iter 27 loss: 0.062
Actual params: [0.9309, 1.4283]
-Original Grad: -0.002, -lr * Pred Grad: 0.120, New P: 1.051
-Original Grad: 0.297, -lr * Pred Grad: 0.592, New P: 2.020
iter 28 loss: 0.045
Actual params: [1.0506, 2.02  ]
-Original Grad: -0.053, -lr * Pred Grad: -0.071, New P: 0.980
-Original Grad: -0.264, -lr * Pred Grad: -0.430, New P: 1.590
iter 29 loss: 0.042
Actual params: [0.9798, 1.5904]
-Original Grad: -0.035, -lr * Pred Grad: -0.117, New P: 0.863
-Original Grad: 0.208, -lr * Pred Grad: 0.225, New P: 1.815
iter 30 loss: 0.026
Actual params: [0.8628, 1.8154]
-Original Grad: -0.028, -lr * Pred Grad: -0.147, New P: 0.716
-Original Grad: -0.058, -lr * Pred Grad: -0.112, New P: 1.704
Target params: [1.3344, 1.5708]
iter 0 loss: 0.288
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.037, -lr * Pred Grad: 0.249, New P: -0.224
-Original Grad: 0.123, -lr * Pred Grad: 1.192, New P: 1.195
iter 1 loss: 0.228
Actual params: [-0.2237,  1.1955]
-Original Grad: 0.161, -lr * Pred Grad: 1.554, New P: 1.331
-Original Grad: 0.012, -lr * Pred Grad: 0.137, New P: 1.333
iter 2 loss: 0.036
Actual params: [1.3306, 1.3327]
-Original Grad: -0.034, -lr * Pred Grad: -0.248, New P: 1.083
-Original Grad: 0.137, -lr * Pred Grad: 0.570, New P: 1.902
iter 3 loss: 0.013
Actual params: [1.0828, 1.9025]
-Original Grad: -0.027, -lr * Pred Grad: 0.081, New P: 1.164
-Original Grad: -0.137, -lr * Pred Grad: -0.427, New P: 1.475
iter 4 loss: 0.021
Actual params: [1.1636, 1.4753]
-Original Grad: -0.035, -lr * Pred Grad: -0.099, New P: 1.065
-Original Grad: 0.113, -lr * Pred Grad: 0.151, New P: 1.626
iter 5 loss: 0.010
Actual params: [1.0645, 1.6262]
-Original Grad: -0.037, -lr * Pred Grad: -0.159, New P: 0.906
-Original Grad: 0.089, -lr * Pred Grad: 0.263, New P: 1.889
iter 6 loss: 0.008
Actual params: [0.9057, 1.8889]
-Original Grad: -0.003, -lr * Pred Grad: -0.101, New P: 0.805
-Original Grad: -0.097, -lr * Pred Grad: -0.172, New P: 1.717
iter 7 loss: 0.007
Actual params: [0.805 , 1.7168]
-Original Grad: 0.035, -lr * Pred Grad: 0.022, New P: 0.827
-Original Grad: 0.016, -lr * Pred Grad: -0.031, New P: 1.686
iter 8 loss: 0.006
Actual params: [0.8272, 1.6862]
-Original Grad: 0.037, -lr * Pred Grad: 0.091, New P: 0.918
-Original Grad: 0.031, -lr * Pred Grad: 0.031, New P: 1.718
iter 9 loss: 0.004
Actual params: [0.9184, 1.7177]
-Original Grad: -0.010, -lr * Pred Grad: 0.009, New P: 0.927
-Original Grad: 0.035, -lr * Pred Grad: 0.093, New P: 1.810
iter 10 loss: 0.005
Actual params: [0.9271, 1.8103]
-Original Grad: -0.007, -lr * Pred Grad: -0.015, New P: 0.912
-Original Grad: -0.025, -lr * Pred Grad: -0.047, New P: 1.763
iter 11 loss: 0.004
Actual params: [0.9125, 1.7633]
-Original Grad: -0.011, -lr * Pred Grad: -0.048, New P: 0.864
-Original Grad: -0.004, -lr * Pred Grad: -0.036, New P: 1.728
iter 12 loss: 0.005
Actual params: [0.8644, 1.7278]
-Original Grad: 0.017, -lr * Pred Grad: 0.009, New P: 0.874
-Original Grad: 0.006, -lr * Pred Grad: -0.021, New P: 1.707
iter 13 loss: 0.005
Actual params: [0.8736, 1.7071]
-Original Grad: -0.008, -lr * Pred Grad: -0.037, New P: 0.837
-Original Grad: 0.024, -lr * Pred Grad: 0.036, New P: 1.743
iter 14 loss: 0.005
Actual params: [0.837 , 1.7428]
-Original Grad: 0.035, -lr * Pred Grad: 0.068, New P: 0.905
-Original Grad: -0.003, -lr * Pred Grad: -0.014, New P: 1.729
iter 15 loss: 0.004
Actual params: [0.9047, 1.7291]
-Original Grad: 0.012, -lr * Pred Grad: 0.048, New P: 0.953
-Original Grad: 0.014, -lr * Pred Grad: 0.015, New P: 1.745
iter 16 loss: 0.004
Actual params: [0.9529, 1.7445]
-Original Grad: -0.010, -lr * Pred Grad: -0.010, New P: 0.943
-Original Grad: 0.021, -lr * Pred Grad: 0.039, New P: 1.784
iter 17 loss: 0.005
Actual params: [0.9427, 1.7839]
-Original Grad: -0.032, -lr * Pred Grad: -0.104, New P: 0.839
-Original Grad: -0.076, -lr * Pred Grad: -0.168, New P: 1.616
iter 18 loss: 0.008
Actual params: [0.8385, 1.6157]
-Original Grad: 0.038, -lr * Pred Grad: 0.038, New P: 0.877
-Original Grad: 0.063, -lr * Pred Grad: 0.056, New P: 1.671
iter 19 loss: 0.005
Actual params: [0.877 , 1.6713]
-Original Grad: 0.025, -lr * Pred Grad: 0.061, New P: 0.938
-Original Grad: 0.020, -lr * Pred Grad: 0.047, New P: 1.719
iter 20 loss: 0.004
Actual params: [0.9377, 1.7187]
-Original Grad: -0.017, -lr * Pred Grad: -0.027, New P: 0.911
-Original Grad: 0.018, -lr * Pred Grad: 0.050, New P: 1.769
iter 21 loss: 0.004
Actual params: [0.9105, 1.7688]
-Original Grad: -0.022, -lr * Pred Grad: -0.083, New P: 0.828
-Original Grad: -0.008, -lr * Pred Grad: -0.022, New P: 1.746
iter 22 loss: 0.006
Actual params: [0.8278, 1.7464]
-Original Grad: 0.051, -lr * Pred Grad: 0.084, New P: 0.912
-Original Grad: -0.003, -lr * Pred Grad: -0.030, New P: 1.717
iter 23 loss: 0.004
Actual params: [0.912 , 1.7169]
-Original Grad: 0.004, -lr * Pred Grad: 0.029, New P: 0.941
-Original Grad: 0.010, -lr * Pred Grad: -0.007, New P: 1.710
iter 24 loss: 0.004
Actual params: [0.941 , 1.7101]
-Original Grad: -0.013, -lr * Pred Grad: -0.025, New P: 0.916
-Original Grad: 0.022, -lr * Pred Grad: 0.033, New P: 1.743
iter 25 loss: 0.004
Actual params: [0.9156, 1.7431]
-Original Grad: -0.006, -lr * Pred Grad: -0.040, New P: 0.876
-Original Grad: -0.030, -lr * Pred Grad: -0.078, New P: 1.665
iter 26 loss: 0.005
Actual params: [0.8759, 1.6653]
-Original Grad: 0.012, -lr * Pred Grad: -0.002, New P: 0.874
-Original Grad: 0.043, -lr * Pred Grad: 0.060, New P: 1.726
iter 27 loss: 0.004
Actual params: [0.8742, 1.7255]
-Original Grad: 0.004, -lr * Pred Grad: -0.007, New P: 0.867
-Original Grad: -0.012, -lr * Pred Grad: -0.032, New P: 1.693
iter 28 loss: 0.005
Actual params: [0.8674, 1.6934]
-Original Grad: 0.008, -lr * Pred Grad: 0.003, New P: 0.871
-Original Grad: 0.019, -lr * Pred Grad: 0.022, New P: 1.716
iter 29 loss: 0.005
Actual params: [0.8709, 1.7157]
-Original Grad: -0.004, -lr * Pred Grad: -0.025, New P: 0.846
-Original Grad: 0.012, -lr * Pred Grad: 0.017, New P: 1.733
iter 30 loss: 0.005
Actual params: [0.8455, 1.733 ]
-Original Grad: 0.029, -lr * Pred Grad: 0.055, New P: 0.901
-Original Grad: -0.013, -lr * Pred Grad: -0.042, New P: 1.691
Target params: [1.3344, 1.5708]
iter 0 loss: 0.879
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.216, -lr * Pred Grad: -1.463, New P: -1.936
-Original Grad: 0.222, -lr * Pred Grad: 1.555, New P: 1.559
iter 1 loss: 0.827
Actual params: [-1.9356,  1.5586]
-Original Grad: 0.000, -lr * Pred Grad: -0.455, New P: -2.390
-Original Grad: -0.000, -lr * Pred Grad: 0.022, New P: 1.581
iter 2 loss: 0.827
Actual params: [-2.3904,  1.5811]
-Original Grad: 0.000, -lr * Pred Grad: -0.327, New P: -2.717
-Original Grad: -0.000, -lr * Pred Grad: -0.059, New P: 1.522
iter 3 loss: 0.827
Actual params: [-2.717,  1.522]
-Original Grad: 0.000, -lr * Pred Grad: -0.263, New P: -2.980
-Original Grad: -0.000, -lr * Pred Grad: -0.011, New P: 1.511
iter 4 loss: 0.827
Actual params: [-2.9798,  1.5112]
-Original Grad: 0.000, -lr * Pred Grad: -0.199, New P: -3.179
-Original Grad: -0.000, -lr * Pred Grad: -0.040, New P: 1.471
iter 5 loss: 0.827
Actual params: [-3.1791,  1.4713]
-Original Grad: 0.000, -lr * Pred Grad: -0.153, New P: -3.332
-Original Grad: -0.000, -lr * Pred Grad: -0.039, New P: 1.432
iter 6 loss: 0.827
Actual params: [-3.3316,  1.4322]
-Original Grad: 0.000, -lr * Pred Grad: -0.119, New P: -3.451
-Original Grad: -0.000, -lr * Pred Grad: -0.036, New P: 1.396
iter 7 loss: 0.827
Actual params: [-3.4509,  1.3963]
-Original Grad: 0.000, -lr * Pred Grad: -0.096, New P: -3.547
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 1.365
iter 8 loss: 0.827
Actual params: [-3.5473,  1.3652]
-Original Grad: 0.000, -lr * Pred Grad: -0.081, New P: -3.628
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 1.336
iter 9 loss: 0.827
Actual params: [-3.6281,  1.3364]
-Original Grad: 0.000, -lr * Pred Grad: -0.070, New P: -3.698
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 1.308
iter 10 loss: 0.827
Actual params: [-3.6981,  1.3083]
-Original Grad: 0.000, -lr * Pred Grad: -0.062, New P: -3.760
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 1.280
iter 11 loss: 0.827
Actual params: [-3.7605,  1.2801]
-Original Grad: 0.000, -lr * Pred Grad: -0.057, New P: -3.817
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 1.251
iter 12 loss: 0.827
Actual params: [-3.8174,  1.2515]
-Original Grad: 0.000, -lr * Pred Grad: -0.053, New P: -3.870
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 1.222
iter 13 loss: 0.827
Actual params: [-3.8704,  1.2222]
-Original Grad: 0.000, -lr * Pred Grad: -0.050, New P: -3.921
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 1.192
iter 14 loss: 0.827
Actual params: [-3.9207,  1.1923]
-Original Grad: 0.000, -lr * Pred Grad: -0.048, New P: -3.969
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 1.162
iter 15 loss: 0.827
Actual params: [-3.9688,  1.1619]
-Original Grad: 0.000, -lr * Pred Grad: -0.047, New P: -4.015
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 1.131
iter 16 loss: 0.827
Actual params: [-4.0155,  1.1309]
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: -4.061
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 1.100
iter 17 loss: 0.827
Actual params: [-4.0611,  1.0996]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -4.106
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.068
iter 18 loss: 0.827
Actual params: [-4.106,  1.068]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -4.150
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.036
iter 19 loss: 0.827
Actual params: [-4.1503,  1.0361]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -4.194
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 1.004
iter 20 loss: 0.827
Actual params: [-4.1943,  1.0041]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -4.238
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.972
iter 21 loss: 0.827
Actual params: [-4.238 ,  0.9719]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -4.282
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.940
iter 22 loss: 0.827
Actual params: [-4.2816,  0.9396]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -4.325
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.907
iter 23 loss: 0.827
Actual params: [-4.325 ,  0.9073]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -4.368
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.875
iter 24 loss: 0.827
Actual params: [-4.3684,  0.8748]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -4.412
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.842
iter 25 loss: 0.827
Actual params: [-4.4118,  0.8424]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -4.455
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.810
iter 26 loss: 0.827
Actual params: [-4.4551,  0.8099]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -4.498
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.777
iter 27 loss: 0.827
Actual params: [-4.4985,  0.7774]
-Original Grad: 0.000, -lr * Pred Grad: -0.043, New P: -4.542
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.745
iter 28 loss: 0.827
Actual params: [-4.5418,  0.7449]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -4.585
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.712
iter 29 loss: 0.827
Actual params: [-4.5852,  0.7123]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -4.629
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.680
iter 30 loss: 0.827
Actual params: [-4.6285,  0.6798]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -4.672
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.647
Target params: [1.3344, 1.5708]
iter 0 loss: 0.118
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.083, -lr * Pred Grad: -0.271, New P: -0.744
-Original Grad: -0.002, -lr * Pred Grad: 0.626, New P: 0.630
iter 1 loss: 0.106
Actual params: [-0.7438,  0.6296]
-Original Grad: -0.003, -lr * Pred Grad: 0.144, New P: -0.599
-Original Grad: 0.004, -lr * Pred Grad: 0.142, New P: 0.771
iter 2 loss: 0.106
Actual params: [-0.5994,  0.7712]
-Original Grad: -0.003, -lr * Pred Grad: 0.017, New P: -0.582
-Original Grad: 0.008, -lr * Pred Grad: -0.016, New P: 0.755
iter 3 loss: 0.106
Actual params: [-0.5821,  0.7551]
-Original Grad: -0.005, -lr * Pred Grad: -0.006, New P: -0.588
-Original Grad: 0.011, -lr * Pred Grad: 0.007, New P: 0.762
iter 4 loss: 0.106
Actual params: [-0.5877,  0.7621]
-Original Grad: -0.004, -lr * Pred Grad: -0.023, New P: -0.611
-Original Grad: 0.009, -lr * Pred Grad: -0.014, New P: 0.748
iter 5 loss: 0.106
Actual params: [-0.611 ,  0.7476]
-Original Grad: -0.004, -lr * Pred Grad: -0.035, New P: -0.646
-Original Grad: 0.007, -lr * Pred Grad: -0.013, New P: 0.734
iter 6 loss: 0.106
Actual params: [-0.6464,  0.7343]
-Original Grad: -0.004, -lr * Pred Grad: -0.044, New P: -0.691
-Original Grad: 0.007, -lr * Pred Grad: -0.011, New P: 0.723
iter 7 loss: 0.106
Actual params: [-0.6906,  0.7228]
-Original Grad: -0.002, -lr * Pred Grad: -0.044, New P: -0.735
-Original Grad: 0.003, -lr * Pred Grad: -0.016, New P: 0.707
iter 8 loss: 0.106
Actual params: [-0.7346,  0.7071]
-Original Grad: -0.002, -lr * Pred Grad: -0.046, New P: -0.780
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: 0.690
iter 9 loss: 0.106
Actual params: [-0.7803,  0.6897]
-Original Grad: -0.001, -lr * Pred Grad: -0.047, New P: -0.827
-Original Grad: 0.002, -lr * Pred Grad: -0.019, New P: 0.671
iter 10 loss: 0.106
Actual params: [-0.8275,  0.6711]
-Original Grad: -0.001, -lr * Pred Grad: -0.047, New P: -0.875
-Original Grad: 0.002, -lr * Pred Grad: -0.021, New P: 0.650
iter 11 loss: 0.106
Actual params: [-0.8746,  0.6503]
-Original Grad: -0.001, -lr * Pred Grad: -0.047, New P: -0.922
-Original Grad: 0.001, -lr * Pred Grad: -0.023, New P: 0.628
iter 12 loss: 0.106
Actual params: [-0.922 ,  0.6276]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -0.968
-Original Grad: 0.001, -lr * Pred Grad: -0.026, New P: 0.602
iter 13 loss: 0.106
Actual params: [-0.9684,  0.6021]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.014
-Original Grad: 0.000, -lr * Pred Grad: -0.027, New P: 0.575
iter 14 loss: 0.106
Actual params: [-1.0145,  0.5748]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.060
-Original Grad: 0.000, -lr * Pred Grad: -0.028, New P: 0.547
iter 15 loss: 0.106
Actual params: [-1.0603,  0.5466]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.106
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 0.517
iter 16 loss: 0.106
Actual params: [-1.1057,  0.5173]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.151
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.487
iter 17 loss: 0.106
Actual params: [-1.1511,  0.4875]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.196
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.457
iter 18 loss: 0.106
Actual params: [-1.1962,  0.457 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.241
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.426
iter 19 loss: 0.106
Actual params: [-1.2411,  0.4261]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.286
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.395
iter 20 loss: 0.106
Actual params: [-1.2859,  0.3947]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.330
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.363
iter 21 loss: 0.106
Actual params: [-1.3305,  0.3631]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.375
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.331
iter 22 loss: 0.106
Actual params: [-1.3748,  0.3311]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.419
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.299
iter 23 loss: 0.106
Actual params: [-1.419,  0.299]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.463
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.267
iter 24 loss: 0.106
Actual params: [-1.463 ,  0.2668]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.507
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.235
iter 25 loss: 0.106
Actual params: [-1.5069,  0.2345]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.551
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.202
iter 26 loss: 0.106
Actual params: [-1.5508,  0.2021]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.595
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.170
iter 27 loss: 0.106
Actual params: [-1.5946,  0.1697]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.638
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.137
iter 28 loss: 0.106
Actual params: [-1.6383,  0.1373]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.682
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.105
iter 29 loss: 0.106
Actual params: [-1.6819,  0.1048]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.726
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.072
iter 30 loss: 0.106
Actual params: [-1.7255,  0.0723]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.769
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.040
Target params: [1.3344, 1.5708]
iter 0 loss: 0.194
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.065, -lr * Pred Grad: -0.075, New P: -0.547
-Original Grad: 0.067, -lr * Pred Grad: 0.949, New P: 0.953
iter 1 loss: 0.179
Actual params: [-0.5473,  0.953 ]
-Original Grad: -0.003, -lr * Pred Grad: 0.211, New P: -0.336
-Original Grad: 0.003, -lr * Pred Grad: 0.119, New P: 1.072
iter 2 loss: 0.179
Actual params: [-0.3358,  1.0715]
-Original Grad: -0.006, -lr * Pred Grad: 0.041, New P: -0.295
-Original Grad: 0.007, -lr * Pred Grad: -0.045, New P: 1.027
iter 3 loss: 0.180
Actual params: [-0.2949,  1.0266]
-Original Grad: -0.011, -lr * Pred Grad: -0.018, New P: -0.313
-Original Grad: 0.011, -lr * Pred Grad: 0.016, New P: 1.042
iter 4 loss: 0.179
Actual params: [-0.313 ,  1.0423]
-Original Grad: -0.008, -lr * Pred Grad: -0.043, New P: -0.356
-Original Grad: 0.008, -lr * Pred Grad: -0.014, New P: 1.029
iter 5 loss: 0.179
Actual params: [-0.3556,  1.0285]
-Original Grad: -0.005, -lr * Pred Grad: -0.049, New P: -0.405
-Original Grad: 0.005, -lr * Pred Grad: -0.017, New P: 1.011
iter 6 loss: 0.179
Actual params: [-0.4046,  1.0112]
-Original Grad: -0.005, -lr * Pred Grad: -0.059, New P: -0.464
-Original Grad: 0.005, -lr * Pred Grad: -0.017, New P: 0.994
iter 7 loss: 0.179
Actual params: [-0.4639,  0.9942]
-Original Grad: -0.005, -lr * Pred Grad: -0.065, New P: -0.529
-Original Grad: 0.005, -lr * Pred Grad: -0.013, New P: 0.982
iter 8 loss: 0.179
Actual params: [-0.5289,  0.9817]
-Original Grad: -0.002, -lr * Pred Grad: -0.062, New P: -0.591
-Original Grad: 0.003, -lr * Pred Grad: -0.016, New P: 0.966
iter 9 loss: 0.178
Actual params: [-0.5908,  0.9659]
-Original Grad: -0.002, -lr * Pred Grad: -0.060, New P: -0.650
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: 0.949
iter 10 loss: 0.178
Actual params: [-0.6505,  0.9491]
-Original Grad: -0.002, -lr * Pred Grad: -0.058, New P: -0.708
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: 0.932
iter 11 loss: 0.178
Actual params: [-0.7084,  0.9317]
-Original Grad: -0.001, -lr * Pred Grad: -0.056, New P: -0.764
-Original Grad: 0.002, -lr * Pred Grad: -0.019, New P: 0.913
iter 12 loss: 0.178
Actual params: [-0.7641,  0.9128]
-Original Grad: -0.001, -lr * Pred Grad: -0.054, New P: -0.818
-Original Grad: 0.002, -lr * Pred Grad: -0.021, New P: 0.892
iter 13 loss: 0.178
Actual params: [-0.8184,  0.8916]
-Original Grad: -0.001, -lr * Pred Grad: -0.053, New P: -0.871
-Original Grad: 0.001, -lr * Pred Grad: -0.023, New P: 0.869
iter 14 loss: 0.178
Actual params: [-0.8712,  0.8686]
-Original Grad: -0.001, -lr * Pred Grad: -0.052, New P: -0.923
-Original Grad: 0.002, -lr * Pred Grad: -0.024, New P: 0.845
iter 15 loss: 0.178
Actual params: [-0.9232,  0.8448]
-Original Grad: -0.001, -lr * Pred Grad: -0.052, New P: -0.975
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 0.820
iter 16 loss: 0.178
Actual params: [-0.9748,  0.8203]
-Original Grad: -0.001, -lr * Pred Grad: -0.051, New P: -1.025
-Original Grad: 0.001, -lr * Pred Grad: -0.026, New P: 0.795
iter 17 loss: 0.178
Actual params: [-1.0255,  0.7946]
-Original Grad: -0.001, -lr * Pred Grad: -0.051, New P: -1.076
-Original Grad: 0.001, -lr * Pred Grad: -0.026, New P: 0.769
iter 18 loss: 0.178
Actual params: [-1.076 ,  0.7688]
-Original Grad: -0.001, -lr * Pred Grad: -0.050, New P: -1.126
-Original Grad: 0.001, -lr * Pred Grad: -0.026, New P: 0.743
iter 19 loss: 0.178
Actual params: [-1.1264,  0.7432]
-Original Grad: -0.001, -lr * Pred Grad: -0.050, New P: -1.176
-Original Grad: 0.001, -lr * Pred Grad: -0.026, New P: 0.717
iter 20 loss: 0.178
Actual params: [-1.1764,  0.7171]
-Original Grad: -0.000, -lr * Pred Grad: -0.049, New P: -1.226
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 0.690
iter 21 loss: 0.178
Actual params: [-1.2258,  0.6902]
-Original Grad: -0.001, -lr * Pred Grad: -0.049, New P: -1.275
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 0.664
iter 22 loss: 0.178
Actual params: [-1.275 ,  0.6635]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.324
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.635
iter 23 loss: 0.178
Actual params: [-1.3235,  0.6354]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.372
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.607
iter 24 loss: 0.178
Actual params: [-1.3718,  0.6071]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.420
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.578
iter 25 loss: 0.178
Actual params: [-1.4197,  0.5784]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.468
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.550
iter 26 loss: 0.178
Actual params: [-1.4677,  0.5499]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.515
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.521
iter 27 loss: 0.178
Actual params: [-1.5153,  0.5206]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.563
-Original Grad: 0.001, -lr * Pred Grad: -0.029, New P: 0.491
iter 28 loss: 0.178
Actual params: [-1.5628,  0.4913]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.610
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 0.462
iter 29 loss: 0.178
Actual params: [-1.6099,  0.4616]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.657
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.431
iter 30 loss: 0.178
Actual params: [-1.6569,  0.4315]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.704
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 0.401
Target params: [1.3344, 1.5708]
iter 0 loss: 0.188
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.035, -lr * Pred Grad: 0.990, New P: 0.518
-Original Grad: -0.018, -lr * Pred Grad: 0.547, New P: 0.550
iter 1 loss: 0.223
Actual params: [0.5179, 0.5503]
-Original Grad: -0.428, -lr * Pred Grad: -1.858, New P: -1.340
-Original Grad: 0.607, -lr * Pred Grad: 2.462, New P: 3.012
iter 2 loss: 0.195
Actual params: [-1.3397,  3.0119]
-Original Grad: -0.000, -lr * Pred Grad: -1.007, New P: -2.347
-Original Grad: -0.000, -lr * Pred Grad: -0.142, New P: 2.870
iter 3 loss: 0.195
Actual params: [-2.3468,  2.8696]
-Original Grad: -0.000, -lr * Pred Grad: -0.860, New P: -3.207
-Original Grad: -0.000, -lr * Pred Grad: -0.019, New P: 2.851
iter 4 loss: 0.195
Actual params: [-3.2072,  2.851 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.722, New P: -3.929
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 2.821
iter 5 loss: 0.195
Actual params: [-3.9287,  2.8214]
-Original Grad: -0.000, -lr * Pred Grad: -0.654, New P: -4.583
-Original Grad: -0.000, -lr * Pred Grad: -0.040, New P: 2.781
iter 6 loss: 0.195
Actual params: [-4.5832,  2.7813]
-Original Grad: -0.000, -lr * Pred Grad: -0.584, New P: -5.167
-Original Grad: -0.000, -lr * Pred Grad: -0.037, New P: 2.744
iter 7 loss: 0.195
Actual params: [-5.1668,  2.7441]
-Original Grad: -0.000, -lr * Pred Grad: -0.514, New P: -5.681
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 2.716
iter 8 loss: 0.195
Actual params: [-5.6805,  2.7161]
-Original Grad: -0.000, -lr * Pred Grad: -0.446, New P: -6.126
-Original Grad: -0.000, -lr * Pred Grad: -0.023, New P: 2.694
iter 9 loss: 0.195
Actual params: [-6.1262,  2.6936]
-Original Grad: -0.000, -lr * Pred Grad: -0.381, New P: -6.507
-Original Grad: -0.000, -lr * Pred Grad: -0.021, New P: 2.672
iter 10 loss: 0.195
Actual params: [-6.5073,  2.6724]
-Original Grad: -0.000, -lr * Pred Grad: -0.322, New P: -6.829
-Original Grad: -0.000, -lr * Pred Grad: -0.022, New P: 2.650
iter 11 loss: 0.195
Actual params: [-6.8288,  2.6504]
-Original Grad: -0.000, -lr * Pred Grad: -0.268, New P: -7.097
-Original Grad: -0.000, -lr * Pred Grad: -0.024, New P: 2.627
iter 12 loss: 0.195
Actual params: [-7.0972,  2.6267]
-Original Grad: -0.000, -lr * Pred Grad: -0.222, New P: -7.320
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 2.601
iter 13 loss: 0.195
Actual params: [-7.3197,  2.6014]
-Original Grad: -0.000, -lr * Pred Grad: -0.184, New P: -7.504
-Original Grad: -0.000, -lr * Pred Grad: -0.027, New P: 2.575
iter 14 loss: 0.195
Actual params: [-7.5037,  2.5746]
-Original Grad: 0.000, -lr * Pred Grad: -0.153, New P: -7.656
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 2.547
iter 15 loss: 0.195
Actual params: [-7.6564,  2.5466]
-Original Grad: 0.000, -lr * Pred Grad: -0.128, New P: -7.784
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 2.518
iter 16 loss: 0.195
Actual params: [-7.7841,  2.5175]
-Original Grad: 0.000, -lr * Pred Grad: -0.108, New P: -7.892
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 2.488
iter 17 loss: 0.195
Actual params: [-7.8921,  2.4877]
-Original Grad: 0.000, -lr * Pred Grad: -0.093, New P: -7.985
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 2.457
iter 18 loss: 0.195
Actual params: [-7.9848,  2.4571]
-Original Grad: 0.000, -lr * Pred Grad: -0.081, New P: -8.066
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 2.426
iter 19 loss: 0.195
Actual params: [-8.0658,  2.4261]
-Original Grad: 0.000, -lr * Pred Grad: -0.072, New P: -8.138
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 2.395
iter 20 loss: 0.195
Actual params: [-8.1378,  2.3947]
-Original Grad: 0.000, -lr * Pred Grad: -0.065, New P: -8.203
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.363
iter 21 loss: 0.195
Actual params: [-8.2028,  2.363 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.060, New P: -8.263
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.331
iter 22 loss: 0.195
Actual params: [-8.2626,  2.331 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.056, New P: -8.318
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.299
iter 23 loss: 0.195
Actual params: [-8.3184,  2.2989]
-Original Grad: 0.000, -lr * Pred Grad: -0.053, New P: -8.371
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.267
iter 24 loss: 0.195
Actual params: [-8.3711,  2.2667]
-Original Grad: 0.000, -lr * Pred Grad: -0.050, New P: -8.421
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.234
iter 25 loss: 0.195
Actual params: [-8.4215,  2.2344]
-Original Grad: 0.000, -lr * Pred Grad: -0.049, New P: -8.470
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.202
iter 26 loss: 0.195
Actual params: [-8.4701,  2.202 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.047, New P: -8.517
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.170
iter 27 loss: 0.195
Actual params: [-8.5174,  2.1696]
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: -8.564
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.137
iter 28 loss: 0.195
Actual params: [-8.5637,  2.1371]
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: -8.609
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.105
iter 29 loss: 0.195
Actual params: [-8.6092,  2.1046]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -8.654
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 2.072
iter 30 loss: 0.195
Actual params: [-8.6542,  2.0721]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -8.699
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 2.040
Target params: [1.3344, 1.5708]
iter 0 loss: 0.548
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad: 0.630, New P: 0.158
-Original Grad: 0.000, -lr * Pred Grad: 0.635, New P: 0.638
iter 1 loss: 0.536
Actual params: [0.1581, 0.6382]
-Original Grad: -0.270, -lr * Pred Grad: -1.630, New P: -1.472
-Original Grad: 0.628, -lr * Pred Grad: 2.526, New P: 3.165
iter 2 loss: 0.548
Actual params: [-1.472 ,  3.1647]
-Original Grad: 0.000, -lr * Pred Grad: -0.702, New P: -2.174
-Original Grad: -0.000, -lr * Pred Grad: -0.141, New P: 3.024
iter 3 loss: 0.548
Actual params: [-2.1737,  3.0235]
-Original Grad: 0.000, -lr * Pred Grad: -0.527, New P: -2.700
-Original Grad: -0.000, -lr * Pred Grad: -0.016, New P: 3.008
iter 4 loss: 0.548
Actual params: [-2.7004,  3.0079]
-Original Grad: 0.000, -lr * Pred Grad: -0.448, New P: -3.149
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 2.975
iter 5 loss: 0.548
Actual params: [-3.1487,  2.975 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.376, New P: -3.525
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: 2.933
iter 6 loss: 0.548
Actual params: [-3.5249,  2.9333]
-Original Grad: 0.000, -lr * Pred Grad: -0.311, New P: -3.836
-Original Grad: -0.000, -lr * Pred Grad: -0.038, New P: 2.895
iter 7 loss: 0.548
Actual params: [-3.8358,  2.8948]
-Original Grad: 0.000, -lr * Pred Grad: -0.255, New P: -4.091
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 2.867
iter 8 loss: 0.548
Actual params: [-4.0911,  2.8666]
-Original Grad: 0.000, -lr * Pred Grad: -0.209, New P: -4.300
-Original Grad: -0.000, -lr * Pred Grad: -0.022, New P: 2.844
iter 9 loss: 0.548
Actual params: [-4.3003,  2.8444]
-Original Grad: 0.000, -lr * Pred Grad: -0.172, New P: -4.472
-Original Grad: -0.000, -lr * Pred Grad: -0.021, New P: 2.824
iter 10 loss: 0.548
Actual params: [-4.472 ,  2.8236]
-Original Grad: 0.000, -lr * Pred Grad: -0.142, New P: -4.614
-Original Grad: -0.000, -lr * Pred Grad: -0.022, New P: 2.802
iter 11 loss: 0.548
Actual params: [-4.614,  2.802]
-Original Grad: 0.000, -lr * Pred Grad: -0.119, New P: -4.733
-Original Grad: -0.000, -lr * Pred Grad: -0.023, New P: 2.779
iter 12 loss: 0.548
Actual params: [-4.7326,  2.7787]
-Original Grad: 0.000, -lr * Pred Grad: -0.101, New P: -4.833
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 2.754
iter 13 loss: 0.548
Actual params: [-4.8332,  2.7536]
-Original Grad: 0.000, -lr * Pred Grad: -0.087, New P: -4.920
-Original Grad: -0.000, -lr * Pred Grad: -0.027, New P: 2.727
iter 14 loss: 0.548
Actual params: [-4.92 ,  2.727]
-Original Grad: 0.000, -lr * Pred Grad: -0.076, New P: -4.996
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 2.699
iter 15 loss: 0.548
Actual params: [-4.9962,  2.6991]
-Original Grad: 0.000, -lr * Pred Grad: -0.068, New P: -5.064
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 2.670
iter 16 loss: 0.548
Actual params: [-5.0645,  2.6702]
-Original Grad: 0.000, -lr * Pred Grad: -0.062, New P: -5.127
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 2.640
iter 17 loss: 0.548
Actual params: [-5.1266,  2.6404]
-Original Grad: 0.000, -lr * Pred Grad: -0.057, New P: -5.184
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 2.610
iter 18 loss: 0.548
Actual params: [-5.1841,  2.6099]
-Original Grad: 0.000, -lr * Pred Grad: -0.054, New P: -5.238
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 2.579
iter 19 loss: 0.548
Actual params: [-5.2381,  2.5789]
-Original Grad: 0.000, -lr * Pred Grad: -0.051, New P: -5.289
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 2.548
iter 20 loss: 0.548
Actual params: [-5.2894,  2.5476]
-Original Grad: 0.000, -lr * Pred Grad: -0.049, New P: -5.339
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.516
iter 21 loss: 0.548
Actual params: [-5.3387,  2.5159]
-Original Grad: 0.000, -lr * Pred Grad: -0.048, New P: -5.387
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.484
iter 22 loss: 0.548
Actual params: [-5.3865,  2.484 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.047, New P: -5.433
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.452
iter 23 loss: 0.548
Actual params: [-5.4332,  2.4519]
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: -5.479
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.420
iter 24 loss: 0.548
Actual params: [-5.479 ,  2.4197]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -5.524
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.387
iter 25 loss: 0.548
Actual params: [-5.5242,  2.3874]
-Original Grad: 0.000, -lr * Pred Grad: -0.045, New P: -5.569
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.355
iter 26 loss: 0.548
Actual params: [-5.5689,  2.355 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -5.613
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.323
iter 27 loss: 0.548
Actual params: [-5.6133,  2.3226]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -5.657
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.290
iter 28 loss: 0.548
Actual params: [-5.6574,  2.2901]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -5.701
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 2.258
iter 29 loss: 0.548
Actual params: [-5.7013,  2.2576]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -5.745
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 2.225
iter 30 loss: 0.548
Actual params: [-5.7451,  2.2251]
-Original Grad: 0.000, -lr * Pred Grad: -0.044, New P: -5.789
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 2.193
Target params: [1.3344, 1.5708]
iter 0 loss: 0.003
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.000, -lr * Pred Grad: 0.641, New P: 0.169
-Original Grad: -0.001, -lr * Pred Grad: 0.630, New P: 0.634
iter 1 loss: 0.035
Actual params: [0.1686, 0.6336]
-Original Grad: -0.110, -lr * Pred Grad: -0.762, New P: -0.593
-Original Grad: 0.031, -lr * Pred Grad: 0.290, New P: 0.924
iter 2 loss: 0.005
Actual params: [-0.5932,  0.9238]
-Original Grad: -0.014, -lr * Pred Grad: -0.226, New P: -0.819
-Original Grad: -0.013, -lr * Pred Grad: -0.114, New P: 0.809
iter 3 loss: 0.003
Actual params: [-0.8189,  0.8094]
-Original Grad: -0.004, -lr * Pred Grad: -0.171, New P: -0.990
-Original Grad: -0.006, -lr * Pred Grad: -0.033, New P: 0.776
iter 4 loss: 0.003
Actual params: [-0.99  ,  0.7759]
-Original Grad: -0.000, -lr * Pred Grad: -0.132, New P: -1.122
-Original Grad: -0.001, -lr * Pred Grad: -0.059, New P: 0.717
iter 5 loss: 0.003
Actual params: [-1.1217,  0.7172]
-Original Grad: -0.000, -lr * Pred Grad: -0.103, New P: -1.225
-Original Grad: -0.001, -lr * Pred Grad: -0.047, New P: 0.670
iter 6 loss: 0.003
Actual params: [-1.2252,  0.6697]
-Original Grad: -0.000, -lr * Pred Grad: -0.083, New P: -1.309
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: 0.627
iter 7 loss: 0.003
Actual params: [-1.3086,  0.627 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.070, New P: -1.378
-Original Grad: -0.000, -lr * Pred Grad: -0.037, New P: 0.590
iter 8 loss: 0.003
Actual params: [-1.3785,  0.5904]
-Original Grad: -0.000, -lr * Pred Grad: -0.061, New P: -1.439
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: 0.557
iter 9 loss: 0.003
Actual params: [-1.4394,  0.5569]
-Original Grad: -0.000, -lr * Pred Grad: -0.055, New P: -1.494
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.525
iter 10 loss: 0.003
Actual params: [-1.4945,  0.5251]
-Original Grad: -0.000, -lr * Pred Grad: -0.051, New P: -1.546
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.494
iter 11 loss: 0.003
Actual params: [-1.5456,  0.4939]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -1.594
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.463
iter 12 loss: 0.003
Actual params: [-1.594 ,  0.4629]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.641
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.432
iter 13 loss: 0.003
Actual params: [-1.6406,  0.4318]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.686
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.400
iter 14 loss: 0.003
Actual params: [-1.6859,  0.4005]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.730
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.369
iter 15 loss: 0.003
Actual params: [-1.7304,  0.3689]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.774
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.337
iter 16 loss: 0.003
Actual params: [-1.7744,  0.3372]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.818
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.305
iter 17 loss: 0.003
Actual params: [-1.818 ,  0.3052]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.861
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.273
iter 18 loss: 0.003
Actual params: [-1.8613,  0.2731]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.905
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.241
iter 19 loss: 0.003
Actual params: [-1.9046,  0.2409]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.948
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.209
iter 20 loss: 0.003
Actual params: [-1.9477,  0.2086]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.991
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.176
iter 21 loss: 0.003
Actual params: [-1.9909,  0.1762]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.034
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.144
iter 22 loss: 0.003
Actual params: [-2.034 ,  0.1437]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.077
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.111
iter 23 loss: 0.003
Actual params: [-2.0772,  0.1113]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.120
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.079
iter 24 loss: 0.003
Actual params: [-2.1204,  0.0787]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.164
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.046
iter 25 loss: 0.003
Actual params: [-2.1636,  0.0462]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.207
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.014
iter 26 loss: 0.003
Actual params: [-2.2068,  0.0137]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.250
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.019
iter 27 loss: 0.003
Actual params: [-2.2501, -0.0189]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.293
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.051
iter 28 loss: 0.003
Actual params: [-2.2934, -0.0514]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.337
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.084
iter 29 loss: 0.003
Actual params: [-2.3368, -0.084 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.380
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.117
iter 30 loss: 0.003
Actual params: [-2.3801, -0.1165]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.423
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.149
Target params: [1.3344, 1.5708]
iter 0 loss: 0.879
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad: 0.636, New P: 0.163
-Original Grad: -0.000, -lr * Pred Grad: 0.633, New P: 0.636
iter 1 loss: 0.920
Actual params: [0.1632, 0.6362]
-Original Grad: -0.100, -lr * Pred Grad: -0.674, New P: -0.511
-Original Grad: -0.015, -lr * Pred Grad: 0.037, New P: 0.673
iter 2 loss: 0.880
Actual params: [-0.5111,  0.6728]
-Original Grad: -0.002, -lr * Pred Grad: -0.105, New P: -0.616
-Original Grad: -0.003, -lr * Pred Grad: -0.043, New P: 0.630
iter 3 loss: 0.880
Actual params: [-0.6164,  0.6298]
-Original Grad: -0.001, -lr * Pred Grad: -0.106, New P: -0.722
-Original Grad: -0.001, -lr * Pred Grad: -0.038, New P: 0.592
iter 4 loss: 0.880
Actual params: [-0.7221,  0.5923]
-Original Grad: -0.000, -lr * Pred Grad: -0.080, New P: -0.802
-Original Grad: -0.001, -lr * Pred Grad: -0.053, New P: 0.539
iter 5 loss: 0.879
Actual params: [-0.8021,  0.5394]
-Original Grad: -0.000, -lr * Pred Grad: -0.064, New P: -0.866
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: 0.492
iter 6 loss: 0.879
Actual params: [-0.8662,  0.4916]
-Original Grad: -0.000, -lr * Pred Grad: -0.054, New P: -0.920
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: 0.449
iter 7 loss: 0.879
Actual params: [-0.9202,  0.4493]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -0.968
-Original Grad: -0.000, -lr * Pred Grad: -0.037, New P: 0.412
iter 8 loss: 0.879
Actual params: [-0.9683,  0.4121]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.013
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: 0.378
iter 9 loss: 0.879
Actual params: [-1.013 ,  0.3779]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.056
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.345
iter 10 loss: 0.879
Actual params: [-1.0558,  0.3454]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -1.098
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.314
iter 11 loss: 0.879
Actual params: [-1.0977,  0.3137]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -1.139
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.282
iter 12 loss: 0.879
Actual params: [-1.1391,  0.2822]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -1.180
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 0.251
iter 13 loss: 0.879
Actual params: [-1.1803,  0.2508]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -1.222
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.219
iter 14 loss: 0.879
Actual params: [-1.2216,  0.2192]
-Original Grad: -0.000, -lr * Pred Grad: -0.041, New P: -1.263
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.187
iter 15 loss: 0.879
Actual params: [-1.263 ,  0.1874]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -1.305
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.156
iter 16 loss: 0.879
Actual params: [-1.3045,  0.1555]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -1.346
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.123
iter 17 loss: 0.879
Actual params: [-1.3463,  0.1234]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -1.388
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.091
iter 18 loss: 0.879
Actual params: [-1.3883,  0.0912]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -1.430
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.059
iter 19 loss: 0.879
Actual params: [-1.4304,  0.0589]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -1.473
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: 0.027
iter 20 loss: 0.879
Actual params: [-1.4728,  0.0266]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.515
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: -0.006
iter 21 loss: 0.879
Actual params: [-1.5154, -0.0059]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.558
-Original Grad: -0.000, -lr * Pred Grad: -0.032, New P: -0.038
iter 22 loss: 0.879
Actual params: [-1.558 , -0.0383]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.601
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.071
iter 23 loss: 0.879
Actual params: [-1.6009, -0.0708]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.644
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.103
iter 24 loss: 0.879
Actual params: [-1.6438, -0.1034]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.687
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.136
iter 25 loss: 0.879
Actual params: [-1.6868, -0.1359]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.730
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.168
iter 26 loss: 0.879
Actual params: [-1.73  , -0.1684]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.773
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.201
iter 27 loss: 0.879
Actual params: [-1.7731, -0.201 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.816
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.234
iter 28 loss: 0.879
Actual params: [-1.8164, -0.2335]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.860
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.266
iter 29 loss: 0.879
Actual params: [-1.8597, -0.2661]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.903
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.299
iter 30 loss: 0.879
Actual params: [-1.903 , -0.2987]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -1.946
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.331
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.011, -lr * Pred Grad: 0.528, New P: 0.056
-Original Grad: -0.013, -lr * Pred Grad: 0.570, New P: 0.574
iter 1 loss: 0.127
Actual params: [0.0556, 0.5736]
-Original Grad: -0.014, -lr * Pred Grad: 0.186, New P: 0.241
-Original Grad: 0.124, -lr * Pred Grad: 0.838, New P: 1.412
iter 2 loss: 0.043
Actual params: [0.2411, 1.4115]
-Original Grad: 0.083, -lr * Pred Grad: 0.815, New P: 1.057
-Original Grad: 0.161, -lr * Pred Grad: 0.796, New P: 2.208
iter 3 loss: 0.047
Actual params: [1.0565, 2.2077]
-Original Grad: -0.071, -lr * Pred Grad: -0.292, New P: 0.765
-Original Grad: -0.325, -lr * Pred Grad: -0.736, New P: 1.471
iter 4 loss: 0.013
Actual params: [0.7646, 1.4713]
-Original Grad: 0.041, -lr * Pred Grad: 0.212, New P: 0.976
-Original Grad: 0.122, -lr * Pred Grad: 0.034, New P: 1.505
iter 5 loss: 0.008
Actual params: [0.9761, 1.5055]
-Original Grad: 0.005, -lr * Pred Grad: 0.063, New P: 1.039
-Original Grad: 0.128, -lr * Pred Grad: 0.242, New P: 1.748
iter 6 loss: 0.005
Actual params: [1.0393, 1.7477]
-Original Grad: -0.012, -lr * Pred Grad: 0.012, New P: 1.051
-Original Grad: -0.053, -lr * Pred Grad: -0.075, New P: 1.672
iter 7 loss: 0.005
Actual params: [1.0511, 1.6724]
-Original Grad: -0.011, -lr * Pred Grad: -0.024, New P: 1.027
-Original Grad: -0.023, -lr * Pred Grad: -0.071, New P: 1.601
iter 8 loss: 0.005
Actual params: [1.027, 1.601]
-Original Grad: -0.010, -lr * Pred Grad: -0.050, New P: 0.977
-Original Grad: 0.012, -lr * Pred Grad: -0.030, New P: 1.571
iter 9 loss: 0.006
Actual params: [0.9775, 1.5713]
-Original Grad: -0.000, -lr * Pred Grad: -0.038, New P: 0.939
-Original Grad: 0.051, -lr * Pred Grad: 0.104, New P: 1.675
iter 10 loss: 0.004
Actual params: [0.9392, 1.6754]
-Original Grad: 0.004, -lr * Pred Grad: -0.025, New P: 0.914
-Original Grad: 0.026, -lr * Pred Grad: 0.090, New P: 1.765
iter 11 loss: 0.005
Actual params: [0.9141, 1.7651]
-Original Grad: -0.014, -lr * Pred Grad: -0.073, New P: 0.842
-Original Grad: -0.049, -lr * Pred Grad: -0.103, New P: 1.662
iter 12 loss: 0.004
Actual params: [0.8415, 1.6616]
-Original Grad: 0.014, -lr * Pred Grad: -0.015, New P: 0.826
-Original Grad: 0.022, -lr * Pred Grad: -0.001, New P: 1.661
iter 13 loss: 0.005
Actual params: [0.8263, 1.6607]
-Original Grad: 0.016, -lr * Pred Grad: 0.014, New P: 0.840
-Original Grad: 0.048, -lr * Pred Grad: 0.103, New P: 1.763
iter 14 loss: 0.004
Actual params: [0.8398, 1.7632]
-Original Grad: 0.004, -lr * Pred Grad: 0.002, New P: 0.842
-Original Grad: -0.010, -lr * Pred Grad: -0.011, New P: 1.752
iter 15 loss: 0.004
Actual params: [0.8417, 1.752 ]
-Original Grad: 0.005, -lr * Pred Grad: 0.003, New P: 0.845
-Original Grad: -0.002, -lr * Pred Grad: -0.020, New P: 1.732
iter 16 loss: 0.004
Actual params: [0.8451, 1.7321]
-Original Grad: 0.005, -lr * Pred Grad: 0.004, New P: 0.849
-Original Grad: 0.012, -lr * Pred Grad: 0.001, New P: 1.733
iter 17 loss: 0.004
Actual params: [0.8494, 1.733 ]
-Original Grad: -0.004, -lr * Pred Grad: -0.022, New P: 0.828
-Original Grad: -0.016, -lr * Pred Grad: -0.056, New P: 1.677
iter 18 loss: 0.004
Actual params: [0.8278, 1.6769]
-Original Grad: 0.016, -lr * Pred Grad: 0.022, New P: 0.850
-Original Grad: 0.037, -lr * Pred Grad: 0.053, New P: 1.730
iter 19 loss: 0.004
Actual params: [0.8498, 1.7296]
-Original Grad: 0.010, -lr * Pred Grad: 0.023, New P: 0.873
-Original Grad: 0.017, -lr * Pred Grad: 0.041, New P: 1.770
iter 20 loss: 0.005
Actual params: [0.8729, 1.7703]
-Original Grad: -0.002, -lr * Pred Grad: -0.006, New P: 0.867
-Original Grad: -0.042, -lr * Pred Grad: -0.100, New P: 1.670
iter 21 loss: 0.004
Actual params: [0.8666, 1.6702]
-Original Grad: -0.000, -lr * Pred Grad: -0.015, New P: 0.852
-Original Grad: 0.025, -lr * Pred Grad: 0.002, New P: 1.672
iter 22 loss: 0.004
Actual params: [0.8517, 1.672 ]
-Original Grad: 0.009, -lr * Pred Grad: 0.003, New P: 0.855
-Original Grad: 0.023, -lr * Pred Grad: 0.035, New P: 1.707
iter 23 loss: 0.004
Actual params: [0.8548, 1.7075]
-Original Grad: 0.004, -lr * Pred Grad: -0.003, New P: 0.852
-Original Grad: -0.002, -lr * Pred Grad: -0.010, New P: 1.698
iter 24 loss: 0.004
Actual params: [0.8516, 1.6975]
-Original Grad: 0.004, -lr * Pred Grad: -0.004, New P: 0.848
-Original Grad: 0.005, -lr * Pred Grad: -0.008, New P: 1.690
iter 25 loss: 0.004
Actual params: [0.8478, 1.69  ]
-Original Grad: 0.009, -lr * Pred Grad: 0.011, New P: 0.859
-Original Grad: 0.018, -lr * Pred Grad: 0.023, New P: 1.713
iter 26 loss: 0.004
Actual params: [0.8586, 1.7135]
-Original Grad: 0.006, -lr * Pred Grad: 0.009, New P: 0.868
-Original Grad: -0.004, -lr * Pred Grad: -0.020, New P: 1.693
iter 27 loss: 0.004
Actual params: [0.8678, 1.693 ]
-Original Grad: 0.012, -lr * Pred Grad: 0.026, New P: 0.893
-Original Grad: 0.001, -lr * Pred Grad: -0.021, New P: 1.672
iter 28 loss: 0.004
Actual params: [0.8934, 1.6725]
-Original Grad: 0.002, -lr * Pred Grad: 0.006, New P: 0.900
-Original Grad: 0.012, -lr * Pred Grad: 0.001, New P: 1.674
iter 29 loss: 0.004
Actual params: [0.8998, 1.6736]
-Original Grad: 0.003, -lr * Pred Grad: 0.002, New P: 0.901
-Original Grad: 0.033, -lr * Pred Grad: 0.069, New P: 1.742
iter 30 loss: 0.004
Actual params: [0.9013, 1.7421]
-Original Grad: -0.004, -lr * Pred Grad: -0.023, New P: 0.878
-Original Grad: -0.040, -lr * Pred Grad: -0.092, New P: 1.650
Target params: [1.3344, 1.5708]
iter 0 loss: 1.009
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.051, -lr * Pred Grad: 1.128, New P: 0.656
-Original Grad: 0.035, -lr * Pred Grad: 0.801, New P: 0.804
iter 1 loss: 0.589
Actual params: [0.656, 0.804]
-Original Grad: 1.475, -lr * Pred Grad: 2.859, New P: 3.515
-Original Grad: 1.109, -lr * Pred Grad: 2.928, New P: 3.732
iter 2 loss: 1.164
Actual params: [3.5153, 3.7318]
-Original Grad: 0.077, -lr * Pred Grad: -1.022, New P: 2.494
-Original Grad: -0.040, -lr * Pred Grad: -0.244, New P: 3.488
iter 3 loss: 1.237
Actual params: [2.4936, 3.4882]
-Original Grad: 0.038, -lr * Pred Grad: 0.279, New P: 2.773
-Original Grad: -0.010, -lr * Pred Grad: 0.013, New P: 3.501
iter 4 loss: 1.213
Actual params: [2.7728, 3.5009]
-Original Grad: 0.059, -lr * Pred Grad: 0.177, New P: 2.950
-Original Grad: -0.035, -lr * Pred Grad: -0.108, New P: 3.393
iter 5 loss: 1.193
Actual params: [2.9496, 3.3933]
-Original Grad: 0.102, -lr * Pred Grad: 0.372, New P: 3.321
-Original Grad: -0.029, -lr * Pred Grad: -0.113, New P: 3.280
iter 6 loss: 1.154
Actual params: [3.3212, 3.2805]
-Original Grad: 0.096, -lr * Pred Grad: 0.389, New P: 3.711
-Original Grad: -0.088, -lr * Pred Grad: -0.207, New P: 3.074
iter 7 loss: 1.104
Actual params: [3.7107, 3.0737]
-Original Grad: 0.079, -lr * Pred Grad: 0.397, New P: 4.108
-Original Grad: -0.116, -lr * Pred Grad: -0.275, New P: 2.799
iter 8 loss: 1.054
Actual params: [4.108 , 2.7987]
-Original Grad: 0.048, -lr * Pred Grad: 0.329, New P: 4.437
-Original Grad: -0.137, -lr * Pred Grad: -0.338, New P: 2.461
iter 9 loss: 1.020
Actual params: [4.4368, 2.4612]
-Original Grad: 0.009, -lr * Pred Grad: 0.207, New P: 4.644
-Original Grad: -0.125, -lr * Pred Grad: -0.363, New P: 2.098
iter 10 loss: 0.995
Actual params: [4.6439, 2.0985]
-Original Grad: -0.005, -lr * Pred Grad: 0.120, New P: 4.764
-Original Grad: -0.120, -lr * Pred Grad: -0.374, New P: 1.725
iter 11 loss: 0.979
Actual params: [4.764 , 1.7247]
-Original Grad: -0.031, -lr * Pred Grad: 0.000, New P: 4.765
-Original Grad: -0.057, -lr * Pred Grad: -0.315, New P: 1.409
iter 12 loss: 0.968
Actual params: [4.7645, 1.4092]
-Original Grad: -0.027, -lr * Pred Grad: -0.057, New P: 4.707
-Original Grad: -0.062, -lr * Pred Grad: -0.288, New P: 1.121
iter 13 loss: 0.955
Actual params: [4.7071, 1.1213]
-Original Grad: -0.038, -lr * Pred Grad: -0.133, New P: 4.574
-Original Grad: -0.083, -lr * Pred Grad: -0.303, New P: 0.818
iter 14 loss: 0.933
Actual params: [4.5736, 0.8179]
-Original Grad: -0.072, -lr * Pred Grad: -0.276, New P: 4.297
-Original Grad: -0.092, -lr * Pred Grad: -0.322, New P: 0.496
iter 15 loss: 0.914
Actual params: [4.2973, 0.4959]
-Original Grad: -0.080, -lr * Pred Grad: -0.400, New P: 3.897
-Original Grad: 0.151, -lr * Pred Grad: 0.111, New P: 0.607
iter 16 loss: 0.862
Actual params: [3.8968, 0.6071]
-Original Grad: -0.131, -lr * Pred Grad: -0.631, New P: 3.266
-Original Grad: 0.140, -lr * Pred Grad: 0.455, New P: 1.062
iter 17 loss: 0.759
Actual params: [3.2655, 1.0621]
-Original Grad: -0.188, -lr * Pred Grad: -0.949, New P: 2.317
-Original Grad: -0.713, -lr * Pred Grad: -0.455, New P: 0.607
iter 18 loss: 0.575
Actual params: [2.3167, 0.6069]
-Original Grad: -0.209, -lr * Pred Grad: -1.248, New P: 1.069
-Original Grad: 0.486, -lr * Pred Grad: 0.245, New P: 0.852
iter 19 loss: 0.257
Actual params: [1.069 , 0.8522]
-Original Grad: -0.257, -lr * Pred Grad: -1.564, New P: -0.495
-Original Grad: 0.893, -lr * Pred Grad: 2.304, New P: 3.156
iter 20 loss: 1.024
Actual params: [-0.4948,  3.1557]
-Original Grad: -0.046, -lr * Pred Grad: -1.517, New P: -2.012
-Original Grad: -0.093, -lr * Pred Grad: -0.296, New P: 2.860
iter 21 loss: 1.016
Actual params: [-2.0117,  2.8595]
-Original Grad: -0.000, -lr * Pred Grad: -1.457, New P: -3.468
-Original Grad: -0.000, -lr * Pred Grad: 0.027, New P: 2.887
iter 22 loss: 1.016
Actual params: [-3.4684,  2.8869]
-Original Grad: -0.000, -lr * Pred Grad: -1.379, New P: -4.848
-Original Grad: -0.000, -lr * Pred Grad: 0.006, New P: 2.893
iter 23 loss: 1.016
Actual params: [-4.8478,  2.8926]
-Original Grad: -0.000, -lr * Pred Grad: -1.327, New P: -6.175
-Original Grad: -0.000, -lr * Pred Grad: -0.008, New P: 2.885
iter 24 loss: 1.016
Actual params: [-6.1748,  2.8847]
-Original Grad: -0.000, -lr * Pred Grad: -1.287, New P: -7.462
-Original Grad: -0.000, -lr * Pred Grad: -0.016, New P: 2.868
iter 25 loss: 1.016
Actual params: [-7.4617,  2.8683]
-Original Grad: 0.000, -lr * Pred Grad: -1.249, New P: -8.710
-Original Grad: -0.000, -lr * Pred Grad: -0.018, New P: 2.850
iter 26 loss: 1.016
Actual params: [-8.7103,  2.8504]
-Original Grad: -0.000, -lr * Pred Grad: -1.204, New P: -9.914
-Original Grad: -0.000, -lr * Pred Grad: -0.019, New P: 2.831
iter 27 loss: 1.016
Actual params: [-9.9144,  2.8312]
-Original Grad: 0.000, -lr * Pred Grad: -1.151, New P: -11.066
-Original Grad: -0.000, -lr * Pred Grad: -0.021, New P: 2.810
iter 28 loss: 1.016
Actual params: [-11.0656,   2.8101]
-Original Grad: 0.000, -lr * Pred Grad: -1.090, New P: -12.156
-Original Grad: 0.000, -lr * Pred Grad: -0.023, New P: 2.787
iter 29 loss: 1.016
Actual params: [-12.1556,   2.787 ]
-Original Grad: -0.000, -lr * Pred Grad: -1.021, New P: -13.177
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 2.762
iter 30 loss: 1.016
Actual params: [-13.1768,   2.7619]
-Original Grad: -0.000, -lr * Pred Grad: -0.945, New P: -14.122
-Original Grad: -0.000, -lr * Pred Grad: -0.027, New P: 2.735
Target params: [1.3344, 1.5708]
iter 0 loss: 0.466
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.000, -lr * Pred Grad: 0.640, New P: 0.167
-Original Grad: 0.000, -lr * Pred Grad: 0.635, New P: 0.639
iter 1 loss: 0.392
Actual params: [0.1672, 0.6386]
-Original Grad: 0.559, -lr * Pred Grad: 2.400, New P: 2.568
-Original Grad: 0.631, -lr * Pred Grad: 2.530, New P: 3.169
iter 2 loss: 0.405
Actual params: [2.5676, 3.1687]
-Original Grad: -0.014, -lr * Pred Grad: -0.805, New P: 1.762
-Original Grad: -0.417, -lr * Pred Grad: -1.075, New P: 2.094
iter 3 loss: 0.109
Actual params: [1.7622, 2.094 ]
-Original Grad: -0.092, -lr * Pred Grad: -0.013, New P: 1.749
-Original Grad: -0.417, -lr * Pred Grad: -0.247, New P: 1.847
iter 4 loss: 0.077
Actual params: [1.7494, 1.847 ]
-Original Grad: -0.151, -lr * Pred Grad: -0.416, New P: 1.333
-Original Grad: -0.299, -lr * Pred Grad: -0.559, New P: 1.288
iter 5 loss: 0.043
Actual params: [1.3331, 1.2884]
-Original Grad: 0.051, -lr * Pred Grad: -0.060, New P: 1.273
-Original Grad: 0.423, -lr * Pred Grad: 0.302, New P: 1.591
iter 6 loss: 0.027
Actual params: [1.2727, 1.5905]
-Original Grad: -0.048, -lr * Pred Grad: -0.191, New P: 1.082
-Original Grad: -0.054, -lr * Pred Grad: -0.168, New P: 1.422
iter 7 loss: 0.028
Actual params: [1.0819, 1.4224]
-Original Grad: 0.013, -lr * Pred Grad: -0.100, New P: 0.982
-Original Grad: 0.086, -lr * Pred Grad: 0.120, New P: 1.542
iter 8 loss: 0.024
Actual params: [0.9817, 1.5419]
-Original Grad: -0.009, -lr * Pred Grad: -0.110, New P: 0.872
-Original Grad: 0.042, -lr * Pred Grad: 0.120, New P: 1.662
iter 9 loss: 0.024
Actual params: [0.8719, 1.6616]
-Original Grad: 0.014, -lr * Pred Grad: -0.053, New P: 0.819
-Original Grad: -0.033, -lr * Pred Grad: -0.058, New P: 1.604
iter 10 loss: 0.026
Actual params: [0.819 , 1.6037]
-Original Grad: 0.033, -lr * Pred Grad: 0.031, New P: 0.850
-Original Grad: -0.003, -lr * Pred Grad: -0.040, New P: 1.564
iter 11 loss: 0.026
Actual params: [0.8504, 1.5638]
-Original Grad: 0.071, -lr * Pred Grad: 0.185, New P: 1.036
-Original Grad: 0.020, -lr * Pred Grad: 0.009, New P: 1.573
iter 12 loss: 0.022
Actual params: [1.0359, 1.5726]
-Original Grad: 0.000, -lr * Pred Grad: 0.080, New P: 1.116
-Original Grad: -0.030, -lr * Pred Grad: -0.084, New P: 1.488
iter 13 loss: 0.023
Actual params: [1.1156, 1.4882]
-Original Grad: 0.027, -lr * Pred Grad: 0.133, New P: 1.249
-Original Grad: 0.056, -lr * Pred Grad: 0.087, New P: 1.575
iter 14 loss: 0.026
Actual params: [1.2491, 1.5752]
-Original Grad: -0.059, -lr * Pred Grad: -0.110, New P: 1.139
-Original Grad: -0.054, -lr * Pred Grad: -0.123, New P: 1.452
iter 15 loss: 0.024
Actual params: [1.1392, 1.4519]
-Original Grad: 0.008, -lr * Pred Grad: -0.031, New P: 1.108
-Original Grad: 0.065, -lr * Pred Grad: 0.099, New P: 1.551
iter 16 loss: 0.022
Actual params: [1.1078, 1.5514]
-Original Grad: -0.027, -lr * Pred Grad: -0.114, New P: 0.994
-Original Grad: -0.051, -lr * Pred Grad: -0.116, New P: 1.435
iter 17 loss: 0.030
Actual params: [0.9943, 1.4351]
-Original Grad: -0.002, -lr * Pred Grad: -0.086, New P: 0.908
-Original Grad: 0.079, -lr * Pred Grad: 0.144, New P: 1.579
iter 18 loss: 0.024
Actual params: [0.9084, 1.5787]
-Original Grad: -0.011, -lr * Pred Grad: -0.105, New P: 0.804
-Original Grad: 0.002, -lr * Pred Grad: 0.025, New P: 1.604
iter 19 loss: 0.026
Actual params: [0.8036, 1.6035]
-Original Grad: 0.035, -lr * Pred Grad: 0.012, New P: 0.815
-Original Grad: -0.027, -lr * Pred Grad: -0.066, New P: 1.538
iter 20 loss: 0.027
Actual params: [0.8152, 1.5376]
-Original Grad: 0.003, -lr * Pred Grad: -0.014, New P: 0.801
-Original Grad: 0.022, -lr * Pred Grad: 0.008, New P: 1.545
iter 21 loss: 0.027
Actual params: [0.8008, 1.5451]
-Original Grad: -0.005, -lr * Pred Grad: -0.037, New P: 0.764
-Original Grad: -0.006, -lr * Pred Grad: -0.033, New P: 1.512
iter 22 loss: 0.029
Actual params: [0.7638, 1.5119]
-Original Grad: 0.121, -lr * Pred Grad: 0.292, New P: 1.056
-Original Grad: 0.033, -lr * Pred Grad: 0.056, New P: 1.568
iter 23 loss: 0.022
Actual params: [1.0556, 1.5675]
-Original Grad: -0.008, -lr * Pred Grad: 0.082, New P: 1.137
-Original Grad: 0.016, -lr * Pred Grad: 0.040, New P: 1.607
iter 24 loss: 0.022
Actual params: [1.1374, 1.6071]
-Original Grad: -0.018, -lr * Pred Grad: 0.020, New P: 1.157
-Original Grad: -0.014, -lr * Pred Grad: -0.037, New P: 1.570
iter 25 loss: 0.022
Actual params: [1.1571, 1.57  ]
-Original Grad: -0.031, -lr * Pred Grad: -0.076, New P: 1.081
-Original Grad: -0.025, -lr * Pred Grad: -0.086, New P: 1.484
iter 26 loss: 0.024
Actual params: [1.0814, 1.4842]
-Original Grad: 0.008, -lr * Pred Grad: -0.024, New P: 1.058
-Original Grad: 0.051, -lr * Pred Grad: 0.070, New P: 1.554
iter 27 loss: 0.022
Actual params: [1.0579, 1.5543]
-Original Grad: -0.024, -lr * Pred Grad: -0.099, New P: 0.959
-Original Grad: -0.040, -lr * Pred Grad: -0.095, New P: 1.460
iter 28 loss: 0.029
Actual params: [0.9591, 1.4595]
-Original Grad: 0.064, -lr * Pred Grad: 0.104, New P: 1.063
-Original Grad: 0.111, -lr * Pred Grad: 0.250, New P: 1.709
iter 29 loss: 0.023
Actual params: [1.063 , 1.7095]
-Original Grad: -0.012, -lr * Pred Grad: -0.010, New P: 1.053
-Original Grad: -0.085, -lr * Pred Grad: -0.172, New P: 1.538
iter 30 loss: 0.022
Actual params: [1.0533, 1.5376]
-Original Grad: 0.022, -lr * Pred Grad: 0.055, New P: 1.108
-Original Grad: 0.013, -lr * Pred Grad: -0.029, New P: 1.508
Target params: [1.3344, 1.5708]
iter 0 loss: 0.644
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad: 0.611, New P: 0.138
-Original Grad: -0.003, -lr * Pred Grad: 0.621, New P: 0.625
iter 1 loss: 0.678
Actual params: [0.1384, 0.6246]
-Original Grad: -0.113, -lr * Pred Grad: -0.794, New P: -0.656
-Original Grad: 0.203, -lr * Pred Grad: 1.285, New P: 1.910
iter 2 loss: 0.609
Actual params: [-0.6561,  1.9096]
-Original Grad: 0.071, -lr * Pred Grad: 0.297, New P: -0.359
-Original Grad: -0.031, -lr * Pred Grad: -0.309, New P: 1.601
iter 3 loss: 0.574
Actual params: [-0.3593,  1.6008]
-Original Grad: 0.054, -lr * Pred Grad: 0.237, New P: -0.122
-Original Grad: 0.027, -lr * Pred Grad: 0.020, New P: 1.621
iter 4 loss: 0.511
Actual params: [-0.1222,  1.6211]
-Original Grad: 0.234, -lr * Pred Grad: 0.986, New P: 0.864
-Original Grad: 0.027, -lr * Pred Grad: 0.046, New P: 1.667
iter 5 loss: 0.089
Actual params: [0.864 , 1.6671]
-Original Grad: 0.078, -lr * Pred Grad: 0.521, New P: 1.385
-Original Grad: 0.064, -lr * Pred Grad: 0.167, New P: 1.834
iter 6 loss: 0.041
Actual params: [1.3853, 1.834 ]
-Original Grad: -0.020, -lr * Pred Grad: 0.274, New P: 1.659
-Original Grad: -0.157, -lr * Pred Grad: -0.258, New P: 1.576
iter 7 loss: 0.022
Actual params: [1.6589, 1.5764]
-Original Grad: 0.021, -lr * Pred Grad: 0.262, New P: 1.921
-Original Grad: 0.026, -lr * Pred Grad: -0.089, New P: 1.488
iter 8 loss: 0.022
Actual params: [1.9209, 1.4876]
-Original Grad: 0.005, -lr * Pred Grad: 0.149, New P: 2.069
-Original Grad: -0.037, -lr * Pred Grad: -0.139, New P: 1.348
iter 9 loss: 0.035
Actual params: [2.0694, 1.3484]
-Original Grad: -0.048, -lr * Pred Grad: -0.044, New P: 2.026
-Original Grad: 0.046, -lr * Pred Grad: 0.029, New P: 1.377
iter 10 loss: 0.028
Actual params: [2.0258, 1.3775]
-Original Grad: -0.054, -lr * Pred Grad: -0.153, New P: 1.872
-Original Grad: 0.007, -lr * Pred Grad: 0.004, New P: 1.382
iter 11 loss: 0.018
Actual params: [1.8725, 1.3816]
-Original Grad: -0.030, -lr * Pred Grad: -0.169, New P: 1.704
-Original Grad: -0.005, -lr * Pred Grad: -0.025, New P: 1.357
iter 12 loss: 0.026
Actual params: [1.7036, 1.3565]
-Original Grad: 0.044, -lr * Pred Grad: -0.005, New P: 1.698
-Original Grad: 0.075, -lr * Pred Grad: 0.175, New P: 1.531
iter 13 loss: 0.021
Actual params: [1.6983, 1.5313]
-Original Grad: 0.011, -lr * Pred Grad: -0.009, New P: 1.689
-Original Grad: -0.026, -lr * Pred Grad: -0.041, New P: 1.491
iter 14 loss: 0.022
Actual params: [1.689 , 1.4907]
-Original Grad: 0.014, -lr * Pred Grad: 0.015, New P: 1.704
-Original Grad: -0.013, -lr * Pred Grad: -0.049, New P: 1.442
iter 15 loss: 0.022
Actual params: [1.7041, 1.442 ]
-Original Grad: 0.027, -lr * Pred Grad: 0.068, New P: 1.772
-Original Grad: -0.008, -lr * Pred Grad: -0.061, New P: 1.381
iter 16 loss: 0.020
Actual params: [1.7718, 1.3815]
-Original Grad: 0.020, -lr * Pred Grad: 0.077, New P: 1.849
-Original Grad: 0.059, -lr * Pred Grad: 0.110, New P: 1.492
iter 17 loss: 0.019
Actual params: [1.8491, 1.4915]
-Original Grad: -0.001, -lr * Pred Grad: 0.031, New P: 1.880
-Original Grad: -0.025, -lr * Pred Grad: -0.050, New P: 1.441
iter 18 loss: 0.018
Actual params: [1.8805, 1.4414]
-Original Grad: -0.027, -lr * Pred Grad: -0.065, New P: 1.815
-Original Grad: -0.084, -lr * Pred Grad: -0.190, New P: 1.252
iter 19 loss: 0.021
Actual params: [1.8152, 1.2519]
-Original Grad: 0.035, -lr * Pred Grad: 0.056, New P: 1.871
-Original Grad: 0.065, -lr * Pred Grad: 0.028, New P: 1.280
iter 20 loss: 0.019
Actual params: [1.8713, 1.28  ]
-Original Grad: -0.031, -lr * Pred Grad: -0.081, New P: 1.790
-Original Grad: -0.034, -lr * Pred Grad: -0.094, New P: 1.186
iter 21 loss: 0.026
Actual params: [1.7904, 1.186 ]
-Original Grad: 0.020, -lr * Pred Grad: 0.004, New P: 1.795
-Original Grad: 0.072, -lr * Pred Grad: 0.129, New P: 1.315
iter 22 loss: 0.020
Actual params: [1.7946, 1.3153]
-Original Grad: 0.049, -lr * Pred Grad: 0.114, New P: 1.909
-Original Grad: 0.027, -lr * Pred Grad: 0.094, New P: 1.410
iter 23 loss: 0.019
Actual params: [1.9089, 1.4096]
-Original Grad: -0.055, -lr * Pred Grad: -0.112, New P: 1.796
-Original Grad: -0.091, -lr * Pred Grad: -0.185, New P: 1.225
iter 24 loss: 0.023
Actual params: [1.7964, 1.2246]
-Original Grad: 0.035, -lr * Pred Grad: 0.039, New P: 1.835
-Original Grad: 0.056, -lr * Pred Grad: 0.033, New P: 1.257
iter 25 loss: 0.020
Actual params: [1.8351, 1.2573]
-Original Grad: 0.066, -lr * Pred Grad: 0.173, New P: 2.008
-Original Grad: 0.000, -lr * Pred Grad: -0.016, New P: 1.241
iter 26 loss: 0.029
Actual params: [2.0077, 1.2411]
-Original Grad: -0.054, -lr * Pred Grad: -0.080, New P: 1.928
-Original Grad: 0.043, -lr * Pred Grad: 0.095, New P: 1.336
iter 27 loss: 0.020
Actual params: [1.9281, 1.3361]
-Original Grad: -0.027, -lr * Pred Grad: -0.113, New P: 1.815
-Original Grad: -0.054, -lr * Pred Grad: -0.120, New P: 1.216
iter 28 loss: 0.022
Actual params: [1.8154, 1.2159]
-Original Grad: 0.034, -lr * Pred Grad: 0.017, New P: 1.833
-Original Grad: 0.031, -lr * Pred Grad: 0.015, New P: 1.230
iter 29 loss: 0.021
Actual params: [1.8328, 1.2304]
-Original Grad: 0.022, -lr * Pred Grad: 0.043, New P: 1.876
-Original Grad: 0.038, -lr * Pred Grad: 0.080, New P: 1.311
iter 30 loss: 0.018
Actual params: [1.8757, 1.3108]
-Original Grad: -0.012, -lr * Pred Grad: -0.024, New P: 1.852
-Original Grad: 0.053, -lr * Pred Grad: 0.166, New P: 1.477
Target params: [1.3344, 1.5708]
iter 0 loss: 0.511
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.022, -lr * Pred Grad: 0.866, New P: 0.393
-Original Grad: 0.053, -lr * Pred Grad: 0.881, New P: 0.885
iter 1 loss: 0.233
Actual params: [0.3934, 0.8845]
-Original Grad: 0.384, -lr * Pred Grad: 2.296, New P: 2.690
-Original Grad: 0.492, -lr * Pred Grad: 2.374, New P: 3.259
iter 2 loss: 0.773
Actual params: [2.6896, 3.2589]
-Original Grad: 0.097, -lr * Pred Grad: 0.184, New P: 2.874
-Original Grad: -0.187, -lr * Pred Grad: -0.749, New P: 2.510
iter 3 loss: 0.441
Actual params: [2.8736, 2.5097]
-Original Grad: -0.136, -lr * Pred Grad: -0.249, New P: 2.624
-Original Grad: -0.927, -lr * Pred Grad: -0.434, New P: 2.075
iter 4 loss: 0.197
Actual params: [2.6244, 2.0753]
-Original Grad: -0.176, -lr * Pred Grad: -0.573, New P: 2.051
-Original Grad: -0.157, -lr * Pred Grad: -0.555, New P: 1.520
iter 5 loss: 0.105
Actual params: [2.051 , 1.5204]
-Original Grad: -0.180, -lr * Pred Grad: -0.758, New P: 1.293
-Original Grad: 0.187, -lr * Pred Grad: -0.188, New P: 1.332
iter 6 loss: 0.049
Actual params: [1.2934, 1.3321]
-Original Grad: -0.122, -lr * Pred Grad: -0.813, New P: 0.481
-Original Grad: 0.306, -lr * Pred Grad: 0.643, New P: 1.975
iter 7 loss: 0.160
Actual params: [0.4805, 1.9749]
-Original Grad: 0.210, -lr * Pred Grad: -0.100, New P: 0.380
-Original Grad: -0.163, -lr * Pred Grad: -0.363, New P: 1.612
iter 8 loss: 0.175
Actual params: [0.3803, 1.6118]
-Original Grad: 0.525, -lr * Pred Grad: 0.610, New P: 0.990
-Original Grad: 0.151, -lr * Pred Grad: 0.212, New P: 1.824
iter 9 loss: 0.080
Actual params: [0.9905, 1.8242]
-Original Grad: -0.012, -lr * Pred Grad: 0.067, New P: 1.057
-Original Grad: -0.283, -lr * Pred Grad: -0.362, New P: 1.462
iter 10 loss: 0.044
Actual params: [1.057 , 1.4619]
-Original Grad: 0.037, -lr * Pred Grad: 0.271, New P: 1.328
-Original Grad: 0.059, -lr * Pred Grad: -0.148, New P: 1.314
iter 11 loss: 0.053
Actual params: [1.3281, 1.3136]
-Original Grad: -0.061, -lr * Pred Grad: -0.025, New P: 1.303
-Original Grad: 0.226, -lr * Pred Grad: 0.513, New P: 1.827
iter 12 loss: 0.046
Actual params: [1.3033, 1.8268]
-Original Grad: 0.095, -lr * Pred Grad: 0.259, New P: 1.562
-Original Grad: -0.199, -lr * Pred Grad: -0.367, New P: 1.460
iter 13 loss: 0.049
Actual params: [1.5624, 1.4597]
-Original Grad: -0.050, -lr * Pred Grad: -0.028, New P: 1.535
-Original Grad: 0.206, -lr * Pred Grad: 0.323, New P: 1.783
iter 14 loss: 0.022
Actual params: [1.5346, 1.7828]
-Original Grad: -0.015, -lr * Pred Grad: -0.028, New P: 1.507
-Original Grad: 0.075, -lr * Pred Grad: 0.343, New P: 2.125
iter 15 loss: 0.060
Actual params: [1.5068, 2.1253]
-Original Grad: 0.163, -lr * Pred Grad: 0.365, New P: 1.871
-Original Grad: -0.418, -lr * Pred Grad: -0.425, New P: 1.700
iter 16 loss: 0.062
Actual params: [1.8714, 1.7003]
-Original Grad: -0.126, -lr * Pred Grad: -0.190, New P: 1.682
-Original Grad: 0.111, -lr * Pred Grad: -0.147, New P: 1.553
iter 17 loss: 0.049
Actual params: [1.6818, 1.553 ]
-Original Grad: -0.068, -lr * Pred Grad: -0.259, New P: 1.423
-Original Grad: 0.171, -lr * Pred Grad: 0.310, New P: 1.863
iter 18 loss: 0.033
Actual params: [1.4231, 1.8635]
-Original Grad: 0.060, -lr * Pred Grad: -0.010, New P: 1.413
-Original Grad: -0.100, -lr * Pred Grad: -0.192, New P: 1.671
iter 19 loss: 0.025
Actual params: [1.4126, 1.6711]
-Original Grad: 0.004, -lr * Pred Grad: -0.039, New P: 1.374
-Original Grad: 0.065, -lr * Pred Grad: 0.078, New P: 1.749
iter 20 loss: 0.028
Actual params: [1.3737, 1.7486]
-Original Grad: 0.031, -lr * Pred Grad: 0.043, New P: 1.416
-Original Grad: 0.069, -lr * Pred Grad: 0.194, New P: 1.943
iter 21 loss: 0.045
Actual params: [1.4165, 1.943 ]
-Original Grad: 0.145, -lr * Pred Grad: 0.367, New P: 1.783
-Original Grad: -0.254, -lr * Pred Grad: -0.360, New P: 1.583
iter 22 loss: 0.057
Actual params: [1.783 , 1.5829]
-Original Grad: -0.113, -lr * Pred Grad: -0.169, New P: 1.614
-Original Grad: 0.269, -lr * Pred Grad: 0.346, New P: 1.929
iter 23 loss: 0.023
Actual params: [1.614 , 1.9288]
-Original Grad: -0.005, -lr * Pred Grad: -0.073, New P: 1.540
-Original Grad: -0.006, -lr * Pred Grad: 0.044, New P: 1.973
iter 24 loss: 0.028
Actual params: [1.5405, 1.9727]
-Original Grad: 0.043, -lr * Pred Grad: 0.056, New P: 1.597
-Original Grad: -0.086, -lr * Pred Grad: -0.163, New P: 1.810
iter 25 loss: 0.023
Actual params: [1.5968, 1.8101]
-Original Grad: -0.028, -lr * Pred Grad: -0.068, New P: 1.528
-Original Grad: 0.100, -lr * Pred Grad: 0.142, New P: 1.952
iter 26 loss: 0.028
Actual params: [1.5284, 1.9521]
-Original Grad: 0.032, -lr * Pred Grad: 0.044, New P: 1.572
-Original Grad: -0.076, -lr * Pred Grad: -0.155, New P: 1.797
iter 27 loss: 0.023
Actual params: [1.5722, 1.797 ]
-Original Grad: -0.051, -lr * Pred Grad: -0.138, New P: 1.434
-Original Grad: 0.081, -lr * Pred Grad: 0.126, New P: 1.923
iter 28 loss: 0.038
Actual params: [1.4344, 1.923 ]
-Original Grad: 0.073, -lr * Pred Grad: 0.113, New P: 1.548
-Original Grad: -0.115, -lr * Pred Grad: -0.227, New P: 1.696
iter 29 loss: 0.026
Actual params: [1.5477, 1.6961]
-Original Grad: -0.043, -lr * Pred Grad: -0.094, New P: 1.454
-Original Grad: 0.147, -lr * Pred Grad: 0.239, New P: 1.935
iter 30 loss: 0.036
Actual params: [1.4537, 1.9354]
-Original Grad: 0.101, -lr * Pred Grad: 0.220, New P: 1.674
-Original Grad: -0.145, -lr * Pred Grad: -0.267, New P: 1.668
Target params: [1.3344, 1.5708]
iter 0 loss: 0.563
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.004, -lr * Pred Grad: 0.686, New P: 0.214
-Original Grad: 0.017, -lr * Pred Grad: 0.716, New P: 0.720
iter 1 loss: 0.500
Actual params: [0.2139, 0.7198]
-Original Grad: 0.218, -lr * Pred Grad: 1.896, New P: 2.110
-Original Grad: 0.195, -lr * Pred Grad: 1.253, New P: 1.973
iter 2 loss: 0.242
Actual params: [2.11  , 1.9731]
-Original Grad: -0.171, -lr * Pred Grad: -1.045, New P: 1.065
-Original Grad: 0.155, -lr * Pred Grad: 0.679, New P: 2.652
iter 3 loss: 0.307
Actual params: [1.0651, 2.6521]
-Original Grad: 0.362, -lr * Pred Grad: 0.887, New P: 1.952
-Original Grad: -1.162, -lr * Pred Grad: -0.615, New P: 2.037
iter 4 loss: 0.220
Actual params: [1.9524, 2.0374]
-Original Grad: -0.187, -lr * Pred Grad: -0.516, New P: 1.436
-Original Grad: 0.258, -lr * Pred Grad: -0.203, New P: 1.835
iter 5 loss: 0.160
Actual params: [1.4362, 1.8346]
-Original Grad: -0.312, -lr * Pred Grad: -0.912, New P: 0.524
-Original Grad: 0.235, -lr * Pred Grad: 0.177, New P: 2.011
iter 6 loss: 0.316
Actual params: [0.5244, 2.0113]
-Original Grad: 0.996, -lr * Pred Grad: 0.752, New P: 1.276
-Original Grad: -0.167, -lr * Pred Grad: -0.277, New P: 1.734
iter 7 loss: 0.124
Actual params: [1.276 , 1.7342]
-Original Grad: -0.305, -lr * Pred Grad: -0.470, New P: 0.806
-Original Grad: -0.577, -lr * Pred Grad: -0.441, New P: 1.293
iter 8 loss: 0.248
Actual params: [0.806 , 1.2927]
-Original Grad: 0.651, -lr * Pred Grad: 0.531, New P: 1.337
-Original Grad: 0.518, -lr * Pred Grad: 0.282, New P: 1.575
iter 9 loss: 0.110
Actual params: [1.3373, 1.5751]
-Original Grad: -0.242, -lr * Pred Grad: -0.310, New P: 1.027
-Original Grad: -0.276, -lr * Pred Grad: -0.384, New P: 1.192
iter 10 loss: 0.188
Actual params: [1.0268, 1.1915]
-Original Grad: 0.414, -lr * Pred Grad: 0.433, New P: 1.460
-Original Grad: 0.562, -lr * Pred Grad: 0.946, New P: 2.138
iter 11 loss: 0.080
Actual params: [1.4598, 2.1379]
-Original Grad: -0.471, -lr * Pred Grad: -0.549, New P: 0.911
-Original Grad: 0.903, -lr * Pred Grad: 2.763, New P: 4.901
iter 12 loss: 0.975
Actual params: [0.9112, 4.901 ]
-Original Grad: -0.393, -lr * Pred Grad: -1.006, New P: -0.095
-Original Grad: 0.649, -lr * Pred Grad: 2.870, New P: 7.771
iter 13 loss: 0.536
Actual params: [-0.0952,  7.7706]
-Original Grad: 0.217, -lr * Pred Grad: -0.410, New P: -0.505
-Original Grad: -0.145, -lr * Pred Grad: -0.294, New P: 7.476
iter 14 loss: 0.559
Actual params: [-0.505 ,  7.4764]
-Original Grad: 0.046, -lr * Pred Grad: -0.348, New P: -0.853
-Original Grad: -0.019, -lr * Pred Grad: -0.141, New P: 7.335
iter 15 loss: 0.563
Actual params: [-0.8533,  7.3353]
-Original Grad: 0.004, -lr * Pred Grad: -0.327, New P: -1.181
-Original Grad: -0.002, -lr * Pred Grad: -0.118, New P: 7.217
iter 16 loss: 0.563
Actual params: [-1.1807,  7.2172]
-Original Grad: 0.000, -lr * Pred Grad: -0.303, New P: -1.483
-Original Grad: -0.000, -lr * Pred Grad: -0.092, New P: 7.125
iter 17 loss: 0.563
Actual params: [-1.4832,  7.1251]
-Original Grad: 0.000, -lr * Pred Grad: -0.269, New P: -1.753
-Original Grad: -0.000, -lr * Pred Grad: -0.055, New P: 7.070
iter 18 loss: 0.563
Actual params: [-1.7525,  7.0702]
-Original Grad: 0.000, -lr * Pred Grad: -0.235, New P: -1.987
-Original Grad: -0.000, -lr * Pred Grad: -0.027, New P: 7.043
iter 19 loss: 0.563
Actual params: [-1.9874,  7.0429]
-Original Grad: 0.000, -lr * Pred Grad: -0.203, New P: -2.190
-Original Grad: -0.000, -lr * Pred Grad: -0.016, New P: 7.027
iter 20 loss: 0.563
Actual params: [-2.1901,  7.0271]
-Original Grad: 0.000, -lr * Pred Grad: -0.174, New P: -2.364
-Original Grad: -0.000, -lr * Pred Grad: -0.014, New P: 7.013
iter 21 loss: 0.563
Actual params: [-2.364 ,  7.0128]
-Original Grad: 0.000, -lr * Pred Grad: -0.149, New P: -2.513
-Original Grad: -0.000, -lr * Pred Grad: -0.017, New P: 6.996
iter 22 loss: 0.563
Actual params: [-2.5129,  6.9962]
-Original Grad: 0.000, -lr * Pred Grad: -0.128, New P: -2.641
-Original Grad: -0.000, -lr * Pred Grad: -0.020, New P: 6.977
iter 23 loss: 0.563
Actual params: [-2.6408,  6.9765]
-Original Grad: 0.000, -lr * Pred Grad: -0.110, New P: -2.751
-Original Grad: -0.000, -lr * Pred Grad: -0.022, New P: 6.954
iter 24 loss: 0.563
Actual params: [-2.7513,  6.9541]
-Original Grad: 0.000, -lr * Pred Grad: -0.096, New P: -2.848
-Original Grad: -0.000, -lr * Pred Grad: -0.025, New P: 6.929
iter 25 loss: 0.563
Actual params: [-2.8476,  6.9293]
-Original Grad: 0.000, -lr * Pred Grad: -0.085, New P: -2.932
-Original Grad: -0.000, -lr * Pred Grad: -0.027, New P: 6.903
iter 26 loss: 0.563
Actual params: [-2.9325,  6.9027]
-Original Grad: 0.000, -lr * Pred Grad: -0.076, New P: -3.008
-Original Grad: -0.000, -lr * Pred Grad: -0.028, New P: 6.875
iter 27 loss: 0.563
Actual params: [-3.0083,  6.8746]
-Original Grad: 0.000, -lr * Pred Grad: -0.069, New P: -3.077
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: 6.845
iter 28 loss: 0.563
Actual params: [-3.077 ,  6.8455]
-Original Grad: 0.000, -lr * Pred Grad: -0.063, New P: -3.140
-Original Grad: -0.000, -lr * Pred Grad: -0.030, New P: 6.815
iter 29 loss: 0.563
Actual params: [-3.14  ,  6.8155]
-Original Grad: 0.000, -lr * Pred Grad: -0.059, New P: -3.199
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 6.785
iter 30 loss: 0.563
Actual params: [-3.1985,  6.7848]
-Original Grad: 0.000, -lr * Pred Grad: -0.055, New P: -3.254
-Original Grad: -0.000, -lr * Pred Grad: -0.031, New P: 6.754
Target params: [1.3344, 1.5708]
iter 0 loss: 0.024
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.040, -lr * Pred Grad: 0.211, New P: -0.262
-Original Grad: 0.044, -lr * Pred Grad: 0.842, New P: 0.846
iter 1 loss: 0.011
Actual params: [-0.2618,  0.8458]
-Original Grad: 0.004, -lr * Pred Grad: 0.339, New P: 0.077
-Original Grad: 0.025, -lr * Pred Grad: 0.253, New P: 1.098
iter 2 loss: 0.007
Actual params: [0.0773, 1.0983]
-Original Grad: 0.012, -lr * Pred Grad: 0.248, New P: 0.326
-Original Grad: 0.022, -lr * Pred Grad: 0.011, New P: 1.109
iter 3 loss: 0.008
Actual params: [0.3256, 1.1094]
-Original Grad: -0.008, -lr * Pred Grad: 0.075, New P: 0.401
-Original Grad: 0.031, -lr * Pred Grad: 0.081, New P: 1.191
iter 4 loss: 0.008
Actual params: [0.4008, 1.1906]
-Original Grad: -0.035, -lr * Pred Grad: -0.117, New P: 0.284
-Original Grad: 0.027, -lr * Pred Grad: 0.062, New P: 1.252
iter 5 loss: 0.005
Actual params: [0.2843, 1.2522]
-Original Grad: -0.013, -lr * Pred Grad: -0.094, New P: 0.190
-Original Grad: 0.006, -lr * Pred Grad: 0.012, New P: 1.264
iter 6 loss: 0.005
Actual params: [0.19  , 1.2644]
-Original Grad: 0.002, -lr * Pred Grad: -0.058, New P: 0.132
-Original Grad: 0.019, -lr * Pred Grad: 0.032, New P: 1.297
iter 7 loss: 0.005
Actual params: [0.1318, 1.2967]
-Original Grad: 0.010, -lr * Pred Grad: -0.022, New P: 0.110
-Original Grad: 0.008, -lr * Pred Grad: 0.013, New P: 1.309
iter 8 loss: 0.005
Actual params: [0.1102, 1.3093]
-Original Grad: 0.017, -lr * Pred Grad: 0.022, New P: 0.132
-Original Grad: 0.007, -lr * Pred Grad: 0.008, New P: 1.317
iter 9 loss: 0.005
Actual params: [0.1321, 1.3172]
-Original Grad: 0.007, -lr * Pred Grad: 0.017, New P: 0.149
-Original Grad: -0.003, -lr * Pred Grad: -0.020, New P: 1.298
iter 10 loss: 0.005
Actual params: [0.1492, 1.2976]
-Original Grad: 0.004, -lr * Pred Grad: 0.010, New P: 0.159
-Original Grad: 0.006, -lr * Pred Grad: -0.008, New P: 1.290
iter 11 loss: 0.005
Actual params: [0.1589, 1.2896]
-Original Grad: -0.002, -lr * Pred Grad: -0.012, New P: 0.147
-Original Grad: -0.001, -lr * Pred Grad: -0.024, New P: 1.266
iter 12 loss: 0.005
Actual params: [0.1472, 1.2656]
-Original Grad: 0.014, -lr * Pred Grad: 0.024, New P: 0.171
-Original Grad: 0.009, -lr * Pred Grad: -0.005, New P: 1.261
iter 13 loss: 0.005
Actual params: [0.1711, 1.2609]
-Original Grad: 0.000, -lr * Pred Grad: -0.001, New P: 0.170
-Original Grad: 0.008, -lr * Pred Grad: -0.002, New P: 1.259
iter 14 loss: 0.005
Actual params: [0.1703, 1.2593]
-Original Grad: 0.001, -lr * Pred Grad: -0.008, New P: 0.162
-Original Grad: 0.011, -lr * Pred Grad: 0.008, New P: 1.268
iter 15 loss: 0.005
Actual params: [0.1619, 1.2676]
-Original Grad: 0.001, -lr * Pred Grad: -0.015, New P: 0.147
-Original Grad: 0.003, -lr * Pred Grad: -0.008, New P: 1.259
iter 16 loss: 0.005
Actual params: [0.1472, 1.2592]
-Original Grad: 0.007, -lr * Pred Grad: -0.003, New P: 0.145
-Original Grad: 0.023, -lr * Pred Grad: 0.039, New P: 1.298
iter 17 loss: 0.005
Actual params: [0.1446, 1.2981]
-Original Grad: -0.000, -lr * Pred Grad: -0.018, New P: 0.127
-Original Grad: 0.015, -lr * Pred Grad: 0.034, New P: 1.332
iter 18 loss: 0.005
Actual params: [0.127, 1.332]
-Original Grad: 0.010, -lr * Pred Grad: 0.005, New P: 0.132
-Original Grad: -0.005, -lr * Pred Grad: -0.017, New P: 1.315
iter 19 loss: 0.005
Actual params: [0.1316, 1.3149]
-Original Grad: 0.007, -lr * Pred Grad: 0.006, New P: 0.137
-Original Grad: -0.006, -lr * Pred Grad: -0.037, New P: 1.277
iter 20 loss: 0.005
Actual params: [0.1372, 1.2774]
-Original Grad: 0.009, -lr * Pred Grad: 0.014, New P: 0.152
-Original Grad: 0.005, -lr * Pred Grad: -0.022, New P: 1.256
iter 21 loss: 0.005
Actual params: [0.1516, 1.2556]
-Original Grad: 0.001, -lr * Pred Grad: -0.001, New P: 0.150
-Original Grad: 0.016, -lr * Pred Grad: 0.011, New P: 1.266
iter 22 loss: 0.005
Actual params: [0.1501, 1.2662]
-Original Grad: 0.005, -lr * Pred Grad: 0.002, New P: 0.152
-Original Grad: 0.004, -lr * Pred Grad: -0.006, New P: 1.260
iter 23 loss: 0.005
Actual params: [0.1525, 1.2598]
-Original Grad: 0.000, -lr * Pred Grad: -0.011, New P: 0.141
-Original Grad: 0.006, -lr * Pred Grad: -0.006, New P: 1.254
iter 24 loss: 0.005
Actual params: [0.141, 1.254]
-Original Grad: 0.012, -lr * Pred Grad: 0.014, New P: 0.155
-Original Grad: 0.012, -lr * Pred Grad: 0.010, New P: 1.264
iter 25 loss: 0.005
Actual params: [0.1552, 1.2638]
-Original Grad: 0.012, -lr * Pred Grad: 0.025, New P: 0.180
-Original Grad: 0.019, -lr * Pred Grad: 0.033, New P: 1.297
iter 26 loss: 0.004
Actual params: [0.1805, 1.2971]
-Original Grad: -0.001, -lr * Pred Grad: -0.001, New P: 0.179
-Original Grad: -0.000, -lr * Pred Grad: -0.007, New P: 1.290
iter 27 loss: 0.004
Actual params: [0.1791, 1.2899]
-Original Grad: 0.006, -lr * Pred Grad: 0.007, New P: 0.186
-Original Grad: 0.001, -lr * Pred Grad: -0.017, New P: 1.273
iter 28 loss: 0.004
Actual params: [0.1861, 1.273 ]
-Original Grad: 0.003, -lr * Pred Grad: -0.001, New P: 0.185
-Original Grad: 0.007, -lr * Pred Grad: -0.009, New P: 1.264
iter 29 loss: 0.005
Actual params: [0.1854, 1.264 ]
-Original Grad: 0.003, -lr * Pred Grad: -0.005, New P: 0.180
-Original Grad: 0.012, -lr * Pred Grad: 0.008, New P: 1.272
iter 30 loss: 0.005
Actual params: [0.1799, 1.2724]
-Original Grad: -0.002, -lr * Pred Grad: -0.023, New P: 0.157
-Original Grad: 0.014, -lr * Pred Grad: 0.021, New P: 1.293
Target params: [1.3344, 1.5708]
iter 0 loss: 0.370
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.109, -lr * Pred Grad: -0.555, New P: -1.028
-Original Grad: -0.005, -lr * Pred Grad: 0.610, New P: 0.614
iter 1 loss: 0.293
Actual params: [-1.0276,  0.6137]
-Original Grad: -0.029, -lr * Pred Grad: -0.203, New P: -1.230
-Original Grad: 0.046, -lr * Pred Grad: 0.380, New P: 0.994
iter 2 loss: 0.284
Actual params: [-1.2301,  0.9938]
-Original Grad: -0.002, -lr * Pred Grad: -0.115, New P: -1.345
-Original Grad: 0.006, -lr * Pred Grad: -0.051, New P: 0.943
iter 3 loss: 0.284
Actual params: [-1.3449,  0.9431]
-Original Grad: -0.001, -lr * Pred Grad: -0.094, New P: -1.439
-Original Grad: 0.002, -lr * Pred Grad: -0.009, New P: 0.934
iter 4 loss: 0.284
Actual params: [-1.4386,  0.9341]
-Original Grad: -0.001, -lr * Pred Grad: -0.076, New P: -1.515
-Original Grad: 0.002, -lr * Pred Grad: -0.034, New P: 0.900
iter 5 loss: 0.284
Actual params: [-1.5146,  0.9005]
-Original Grad: -0.001, -lr * Pred Grad: -0.063, New P: -1.578
-Original Grad: 0.001, -lr * Pred Grad: -0.034, New P: 0.866
iter 6 loss: 0.284
Actual params: [-1.5777,  0.8661]
-Original Grad: -0.001, -lr * Pred Grad: -0.056, New P: -1.634
-Original Grad: 0.001, -lr * Pred Grad: -0.033, New P: 0.833
iter 7 loss: 0.284
Actual params: [-1.6337,  0.8333]
-Original Grad: -0.000, -lr * Pred Grad: -0.051, New P: -1.685
-Original Grad: 0.001, -lr * Pred Grad: -0.030, New P: 0.803
iter 8 loss: 0.284
Actual params: [-1.6848,  0.8032]
-Original Grad: -0.000, -lr * Pred Grad: -0.049, New P: -1.733
-Original Grad: 0.001, -lr * Pred Grad: -0.028, New P: 0.775
iter 9 loss: 0.284
Actual params: [-1.7334,  0.7752]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -1.780
-Original Grad: 0.001, -lr * Pred Grad: -0.027, New P: 0.748
iter 10 loss: 0.284
Actual params: [-1.7805,  0.748 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.826
-Original Grad: 0.000, -lr * Pred Grad: -0.028, New P: 0.720
iter 11 loss: 0.284
Actual params: [-1.8262,  0.7204]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.871
-Original Grad: 0.000, -lr * Pred Grad: -0.028, New P: 0.692
iter 12 loss: 0.284
Actual params: [-1.8711,  0.6922]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.916
-Original Grad: 0.000, -lr * Pred Grad: -0.029, New P: 0.663
iter 13 loss: 0.284
Actual params: [-1.9155,  0.6634]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -1.959
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.634
iter 14 loss: 0.284
Actual params: [-1.9592,  0.6337]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -2.003
-Original Grad: 0.000, -lr * Pred Grad: -0.030, New P: 0.603
iter 15 loss: 0.284
Actual params: [-2.0027,  0.6035]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -2.046
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.573
iter 16 loss: 0.284
Actual params: [-2.0462,  0.573 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -2.090
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.542
iter 17 loss: 0.284
Actual params: [-2.0898,  0.542 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.133
-Original Grad: 0.000, -lr * Pred Grad: -0.031, New P: 0.511
iter 18 loss: 0.284
Actual params: [-2.1332,  0.5107]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.177
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.479
iter 19 loss: 0.284
Actual params: [-2.1767,  0.4791]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.220
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.447
iter 20 loss: 0.284
Actual params: [-2.2202,  0.4472]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.264
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.415
iter 21 loss: 0.284
Actual params: [-2.2636,  0.4152]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.307
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.383
iter 22 loss: 0.284
Actual params: [-2.3071,  0.383 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.351
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.351
iter 23 loss: 0.284
Actual params: [-2.3505,  0.3508]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.394
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.318
iter 24 loss: 0.284
Actual params: [-2.394 ,  0.3185]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.438
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.286
iter 25 loss: 0.284
Actual params: [-2.4375,  0.286 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.481
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.254
iter 26 loss: 0.284
Actual params: [-2.481 ,  0.2536]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.524
-Original Grad: 0.000, -lr * Pred Grad: -0.032, New P: 0.221
iter 27 loss: 0.284
Actual params: [-2.5244,  0.2211]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.568
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.189
iter 28 loss: 0.284
Actual params: [-2.5679,  0.1886]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.611
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.156
iter 29 loss: 0.284
Actual params: [-2.6114,  0.1561]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.655
-Original Grad: 0.000, -lr * Pred Grad: -0.033, New P: 0.124
iter 30 loss: 0.284
Actual params: [-2.6549,  0.1236]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.698
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: 0.091
Target params: [1.3344, 1.5708]
iter 0 loss: 0.137
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.130, -lr * Pred Grad: 1.673, New P: 1.200
-Original Grad: 0.224, -lr * Pred Grad: 1.562, New P: 1.566
iter 1 loss: 0.018
Actual params: [1.2002, 1.5657]
-Original Grad: -0.011, -lr * Pred Grad: -0.030, New P: 1.170
-Original Grad: -0.024, -lr * Pred Grad: -0.092, New P: 1.474
iter 2 loss: 0.019
Actual params: [1.1703, 1.4738]
-Original Grad: -0.003, -lr * Pred Grad: 0.370, New P: 1.540
-Original Grad: 0.054, -lr * Pred Grad: 0.147, New P: 1.621
iter 3 loss: 0.026
Actual params: [1.54  , 1.6206]
-Original Grad: -0.031, -lr * Pred Grad: -0.046, New P: 1.494
-Original Grad: -0.123, -lr * Pred Grad: -0.259, New P: 1.362
iter 4 loss: 0.020
Actual params: [1.494 , 1.3621]
-Original Grad: 0.009, -lr * Pred Grad: 0.095, New P: 1.588
-Original Grad: 0.055, -lr * Pred Grad: -0.014, New P: 1.349
iter 5 loss: 0.019
Actual params: [1.5885, 1.3486]
-Original Grad: 0.004, -lr * Pred Grad: 0.048, New P: 1.636
-Original Grad: 0.007, -lr * Pred Grad: -0.032, New P: 1.317
iter 6 loss: 0.019
Actual params: [1.6362, 1.3166]
-Original Grad: 0.001, -lr * Pred Grad: 0.032, New P: 1.668
-Original Grad: 0.011, -lr * Pred Grad: -0.000, New P: 1.316
iter 7 loss: 0.019
Actual params: [1.6684, 1.3165]
-Original Grad: -0.010, -lr * Pred Grad: -0.015, New P: 1.653
-Original Grad: 0.002, -lr * Pred Grad: -0.015, New P: 1.302
iter 8 loss: 0.019
Actual params: [1.6534, 1.3016]
-Original Grad: 0.016, -lr * Pred Grad: 0.036, New P: 1.690
-Original Grad: 0.053, -lr * Pred Grad: 0.117, New P: 1.419
iter 9 loss: 0.020
Actual params: [1.6897, 1.4185]
-Original Grad: -0.014, -lr * Pred Grad: -0.036, New P: 1.654
-Original Grad: -0.045, -lr * Pred Grad: -0.092, New P: 1.326
iter 10 loss: 0.019
Actual params: [1.6536, 1.3265]
-Original Grad: -0.007, -lr * Pred Grad: -0.047, New P: 1.606
-Original Grad: -0.019, -lr * Pred Grad: -0.082, New P: 1.244
iter 11 loss: 0.021
Actual params: [1.6062, 1.2444]
-Original Grad: 0.034, -lr * Pred Grad: 0.055, New P: 1.661
-Original Grad: 0.120, -lr * Pred Grad: 0.266, New P: 1.510
iter 12 loss: 0.023
Actual params: [1.6614, 1.5102]
-Original Grad: -0.026, -lr * Pred Grad: -0.066, New P: 1.596
-Original Grad: -0.100, -lr * Pred Grad: -0.196, New P: 1.314
iter 13 loss: 0.020
Actual params: [1.5958, 1.3139]
-Original Grad: 0.019, -lr * Pred Grad: 0.012, New P: 1.608
-Original Grad: 0.043, -lr * Pred Grad: 0.026, New P: 1.340
iter 14 loss: 0.019
Actual params: [1.6078, 1.3396]
-Original Grad: -0.006, -lr * Pred Grad: -0.033, New P: 1.575
-Original Grad: -0.005, -lr * Pred Grad: -0.036, New P: 1.303
iter 15 loss: 0.020
Actual params: [1.5752, 1.3033]
-Original Grad: 0.005, -lr * Pred Grad: -0.017, New P: 1.559
-Original Grad: 0.034, -lr * Pred Grad: 0.061, New P: 1.364
iter 16 loss: 0.019
Actual params: [1.5587, 1.3639]
-Original Grad: 0.014, -lr * Pred Grad: 0.014, New P: 1.573
-Original Grad: 0.075, -lr * Pred Grad: 0.224, New P: 1.588
iter 17 loss: 0.025
Actual params: [1.5728, 1.5883]
-Original Grad: -0.012, -lr * Pred Grad: -0.044, New P: 1.529
-Original Grad: -0.068, -lr * Pred Grad: -0.137, New P: 1.451
iter 18 loss: 0.020
Actual params: [1.5292, 1.451 ]
-Original Grad: -0.007, -lr * Pred Grad: -0.057, New P: 1.473
-Original Grad: -0.006, -lr * Pred Grad: -0.059, New P: 1.392
iter 19 loss: 0.019
Actual params: [1.4726, 1.3916]
-Original Grad: -0.008, -lr * Pred Grad: -0.070, New P: 1.402
-Original Grad: 0.015, -lr * Pred Grad: -0.020, New P: 1.372
iter 20 loss: 0.021
Actual params: [1.4023, 1.3718]
-Original Grad: 0.011, -lr * Pred Grad: -0.028, New P: 1.374
-Original Grad: 0.071, -lr * Pred Grad: 0.170, New P: 1.542
iter 21 loss: 0.019
Actual params: [1.3739, 1.5418]
-Original Grad: -0.014, -lr * Pred Grad: -0.080, New P: 1.294
-Original Grad: -0.027, -lr * Pred Grad: -0.044, New P: 1.497
iter 22 loss: 0.018
Actual params: [1.2944, 1.4975]
-Original Grad: 0.003, -lr * Pred Grad: -0.054, New P: 1.241
-Original Grad: 0.019, -lr * Pred Grad: 0.027, New P: 1.525
iter 23 loss: 0.018
Actual params: [1.2406, 1.5249]
-Original Grad: -0.012, -lr * Pred Grad: -0.088, New P: 1.153
-Original Grad: -0.023, -lr * Pred Grad: -0.069, New P: 1.456
iter 24 loss: 0.020
Actual params: [1.1526, 1.4558]
-Original Grad: -0.003, -lr * Pred Grad: -0.078, New P: 1.074
-Original Grad: 0.118, -lr * Pred Grad: 0.287, New P: 1.743
iter 25 loss: 0.025
Actual params: [1.0743, 1.7431]
-Original Grad: 0.003, -lr * Pred Grad: -0.060, New P: 1.014
-Original Grad: -0.163, -lr * Pred Grad: -0.297, New P: 1.446
iter 26 loss: 0.019
Actual params: [1.0145, 1.4464]
-Original Grad: 0.000, -lr * Pred Grad: -0.057, New P: 0.958
-Original Grad: 0.080, -lr * Pred Grad: 0.038, New P: 1.484
iter 27 loss: 0.018
Actual params: [0.9578, 1.4842]
-Original Grad: 0.006, -lr * Pred Grad: -0.036, New P: 0.922
-Original Grad: 0.065, -lr * Pred Grad: 0.157, New P: 1.642
iter 28 loss: 0.019
Actual params: [0.9217, 1.6416]
-Original Grad: -0.005, -lr * Pred Grad: -0.057, New P: 0.864
-Original Grad: -0.039, -lr * Pred Grad: -0.069, New P: 1.573
iter 29 loss: 0.017
Actual params: [0.8644, 1.573 ]
-Original Grad: 0.024, -lr * Pred Grad: 0.017, New P: 0.882
-Original Grad: -0.035, -lr * Pred Grad: -0.104, New P: 1.469
iter 30 loss: 0.018
Actual params: [0.8815, 1.4689]
-Original Grad: 0.007, -lr * Pred Grad: 0.007, New P: 0.888
-Original Grad: 0.030, -lr * Pred Grad: -0.002, New P: 1.467
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad: 0.651, New P: 0.179
-Original Grad: 0.000, -lr * Pred Grad: 0.635, New P: 0.639
iter 1 loss: 0.306
Actual params: [0.1791, 0.6386]
-Original Grad: 0.260, -lr * Pred Grad: 2.014, New P: 2.193
-Original Grad: 0.167, -lr * Pred Grad: 1.091, New P: 1.730
iter 2 loss: 0.176
Actual params: [2.1934, 1.7297]
-Original Grad: -0.054, -lr * Pred Grad: -0.563, New P: 1.630
-Original Grad: -0.236, -lr * Pred Grad: -0.821, New P: 0.909
iter 3 loss: 0.034
Actual params: [1.6299, 0.909 ]
-Original Grad: -0.112, -lr * Pred Grad: -0.320, New P: 1.309
-Original Grad: -0.039, -lr * Pred Grad: -0.098, New P: 0.811
iter 4 loss: 0.026
Actual params: [1.3095, 0.8112]
-Original Grad: 0.031, -lr * Pred Grad: 0.054, New P: 1.364
-Original Grad: 0.146, -lr * Pred Grad: 0.122, New P: 0.933
iter 5 loss: 0.012
Actual params: [1.3637, 0.9331]
-Original Grad: -0.014, -lr * Pred Grad: -0.056, New P: 1.308
-Original Grad: 0.083, -lr * Pred Grad: 0.288, New P: 1.222
iter 6 loss: 0.010
Actual params: [1.3077, 1.2215]
-Original Grad: -0.014, -lr * Pred Grad: -0.068, New P: 1.240
-Original Grad: -0.027, -lr * Pred Grad: -0.030, New P: 1.192
iter 7 loss: 0.010
Actual params: [1.2399, 1.192 ]
-Original Grad: 0.019, -lr * Pred Grad: 0.006, New P: 1.246
-Original Grad: -0.009, -lr * Pred Grad: -0.025, New P: 1.167
iter 8 loss: 0.010
Actual params: [1.2462, 1.1671]
-Original Grad: 0.023, -lr * Pred Grad: 0.049, New P: 1.296
-Original Grad: 0.031, -lr * Pred Grad: 0.043, New P: 1.210
iter 9 loss: 0.010
Actual params: [1.2956, 1.21  ]
-Original Grad: 0.005, -lr * Pred Grad: 0.029, New P: 1.325
-Original Grad: -0.001, -lr * Pred Grad: -0.004, New P: 1.206
iter 10 loss: 0.010
Actual params: [1.3251, 1.2063]
-Original Grad: -0.016, -lr * Pred Grad: -0.035, New P: 1.290
-Original Grad: 0.007, -lr * Pred Grad: 0.002, New P: 1.208
iter 11 loss: 0.010
Actual params: [1.29  , 1.2084]
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: 1.291
-Original Grad: 0.021, -lr * Pred Grad: 0.035, New P: 1.243
iter 12 loss: 0.010
Actual params: [1.2906, 1.2434]
-Original Grad: -0.010, -lr * Pred Grad: -0.045, New P: 1.246
-Original Grad: -0.031, -lr * Pred Grad: -0.079, New P: 1.164
iter 13 loss: 0.011
Actual params: [1.2456, 1.1642]
-Original Grad: 0.023, -lr * Pred Grad: 0.026, New P: 1.272
-Original Grad: 0.047, -lr * Pred Grad: 0.070, New P: 1.235
iter 14 loss: 0.010
Actual params: [1.272 , 1.2346]
-Original Grad: -0.010, -lr * Pred Grad: -0.034, New P: 1.239
-Original Grad: -0.037, -lr * Pred Grad: -0.087, New P: 1.147
iter 15 loss: 0.011
Actual params: [1.2385, 1.1474]
-Original Grad: 0.019, -lr * Pred Grad: 0.026, New P: 1.264
-Original Grad: 0.022, -lr * Pred Grad: 0.007, New P: 1.154
iter 16 loss: 0.010
Actual params: [1.2644, 1.1539]
-Original Grad: 0.024, -lr * Pred Grad: 0.065, New P: 1.329
-Original Grad: 0.042, -lr * Pred Grad: 0.091, New P: 1.245
iter 17 loss: 0.011
Actual params: [1.3291, 1.2445]
-Original Grad: -0.010, -lr * Pred Grad: -0.007, New P: 1.322
-Original Grad: -0.014, -lr * Pred Grad: -0.024, New P: 1.221
iter 18 loss: 0.010
Actual params: [1.3224, 1.2205]
-Original Grad: -0.008, -lr * Pred Grad: -0.032, New P: 1.290
-Original Grad: 0.026, -lr * Pred Grad: 0.044, New P: 1.264
iter 19 loss: 0.011
Actual params: [1.2901, 1.2644]
-Original Grad: 0.003, -lr * Pred Grad: -0.022, New P: 1.268
-Original Grad: -0.018, -lr * Pred Grad: -0.051, New P: 1.213
iter 20 loss: 0.010
Actual params: [1.2677, 1.2131]
-Original Grad: -0.002, -lr * Pred Grad: -0.034, New P: 1.234
-Original Grad: -0.090, -lr * Pred Grad: -0.204, New P: 1.009
iter 21 loss: 0.014
Actual params: [1.2336, 1.0089]
-Original Grad: 0.073, -lr * Pred Grad: 0.167, New P: 1.400
-Original Grad: 0.008, -lr * Pred Grad: -0.103, New P: 0.906
iter 22 loss: 0.014
Actual params: [1.4003, 0.9057]
-Original Grad: -0.048, -lr * Pred Grad: -0.077, New P: 1.324
-Original Grad: 0.022, -lr * Pred Grad: -0.018, New P: 0.888
iter 23 loss: 0.016
Actual params: [1.3235, 0.8879]
-Original Grad: 0.020, -lr * Pred Grad: 0.022, New P: 1.345
-Original Grad: 0.138, -lr * Pred Grad: 0.389, New P: 1.277
iter 24 loss: 0.012
Actual params: [1.3454, 1.277 ]
-Original Grad: -0.025, -lr * Pred Grad: -0.078, New P: 1.267
-Original Grad: -0.037, -lr * Pred Grad: -0.053, New P: 1.224
iter 25 loss: 0.010
Actual params: [1.2674, 1.2236]
-Original Grad: -0.003, -lr * Pred Grad: -0.062, New P: 1.206
-Original Grad: -0.018, -lr * Pred Grad: -0.043, New P: 1.180
iter 26 loss: 0.011
Actual params: [1.2058, 1.1805]
-Original Grad: 0.027, -lr * Pred Grad: 0.021, New P: 1.227
-Original Grad: 0.041, -lr * Pred Grad: 0.056, New P: 1.236
iter 27 loss: 0.011
Actual params: [1.2273, 1.2364]
-Original Grad: 0.015, -lr * Pred Grad: 0.031, New P: 1.258
-Original Grad: 0.019, -lr * Pred Grad: 0.052, New P: 1.288
iter 28 loss: 0.011
Actual params: [1.2579, 1.2883]
-Original Grad: 0.006, -lr * Pred Grad: 0.021, New P: 1.279
-Original Grad: -0.017, -lr * Pred Grad: -0.040, New P: 1.249
iter 29 loss: 0.010
Actual params: [1.2789, 1.2487]
-Original Grad: -0.002, -lr * Pred Grad: -0.004, New P: 1.275
-Original Grad: -0.062, -lr * Pred Grad: -0.156, New P: 1.093
iter 30 loss: 0.011
Actual params: [1.2748, 1.093 ]
-Original Grad: 0.012, -lr * Pred Grad: 0.023, New P: 1.298
-Original Grad: 0.016, -lr * Pred Grad: -0.055, New P: 1.038
Target params: [1.3344, 1.5708]
iter 0 loss: 0.296
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.030, -lr * Pred Grad: 0.325, New P: -0.147
-Original Grad: 0.057, -lr * Pred Grad: 0.902, New P: 0.906
iter 1 loss: 0.211
Actual params: [-0.1469,  0.9057]
-Original Grad: 0.125, -lr * Pred Grad: 1.385, New P: 1.238
-Original Grad: 0.209, -lr * Pred Grad: 1.334, New P: 2.239
iter 2 loss: 0.056
Actual params: [1.238 , 2.2392]
-Original Grad: 0.048, -lr * Pred Grad: 0.485, New P: 1.723
-Original Grad: -0.193, -lr * Pred Grad: -0.835, New P: 1.404
iter 3 loss: 0.049
Actual params: [1.7228, 1.4042]
-Original Grad: -0.066, -lr * Pred Grad: -0.117, New P: 1.606
-Original Grad: 0.204, -lr * Pred Grad: 0.324, New P: 1.728
iter 4 loss: 0.015
Actual params: [1.606 , 1.7281]
-Original Grad: -0.013, -lr * Pred Grad: 0.028, New P: 1.634
-Original Grad: 0.068, -lr * Pred Grad: 0.250, New P: 1.978
iter 5 loss: 0.013
Actual params: [1.6343, 1.9783]
-Original Grad: 0.028, -lr * Pred Grad: 0.120, New P: 1.755
-Original Grad: -0.037, -lr * Pred Grad: -0.046, New P: 1.932
iter 6 loss: 0.012
Actual params: [1.7548, 1.932 ]
-Original Grad: -0.025, -lr * Pred Grad: -0.035, New P: 1.720
-Original Grad: 0.066, -lr * Pred Grad: 0.145, New P: 2.077
iter 7 loss: 0.012
Actual params: [1.7197, 2.0774]
-Original Grad: 0.048, -lr * Pred Grad: 0.136, New P: 1.856
-Original Grad: -0.065, -lr * Pred Grad: -0.122, New P: 1.955
iter 8 loss: 0.015
Actual params: [1.8556, 1.9551]
-Original Grad: -0.032, -lr * Pred Grad: -0.045, New P: 1.810
-Original Grad: 0.051, -lr * Pred Grad: 0.062, New P: 2.017
iter 9 loss: 0.011
Actual params: [1.8104, 2.0167]
-Original Grad: -0.025, -lr * Pred Grad: -0.089, New P: 1.721
-Original Grad: 0.045, -lr * Pred Grad: 0.124, New P: 2.140
iter 10 loss: 0.014
Actual params: [1.721 , 2.1403]
-Original Grad: 0.038, -lr * Pred Grad: 0.047, New P: 1.768
-Original Grad: -0.046, -lr * Pred Grad: -0.086, New P: 2.054
iter 11 loss: 0.011
Actual params: [1.7677, 2.0545]
-Original Grad: -0.009, -lr * Pred Grad: -0.025, New P: 1.743
-Original Grad: 0.036, -lr * Pred Grad: 0.047, New P: 2.101
iter 12 loss: 0.012
Actual params: [1.7426, 2.1014]
-Original Grad: 0.044, -lr * Pred Grad: 0.103, New P: 1.845
-Original Grad: -0.056, -lr * Pred Grad: -0.131, New P: 1.971
iter 13 loss: 0.014
Actual params: [1.8452, 1.9707]
-Original Grad: -0.036, -lr * Pred Grad: -0.068, New P: 1.777
-Original Grad: 0.067, -lr * Pred Grad: 0.095, New P: 2.066
iter 14 loss: 0.011
Actual params: [1.7773, 2.0662]
-Original Grad: 0.009, -lr * Pred Grad: -0.012, New P: 1.766
-Original Grad: 0.004, -lr * Pred Grad: 0.019, New P: 2.085
iter 15 loss: 0.011
Actual params: [1.7655, 2.0853]
-Original Grad: 0.025, -lr * Pred Grad: 0.044, New P: 1.809
-Original Grad: -0.022, -lr * Pred Grad: -0.056, New P: 2.029
iter 16 loss: 0.011
Actual params: [1.8094, 2.0294]
-Original Grad: -0.003, -lr * Pred Grad: -0.001, New P: 1.808
-Original Grad: 0.021, -lr * Pred Grad: 0.012, New P: 2.041
iter 17 loss: 0.011
Actual params: [1.808 , 2.0412]
-Original Grad: 0.001, -lr * Pred Grad: -0.006, New P: 1.802
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: 2.042
iter 18 loss: 0.011
Actual params: [1.8022, 2.0415]
-Original Grad: -0.013, -lr * Pred Grad: -0.054, New P: 1.748
-Original Grad: 0.031, -lr * Pred Grad: 0.063, New P: 2.105
iter 19 loss: 0.012
Actual params: [1.7483, 2.105 ]
-Original Grad: 0.028, -lr * Pred Grad: 0.036, New P: 1.785
-Original Grad: -0.023, -lr * Pred Grad: -0.056, New P: 2.049
iter 20 loss: 0.011
Actual params: [1.7845, 2.0494]
-Original Grad: 0.008, -lr * Pred Grad: 0.021, New P: 1.805
-Original Grad: 0.001, -lr * Pred Grad: -0.031, New P: 2.018
iter 21 loss: 0.011
Actual params: [1.8055, 2.0181]
-Original Grad: -0.006, -lr * Pred Grad: -0.016, New P: 1.789
-Original Grad: 0.032, -lr * Pred Grad: 0.045, New P: 2.063
iter 22 loss: 0.011
Actual params: [1.7894, 2.0628]
-Original Grad: 0.000, -lr * Pred Grad: -0.018, New P: 1.771
-Original Grad: 0.012, -lr * Pred Grad: 0.028, New P: 2.091
iter 23 loss: 0.011
Actual params: [1.7711, 2.0905]
-Original Grad: 0.023, -lr * Pred Grad: 0.041, New P: 1.812
-Original Grad: -0.028, -lr * Pred Grad: -0.074, New P: 2.016
iter 24 loss: 0.011
Actual params: [1.8118, 2.0163]
-Original Grad: -0.017, -lr * Pred Grad: -0.044, New P: 1.768
-Original Grad: 0.037, -lr * Pred Grad: 0.044, New P: 2.060
iter 25 loss: 0.011
Actual params: [1.7678, 2.0601]
-Original Grad: 0.007, -lr * Pred Grad: -0.013, New P: 1.754
-Original Grad: 0.004, -lr * Pred Grad: 0.003, New P: 2.063
iter 26 loss: 0.011
Actual params: [1.7545, 2.0629]
-Original Grad: 0.010, -lr * Pred Grad: 0.004, New P: 1.759
-Original Grad: -0.005, -lr * Pred Grad: -0.027, New P: 2.036
iter 27 loss: 0.011
Actual params: [1.7587, 2.0362]
-Original Grad: 0.008, -lr * Pred Grad: 0.010, New P: 1.769
-Original Grad: 0.012, -lr * Pred Grad: 0.001, New P: 2.037
iter 28 loss: 0.011
Actual params: [1.7689, 2.0369]
-Original Grad: -0.003, -lr * Pred Grad: -0.016, New P: 1.753
-Original Grad: 0.019, -lr * Pred Grad: 0.029, New P: 2.066
iter 29 loss: 0.011
Actual params: [1.7532, 2.0662]
-Original Grad: 0.018, -lr * Pred Grad: 0.030, New P: 1.783
-Original Grad: -0.020, -lr * Pred Grad: -0.057, New P: 2.009
iter 30 loss: 0.011
Actual params: [1.7835, 2.0088]
-Original Grad: 0.001, -lr * Pred Grad: 0.002, New P: 1.785
-Original Grad: 0.011, -lr * Pred Grad: -0.012, New P: 1.997
Target params: [1.3344, 1.5708]
iter 0 loss: 0.494
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.071, -lr * Pred Grad: -0.141, New P: -0.614
-Original Grad: -0.086, -lr * Pred Grad: 0.238, New P: 0.242
iter 1 loss: 0.487
Actual params: [-0.6135,  0.2416]
-Original Grad: -0.071, -lr * Pred Grad: -0.493, New P: -1.106
-Original Grad: -0.037, -lr * Pred Grad: -0.048, New P: 0.194
iter 2 loss: 0.439
Actual params: [-1.1065,  0.194 ]
-Original Grad: -0.024, -lr * Pred Grad: -0.293, New P: -1.399
-Original Grad: -0.023, -lr * Pred Grad: -0.086, New P: 0.108
iter 3 loss: 0.434
Actual params: [-1.3991,  0.1076]
-Original Grad: -0.003, -lr * Pred Grad: -0.207, New P: -1.606
-Original Grad: -0.004, -lr * Pred Grad: -0.084, New P: 0.023
iter 4 loss: 0.433
Actual params: [-1.6059,  0.0234]
-Original Grad: -0.001, -lr * Pred Grad: -0.161, New P: -1.767
-Original Grad: -0.001, -lr * Pred Grad: -0.072, New P: -0.049
iter 5 loss: 0.433
Actual params: [-1.7671, -0.049 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.127, New P: -1.894
-Original Grad: -0.000, -lr * Pred Grad: -0.059, New P: -0.108
iter 6 loss: 0.433
Actual params: [-1.8941, -0.1083]
-Original Grad: -0.000, -lr * Pred Grad: -0.102, New P: -1.996
-Original Grad: -0.000, -lr * Pred Grad: -0.049, New P: -0.157
iter 7 loss: 0.433
Actual params: [-1.996 , -0.1573]
-Original Grad: -0.000, -lr * Pred Grad: -0.084, New P: -2.080
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -0.199
iter 8 loss: 0.433
Actual params: [-2.0804, -0.1994]
-Original Grad: -0.000, -lr * Pred Grad: -0.072, New P: -2.153
-Original Grad: -0.000, -lr * Pred Grad: -0.038, New P: -0.237
iter 9 loss: 0.433
Actual params: [-2.1526, -0.2371]
-Original Grad: -0.000, -lr * Pred Grad: -0.064, New P: -2.216
-Original Grad: -0.000, -lr * Pred Grad: -0.035, New P: -0.272
iter 10 loss: 0.433
Actual params: [-2.2163, -0.2724]
-Original Grad: -0.000, -lr * Pred Grad: -0.058, New P: -2.274
-Original Grad: -0.000, -lr * Pred Grad: -0.034, New P: -0.306
iter 11 loss: 0.433
Actual params: [-2.274 , -0.3063]
-Original Grad: -0.000, -lr * Pred Grad: -0.054, New P: -2.328
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.339
iter 12 loss: 0.433
Actual params: [-2.3276, -0.3394]
-Original Grad: -0.000, -lr * Pred Grad: -0.051, New P: -2.378
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.372
iter 13 loss: 0.433
Actual params: [-2.3781, -0.3721]
-Original Grad: -0.000, -lr * Pred Grad: -0.048, New P: -2.426
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.405
iter 14 loss: 0.433
Actual params: [-2.4264, -0.4047]
-Original Grad: -0.000, -lr * Pred Grad: -0.047, New P: -2.473
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.437
iter 15 loss: 0.433
Actual params: [-2.4732, -0.4373]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -2.519
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.470
iter 16 loss: 0.433
Actual params: [-2.5189, -0.4698]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -2.564
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.502
iter 17 loss: 0.433
Actual params: [-2.5637, -0.5024]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -2.608
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.535
iter 18 loss: 0.433
Actual params: [-2.608, -0.535]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -2.652
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.568
iter 19 loss: 0.433
Actual params: [-2.652 , -0.5676]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -2.696
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.600
iter 20 loss: 0.433
Actual params: [-2.6957, -0.6002]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -2.739
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.633
iter 21 loss: 0.433
Actual params: [-2.7392, -0.6328]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.783
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.665
iter 22 loss: 0.433
Actual params: [-2.7826, -0.6654]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.826
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.698
iter 23 loss: 0.433
Actual params: [-2.826, -0.698]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.869
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.731
iter 24 loss: 0.433
Actual params: [-2.8693, -0.7306]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.913
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.763
iter 25 loss: 0.433
Actual params: [-2.9127, -0.7631]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.956
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.796
iter 26 loss: 0.433
Actual params: [-2.956 , -0.7957]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -2.999
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.828
iter 27 loss: 0.433
Actual params: [-2.9993, -0.8283]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -3.043
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.861
iter 28 loss: 0.433
Actual params: [-3.0426, -0.8609]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -3.086
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.893
iter 29 loss: 0.433
Actual params: [-3.086 , -0.8935]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -3.129
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.926
iter 30 loss: 0.433
Actual params: [-3.1294, -0.926 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -3.173
-Original Grad: -0.000, -lr * Pred Grad: -0.033, New P: -0.959
Target params: [1.3344, 1.5708]
iter 0 loss: 0.746
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.029, -lr * Pred Grad: 0.935, New P: 0.463
-Original Grad: 0.029, -lr * Pred Grad: 0.773, New P: 0.777
iter 1 loss: 0.522
Actual params: [0.4627, 0.7769]
-Original Grad: 0.546, -lr * Pred Grad: 2.471, New P: 2.934
-Original Grad: 0.323, -lr * Pred Grad: 1.854, New P: 2.631
iter 2 loss: 0.677
Actual params: [2.9341, 2.6308]
-Original Grad: -0.052, -lr * Pred Grad: -0.938, New P: 1.996
-Original Grad: -0.793, -lr * Pred Grad: -0.910, New P: 1.721
iter 3 loss: 0.324
Actual params: [1.9959, 1.7211]
-Original Grad: -0.236, -lr * Pred Grad: -0.595, New P: 1.401
-Original Grad: 0.326, -lr * Pred Grad: 0.144, New P: 1.865
iter 4 loss: 0.070
Actual params: [1.4014, 1.8647]
-Original Grad: 0.068, -lr * Pred Grad: 0.000, New P: 1.402
-Original Grad: 0.045, -lr * Pred Grad: -0.057, New P: 1.808
iter 5 loss: 0.072
Actual params: [1.4015, 1.8079]
-Original Grad: 0.041, -lr * Pred Grad: 0.027, New P: 1.429
-Original Grad: 0.096, -lr * Pred Grad: 0.228, New P: 2.036
iter 6 loss: 0.068
Actual params: [1.4287, 2.0362]
-Original Grad: -0.046, -lr * Pred Grad: -0.101, New P: 1.327
-Original Grad: 0.039, -lr * Pred Grad: 0.165, New P: 2.201
iter 7 loss: 0.101
Actual params: [1.3274, 2.201 ]
-Original Grad: 0.229, -lr * Pred Grad: 0.451, New P: 1.778
-Original Grad: -0.197, -lr * Pred Grad: -0.303, New P: 1.898
iter 8 loss: 0.199
Actual params: [1.7782, 1.8981]
-Original Grad: -0.667, -lr * Pred Grad: -0.951, New P: 0.827
-Original Grad: 0.475, -lr * Pred Grad: 1.054, New P: 2.952
iter 9 loss: 0.868
Actual params: [0.8274, 2.9521]
-Original Grad: 0.187, -lr * Pred Grad: -0.269, New P: 0.559
-Original Grad: -1.618, -lr * Pred Grad: -0.495, New P: 2.457
iter 10 loss: 0.509
Actual params: [0.5586, 2.457 ]
-Original Grad: 1.134, -lr * Pred Grad: 0.828, New P: 1.387
-Original Grad: -0.610, -lr * Pred Grad: -0.484, New P: 1.973
iter 11 loss: 0.072
Actual params: [1.3869, 1.9729]
-Original Grad: 0.095, -lr * Pred Grad: 0.035, New P: 1.422
-Original Grad: -0.044, -lr * Pred Grad: -0.566, New P: 1.407
iter 12 loss: 0.203
Actual params: [1.4217, 1.407 ]
-Original Grad: -0.404, -lr * Pred Grad: -0.483, New P: 0.939
-Original Grad: 1.066, -lr * Pred Grad: 1.260, New P: 2.667
iter 13 loss: 0.609
Actual params: [0.939 , 2.6673]
-Original Grad: 0.169, -lr * Pred Grad: 0.051, New P: 0.990
-Original Grad: -1.693, -lr * Pred Grad: -0.545, New P: 2.122
iter 14 loss: 0.240
Actual params: [0.9901, 2.122 ]
-Original Grad: 0.738, -lr * Pred Grad: 0.850, New P: 1.840
-Original Grad: -0.552, -lr * Pred Grad: -0.474, New P: 1.648
iter 15 loss: 0.293
Actual params: [1.8403, 1.6482]
-Original Grad: -0.461, -lr * Pred Grad: -0.602, New P: 1.238
-Original Grad: 0.609, -lr * Pred Grad: -0.091, New P: 1.557
iter 16 loss: 0.092
Actual params: [1.238 , 1.5574]
-Original Grad: 0.036, -lr * Pred Grad: -0.216, New P: 1.022
-Original Grad: 0.167, -lr * Pred Grad: 0.084, New P: 1.642
iter 17 loss: 0.142
Actual params: [1.022 , 1.6416]
-Original Grad: 0.271, -lr * Pred Grad: 0.233, New P: 1.255
-Original Grad: -0.056, -lr * Pred Grad: -0.184, New P: 1.457
iter 18 loss: 0.115
Actual params: [1.2547, 1.4575]
-Original Grad: -0.220, -lr * Pred Grad: -0.322, New P: 0.933
-Original Grad: 0.786, -lr * Pred Grad: 1.777, New P: 3.234
iter 19 loss: 1.021
Actual params: [0.9331, 3.2345]
-Original Grad: -0.510, -lr * Pred Grad: -1.103, New P: -0.170
-Original Grad: -0.757, -lr * Pred Grad: -0.744, New P: 2.491
iter 20 loss: 0.749
Actual params: [-0.1704,  2.4909]
-Original Grad: 0.012, -lr * Pred Grad: -0.847, New P: -1.018
-Original Grad: -0.006, -lr * Pred Grad: -0.182, New P: 2.309
iter 21 loss: 0.751
Actual params: [-1.0176,  2.3093]
-Original Grad: 0.000, -lr * Pred Grad: -0.799, New P: -1.817
-Original Grad: 0.000, -lr * Pred Grad: -0.320, New P: 1.989
iter 22 loss: 0.751
Actual params: [-1.8165,  1.9893]
-Original Grad: 0.000, -lr * Pred Grad: -0.716, New P: -2.532
-Original Grad: 0.000, -lr * Pred Grad: -0.140, New P: 1.849
iter 23 loss: 0.751
Actual params: [-2.532 ,  1.8491]
-Original Grad: 0.000, -lr * Pred Grad: -0.668, New P: -3.200
-Original Grad: 0.000, -lr * Pred Grad: -0.114, New P: 1.735
iter 24 loss: 0.751
Actual params: [-3.2003,  1.735 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.623, New P: -3.824
-Original Grad: 0.000, -lr * Pred Grad: -0.078, New P: 1.657
iter 25 loss: 0.751
Actual params: [-3.8237,  1.6567]
-Original Grad: 0.000, -lr * Pred Grad: -0.574, New P: -4.398
-Original Grad: 0.000, -lr * Pred Grad: -0.064, New P: 1.592
iter 26 loss: 0.751
Actual params: [-4.3982,  1.5925]
-Original Grad: 0.000, -lr * Pred Grad: -0.520, New P: -4.918
-Original Grad: 0.000, -lr * Pred Grad: -0.052, New P: 1.540
iter 27 loss: 0.751
Actual params: [-4.9182,  1.54  ]
-Original Grad: 0.000, -lr * Pred Grad: -0.463, New P: -5.381
-Original Grad: 0.000, -lr * Pred Grad: -0.046, New P: 1.494
iter 28 loss: 0.751
Actual params: [-5.381 ,  1.4941]
-Original Grad: 0.000, -lr * Pred Grad: -0.405, New P: -5.786
-Original Grad: 0.000, -lr * Pred Grad: -0.042, New P: 1.452
iter 29 loss: 0.751
Actual params: [-5.7864,  1.4525]
-Original Grad: 0.000, -lr * Pred Grad: -0.350, New P: -6.136
-Original Grad: 0.000, -lr * Pred Grad: -0.039, New P: 1.413
iter 30 loss: 0.751
Actual params: [-6.1364,  1.4134]
-Original Grad: 0.000, -lr * Pred Grad: -0.299, New P: -6.435
-Original Grad: 0.000, -lr * Pred Grad: -0.038, New P: 1.376
Target params: [1.3344, 1.5708]
iter 0 loss: 0.103
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.025, -lr * Pred Grad: 0.374, New P: -0.098
-Original Grad: 0.032, -lr * Pred Grad: 0.786, New P: 0.790
iter 1 loss: 0.096
Actual params: [-0.0981,  0.7897]
-Original Grad: 0.004, -lr * Pred Grad: 0.363, New P: 0.265
-Original Grad: 0.081, -lr * Pred Grad: 0.590, New P: 1.379
iter 2 loss: 0.049
Actual params: [0.2654, 1.3793]
-Original Grad: 0.073, -lr * Pred Grad: 0.756, New P: 1.022
-Original Grad: 0.142, -lr * Pred Grad: 0.674, New P: 2.053
iter 3 loss: 0.034
Actual params: [1.0216, 2.053 ]
-Original Grad: 0.070, -lr * Pred Grad: 0.643, New P: 1.665
-Original Grad: -0.346, -lr * Pred Grad: -0.663, New P: 1.390
iter 4 loss: 0.016
Actual params: [1.6647, 1.3903]
-Original Grad: -0.030, -lr * Pred Grad: 0.079, New P: 1.744
-Original Grad: 0.135, -lr * Pred Grad: 0.017, New P: 1.408
iter 5 loss: 0.017
Actual params: [1.744 , 1.4075]
-Original Grad: -0.039, -lr * Pred Grad: -0.046, New P: 1.698
-Original Grad: 0.100, -lr * Pred Grad: 0.157, New P: 1.564
iter 6 loss: 0.011
Actual params: [1.698 , 1.5642]
-Original Grad: -0.026, -lr * Pred Grad: -0.084, New P: 1.614
-Original Grad: 0.019, -lr * Pred Grad: 0.098, New P: 1.662
iter 7 loss: 0.010
Actual params: [1.6141, 1.6621]
-Original Grad: -0.014, -lr * Pred Grad: -0.089, New P: 1.525
-Original Grad: -0.003, -lr * Pred Grad: 0.014, New P: 1.676
iter 8 loss: 0.010
Actual params: [1.5253, 1.6758]
-Original Grad: -0.000, -lr * Pred Grad: -0.063, New P: 1.462
-Original Grad: -0.026, -lr * Pred Grad: -0.066, New P: 1.609
iter 9 loss: 0.009
Actual params: [1.4619, 1.6093]
-Original Grad: -0.004, -lr * Pred Grad: -0.068, New P: 1.394
-Original Grad: -0.007, -lr * Pred Grad: -0.058, New P: 1.551
iter 10 loss: 0.008
Actual params: [1.3942, 1.5509]
-Original Grad: -0.004, -lr * Pred Grad: -0.068, New P: 1.326
-Original Grad: 0.005, -lr * Pred Grad: -0.032, New P: 1.518
iter 11 loss: 0.008
Actual params: [1.3257, 1.5185]
-Original Grad: -0.012, -lr * Pred Grad: -0.094, New P: 1.231
-Original Grad: 0.012, -lr * Pred Grad: -0.001, New P: 1.517
iter 12 loss: 0.007
Actual params: [1.2314, 1.5175]
-Original Grad: -0.007, -lr * Pred Grad: -0.093, New P: 1.138
-Original Grad: 0.031, -lr * Pred Grad: 0.060, New P: 1.578
iter 13 loss: 0.006
Actual params: [1.1381, 1.5775]
-Original Grad: -0.011, -lr * Pred Grad: -0.109, New P: 1.029
-Original Grad: 0.009, -lr * Pred Grad: 0.026, New P: 1.604
iter 14 loss: 0.006
Actual params: [1.0291, 1.6036]
-Original Grad: -0.009, -lr * Pred Grad: -0.114, New P: 0.915
-Original Grad: -0.023, -lr * Pred Grad: -0.062, New P: 1.542
iter 15 loss: 0.005
Actual params: [0.9147, 1.542 ]
-Original Grad: 0.025, -lr * Pred Grad: -0.022, New P: 0.892
-Original Grad: 0.086, -lr * Pred Grad: 0.191, New P: 1.733
iter 16 loss: 0.009
Actual params: [0.8924, 1.7333]
-Original Grad: -0.004, -lr * Pred Grad: -0.054, New P: 0.839
-Original Grad: -0.102, -lr * Pred Grad: -0.203, New P: 1.530
iter 17 loss: 0.007
Actual params: [0.8385, 1.53  ]
-Original Grad: 0.051, -lr * Pred Grad: 0.096, New P: 0.935
-Original Grad: 0.063, -lr * Pred Grad: 0.055, New P: 1.585
iter 18 loss: 0.005
Actual params: [0.9348, 1.5852]
-Original Grad: 0.011, -lr * Pred Grad: 0.057, New P: 0.991
-Original Grad: 0.005, -lr * Pred Grad: 0.001, New P: 1.586
iter 19 loss: 0.005
Actual params: [0.9914, 1.5858]
-Original Grad: -0.009, -lr * Pred Grad: 0.000, New P: 0.992
-Original Grad: -0.006, -lr * Pred Grad: -0.025, New P: 1.560
iter 20 loss: 0.005
Actual params: [0.9918, 1.5604]
-Original Grad: 0.017, -lr * Pred Grad: 0.047, New P: 1.038
-Original Grad: 0.034, -lr * Pred Grad: 0.056, New P: 1.617
iter 21 loss: 0.006
Actual params: [1.0384, 1.6166]
-Original Grad: -0.009, -lr * Pred Grad: -0.015, New P: 1.023
-Original Grad: -0.047, -lr * Pred Grad: -0.110, New P: 1.507
iter 22 loss: 0.006
Actual params: [1.0231, 1.5068]
-Original Grad: -0.004, -lr * Pred Grad: -0.026, New P: 0.997
-Original Grad: 0.051, -lr * Pred Grad: 0.065, New P: 1.572
iter 23 loss: 0.005
Actual params: [0.9974, 1.5719]
-Original Grad: -0.008, -lr * Pred Grad: -0.052, New P: 0.946
-Original Grad: -0.002, -lr * Pred Grad: -0.008, New P: 1.564
iter 24 loss: 0.005
Actual params: [0.9456, 1.5638]
-Original Grad: 0.011, -lr * Pred Grad: -0.013, New P: 0.933
-Original Grad: 0.026, -lr * Pred Grad: 0.051, New P: 1.615
iter 25 loss: 0.005
Actual params: [0.9326, 1.6147]
-Original Grad: 0.016, -lr * Pred Grad: 0.017, New P: 0.950
-Original Grad: 0.013, -lr * Pred Grad: 0.032, New P: 1.647
iter 26 loss: 0.006
Actual params: [0.9497, 1.6466]
-Original Grad: -0.007, -lr * Pred Grad: -0.027, New P: 0.922
-Original Grad: -0.040, -lr * Pred Grad: -0.098, New P: 1.549
iter 27 loss: 0.005
Actual params: [0.9224, 1.5489]
-Original Grad: 0.019, -lr * Pred Grad: 0.028, New P: 0.951
-Original Grad: 0.057, -lr * Pred Grad: 0.084, New P: 1.632
iter 28 loss: 0.006
Actual params: [0.9508, 1.6324]
-Original Grad: 0.004, -lr * Pred Grad: 0.008, New P: 0.959
-Original Grad: -0.002, -lr * Pred Grad: -0.001, New P: 1.631
iter 29 loss: 0.006
Actual params: [0.9589, 1.6311]
-Original Grad: 0.021, -lr * Pred Grad: 0.055, New P: 1.014
-Original Grad: -0.000, -lr * Pred Grad: -0.013, New P: 1.618
iter 30 loss: 0.006
Actual params: [1.0139, 1.6182]
-Original Grad: -0.005, -lr * Pred Grad: 0.000, New P: 1.014
-Original Grad: -0.081, -lr * Pred Grad: -0.188, New P: 1.431
Target params: [1.3344, 1.5708]
iter 0 loss: 0.185
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.050, -lr * Pred Grad: 1.120, New P: 0.647
-Original Grad: 0.038, -lr * Pred Grad: 0.811, New P: 0.815
iter 1 loss: 0.036
Actual params: [0.6475, 0.8148]
-Original Grad: -0.028, -lr * Pred Grad: -0.017, New P: 0.630
-Original Grad: 0.149, -lr * Pred Grad: 1.002, New P: 1.817
iter 2 loss: 0.038
Actual params: [0.6302, 1.8172]
-Original Grad: 0.137, -lr * Pred Grad: 1.198, New P: 1.828
-Original Grad: -0.235, -lr * Pred Grad: -0.815, New P: 1.002
iter 3 loss: 0.021
Actual params: [1.8279, 1.0019]
-Original Grad: 0.085, -lr * Pred Grad: 0.573, New P: 2.401
-Original Grad: 0.250, -lr * Pred Grad: 0.358, New P: 1.360
iter 4 loss: 0.017
Actual params: [2.4007, 1.3598]
-Original Grad: -0.016, -lr * Pred Grad: 0.255, New P: 2.656
-Original Grad: -0.096, -lr * Pred Grad: -0.198, New P: 1.162
iter 5 loss: 0.013
Actual params: [2.6556, 1.1617]
-Original Grad: -0.020, -lr * Pred Grad: 0.096, New P: 2.752
-Original Grad: -0.071, -lr * Pred Grad: -0.158, New P: 1.004
iter 6 loss: 0.010
Actual params: [2.7519, 1.0036]
-Original Grad: -0.019, -lr * Pred Grad: 0.017, New P: 2.769
-Original Grad: -0.035, -lr * Pred Grad: -0.191, New P: 0.813
iter 7 loss: 0.008
Actual params: [2.7688, 0.813 ]
-Original Grad: -0.009, -lr * Pred Grad: -0.002, New P: 2.767
-Original Grad: -0.006, -lr * Pred Grad: -0.112, New P: 0.701
iter 8 loss: 0.007
Actual params: [2.7671, 0.7008]
-Original Grad: -0.014, -lr * Pred Grad: -0.040, New P: 2.727
-Original Grad: 0.007, -lr * Pred Grad: -0.058, New P: 0.643
iter 9 loss: 0.007
Actual params: [2.7271, 0.6428]
-Original Grad: 0.008, -lr * Pred Grad: -0.007, New P: 2.720
-Original Grad: 0.019, -lr * Pred Grad: 0.001, New P: 0.644
iter 10 loss: 0.007
Actual params: [2.7204, 0.6437]
-Original Grad: -0.004, -lr * Pred Grad: -0.030, New P: 2.690
-Original Grad: 0.014, -lr * Pred Grad: 0.013, New P: 0.656
iter 11 loss: 0.007
Actual params: [2.6905, 0.6564]
-Original Grad: 0.008, -lr * Pred Grad: -0.006, New P: 2.684
-Original Grad: 0.021, -lr * Pred Grad: 0.039, New P: 0.695
iter 12 loss: 0.007
Actual params: [2.6843, 0.6952]
-Original Grad: -0.000, -lr * Pred Grad: -0.020, New P: 2.664
-Original Grad: 0.011, -lr * Pred Grad: 0.024, New P: 0.719
iter 13 loss: 0.007
Actual params: [2.664 , 0.7188]
-Original Grad: -0.001, -lr * Pred Grad: -0.027, New P: 2.637
-Original Grad: 0.009, -lr * Pred Grad: 0.015, New P: 0.734
iter 14 loss: 0.007
Actual params: [2.6369, 0.7337]
-Original Grad: 0.008, -lr * Pred Grad: -0.007, New P: 2.630
-Original Grad: 0.033, -lr * Pred Grad: 0.076, New P: 0.809
iter 15 loss: 0.007
Actual params: [2.6296, 0.8095]
-Original Grad: -0.000, -lr * Pred Grad: -0.022, New P: 2.608
-Original Grad: 0.007, -lr * Pred Grad: 0.024, New P: 0.834
iter 16 loss: 0.006
Actual params: [2.6079, 0.8337]
-Original Grad: 0.004, -lr * Pred Grad: -0.015, New P: 2.592
-Original Grad: 0.022, -lr * Pred Grad: 0.052, New P: 0.886
iter 17 loss: 0.006
Actual params: [2.5925, 0.8858]
-Original Grad: -0.003, -lr * Pred Grad: -0.032, New P: 2.560
-Original Grad: -0.005, -lr * Pred Grad: -0.014, New P: 0.872
iter 18 loss: 0.006
Actual params: [2.5604, 0.872 ]
-Original Grad: 0.002, -lr * Pred Grad: -0.028, New P: 2.533
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: 0.873
iter 19 loss: 0.006
Actual params: [2.5327, 0.8733]
-Original Grad: -0.003, -lr * Pred Grad: -0.042, New P: 2.490
-Original Grad: -0.006, -lr * Pred Grad: -0.035, New P: 0.839
iter 20 loss: 0.007
Actual params: [2.4904, 0.8387]
-Original Grad: 0.010, -lr * Pred Grad: -0.012, New P: 2.479
-Original Grad: 0.046, -lr * Pred Grad: 0.090, New P: 0.928
iter 21 loss: 0.006
Actual params: [2.4786, 0.9283]
-Original Grad: -0.003, -lr * Pred Grad: -0.033, New P: 2.445
-Original Grad: -0.003, -lr * Pred Grad: 0.002, New P: 0.930
iter 22 loss: 0.006
Actual params: [2.4451, 0.9299]
-Original Grad: 0.002, -lr * Pred Grad: -0.028, New P: 2.417
-Original Grad: 0.012, -lr * Pred Grad: 0.019, New P: 0.949
iter 23 loss: 0.006
Actual params: [2.4173, 0.9491]
-Original Grad: 0.004, -lr * Pred Grad: -0.021, New P: 2.396
-Original Grad: 0.010, -lr * Pred Grad: 0.013, New P: 0.962
iter 24 loss: 0.006
Actual params: [2.3964, 0.9619]
-Original Grad: 0.005, -lr * Pred Grad: -0.014, New P: 2.383
-Original Grad: 0.012, -lr * Pred Grad: 0.020, New P: 0.981
iter 25 loss: 0.006
Actual params: [2.3825, 0.9814]
-Original Grad: 0.001, -lr * Pred Grad: -0.020, New P: 2.362
-Original Grad: -0.005, -lr * Pred Grad: -0.022, New P: 0.959
iter 26 loss: 0.006
Actual params: [2.3621, 0.9591]
-Original Grad: 0.001, -lr * Pred Grad: -0.024, New P: 2.338
-Original Grad: -0.007, -lr * Pred Grad: -0.040, New P: 0.919
iter 27 loss: 0.007
Actual params: [2.3377, 0.9191]
-Original Grad: 0.006, -lr * Pred Grad: -0.013, New P: 2.325
-Original Grad: 0.006, -lr * Pred Grad: -0.020, New P: 0.900
iter 28 loss: 0.007
Actual params: [2.3245, 0.8996]
-Original Grad: 0.010, -lr * Pred Grad: 0.005, New P: 2.330
-Original Grad: 0.022, -lr * Pred Grad: 0.027, New P: 0.926
iter 29 loss: 0.007
Actual params: [2.3297, 0.9264]
-Original Grad: 0.009, -lr * Pred Grad: 0.012, New P: 2.341
-Original Grad: 0.017, -lr * Pred Grad: 0.035, New P: 0.961
iter 30 loss: 0.007
Actual params: [2.3414, 0.961 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.011, New P: 2.330
-Original Grad: -0.013, -lr * Pred Grad: -0.037, New P: 0.924
