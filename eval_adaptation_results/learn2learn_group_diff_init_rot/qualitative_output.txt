Target params: [-4.5633, -1.0472]
Actual params: [1.084 , 0.5507]
-Original Grad: 0.008, -lr * Pred Grad: 0.723, New P: 1.807
-Original Grad: -0.516, -lr * Pred Grad: -0.543, New P: 0.007
iter 0 loss: 0.133
Actual params: [1.8075, 0.0073]
-Original Grad: -0.002, -lr * Pred Grad: 0.315, New P: 2.123
-Original Grad: -0.205, -lr * Pred Grad: -0.460, New P: -0.452
iter 1 loss: 0.036
Actual params: [ 2.1228, -0.4525]
-Original Grad: 0.003, -lr * Pred Grad: 0.266, New P: 2.389
-Original Grad: 0.029, -lr * Pred Grad: -0.292, New P: -0.745
iter 2 loss: 0.016
Actual params: [ 2.3887, -0.7449]
-Original Grad: 0.017, -lr * Pred Grad: 0.240, New P: 2.629
-Original Grad: 0.179, -lr * Pred Grad: 0.258, New P: -0.487
iter 3 loss: 0.029
Actual params: [ 2.6288, -0.487 ]
-Original Grad: 0.009, -lr * Pred Grad: 0.156, New P: 2.785
-Original Grad: 0.006, -lr * Pred Grad: 0.007, New P: -0.480
iter 4 loss: 0.013
Actual params: [ 2.7846, -0.4801]
-Original Grad: -0.003, -lr * Pred Grad: 0.075, New P: 2.860
-Original Grad: -0.046, -lr * Pred Grad: -0.115, New P: -0.595
iter 5 loss: 0.012
Actual params: [ 2.8601, -0.5947]
-Original Grad: 0.006, -lr * Pred Grad: 0.070, New P: 2.930
-Original Grad: 0.004, -lr * Pred Grad: -0.070, New P: -0.665
iter 6 loss: 0.011
Actual params: [ 2.9302, -0.6649]
-Original Grad: 0.011, -lr * Pred Grad: 0.072, New P: 3.002
-Original Grad: 0.028, -lr * Pred Grad: 0.011, New P: -0.654
iter 7 loss: 0.010
Actual params: [ 3.0025, -0.6543]
-Original Grad: 0.001, -lr * Pred Grad: 0.036, New P: 3.038
-Original Grad: -0.006, -lr * Pred Grad: -0.035, New P: -0.689
iter 8 loss: 0.010
Actual params: [ 3.0385, -0.689 ]
-Original Grad: 0.003, -lr * Pred Grad: 0.027, New P: 3.066
-Original Grad: 0.006, -lr * Pred Grad: -0.018, New P: -0.707
iter 9 loss: 0.010
Actual params: [ 3.0656, -0.707 ]
-Original Grad: 0.003, -lr * Pred Grad: 0.018, New P: 3.083
-Original Grad: 0.008, -lr * Pred Grad: -0.010, New P: -0.717
iter 10 loss: 0.010
Actual params: [ 3.0831, -0.7168]
-Original Grad: 0.002, -lr * Pred Grad: 0.007, New P: 3.090
-Original Grad: 0.008, -lr * Pred Grad: -0.005, New P: -0.722
iter 11 loss: 0.010
Actual params: [ 3.0898, -0.7223]
-Original Grad: 0.002, -lr * Pred Grad: -0.000, New P: 3.090
-Original Grad: 0.008, -lr * Pred Grad: -0.001, New P: -0.724
iter 12 loss: 0.010
Actual params: [ 3.0896, -0.7235]
-Original Grad: 0.002, -lr * Pred Grad: -0.005, New P: 3.085
-Original Grad: 0.009, -lr * Pred Grad: 0.003, New P: -0.720
iter 13 loss: 0.010
Actual params: [ 3.0847, -0.7205]
-Original Grad: 0.002, -lr * Pred Grad: -0.009, New P: 3.076
-Original Grad: 0.009, -lr * Pred Grad: 0.005, New P: -0.716
iter 14 loss: 0.010
Actual params: [ 3.0761, -0.7156]
-Original Grad: 0.003, -lr * Pred Grad: -0.010, New P: 3.066
-Original Grad: 0.009, -lr * Pred Grad: 0.007, New P: -0.709
iter 15 loss: 0.010
Actual params: [ 3.0657, -0.7091]
-Original Grad: 0.004, -lr * Pred Grad: -0.010, New P: 3.056
-Original Grad: 0.009, -lr * Pred Grad: 0.007, New P: -0.702
iter 16 loss: 0.010
Actual params: [ 3.056 , -0.7019]
-Original Grad: 0.003, -lr * Pred Grad: -0.011, New P: 3.046
-Original Grad: 0.008, -lr * Pred Grad: 0.004, New P: -0.698
iter 17 loss: 0.010
Actual params: [ 3.0455, -0.6978]
-Original Grad: 0.004, -lr * Pred Grad: -0.009, New P: 3.036
-Original Grad: 0.009, -lr * Pred Grad: 0.006, New P: -0.691
iter 18 loss: 0.010
Actual params: [ 3.0364, -0.6915]
-Original Grad: 0.004, -lr * Pred Grad: -0.008, New P: 3.028
-Original Grad: 0.008, -lr * Pred Grad: 0.005, New P: -0.687
iter 19 loss: 0.010
Actual params: [ 3.028 , -0.6868]
-Original Grad: 0.004, -lr * Pred Grad: -0.007, New P: 3.021
-Original Grad: 0.008, -lr * Pred Grad: 0.004, New P: -0.683
iter 20 loss: 0.010
Actual params: [ 3.0207, -0.6829]
-Original Grad: 0.004, -lr * Pred Grad: -0.007, New P: 3.014
-Original Grad: 0.008, -lr * Pred Grad: 0.003, New P: -0.680
iter 21 loss: 0.010
Actual params: [ 3.0139, -0.6796]
-Original Grad: 0.004, -lr * Pred Grad: -0.007, New P: 3.007
-Original Grad: 0.008, -lr * Pred Grad: 0.003, New P: -0.677
iter 22 loss: 0.010
Actual params: [ 3.007, -0.677]
-Original Grad: 0.004, -lr * Pred Grad: -0.006, New P: 3.001
-Original Grad: 0.009, -lr * Pred Grad: 0.004, New P: -0.673
iter 23 loss: 0.010
Actual params: [ 3.0011, -0.6728]
-Original Grad: 0.004, -lr * Pred Grad: -0.007, New P: 2.994
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: -0.671
iter 24 loss: 0.010
Actual params: [ 2.994 , -0.6715]
-Original Grad: 0.003, -lr * Pred Grad: -0.008, New P: 2.986
-Original Grad: 0.009, -lr * Pred Grad: 0.004, New P: -0.667
iter 25 loss: 0.010
Actual params: [ 2.9858, -0.6674]
-Original Grad: 0.005, -lr * Pred Grad: -0.004, New P: 2.981
-Original Grad: 0.009, -lr * Pred Grad: 0.005, New P: -0.662
iter 26 loss: 0.010
Actual params: [ 2.9813, -0.662 ]
-Original Grad: 0.005, -lr * Pred Grad: -0.003, New P: 2.978
-Original Grad: 0.006, -lr * Pred Grad: -0.000, New P: -0.662
iter 27 loss: 0.010
Actual params: [ 2.9779, -0.6624]
-Original Grad: 0.006, -lr * Pred Grad: -0.000, New P: 2.978
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -0.661
iter 28 loss: 0.010
Actual params: [ 2.9777, -0.6608]
-Original Grad: 0.005, -lr * Pred Grad: 0.001, New P: 2.978
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -0.661
iter 29 loss: 0.010
Actual params: [ 2.9784, -0.6613]
-Original Grad: 0.005, -lr * Pred Grad: 0.002, New P: 2.980
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -0.662
iter 30 loss: 0.010
Actual params: [ 2.9802, -0.6622]
Target params: [-4.5633, -1.0472]
Actual params: [-1.0585,  1.4018]
-Original Grad: -0.003, -lr * Pred Grad: 0.614, New P: -0.444
-Original Grad: -0.734, -lr * Pred Grad: -0.542, New P: 0.859
iter 0 loss: 0.680
Actual params: [-0.444 ,  0.8594]
-Original Grad: -0.002, -lr * Pred Grad: 0.310, New P: -0.134
-Original Grad: -0.747, -lr * Pred Grad: -0.562, New P: 0.298
iter 1 loss: 0.478
Actual params: [-0.1339,  0.2979]
-Original Grad: -0.005, -lr * Pred Grad: 0.176, New P: 0.042
-Original Grad: -0.649, -lr * Pred Grad: -0.630, New P: -0.333
iter 2 loss: 0.281
Actual params: [ 0.042 , -0.3326]
-Original Grad: 0.002, -lr * Pred Grad: 0.125, New P: 0.167
-Original Grad: -0.426, -lr * Pred Grad: -0.648, New P: -0.980
iter 3 loss: 0.108
Actual params: [ 0.1671, -0.9804]
-Original Grad: 0.009, -lr * Pred Grad: 0.114, New P: 0.281
-Original Grad: -0.105, -lr * Pred Grad: -0.624, New P: -1.604
iter 4 loss: 0.025
Actual params: [ 0.2811, -1.6042]
-Original Grad: -0.010, -lr * Pred Grad: 0.012, New P: 0.294
-Original Grad: 0.306, -lr * Pred Grad: -0.087, New P: -1.691
iter 5 loss: 0.040
Actual params: [ 0.2935, -1.6909]
-Original Grad: -0.013, -lr * Pred Grad: -0.035, New P: 0.259
-Original Grad: 0.421, -lr * Pred Grad: 0.987, New P: -0.703
iter 6 loss: 0.056
Actual params: [ 0.2588, -0.7034]
-Original Grad: 0.010, -lr * Pred Grad: 0.010, New P: 0.269
-Original Grad: -0.228, -lr * Pred Grad: -0.541, New P: -1.244
iter 7 loss: 0.047
Actual params: [ 0.269 , -1.2444]
-Original Grad: 0.005, -lr * Pred Grad: 0.006, New P: 0.275
-Original Grad: 0.014, -lr * Pred Grad: -0.078, New P: -1.323
iter 8 loss: 0.018
Actual params: [ 0.2748, -1.3228]
-Original Grad: 0.003, -lr * Pred Grad: 0.000, New P: 0.275
-Original Grad: 0.053, -lr * Pred Grad: -0.051, New P: -1.374
iter 9 loss: 0.019
Actual params: [ 0.2749, -1.3738]
-Original Grad: 0.001, -lr * Pred Grad: -0.009, New P: 0.266
-Original Grad: 0.080, -lr * Pred Grad: 0.191, New P: -1.183
iter 10 loss: 0.020
Actual params: [ 0.2658, -1.1833]
-Original Grad: 0.007, -lr * Pred Grad: 0.004, New P: 0.270
-Original Grad: -0.016, -lr * Pred Grad: -0.011, New P: -1.194
iter 11 loss: 0.018
Actual params: [ 0.2696, -1.1944]
-Original Grad: 0.007, -lr * Pred Grad: 0.007, New P: 0.277
-Original Grad: -0.010, -lr * Pred Grad: -0.034, New P: -1.229
iter 12 loss: 0.017
Actual params: [ 0.2771, -1.2289]
-Original Grad: 0.005, -lr * Pred Grad: 0.007, New P: 0.284
-Original Grad: 0.007, -lr * Pred Grad: -0.021, New P: -1.250
iter 13 loss: 0.017
Actual params: [ 0.2841, -1.2499]
-Original Grad: 0.005, -lr * Pred Grad: 0.005, New P: 0.289
-Original Grad: 0.017, -lr * Pred Grad: 0.015, New P: -1.235
iter 14 loss: 0.017
Actual params: [ 0.2895, -1.235 ]
-Original Grad: 0.005, -lr * Pred Grad: 0.006, New P: 0.296
-Original Grad: 0.011, -lr * Pred Grad: 0.011, New P: -1.224
iter 15 loss: 0.017
Actual params: [ 0.2955, -1.2239]
-Original Grad: 0.006, -lr * Pred Grad: 0.007, New P: 0.303
-Original Grad: 0.005, -lr * Pred Grad: -0.002, New P: -1.225
iter 16 loss: 0.017
Actual params: [ 0.3026, -1.2254]
-Original Grad: 0.006, -lr * Pred Grad: 0.008, New P: 0.310
-Original Grad: 0.006, -lr * Pred Grad: -0.004, New P: -1.229
iter 17 loss: 0.017
Actual params: [ 0.3101, -1.2292]
-Original Grad: 0.006, -lr * Pred Grad: 0.007, New P: 0.317
-Original Grad: 0.009, -lr * Pred Grad: 0.001, New P: -1.228
iter 18 loss: 0.017
Actual params: [ 0.3175, -1.2278]
-Original Grad: 0.006, -lr * Pred Grad: 0.008, New P: 0.325
-Original Grad: 0.009, -lr * Pred Grad: 0.003, New P: -1.224
iter 19 loss: 0.017
Actual params: [ 0.3251, -1.2243]
-Original Grad: 0.006, -lr * Pred Grad: 0.008, New P: 0.334
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: -1.224
iter 20 loss: 0.017
Actual params: [ 0.3335, -1.2236]
-Original Grad: 0.006, -lr * Pred Grad: 0.009, New P: 0.343
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: -1.223
iter 21 loss: 0.017
Actual params: [ 0.3428, -1.2233]
-Original Grad: 0.006, -lr * Pred Grad: 0.010, New P: 0.353
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.222
iter 22 loss: 0.017
Actual params: [ 0.3528, -1.2222]
-Original Grad: 0.006, -lr * Pred Grad: 0.011, New P: 0.364
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -1.221
iter 23 loss: 0.017
Actual params: [ 0.3635, -1.2205]
-Original Grad: 0.006, -lr * Pred Grad: 0.011, New P: 0.375
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -1.219
iter 24 loss: 0.017
Actual params: [ 0.3746, -1.2189]
-Original Grad: 0.006, -lr * Pred Grad: 0.012, New P: 0.386
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.218
iter 25 loss: 0.017
Actual params: [ 0.3862, -1.2176]
-Original Grad: 0.006, -lr * Pred Grad: 0.013, New P: 0.399
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.216
iter 26 loss: 0.017
Actual params: [ 0.399 , -1.2163]
-Original Grad: 0.006, -lr * Pred Grad: 0.013, New P: 0.412
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -1.214
iter 27 loss: 0.017
Actual params: [ 0.4123, -1.2142]
-Original Grad: 0.006, -lr * Pred Grad: 0.014, New P: 0.426
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -1.213
iter 28 loss: 0.017
Actual params: [ 0.4259, -1.2126]
-Original Grad: 0.006, -lr * Pred Grad: 0.014, New P: 0.440
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -1.211
iter 29 loss: 0.016
Actual params: [ 0.4395, -1.2108]
-Original Grad: 0.006, -lr * Pred Grad: 0.013, New P: 0.453
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -1.209
iter 30 loss: 0.016
Actual params: [ 0.4527, -1.2092]
Target params: [-4.5633, -1.0472]
Actual params: [1.5477, 0.5327]
-Original Grad: 0.018, -lr * Pred Grad: 0.830, New P: 2.378
-Original Grad: -0.765, -lr * Pred Grad: -0.540, New P: -0.007
iter 0 loss: 0.455
Actual params: [ 2.3779, -0.0069]
-Original Grad: 0.025, -lr * Pred Grad: 0.589, New P: 2.966
-Original Grad: -0.502, -lr * Pred Grad: -0.559, New P: -0.566
iter 1 loss: 0.260
Actual params: [ 2.9664, -0.5657]
-Original Grad: -0.007, -lr * Pred Grad: 0.217, New P: 3.183
-Original Grad: -0.300, -lr * Pred Grad: -0.615, New P: -1.181
iter 2 loss: 0.137
Actual params: [ 3.183 , -1.1812]
-Original Grad: -0.021, -lr * Pred Grad: 0.025, New P: 3.208
-Original Grad: -0.085, -lr * Pred Grad: -0.548, New P: -1.729
iter 3 loss: 0.083
Actual params: [ 3.2078, -1.7291]
-Original Grad: -0.011, -lr * Pred Grad: -0.001, New P: 3.207
-Original Grad: 0.044, -lr * Pred Grad: -0.337, New P: -2.066
iter 4 loss: 0.079
Actual params: [ 3.2072, -2.0658]
-Original Grad: 0.026, -lr * Pred Grad: 0.106, New P: 3.313
-Original Grad: 0.112, -lr * Pred Grad: -0.033, New P: -2.099
iter 5 loss: 0.094
Actual params: [ 3.3129, -2.0987]
-Original Grad: 0.013, -lr * Pred Grad: 0.079, New P: 3.392
-Original Grad: 0.082, -lr * Pred Grad: 0.091, New P: -2.007
iter 6 loss: 0.093
Actual params: [ 3.3924, -2.0074]
-Original Grad: -0.017, -lr * Pred Grad: -0.014, New P: 3.378
-Original Grad: 0.056, -lr * Pred Grad: 0.124, New P: -1.884
iter 7 loss: 0.090
Actual params: [ 3.378 , -1.8838]
-Original Grad: -0.025, -lr * Pred Grad: -0.083, New P: 3.295
-Original Grad: 0.048, -lr * Pred Grad: 0.149, New P: -1.735
iter 8 loss: 0.087
Actual params: [ 3.2952, -1.7346]
-Original Grad: -0.025, -lr * Pred Grad: -0.129, New P: 3.166
-Original Grad: 0.042, -lr * Pred Grad: 0.154, New P: -1.581
iter 9 loss: 0.081
Actual params: [ 3.1662, -1.5808]
-Original Grad: -0.008, -lr * Pred Grad: -0.115, New P: 3.052
-Original Grad: 0.016, -lr * Pred Grad: 0.073, New P: -1.508
iter 10 loss: 0.076
Actual params: [ 3.0517, -1.508 ]
-Original Grad: -0.003, -lr * Pred Grad: -0.100, New P: 2.952
-Original Grad: 0.009, -lr * Pred Grad: 0.034, New P: -1.474
iter 11 loss: 0.075
Actual params: [ 2.9515, -1.4741]
-Original Grad: -0.001, -lr * Pred Grad: -0.086, New P: 2.865
-Original Grad: 0.015, -lr * Pred Grad: 0.035, New P: -1.439
iter 12 loss: 0.075
Actual params: [ 2.8651, -1.4394]
-Original Grad: 0.001, -lr * Pred Grad: -0.074, New P: 2.791
-Original Grad: 0.021, -lr * Pred Grad: 0.047, New P: -1.392
iter 13 loss: 0.075
Actual params: [ 2.7914, -1.392 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.069, New P: 2.723
-Original Grad: 0.017, -lr * Pred Grad: 0.043, New P: -1.349
iter 14 loss: 0.074
Actual params: [ 2.7228, -1.3487]
-Original Grad: -0.003, -lr * Pred Grad: -0.070, New P: 2.652
-Original Grad: 0.009, -lr * Pred Grad: 0.021, New P: -1.328
iter 15 loss: 0.074
Actual params: [ 2.6524, -1.3278]
-Original Grad: -0.005, -lr * Pred Grad: -0.076, New P: 2.576
-Original Grad: 0.010, -lr * Pred Grad: 0.015, New P: -1.313
iter 16 loss: 0.074
Actual params: [ 2.5762, -1.3127]
-Original Grad: -0.005, -lr * Pred Grad: -0.080, New P: 2.496
-Original Grad: 0.009, -lr * Pred Grad: 0.011, New P: -1.301
iter 17 loss: 0.073
Actual params: [ 2.4958, -1.3014]
-Original Grad: -0.007, -lr * Pred Grad: -0.089, New P: 2.407
-Original Grad: 0.012, -lr * Pred Grad: 0.016, New P: -1.286
iter 18 loss: 0.073
Actual params: [ 2.4073, -1.2857]
-Original Grad: -0.008, -lr * Pred Grad: -0.097, New P: 2.310
-Original Grad: 0.009, -lr * Pred Grad: 0.010, New P: -1.276
iter 19 loss: 0.072
Actual params: [ 2.3098, -1.2756]
-Original Grad: -0.012, -lr * Pred Grad: -0.115, New P: 2.195
-Original Grad: 0.011, -lr * Pred Grad: 0.015, New P: -1.261
iter 20 loss: 0.071
Actual params: [ 2.1947, -1.2608]
-Original Grad: -0.016, -lr * Pred Grad: -0.139, New P: 2.056
-Original Grad: 0.008, -lr * Pred Grad: 0.008, New P: -1.253
iter 21 loss: 0.069
Actual params: [ 2.0559, -1.2529]
-Original Grad: -0.017, -lr * Pred Grad: -0.158, New P: 1.898
-Original Grad: 0.002, -lr * Pred Grad: -0.011, New P: -1.264
iter 22 loss: 0.067
Actual params: [ 1.8979, -1.2635]
-Original Grad: -0.020, -lr * Pred Grad: -0.182, New P: 1.716
-Original Grad: 0.004, -lr * Pred Grad: -0.011, New P: -1.274
iter 23 loss: 0.064
Actual params: [ 1.7162, -1.2744]
-Original Grad: -0.021, -lr * Pred Grad: -0.201, New P: 1.515
-Original Grad: 0.008, -lr * Pred Grad: -0.003, New P: -1.278
iter 24 loss: 0.060
Actual params: [ 1.5153, -1.2779]
-Original Grad: -0.021, -lr * Pred Grad: -0.216, New P: 1.300
-Original Grad: 0.008, -lr * Pred Grad: -0.001, New P: -1.279
iter 25 loss: 0.056
Actual params: [ 1.2997, -1.279 ]
-Original Grad: -0.019, -lr * Pred Grad: -0.224, New P: 1.076
-Original Grad: 0.005, -lr * Pred Grad: -0.006, New P: -1.285
iter 26 loss: 0.051
Actual params: [ 1.0761, -1.2845]
-Original Grad: -0.018, -lr * Pred Grad: -0.228, New P: 0.849
-Original Grad: 0.005, -lr * Pred Grad: -0.007, New P: -1.292
iter 27 loss: 0.047
Actual params: [ 0.8485, -1.2918]
-Original Grad: -0.017, -lr * Pred Grad: -0.229, New P: 0.619
-Original Grad: 0.001, -lr * Pred Grad: -0.019, New P: -1.310
iter 28 loss: 0.043
Actual params: [ 0.6192, -1.3104]
-Original Grad: -0.017, -lr * Pred Grad: -0.232, New P: 0.388
-Original Grad: -0.001, -lr * Pred Grad: -0.029, New P: -1.339
iter 29 loss: 0.039
Actual params: [ 0.3875, -1.3389]
-Original Grad: -0.017, -lr * Pred Grad: -0.234, New P: 0.154
-Original Grad: -0.011, -lr * Pred Grad: -0.055, New P: -1.394
iter 30 loss: 0.035
Actual params: [ 0.1539, -1.3944]
Target params: [-4.5633, -1.0472]
Actual params: [0.0029, 0.9353]
-Original Grad: 0.008, -lr * Pred Grad: 0.725, New P: 0.727
-Original Grad: -0.126, -lr * Pred Grad: 0.076, New P: 1.011
iter 0 loss: 1.003
Actual params: [0.7274, 1.0114]
-Original Grad: 0.019, -lr * Pred Grad: 0.528, New P: 1.255
-Original Grad: -0.083, -lr * Pred Grad: -0.186, New P: 0.825
iter 1 loss: 0.997
Actual params: [1.2553, 0.8249]
-Original Grad: 0.040, -lr * Pred Grad: 0.605, New P: 1.860
-Original Grad: -0.267, -lr * Pred Grad: -0.408, New P: 0.417
iter 2 loss: 0.967
Actual params: [1.86  , 0.4166]
-Original Grad: 0.045, -lr * Pred Grad: 0.503, New P: 2.363
-Original Grad: -0.697, -lr * Pred Grad: -0.537, New P: -0.120
iter 3 loss: 0.836
Actual params: [ 2.3626, -0.1201]
-Original Grad: 0.030, -lr * Pred Grad: 0.369, New P: 2.731
-Original Grad: -1.446, -lr * Pred Grad: -0.611, New P: -0.731
iter 4 loss: 0.554
Actual params: [ 2.7314, -0.7313]
-Original Grad: 0.099, -lr * Pred Grad: 0.604, New P: 3.336
-Original Grad: -1.262, -lr * Pred Grad: -0.639, New P: -1.370
iter 5 loss: 0.186
Actual params: [ 3.3356, -1.3699]
-Original Grad: -0.477, -lr * Pred Grad: -1.067, New P: 2.269
-Original Grad: 0.431, -lr * Pred Grad: -0.550, New P: -1.920
iter 6 loss: 0.179
Actual params: [ 2.2686, -1.9196]
-Original Grad: -0.037, -lr * Pred Grad: -0.694, New P: 1.575
-Original Grad: 0.366, -lr * Pred Grad: 0.014, New P: -1.906
iter 7 loss: 0.104
Actual params: [ 1.5747, -1.9059]
-Original Grad: -0.002, -lr * Pred Grad: -0.591, New P: 0.984
-Original Grad: 0.317, -lr * Pred Grad: 0.565, New P: -1.340
iter 8 loss: 0.093
Actual params: [ 0.9841, -1.3405]
-Original Grad: -0.013, -lr * Pred Grad: -0.517, New P: 0.467
-Original Grad: -0.091, -lr * Pred Grad: -0.261, New P: -1.601
iter 9 loss: 0.058
Actual params: [ 0.467, -1.601]
-Original Grad: 0.015, -lr * Pred Grad: -0.419, New P: 0.048
-Original Grad: 0.124, -lr * Pred Grad: 0.159, New P: -1.442
iter 10 loss: 0.066
Actual params: [ 0.0478, -1.4418]
-Original Grad: 0.013, -lr * Pred Grad: -0.341, New P: -0.293
-Original Grad: 0.124, -lr * Pred Grad: 0.416, New P: -1.026
iter 11 loss: 0.063
Actual params: [-0.2928, -1.0258]
-Original Grad: -0.017, -lr * Pred Grad: -0.353, New P: -0.646
-Original Grad: -0.363, -lr * Pred Grad: -0.430, New P: -1.456
iter 12 loss: 0.064
Actual params: [-0.6455, -1.4556]
-Original Grad: 0.009, -lr * Pred Grad: -0.276, New P: -0.921
-Original Grad: 0.160, -lr * Pred Grad: -0.006, New P: -1.462
iter 13 loss: 0.072
Actual params: [-0.9214, -1.4616]
-Original Grad: 0.016, -lr * Pred Grad: -0.191, New P: -1.112
-Original Grad: 0.117, -lr * Pred Grad: 0.270, New P: -1.192
iter 14 loss: 0.076
Actual params: [-1.1125, -1.192 ]
-Original Grad: 0.033, -lr * Pred Grad: -0.075, New P: -1.187
-Original Grad: 0.276, -lr * Pred Grad: 1.204, New P: 0.012
iter 15 loss: 0.062
Actual params: [-1.1874,  0.0116]
-Original Grad: 0.002, -lr * Pred Grad: -0.081, New P: -1.269
-Original Grad: -1.564, -lr * Pred Grad: -0.559, New P: -0.548
iter 16 loss: 0.728
Actual params: [-1.2686, -0.5478]
-Original Grad: 0.005, -lr * Pred Grad: -0.060, New P: -1.329
-Original Grad: -1.446, -lr * Pred Grad: -0.502, New P: -1.050
iter 17 loss: 0.382
Actual params: [-1.3288, -1.0495]
-Original Grad: 0.010, -lr * Pred Grad: -0.028, New P: -1.356
-Original Grad: 0.027, -lr * Pred Grad: -0.589, New P: -1.638
iter 18 loss: 0.053
Actual params: [-1.3564, -1.6381]
-Original Grad: 0.008, -lr * Pred Grad: -0.013, New P: -1.369
-Original Grad: 0.203, -lr * Pred Grad: -0.370, New P: -2.008
iter 19 loss: 0.096
Actual params: [-1.369 , -2.0083]
-Original Grad: 0.010, -lr * Pred Grad: 0.004, New P: -1.365
-Original Grad: 0.419, -lr * Pred Grad: 0.535, New P: -1.474
iter 20 loss: 0.150
Actual params: [-1.3648, -1.4737]
-Original Grad: 0.016, -lr * Pred Grad: 0.035, New P: -1.330
-Original Grad: 0.067, -lr * Pred Grad: 0.111, New P: -1.362
iter 21 loss: 0.085
Actual params: [-1.3301, -1.3624]
-Original Grad: 0.023, -lr * Pred Grad: 0.072, New P: -1.258
-Original Grad: 0.009, -lr * Pred Grad: 0.013, New P: -1.350
iter 22 loss: 0.082
Actual params: [-1.2584, -1.3496]
-Original Grad: 0.025, -lr * Pred Grad: 0.100, New P: -1.158
-Original Grad: 0.004, -lr * Pred Grad: -0.016, New P: -1.365
iter 23 loss: 0.080
Actual params: [-1.1581, -1.3652]
-Original Grad: 0.025, -lr * Pred Grad: 0.121, New P: -1.037
-Original Grad: 0.031, -lr * Pred Grad: 0.050, New P: -1.315
iter 24 loss: 0.078
Actual params: [-1.0373, -1.3154]
-Original Grad: 0.031, -lr * Pred Grad: 0.150, New P: -0.887
-Original Grad: 0.117, -lr * Pred Grad: 0.368, New P: -0.947
iter 25 loss: 0.073
Actual params: [-0.887, -0.947]
-Original Grad: -0.020, -lr * Pred Grad: 0.015, New P: -0.872
-Original Grad: -0.578, -lr * Pred Grad: -0.434, New P: -1.381
iter 26 loss: 0.071
Actual params: [-0.8724, -1.3809]
-Original Grad: 0.021, -lr * Pred Grad: 0.079, New P: -0.793
-Original Grad: 0.077, -lr * Pred Grad: -0.287, New P: -1.668
iter 27 loss: 0.072
Actual params: [-0.7935, -1.6679]
-Original Grad: 0.016, -lr * Pred Grad: 0.076, New P: -0.718
-Original Grad: 0.188, -lr * Pred Grad: 0.177, New P: -1.491
iter 28 loss: 0.093
Actual params: [-0.7175, -1.491 ]
-Original Grad: 0.009, -lr * Pred Grad: 0.061, New P: -0.656
-Original Grad: 0.172, -lr * Pred Grad: 0.620, New P: -0.871
iter 29 loss: 0.076
Actual params: [-0.6561, -0.8705]
-Original Grad: -0.024, -lr * Pred Grad: -0.041, New P: -0.697
-Original Grad: -0.982, -lr * Pred Grad: -0.458, New P: -1.328
iter 30 loss: 0.106
Actual params: [-0.6968, -1.3282]
Target params: [-4.5633, -1.0472]
Actual params: [-0.6756, -1.5044]
-Original Grad: 0.015, -lr * Pred Grad: 0.795, New P: 0.119
-Original Grad: 0.172, -lr * Pred Grad: 1.383, New P: -0.122
iter 0 loss: 0.067
Actual params: [ 0.1193, -0.1216]
-Original Grad: -0.010, -lr * Pred Grad: 0.219, New P: 0.339
-Original Grad: -0.907, -lr * Pred Grad: -0.736, New P: -0.857
iter 1 loss: 0.203
Actual params: [ 0.3387, -0.8573]
-Original Grad: 0.005, -lr * Pred Grad: 0.282, New P: 0.620
-Original Grad: -0.099, -lr * Pred Grad: -0.326, New P: -1.183
iter 2 loss: 0.022
Actual params: [ 0.6203, -1.1828]
-Original Grad: 0.025, -lr * Pred Grad: 0.283, New P: 0.903
-Original Grad: 0.100, -lr * Pred Grad: -0.303, New P: -1.486
iter 3 loss: 0.019
Actual params: [ 0.9028, -1.4862]
-Original Grad: 0.020, -lr * Pred Grad: 0.224, New P: 1.127
-Original Grad: 0.182, -lr * Pred Grad: 0.237, New P: -1.249
iter 4 loss: 0.033
Actual params: [ 1.1269, -1.2491]
-Original Grad: 0.015, -lr * Pred Grad: 0.177, New P: 1.304
-Original Grad: 0.102, -lr * Pred Grad: 0.320, New P: -0.929
iter 5 loss: 0.012
Actual params: [ 1.3037, -0.9294]
-Original Grad: 0.003, -lr * Pred Grad: 0.108, New P: 1.412
-Original Grad: -0.123, -lr * Pred Grad: -0.249, New P: -1.179
iter 6 loss: 0.012
Actual params: [ 1.4118, -1.1788]
-Original Grad: 0.004, -lr * Pred Grad: 0.078, New P: 1.490
-Original Grad: 0.040, -lr * Pred Grad: -0.012, New P: -1.191
iter 7 loss: 0.007
Actual params: [ 1.4898, -1.1906]
-Original Grad: 0.003, -lr * Pred Grad: 0.053, New P: 1.543
-Original Grad: 0.046, -lr * Pred Grad: 0.068, New P: -1.123
iter 8 loss: 0.007
Actual params: [ 1.5428, -1.1227]
-Original Grad: 0.003, -lr * Pred Grad: 0.036, New P: 1.579
-Original Grad: 0.004, -lr * Pred Grad: 0.018, New P: -1.105
iter 9 loss: 0.006
Actual params: [ 1.5786, -1.1049]
-Original Grad: 0.002, -lr * Pred Grad: 0.020, New P: 1.598
-Original Grad: -0.008, -lr * Pred Grad: -0.031, New P: -1.136
iter 10 loss: 0.006
Actual params: [ 1.5983, -1.1356]
-Original Grad: 0.002, -lr * Pred Grad: 0.009, New P: 1.607
-Original Grad: 0.011, -lr * Pred Grad: -0.003, New P: -1.139
iter 11 loss: 0.006
Actual params: [ 1.6074, -1.1388]
-Original Grad: 0.002, -lr * Pred Grad: 0.001, New P: 1.608
-Original Grad: 0.013, -lr * Pred Grad: 0.011, New P: -1.128
iter 12 loss: 0.006
Actual params: [ 1.608 , -1.1281]
-Original Grad: 0.001, -lr * Pred Grad: -0.006, New P: 1.602
-Original Grad: 0.006, -lr * Pred Grad: -0.000, New P: -1.128
iter 13 loss: 0.006
Actual params: [ 1.6018, -1.1284]
-Original Grad: 0.002, -lr * Pred Grad: -0.011, New P: 1.591
-Original Grad: 0.007, -lr * Pred Grad: -0.002, New P: -1.130
iter 14 loss: 0.006
Actual params: [ 1.5905, -1.1305]
-Original Grad: 0.002, -lr * Pred Grad: -0.015, New P: 1.575
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: -1.130
iter 15 loss: 0.006
Actual params: [ 1.5754, -1.1303]
-Original Grad: 0.002, -lr * Pred Grad: -0.017, New P: 1.558
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.129
iter 16 loss: 0.006
Actual params: [ 1.5585, -1.1291]
-Original Grad: 0.002, -lr * Pred Grad: -0.018, New P: 1.540
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: -1.129
iter 17 loss: 0.006
Actual params: [ 1.5401, -1.1289]
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: 1.523
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: -1.129
iter 18 loss: 0.006
Actual params: [ 1.5229, -1.1289]
-Original Grad: 0.003, -lr * Pred Grad: -0.016, New P: 1.507
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.128
iter 19 loss: 0.006
Actual params: [ 1.5068, -1.1283]
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: 1.490
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.129
iter 20 loss: 0.006
Actual params: [ 1.4896, -1.1292]
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: 1.473
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.128
iter 21 loss: 0.006
Actual params: [ 1.4725, -1.1284]
-Original Grad: 0.003, -lr * Pred Grad: -0.018, New P: 1.455
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.129
iter 22 loss: 0.006
Actual params: [ 1.4549, -1.1292]
-Original Grad: 0.003, -lr * Pred Grad: -0.018, New P: 1.437
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.128
iter 23 loss: 0.006
Actual params: [ 1.4371, -1.1282]
-Original Grad: 0.003, -lr * Pred Grad: -0.017, New P: 1.420
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.127
iter 24 loss: 0.006
Actual params: [ 1.4202, -1.1267]
-Original Grad: 0.003, -lr * Pred Grad: -0.016, New P: 1.405
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.126
iter 25 loss: 0.006
Actual params: [ 1.4047, -1.1256]
-Original Grad: 0.004, -lr * Pred Grad: -0.014, New P: 1.391
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.127
iter 26 loss: 0.006
Actual params: [ 1.3908, -1.1267]
-Original Grad: 0.004, -lr * Pred Grad: -0.013, New P: 1.378
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.126
iter 27 loss: 0.007
Actual params: [ 1.3777, -1.1257]
-Original Grad: 0.004, -lr * Pred Grad: -0.011, New P: 1.366
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -1.124
iter 28 loss: 0.007
Actual params: [ 1.3664, -1.1241]
-Original Grad: 0.004, -lr * Pred Grad: -0.009, New P: 1.357
-Original Grad: 0.006, -lr * Pred Grad: -0.004, New P: -1.128
iter 29 loss: 0.007
Actual params: [ 1.3573, -1.1276]
-Original Grad: 0.004, -lr * Pred Grad: -0.008, New P: 1.350
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.126
iter 30 loss: 0.007
Actual params: [ 1.3496, -1.1262]
Target params: [-4.5633, -1.0472]
Actual params: [-0.6634, -0.2295]
-Original Grad: 0.014, -lr * Pred Grad: 0.791, New P: 0.127
-Original Grad: -1.067, -lr * Pred Grad: -0.516, New P: -0.745
iter 0 loss: 0.563
Actual params: [ 0.1273, -0.7452]
-Original Grad: -0.019, -lr * Pred Grad: 0.126, New P: 0.253
-Original Grad: -1.470, -lr * Pred Grad: -0.576, New P: -1.321
iter 1 loss: 0.200
Actual params: [ 0.2533, -1.3211]
-Original Grad: 0.001, -lr * Pred Grad: 0.224, New P: 0.477
-Original Grad: 0.076, -lr * Pred Grad: -0.591, New P: -1.912
iter 2 loss: 0.055
Actual params: [ 0.4768, -1.9123]
-Original Grad: 0.014, -lr * Pred Grad: 0.186, New P: 0.663
-Original Grad: 0.326, -lr * Pred Grad: -0.109, New P: -2.022
iter 3 loss: 0.100
Actual params: [ 0.6629, -2.0216]
-Original Grad: 0.010, -lr * Pred Grad: 0.141, New P: 0.804
-Original Grad: 0.411, -lr * Pred Grad: 0.901, New P: -1.121
iter 4 loss: 0.117
Actual params: [ 0.8037, -1.1206]
-Original Grad: -0.026, -lr * Pred Grad: -0.035, New P: 0.769
-Original Grad: -0.328, -lr * Pred Grad: -0.531, New P: -1.651
iter 5 loss: 0.077
Actual params: [ 0.7691, -1.6513]
-Original Grad: 0.009, -lr * Pred Grad: 0.031, New P: 0.800
-Original Grad: 0.136, -lr * Pred Grad: 0.000, New P: -1.651
iter 6 loss: 0.066
Actual params: [ 0.8   , -1.6512]
-Original Grad: 0.009, -lr * Pred Grad: 0.031, New P: 0.831
-Original Grad: 0.130, -lr * Pred Grad: 0.234, New P: -1.418
iter 7 loss: 0.066
Actual params: [ 0.8308, -1.4176]
-Original Grad: -0.001, -lr * Pred Grad: 0.005, New P: 0.835
-Original Grad: 0.034, -lr * Pred Grad: 0.169, New P: -1.249
iter 8 loss: 0.056
Actual params: [ 0.8354, -1.249 ]
-Original Grad: -0.015, -lr * Pred Grad: -0.051, New P: 0.785
-Original Grad: -0.170, -lr * Pred Grad: -0.288, New P: -1.537
iter 9 loss: 0.061
Actual params: [ 0.7846, -1.537 ]
-Original Grad: 0.004, -lr * Pred Grad: -0.024, New P: 0.761
-Original Grad: 0.090, -lr * Pred Grad: 0.026, New P: -1.511
iter 10 loss: 0.060
Actual params: [ 0.7609, -1.5113]
-Original Grad: 0.008, -lr * Pred Grad: -0.006, New P: 0.755
-Original Grad: 0.069, -lr * Pred Grad: 0.171, New P: -1.341
iter 11 loss: 0.059
Actual params: [ 0.7547, -1.3407]
-Original Grad: -0.006, -lr * Pred Grad: -0.038, New P: 0.717
-Original Grad: -0.032, -lr * Pred Grad: -0.052, New P: -1.392
iter 12 loss: 0.056
Actual params: [ 0.7172, -1.3925]
-Original Grad: -0.001, -lr * Pred Grad: -0.037, New P: 0.680
-Original Grad: 0.038, -lr * Pred Grad: 0.069, New P: -1.323
iter 13 loss: 0.056
Actual params: [ 0.6799, -1.3231]
-Original Grad: -0.008, -lr * Pred Grad: -0.062, New P: 0.618
-Original Grad: -0.038, -lr * Pred Grad: -0.092, New P: -1.415
iter 14 loss: 0.055
Actual params: [ 0.618 , -1.4151]
-Original Grad: 0.004, -lr * Pred Grad: -0.040, New P: 0.579
-Original Grad: 0.060, -lr * Pred Grad: 0.103, New P: -1.312
iter 15 loss: 0.056
Actual params: [ 0.5785, -1.3124]
-Original Grad: -0.006, -lr * Pred Grad: -0.060, New P: 0.518
-Original Grad: -0.041, -lr * Pred Grad: -0.093, New P: -1.405
iter 16 loss: 0.055
Actual params: [ 0.5185, -1.4051]
-Original Grad: 0.004, -lr * Pred Grad: -0.041, New P: 0.477
-Original Grad: 0.075, -lr * Pred Grad: 0.147, New P: -1.259
iter 17 loss: 0.056
Actual params: [ 0.4771, -1.2585]
-Original Grad: -0.014, -lr * Pred Grad: -0.083, New P: 0.394
-Original Grad: -0.076, -lr * Pred Grad: -0.159, New P: -1.418
iter 18 loss: 0.056
Actual params: [ 0.3937, -1.4179]
-Original Grad: 0.011, -lr * Pred Grad: -0.033, New P: 0.360
-Original Grad: 0.071, -lr * Pred Grad: 0.099, New P: -1.319
iter 19 loss: 0.058
Actual params: [ 0.3605, -1.3189]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: 0.317
-Original Grad: 0.044, -lr * Pred Grad: 0.135, New P: -1.184
iter 20 loss: 0.055
Actual params: [ 0.3168, -1.1839]
-Original Grad: -0.020, -lr * Pred Grad: -0.101, New P: 0.216
-Original Grad: -0.156, -lr * Pred Grad: -0.277, New P: -1.461
iter 21 loss: 0.058
Actual params: [ 0.2158, -1.4605]
-Original Grad: 0.014, -lr * Pred Grad: -0.036, New P: 0.180
-Original Grad: 0.099, -lr * Pred Grad: 0.060, New P: -1.401
iter 22 loss: 0.062
Actual params: [ 0.1802, -1.4005]
-Original Grad: 0.011, -lr * Pred Grad: -0.016, New P: 0.164
-Original Grad: 0.118, -lr * Pred Grad: 0.365, New P: -1.036
iter 23 loss: 0.059
Actual params: [ 0.164 , -1.0358]
-Original Grad: -0.032, -lr * Pred Grad: -0.121, New P: 0.043
-Original Grad: -0.428, -lr * Pred Grad: -0.433, New P: -1.468
iter 24 loss: 0.074
Actual params: [ 0.0431, -1.4683]
-Original Grad: 0.016, -lr * Pred Grad: -0.039, New P: 0.004
-Original Grad: 0.106, -lr * Pred Grad: -0.161, New P: -1.630
iter 25 loss: 0.065
Actual params: [ 0.0037, -1.6298]
-Original Grad: 0.018, -lr * Pred Grad: 0.002, New P: 0.005
-Original Grad: 0.157, -lr * Pred Grad: 0.252, New P: -1.378
iter 26 loss: 0.076
Actual params: [ 0.0055, -1.3779]
-Original Grad: 0.012, -lr * Pred Grad: 0.013, New P: 0.019
-Original Grad: 0.134, -lr * Pred Grad: 0.547, New P: -0.831
iter 27 loss: 0.060
Actual params: [ 0.0186, -0.8312]
-Original Grad: -0.032, -lr * Pred Grad: -0.100, New P: -0.081
-Original Grad: -1.100, -lr * Pred Grad: -0.453, New P: -1.284
iter 28 loss: 0.143
Actual params: [-0.0814, -1.284 ]
-Original Grad: 0.003, -lr * Pred Grad: -0.057, New P: -0.139
-Original Grad: 0.104, -lr * Pred Grad: -0.382, New P: -1.666
iter 29 loss: 0.054
Actual params: [-0.1389, -1.6656]
-Original Grad: 0.015, -lr * Pred Grad: -0.013, New P: -0.152
-Original Grad: 0.185, -lr * Pred Grad: -0.065, New P: -1.731
iter 30 loss: 0.081
Actual params: [-0.1518, -1.7308]
Target params: [-4.5633, -1.0472]
Actual params: [-0.8962,  0.1733]
-Original Grad: -0.038, -lr * Pred Grad: 0.231, New P: -0.666
-Original Grad: -0.802, -lr * Pred Grad: -0.536, New P: -0.363
iter 0 loss: 0.385
Actual params: [-0.6657, -0.3629]
-Original Grad: -0.025, -lr * Pred Grad: 0.043, New P: -0.623
-Original Grad: -0.602, -lr * Pred Grad: -0.563, New P: -0.926
iter 1 loss: 0.198
Actual params: [-0.6227, -0.9259]
-Original Grad: -0.005, -lr * Pred Grad: 0.057, New P: -0.565
-Original Grad: -0.285, -lr * Pred Grad: -0.619, New P: -1.545
iter 2 loss: 0.078
Actual params: [-0.5653, -1.5453]
-Original Grad: -0.000, -lr * Pred Grad: 0.028, New P: -0.538
-Original Grad: 0.006, -lr * Pred Grad: -0.499, New P: -2.044
iter 3 loss: 0.035
Actual params: [-0.5376, -2.0443]
-Original Grad: 0.000, -lr * Pred Grad: 0.013, New P: -0.525
-Original Grad: 0.233, -lr * Pred Grad: 0.062, New P: -1.982
iter 4 loss: 0.065
Actual params: [-0.525 , -1.9823]
-Original Grad: 0.000, -lr * Pred Grad: 0.001, New P: -0.524
-Original Grad: 0.205, -lr * Pred Grad: 0.494, New P: -1.488
iter 5 loss: 0.058
Actual params: [-0.5244, -1.4881]
-Original Grad: -0.001, -lr * Pred Grad: -0.011, New P: -0.536
-Original Grad: -0.021, -lr * Pred Grad: -0.057, New P: -1.545
iter 6 loss: 0.036
Actual params: [-0.5358, -1.545 ]
-Original Grad: -0.001, -lr * Pred Grad: -0.018, New P: -0.554
-Original Grad: 0.005, -lr * Pred Grad: 0.005, New P: -1.540
iter 7 loss: 0.035
Actual params: [-0.5543, -1.5396]
-Original Grad: -0.001, -lr * Pred Grad: -0.024, New P: -0.579
-Original Grad: 0.003, -lr * Pred Grad: -0.027, New P: -1.567
iter 8 loss: 0.035
Actual params: [-0.5785, -1.5669]
-Original Grad: -0.000, -lr * Pred Grad: -0.029, New P: -0.607
-Original Grad: 0.016, -lr * Pred Grad: 0.011, New P: -1.556
iter 9 loss: 0.035
Actual params: [-0.6071, -1.5556]
-Original Grad: -0.001, -lr * Pred Grad: -0.032, New P: -0.639
-Original Grad: 0.011, -lr * Pred Grad: 0.009, New P: -1.547
iter 10 loss: 0.035
Actual params: [-0.6393, -1.5467]
-Original Grad: -0.001, -lr * Pred Grad: -0.035, New P: -0.674
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: -1.546
iter 11 loss: 0.035
Actual params: [-0.6744, -1.5457]
-Original Grad: -0.000, -lr * Pred Grad: -0.037, New P: -0.712
-Original Grad: 0.006, -lr * Pred Grad: -0.003, New P: -1.549
iter 12 loss: 0.035
Actual params: [-0.7115, -1.549 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.039, New P: -0.750
-Original Grad: 0.008, -lr * Pred Grad: -0.001, New P: -1.550
iter 13 loss: 0.035
Actual params: [-0.7504, -1.5498]
-Original Grad: -0.000, -lr * Pred Grad: -0.040, New P: -0.791
-Original Grad: 0.009, -lr * Pred Grad: 0.002, New P: -1.548
iter 14 loss: 0.035
Actual params: [-0.7908, -1.5483]
-Original Grad: -0.000, -lr * Pred Grad: -0.042, New P: -0.832
-Original Grad: 0.008, -lr * Pred Grad: 0.002, New P: -1.547
iter 15 loss: 0.035
Actual params: [-0.8323, -1.5468]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.875
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: -1.546
iter 16 loss: 0.035
Actual params: [-0.8749, -1.5464]
-Original Grad: -0.000, -lr * Pred Grad: -0.043, New P: -0.918
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: -1.546
iter 17 loss: 0.035
Actual params: [-0.9183, -1.5462]
-Original Grad: -0.000, -lr * Pred Grad: -0.044, New P: -0.962
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: -1.546
iter 18 loss: 0.035
Actual params: [-0.9624, -1.5458]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.007
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -1.546
iter 19 loss: 0.035
Actual params: [-1.007 , -1.5458]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.052
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.545
iter 20 loss: 0.035
Actual params: [-1.0522, -1.5451]
-Original Grad: -0.000, -lr * Pred Grad: -0.045, New P: -1.098
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.545
iter 21 loss: 0.035
Actual params: [-1.0976, -1.5445]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.143
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: -1.544
iter 22 loss: 0.035
Actual params: [-1.1435, -1.5443]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.190
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: -1.544
iter 23 loss: 0.035
Actual params: [-1.1895, -1.544 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.236
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.543
iter 24 loss: 0.035
Actual params: [-1.2357, -1.5435]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.282
-Original Grad: 0.008, -lr * Pred Grad: 0.000, New P: -1.543
iter 25 loss: 0.035
Actual params: [-1.282, -1.543]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.328
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: -1.543
iter 26 loss: 0.035
Actual params: [-1.3283, -1.5429]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.375
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: -1.543
iter 27 loss: 0.035
Actual params: [-1.3745, -1.5429]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.421
-Original Grad: 0.007, -lr * Pred Grad: 0.000, New P: -1.543
iter 28 loss: 0.035
Actual params: [-1.4206, -1.5428]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.467
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -1.543
iter 29 loss: 0.035
Actual params: [-1.4666, -1.543 ]
-Original Grad: -0.000, -lr * Pred Grad: -0.046, New P: -1.512
-Original Grad: 0.007, -lr * Pred Grad: -0.000, New P: -1.543
iter 30 loss: 0.035
Actual params: [-1.5125, -1.543 ]
Target params: [-4.5633, -1.0472]
Actual params: [1.5544, 0.3381]
-Original Grad: 0.011, -lr * Pred Grad: 0.755, New P: 2.309
-Original Grad: -0.627, -lr * Pred Grad: -0.549, New P: -0.211
iter 0 loss: 0.275
Actual params: [ 2.3091, -0.2112]
-Original Grad: 0.038, -lr * Pred Grad: 0.729, New P: 3.038
-Original Grad: -0.399, -lr * Pred Grad: -0.541, New P: -0.752
iter 1 loss: 0.084
Actual params: [ 3.0378, -0.7517]
-Original Grad: -0.028, -lr * Pred Grad: 0.010, New P: 3.048
-Original Grad: 0.113, -lr * Pred Grad: -0.329, New P: -1.080
iter 2 loss: 0.043
Actual params: [ 3.0475, -1.0804]
-Original Grad: 0.000, -lr * Pred Grad: 0.145, New P: 3.193
-Original Grad: 0.538, -lr * Pred Grad: 1.238, New P: 0.158
iter 3 loss: 0.095
Actual params: [3.1927, 0.1577]
-Original Grad: -0.176, -lr * Pred Grad: -0.710, New P: 2.482
-Original Grad: -0.535, -lr * Pred Grad: -0.641, New P: -0.483
iter 4 loss: 0.291
Actual params: [ 2.4825, -0.4829]
-Original Grad: 0.040, -lr * Pred Grad: -0.203, New P: 2.279
-Original Grad: -0.072, -lr * Pred Grad: -0.260, New P: -0.742
iter 5 loss: 0.048
Actual params: [ 2.2791, -0.7424]
-Original Grad: 0.054, -lr * Pred Grad: -0.030, New P: 2.249
-Original Grad: 0.153, -lr * Pred Grad: -0.086, New P: -0.829
iter 6 loss: 0.067
Actual params: [ 2.249 , -0.8286]
-Original Grad: 0.041, -lr * Pred Grad: 0.052, New P: 2.301
-Original Grad: 0.208, -lr * Pred Grad: 0.555, New P: -0.274
iter 7 loss: 0.077
Actual params: [ 2.3012, -0.2737]
-Original Grad: 0.049, -lr * Pred Grad: 0.153, New P: 2.455
-Original Grad: -0.293, -lr * Pred Grad: -0.434, New P: -0.707
iter 8 loss: 0.074
Actual params: [ 2.4547, -0.7072]
-Original Grad: 0.058, -lr * Pred Grad: 0.245, New P: 2.699
-Original Grad: 0.158, -lr * Pred Grad: 0.076, New P: -0.631
iter 9 loss: 0.055
Actual params: [ 2.6994, -0.6314]
-Original Grad: 0.032, -lr * Pred Grad: 0.222, New P: 2.921
-Original Grad: 0.055, -lr * Pred Grad: 0.122, New P: -0.510
iter 10 loss: 0.039
Actual params: [ 2.9212, -0.5098]
-Original Grad: -0.033, -lr * Pred Grad: 0.020, New P: 2.941
-Original Grad: -0.151, -lr * Pred Grad: -0.258, New P: -0.767
iter 11 loss: 0.042
Actual params: [ 2.9414, -0.7675]
-Original Grad: -0.006, -lr * Pred Grad: 0.015, New P: 2.956
-Original Grad: 0.150, -lr * Pred Grad: 0.185, New P: -0.583
iter 12 loss: 0.043
Actual params: [ 2.9563, -0.5826]
-Original Grad: -0.032, -lr * Pred Grad: -0.090, New P: 2.866
-Original Grad: -0.070, -lr * Pred Grad: -0.140, New P: -0.723
iter 13 loss: 0.039
Actual params: [ 2.8663, -0.7229]
-Original Grad: 0.006, -lr * Pred Grad: -0.040, New P: 2.827
-Original Grad: 0.118, -lr * Pred Grad: 0.247, New P: -0.476
iter 14 loss: 0.040
Actual params: [ 2.8266, -0.476 ]
-Original Grad: -0.014, -lr * Pred Grad: -0.082, New P: 2.744
-Original Grad: -0.164, -lr * Pred Grad: -0.292, New P: -0.768
iter 15 loss: 0.042
Actual params: [ 2.7443, -0.7683]
-Original Grad: 0.037, -lr * Pred Grad: 0.037, New P: 2.781
-Original Grad: 0.194, -lr * Pred Grad: 0.311, New P: -0.458
iter 16 loss: 0.046
Actual params: [ 2.7814, -0.4578]
-Original Grad: -0.007, -lr * Pred Grad: -0.025, New P: 2.757
-Original Grad: -0.173, -lr * Pred Grad: -0.301, New P: -0.759
iter 17 loss: 0.043
Actual params: [ 2.7566, -0.7592]
-Original Grad: 0.036, -lr * Pred Grad: 0.078, New P: 2.835
-Original Grad: 0.184, -lr * Pred Grad: 0.278, New P: -0.481
iter 18 loss: 0.045
Actual params: [ 2.8347, -0.4809]
-Original Grad: -0.015, -lr * Pred Grad: -0.022, New P: 2.813
-Original Grad: -0.161, -lr * Pred Grad: -0.281, New P: -0.762
iter 19 loss: 0.042
Actual params: [ 2.8132, -0.7619]
-Original Grad: 0.025, -lr * Pred Grad: 0.055, New P: 2.868
-Original Grad: 0.174, -lr * Pred Grad: 0.267, New P: -0.494
iter 20 loss: 0.044
Actual params: [ 2.8683, -0.4945]
-Original Grad: -0.021, -lr * Pred Grad: -0.046, New P: 2.822
-Original Grad: -0.153, -lr * Pred Grad: -0.271, New P: -0.765
iter 21 loss: 0.041
Actual params: [ 2.822 , -0.7653]
-Original Grad: 0.024, -lr * Pred Grad: 0.036, New P: 2.859
-Original Grad: 0.178, -lr * Pred Grad: 0.290, New P: -0.475
iter 22 loss: 0.044
Actual params: [ 2.8585, -0.4749]
-Original Grad: -0.021, -lr * Pred Grad: -0.060, New P: 2.798
-Original Grad: -0.176, -lr * Pred Grad: -0.299, New P: -0.774
iter 23 loss: 0.043
Actual params: [ 2.7982, -0.774 ]
-Original Grad: 0.026, -lr * Pred Grad: 0.032, New P: 2.830
-Original Grad: 0.189, -lr * Pred Grad: 0.287, New P: -0.487
iter 24 loss: 0.045
Actual params: [ 2.8301, -0.4874]
-Original Grad: -0.013, -lr * Pred Grad: -0.042, New P: 2.788
-Original Grad: -0.150, -lr * Pred Grad: -0.268, New P: -0.755
iter 25 loss: 0.041
Actual params: [ 2.7881, -0.7551]
-Original Grad: 0.025, -lr * Pred Grad: 0.039, New P: 2.827
-Original Grad: 0.170, -lr * Pred Grad: 0.277, New P: -0.479
iter 26 loss: 0.044
Actual params: [ 2.8272, -0.4786]
-Original Grad: -0.014, -lr * Pred Grad: -0.038, New P: 2.789
-Original Grad: -0.161, -lr * Pred Grad: -0.281, New P: -0.760
iter 27 loss: 0.042
Actual params: [ 2.7889, -0.7596]
-Original Grad: 0.026, -lr * Pred Grad: 0.044, New P: 2.833
-Original Grad: 0.175, -lr * Pred Grad: 0.270, New P: -0.490
iter 28 loss: 0.044
Actual params: [ 2.8325, -0.4901]
-Original Grad: -0.014, -lr * Pred Grad: -0.035, New P: 2.798
-Original Grad: -0.148, -lr * Pred Grad: -0.264, New P: -0.754
iter 29 loss: 0.041
Actual params: [ 2.7977, -0.7545]
-Original Grad: 0.026, -lr * Pred Grad: 0.046, New P: 2.843
-Original Grad: 0.168, -lr * Pred Grad: 0.273, New P: -0.482
iter 30 loss: 0.043
Actual params: [ 2.8435, -0.4816]
Target params: [-4.5633, -1.0472]
Actual params: [-0.7899, -0.493 ]
-Original Grad: 0.001, -lr * Pred Grad: 0.655, New P: -0.135
-Original Grad: -0.514, -lr * Pred Grad: -0.543, New P: -1.036
iter 0 loss: 0.200
Actual params: [-0.1349, -1.036 ]
-Original Grad: -0.005, -lr * Pred Grad: 0.281, New P: 0.146
-Original Grad: -0.285, -lr * Pred Grad: -0.496, New P: -1.532
iter 1 loss: 0.089
Actual params: [ 0.146 , -1.5323]
-Original Grad: -0.024, -lr * Pred Grad: 0.014, New P: 0.160
-Original Grad: 0.012, -lr * Pred Grad: -0.371, New P: -1.903
iter 2 loss: 0.058
Actual params: [ 0.1604, -1.9033]
-Original Grad: -0.055, -lr * Pred Grad: -0.271, New P: -0.110
-Original Grad: 0.147, -lr * Pred Grad: 0.092, New P: -1.811
iter 3 loss: 0.073
Actual params: [-0.1105, -1.8114]
-Original Grad: -0.033, -lr * Pred Grad: -0.246, New P: -0.356
-Original Grad: 0.078, -lr * Pred Grad: 0.169, New P: -1.642
iter 4 loss: 0.056
Actual params: [-0.3562, -1.6425]
-Original Grad: -0.019, -lr * Pred Grad: -0.222, New P: -0.578
-Original Grad: 0.007, -lr * Pred Grad: 0.024, New P: -1.619
iter 5 loss: 0.047
Actual params: [-0.578 , -1.6188]
-Original Grad: -0.015, -lr * Pred Grad: -0.208, New P: -0.786
-Original Grad: -0.023, -lr * Pred Grad: -0.066, New P: -1.685
iter 6 loss: 0.043
Actual params: [-0.7865, -1.6851]
-Original Grad: -0.013, -lr * Pred Grad: -0.199, New P: -0.986
-Original Grad: -0.022, -lr * Pred Grad: -0.098, New P: -1.783
iter 7 loss: 0.039
Actual params: [-0.9859, -1.7831]
-Original Grad: -0.010, -lr * Pred Grad: -0.185, New P: -1.170
-Original Grad: 0.012, -lr * Pred Grad: -0.039, New P: -1.822
iter 8 loss: 0.037
Actual params: [-1.1704, -1.8223]
-Original Grad: -0.007, -lr * Pred Grad: -0.169, New P: -1.339
-Original Grad: 0.015, -lr * Pred Grad: -0.003, New P: -1.825
iter 9 loss: 0.036
Actual params: [-1.3393, -1.8249]
-Original Grad: -0.007, -lr * Pred Grad: -0.157, New P: -1.496
-Original Grad: -0.002, -lr * Pred Grad: -0.027, New P: -1.852
iter 10 loss: 0.034
Actual params: [-1.4962, -1.8518]
-Original Grad: -0.005, -lr * Pred Grad: -0.144, New P: -1.640
-Original Grad: -0.005, -lr * Pred Grad: -0.042, New P: -1.894
iter 11 loss: 0.033
Actual params: [-1.6399, -1.8943]
-Original Grad: -0.004, -lr * Pred Grad: -0.131, New P: -1.770
-Original Grad: -0.006, -lr * Pred Grad: -0.051, New P: -1.945
iter 12 loss: 0.032
Actual params: [-1.7705, -1.9449]
-Original Grad: -0.003, -lr * Pred Grad: -0.119, New P: -1.889
-Original Grad: 0.009, -lr * Pred Grad: -0.019, New P: -1.964
iter 13 loss: 0.032
Actual params: [-1.8893, -1.9638]
-Original Grad: -0.002, -lr * Pred Grad: -0.107, New P: -1.996
-Original Grad: 0.012, -lr * Pred Grad: 0.001, New P: -1.963
iter 14 loss: 0.032
Actual params: [-1.9958, -1.9626]
-Original Grad: -0.002, -lr * Pred Grad: -0.096, New P: -2.092
-Original Grad: 0.011, -lr * Pred Grad: 0.009, New P: -1.954
iter 15 loss: 0.032
Actual params: [-2.0917, -1.9539]
-Original Grad: -0.001, -lr * Pred Grad: -0.086, New P: -2.177
-Original Grad: 0.007, -lr * Pred Grad: 0.001, New P: -1.953
iter 16 loss: 0.031
Actual params: [-2.1774, -1.9531]
-Original Grad: -0.001, -lr * Pred Grad: -0.077, New P: -2.255
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.954
iter 17 loss: 0.031
Actual params: [-2.2548, -1.9539]
-Original Grad: -0.000, -lr * Pred Grad: -0.070, New P: -2.324
-Original Grad: 0.006, -lr * Pred Grad: -0.005, New P: -1.959
iter 18 loss: 0.031
Actual params: [-2.3244, -1.959 ]
-Original Grad: 0.000, -lr * Pred Grad: -0.063, New P: -2.388
-Original Grad: 0.006, -lr * Pred Grad: -0.006, New P: -1.965
iter 19 loss: 0.031
Actual params: [-2.3876, -1.9647]
-Original Grad: 0.000, -lr * Pred Grad: -0.057, New P: -2.445
-Original Grad: 0.007, -lr * Pred Grad: -0.004, New P: -1.969
iter 20 loss: 0.031
Actual params: [-2.4449, -1.9688]
-Original Grad: 0.001, -lr * Pred Grad: -0.053, New P: -2.498
-Original Grad: 0.010, -lr * Pred Grad: 0.004, New P: -1.965
iter 21 loss: 0.031
Actual params: [-2.4975, -1.9649]
-Original Grad: 0.001, -lr * Pred Grad: -0.049, New P: -2.546
-Original Grad: 0.005, -lr * Pred Grad: -0.005, New P: -1.970
iter 22 loss: 0.031
Actual params: [-2.5464, -1.9697]
-Original Grad: 0.001, -lr * Pred Grad: -0.046, New P: -2.592
-Original Grad: 0.009, -lr * Pred Grad: 0.002, New P: -1.968
iter 23 loss: 0.031
Actual params: [-2.592 , -1.9677]
-Original Grad: 0.001, -lr * Pred Grad: -0.043, New P: -2.635
-Original Grad: 0.005, -lr * Pred Grad: -0.006, New P: -1.974
iter 24 loss: 0.031
Actual params: [-2.6349, -1.9735]
-Original Grad: 0.001, -lr * Pred Grad: -0.041, New P: -2.676
-Original Grad: 0.009, -lr * Pred Grad: 0.002, New P: -1.972
iter 25 loss: 0.032
Actual params: [-2.6758, -1.9719]
-Original Grad: 0.001, -lr * Pred Grad: -0.039, New P: -2.715
-Original Grad: 0.008, -lr * Pred Grad: 0.001, New P: -1.971
iter 26 loss: 0.032
Actual params: [-2.7146, -1.9713]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: -2.752
-Original Grad: 0.007, -lr * Pred Grad: -0.001, New P: -1.972
iter 27 loss: 0.032
Actual params: [-2.7517, -1.9722]
-Original Grad: 0.001, -lr * Pred Grad: -0.036, New P: -2.787
-Original Grad: 0.006, -lr * Pred Grad: -0.003, New P: -1.975
iter 28 loss: 0.032
Actual params: [-2.7873, -1.9752]
-Original Grad: 0.001, -lr * Pred Grad: -0.034, New P: -2.822
-Original Grad: 0.007, -lr * Pred Grad: -0.003, New P: -1.978
iter 29 loss: 0.032
Actual params: [-2.8218, -1.9779]
-Original Grad: 0.001, -lr * Pred Grad: -0.033, New P: -2.855
-Original Grad: 0.007, -lr * Pred Grad: -0.002, New P: -1.980
iter 30 loss: 0.032
Actual params: [-2.8552, -1.9803]
Target params: [-4.5633, -1.0472]
Actual params: [0.3685, 0.155 ]
-Original Grad: 0.041, -lr * Pred Grad: 1.038, New P: 1.407
-Original Grad: -0.814, -lr * Pred Grad: -0.535, New P: -0.380
iter 0 loss: 0.952
Actual params: [ 1.4065, -0.3801]
-Original Grad: -0.052, -lr * Pred Grad: -0.243, New P: 1.163
-Original Grad: -1.741, -lr * Pred Grad: -0.572, New P: -0.952
iter 1 loss: 0.595
Actual params: [ 1.1635, -0.9522]
-Original Grad: -0.096, -lr * Pred Grad: -0.606, New P: 0.558
-Original Grad: -0.301, -lr * Pred Grad: -0.618, New P: -1.571
iter 2 loss: 0.163
Actual params: [ 0.5578, -1.5707]
-Original Grad: -0.020, -lr * Pred Grad: -0.289, New P: 0.268
-Original Grad: 0.378, -lr * Pred Grad: -0.307, New P: -1.878
iter 3 loss: 0.124
Actual params: [ 0.2685, -1.8776]
-Original Grad: 0.030, -lr * Pred Grad: -0.082, New P: 0.186
-Original Grad: 0.526, -lr * Pred Grad: 0.910, New P: -0.968
iter 4 loss: 0.200
Actual params: [ 0.1863, -0.9676]
-Original Grad: -0.064, -lr * Pred Grad: -0.312, New P: -0.125
-Original Grad: -0.386, -lr * Pred Grad: -0.519, New P: -1.486
iter 5 loss: 0.087
Actual params: [-0.1253, -1.4862]
-Original Grad: 0.012, -lr * Pred Grad: -0.174, New P: -0.299
-Original Grad: 0.504, -lr * Pred Grad: 0.555, New P: -0.931
iter 6 loss: 0.105
Actual params: [-0.2988, -0.9312]
-Original Grad: -0.042, -lr * Pred Grad: -0.273, New P: -0.572
-Original Grad: -0.546, -lr * Pred Grad: -0.390, New P: -1.321
iter 7 loss: 0.071
Actual params: [-0.5716, -1.3209]
-Original Grad: 0.005, -lr * Pred Grad: -0.186, New P: -0.757
-Original Grad: 0.480, -lr * Pred Grad: 0.395, New P: -0.926
iter 8 loss: 0.067
Actual params: [-0.7574, -0.926 ]
-Original Grad: -0.031, -lr * Pred Grad: -0.245, New P: -1.003
-Original Grad: -0.625, -lr * Pred Grad: -0.390, New P: -1.316
iter 9 loss: 0.055
Actual params: [-1.0028, -1.3157]
-Original Grad: 0.022, -lr * Pred Grad: -0.121, New P: -1.124
-Original Grad: 0.619, -lr * Pred Grad: 0.557, New P: -0.758
iter 10 loss: 0.071
Actual params: [-1.1241, -0.7583]
-Original Grad: -0.007, -lr * Pred Grad: -0.135, New P: -1.259
-Original Grad: -1.115, -lr * Pred Grad: -0.410, New P: -1.168
iter 11 loss: 0.121
Actual params: [-1.2591, -1.1684]
-Original Grad: 0.000, -lr * Pred Grad: -0.111, New P: -1.370
-Original Grad: 0.406, -lr * Pred Grad: -0.133, New P: -1.301
iter 12 loss: 0.036
Actual params: [-1.37  , -1.3013]
-Original Grad: 0.029, -lr * Pred Grad: -0.010, New P: -1.380
-Original Grad: 0.725, -lr * Pred Grad: 1.591, New P: 0.290
iter 13 loss: 0.076
Actual params: [-1.38  ,  0.2897]
-Original Grad: 0.001, -lr * Pred Grad: -0.032, New P: -1.412
-Original Grad: -0.647, -lr * Pred Grad: -0.706, New P: -0.417
iter 14 loss: 1.030
Actual params: [-1.4119, -0.4165]
-Original Grad: -0.001, -lr * Pred Grad: -0.039, New P: -1.450
-Original Grad: -3.085, -lr * Pred Grad: -0.444, New P: -0.861
iter 15 loss: 0.508
Actual params: [-1.4504, -0.8608]
-Original Grad: 0.001, -lr * Pred Grad: -0.037, New P: -1.487
-Original Grad: -0.912, -lr * Pred Grad: -0.605, New P: -1.466
iter 16 loss: 0.067
Actual params: [-1.4874, -1.4662]
-Original Grad: 0.040, -lr * Pred Grad: 0.080, New P: -1.407
-Original Grad: 0.776, -lr * Pred Grad: -0.253, New P: -1.719
iter 17 loss: 0.144
Actual params: [-1.4074, -1.7194]
-Original Grad: 0.045, -lr * Pred Grad: 0.152, New P: -1.256
-Original Grad: 0.593, -lr * Pred Grad: 0.797, New P: -0.923
iter 18 loss: 0.227
Actual params: [-1.2557, -0.9226]
-Original Grad: -0.010, -lr * Pred Grad: 0.041, New P: -1.214
-Original Grad: -0.589, -lr * Pred Grad: -0.440, New P: -1.362
iter 19 loss: 0.045
Actual params: [-1.2145, -1.3622]
-Original Grad: 0.036, -lr * Pred Grad: 0.138, New P: -1.076
-Original Grad: 0.726, -lr * Pred Grad: 0.615, New P: -0.747
iter 20 loss: 0.093
Actual params: [-1.0763, -0.747 ]
-Original Grad: -0.006, -lr * Pred Grad: 0.041, New P: -1.036
-Original Grad: -1.141, -lr * Pred Grad: -0.396, New P: -1.143
iter 21 loss: 0.127
Actual params: [-1.0357, -1.1426]
-Original Grad: -0.007, -lr * Pred Grad: 0.005, New P: -1.031
-Original Grad: 0.238, -lr * Pred Grad: -0.299, New P: -1.441
iter 22 loss: 0.032
Actual params: [-1.0307, -1.4412]
-Original Grad: 0.042, -lr * Pred Grad: 0.118, New P: -0.913
-Original Grad: 0.709, -lr * Pred Grad: 1.226, New P: -0.215
iter 23 loss: 0.114
Actual params: [-0.9131, -0.2151]
-Original Grad: -0.002, -lr * Pred Grad: 0.038, New P: -0.875
-Original Grad: -1.794, -lr * Pred Grad: -0.536, New P: -0.751
iter 24 loss: 0.748
Actual params: [-0.8749, -0.7509]
-Original Grad: -0.014, -lr * Pred Grad: -0.019, New P: -0.894
-Original Grad: -1.090, -lr * Pred Grad: -0.492, New P: -1.243
iter 25 loss: 0.127
Actual params: [-0.8939, -1.2433]
-Original Grad: 0.005, -lr * Pred Grad: -0.001, New P: -0.895
-Original Grad: 0.450, -lr * Pred Grad: -0.421, New P: -1.664
iter 26 loss: 0.050
Actual params: [-0.8953, -1.664 ]
-Original Grad: 0.058, -lr * Pred Grad: 0.146, New P: -0.749
-Original Grad: 0.618, -lr * Pred Grad: 0.786, New P: -0.878
iter 27 loss: 0.183
Actual params: [-0.7492, -0.8777]
-Original Grad: -0.031, -lr * Pred Grad: -0.032, New P: -0.781
-Original Grad: -0.769, -lr * Pred Grad: -0.447, New P: -1.325
iter 28 loss: 0.072
Actual params: [-0.7811, -1.3251]
-Original Grad: 0.014, -lr * Pred Grad: 0.030, New P: -0.751
-Original Grad: 0.546, -lr * Pred Grad: 0.195, New P: -1.130
iter 29 loss: 0.070
Actual params: [-0.7511, -1.13  ]
-Original Grad: -0.012, -lr * Pred Grad: -0.033, New P: -0.784
-Original Grad: 0.159, -lr * Pred Grad: 0.352, New P: -0.778
iter 30 loss: 0.033
Actual params: [-0.7841, -0.7782]
