Target params: [1.3344, 1.5708]
Actual params: [1.084 , 0.5507]
-Original Grad: 0.016, -lr * Pred Grad:  0.012, New P: 1.096
-Original Grad: 0.390, -lr * Pred Grad:  0.430, New P: 0.981
iter 0 loss: 0.440
Actual params: [1.0962, 0.9809]
-Original Grad: -0.010, -lr * Pred Grad:  -0.408, New P: 0.689
-Original Grad: 0.215, -lr * Pred Grad:  0.230, New P: 1.211
iter 1 loss: 0.281
Actual params: [0.6887, 1.2114]
-Original Grad: 0.082, -lr * Pred Grad:  0.571, New P: 1.259
-Original Grad: 0.122, -lr * Pred Grad:  0.033, New P: 1.244
iter 2 loss: 0.206
Actual params: [1.2592, 1.2441]
-Original Grad: -0.034, -lr * Pred Grad:  -0.396, New P: 0.863
-Original Grad: 0.133, -lr * Pred Grad:  0.211, New P: 1.455
iter 3 loss: 0.205
Actual params: [0.8633, 1.4547]
-Original Grad: 0.083, -lr * Pred Grad:  0.159, New P: 1.023
-Original Grad: 0.079, -lr * Pred Grad:  0.065, New P: 1.520
iter 4 loss: 0.108
Actual params: [1.0226, 1.5198]
-Original Grad: 0.011, -lr * Pred Grad:  0.020, New P: 1.043
-Original Grad: 0.078, -lr * Pred Grad:  0.097, New P: 1.617
iter 5 loss: 0.085
Actual params: [1.0427, 1.6171]
-Original Grad: 0.015, -lr * Pred Grad:  0.021, New P: 1.063
-Original Grad: 0.041, -lr * Pred Grad:  0.060, New P: 1.677
iter 6 loss: 0.054
Actual params: [1.0633, 1.6774]
-Original Grad: 0.008, -lr * Pred Grad:  0.011, New P: 1.074
-Original Grad: 0.026, -lr * Pred Grad:  0.048, New P: 1.725
iter 7 loss: 0.057
Actual params: [1.0742, 1.7254]
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: 1.072
-Original Grad: -0.036, -lr * Pred Grad:  -0.042, New P: 1.684
iter 8 loss: 0.065
Actual params: [1.072 , 1.6837]
-Original Grad: 0.001, -lr * Pred Grad:  0.004, New P: 1.076
-Original Grad: 0.023, -lr * Pred Grad:  0.038, New P: 1.722
iter 9 loss: 0.058
Actual params: [1.0763, 1.7221]
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: 1.074
-Original Grad: -0.035, -lr * Pred Grad:  -0.040, New P: 1.682
iter 10 loss: 0.064
Actual params: [1.0744, 1.6822]
-Original Grad: -0.001, -lr * Pred Grad:  -0.000, New P: 1.074
-Original Grad: 0.023, -lr * Pred Grad:  0.039, New P: 1.721
iter 11 loss: 0.058
Actual params: [1.074 , 1.7207]
-Original Grad: -0.003, -lr * Pred Grad:  -0.017, New P: 1.057
-Original Grad: -0.024, -lr * Pred Grad:  -0.030, New P: 1.691
iter 12 loss: 0.064
Actual params: [1.0567, 1.6909]
-Original Grad: 0.007, -lr * Pred Grad:  0.015, New P: 1.071
-Original Grad: 0.016, -lr * Pred Grad:  0.025, New P: 1.716
iter 13 loss: 0.058
Actual params: [1.0712, 1.716 ]
-Original Grad: 0.009, -lr * Pred Grad:  0.016, New P: 1.087
-Original Grad: -0.015, -lr * Pred Grad:  -0.020, New P: 1.696
iter 14 loss: 0.063
Actual params: [1.0869, 1.6964]
-Original Grad: -0.009, -lr * Pred Grad:  -0.023, New P: 1.064
-Original Grad: 0.009, -lr * Pred Grad:  0.010, New P: 1.707
iter 15 loss: 0.060
Actual params: [1.0637, 1.7065]
-Original Grad: 0.010, -lr * Pred Grad:  0.015, New P: 1.079
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: 1.704
iter 16 loss: 0.061
Actual params: [1.0787, 1.7044]
-Original Grad: -0.008, -lr * Pred Grad:  -0.023, New P: 1.056
-Original Grad: -0.003, -lr * Pred Grad:  -0.006, New P: 1.699
iter 17 loss: 0.061
Actual params: [1.0558, 1.6985]
-Original Grad: 0.009, -lr * Pred Grad:  0.015, New P: 1.071
-Original Grad: 0.005, -lr * Pred Grad:  0.008, New P: 1.707
iter 18 loss: 0.059
Actual params: [1.071 , 1.7065]
-Original Grad: 0.007, -lr * Pred Grad:  0.012, New P: 1.083
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: 1.704
iter 19 loss: 0.061
Actual params: [1.0833, 1.704 ]
-Original Grad: -0.007, -lr * Pred Grad:  -0.023, New P: 1.061
-Original Grad: -0.003, -lr * Pred Grad:  -0.005, New P: 1.699
iter 20 loss: 0.061
Actual params: [1.0606, 1.6987]
-Original Grad: 0.009, -lr * Pred Grad:  0.014, New P: 1.074
-Original Grad: 0.005, -lr * Pred Grad:  0.008, New P: 1.706
iter 21 loss: 0.060
Actual params: [1.0745, 1.7065]
-Original Grad: 0.004, -lr * Pred Grad:  0.006, New P: 1.081
-Original Grad: 0.004, -lr * Pred Grad:  0.006, New P: 1.712
iter 22 loss: 0.061
Actual params: [1.0807, 1.7124]
-Original Grad: -0.005, -lr * Pred Grad:  -0.019, New P: 1.061
-Original Grad: -0.016, -lr * Pred Grad:  -0.022, New P: 1.691
iter 23 loss: 0.063
Actual params: [1.0614, 1.6906]
-Original Grad: 0.006, -lr * Pred Grad:  0.012, New P: 1.073
-Original Grad: 0.016, -lr * Pred Grad:  0.026, New P: 1.717
iter 24 loss: 0.058
Actual params: [1.073 , 1.7169]
-Original Grad: -0.002, -lr * Pred Grad:  -0.008, New P: 1.065
-Original Grad: -0.023, -lr * Pred Grad:  -0.027, New P: 1.690
iter 25 loss: 0.063
Actual params: [1.0649, 1.6895]
-Original Grad: 0.007, -lr * Pred Grad:  0.013, New P: 1.077
-Original Grad: 0.016, -lr * Pred Grad:  0.025, New P: 1.714
iter 26 loss: 0.058
Actual params: [1.0775, 1.7144]
-Original Grad: -0.003, -lr * Pred Grad:  -0.012, New P: 1.065
-Original Grad: -0.022, -lr * Pred Grad:  -0.026, New P: 1.688
iter 27 loss: 0.063
Actual params: [1.0654, 1.6882]
-Original Grad: 0.008, -lr * Pred Grad:  0.012, New P: 1.078
-Original Grad: 0.017, -lr * Pred Grad:  0.025, New P: 1.713
iter 28 loss: 0.058
Actual params: [1.0779, 1.7132]
-Original Grad: -0.005, -lr * Pred Grad:  -0.016, New P: 1.062
-Original Grad: -0.016, -lr * Pred Grad:  -0.022, New P: 1.691
iter 29 loss: 0.063
Actual params: [1.0623, 1.6913]
-Original Grad: 0.007, -lr * Pred Grad:  0.012, New P: 1.074
-Original Grad: 0.016, -lr * Pred Grad:  0.026, New P: 1.717
iter 30 loss: 0.058
Actual params: [1.074 , 1.7171]
Target params: [1.3344, 1.5708]
Actual params: [-1.0585,  1.4018]
-Original Grad: 0.052, -lr * Pred Grad:  0.934, New P: -0.124
-Original Grad: 0.032, -lr * Pred Grad:  0.572, New P: 1.974
iter 0 loss: 0.711
Actual params: [-0.1244,  1.9742]
-Original Grad: 0.530, -lr * Pred Grad:  0.358, New P: 0.234
-Original Grad: 0.022, -lr * Pred Grad:  -0.378, New P: 1.596
iter 1 loss: 0.522
Actual params: [0.2338, 1.5958]
-Original Grad: 0.403, -lr * Pred Grad:  0.036, New P: 0.269
-Original Grad: 0.211, -lr * Pred Grad:  0.425, New P: 2.021
iter 2 loss: 0.377
Actual params: [0.2695, 2.021 ]
-Original Grad: 0.266, -lr * Pred Grad:  0.225, New P: 0.494
-Original Grad: -0.026, -lr * Pred Grad:  -0.469, New P: 1.552
iter 3 loss: 0.263
Actual params: [0.4943, 1.5516]
-Original Grad: 0.260, -lr * Pred Grad:  0.036, New P: 0.530
-Original Grad: 0.170, -lr * Pred Grad:  0.194, New P: 1.745
iter 4 loss: 0.241
Actual params: [0.5299, 1.7452]
-Original Grad: 0.208, -lr * Pred Grad:  0.143, New P: 0.673
-Original Grad: 0.035, -lr * Pred Grad:  -0.188, New P: 1.557
iter 5 loss: 0.167
Actual params: [0.6729, 1.5571]
-Original Grad: 0.152, -lr * Pred Grad:  0.035, New P: 0.708
-Original Grad: 0.054, -lr * Pred Grad:  0.101, New P: 1.658
iter 6 loss: 0.144
Actual params: [0.7082, 1.6584]
-Original Grad: 0.105, -lr * Pred Grad:  0.087, New P: 0.796
-Original Grad: 0.019, -lr * Pred Grad:  -0.100, New P: 1.559
iter 7 loss: 0.103
Actual params: [0.7956, 1.5588]
-Original Grad: 0.086, -lr * Pred Grad:  0.035, New P: 0.831
-Original Grad: 0.020, -lr * Pred Grad:  0.122, New P: 1.681
iter 8 loss: 0.090
Actual params: [0.8308, 1.681 ]
-Original Grad: 0.008, -lr * Pred Grad:  0.067, New P: 0.898
-Original Grad: -0.027, -lr * Pred Grad:  -0.167, New P: 1.514
iter 9 loss: 0.104
Actual params: [0.8975, 1.5145]
-Original Grad: 0.045, -lr * Pred Grad:  0.092, New P: 0.989
-Original Grad: -0.008, -lr * Pred Grad:  -0.134, New P: 1.381
iter 10 loss: 0.090
Actual params: [0.9893, 1.3806]
-Original Grad: -0.042, -lr * Pred Grad:  -0.038, New P: 0.951
-Original Grad: 0.153, -lr * Pred Grad:  0.099, New P: 1.480
iter 11 loss: 0.119
Actual params: [0.9509, 1.4799]
-Original Grad: 0.020, -lr * Pred Grad:  0.016, New P: 0.966
-Original Grad: 0.014, -lr * Pred Grad:  0.013, New P: 1.493
iter 12 loss: 0.090
Actual params: [0.9665, 1.4933]
-Original Grad: 0.013, -lr * Pred Grad:  0.016, New P: 0.982
-Original Grad: 0.009, -lr * Pred Grad:  0.010, New P: 1.503
iter 13 loss: 0.089
Actual params: [0.9824, 1.503 ]
-Original Grad: 0.014, -lr * Pred Grad:  0.019, New P: 1.001
-Original Grad: -0.009, -lr * Pred Grad:  -0.010, New P: 1.493
iter 14 loss: 0.090
Actual params: [1.0012, 1.4925]
-Original Grad: 0.007, -lr * Pred Grad:  0.011, New P: 1.012
-Original Grad: 0.016, -lr * Pred Grad:  0.017, New P: 1.510
iter 15 loss: 0.091
Actual params: [1.0118, 1.5098]
-Original Grad: 0.001, -lr * Pred Grad:  -0.000, New P: 1.012
-Original Grad: -0.005, -lr * Pred Grad:  -0.005, New P: 1.505
iter 16 loss: 0.089
Actual params: [1.0118, 1.5046]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 1.012
-Original Grad: -0.004, -lr * Pred Grad:  -0.004, New P: 1.501
iter 17 loss: 0.090
Actual params: [1.0118, 1.5007]
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: 1.010
-Original Grad: -0.003, -lr * Pred Grad:  -0.003, New P: 1.498
iter 18 loss: 0.090
Actual params: [1.0101, 1.4979]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 1.011
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: 1.496
iter 19 loss: 0.090
Actual params: [1.0105, 1.496 ]
-Original Grad: -0.000, -lr * Pred Grad:  0.002, New P: 1.012
-Original Grad: 0.007, -lr * Pred Grad:  0.006, New P: 1.502
iter 20 loss: 0.090
Actual params: [1.0123, 1.5022]
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: 1.011
-Original Grad: -0.003, -lr * Pred Grad:  -0.003, New P: 1.499
iter 21 loss: 0.090
Actual params: [1.0107, 1.4991]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 1.011
-Original Grad: -0.002, -lr * Pred Grad:  -0.002, New P: 1.497
iter 22 loss: 0.090
Actual params: [1.011 , 1.4969]
-Original Grad: -0.018, -lr * Pred Grad:  -0.044, New P: 0.967
-Original Grad: 0.007, -lr * Pred Grad:  -0.003, New P: 1.494
iter 23 loss: 0.090
Actual params: [0.9668, 1.4938]
-Original Grad: 0.013, -lr * Pred Grad:  0.029, New P: 0.996
-Original Grad: 0.008, -lr * Pred Grad:  0.012, New P: 1.506
iter 24 loss: 0.089
Actual params: [0.996 , 1.5055]
-Original Grad: 0.027, -lr * Pred Grad:  0.022, New P: 1.018
-Original Grad: -0.011, -lr * Pred Grad:  -0.010, New P: 1.496
iter 25 loss: 0.090
Actual params: [1.0183, 1.496 ]
-Original Grad: -0.033, -lr * Pred Grad:  -0.037, New P: 0.981
-Original Grad: 0.011, -lr * Pred Grad:  0.008, New P: 1.504
iter 26 loss: 0.090
Actual params: [0.9815, 1.504 ]
-Original Grad: 0.015, -lr * Pred Grad:  0.019, New P: 1.000
-Original Grad: -0.009, -lr * Pred Grad:  -0.010, New P: 1.494
iter 27 loss: 0.089
Actual params: [1.0005, 1.4944]
-Original Grad: 0.009, -lr * Pred Grad:  0.010, New P: 1.011
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: 1.496
iter 28 loss: 0.090
Actual params: [1.0105, 1.4956]
-Original Grad: -0.003, -lr * Pred Grad:  -0.000, New P: 1.011
-Original Grad: 0.014, -lr * Pred Grad:  0.014, New P: 1.509
iter 29 loss: 0.090
Actual params: [1.0105, 1.5091]
-Original Grad: 0.001, -lr * Pred Grad:  0.000, New P: 1.011
-Original Grad: -0.005, -lr * Pred Grad:  -0.005, New P: 1.504
iter 30 loss: 0.090
Actual params: [1.0106, 1.5042]
Target params: [1.3344, 1.5708]
Actual params: [1.5477, 0.5327]
-Original Grad: -0.159, -lr * Pred Grad:  -0.204, New P: 1.344
-Original Grad: 0.378, -lr * Pred Grad:  0.535, New P: 1.067
iter 0 loss: 0.893
Actual params: [1.3437, 1.0672]
-Original Grad: -0.350, -lr * Pred Grad:  -0.253, New P: 1.091
-Original Grad: 0.201, -lr * Pred Grad:  0.217, New P: 1.285
iter 1 loss: 0.626
Actual params: [1.0906, 1.2847]
-Original Grad: 0.263, -lr * Pred Grad:  0.155, New P: 1.246
-Original Grad: 0.149, -lr * Pred Grad:  0.387, New P: 1.671
iter 2 loss: 0.471
Actual params: [1.2459, 1.6714]
-Original Grad: 0.079, -lr * Pred Grad:  0.046, New P: 1.291
-Original Grad: 0.139, -lr * Pred Grad:  0.382, New P: 2.053
iter 3 loss: 0.330
Actual params: [1.2915, 2.0535]
-Original Grad: 0.348, -lr * Pred Grad:  0.158, New P: 1.450
-Original Grad: -0.128, -lr * Pred Grad:  0.061, New P: 2.115
iter 4 loss: 0.277
Actual params: [1.4495, 2.1149]
-Original Grad: -0.038, -lr * Pred Grad:  0.058, New P: 1.508
-Original Grad: 0.135, -lr * Pred Grad:  0.189, New P: 2.304
iter 5 loss: 0.246
Actual params: [1.5078, 2.3042]
-Original Grad: 0.017, -lr * Pred Grad:  -0.008, New P: 1.500
-Original Grad: -0.124, -lr * Pred Grad:  -0.083, New P: 2.221
iter 6 loss: 0.228
Actual params: [1.5   , 2.2211]
-Original Grad: -0.054, -lr * Pred Grad:  -0.007, New P: 1.493
-Original Grad: 0.067, -lr * Pred Grad:  0.061, New P: 2.282
iter 7 loss: 0.244
Actual params: [1.4933, 2.2818]
-Original Grad: 0.094, -lr * Pred Grad:  0.007, New P: 1.500
-Original Grad: -0.073, -lr * Pred Grad:  -0.052, New P: 2.230
iter 8 loss: 0.225
Actual params: [1.5003, 2.2301]
-Original Grad: -0.052, -lr * Pred Grad:  -0.006, New P: 1.494
-Original Grad: 0.066, -lr * Pred Grad:  0.065, New P: 2.295
iter 9 loss: 0.242
Actual params: [1.494 , 2.2954]
-Original Grad: 0.096, -lr * Pred Grad:  0.007, New P: 1.501
-Original Grad: -0.082, -lr * Pred Grad:  -0.059, New P: 2.236
iter 10 loss: 0.222
Actual params: [1.5008, 2.2363]
-Original Grad: -0.068, -lr * Pred Grad:  -0.013, New P: 1.488
-Original Grad: 0.061, -lr * Pred Grad:  0.057, New P: 2.293
iter 11 loss: 0.240
Actual params: [1.4878, 2.293 ]
-Original Grad: 0.238, -lr * Pred Grad:  0.015, New P: 1.503
-Original Grad: -0.101, -lr * Pred Grad:  -0.049, New P: 2.244
iter 12 loss: 0.220
Actual params: [1.5026, 2.2438]
-Original Grad: -0.045, -lr * Pred Grad:  -0.008, New P: 1.495
-Original Grad: 0.023, -lr * Pred Grad:  0.024, New P: 2.268
iter 13 loss: 0.240
Actual params: [1.4948, 2.2679]
-Original Grad: 0.167, -lr * Pred Grad:  0.029, New P: 1.523
-Original Grad: -0.034, -lr * Pred Grad:  0.000, New P: 2.268
iter 14 loss: 0.229
Actual params: [1.5235, 2.2683]
-Original Grad: -0.173, -lr * Pred Grad:  -0.035, New P: 1.489
-Original Grad: -0.039, -lr * Pred Grad:  -0.027, New P: 2.241
iter 15 loss: 0.250
Actual params: [1.4887, 2.2409]
-Original Grad: 0.187, -lr * Pred Grad:  0.042, New P: 1.530
-Original Grad: -0.019, -lr * Pred Grad:  0.021, New P: 2.262
iter 16 loss: 0.232
Actual params: [1.5303, 2.2615]
-Original Grad: -0.172, -lr * Pred Grad:  -0.039, New P: 1.491
-Original Grad: -0.036, -lr * Pred Grad:  -0.023, New P: 2.239
iter 17 loss: 0.258
Actual params: [1.4915, 2.2389]
-Original Grad: 0.167, -lr * Pred Grad:  0.037, New P: 1.528
-Original Grad: -0.017, -lr * Pred Grad:  0.016, New P: 2.255
iter 18 loss: 0.233
Actual params: [1.5281, 2.2553]
-Original Grad: -0.199, -lr * Pred Grad:  -0.042, New P: 1.486
-Original Grad: 0.008, -lr * Pred Grad:  0.007, New P: 2.262
iter 19 loss: 0.258
Actual params: [1.4863, 2.2624]
-Original Grad: 0.315, -lr * Pred Grad:  0.040, New P: 1.527
-Original Grad: -0.056, -lr * Pred Grad:  0.027, New P: 2.289
iter 20 loss: 0.226
Actual params: [1.5266, 2.2894]
-Original Grad: -0.188, -lr * Pred Grad:  -0.030, New P: 1.497
-Original Grad: -0.056, -lr * Pred Grad:  -0.048, New P: 2.242
iter 21 loss: 0.247
Actual params: [1.4965, 2.2415]
-Original Grad: 0.149, -lr * Pred Grad:  0.024, New P: 1.521
-Original Grad: -0.017, -lr * Pred Grad:  0.008, New P: 2.250
iter 22 loss: 0.236
Actual params: [1.5207, 2.2495]
-Original Grad: -0.180, -lr * Pred Grad:  -0.034, New P: 1.487
-Original Grad: 0.017, -lr * Pred Grad:  0.011, New P: 2.260
iter 23 loss: 0.252
Actual params: [1.4871, 2.2603]
-Original Grad: 0.179, -lr * Pred Grad:  0.037, New P: 1.524
-Original Grad: -0.032, -lr * Pred Grad:  0.007, New P: 2.268
iter 24 loss: 0.226
Actual params: [1.5237, 2.2677]
-Original Grad: -0.173, -lr * Pred Grad:  -0.037, New P: 1.486
-Original Grad: -0.039, -lr * Pred Grad:  -0.024, New P: 2.243
iter 25 loss: 0.250
Actual params: [1.4865, 2.2433]
-Original Grad: 0.187, -lr * Pred Grad:  0.043, New P: 1.530
-Original Grad: -0.021, -lr * Pred Grad:  0.018, New P: 2.262
iter 26 loss: 0.230
Actual params: [1.53  , 2.2617]
-Original Grad: -0.172, -lr * Pred Grad:  -0.040, New P: 1.490
-Original Grad: -0.036, -lr * Pred Grad:  -0.021, New P: 2.241
iter 27 loss: 0.258
Actual params: [1.49 , 2.241]
-Original Grad: 0.174, -lr * Pred Grad:  0.037, New P: 1.527
-Original Grad: -0.025, -lr * Pred Grad:  0.010, New P: 2.251
iter 28 loss: 0.232
Actual params: [1.5272, 2.2508]
-Original Grad: -0.217, -lr * Pred Grad:  -0.040, New P: 1.487
-Original Grad: 0.042, -lr * Pred Grad:  0.027, New P: 2.277
iter 29 loss: 0.258
Actual params: [1.4869, 2.2773]
-Original Grad: 0.329, -lr * Pred Grad:  0.042, New P: 1.529
-Original Grad: -0.062, -lr * Pred Grad:  0.029, New P: 2.306
iter 30 loss: 0.223
Actual params: [1.5288, 2.3061]
Target params: [1.3344, 1.5708]
Actual params: [0.0029, 0.9353]
-Original Grad: 0.105, -lr * Pred Grad:  0.414, New P: 0.417
-Original Grad: 0.074, -lr * Pred Grad:  0.448, New P: 1.383
iter 0 loss: 0.426
Actual params: [0.4167, 1.3835]
-Original Grad: 0.104, -lr * Pred Grad:  0.518, New P: 0.934
-Original Grad: -0.006, -lr * Pred Grad:  -0.341, New P: 1.042
iter 1 loss: 0.242
Actual params: [0.9344, 1.0425]
-Original Grad: -0.054, -lr * Pred Grad:  0.091, New P: 1.025
-Original Grad: 0.471, -lr * Pred Grad:  0.228, New P: 1.270
iter 2 loss: 0.299
Actual params: [1.0251, 1.2702]
-Original Grad: 0.096, -lr * Pred Grad:  0.211, New P: 1.237
-Original Grad: 0.164, -lr * Pred Grad:  0.079, New P: 1.349
iter 3 loss: 0.191
Actual params: [1.2365, 1.3488]
-Original Grad: 0.009, -lr * Pred Grad:  0.006, New P: 1.242
-Original Grad: 0.024, -lr * Pred Grad:  0.023, New P: 1.372
iter 4 loss: 0.144
Actual params: [1.2421, 1.3721]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: 1.222
-Original Grad: 0.009, -lr * Pred Grad:  0.027, New P: 1.399
iter 5 loss: 0.138
Actual params: [1.2217, 1.3989]
-Original Grad: -0.002, -lr * Pred Grad:  -0.034, New P: 1.188
-Original Grad: 0.005, -lr * Pred Grad:  0.037, New P: 1.436
iter 6 loss: 0.135
Actual params: [1.1878, 1.4357]
-Original Grad: -0.005, -lr * Pred Grad:  -0.074, New P: 1.114
-Original Grad: 0.004, -lr * Pred Grad:  0.059, New P: 1.495
iter 7 loss: 0.133
Actual params: [1.1138, 1.4951]
-Original Grad: -0.006, -lr * Pred Grad:  0.004, New P: 1.118
-Original Grad: -0.011, -lr * Pred Grad:  -0.036, New P: 1.459
iter 8 loss: 0.134
Actual params: [1.1176, 1.4588]
-Original Grad: 0.001, -lr * Pred Grad:  -0.051, New P: 1.066
-Original Grad: 0.008, -lr * Pred Grad:  0.052, New P: 1.511
iter 9 loss: 0.131
Actual params: [1.0664, 1.5109]
-Original Grad: 0.003, -lr * Pred Grad:  0.216, New P: 1.282
-Original Grad: -0.015, -lr * Pred Grad:  -0.138, New P: 1.373
iter 10 loss: 0.137
Actual params: [1.282 , 1.3732]
-Original Grad: -0.016, -lr * Pred Grad:  -0.186, New P: 1.096
-Original Grad: -0.004, -lr * Pred Grad:  0.089, New P: 1.462
iter 11 loss: 0.138
Actual params: [1.0958, 1.4623]
-Original Grad: 0.002, -lr * Pred Grad:  -0.094, New P: 1.001
-Original Grad: 0.011, -lr * Pred Grad:  0.084, New P: 1.546
iter 12 loss: 0.133
Actual params: [1.0014, 1.5463]
-Original Grad: 0.041, -lr * Pred Grad:  0.129, New P: 1.130
-Original Grad: -0.010, -lr * Pred Grad:  -0.032, New P: 1.515
iter 13 loss: 0.145
Actual params: [1.1301, 1.5146]
-Original Grad: -0.006, -lr * Pred Grad:  -0.013, New P: 1.117
-Original Grad: -0.017, -lr * Pred Grad:  -0.045, New P: 1.470
iter 14 loss: 0.138
Actual params: [1.1166, 1.4698]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: 1.099
-Original Grad: 0.003, -lr * Pred Grad:  0.015, New P: 1.485
iter 15 loss: 0.131
Actual params: [1.0993, 1.4846]
-Original Grad: -0.005, -lr * Pred Grad:  0.008, New P: 1.107
-Original Grad: -0.013, -lr * Pred Grad:  -0.040, New P: 1.445
iter 16 loss: 0.133
Actual params: [1.1073, 1.4449]
-Original Grad: 0.000, -lr * Pred Grad:  -0.112, New P: 0.995
-Original Grad: 0.013, -lr * Pred Grad:  0.091, New P: 1.536
iter 17 loss: 0.132
Actual params: [0.9955, 1.5359]
-Original Grad: 0.046, -lr * Pred Grad:  0.123, New P: 1.119
-Original Grad: -0.012, -lr * Pred Grad:  -0.031, New P: 1.505
iter 18 loss: 0.143
Actual params: [1.1187, 1.5051]
-Original Grad: -0.001, -lr * Pred Grad:  0.014, New P: 1.133
-Original Grad: -0.014, -lr * Pred Grad:  -0.042, New P: 1.463
iter 19 loss: 0.136
Actual params: [1.1325, 1.463 ]
-Original Grad: -0.006, -lr * Pred Grad:  -0.077, New P: 1.056
-Original Grad: 0.005, -lr * Pred Grad:  0.048, New P: 1.512
iter 20 loss: 0.131
Actual params: [1.0558, 1.5115]
-Original Grad: -0.002, -lr * Pred Grad:  0.072, New P: 1.128
-Original Grad: -0.013, -lr * Pred Grad:  -0.068, New P: 1.444
iter 21 loss: 0.137
Actual params: [1.1276, 1.4436]
-Original Grad: -0.005, -lr * Pred Grad:  -0.222, New P: 0.905
-Original Grad: 0.009, -lr * Pred Grad:  0.150, New P: 1.593
iter 22 loss: 0.131
Actual params: [0.9051, 1.5932]
-Original Grad: 0.095, -lr * Pred Grad:  0.167, New P: 1.072
-Original Grad: -0.047, -lr * Pred Grad:  -0.050, New P: 1.543
iter 23 loss: 0.158
Actual params: [1.0718, 1.5431]
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: 1.070
-Original Grad: -0.013, -lr * Pred Grad:  -0.031, New P: 1.512
iter 24 loss: 0.143
Actual params: [1.07  , 1.5122]
-Original Grad: -0.007, -lr * Pred Grad:  -0.027, New P: 1.043
-Original Grad: -0.014, -lr * Pred Grad:  -0.030, New P: 1.482
iter 25 loss: 0.137
Actual params: [1.0432, 1.4823]
-Original Grad: 0.026, -lr * Pred Grad:  0.193, New P: 1.236
-Original Grad: 0.024, -lr * Pred Grad:  -0.039, New P: 1.443
iter 26 loss: 0.137
Actual params: [1.2363, 1.4432]
-Original Grad: -0.008, -lr * Pred Grad:  -0.166, New P: 1.070
-Original Grad: 0.001, -lr * Pred Grad:  0.085, New P: 1.528
iter 27 loss: 0.140
Actual params: [1.0703, 1.5279]
-Original Grad: -0.002, -lr * Pred Grad:  0.141, New P: 1.212
-Original Grad: -0.016, -lr * Pred Grad:  -0.108, New P: 1.420
iter 28 loss: 0.140
Actual params: [1.2116, 1.4198]
-Original Grad: -0.003, -lr * Pred Grad:  -0.030, New P: 1.182
-Original Grad: -0.002, -lr * Pred Grad:  0.010, New P: 1.430
iter 29 loss: 0.135
Actual params: [1.1816, 1.4296]
-Original Grad: -0.002, -lr * Pred Grad:  -0.085, New P: 1.097
-Original Grad: 0.006, -lr * Pred Grad:  0.072, New P: 1.501
iter 30 loss: 0.133
Actual params: [1.097 , 1.5012]
Target params: [1.3344, 1.5708]
Actual params: [-0.6756, -1.5044]
-Original Grad: -0.037, -lr * Pred Grad:  -0.799, New P: -1.475
-Original Grad: 0.004, -lr * Pred Grad:  0.088, New P: -1.416
iter 0 loss: 0.728
Actual params: [-1.4749, -1.4164]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -1.476
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.416
iter 1 loss: 0.726
Actual params: [-1.4756, -1.4164]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -1.477
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.416
iter 2 loss: 0.726
Actual params: [-1.4773, -1.4165]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.481
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.417
iter 3 loss: 0.726
Actual params: [-1.4815, -1.4167]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.491
-Original Grad: -0.000, -lr * Pred Grad:  -0.000, New P: -1.417
iter 4 loss: 0.726
Actual params: [-1.4915, -1.4172]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.514
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: -1.418
iter 5 loss: 0.726
Actual params: [-1.5142, -1.4183]
-Original Grad: -0.000, -lr * Pred Grad:  -0.046, New P: -1.560
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -1.421
iter 6 loss: 0.726
Actual params: [-1.5599, -1.4207]
-Original Grad: -0.000, -lr * Pred Grad:  -0.074, New P: -1.633
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.424
iter 7 loss: 0.726
Actual params: [-1.6334, -1.4245]
-Original Grad: -0.000, -lr * Pred Grad:  -0.090, New P: -1.724
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.429
iter 8 loss: 0.726
Actual params: [-1.7239, -1.4291]
-Original Grad: -0.000, -lr * Pred Grad:  -0.095, New P: -1.819
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.434
iter 9 loss: 0.726
Actual params: [-1.8185, -1.4338]
-Original Grad: -0.000, -lr * Pred Grad:  -0.096, New P: -1.914
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.439
iter 10 loss: 0.726
Actual params: [-1.914 , -1.4387]
-Original Grad: -0.000, -lr * Pred Grad:  -0.095, New P: -2.009
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.444
iter 11 loss: 0.726
Actual params: [-2.009 , -1.4437]
-Original Grad: -0.000, -lr * Pred Grad:  -0.091, New P: -2.100
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -1.446
iter 12 loss: 0.726
Actual params: [-2.0996, -1.4465]
-Original Grad: -0.000, -lr * Pred Grad:  -0.097, New P: -2.197
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -1.448
iter 13 loss: 0.726
Actual params: [-2.1969, -1.4482]
-Original Grad: -0.000, -lr * Pred Grad:  -0.102, New P: -2.299
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -1.451
iter 14 loss: 0.726
Actual params: [-2.2989, -1.4505]
-Original Grad: -0.000, -lr * Pred Grad:  -0.102, New P: -2.401
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -1.453
iter 15 loss: 0.726
Actual params: [-2.4011, -1.4528]
-Original Grad: -0.000, -lr * Pred Grad:  -0.102, New P: -2.503
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -1.456
iter 16 loss: 0.726
Actual params: [-2.5035, -1.456 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.102, New P: -2.605
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -1.459
iter 17 loss: 0.726
Actual params: [-2.6051, -1.4588]
-Original Grad: -0.000, -lr * Pred Grad:  -0.101, New P: -2.706
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -1.461
iter 18 loss: 0.726
Actual params: [-2.7063, -1.461 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.102, New P: -2.808
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -1.462
iter 19 loss: 0.726
Actual params: [-2.808 , -1.4621]
-Original Grad: -0.000, -lr * Pred Grad:  -0.106, New P: -2.914
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -1.463
iter 20 loss: 0.726
Actual params: [-2.9136, -1.463 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.107, New P: -3.021
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -1.464
iter 21 loss: 0.726
Actual params: [-3.0207, -1.4641]
-Original Grad: -0.000, -lr * Pred Grad:  -0.107, New P: -3.128
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -1.465
iter 22 loss: 0.726
Actual params: [-3.1277, -1.4651]
-Original Grad: -0.000, -lr * Pred Grad:  -0.107, New P: -3.235
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -1.467
iter 23 loss: 0.726
Actual params: [-3.2348, -1.4666]
-Original Grad: -0.000, -lr * Pred Grad:  -0.107, New P: -3.342
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -1.468
iter 24 loss: 0.726
Actual params: [-3.3417, -1.4679]
-Original Grad: -0.000, -lr * Pred Grad:  -0.107, New P: -3.449
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: -1.469
iter 25 loss: 0.726
Actual params: [-3.4485, -1.4695]
-Original Grad: -0.000, -lr * Pred Grad:  -0.107, New P: -3.555
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -1.471
iter 26 loss: 0.726
Actual params: [-3.5553, -1.4709]
-Original Grad: -0.000, -lr * Pred Grad:  -0.107, New P: -3.662
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -1.472
iter 27 loss: 0.726
Actual params: [-3.662 , -1.4718]
-Original Grad: -0.000, -lr * Pred Grad:  -0.107, New P: -3.769
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: -1.472
iter 28 loss: 0.726
Actual params: [-3.7689, -1.4721]
-Original Grad: -0.000, -lr * Pred Grad:  -0.107, New P: -3.876
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -1.472
iter 29 loss: 0.726
Actual params: [-3.8758, -1.472 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.107, New P: -3.983
-Original Grad: 0.000, -lr * Pred Grad:  0.000, New P: -1.472
iter 30 loss: 0.726
Actual params: [-3.9828, -1.4715]
Target params: [1.3344, 1.5708]
Actual params: [-0.6634, -0.2295]
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: -0.654
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.227
iter 0 loss: 0.552
Actual params: [-0.654 , -0.2274]
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: -0.629
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -0.221
iter 1 loss: 0.552
Actual params: [-0.6286, -0.2214]
-Original Grad: 0.001, -lr * Pred Grad:  0.080, New P: -0.548
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: -0.203
iter 2 loss: 0.552
Actual params: [-0.5484, -0.2029]
-Original Grad: 0.001, -lr * Pred Grad:  0.415, New P: -0.133
-Original Grad: 0.000, -lr * Pred Grad:  0.095, New P: -0.107
iter 3 loss: 0.552
Actual params: [-0.1332, -0.1074]
-Original Grad: 0.026, -lr * Pred Grad:  2.887, New P: 2.754
-Original Grad: 0.006, -lr * Pred Grad:  0.685, New P: 0.578
iter 4 loss: 0.549
Actual params: [2.7541, 0.5779]
-Original Grad: -0.036, -lr * Pred Grad:  -1.257, New P: 1.497
-Original Grad: 0.006, -lr * Pred Grad:  0.502, New P: 1.080
iter 5 loss: 0.195
Actual params: [1.4967, 1.0799]
-Original Grad: 0.221, -lr * Pred Grad:  0.386, New P: 1.883
-Original Grad: 0.268, -lr * Pred Grad:  -0.166, New P: 0.914
iter 6 loss: 0.235
Actual params: [1.8826, 0.9138]
-Original Grad: 0.014, -lr * Pred Grad:  -0.314, New P: 1.569
-Original Grad: 0.031, -lr * Pred Grad:  0.283, New P: 1.197
iter 7 loss: 0.187
Actual params: [1.5691, 1.1971]
-Original Grad: 0.031, -lr * Pred Grad:  -0.318, New P: 1.251
-Original Grad: 0.101, -lr * Pred Grad:  0.243, New P: 1.440
iter 8 loss: 0.166
Actual params: [1.2508, 1.44  ]
-Original Grad: -0.008, -lr * Pred Grad:  -0.135, New P: 1.116
-Original Grad: 0.002, -lr * Pred Grad:  0.086, New P: 1.526
iter 9 loss: 0.142
Actual params: [1.1156, 1.5261]
-Original Grad: -0.005, -lr * Pred Grad:  0.054, New P: 1.169
-Original Grad: -0.013, -lr * Pred Grad:  -0.051, New P: 1.475
iter 10 loss: 0.139
Actual params: [1.1693, 1.4753]
-Original Grad: -0.006, -lr * Pred Grad:  -0.075, New P: 1.094
-Original Grad: -0.006, -lr * Pred Grad:  0.027, New P: 1.502
iter 11 loss: 0.135
Actual params: [1.0944, 1.5023]
-Original Grad: -0.004, -lr * Pred Grad:  0.076, New P: 1.170
-Original Grad: -0.013, -lr * Pred Grad:  -0.072, New P: 1.430
iter 12 loss: 0.135
Actual params: [1.1701, 1.4305]
-Original Grad: 0.002, -lr * Pred Grad:  -0.115, New P: 1.055
-Original Grad: 0.013, -lr * Pred Grad:  0.110, New P: 1.541
iter 13 loss: 0.132
Actual params: [1.0553, 1.541 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.243, New P: 1.299
-Original Grad: -0.015, -lr * Pred Grad:  -0.162, New P: 1.379
iter 14 loss: 0.143
Actual params: [1.2988, 1.3786]
-Original Grad: -0.013, -lr * Pred Grad:  -0.194, New P: 1.105
-Original Grad: 0.000, -lr * Pred Grad:  0.113, New P: 1.492
iter 15 loss: 0.139
Actual params: [1.1049, 1.4916]
-Original Grad: -0.006, -lr * Pred Grad:  -0.013, New P: 1.092
-Original Grad: -0.011, -lr * Pred Grad:  -0.028, New P: 1.464
iter 16 loss: 0.133
Actual params: [1.0919, 1.4639]
-Original Grad: 0.006, -lr * Pred Grad:  0.008, New P: 1.100
-Original Grad: 0.012, -lr * Pred Grad:  0.025, New P: 1.489
iter 17 loss: 0.133
Actual params: [1.0999, 1.4891]
-Original Grad: -0.005, -lr * Pred Grad:  0.021, New P: 1.121
-Original Grad: -0.013, -lr * Pred Grad:  -0.044, New P: 1.445
iter 18 loss: 0.133
Actual params: [1.1211, 1.4452]
-Original Grad: -0.001, -lr * Pred Grad:  -0.249, New P: 0.872
-Original Grad: 0.014, -lr * Pred Grad:  0.172, New P: 1.617
iter 19 loss: 0.131
Actual params: [0.8721, 1.6173]
-Original Grad: 0.105, -lr * Pred Grad:  0.135, New P: 1.007
-Original Grad: -0.072, -lr * Pred Grad:  -0.087, New P: 1.530
iter 20 loss: 0.164
Actual params: [1.007 , 1.5302]
-Original Grad: 0.048, -lr * Pred Grad:  0.096, New P: 1.103
-Original Grad: -0.014, -lr * Pred Grad:  -0.007, New P: 1.524
iter 21 loss: 0.142
Actual params: [1.103 , 1.5236]
-Original Grad: -0.003, -lr * Pred Grad:  -0.008, New P: 1.095
-Original Grad: -0.014, -lr * Pred Grad:  -0.035, New P: 1.489
iter 22 loss: 0.139
Actual params: [1.0952, 1.4887]
-Original Grad: -0.006, -lr * Pred Grad:  -0.014, New P: 1.081
-Original Grad: -0.013, -lr * Pred Grad:  -0.029, New P: 1.459
iter 23 loss: 0.133
Actual params: [1.0812, 1.4593]
-Original Grad: 0.005, -lr * Pred Grad:  0.011, New P: 1.092
-Original Grad: 0.010, -lr * Pred Grad:  0.019, New P: 1.478
iter 24 loss: 0.134
Actual params: [1.0917, 1.4781]
-Original Grad: 0.002, -lr * Pred Grad:  -0.010, New P: 1.082
-Original Grad: 0.006, -lr * Pred Grad:  0.019, New P: 1.497
iter 25 loss: 0.133
Actual params: [1.0821, 1.497 ]
-Original Grad: -0.001, -lr * Pred Grad:  0.161, New P: 1.243
-Original Grad: -0.014, -lr * Pred Grad:  -0.106, New P: 1.391
iter 26 loss: 0.135
Actual params: [1.2427, 1.3913]
-Original Grad: -0.000, -lr * Pred Grad:  -0.067, New P: 1.176
-Original Grad: 0.009, -lr * Pred Grad:  0.075, New P: 1.467
iter 27 loss: 0.136
Actual params: [1.1761, 1.4667]
-Original Grad: -0.005, -lr * Pred Grad:  -0.027, New P: 1.149
-Original Grad: -0.005, -lr * Pred Grad:  -0.003, New P: 1.464
iter 28 loss: 0.135
Actual params: [1.1494, 1.464 ]
-Original Grad: -0.008, -lr * Pred Grad:  -0.088, New P: 1.061
-Original Grad: -0.006, -lr * Pred Grad:  0.027, New P: 1.491
iter 29 loss: 0.132
Actual params: [1.061 , 1.4913]
-Original Grad: 0.010, -lr * Pred Grad:  0.138, New P: 1.199
-Original Grad: 0.004, -lr * Pred Grad:  -0.064, New P: 1.427
iter 30 loss: 0.135
Actual params: [1.1992, 1.4272]
Target params: [1.3344, 1.5708]
Actual params: [-0.8962,  0.1733]
-Original Grad: 0.065, -lr * Pred Grad:  0.949, New P: 0.053
-Original Grad: -0.016, -lr * Pred Grad:  -0.192, New P: -0.019
iter 0 loss: 0.361
Actual params: [ 0.053 , -0.0186]
-Original Grad: 0.159, -lr * Pred Grad:  0.391, New P: 0.444
-Original Grad: 0.082, -lr * Pred Grad:  0.313, New P: 0.295
iter 1 loss: 0.201
Actual params: [0.4445, 0.2948]
-Original Grad: -0.843, -lr * Pred Grad:  -0.002, New P: 0.443
-Original Grad: -0.475, -lr * Pred Grad:  -0.261, New P: 0.034
iter 2 loss: 0.256
Actual params: [0.4427, 0.0335]
-Original Grad: -0.355, -lr * Pred Grad:  -0.082, New P: 0.361
-Original Grad: -0.117, -lr * Pred Grad:  0.062, New P: 0.095
iter 3 loss: 0.215
Actual params: [0.3606, 0.095 ]
-Original Grad: -0.268, -lr * Pred Grad:  -0.113, New P: 0.248
-Original Grad: -0.032, -lr * Pred Grad:  0.205, New P: 0.300
iter 4 loss: 0.195
Actual params: [0.248 , 0.2998]
-Original Grad: -0.066, -lr * Pred Grad:  0.070, New P: 0.318
-Original Grad: -0.068, -lr * Pred Grad:  -0.261, New P: 0.039
iter 5 loss: 0.184
Actual params: [0.3177, 0.0392]
-Original Grad: -0.130, -lr * Pred Grad:  -0.133, New P: 0.185
-Original Grad: 0.027, -lr * Pred Grad:  0.398, New P: 0.438
iter 6 loss: 0.189
Actual params: [0.1852, 0.4376]
-Original Grad: 0.048, -lr * Pred Grad:  0.114, New P: 0.299
-Original Grad: -0.058, -lr * Pred Grad:  -0.464, New P: -0.027
iter 7 loss: 0.187
Actual params: [ 0.2989, -0.0266]
-Original Grad: -0.047, -lr * Pred Grad:  -0.132, New P: 0.167
-Original Grad: 0.042, -lr * Pred Grad:  0.463, New P: 0.436
iter 8 loss: 0.189
Actual params: [0.1667, 0.4361]
-Original Grad: 0.187, -lr * Pred Grad:  0.159, New P: 0.325
-Original Grad: -0.043, -lr * Pred Grad:  -0.407, New P: 0.029
iter 9 loss: 0.184
Actual params: [0.3255, 0.0293]
-Original Grad: -0.134, -lr * Pred Grad:  -0.120, New P: 0.206
-Original Grad: 0.026, -lr * Pred Grad:  0.395, New P: 0.424
iter 10 loss: 0.191
Actual params: [0.2055, 0.4243]
-Original Grad: -0.092, -lr * Pred Grad:  0.093, New P: 0.299
-Original Grad: -0.091, -lr * Pred Grad:  -0.464, New P: -0.040
iter 11 loss: 0.189
Actual params: [ 0.2988, -0.0398]
-Original Grad: 0.011, -lr * Pred Grad:  -0.128, New P: 0.170
-Original Grad: 0.068, -lr * Pred Grad:  0.504, New P: 0.464
iter 12 loss: 0.189
Actual params: [0.1704, 0.4638]
-Original Grad: 0.029, -lr * Pred Grad:  0.074, New P: 0.245
-Original Grad: -0.065, -lr * Pred Grad:  -0.449, New P: 0.015
iter 13 loss: 0.187
Actual params: [0.2445, 0.0152]
-Original Grad: 0.121, -lr * Pred Grad:  -0.048, New P: 0.196
-Original Grad: 0.077, -lr * Pred Grad:  0.467, New P: 0.482
iter 14 loss: 0.182
Actual params: [0.1962, 0.4821]
-Original Grad: -0.115, -lr * Pred Grad:  0.100, New P: 0.296
-Original Grad: -0.117, -lr * Pred Grad:  -0.513, New P: -0.031
iter 15 loss: 0.193
Actual params: [ 0.296 , -0.0314]
-Original Grad: -0.043, -lr * Pred Grad:  -0.112, New P: 0.184
-Original Grad: 0.044, -lr * Pred Grad:  0.401, New P: 0.370
iter 16 loss: 0.188
Actual params: [0.1842, 0.3698]
-Original Grad: 0.162, -lr * Pred Grad:  0.138, New P: 0.322
-Original Grad: -0.028, -lr * Pred Grad:  -0.334, New P: 0.036
iter 17 loss: 0.181
Actual params: [0.3218, 0.0357]
-Original Grad: -0.132, -lr * Pred Grad:  -0.121, New P: 0.201
-Original Grad: 0.027, -lr * Pred Grad:  0.402, New P: 0.438
iter 18 loss: 0.190
Actual params: [0.201 , 0.4377]
-Original Grad: -0.103, -lr * Pred Grad:  0.098, New P: 0.299
-Original Grad: -0.099, -lr * Pred Grad:  -0.491, New P: -0.054
iter 19 loss: 0.189
Actual params: [ 0.2986, -0.0536]
-Original Grad: 0.012, -lr * Pred Grad:  -0.126, New P: 0.172
-Original Grad: 0.070, -lr * Pred Grad:  0.497, New P: 0.443
iter 20 loss: 0.189
Actual params: [0.1725, 0.4434]
-Original Grad: 0.192, -lr * Pred Grad:  0.161, New P: 0.333
-Original Grad: -0.038, -lr * Pred Grad:  -0.422, New P: 0.022
iter 21 loss: 0.186
Actual params: [0.3335, 0.0216]
-Original Grad: -0.135, -lr * Pred Grad:  -0.124, New P: 0.210
-Original Grad: 0.025, -lr * Pred Grad:  0.385, New P: 0.407
iter 22 loss: 0.192
Actual params: [0.2099, 0.4069]
-Original Grad: -0.090, -lr * Pred Grad:  0.080, New P: 0.290
-Original Grad: -0.091, -lr * Pred Grad:  -0.403, New P: 0.004
iter 23 loss: 0.188
Actual params: [0.2899, 0.0043]
-Original Grad: -0.056, -lr * Pred Grad:  -0.121, New P: 0.169
-Original Grad: 0.047, -lr * Pred Grad:  0.465, New P: 0.469
iter 24 loss: 0.186
Actual params: [0.1691, 0.4695]
-Original Grad: 0.059, -lr * Pred Grad:  0.077, New P: 0.246
-Original Grad: -0.071, -lr * Pred Grad:  -0.467, New P: 0.002
iter 25 loss: 0.187
Actual params: [0.2463, 0.0021]
-Original Grad: 0.072, -lr * Pred Grad:  -0.064, New P: 0.182
-Original Grad: 0.078, -lr * Pred Grad:  0.494, New P: 0.496
iter 26 loss: 0.183
Actual params: [0.1822, 0.4965]
-Original Grad: -0.037, -lr * Pred Grad:  -0.000, New P: 0.182
-Original Grad: 0.011, -lr * Pred Grad:  0.018, New P: 0.514
iter 27 loss: 0.192
Actual params: [0.1817, 0.5143]
-Original Grad: -0.258, -lr * Pred Grad:  -0.018, New P: 0.164
-Original Grad: -0.023, -lr * Pred Grad:  -0.065, New P: 0.450
iter 28 loss: 0.192
Actual params: [0.1641, 0.4497]
-Original Grad: 0.191, -lr * Pred Grad:  0.014, New P: 0.178
-Original Grad: -0.039, -lr * Pred Grad:  -0.067, New P: 0.383
iter 29 loss: 0.185
Actual params: [0.1784, 0.3827]
-Original Grad: 0.170, -lr * Pred Grad:  0.033, New P: 0.211
-Original Grad: -0.031, -lr * Pred Grad:  -0.078, New P: 0.305
iter 30 loss: 0.181
Actual params: [0.2114, 0.3046]
Target params: [1.3344, 1.5708]
Actual params: [1.5544, 0.3381]
-Original Grad: 0.466, -lr * Pred Grad:  -0.075, New P: 1.479
-Original Grad: 1.529, -lr * Pred Grad:  0.179, New P: 0.517
iter 0 loss: 0.552
Actual params: [1.479, 0.517]
-Original Grad: 0.169, -lr * Pred Grad:  -0.139, New P: 1.340
-Original Grad: 1.039, -lr * Pred Grad:  0.127, New P: 0.644
iter 1 loss: 0.424
Actual params: [1.34  , 0.6436]
-Original Grad: 0.480, -lr * Pred Grad:  0.335, New P: 1.675
-Original Grad: -0.175, -lr * Pred Grad:  -0.066, New P: 0.577
iter 2 loss: 0.425
Actual params: [1.675 , 0.5775]
-Original Grad: 0.122, -lr * Pred Grad:  0.116, New P: 1.791
-Original Grad: 0.276, -lr * Pred Grad:  0.104, New P: 0.681
iter 3 loss: 0.360
Actual params: [1.7912, 0.6814]
-Original Grad: 0.131, -lr * Pred Grad:  0.252, New P: 2.043
-Original Grad: 0.083, -lr * Pred Grad:  0.045, New P: 0.727
iter 4 loss: 0.317
Actual params: [2.043 , 0.7267]
-Original Grad: 0.100, -lr * Pred Grad:  0.283, New P: 2.326
-Original Grad: 0.092, -lr * Pred Grad:  0.107, New P: 0.834
iter 5 loss: 0.282
Actual params: [2.3257, 0.8342]
-Original Grad: 0.167, -lr * Pred Grad:  0.319, New P: 2.645
-Original Grad: -0.026, -lr * Pred Grad:  -0.068, New P: 0.767
iter 6 loss: 0.230
Actual params: [2.6447, 0.7665]
-Original Grad: 0.093, -lr * Pred Grad:  0.184, New P: 2.828
-Original Grad: -0.031, -lr * Pred Grad:  -0.092, New P: 0.675
iter 7 loss: 0.176
Actual params: [2.8285, 0.6749]
-Original Grad: 0.050, -lr * Pred Grad:  0.169, New P: 2.998
-Original Grad: 0.006, -lr * Pred Grad:  0.096, New P: 0.771
iter 8 loss: 0.156
Actual params: [2.9979, 0.771 ]
-Original Grad: 0.036, -lr * Pred Grad:  0.145, New P: 3.143
-Original Grad: -0.014, -lr * Pred Grad:  -0.179, New P: 0.592
iter 9 loss: 0.124
Actual params: [3.1433, 0.5921]
-Original Grad: 0.026, -lr * Pred Grad:  0.191, New P: 3.334
-Original Grad: 0.001, -lr * Pred Grad:  0.105, New P: 0.697
iter 10 loss: 0.123
Actual params: [3.3338, 0.6967]
-Original Grad: 0.015, -lr * Pred Grad:  0.171, New P: 3.505
-Original Grad: -0.002, -lr * Pred Grad:  -0.191, New P: 0.506
iter 11 loss: 0.093
Actual params: [3.5052, 0.506 ]
-Original Grad: 0.013, -lr * Pred Grad:  0.085, New P: 3.590
-Original Grad: 0.017, -lr * Pred Grad:  0.179, New P: 0.685
iter 12 loss: 0.097
Actual params: [3.5901, 0.6849]
-Original Grad: 0.008, -lr * Pred Grad:  0.332, New P: 3.922
-Original Grad: -0.003, -lr * Pred Grad:  -0.262, New P: 0.423
iter 13 loss: 0.075
Actual params: [3.9222, 0.423 ]
-Original Grad: 0.007, -lr * Pred Grad:  0.202, New P: 4.124
-Original Grad: 0.007, -lr * Pred Grad:  0.049, New P: 0.472
iter 14 loss: 0.074
Actual params: [4.1238, 0.4724]
-Original Grad: -0.000, -lr * Pred Grad:  0.331, New P: 4.455
-Original Grad: -0.004, -lr * Pred Grad:  -0.241, New P: 0.232
iter 15 loss: 0.065
Actual params: [4.4549, 0.2318]
-Original Grad: -0.001, -lr * Pred Grad:  0.309, New P: 4.764
-Original Grad: -0.004, -lr * Pred Grad:  -0.334, New P: -0.102
iter 16 loss: 0.059
Actual params: [ 4.7639, -0.1019]
-Original Grad: -0.002, -lr * Pred Grad:  0.079, New P: 4.843
-Original Grad: -0.004, -lr * Pred Grad:  -0.317, New P: -0.419
iter 17 loss: 0.048
Actual params: [ 4.8433, -0.4188]
-Original Grad: -0.004, -lr * Pred Grad:  -0.241, New P: 4.602
-Original Grad: 0.004, -lr * Pred Grad:  0.110, New P: -0.309
iter 18 loss: 0.051
Actual params: [ 4.6025, -0.3087]
-Original Grad: -0.002, -lr * Pred Grad:  -0.326, New P: 4.276
-Original Grad: -0.000, -lr * Pred Grad:  -0.167, New P: -0.476
iter 19 loss: 0.046
Actual params: [ 4.2762, -0.4762]
-Original Grad: -0.001, -lr * Pred Grad:  -0.327, New P: 3.949
-Original Grad: -0.003, -lr * Pred Grad:  -0.350, New P: -0.827
iter 20 loss: 0.047
Actual params: [ 3.949 , -0.8265]
-Original Grad: -0.003, -lr * Pred Grad:  -0.083, New P: 3.866
-Original Grad: 0.013, -lr * Pred Grad:  0.082, New P: -0.745
iter 21 loss: 0.063
Actual params: [ 3.8659, -0.7448]
-Original Grad: 0.004, -lr * Pred Grad:  0.118, New P: 3.984
-Original Grad: -0.003, -lr * Pred Grad:  -0.015, New P: -0.760
iter 22 loss: 0.061
Actual params: [ 3.9843, -0.7601]
-Original Grad: -0.003, -lr * Pred Grad:  -0.094, New P: 3.890
-Original Grad: 0.007, -lr * Pred Grad:  0.024, New P: -0.736
iter 23 loss: 0.059
Actual params: [ 3.8899, -0.7357]
-Original Grad: 0.004, -lr * Pred Grad:  0.099, New P: 3.989
-Original Grad: -0.003, -lr * Pred Grad:  -0.009, New P: -0.745
iter 24 loss: 0.060
Actual params: [ 3.9893, -0.7449]
-Original Grad: -0.003, -lr * Pred Grad:  -0.087, New P: 3.902
-Original Grad: 0.003, -lr * Pred Grad:  0.008, New P: -0.737
iter 25 loss: 0.058
Actual params: [ 3.9025, -0.7369]
-Original Grad: 0.002, -lr * Pred Grad:  0.070, New P: 3.973
-Original Grad: -0.002, -lr * Pred Grad:  -0.021, New P: -0.758
iter 26 loss: 0.060
Actual params: [ 3.9727, -0.758 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.079, New P: 3.893
-Original Grad: 0.005, -lr * Pred Grad:  0.024, New P: -0.734
iter 27 loss: 0.059
Actual params: [ 3.8933, -0.7343]
-Original Grad: 0.002, -lr * Pred Grad:  0.077, New P: 3.970
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: -0.756
iter 28 loss: 0.060
Actual params: [ 3.9704, -0.7564]
-Original Grad: -0.002, -lr * Pred Grad:  -0.080, New P: 3.890
-Original Grad: 0.004, -lr * Pred Grad:  0.023, New P: -0.733
iter 29 loss: 0.059
Actual params: [ 3.8904, -0.7331]
-Original Grad: 0.004, -lr * Pred Grad:  0.099, New P: 3.990
-Original Grad: -0.003, -lr * Pred Grad:  -0.017, New P: -0.750
iter 30 loss: 0.060
Actual params: [ 3.9899, -0.7499]
Target params: [1.3344, 1.5708]
Actual params: [-0.7899, -0.493 ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.064, New P: -0.853
-Original Grad: -0.002, -lr * Pred Grad:  -0.045, New P: -0.538
iter 0 loss: 0.915
Actual params: [-0.8535, -0.5379]
-Original Grad: -0.001, -lr * Pred Grad:  -0.090, New P: -0.944
-Original Grad: -0.001, -lr * Pred Grad:  -0.065, New P: -0.603
iter 1 loss: 0.915
Actual params: [-0.9436, -0.603 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: -1.043
-Original Grad: -0.000, -lr * Pred Grad:  -0.074, New P: -0.677
iter 2 loss: 0.914
Actual params: [-1.0433, -0.6766]
-Original Grad: -0.000, -lr * Pred Grad:  -0.099, New P: -1.143
-Original Grad: -0.000, -lr * Pred Grad:  -0.074, New P: -0.750
iter 3 loss: 0.914
Actual params: [-1.1426, -0.7505]
-Original Grad: -0.000, -lr * Pred Grad:  -0.099, New P: -1.241
-Original Grad: -0.000, -lr * Pred Grad:  -0.078, New P: -0.829
iter 4 loss: 0.914
Actual params: [-1.2412, -0.8286]
-Original Grad: -0.000, -lr * Pred Grad:  -0.095, New P: -1.336
-Original Grad: -0.000, -lr * Pred Grad:  -0.077, New P: -0.906
iter 5 loss: 0.914
Actual params: [-1.336, -0.906]
-Original Grad: -0.000, -lr * Pred Grad:  -0.093, New P: -1.429
-Original Grad: -0.000, -lr * Pred Grad:  -0.074, New P: -0.980
iter 6 loss: 0.914
Actual params: [-1.4292, -0.9799]
-Original Grad: -0.000, -lr * Pred Grad:  -0.096, New P: -1.525
-Original Grad: -0.000, -lr * Pred Grad:  -0.072, New P: -1.052
iter 7 loss: 0.914
Actual params: [-1.5249, -1.0515]
-Original Grad: -0.000, -lr * Pred Grad:  -0.099, New P: -1.624
-Original Grad: -0.000, -lr * Pred Grad:  -0.072, New P: -1.124
iter 8 loss: 0.914
Actual params: [-1.6244, -1.1238]
-Original Grad: -0.000, -lr * Pred Grad:  -0.103, New P: -1.727
-Original Grad: -0.000, -lr * Pred Grad:  -0.071, New P: -1.194
iter 9 loss: 0.914
Actual params: [-1.7273, -1.1943]
-Original Grad: -0.000, -lr * Pred Grad:  -0.106, New P: -1.833
-Original Grad: -0.000, -lr * Pred Grad:  -0.069, New P: -1.263
iter 10 loss: 0.914
Actual params: [-1.8334, -1.2631]
-Original Grad: -0.000, -lr * Pred Grad:  -0.110, New P: -1.944
-Original Grad: -0.000, -lr * Pred Grad:  -0.066, New P: -1.330
iter 11 loss: 0.914
Actual params: [-1.9439, -1.3295]
-Original Grad: -0.000, -lr * Pred Grad:  -0.115, New P: -2.059
-Original Grad: -0.000, -lr * Pred Grad:  -0.063, New P: -1.393
iter 12 loss: 0.914
Actual params: [-2.0587, -1.3927]
-Original Grad: -0.000, -lr * Pred Grad:  -0.118, New P: -2.177
-Original Grad: -0.000, -lr * Pred Grad:  -0.060, New P: -1.452
iter 13 loss: 0.914
Actual params: [-2.1767, -1.4524]
-Original Grad: -0.000, -lr * Pred Grad:  -0.122, New P: -2.299
-Original Grad: -0.000, -lr * Pred Grad:  -0.056, New P: -1.508
iter 14 loss: 0.914
Actual params: [-2.299 , -1.5082]
-Original Grad: -0.000, -lr * Pred Grad:  -0.125, New P: -2.424
-Original Grad: -0.000, -lr * Pred Grad:  -0.052, New P: -1.560
iter 15 loss: 0.914
Actual params: [-2.4241, -1.5599]
-Original Grad: -0.000, -lr * Pred Grad:  -0.129, New P: -2.553
-Original Grad: -0.000, -lr * Pred Grad:  -0.048, New P: -1.608
iter 16 loss: 0.914
Actual params: [-2.5533, -1.6075]
-Original Grad: -0.000, -lr * Pred Grad:  -0.132, New P: -2.685
-Original Grad: -0.000, -lr * Pred Grad:  -0.043, New P: -1.651
iter 17 loss: 0.914
Actual params: [-2.6851, -1.6509]
-Original Grad: -0.000, -lr * Pred Grad:  -0.134, New P: -2.819
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -1.690
iter 18 loss: 0.914
Actual params: [-2.8188, -1.6904]
-Original Grad: -0.000, -lr * Pred Grad:  -0.136, New P: -2.955
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -1.726
iter 19 loss: 0.914
Actual params: [-2.9548, -1.7262]
-Original Grad: -0.000, -lr * Pred Grad:  -0.138, New P: -3.093
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -1.759
iter 20 loss: 0.914
Actual params: [-3.0927, -1.7585]
-Original Grad: -0.000, -lr * Pred Grad:  -0.140, New P: -3.233
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.788
iter 21 loss: 0.914
Actual params: [-3.2326, -1.788 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.140, New P: -3.373
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.815
iter 22 loss: 0.914
Actual params: [-3.373 , -1.8146]
-Original Grad: -0.000, -lr * Pred Grad:  -0.141, New P: -3.514
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.839
iter 23 loss: 0.914
Actual params: [-3.5144, -1.8386]
-Original Grad: -0.000, -lr * Pred Grad:  -0.142, New P: -3.656
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.866
iter 24 loss: 0.914
Actual params: [-3.656 , -1.8655]
-Original Grad: -0.000, -lr * Pred Grad:  -0.136, New P: -3.792
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.892
iter 25 loss: 0.914
Actual params: [-3.7917, -1.8925]
-Original Grad: -0.000, -lr * Pred Grad:  -0.136, New P: -3.928
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -1.927
iter 26 loss: 0.914
Actual params: [-3.9277, -1.9266]
-Original Grad: -0.000, -lr * Pred Grad:  -0.126, New P: -4.054
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.958
iter 27 loss: 0.914
Actual params: [-4.0537, -1.958 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.124, New P: -4.177
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.989
iter 28 loss: 0.914
Actual params: [-4.1775, -1.9894]
-Original Grad: -0.000, -lr * Pred Grad:  -0.121, New P: -4.299
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -2.022
iter 29 loss: 0.914
Actual params: [-4.2986, -2.0218]
-Original Grad: -0.000, -lr * Pred Grad:  -0.118, New P: -4.416
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -2.056
iter 30 loss: 0.914
Actual params: [-4.4164, -2.056 ]
Target params: [1.3344, 1.5708]
Actual params: [0.3685, 0.155 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.041, New P: 0.410
-Original Grad: 0.003, -lr * Pred Grad:  0.066, New P: 0.221
iter 0 loss: 0.823
Actual params: [0.4095, 0.2208]
-Original Grad: 0.002, -lr * Pred Grad:  0.148, New P: 0.558
-Original Grad: 0.003, -lr * Pred Grad:  0.205, New P: 0.425
iter 1 loss: 0.799
Actual params: [0.5577, 0.4254]
-Original Grad: 0.010, -lr * Pred Grad:  1.261, New P: 1.819
-Original Grad: 0.008, -lr * Pred Grad:  1.000, New P: 1.425
iter 2 loss: 0.758
Actual params: [1.8191, 1.425 ]
-Original Grad: -0.007, -lr * Pred Grad:  1.698, New P: 3.517
-Original Grad: -0.660, -lr * Pred Grad:  -0.213, New P: 1.212
iter 3 loss: 0.579
Actual params: [3.517 , 1.2123]
-Original Grad: -0.172, -lr * Pred Grad:  -0.529, New P: 2.988
-Original Grad: -0.338, -lr * Pred Grad:  0.062, New P: 1.274
iter 4 loss: 0.755
Actual params: [2.9876, 1.2743]
-Original Grad: -0.162, -lr * Pred Grad:  -0.523, New P: 2.464
-Original Grad: -0.515, -lr * Pred Grad:  0.055, New P: 1.329
iter 5 loss: 0.690
Actual params: [2.4644, 1.3292]
-Original Grad: -0.062, -lr * Pred Grad:  -0.051, New P: 2.413
-Original Grad: -0.415, -lr * Pred Grad:  -0.027, New P: 1.302
iter 6 loss: 0.632
Actual params: [2.4131, 1.3019]
-Original Grad: -0.052, -lr * Pred Grad:  0.123, New P: 2.536
-Original Grad: -0.419, -lr * Pred Grad:  -0.086, New P: 1.215
iter 7 loss: 0.618
Actual params: [2.5365, 1.2154]
-Original Grad: -0.074, -lr * Pred Grad:  -0.980, New P: 1.556
-Original Grad: -0.425, -lr * Pred Grad:  0.021, New P: 1.236
iter 8 loss: 0.612
Actual params: [1.556 , 1.2364]
-Original Grad: 0.079, -lr * Pred Grad:  0.977, New P: 2.533
-Original Grad: -0.437, -lr * Pred Grad:  -0.126, New P: 1.110
iter 9 loss: 0.440
Actual params: [2.5331, 1.11  ]
-Original Grad: -0.064, -lr * Pred Grad:  -0.951, New P: 1.582
-Original Grad: -0.263, -lr * Pred Grad:  -0.090, New P: 1.020
iter 10 loss: 0.586
Actual params: [1.5818, 1.0196]
-Original Grad: 0.081, -lr * Pred Grad:  0.859, New P: 2.440
-Original Grad: -0.259, -lr * Pred Grad:  -0.120, New P: 0.900
iter 11 loss: 0.366
Actual params: [2.4403, 0.8997]
-Original Grad: -0.059, -lr * Pred Grad:  -0.784, New P: 1.657
-Original Grad: -0.130, -lr * Pred Grad:  -0.086, New P: 0.813
iter 12 loss: 0.527
Actual params: [1.6566, 0.8135]
-Original Grad: 0.046, -lr * Pred Grad:  0.861, New P: 2.518
-Original Grad: -0.080, -lr * Pred Grad:  -0.148, New P: 0.665
iter 13 loss: 0.355
Actual params: [2.5179, 0.665 ]
-Original Grad: -0.051, -lr * Pred Grad:  -1.027, New P: 1.491
-Original Grad: -0.014, -lr * Pred Grad:  0.065, New P: 0.731
iter 14 loss: 0.522
Actual params: [1.4914, 0.7305]
-Original Grad: 0.071, -lr * Pred Grad:  0.963, New P: 2.454
-Original Grad: -0.074, -lr * Pred Grad:  -0.029, New P: 0.701
iter 15 loss: 0.299
Actual params: [2.4541, 0.7014]
-Original Grad: -0.049, -lr * Pred Grad:  -0.875, New P: 1.579
-Original Grad: -0.018, -lr * Pred Grad:  -0.054, New P: 0.647
iter 16 loss: 0.512
Actual params: [1.5791, 0.6473]
-Original Grad: 0.060, -lr * Pred Grad:  0.923, New P: 2.502
-Original Grad: 0.061, -lr * Pred Grad:  -0.093, New P: 0.554
iter 17 loss: 0.296
Actual params: [2.5024, 0.5538]
-Original Grad: -0.050, -lr * Pred Grad:  -0.905, New P: 1.597
-Original Grad: 0.079, -lr * Pred Grad:  0.203, New P: 0.757
iter 18 loss: 0.515
Actual params: [1.5971, 0.7573]
-Original Grad: 0.051, -lr * Pred Grad:  0.895, New P: 2.492
-Original Grad: -0.074, -lr * Pred Grad:  -0.142, New P: 0.616
iter 19 loss: 0.337
Actual params: [2.4922, 0.6156]
-Original Grad: -0.050, -lr * Pred Grad:  -0.937, New P: 1.556
-Original Grad: 0.067, -lr * Pred Grad:  0.053, New P: 0.668
iter 20 loss: 0.515
Actual params: [1.5556, 0.6682]
-Original Grad: 0.062, -lr * Pred Grad:  0.846, New P: 2.401
-Original Grad: 0.058, -lr * Pred Grad:  -0.049, New P: 0.619
iter 21 loss: 0.295
Actual params: [2.4012, 0.6191]
-Original Grad: -0.039, -lr * Pred Grad:  -0.768, New P: 1.633
-Original Grad: 0.074, -lr * Pred Grad:  0.151, New P: 0.770
iter 22 loss: 0.499
Actual params: [1.6335, 0.7702]
-Original Grad: 0.047, -lr * Pred Grad:  0.912, New P: 2.545
-Original Grad: -0.074, -lr * Pred Grad:  -0.161, New P: 0.609
iter 23 loss: 0.347
Actual params: [2.5451, 0.6093]
-Original Grad: -0.054, -lr * Pred Grad:  -0.974, New P: 1.572
-Original Grad: 0.069, -lr * Pred Grad:  0.058, New P: 0.667
iter 24 loss: 0.526
Actual params: [1.5716, 0.6671]
-Original Grad: 0.061, -lr * Pred Grad:  0.834, New P: 2.406
-Original Grad: 0.058, -lr * Pred Grad:  -0.047, New P: 0.620
iter 25 loss: 0.296
Actual params: [2.4058, 0.6196]
-Original Grad: -0.039, -lr * Pred Grad:  -0.774, New P: 1.632
-Original Grad: 0.074, -lr * Pred Grad:  0.151, New P: 0.770
iter 26 loss: 0.500
Actual params: [1.6316, 0.7705]
-Original Grad: 0.047, -lr * Pred Grad:  0.911, New P: 2.543
-Original Grad: -0.074, -lr * Pred Grad:  -0.161, New P: 0.610
iter 27 loss: 0.347
Actual params: [2.5427, 0.6095]
-Original Grad: -0.054, -lr * Pred Grad:  -0.973, New P: 1.570
-Original Grad: 0.069, -lr * Pred Grad:  0.058, New P: 0.667
iter 28 loss: 0.525
Actual params: [1.5697, 0.6671]
-Original Grad: 0.061, -lr * Pred Grad:  0.834, New P: 2.404
-Original Grad: 0.058, -lr * Pred Grad:  -0.048, New P: 0.620
iter 29 loss: 0.296
Actual params: [2.4036, 0.6196]
-Original Grad: -0.039, -lr * Pred Grad:  -0.773, New P: 1.630
-Original Grad: 0.074, -lr * Pred Grad:  0.151, New P: 0.770
iter 30 loss: 0.500
Actual params: [1.6301, 0.7705]
