Target params: [1.1812, 0.2779]
iter 0 loss: 0.739
Actual params: [0.5941, 0.5941]
-Original Grad: 0.051, -lr * Pred Grad:  0.323, New P: 0.917
-Original Grad: -0.803, -lr * Pred Grad:  -0.206, New P: 0.389
iter 1 loss: 0.539
Actual params: [0.9168, 0.3885]
-Original Grad: 0.551, -lr * Pred Grad:  0.058, New P: 0.975
-Original Grad: -0.189, -lr * Pred Grad:  -0.032, New P: 0.357
iter 2 loss: 0.480
Actual params: [0.9746, 0.3569]
-Original Grad: 0.281, -lr * Pred Grad:  0.025, New P: 0.999
-Original Grad: -0.177, -lr * Pred Grad:  -0.019, New P: 0.338
iter 3 loss: 0.465
Actual params: [0.9994, 0.3383]
-Original Grad: 0.629, -lr * Pred Grad:  0.018, New P: 1.018
-Original Grad: 0.121, -lr * Pred Grad:  -0.004, New P: 0.334
iter 4 loss: 0.471
Actual params: [1.0179, 0.3342]
-Original Grad: 0.218, -lr * Pred Grad:  0.007, New P: 1.025
-Original Grad: -0.071, -lr * Pred Grad:  -0.008, New P: 0.326
iter 5 loss: 0.462
Actual params: [1.0249, 0.3264]
-Original Grad: 0.107, -lr * Pred Grad:  0.002, New P: 1.027
-Original Grad: 0.049, -lr * Pred Grad:  0.001, New P: 0.327
iter 6 loss: 0.464
Actual params: [1.0268, 0.3269]
-Original Grad: -0.043, -lr * Pred Grad:  -0.001, New P: 1.026
-Original Grad: 0.005, -lr * Pred Grad:  0.001, New P: 0.328
iter 7 loss: 0.465
Actual params: [1.0258, 0.3278]
-Original Grad: -0.087, -lr * Pred Grad:  0.000, New P: 1.026
-Original Grad: -0.144, -lr * Pred Grad:  -0.005, New P: 0.323
iter 8 loss: 0.455
Actual params: [1.026 , 0.3227]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 1.025
-Original Grad: 0.092, -lr * Pred Grad:  0.004, New P: 0.327
iter 9 loss: 0.462
Actual params: [1.0247, 0.3265]
-Original Grad: -0.243, -lr * Pred Grad:  -0.003, New P: 1.022
-Original Grad: -0.091, -lr * Pred Grad:  -0.001, New P: 0.326
iter 10 loss: 0.457
Actual params: [1.0219, 0.3256]
-Original Grad: 0.060, -lr * Pred Grad:  -0.000, New P: 1.022
-Original Grad: 0.112, -lr * Pred Grad:  0.004, New P: 0.329
iter 11 loss: 0.464
Actual params: [1.0215, 0.3292]
-Original Grad: 0.245, -lr * Pred Grad:  0.002, New P: 1.024
-Original Grad: 0.073, -lr * Pred Grad:  -0.000, New P: 0.329
iter 12 loss: 0.466
Actual params: [1.024 , 0.3291]
-Original Grad: 0.062, -lr * Pred Grad:  0.001, New P: 1.025
-Original Grad: -0.038, -lr * Pred Grad:  -0.002, New P: 0.327
iter 13 loss: 0.463
Actual params: [1.0252, 0.3272]
-Original Grad: 0.194, -lr * Pred Grad:  0.002, New P: 1.027
-Original Grad: 0.032, -lr * Pred Grad:  -0.001, New P: 0.326
iter 14 loss: 0.463
Actual params: [1.0273, 0.3263]
-Original Grad: -0.105, -lr * Pred Grad:  -0.002, New P: 1.026
-Original Grad: 0.020, -lr * Pred Grad:  0.002, New P: 0.328
iter 15 loss: 0.466
Actual params: [1.0258, 0.328 ]
-Original Grad: 0.085, -lr * Pred Grad:  0.001, New P: 1.027
-Original Grad: -0.021, -lr * Pred Grad:  -0.002, New P: 0.326
iter 16 loss: 0.464
Actual params: [1.0271, 0.3265]
-Original Grad: -0.085, -lr * Pred Grad:  0.001, New P: 1.028
-Original Grad: -0.225, -lr * Pred Grad:  -0.006, New P: 0.321
iter 17 loss: 0.453
Actual params: [1.0282, 0.3205]
-Original Grad: 0.007, -lr * Pred Grad:  -0.000, New P: 1.028
-Original Grad: 0.052, -lr * Pred Grad:  0.001, New P: 0.322
iter 18 loss: 0.456
Actual params: [1.0277, 0.322 ]
-Original Grad: -0.213, -lr * Pred Grad:  -0.003, New P: 1.025
-Original Grad: 0.045, -lr * Pred Grad:  0.003, New P: 0.325
iter 19 loss: 0.459
Actual params: [1.0249, 0.325 ]
-Original Grad: -0.079, -lr * Pred Grad:  -0.001, New P: 1.024
-Original Grad: 0.026, -lr * Pred Grad:  0.001, New P: 0.326
iter 20 loss: 0.461
Actual params: [1.0238, 0.3264]
-Original Grad: -0.210, -lr * Pred Grad:  -0.002, New P: 1.022
-Original Grad: -0.072, -lr * Pred Grad:  -0.000, New P: 0.326
Target params: [1.1812, 0.2779]
iter 0 loss: 0.312
Actual params: [0.5941, 0.5941]
-Original Grad: 0.081, -lr * Pred Grad:  0.027, New P: 0.621
-Original Grad: -0.408, -lr * Pred Grad:  -0.088, New P: 0.506
iter 1 loss: 0.262
Actual params: [0.6214, 0.5059]
-Original Grad: 0.015, -lr * Pred Grad:  -0.019, New P: 0.602
-Original Grad: -0.080, -lr * Pred Grad:  -0.019, New P: 0.487
iter 2 loss: 0.261
Actual params: [0.602 , 0.4871]
-Original Grad: 0.007, -lr * Pred Grad:  -0.019, New P: 0.583
-Original Grad: -0.039, -lr * Pred Grad:  -0.011, New P: 0.476
iter 3 loss: 0.265
Actual params: [0.5833, 0.4763]
-Original Grad: -0.001, -lr * Pred Grad:  -0.084, New P: 0.499
-Original Grad: -0.036, -lr * Pred Grad:  -0.025, New P: 0.452
iter 4 loss: 0.284
Actual params: [0.4992, 0.4517]
-Original Grad: -0.048, -lr * Pred Grad:  -0.154, New P: 0.345
-Original Grad: -0.003, -lr * Pred Grad:  -0.037, New P: 0.415
iter 5 loss: 0.340
Actual params: [0.3447, 0.4146]
-Original Grad: 0.062, -lr * Pred Grad:  0.073, New P: 0.418
-Original Grad: -0.112, -lr * Pred Grad:  0.002, New P: 0.417
iter 6 loss: 0.302
Actual params: [0.4179, 0.417 ]
-Original Grad: -0.005, -lr * Pred Grad:  -0.022, New P: 0.395
-Original Grad: -0.044, -lr * Pred Grad:  -0.013, New P: 0.404
iter 7 loss: 0.307
Actual params: [0.3955, 0.404 ]
-Original Grad: 0.055, -lr * Pred Grad:  0.038, New P: 0.433
-Original Grad: -0.062, -lr * Pred Grad:  0.002, New P: 0.406
iter 8 loss: 0.292
Actual params: [0.4335, 0.4064]
-Original Grad: -0.079, -lr * Pred Grad:  -0.047, New P: 0.386
-Original Grad: -0.025, -lr * Pred Grad:  -0.018, New P: 0.388
iter 9 loss: 0.305
Actual params: [0.3864, 0.388 ]
-Original Grad: 0.047, -lr * Pred Grad:  0.015, New P: 0.402
-Original Grad: -0.058, -lr * Pred Grad:  -0.003, New P: 0.385
iter 10 loss: 0.297
Actual params: [0.4015, 0.3847]
-Original Grad: 0.007, -lr * Pred Grad:  0.000, New P: 0.402
-Original Grad: -0.019, -lr * Pred Grad:  -0.003, New P: 0.382
iter 11 loss: 0.295
Actual params: [0.4017, 0.3817]
-Original Grad: -0.001, -lr * Pred Grad:  -0.004, New P: 0.398
-Original Grad: -0.020, -lr * Pred Grad:  -0.005, New P: 0.377
iter 12 loss: 0.295
Actual params: [0.3978, 0.3768]
-Original Grad: 0.042, -lr * Pred Grad:  0.012, New P: 0.410
-Original Grad: -0.025, -lr * Pred Grad:  0.001, New P: 0.377
iter 13 loss: 0.291
Actual params: [0.4102, 0.3775]
-Original Grad: -0.026, -lr * Pred Grad:  -0.010, New P: 0.400
-Original Grad: -0.006, -lr * Pred Grad:  -0.006, New P: 0.372
iter 14 loss: 0.293
Actual params: [0.4001, 0.3716]
-Original Grad: 0.024, -lr * Pred Grad:  0.006, New P: 0.406
-Original Grad: -0.014, -lr * Pred Grad:  -0.000, New P: 0.371
iter 15 loss: 0.290
Actual params: [0.4059, 0.3713]
-Original Grad: -0.051, -lr * Pred Grad:  -0.014, New P: 0.391
-Original Grad: 0.009, -lr * Pred Grad:  -0.005, New P: 0.366
iter 16 loss: 0.294
Actual params: [0.3914, 0.3658]
-Original Grad: -0.025, -lr * Pred Grad:  -0.009, New P: 0.383
-Original Grad: -0.009, -lr * Pred Grad:  -0.007, New P: 0.359
iter 17 loss: 0.295
Actual params: [0.3829, 0.3587]
-Original Grad: 0.049, -lr * Pred Grad:  0.010, New P: 0.393
-Original Grad: -0.027, -lr * Pred Grad:  -0.001, New P: 0.357
iter 18 loss: 0.291
Actual params: [0.3927, 0.3572]
-Original Grad: 0.009, -lr * Pred Grad:  -0.002, New P: 0.390
-Original Grad: -0.026, -lr * Pred Grad:  -0.009, New P: 0.348
iter 19 loss: 0.289
Actual params: [0.3905, 0.3484]
-Original Grad: 0.010, -lr * Pred Grad:  -0.000, New P: 0.390
-Original Grad: -0.015, -lr * Pred Grad:  -0.005, New P: 0.344
iter 20 loss: 0.288
Actual params: [0.3903, 0.3438]
-Original Grad: -0.018, -lr * Pred Grad:  -0.006, New P: 0.384
-Original Grad: -0.007, -lr * Pred Grad:  -0.007, New P: 0.337
Target params: [1.1812, 0.2779]
iter 0 loss: 0.445
Actual params: [0.5941, 0.5941]
-Original Grad: -0.137, -lr * Pred Grad:  -0.007, New P: 0.587
-Original Grad: -2.171, -lr * Pred Grad:  -0.081, New P: 0.513
iter 1 loss: 0.327
Actual params: [0.5868, 0.5127]
-Original Grad: -0.208, -lr * Pred Grad:  -0.135, New P: 0.452
-Original Grad: -1.304, -lr * Pred Grad:  -0.014, New P: 0.498
iter 2 loss: 0.286
Actual params: [0.4516, 0.4985]
-Original Grad: -0.475, -lr * Pred Grad:  -0.072, New P: 0.380
-Original Grad: -0.893, -lr * Pred Grad:  0.004, New P: 0.503
iter 3 loss: 0.300
Actual params: [0.3796, 0.5028]
-Original Grad: -0.078, -lr * Pred Grad:  -0.003, New P: 0.376
-Original Grad: -0.188, -lr * Pred Grad:  -0.002, New P: 0.501
iter 4 loss: 0.311
Actual params: [0.3764, 0.5008]
-Original Grad: -0.108, -lr * Pred Grad:  -0.003, New P: 0.374
-Original Grad: -0.190, -lr * Pred Grad:  -0.002, New P: 0.499
iter 5 loss: 0.312
Actual params: [0.3737, 0.4989]
-Original Grad: -0.205, -lr * Pred Grad:  -0.004, New P: 0.369
-Original Grad: -0.283, -lr * Pred Grad:  -0.002, New P: 0.497
iter 6 loss: 0.323
Actual params: [0.3694, 0.497 ]
-Original Grad: 0.035, -lr * Pred Grad:  0.006, New P: 0.375
-Original Grad: -0.088, -lr * Pred Grad:  -0.004, New P: 0.493
iter 7 loss: 0.313
Actual params: [0.3752, 0.4926]
-Original Grad: 0.056, -lr * Pred Grad:  0.006, New P: 0.381
-Original Grad: -0.051, -lr * Pred Grad:  -0.004, New P: 0.489
iter 8 loss: 0.312
Actual params: [0.3808, 0.4886]
-Original Grad: -0.049, -lr * Pred Grad:  0.002, New P: 0.383
-Original Grad: -0.134, -lr * Pred Grad:  -0.003, New P: 0.485
iter 9 loss: 0.312
Actual params: [0.3828, 0.4852]
-Original Grad: -0.080, -lr * Pred Grad:  0.001, New P: 0.383
-Original Grad: -0.136, -lr * Pred Grad:  -0.003, New P: 0.483
iter 10 loss: 0.312
Actual params: [0.3834, 0.4826]
-Original Grad: -0.167, -lr * Pred Grad:  -0.002, New P: 0.382
-Original Grad: -0.192, -lr * Pred Grad:  -0.002, New P: 0.481
iter 11 loss: 0.313
Actual params: [0.3818, 0.4806]
-Original Grad: 0.140, -lr * Pred Grad:  0.008, New P: 0.389
-Original Grad: 0.008, -lr * Pred Grad:  -0.005, New P: 0.475
iter 12 loss: 0.308
Actual params: [0.3893, 0.4753]
-Original Grad: 0.082, -lr * Pred Grad:  0.006, New P: 0.395
-Original Grad: -0.025, -lr * Pred Grad:  -0.005, New P: 0.471
iter 13 loss: 0.310
Actual params: [0.3948, 0.4707]
-Original Grad: -0.112, -lr * Pred Grad:  -0.000, New P: 0.395
-Original Grad: -0.145, -lr * Pred Grad:  -0.003, New P: 0.468
iter 14 loss: 0.310
Actual params: [0.3948, 0.4678]
-Original Grad: 0.013, -lr * Pred Grad:  0.004, New P: 0.399
-Original Grad: -0.078, -lr * Pred Grad:  -0.005, New P: 0.463
iter 15 loss: 0.310
Actual params: [0.399 , 0.4628]
-Original Grad: -0.134, -lr * Pred Grad:  -0.000, New P: 0.399
-Original Grad: -0.154, -lr * Pred Grad:  -0.003, New P: 0.460
iter 16 loss: 0.310
Actual params: [0.3985, 0.4598]
-Original Grad: -0.001, -lr * Pred Grad:  0.004, New P: 0.403
-Original Grad: -0.080, -lr * Pred Grad:  -0.005, New P: 0.455
iter 17 loss: 0.310
Actual params: [0.4025, 0.4546]
-Original Grad: 0.021, -lr * Pred Grad:  0.005, New P: 0.408
-Original Grad: -0.071, -lr * Pred Grad:  -0.006, New P: 0.449
iter 18 loss: 0.309
Actual params: [0.4075, 0.4486]
-Original Grad: -0.060, -lr * Pred Grad:  0.003, New P: 0.411
-Original Grad: -0.126, -lr * Pred Grad:  -0.006, New P: 0.443
iter 19 loss: 0.309
Actual params: [0.4107, 0.4428]
-Original Grad: -0.088, -lr * Pred Grad:  0.003, New P: 0.413
-Original Grad: -0.143, -lr * Pred Grad:  -0.006, New P: 0.437
iter 20 loss: 0.310
Actual params: [0.4132, 0.437 ]
-Original Grad: 0.072, -lr * Pred Grad:  0.007, New P: 0.420
-Original Grad: -0.033, -lr * Pred Grad:  -0.007, New P: 0.430
Target params: [1.1812, 0.2779]
iter 0 loss: 1.269
Actual params: [0.5941, 0.5941]
-Original Grad: -0.055, -lr * Pred Grad:  -0.359, New P: 0.235
-Original Grad: -0.177, -lr * Pred Grad:  -0.616, New P: -0.021
iter 1 loss: 1.265
Actual params: [ 0.235 , -0.0215]
-Original Grad: 0.043, -lr * Pred Grad:  0.322, New P: 0.557
-Original Grad: -0.222, -lr * Pred Grad:  -0.328, New P: -0.349
iter 2 loss: 1.167
Actual params: [ 0.5571, -0.3494]
-Original Grad: 0.319, -lr * Pred Grad:  0.442, New P: 0.999
-Original Grad: 0.076, -lr * Pred Grad:  0.038, New P: -0.311
iter 3 loss: 0.461
Actual params: [ 0.9992, -0.3114]
-Original Grad: 0.156, -lr * Pred Grad:  0.147, New P: 1.146
-Original Grad: 0.049, -lr * Pred Grad:  0.026, New P: -0.285
iter 4 loss: 0.445
Actual params: [ 1.1462, -0.2852]
-Original Grad: 0.040, -lr * Pred Grad:  0.012, New P: 1.158
-Original Grad: 0.082, -lr * Pred Grad:  0.106, New P: -0.179
iter 5 loss: 0.425
Actual params: [ 1.1578, -0.1794]
-Original Grad: 0.012, -lr * Pred Grad:  -0.024, New P: 1.133
-Original Grad: 0.106, -lr * Pred Grad:  0.104, New P: -0.076
iter 6 loss: 0.411
Actual params: [ 1.1334, -0.0758]
-Original Grad: 0.028, -lr * Pred Grad:  0.019, New P: 1.153
-Original Grad: 0.057, -lr * Pred Grad:  0.041, New P: -0.035
iter 7 loss: 0.411
Actual params: [ 1.1526, -0.0353]
-Original Grad: 0.018, -lr * Pred Grad:  -0.011, New P: 1.141
-Original Grad: 0.118, -lr * Pred Grad:  0.079, New P: 0.043
iter 8 loss: 0.409
Actual params: [1.1413, 0.0434]
-Original Grad: 0.025, -lr * Pred Grad:  0.015, New P: 1.157
-Original Grad: 0.072, -lr * Pred Grad:  0.039, New P: 0.083
iter 9 loss: 0.415
Actual params: [1.1568, 0.0828]
-Original Grad: 0.024, -lr * Pred Grad:  0.015, New P: 1.172
-Original Grad: 0.076, -lr * Pred Grad:  0.036, New P: 0.118
iter 10 loss: 0.423
Actual params: [1.1719, 0.1185]
-Original Grad: 0.025, -lr * Pred Grad:  0.029, New P: 1.201
-Original Grad: 0.035, -lr * Pred Grad:  0.013, New P: 0.132
iter 11 loss: 0.430
Actual params: [1.2006, 0.1316]
-Original Grad: -0.007, -lr * Pred Grad:  -0.014, New P: 1.187
-Original Grad: 0.018, -lr * Pred Grad:  0.011, New P: 0.143
iter 12 loss: 0.431
Actual params: [1.1867, 0.143 ]
-Original Grad: 0.023, -lr * Pred Grad:  0.027, New P: 1.214
-Original Grad: 0.023, -lr * Pred Grad:  0.008, New P: 0.151
iter 13 loss: 0.442
Actual params: [1.2138, 0.1509]
-Original Grad: -0.009, -lr * Pred Grad:  -0.018, New P: 1.196
-Original Grad: 0.046, -lr * Pred Grad:  0.025, New P: 0.175
iter 14 loss: 0.433
Actual params: [1.1956, 0.1755]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: 1.190
-Original Grad: 0.043, -lr * Pred Grad:  0.021, New P: 0.196
iter 15 loss: 0.432
Actual params: [1.1897, 0.1964]
-Original Grad: 0.026, -lr * Pred Grad:  0.031, New P: 1.220
-Original Grad: 0.034, -lr * Pred Grad:  0.014, New P: 0.210
iter 16 loss: 0.454
Actual params: [1.2203, 0.2104]
-Original Grad: -0.040, -lr * Pred Grad:  -0.040, New P: 1.181
-Original Grad: -0.021, -lr * Pred Grad:  -0.005, New P: 0.206
iter 17 loss: 0.429
Actual params: [1.1806, 0.2058]
-Original Grad: 0.009, -lr * Pred Grad:  0.007, New P: 1.188
-Original Grad: 0.016, -lr * Pred Grad:  0.007, New P: 0.213
iter 18 loss: 0.434
Actual params: [1.188 , 0.2129]
-Original Grad: 0.020, -lr * Pred Grad:  0.021, New P: 1.209
-Original Grad: 0.012, -lr * Pred Grad:  0.003, New P: 0.216
iter 19 loss: 0.447
Actual params: [1.2086, 0.2163]
-Original Grad: -0.026, -lr * Pred Grad:  -0.021, New P: 1.187
-Original Grad: -0.019, -lr * Pred Grad:  -0.006, New P: 0.211
iter 20 loss: 0.433
Actual params: [1.1872, 0.2106]
-Original Grad: -0.004, -lr * Pred Grad:  -0.002, New P: 1.185
-Original Grad: -0.008, -lr * Pred Grad:  -0.004, New P: 0.206
Target params: [1.1812, 0.2779]
iter 0 loss: 0.863
Actual params: [0.5941, 0.5941]
-Original Grad: 0.317, -lr * Pred Grad:  0.141, New P: 0.735
-Original Grad: -1.078, -lr * Pred Grad:  -0.138, New P: 0.456
iter 1 loss: 0.754
Actual params: [0.7351, 0.4558]
-Original Grad: 0.380, -lr * Pred Grad:  0.250, New P: 0.985
-Original Grad: -0.486, -lr * Pred Grad:  0.048, New P: 0.504
iter 2 loss: 0.532
Actual params: [0.985 , 0.5042]
-Original Grad: 0.079, -lr * Pred Grad:  -0.080, New P: 0.905
-Original Grad: -0.401, -lr * Pred Grad:  -0.066, New P: 0.438
iter 3 loss: 0.592
Actual params: [0.9046, 0.4385]
-Original Grad: 0.230, -lr * Pred Grad:  0.097, New P: 1.001
-Original Grad: -0.313, -lr * Pred Grad:  0.016, New P: 0.455
iter 4 loss: 0.523
Actual params: [1.0013, 0.4546]
-Original Grad: 0.051, -lr * Pred Grad:  -0.011, New P: 0.990
-Original Grad: -0.150, -lr * Pred Grad:  -0.017, New P: 0.438
iter 5 loss: 0.528
Actual params: [0.9901, 0.4377]
-Original Grad: 0.061, -lr * Pred Grad:  -0.040, New P: 0.950
-Original Grad: -0.260, -lr * Pred Grad:  -0.035, New P: 0.402
iter 6 loss: 0.549
Actual params: [0.9499, 0.4022]
-Original Grad: 0.058, -lr * Pred Grad:  -0.019, New P: 0.931
-Original Grad: -0.210, -lr * Pred Grad:  -0.023, New P: 0.379
iter 7 loss: 0.563
Actual params: [0.9307, 0.3793]
-Original Grad: 0.084, -lr * Pred Grad:  0.055, New P: 0.985
-Original Grad: -0.082, -lr * Pred Grad:  0.014, New P: 0.393
iter 8 loss: 0.527
Actual params: [0.9854, 0.393 ]
-Original Grad: 0.052, -lr * Pred Grad:  -0.021, New P: 0.964
-Original Grad: -0.206, -lr * Pred Grad:  -0.024, New P: 0.369
iter 9 loss: 0.528
Actual params: [0.9642, 0.3693]
-Original Grad: 0.063, -lr * Pred Grad:  0.020, New P: 0.984
-Original Grad: -0.126, -lr * Pred Grad:  -0.003, New P: 0.366
iter 10 loss: 0.524
Actual params: [0.9843, 0.3661]
-Original Grad: 0.039, -lr * Pred Grad:  -0.007, New P: 0.978
-Original Grad: -0.129, -lr * Pred Grad:  -0.013, New P: 0.353
iter 11 loss: 0.527
Actual params: [0.9778, 0.3528]
-Original Grad: 0.061, -lr * Pred Grad:  0.052, New P: 1.030
-Original Grad: -0.034, -lr * Pred Grad:  0.015, New P: 0.368
iter 12 loss: 0.495
Actual params: [1.03  , 0.3675]
-Original Grad: 0.035, -lr * Pred Grad:  0.023, New P: 1.053
-Original Grad: -0.040, -lr * Pred Grad:  0.004, New P: 0.371
iter 13 loss: 0.489
Actual params: [1.0532, 0.3714]
-Original Grad: -0.016, -lr * Pred Grad:  -0.043, New P: 1.011
-Original Grad: -0.090, -lr * Pred Grad:  -0.023, New P: 0.349
iter 14 loss: 0.502
Actual params: [1.0106, 0.3489]
-Original Grad: 0.040, -lr * Pred Grad:  0.011, New P: 1.021
-Original Grad: -0.097, -lr * Pred Grad:  -0.007, New P: 0.342
iter 15 loss: 0.495
Actual params: [1.0211, 0.3417]
-Original Grad: 0.036, -lr * Pred Grad:  0.003, New P: 1.024
-Original Grad: -0.113, -lr * Pred Grad:  -0.012, New P: 0.330
iter 16 loss: 0.494
Actual params: [1.0243, 0.3302]
-Original Grad: 0.037, -lr * Pred Grad:  0.019, New P: 1.043
-Original Grad: -0.069, -lr * Pred Grad:  -0.003, New P: 0.328
iter 17 loss: 0.497
Actual params: [1.0432, 0.3277]
-Original Grad: 0.040, -lr * Pred Grad:  0.041, New P: 1.084
-Original Grad: -0.011, -lr * Pred Grad:  0.011, New P: 0.339
iter 18 loss: 0.496
Actual params: [1.0845, 0.3385]
-Original Grad: -0.101, -lr * Pred Grad:  -0.085, New P: 0.999
-Original Grad: -0.136, -lr * Pred Grad:  -0.035, New P: 0.304
iter 19 loss: 0.502
Actual params: [0.9993, 0.3037]
-Original Grad: 0.038, -lr * Pred Grad:  0.013, New P: 1.012
-Original Grad: -0.096, -lr * Pred Grad:  -0.010, New P: 0.294
iter 20 loss: 0.501
Actual params: [1.012 , 0.2935]
-Original Grad: 0.033, -lr * Pred Grad:  0.019, New P: 1.031
-Original Grad: -0.042, -lr * Pred Grad:  -0.002, New P: 0.291
Target params: [1.1812, 0.2779]
iter 0 loss: 0.885
Actual params: [0.5941, 0.5941]
-Original Grad: -0.040, -lr * Pred Grad:  0.170, New P: 0.764
-Original Grad: -1.129, -lr * Pred Grad:  -0.161, New P: 0.433
iter 1 loss: 0.700
Actual params: [0.7643, 0.433 ]
-Original Grad: 0.494, -lr * Pred Grad:  0.207, New P: 0.971
-Original Grad: -0.338, -lr * Pred Grad:  -0.011, New P: 0.422
iter 2 loss: 0.446
Actual params: [0.9712, 0.4216]
-Original Grad: 0.190, -lr * Pred Grad:  0.056, New P: 1.028
-Original Grad: -0.143, -lr * Pred Grad:  -0.005, New P: 0.416
iter 3 loss: 0.414
Actual params: [1.0277, 0.4162]
-Original Grad: 0.185, -lr * Pred Grad:  0.046, New P: 1.074
-Original Grad: -0.090, -lr * Pred Grad:  0.001, New P: 0.417
iter 4 loss: 0.402
Actual params: [1.0739, 0.4174]
-Original Grad: -0.047, -lr * Pred Grad:  -0.019, New P: 1.055
-Original Grad: -0.059, -lr * Pred Grad:  -0.014, New P: 0.404
iter 5 loss: 0.405
Actual params: [1.0547, 0.4038]
-Original Grad: -0.011, -lr * Pred Grad:  -0.009, New P: 1.046
-Original Grad: -0.070, -lr * Pred Grad:  -0.013, New P: 0.391
iter 6 loss: 0.406
Actual params: [1.0462, 0.3913]
-Original Grad: 0.014, -lr * Pred Grad:  0.000, New P: 1.046
-Original Grad: -0.064, -lr * Pred Grad:  -0.010, New P: 0.381
iter 7 loss: 0.405
Actual params: [1.0463, 0.3812]
-Original Grad: 0.017, -lr * Pred Grad:  0.002, New P: 1.049
-Original Grad: -0.046, -lr * Pred Grad:  -0.007, New P: 0.374
iter 8 loss: 0.404
Actual params: [1.0487, 0.3738]
-Original Grad: 0.010, -lr * Pred Grad:  -0.001, New P: 1.048
-Original Grad: -0.058, -lr * Pred Grad:  -0.011, New P: 0.363
iter 9 loss: 0.403
Actual params: [1.0479, 0.3627]
-Original Grad: 0.041, -lr * Pred Grad:  0.011, New P: 1.059
-Original Grad: -0.059, -lr * Pred Grad:  -0.009, New P: 0.353
iter 10 loss: 0.399
Actual params: [1.0593, 0.3533]
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: 1.055
-Original Grad: -0.050, -lr * Pred Grad:  -0.011, New P: 0.342
iter 11 loss: 0.400
Actual params: [1.0551, 0.342 ]
-Original Grad: 0.003, -lr * Pred Grad:  -0.004, New P: 1.052
-Original Grad: -0.064, -lr * Pred Grad:  -0.015, New P: 0.327
iter 12 loss: 0.400
Actual params: [1.0515, 0.3271]
-Original Grad: 0.021, -lr * Pred Grad:  0.007, New P: 1.058
-Original Grad: -0.040, -lr * Pred Grad:  -0.009, New P: 0.318
iter 13 loss: 0.397
Actual params: [1.0581, 0.3185]
-Original Grad: 0.023, -lr * Pred Grad:  0.007, New P: 1.066
-Original Grad: -0.046, -lr * Pred Grad:  -0.011, New P: 0.308
iter 14 loss: 0.395
Actual params: [1.0656, 0.3079]
-Original Grad: -0.022, -lr * Pred Grad:  -0.016, New P: 1.049
-Original Grad: -0.035, -lr * Pred Grad:  -0.013, New P: 0.295
iter 15 loss: 0.398
Actual params: [1.0494, 0.295 ]
-Original Grad: 0.017, -lr * Pred Grad:  0.007, New P: 1.056
-Original Grad: -0.033, -lr * Pred Grad:  -0.009, New P: 0.286
iter 16 loss: 0.395
Actual params: [1.0565, 0.286 ]
-Original Grad: 0.015, -lr * Pred Grad:  0.006, New P: 1.063
-Original Grad: -0.028, -lr * Pred Grad:  -0.008, New P: 0.278
iter 17 loss: 0.393
Actual params: [1.0628, 0.2776]
-Original Grad: -0.012, -lr * Pred Grad:  -0.014, New P: 1.049
-Original Grad: -0.046, -lr * Pred Grad:  -0.019, New P: 0.258
iter 18 loss: 0.395
Actual params: [1.0486, 0.2581]
-Original Grad: 0.019, -lr * Pred Grad:  0.011, New P: 1.060
-Original Grad: -0.027, -lr * Pred Grad:  -0.009, New P: 0.249
iter 19 loss: 0.391
Actual params: [1.0596, 0.2494]
-Original Grad: -0.008, -lr * Pred Grad:  -0.012, New P: 1.048
-Original Grad: -0.035, -lr * Pred Grad:  -0.017, New P: 0.232
iter 20 loss: 0.393
Actual params: [1.0478, 0.2324]
-Original Grad: 0.020, -lr * Pred Grad:  0.020, New P: 1.067
-Original Grad: 0.023, -lr * Pred Grad:  0.009, New P: 0.241
Target params: [1.1812, 0.2779]
iter 0 loss: 0.338
Actual params: [0.5941, 0.5941]
-Original Grad: -0.054, -lr * Pred Grad:  0.083, New P: 0.677
-Original Grad: -0.376, -lr * Pred Grad:  -0.252, New P: 0.342
iter 1 loss: 0.285
Actual params: [0.677 , 0.3423]
-Original Grad: 0.020, -lr * Pred Grad:  0.058, New P: 0.735
-Original Grad: -0.132, -lr * Pred Grad:  -0.085, New P: 0.258
iter 2 loss: 0.282
Actual params: [0.7347, 0.2578]
-Original Grad: 0.022, -lr * Pred Grad:  0.036, New P: 0.771
-Original Grad: -0.083, -lr * Pred Grad:  -0.051, New P: 0.207
iter 3 loss: 0.277
Actual params: [0.7709, 0.2072]
-Original Grad: -0.006, -lr * Pred Grad:  0.004, New P: 0.775
-Original Grad: -0.042, -lr * Pred Grad:  -0.024, New P: 0.184
iter 4 loss: 0.278
Actual params: [0.7753, 0.1835]
-Original Grad: -0.031, -lr * Pred Grad:  -0.019, New P: 0.756
-Original Grad: -0.034, -lr * Pred Grad:  -0.014, New P: 0.169
iter 5 loss: 0.282
Actual params: [0.7563, 0.1691]
-Original Grad: 0.030, -lr * Pred Grad:  0.028, New P: 0.784
-Original Grad: -0.028, -lr * Pred Grad:  -0.026, New P: 0.143
iter 6 loss: 0.279
Actual params: [0.7842, 0.1431]
-Original Grad: -0.022, -lr * Pred Grad:  -0.016, New P: 0.768
-Original Grad: -0.007, -lr * Pred Grad:  0.000, New P: 0.143
iter 7 loss: 0.282
Actual params: [0.7683, 0.1434]
-Original Grad: -0.030, -lr * Pred Grad:  -0.026, New P: 0.742
-Original Grad: -0.001, -lr * Pred Grad:  0.007, New P: 0.151
iter 8 loss: 0.286
Actual params: [0.7421, 0.1508]
-Original Grad: 0.011, -lr * Pred Grad:  0.009, New P: 0.752
-Original Grad: -0.005, -lr * Pred Grad:  -0.007, New P: 0.144
iter 9 loss: 0.285
Actual params: [0.7516, 0.1437]
-Original Grad: -0.026, -lr * Pred Grad:  -0.017, New P: 0.735
-Original Grad: -0.019, -lr * Pred Grad:  -0.012, New P: 0.132
iter 10 loss: 0.287
Actual params: [0.735 , 0.1318]
-Original Grad: 0.056, -lr * Pred Grad:  0.034, New P: 0.769
-Original Grad: 0.014, -lr * Pred Grad:  0.001, New P: 0.133
iter 11 loss: 0.282
Actual params: [0.769 , 0.1333]
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: 0.771
-Original Grad: -0.006, -lr * Pred Grad:  -0.007, New P: 0.126
iter 12 loss: 0.282
Actual params: [0.771 , 0.1262]
-Original Grad: 0.009, -lr * Pred Grad:  0.010, New P: 0.781
-Original Grad: -0.015, -lr * Pred Grad:  -0.020, New P: 0.106
iter 13 loss: 0.282
Actual params: [0.781 , 0.1059]
-Original Grad: -0.006, -lr * Pred Grad:  -0.006, New P: 0.775
-Original Grad: 0.010, -lr * Pred Grad:  0.015, New P: 0.121
iter 14 loss: 0.282
Actual params: [0.7746, 0.1207]
-Original Grad: 0.029, -lr * Pred Grad:  0.020, New P: 0.795
-Original Grad: -0.010, -lr * Pred Grad:  -0.022, New P: 0.099
iter 15 loss: 0.280
Actual params: [0.7947, 0.0987]
-Original Grad: -0.030, -lr * Pred Grad:  -0.019, New P: 0.776
-Original Grad: -0.002, -lr * Pred Grad:  0.006, New P: 0.104
iter 16 loss: 0.283
Actual params: [0.7757, 0.1045]
-Original Grad: -0.023, -lr * Pred Grad:  -0.013, New P: 0.762
-Original Grad: -0.009, -lr * Pred Grad:  -0.007, New P: 0.098
iter 17 loss: 0.286
Actual params: [0.7622, 0.0975]
-Original Grad: -0.026, -lr * Pred Grad:  -0.011, New P: 0.752
-Original Grad: -0.028, -lr * Pred Grad:  -0.040, New P: 0.058
iter 18 loss: 0.289
Actual params: [0.7517, 0.058 ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.006, New P: 0.745
-Original Grad: 0.011, -lr * Pred Grad:  0.022, New P: 0.080
iter 19 loss: 0.288
Actual params: [0.7453, 0.0797]
-Original Grad: -0.028, -lr * Pred Grad:  -0.021, New P: 0.724
-Original Grad: -0.007, -lr * Pred Grad:  -0.003, New P: 0.077
iter 20 loss: 0.291
Actual params: [0.7243, 0.0771]
-Original Grad: 0.038, -lr * Pred Grad:  0.025, New P: 0.750
-Original Grad: 0.004, -lr * Pred Grad:  -0.006, New P: 0.071
Target params: [1.1812, 0.2779]
iter 0 loss: 0.605
Actual params: [0.5941, 0.5941]
-Original Grad: 0.107, -lr * Pred Grad:  0.487, New P: 1.081
-Original Grad: -0.107, -lr * Pred Grad:  -0.085, New P: 0.509
iter 1 loss: 0.220
Actual params: [1.0806, 0.5095]
-Original Grad: -0.022, -lr * Pred Grad:  0.026, New P: 1.106
-Original Grad: -0.253, -lr * Pred Grad:  -0.058, New P: 0.451
iter 2 loss: 0.235
Actual params: [1.1062, 0.4512]
-Original Grad: -0.072, -lr * Pred Grad:  -0.070, New P: 1.036
-Original Grad: -0.039, -lr * Pred Grad:  0.002, New P: 0.453
iter 3 loss: 0.250
Actual params: [1.0363, 0.453 ]
-Original Grad: 0.023, -lr * Pred Grad:  0.030, New P: 1.066
-Original Grad: -0.033, -lr * Pred Grad:  -0.011, New P: 0.442
iter 4 loss: 0.242
Actual params: [1.0663, 0.4421]
-Original Grad: 0.037, -lr * Pred Grad:  0.037, New P: 1.103
-Original Grad: -0.057, -lr * Pred Grad:  -0.016, New P: 0.426
iter 5 loss: 0.239
Actual params: [1.103, 0.426]
-Original Grad: -0.006, -lr * Pred Grad:  -0.003, New P: 1.100
-Original Grad: -0.027, -lr * Pred Grad:  -0.006, New P: 0.420
iter 6 loss: 0.239
Actual params: [1.0997, 0.42  ]
-Original Grad: -0.048, -lr * Pred Grad:  -0.035, New P: 1.065
-Original Grad: -0.013, -lr * Pred Grad:  -0.002, New P: 0.418
iter 7 loss: 0.245
Actual params: [1.065 , 0.4179]
-Original Grad: 0.047, -lr * Pred Grad:  0.031, New P: 1.096
-Original Grad: -0.011, -lr * Pred Grad:  -0.004, New P: 0.413
iter 8 loss: 0.242
Actual params: [1.0962, 0.4135]
-Original Grad: -0.053, -lr * Pred Grad:  -0.034, New P: 1.063
-Original Grad: 0.006, -lr * Pred Grad:  0.003, New P: 0.416
iter 9 loss: 0.246
Actual params: [1.0627, 0.4164]
-Original Grad: 0.048, -lr * Pred Grad:  0.030, New P: 1.092
-Original Grad: -0.010, -lr * Pred Grad:  -0.004, New P: 0.412
iter 10 loss: 0.243
Actual params: [1.0922, 0.4121]
-Original Grad: 0.004, -lr * Pred Grad:  0.002, New P: 1.094
-Original Grad: -0.040, -lr * Pred Grad:  -0.012, New P: 0.400
iter 11 loss: 0.248
Actual params: [1.094 , 0.3997]
-Original Grad: -0.035, -lr * Pred Grad:  -0.018, New P: 1.076
-Original Grad: -0.023, -lr * Pred Grad:  -0.008, New P: 0.391
iter 12 loss: 0.253
Actual params: [1.0758, 0.3914]
-Original Grad: 0.033, -lr * Pred Grad:  0.018, New P: 1.093
-Original Grad: 0.011, -lr * Pred Grad:  0.004, New P: 0.396
iter 13 loss: 0.249
Actual params: [1.0934, 0.3958]
-Original Grad: -0.060, -lr * Pred Grad:  -0.027, New P: 1.066
-Original Grad: 0.029, -lr * Pred Grad:  0.008, New P: 0.404
iter 14 loss: 0.250
Actual params: [1.0664, 0.4042]
-Original Grad: 0.056, -lr * Pred Grad:  0.026, New P: 1.092
-Original Grad: -0.011, -lr * Pred Grad:  -0.003, New P: 0.401
iter 15 loss: 0.247
Actual params: [1.0924, 0.4011]
-Original Grad: -0.015, -lr * Pred Grad:  -0.007, New P: 1.085
-Original Grad: -0.015, -lr * Pred Grad:  -0.006, New P: 0.395
iter 16 loss: 0.250
Actual params: [1.0855, 0.3952]
-Original Grad: 0.022, -lr * Pred Grad:  0.009, New P: 1.094
-Original Grad: 0.035, -lr * Pred Grad:  0.011, New P: 0.406
iter 17 loss: 0.245
Actual params: [1.094 , 0.4057]
-Original Grad: -0.085, -lr * Pred Grad:  -0.035, New P: 1.059
-Original Grad: -0.017, -lr * Pred Grad:  -0.004, New P: 0.402
iter 18 loss: 0.253
Actual params: [1.0593, 0.402 ]
-Original Grad: 0.049, -lr * Pred Grad:  0.022, New P: 1.081
-Original Grad: -0.015, -lr * Pred Grad:  -0.007, New P: 0.395
iter 19 loss: 0.251
Actual params: [1.0809, 0.395 ]
-Original Grad: 0.022, -lr * Pred Grad:  0.010, New P: 1.091
-Original Grad: -0.012, -lr * Pred Grad:  -0.005, New P: 0.390
iter 20 loss: 0.252
Actual params: [1.091 , 0.3896]
-Original Grad: -0.042, -lr * Pred Grad:  -0.017, New P: 1.074
-Original Grad: -0.002, -lr * Pred Grad:  0.001, New P: 0.390
Target params: [1.1812, 0.2779]
iter 0 loss: 0.789
Actual params: [0.5941, 0.5941]
-Original Grad: 0.555, -lr * Pred Grad:  0.259, New P: 0.853
-Original Grad: -0.052, -lr * Pred Grad:  -0.120, New P: 0.474
iter 1 loss: 0.543
Actual params: [0.8533, 0.4737]
-Original Grad: 0.050, -lr * Pred Grad:  0.059, New P: 0.912
-Original Grad: 0.253, -lr * Pred Grad:  0.171, New P: 0.645
iter 2 loss: 0.521
Actual params: [0.912, 0.645]
-Original Grad: -0.114, -lr * Pred Grad:  -0.037, New P: 0.875
-Original Grad: -0.529, -lr * Pred Grad:  -0.124, New P: 0.521
iter 3 loss: 0.533
Actual params: [0.8753, 0.521 ]
-Original Grad: 0.069, -lr * Pred Grad:  0.022, New P: 0.898
-Original Grad: -0.032, -lr * Pred Grad:  -0.005, New P: 0.516
iter 4 loss: 0.504
Actual params: [0.8975, 0.5161]
-Original Grad: -0.034, -lr * Pred Grad:  -0.011, New P: 0.886
-Original Grad: 0.061, -lr * Pred Grad:  0.006, New P: 0.522
iter 5 loss: 0.521
Actual params: [0.8863, 0.5224]
-Original Grad: 0.116, -lr * Pred Grad:  0.037, New P: 0.923
-Original Grad: -0.019, -lr * Pred Grad:  0.000, New P: 0.523
iter 6 loss: 0.471
Actual params: [0.923 , 0.5228]
-Original Grad: -0.048, -lr * Pred Grad:  -0.020, New P: 0.903
-Original Grad: -0.334, -lr * Pred Grad:  -0.022, New P: 0.501
iter 7 loss: 0.487
Actual params: [0.9031, 0.5006]
-Original Grad: -0.045, -lr * Pred Grad:  -0.015, New P: 0.888
-Original Grad: -0.001, -lr * Pred Grad:  -0.000, New P: 0.500
iter 8 loss: 0.507
Actual params: [0.8881, 0.5002]
-Original Grad: 0.038, -lr * Pred Grad:  0.012, New P: 0.900
-Original Grad: -0.140, -lr * Pred Grad:  -0.006, New P: 0.494
iter 9 loss: 0.486
Actual params: [0.9002, 0.4944]
-Original Grad: 0.014, -lr * Pred Grad:  0.005, New P: 0.905
-Original Grad: 0.027, -lr * Pred Grad:  0.001, New P: 0.496
iter 10 loss: 0.482
Actual params: [0.9048, 0.4955]
-Original Grad: -0.014, -lr * Pred Grad:  -0.001, New P: 0.904
-Original Grad: 0.340, -lr * Pred Grad:  0.013, New P: 0.509
iter 11 loss: 0.491
Actual params: [0.9039, 0.5085]
-Original Grad: 0.013, -lr * Pred Grad:  0.008, New P: 0.912
-Original Grad: 0.311, -lr * Pred Grad:  0.013, New P: 0.521
iter 12 loss: 0.486
Actual params: [0.9119, 0.5211]
-Original Grad: -0.055, -lr * Pred Grad:  -0.018, New P: 0.894
-Original Grad: 0.138, -lr * Pred Grad:  0.004, New P: 0.526
iter 13 loss: 0.514
Actual params: [0.8939, 0.5255]
-Original Grad: -0.035, -lr * Pred Grad:  -0.013, New P: 0.881
-Original Grad: -0.243, -lr * Pred Grad:  -0.009, New P: 0.517
iter 14 loss: 0.526
Actual params: [0.881 , 0.5167]
-Original Grad: -0.026, -lr * Pred Grad:  -0.008, New P: 0.873
-Original Grad: 0.224, -lr * Pred Grad:  0.008, New P: 0.525
iter 15 loss: 0.538
Actual params: [0.8728, 0.5245]
-Original Grad: 0.145, -lr * Pred Grad:  0.032, New P: 0.905
-Original Grad: 0.012, -lr * Pred Grad:  0.001, New P: 0.526
iter 16 loss: 0.497
Actual params: [0.9051, 0.526 ]
-Original Grad: -0.049, -lr * Pred Grad:  -0.011, New P: 0.894
-Original Grad: -0.108, -lr * Pred Grad:  -0.004, New P: 0.522
iter 17 loss: 0.512
Actual params: [0.8939, 0.522 ]
-Original Grad: -0.020, -lr * Pred Grad:  -0.005, New P: 0.889
-Original Grad: -0.080, -lr * Pred Grad:  -0.003, New P: 0.519
iter 18 loss: 0.515
Actual params: [0.8893, 0.5191]
-Original Grad: 0.039, -lr * Pred Grad:  0.009, New P: 0.899
-Original Grad: 0.087, -lr * Pred Grad:  0.003, New P: 0.522
iter 19 loss: 0.505
Actual params: [0.8986, 0.5224]
-Original Grad: -0.024, -lr * Pred Grad:  -0.005, New P: 0.893
-Original Grad: -0.176, -lr * Pred Grad:  -0.006, New P: 0.516
iter 20 loss: 0.510
Actual params: [0.8935, 0.5164]
-Original Grad: 0.021, -lr * Pred Grad:  0.007, New P: 0.900
-Original Grad: -0.189, -lr * Pred Grad:  -0.006, New P: 0.510
Target params: [1.1812, 0.2779]
iter 0 loss: 0.743
Actual params: [0.5941, 0.5941]
-Original Grad: 0.127, -lr * Pred Grad:  0.014, New P: 0.608
-Original Grad: -1.234, -lr * Pred Grad:  -0.096, New P: 0.498
iter 1 loss: 0.423
Actual params: [0.6076, 0.4978]
-Original Grad: 0.089, -lr * Pred Grad:  0.077, New P: 0.684
-Original Grad: -0.530, -lr * Pred Grad:  -0.021, New P: 0.477
iter 2 loss: 0.366
Actual params: [0.6845, 0.477 ]
-Original Grad: 0.156, -lr * Pred Grad:  0.088, New P: 0.773
-Original Grad: -0.318, -lr * Pred Grad:  0.002, New P: 0.479
iter 3 loss: 0.351
Actual params: [0.7728, 0.4793]
-Original Grad: 0.137, -lr * Pred Grad:  0.029, New P: 0.802
-Original Grad: -0.346, -lr * Pred Grad:  -0.010, New P: 0.469
iter 4 loss: 0.339
Actual params: [0.802 , 0.4694]
-Original Grad: 0.025, -lr * Pred Grad:  -0.002, New P: 0.800
-Original Grad: -0.232, -lr * Pred Grad:  -0.010, New P: 0.460
iter 5 loss: 0.327
Actual params: [0.7999, 0.4597]
-Original Grad: -0.015, -lr * Pred Grad:  -0.009, New P: 0.791
-Original Grad: -0.266, -lr * Pred Grad:  -0.010, New P: 0.450
iter 6 loss: 0.315
Actual params: [0.7909, 0.4495]
-Original Grad: 0.116, -lr * Pred Grad:  0.023, New P: 0.814
-Original Grad: -0.103, -lr * Pred Grad:  -0.001, New P: 0.448
iter 7 loss: 0.315
Actual params: [0.814 , 0.4481]
-Original Grad: 0.072, -lr * Pred Grad:  0.010, New P: 0.824
-Original Grad: -0.186, -lr * Pred Grad:  -0.006, New P: 0.442
iter 8 loss: 0.311
Actual params: [0.8241, 0.4421]
-Original Grad: 0.087, -lr * Pred Grad:  0.014, New P: 0.838
-Original Grad: -0.107, -lr * Pred Grad:  -0.003, New P: 0.439
iter 9 loss: 0.312
Actual params: [0.8377, 0.4393]
-Original Grad: 0.001, -lr * Pred Grad:  -0.001, New P: 0.836
-Original Grad: -0.101, -lr * Pred Grad:  -0.004, New P: 0.435
iter 10 loss: 0.307
Actual params: [0.8362, 0.435 ]
-Original Grad: -0.023, -lr * Pred Grad:  -0.005, New P: 0.831
-Original Grad: -0.125, -lr * Pred Grad:  -0.006, New P: 0.429
iter 11 loss: 0.301
Actual params: [0.831 , 0.4294]
-Original Grad: 0.048, -lr * Pred Grad:  0.008, New P: 0.839
-Original Grad: -0.042, -lr * Pred Grad:  -0.001, New P: 0.428
iter 12 loss: 0.302
Actual params: [0.8393, 0.4281]
-Original Grad: 0.031, -lr * Pred Grad:  0.006, New P: 0.845
-Original Grad: -0.033, -lr * Pred Grad:  -0.001, New P: 0.427
iter 13 loss: 0.302
Actual params: [0.845 , 0.4268]
-Original Grad: 0.002, -lr * Pred Grad:  -0.000, New P: 0.845
-Original Grad: -0.072, -lr * Pred Grad:  -0.004, New P: 0.423
iter 14 loss: 0.299
Actual params: [0.8447, 0.4229]
-Original Grad: 0.055, -lr * Pred Grad:  0.010, New P: 0.855
-Original Grad: -0.063, -lr * Pred Grad:  -0.003, New P: 0.420
iter 15 loss: 0.300
Actual params: [0.8548, 0.4199]
-Original Grad: 0.012, -lr * Pred Grad:  0.001, New P: 0.856
-Original Grad: -0.091, -lr * Pred Grad:  -0.005, New P: 0.414
iter 16 loss: 0.296
Actual params: [0.8562, 0.4144]
-Original Grad: 0.019, -lr * Pred Grad:  0.004, New P: 0.860
-Original Grad: -0.029, -lr * Pred Grad:  -0.002, New P: 0.413
iter 17 loss: 0.296
Actual params: [0.86  , 0.4127]
-Original Grad: 0.018, -lr * Pred Grad:  0.004, New P: 0.864
-Original Grad: -0.021, -lr * Pred Grad:  -0.001, New P: 0.411
iter 18 loss: 0.296
Actual params: [0.864 , 0.4114]
-Original Grad: 0.023, -lr * Pred Grad:  0.005, New P: 0.869
-Original Grad: -0.050, -lr * Pred Grad:  -0.004, New P: 0.408
iter 19 loss: 0.295
Actual params: [0.8691, 0.4078]
-Original Grad: 0.001, -lr * Pred Grad:  -0.001, New P: 0.868
-Original Grad: -0.083, -lr * Pred Grad:  -0.007, New P: 0.401
iter 20 loss: 0.290
Actual params: [0.8679, 0.4009]
-Original Grad: 0.012, -lr * Pred Grad:  0.003, New P: 0.871
-Original Grad: -0.030, -lr * Pred Grad:  -0.003, New P: 0.398
Target params: [1.1812, 0.2779]
iter 0 loss: 0.596
Actual params: [0.5941, 0.5941]
-Original Grad: 0.595, -lr * Pred Grad:  0.139, New P: 0.733
-Original Grad: 0.212, -lr * Pred Grad:  0.088, New P: 0.682
iter 1 loss: 0.356
Actual params: [0.7327, 0.6816]
-Original Grad: 0.083, -lr * Pred Grad:  -0.002, New P: 0.731
-Original Grad: 0.075, -lr * Pred Grad:  0.045, New P: 0.726
iter 2 loss: 0.334
Actual params: [0.731 , 0.7265]
-Original Grad: -0.182, -lr * Pred Grad:  0.033, New P: 0.764
-Original Grad: -0.261, -lr * Pred Grad:  -0.107, New P: 0.619
iter 3 loss: 0.366
Actual params: [0.7639, 0.6195]
-Original Grad: 0.193, -lr * Pred Grad:  0.013, New P: 0.777
-Original Grad: 0.187, -lr * Pred Grad:  0.033, New P: 0.653
iter 4 loss: 0.323
Actual params: [0.7772, 0.6529]
-Original Grad: -0.071, -lr * Pred Grad:  -0.019, New P: 0.758
-Original Grad: -0.040, -lr * Pred Grad:  0.011, New P: 0.664
iter 5 loss: 0.338
Actual params: [0.7578, 0.6642]
-Original Grad: -0.138, -lr * Pred Grad:  0.013, New P: 0.771
-Original Grad: -0.168, -lr * Pred Grad:  -0.037, New P: 0.627
iter 6 loss: 0.350
Actual params: [0.7709, 0.6273]
-Original Grad: 0.192, -lr * Pred Grad:  0.023, New P: 0.793
-Original Grad: 0.163, -lr * Pred Grad:  -0.000, New P: 0.627
iter 7 loss: 0.320
Actual params: [0.7935, 0.627 ]
-Original Grad: -0.105, -lr * Pred Grad:  -0.006, New P: 0.788
-Original Grad: -0.096, -lr * Pred Grad:  -0.006, New P: 0.621
iter 8 loss: 0.331
Actual params: [0.7879, 0.6208]
-Original Grad: -0.045, -lr * Pred Grad:  0.004, New P: 0.792
-Original Grad: -0.049, -lr * Pred Grad:  -0.010, New P: 0.611
iter 9 loss: 0.334
Actual params: [0.7918, 0.611 ]
-Original Grad: -0.004, -lr * Pred Grad:  0.002, New P: 0.794
-Original Grad: -0.006, -lr * Pred Grad:  -0.003, New P: 0.608
iter 10 loss: 0.333
Actual params: [0.7938, 0.6083]
-Original Grad: 0.052, -lr * Pred Grad:  0.013, New P: 0.807
-Original Grad: 0.031, -lr * Pred Grad:  -0.011, New P: 0.597
iter 11 loss: 0.324
Actual params: [0.8067, 0.5972]
-Original Grad: 0.072, -lr * Pred Grad:  0.035, New P: 0.842
-Original Grad: 0.019, -lr * Pred Grad:  -0.037, New P: 0.560
iter 12 loss: 0.304
Actual params: [0.8416, 0.5598]
-Original Grad: -0.125, -lr * Pred Grad:  0.023, New P: 0.864
-Original Grad: -0.144, -lr * Pred Grad:  -0.038, New P: 0.522
iter 13 loss: 0.302
Actual params: [0.8643, 0.522 ]
-Original Grad: -0.049, -lr * Pred Grad:  0.017, New P: 0.882
-Original Grad: -0.067, -lr * Pred Grad:  -0.025, New P: 0.497
iter 14 loss: 0.296
Actual params: [0.8816, 0.497 ]
-Original Grad: -0.004, -lr * Pred Grad:  0.010, New P: 0.892
-Original Grad: -0.017, -lr * Pred Grad:  -0.013, New P: 0.484
iter 15 loss: 0.293
Actual params: [0.8919, 0.4839]
-Original Grad: 0.051, -lr * Pred Grad:  -0.000, New P: 0.892
-Original Grad: 0.044, -lr * Pred Grad:  0.004, New P: 0.488
iter 16 loss: 0.291
Actual params: [0.8916, 0.4881]
-Original Grad: 0.079, -lr * Pred Grad:  0.019, New P: 0.910
-Original Grad: 0.047, -lr * Pred Grad:  -0.018, New P: 0.470
iter 17 loss: 0.283
Actual params: [0.9105, 0.4701]
-Original Grad: 0.042, -lr * Pred Grad:  0.021, New P: 0.931
-Original Grad: 0.015, -lr * Pred Grad:  -0.023, New P: 0.447
iter 18 loss: 0.277
Actual params: [0.931 , 0.4472]
-Original Grad: 0.034, -lr * Pred Grad:  0.024, New P: 0.955
-Original Grad: 0.006, -lr * Pred Grad:  -0.028, New P: 0.419
iter 19 loss: 0.274
Actual params: [0.9551, 0.4194]
-Original Grad: -0.031, -lr * Pred Grad:  0.032, New P: 0.987
-Original Grad: -0.056, -lr * Pred Grad:  -0.042, New P: 0.378
iter 20 loss: 0.275
Actual params: [0.9869, 0.3776]
-Original Grad: 0.065, -lr * Pred Grad:  0.007, New P: 0.994
-Original Grad: 0.051, -lr * Pred Grad:  -0.003, New P: 0.374
Target params: [1.1812, 0.2779]
iter 0 loss: 0.722
Actual params: [0.5941, 0.5941]
-Original Grad: 0.144, -lr * Pred Grad:  0.461, New P: 1.055
-Original Grad: -0.313, -lr * Pred Grad:  -0.286, New P: 0.308
iter 1 loss: 0.201
Actual params: [1.0555, 0.308 ]
-Original Grad: 0.121, -lr * Pred Grad:  0.184, New P: 1.239
-Original Grad: 0.077, -lr * Pred Grad:  0.045, New P: 0.353
iter 2 loss: 0.309
Actual params: [1.2395, 0.3526]
-Original Grad: -0.216, -lr * Pred Grad:  -0.071, New P: 1.169
-Original Grad: -0.270, -lr * Pred Grad:  -0.041, New P: 0.311
iter 3 loss: 0.224
Actual params: [1.1687, 0.3112]
-Original Grad: 0.116, -lr * Pred Grad:  0.062, New P: 1.231
-Original Grad: 0.061, -lr * Pred Grad:  -0.015, New P: 0.296
iter 4 loss: 0.253
Actual params: [1.2311, 0.296 ]
-Original Grad: -0.078, -lr * Pred Grad:  -0.012, New P: 1.219
-Original Grad: -0.092, -lr * Pred Grad:  -0.014, New P: 0.282
iter 5 loss: 0.234
Actual params: [1.2187, 0.282 ]
-Original Grad: -0.011, -lr * Pred Grad:  0.013, New P: 1.231
-Original Grad: -0.043, -lr * Pred Grad:  -0.018, New P: 0.264
iter 6 loss: 0.229
Actual params: [1.2313, 0.264 ]
-Original Grad: -0.022, -lr * Pred Grad:  0.005, New P: 1.236
-Original Grad: -0.040, -lr * Pred Grad:  -0.012, New P: 0.252
iter 7 loss: 0.224
Actual params: [1.2359, 0.2524]
-Original Grad: 0.085, -lr * Pred Grad:  0.039, New P: 1.275
-Original Grad: 0.033, -lr * Pred Grad:  -0.022, New P: 0.231
iter 8 loss: 0.234
Actual params: [1.2746, 0.2307]
-Original Grad: 0.033, -lr * Pred Grad:  0.038, New P: 1.313
-Original Grad: -0.035, -lr * Pred Grad:  -0.036, New P: 0.195
iter 9 loss: 0.233
Actual params: [1.3125, 0.1948]
-Original Grad: 0.036, -lr * Pred Grad:  0.033, New P: 1.345
-Original Grad: -0.021, -lr * Pred Grad:  -0.028, New P: 0.167
iter 10 loss: 0.231
Actual params: [1.3451, 0.1666]
-Original Grad: 0.011, -lr * Pred Grad:  0.032, New P: 1.377
-Original Grad: -0.056, -lr * Pred Grad:  -0.035, New P: 0.132
iter 11 loss: 0.227
Actual params: [1.3769, 0.1315]
-Original Grad: 0.046, -lr * Pred Grad:  0.033, New P: 1.410
-Original Grad: -0.015, -lr * Pred Grad:  -0.026, New P: 0.105
iter 12 loss: 0.230
Actual params: [1.4101, 0.1054]
-Original Grad: 0.015, -lr * Pred Grad:  0.020, New P: 1.430
-Original Grad: -0.030, -lr * Pred Grad:  -0.021, New P: 0.085
iter 13 loss: 0.229
Actual params: [1.4304, 0.0849]
-Original Grad: -0.011, -lr * Pred Grad:  -0.001, New P: 1.429
-Original Grad: -0.015, -lr * Pred Grad:  -0.004, New P: 0.081
iter 14 loss: 0.227
Actual params: [1.4294, 0.0814]
-Original Grad: 0.010, -lr * Pred Grad:  0.012, New P: 1.441
-Original Grad: -0.018, -lr * Pred Grad:  -0.013, New P: 0.068
iter 15 loss: 0.226
Actual params: [1.4414, 0.0681]
-Original Grad: -0.008, -lr * Pred Grad:  0.008, New P: 1.450
-Original Grad: -0.037, -lr * Pred Grad:  -0.017, New P: 0.051
iter 16 loss: 0.223
Actual params: [1.4497, 0.0513]
-Original Grad: -0.006, -lr * Pred Grad:  0.002, New P: 1.452
-Original Grad: -0.016, -lr * Pred Grad:  -0.006, New P: 0.045
iter 17 loss: 0.221
Actual params: [1.4515, 0.0448]
-Original Grad: 0.024, -lr * Pred Grad:  0.019, New P: 1.470
-Original Grad: -0.018, -lr * Pred Grad:  -0.018, New P: 0.027
iter 18 loss: 0.218
Actual params: [1.4701, 0.027 ]
-Original Grad: 0.007, -lr * Pred Grad:  0.008, New P: 1.478
-Original Grad: -0.014, -lr * Pred Grad:  -0.011, New P: 0.016
iter 19 loss: 0.218
Actual params: [1.4784, 0.0164]
-Original Grad: 0.016, -lr * Pred Grad:  0.014, New P: 1.492
-Original Grad: -0.019, -lr * Pred Grad:  -0.016, New P: 0.000
iter 20 loss: 0.216
Actual params: [1.4922e+00, 2.7527e-04]
-Original Grad: -0.019, -lr * Pred Grad:  -0.007, New P: 1.485
-Original Grad: -0.011, -lr * Pred Grad:  -0.001, New P: -0.001
Target params: [1.1812, 0.2779]
iter 0 loss: 0.519
Actual params: [0.5941, 0.5941]
-Original Grad: 0.308, -lr * Pred Grad:  0.127, New P: 0.721
-Original Grad: -0.286, -lr * Pred Grad:  -0.126, New P: 0.468
iter 1 loss: 0.506
Actual params: [0.7209, 0.4685]
-Original Grad: 0.448, -lr * Pred Grad:  0.042, New P: 0.763
-Original Grad: 0.081, -lr * Pred Grad:  0.032, New P: 0.501
iter 2 loss: 0.516
Actual params: [0.7629, 0.5009]
-Original Grad: -0.167, -lr * Pred Grad:  -0.001, New P: 0.762
-Original Grad: -0.238, -lr * Pred Grad:  -0.035, New P: 0.466
iter 3 loss: 0.506
Actual params: [0.7618, 0.466 ]
-Original Grad: 0.097, -lr * Pred Grad:  0.009, New P: 0.771
-Original Grad: -0.002, -lr * Pred Grad:  -0.008, New P: 0.458
iter 4 loss: 0.506
Actual params: [0.7713, 0.4581]
-Original Grad: 0.025, -lr * Pred Grad:  0.004, New P: 0.775
-Original Grad: -0.020, -lr * Pred Grad:  -0.006, New P: 0.452
iter 5 loss: 0.505
Actual params: [0.7752, 0.4517]
-Original Grad: 0.008, -lr * Pred Grad:  0.002, New P: 0.778
-Original Grad: -0.017, -lr * Pred Grad:  -0.005, New P: 0.447
iter 6 loss: 0.504
Actual params: [0.7775, 0.4472]
-Original Grad: 0.063, -lr * Pred Grad:  0.004, New P: 0.782
-Original Grad: 0.025, -lr * Pred Grad:  -0.001, New P: 0.447
iter 7 loss: 0.505
Actual params: [0.7815, 0.4465]
-Original Grad: -0.065, -lr * Pred Grad:  0.001, New P: 0.783
-Original Grad: -0.076, -lr * Pred Grad:  -0.011, New P: 0.436
iter 8 loss: 0.502
Actual params: [0.7827, 0.4355]
-Original Grad: 0.122, -lr * Pred Grad:  0.006, New P: 0.789
-Original Grad: 0.066, -lr * Pred Grad:  0.001, New P: 0.437
iter 9 loss: 0.503
Actual params: [0.7889, 0.4366]
-Original Grad: 0.007, -lr * Pred Grad:  0.001, New P: 0.790
-Original Grad: -0.003, -lr * Pred Grad:  -0.002, New P: 0.435
iter 10 loss: 0.503
Actual params: [0.7903, 0.4347]
-Original Grad: -0.022, -lr * Pred Grad:  -0.001, New P: 0.790
-Original Grad: -0.016, -lr * Pred Grad:  -0.001, New P: 0.433
iter 11 loss: 0.503
Actual params: [0.7897, 0.4333]
-Original Grad: 0.067, -lr * Pred Grad:  0.004, New P: 0.794
-Original Grad: 0.033, -lr * Pred Grad:  -0.001, New P: 0.432
iter 12 loss: 0.502
Actual params: [0.794 , 0.4323]
-Original Grad: -0.024, -lr * Pred Grad:  0.005, New P: 0.799
-Original Grad: -0.048, -lr * Pred Grad:  -0.010, New P: 0.422
iter 13 loss: 0.501
Actual params: [0.7989, 0.4222]
-Original Grad: -0.008, -lr * Pred Grad:  0.000, New P: 0.799
-Original Grad: -0.008, -lr * Pred Grad:  -0.001, New P: 0.421
iter 14 loss: 0.501
Actual params: [0.799 , 0.4213]
-Original Grad: -0.010, -lr * Pred Grad:  0.001, New P: 0.800
-Original Grad: -0.012, -lr * Pred Grad:  -0.002, New P: 0.419
iter 15 loss: 0.500
Actual params: [0.7997, 0.4194]
-Original Grad: 0.013, -lr * Pred Grad:  0.000, New P: 0.800
-Original Grad: 0.009, -lr * Pred Grad:  0.000, New P: 0.420
iter 16 loss: 0.501
Actual params: [0.8001, 0.4199]
-Original Grad: -0.014, -lr * Pred Grad:  0.001, New P: 0.801
-Original Grad: -0.016, -lr * Pred Grad:  -0.003, New P: 0.417
iter 17 loss: 0.500
Actual params: [0.8011, 0.417 ]
-Original Grad: -0.005, -lr * Pred Grad:  0.001, New P: 0.802
-Original Grad: -0.009, -lr * Pred Grad:  -0.003, New P: 0.415
iter 18 loss: 0.500
Actual params: [0.8024, 0.4145]
-Original Grad: 0.014, -lr * Pred Grad:  0.004, New P: 0.806
-Original Grad: -0.001, -lr * Pred Grad:  -0.005, New P: 0.409
iter 19 loss: 0.499
Actual params: [0.8065, 0.4095]
-Original Grad: 0.012, -lr * Pred Grad:  0.003, New P: 0.809
-Original Grad: 0.002, -lr * Pred Grad:  -0.003, New P: 0.406
iter 20 loss: 0.498
Actual params: [0.8093, 0.4063]
-Original Grad: -0.042, -lr * Pred Grad:  0.001, New P: 0.810
-Original Grad: -0.037, -lr * Pred Grad:  -0.006, New P: 0.401
Target params: [1.1812, 0.2779]
iter 0 loss: 0.729
Actual params: [0.5941, 0.5941]
-Original Grad: -0.110, -lr * Pred Grad:  -0.151, New P: 0.443
-Original Grad: -0.561, -lr * Pred Grad:  -0.216, New P: 0.378
iter 1 loss: 0.482
Actual params: [0.4434, 0.3777]
-Original Grad: -0.007, -lr * Pred Grad:  0.239, New P: 0.682
-Original Grad: -0.258, -lr * Pred Grad:  -0.121, New P: 0.257
iter 2 loss: 0.479
Actual params: [0.6822, 0.2567]
-Original Grad: -0.026, -lr * Pred Grad:  0.067, New P: 0.749
-Original Grad: -0.274, -lr * Pred Grad:  -0.084, New P: 0.173
iter 3 loss: 0.447
Actual params: [0.7489, 0.1727]
-Original Grad: -0.019, -lr * Pred Grad:  0.055, New P: 0.804
-Original Grad: -0.218, -lr * Pred Grad:  -0.060, New P: 0.113
iter 4 loss: 0.418
Actual params: [0.804 , 0.1132]
-Original Grad: 0.000, -lr * Pred Grad:  0.133, New P: 0.937
-Original Grad: -0.109, -lr * Pred Grad:  -0.041, New P: 0.072
iter 5 loss: 0.383
Actual params: [0.9368, 0.0719]
-Original Grad: 0.023, -lr * Pred Grad:  0.293, New P: 1.230
-Original Grad: -0.100, -lr * Pred Grad:  -0.051, New P: 0.021
iter 6 loss: 0.295
Actual params: [1.2302, 0.0214]
-Original Grad: 0.007, -lr * Pred Grad:  0.154, New P: 1.385
-Original Grad: -0.189, -lr * Pred Grad:  -0.047, New P: -0.025
iter 7 loss: 0.275
Actual params: [ 1.3847, -0.0252]
-Original Grad: 0.001, -lr * Pred Grad:  0.043, New P: 1.428
-Original Grad: -0.125, -lr * Pred Grad:  -0.024, New P: -0.049
iter 8 loss: 0.269
Actual params: [ 1.4276, -0.0494]
-Original Grad: -0.007, -lr * Pred Grad:  -0.036, New P: 1.391
-Original Grad: -0.075, -lr * Pred Grad:  -0.013, New P: -0.062
iter 9 loss: 0.269
Actual params: [ 1.3912, -0.0619]
-Original Grad: -0.004, -lr * Pred Grad:  -0.016, New P: 1.376
-Original Grad: -0.057, -lr * Pred Grad:  -0.011, New P: -0.072
iter 10 loss: 0.268
Actual params: [ 1.3756, -0.0725]
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 1.390
-Original Grad: -0.067, -lr * Pred Grad:  -0.014, New P: -0.086
iter 11 loss: 0.266
Actual params: [ 1.3903, -0.0862]
-Original Grad: 0.002, -lr * Pred Grad:  0.016, New P: 1.406
-Original Grad: -0.067, -lr * Pred Grad:  -0.014, New P: -0.100
iter 12 loss: 0.264
Actual params: [ 1.4065, -0.1002]
-Original Grad: -0.004, -lr * Pred Grad:  -0.019, New P: 1.388
-Original Grad: -0.049, -lr * Pred Grad:  -0.011, New P: -0.111
iter 13 loss: 0.263
Actual params: [ 1.3877, -0.1107]
-Original Grad: -0.008, -lr * Pred Grad:  -0.044, New P: 1.344
-Original Grad: -0.045, -lr * Pred Grad:  -0.010, New P: -0.121
iter 14 loss: 0.264
Actual params: [ 1.3441, -0.1207]
-Original Grad: 0.015, -lr * Pred Grad:  0.079, New P: 1.423
-Original Grad: -0.061, -lr * Pred Grad:  -0.014, New P: -0.135
iter 15 loss: 0.267
Actual params: [ 1.4231, -0.1347]
-Original Grad: 0.002, -lr * Pred Grad:  0.010, New P: 1.433
-Original Grad: -0.035, -lr * Pred Grad:  -0.009, New P: -0.144
iter 16 loss: 0.265
Actual params: [ 1.433 , -0.1437]
-Original Grad: -0.012, -lr * Pred Grad:  -0.062, New P: 1.371
-Original Grad: -0.057, -lr * Pred Grad:  -0.014, New P: -0.158
iter 17 loss: 0.259
Actual params: [ 1.3709, -0.158 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.007, New P: 1.364
-Original Grad: -0.038, -lr * Pred Grad:  -0.010, New P: -0.168
iter 18 loss: 0.259
Actual params: [ 1.3639, -0.1683]
-Original Grad: 0.008, -lr * Pred Grad:  0.036, New P: 1.400
-Original Grad: -0.042, -lr * Pred Grad:  -0.011, New P: -0.179
iter 19 loss: 0.258
Actual params: [ 1.4004, -0.1794]
-Original Grad: 0.005, -lr * Pred Grad:  0.017, New P: 1.417
-Original Grad: -0.031, -lr * Pred Grad:  -0.009, New P: -0.188
iter 20 loss: 0.255
Actual params: [ 1.4174, -0.1881]
-Original Grad: -0.009, -lr * Pred Grad:  -0.040, New P: 1.377
-Original Grad: -0.020, -lr * Pred Grad:  -0.008, New P: -0.196
Target params: [1.1812, 0.2779]
iter 0 loss: 0.763
Actual params: [0.5941, 0.5941]
-Original Grad: 0.602, -lr * Pred Grad:  0.263, New P: 0.858
-Original Grad: 0.017, -lr * Pred Grad:  0.004, New P: 0.598
iter 1 loss: 0.293
Actual params: [0.8575, 0.5984]
-Original Grad: 0.081, -lr * Pred Grad:  0.000, New P: 0.858
-Original Grad: -0.925, -lr * Pred Grad:  -0.065, New P: 0.533
iter 2 loss: 0.338
Actual params: [0.8578, 0.5332]
-Original Grad: 0.040, -lr * Pred Grad:  -0.002, New P: 0.856
-Original Grad: -0.530, -lr * Pred Grad:  -0.021, New P: 0.512
iter 3 loss: 0.361
Actual params: [0.856, 0.512]
-Original Grad: 0.149, -lr * Pred Grad:  0.030, New P: 0.886
-Original Grad: -0.281, -lr * Pred Grad:  -0.008, New P: 0.504
iter 4 loss: 0.326
Actual params: [0.8856, 0.5042]
-Original Grad: 0.156, -lr * Pred Grad:  0.021, New P: 0.907
-Original Grad: -0.106, -lr * Pred Grad:  -0.003, New P: 0.502
iter 5 loss: 0.303
Actual params: [0.9065, 0.5016]
-Original Grad: 0.045, -lr * Pred Grad:  0.005, New P: 0.912
-Original Grad: -0.161, -lr * Pred Grad:  -0.005, New P: 0.497
iter 6 loss: 0.301
Actual params: [0.9115, 0.4965]
-Original Grad: 0.104, -lr * Pred Grad:  0.011, New P: 0.923
-Original Grad: -0.046, -lr * Pred Grad:  -0.001, New P: 0.495
iter 7 loss: 0.288
Actual params: [0.9227, 0.4954]
-Original Grad: 0.010, -lr * Pred Grad:  0.000, New P: 0.923
-Original Grad: -0.205, -lr * Pred Grad:  -0.007, New P: 0.489
iter 8 loss: 0.293
Actual params: [0.9228, 0.4887]
-Original Grad: 0.035, -lr * Pred Grad:  0.004, New P: 0.927
-Original Grad: -0.049, -lr * Pred Grad:  -0.002, New P: 0.487
iter 9 loss: 0.290
Actual params: [0.9268, 0.4872]
-Original Grad: -0.004, -lr * Pred Grad:  -0.001, New P: 0.926
-Original Grad: -0.020, -lr * Pred Grad:  -0.001, New P: 0.486
iter 10 loss: 0.291
Actual params: [0.9262, 0.4864]
-Original Grad: 0.002, -lr * Pred Grad:  -0.000, New P: 0.926
-Original Grad: -0.064, -lr * Pred Grad:  -0.002, New P: 0.484
iter 11 loss: 0.293
Actual params: [0.9261, 0.4839]
-Original Grad: 0.010, -lr * Pred Grad:  0.001, New P: 0.928
-Original Grad: -0.023, -lr * Pred Grad:  -0.001, New P: 0.483
iter 12 loss: 0.292
Actual params: [0.9276, 0.483 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.001, New P: 0.927
-Original Grad: -0.046, -lr * Pred Grad:  -0.002, New P: 0.481
iter 13 loss: 0.294
Actual params: [0.927, 0.481]
-Original Grad: 0.008, -lr * Pred Grad:  0.002, New P: 0.929
-Original Grad: 0.005, -lr * Pred Grad:  0.000, New P: 0.481
iter 14 loss: 0.292
Actual params: [0.9286, 0.4813]
-Original Grad: -0.002, -lr * Pred Grad:  -0.000, New P: 0.928
-Original Grad: 0.008, -lr * Pred Grad:  0.000, New P: 0.482
iter 15 loss: 0.292
Actual params: [0.9283, 0.4817]
-Original Grad: -0.007, -lr * Pred Grad:  -0.002, New P: 0.927
-Original Grad: 0.005, -lr * Pred Grad:  0.000, New P: 0.482
iter 16 loss: 0.294
Actual params: [0.9267, 0.4819]
-Original Grad: -0.004, -lr * Pred Grad:  -0.001, New P: 0.926
-Original Grad: 0.007, -lr * Pred Grad:  0.000, New P: 0.482
iter 17 loss: 0.294
Actual params: [0.9256, 0.4824]
-Original Grad: 0.003, -lr * Pred Grad:  0.001, New P: 0.926
-Original Grad: -0.021, -lr * Pred Grad:  -0.001, New P: 0.481
iter 18 loss: 0.295
Actual params: [0.9262, 0.4809]
-Original Grad: 0.011, -lr * Pred Grad:  0.004, New P: 0.930
-Original Grad: 0.005, -lr * Pred Grad:  0.001, New P: 0.482
iter 19 loss: 0.290
Actual params: [0.9301, 0.4815]
-Original Grad: -0.003, -lr * Pred Grad:  -0.002, New P: 0.929
-Original Grad: -0.028, -lr * Pred Grad:  -0.002, New P: 0.479
iter 20 loss: 0.293
Actual params: [0.9285, 0.4792]
-Original Grad: -0.001, -lr * Pred Grad:  -0.000, New P: 0.928
-Original Grad: 0.006, -lr * Pred Grad:  0.001, New P: 0.480
Target params: [1.1812, 0.2779]
iter 0 loss: 0.333
Actual params: [0.5941, 0.5941]
-Original Grad: 0.085, -lr * Pred Grad:  0.160, New P: 0.754
-Original Grad: 0.031, -lr * Pred Grad:  0.077, New P: 0.671
iter 1 loss: 0.454
Actual params: [0.7543, 0.6713]
-Original Grad: -0.326, -lr * Pred Grad:  -0.043, New P: 0.711
-Original Grad: -0.050, -lr * Pred Grad:  0.012, New P: 0.684
iter 2 loss: 0.411
Actual params: [0.7109, 0.6836]
-Original Grad: -0.124, -lr * Pred Grad:  0.005, New P: 0.716
-Original Grad: -0.087, -lr * Pred Grad:  -0.108, New P: 0.576
iter 3 loss: 0.356
Actual params: [0.716 , 0.5757]
-Original Grad: -0.035, -lr * Pred Grad:  -0.008, New P: 0.708
-Original Grad: 0.011, -lr * Pred Grad:  0.024, New P: 0.600
iter 4 loss: 0.360
Actual params: [0.7076, 0.6   ]
-Original Grad: 0.033, -lr * Pred Grad:  0.002, New P: 0.710
-Original Grad: 0.013, -lr * Pred Grad:  0.008, New P: 0.608
iter 5 loss: 0.365
Actual params: [0.7096, 0.6081]
-Original Grad: 0.016, -lr * Pred Grad:  -0.001, New P: 0.709
-Original Grad: 0.014, -lr * Pred Grad:  0.011, New P: 0.620
iter 6 loss: 0.370
Actual params: [0.7091, 0.6195]
-Original Grad: 0.033, -lr * Pred Grad:  -0.005, New P: 0.704
-Original Grad: 0.053, -lr * Pred Grad:  0.039, New P: 0.658
iter 7 loss: 0.387
Actual params: [0.7042, 0.658 ]
-Original Grad: -0.032, -lr * Pred Grad:  -0.010, New P: 0.695
-Original Grad: 0.031, -lr * Pred Grad:  0.021, New P: 0.679
iter 8 loss: 0.391
Actual params: [0.6946, 0.6788]
-Original Grad: 0.015, -lr * Pred Grad:  0.003, New P: 0.698
-Original Grad: -0.010, -lr * Pred Grad:  -0.005, New P: 0.673
iter 9 loss: 0.391
Actual params: [0.6981, 0.6733]
-Original Grad: -0.015, -lr * Pred Grad:  0.003, New P: 0.701
-Original Grad: -0.058, -lr * Pred Grad:  -0.018, New P: 0.655
iter 10 loss: 0.383
Actual params: [0.7008, 0.6554]
-Original Grad: -0.010, -lr * Pred Grad:  -0.000, New P: 0.701
-Original Grad: -0.017, -lr * Pred Grad:  -0.005, New P: 0.651
iter 11 loss: 0.380
Actual params: [0.7007, 0.6505]
-Original Grad: 0.017, -lr * Pred Grad:  0.002, New P: 0.703
-Original Grad: 0.005, -lr * Pred Grad:  0.000, New P: 0.651
iter 12 loss: 0.382
Actual params: [0.703 , 0.6506]
-Original Grad: -0.062, -lr * Pred Grad:  -0.009, New P: 0.694
-Original Grad: -0.020, -lr * Pred Grad:  -0.000, New P: 0.650
iter 13 loss: 0.375
Actual params: [0.6945, 0.6504]
-Original Grad: 0.038, -lr * Pred Grad:  0.004, New P: 0.698
-Original Grad: 0.027, -lr * Pred Grad:  0.005, New P: 0.655
iter 14 loss: 0.381
Actual params: [0.6982, 0.6555]
-Original Grad: 0.007, -lr * Pred Grad:  0.006, New P: 0.704
-Original Grad: -0.051, -lr * Pred Grad:  -0.016, New P: 0.639
iter 15 loss: 0.376
Actual params: [0.7042, 0.639 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.704
-Original Grad: 0.006, -lr * Pred Grad:  0.002, New P: 0.641
iter 16 loss: 0.377
Actual params: [0.7037, 0.6407]
-Original Grad: -0.006, -lr * Pred Grad:  -0.002, New P: 0.702
-Original Grad: 0.012, -lr * Pred Grad:  0.004, New P: 0.644
iter 17 loss: 0.377
Actual params: [0.7015, 0.6444]
-Original Grad: 0.002, -lr * Pred Grad:  0.002, New P: 0.703
-Original Grad: -0.016, -lr * Pred Grad:  -0.005, New P: 0.640
iter 18 loss: 0.376
Actual params: [0.7034, 0.6397]
-Original Grad: 0.012, -lr * Pred Grad:  0.003, New P: 0.707
-Original Grad: -0.010, -lr * Pred Grad:  -0.004, New P: 0.636
iter 19 loss: 0.376
Actual params: [0.7067, 0.6356]
-Original Grad: 0.017, -lr * Pred Grad:  0.001, New P: 0.707
-Original Grad: 0.028, -lr * Pred Grad:  0.007, New P: 0.642
iter 20 loss: 0.380
Actual params: [0.7072, 0.6422]
-Original Grad: 0.061, -lr * Pred Grad:  0.010, New P: 0.717
-Original Grad: 0.033, -lr * Pred Grad:  0.003, New P: 0.645
Target params: [1.1812, 0.2779]
iter 0 loss: 0.324
Actual params: [0.5941, 0.5941]
-Original Grad: 0.346, -lr * Pred Grad:  0.134, New P: 0.728
-Original Grad: -0.126, -lr * Pred Grad:  0.105, New P: 0.699
iter 1 loss: 0.244
Actual params: [0.7282, 0.6986]
-Original Grad: -0.269, -lr * Pred Grad:  0.007, New P: 0.735
-Original Grad: -0.322, -lr * Pred Grad:  -0.059, New P: 0.640
iter 2 loss: 0.299
Actual params: [0.7355, 0.6396]
-Original Grad: -0.437, -lr * Pred Grad:  -0.002, New P: 0.733
-Original Grad: -0.280, -lr * Pred Grad:  -0.028, New P: 0.612
iter 3 loss: 0.256
Actual params: [0.7332, 0.6119]
-Original Grad: -0.274, -lr * Pred Grad:  -0.003, New P: 0.731
-Original Grad: -0.153, -lr * Pred Grad:  -0.013, New P: 0.599
iter 4 loss: 0.242
Actual params: [0.7306, 0.5994]
-Original Grad: -0.340, -lr * Pred Grad:  -0.009, New P: 0.722
-Original Grad: -0.105, -lr * Pred Grad:  0.002, New P: 0.601
iter 5 loss: 0.216
Actual params: [0.722 , 0.6014]
-Original Grad: -0.051, -lr * Pred Grad:  0.006, New P: 0.728
-Original Grad: -0.123, -lr * Pred Grad:  -0.021, New P: 0.580
iter 6 loss: 0.233
Actual params: [0.7276, 0.58  ]
-Original Grad: -0.373, -lr * Pred Grad:  -0.002, New P: 0.725
-Original Grad: -0.201, -lr * Pred Grad:  -0.016, New P: 0.564
iter 7 loss: 0.231
Actual params: [0.7253, 0.5643]
-Original Grad: -0.255, -lr * Pred Grad:  -0.000, New P: 0.725
-Original Grad: -0.155, -lr * Pred Grad:  -0.014, New P: 0.550
iter 8 loss: 0.231
Actual params: [0.7251, 0.5505]
-Original Grad: 0.007, -lr * Pred Grad:  0.004, New P: 0.729
-Original Grad: -0.048, -lr * Pred Grad:  -0.010, New P: 0.540
iter 9 loss: 0.235
Actual params: [0.7287, 0.5401]
-Original Grad: -0.032, -lr * Pred Grad:  0.001, New P: 0.730
-Original Grad: -0.041, -lr * Pred Grad:  -0.007, New P: 0.533
iter 10 loss: 0.236
Actual params: [0.7301, 0.5334]
-Original Grad: 0.085, -lr * Pred Grad:  0.006, New P: 0.736
-Original Grad: -0.048, -lr * Pred Grad:  -0.015, New P: 0.518
iter 11 loss: 0.241
Actual params: [0.7361, 0.518 ]
-Original Grad: -0.066, -lr * Pred Grad:  0.001, New P: 0.737
-Original Grad: -0.052, -lr * Pred Grad:  -0.008, New P: 0.510
iter 12 loss: 0.241
Actual params: [0.737 , 0.5102]
-Original Grad: -0.077, -lr * Pred Grad:  0.002, New P: 0.738
-Original Grad: -0.068, -lr * Pred Grad:  -0.011, New P: 0.499
iter 13 loss: 0.242
Actual params: [0.7385, 0.4992]
-Original Grad: -0.130, -lr * Pred Grad:  0.001, New P: 0.740
-Original Grad: -0.091, -lr * Pred Grad:  -0.014, New P: 0.486
iter 14 loss: 0.242
Actual params: [0.7397, 0.4855]
-Original Grad: 0.065, -lr * Pred Grad:  0.004, New P: 0.744
-Original Grad: -0.032, -lr * Pred Grad:  -0.012, New P: 0.473
iter 15 loss: 0.244
Actual params: [0.7442, 0.4732]
-Original Grad: 0.099, -lr * Pred Grad:  0.007, New P: 0.751
-Original Grad: -0.062, -lr * Pred Grad:  -0.021, New P: 0.452
iter 16 loss: 0.246
Actual params: [0.7514, 0.4519]
-Original Grad: 0.015, -lr * Pred Grad:  0.002, New P: 0.753
-Original Grad: -0.027, -lr * Pred Grad:  -0.008, New P: 0.444
iter 17 loss: 0.246
Actual params: [0.7535, 0.4442]
-Original Grad: -0.078, -lr * Pred Grad:  -0.002, New P: 0.751
-Original Grad: -0.008, -lr * Pred Grad:  0.002, New P: 0.447
iter 18 loss: 0.246
Actual params: [0.7514, 0.4467]
-Original Grad: -0.133, -lr * Pred Grad:  -0.003, New P: 0.748
-Original Grad: -0.020, -lr * Pred Grad:  0.002, New P: 0.449
iter 19 loss: 0.243
Actual params: [0.7481, 0.449 ]
-Original Grad: -0.044, -lr * Pred Grad:  0.000, New P: 0.748
-Original Grad: -0.027, -lr * Pred Grad:  -0.006, New P: 0.443
iter 20 loss: 0.242
Actual params: [0.7483, 0.4435]
-Original Grad: -0.067, -lr * Pred Grad:  -0.001, New P: 0.748
-Original Grad: -0.027, -lr * Pred Grad:  -0.005, New P: 0.439
Target params: [1.1812, 0.2779]
iter 0 loss: 0.541
Actual params: [0.5941, 0.5941]
-Original Grad: 0.300, -lr * Pred Grad:  0.254, New P: 0.848
-Original Grad: -0.391, -lr * Pred Grad:  -0.266, New P: 0.328
iter 1 loss: 0.341
Actual params: [0.8476, 0.328 ]
-Original Grad: 0.074, -lr * Pred Grad:  0.201, New P: 1.049
-Original Grad: 0.028, -lr * Pred Grad:  0.143, New P: 0.471
iter 2 loss: 0.206
Actual params: [1.0489, 0.4714]
-Original Grad: 0.160, -lr * Pred Grad:  0.158, New P: 1.207
-Original Grad: -0.012, -lr * Pred Grad:  0.078, New P: 0.550
iter 3 loss: 0.148
Actual params: [1.2072, 0.5496]
-Original Grad: 0.099, -lr * Pred Grad:  0.047, New P: 1.254
-Original Grad: -0.051, -lr * Pred Grad:  -0.026, New P: 0.524
iter 4 loss: 0.143
Actual params: [1.2541, 0.524 ]
-Original Grad: 0.032, -lr * Pred Grad:  0.006, New P: 1.260
-Original Grad: -0.045, -lr * Pred Grad:  -0.034, New P: 0.490
iter 5 loss: 0.146
Actual params: [1.2597, 0.4903]
-Original Grad: 0.040, -lr * Pred Grad:  0.014, New P: 1.274
-Original Grad: -0.007, -lr * Pred Grad:  -0.000, New P: 0.490
iter 6 loss: 0.139
Actual params: [1.2742, 0.49  ]
-Original Grad: 0.005, -lr * Pred Grad:  0.001, New P: 1.275
-Original Grad: -0.013, -lr * Pred Grad:  -0.012, New P: 0.478
iter 7 loss: 0.140
Actual params: [1.2747, 0.4783]
-Original Grad: -0.004, -lr * Pred Grad:  -0.002, New P: 1.273
-Original Grad: -0.011, -lr * Pred Grad:  -0.010, New P: 0.468
iter 8 loss: 0.141
Actual params: [1.273 , 0.4682]
-Original Grad: -0.032, -lr * Pred Grad:  -0.007, New P: 1.266
-Original Grad: 0.004, -lr * Pred Grad:  0.001, New P: 0.469
iter 9 loss: 0.143
Actual params: [1.2661, 0.4695]
-Original Grad: 0.025, -lr * Pred Grad:  0.005, New P: 1.271
-Original Grad: -0.001, -lr * Pred Grad:  0.001, New P: 0.471
iter 10 loss: 0.141
Actual params: [1.2713, 0.4707]
-Original Grad: -0.006, -lr * Pred Grad:  -0.002, New P: 1.269
-Original Grad: -0.010, -lr * Pred Grad:  -0.011, New P: 0.460
iter 11 loss: 0.142
Actual params: [1.2691, 0.4598]
-Original Grad: 0.021, -lr * Pred Grad:  0.004, New P: 1.273
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: 0.462
iter 12 loss: 0.142
Actual params: [1.2732, 0.4621]
-Original Grad: -0.001, -lr * Pred Grad:  -0.000, New P: 1.273
-Original Grad: -0.001, -lr * Pred Grad:  -0.002, New P: 0.460
iter 13 loss: 0.142
Actual params: [1.2728, 0.4604]
-Original Grad: 0.014, -lr * Pred Grad:  0.004, New P: 1.277
-Original Grad: 0.017, -lr * Pred Grad:  0.021, New P: 0.481
iter 14 loss: 0.139
Actual params: [1.2767, 0.4813]
-Original Grad: -0.010, -lr * Pred Grad:  -0.003, New P: 1.274
-Original Grad: -0.017, -lr * Pred Grad:  -0.019, New P: 0.462
iter 15 loss: 0.142
Actual params: [1.2736, 0.4622]
-Original Grad: -0.022, -lr * Pred Grad:  -0.004, New P: 1.270
-Original Grad: -0.003, -lr * Pred Grad:  -0.005, New P: 0.458
iter 16 loss: 0.143
Actual params: [1.2695, 0.4576]
-Original Grad: 0.089, -lr * Pred Grad:  0.014, New P: 1.284
-Original Grad: 0.007, -lr * Pred Grad:  0.014, New P: 0.472
iter 17 loss: 0.140
Actual params: [1.284, 0.472]
-Original Grad: 0.012, -lr * Pred Grad:  0.002, New P: 1.286
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: 0.475
iter 18 loss: 0.139
Actual params: [1.286 , 0.4752]
-Original Grad: 0.007, -lr * Pred Grad:  0.001, New P: 1.287
-Original Grad: 0.001, -lr * Pred Grad:  0.002, New P: 0.477
iter 19 loss: 0.139
Actual params: [1.2873, 0.4772]
-Original Grad: 0.035, -lr * Pred Grad:  0.005, New P: 1.292
-Original Grad: -0.007, -lr * Pred Grad:  -0.005, New P: 0.472
iter 20 loss: 0.139
Actual params: [1.2918, 0.4722]
-Original Grad: -0.038, -lr * Pred Grad:  -0.005, New P: 1.287
-Original Grad: 0.006, -lr * Pred Grad:  0.003, New P: 0.475
Target params: [1.1812, 0.2779]
iter 0 loss: 0.743
Actual params: [0.5941, 0.5941]
-Original Grad: 0.149, -lr * Pred Grad:  -0.091, New P: 0.503
-Original Grad: -1.321, -lr * Pred Grad:  -0.099, New P: 0.495
iter 1 loss: 0.455
Actual params: [0.5033, 0.4951]
-Original Grad: 0.055, -lr * Pred Grad:  -0.061, New P: 0.442
-Original Grad: -0.551, -lr * Pred Grad:  -0.037, New P: 0.458
iter 2 loss: 0.391
Actual params: [0.4421, 0.4579]
-Original Grad: 0.051, -lr * Pred Grad:  0.013, New P: 0.455
-Original Grad: -0.403, -lr * Pred Grad:  -0.016, New P: 0.442
iter 3 loss: 0.362
Actual params: [0.4547, 0.4415]
-Original Grad: 0.045, -lr * Pred Grad:  0.013, New P: 0.468
-Original Grad: -0.354, -lr * Pred Grad:  -0.014, New P: 0.428
iter 4 loss: 0.343
Actual params: [0.4675, 0.4279]
-Original Grad: 0.027, -lr * Pred Grad:  0.004, New P: 0.472
-Original Grad: -0.217, -lr * Pred Grad:  -0.009, New P: 0.419
iter 5 loss: 0.333
Actual params: [0.4718, 0.419 ]
-Original Grad: 0.046, -lr * Pred Grad:  0.079, New P: 0.550
-Original Grad: -0.290, -lr * Pred Grad:  -0.002, New P: 0.416
iter 6 loss: 0.320
Actual params: [0.5505, 0.4165]
-Original Grad: 0.051, -lr * Pred Grad:  0.133, New P: 0.683
-Original Grad: -0.269, -lr * Pred Grad:  0.006, New P: 0.423
iter 7 loss: 0.306
Actual params: [0.6833, 0.4225]
-Original Grad: 0.105, -lr * Pred Grad:  0.170, New P: 0.854
-Original Grad: -0.220, -lr * Pred Grad:  0.018, New P: 0.441
iter 8 loss: 0.320
Actual params: [0.8538, 0.4406]
-Original Grad: 0.004, -lr * Pred Grad:  -0.014, New P: 0.840
-Original Grad: -0.148, -lr * Pred Grad:  -0.009, New P: 0.432
iter 9 loss: 0.305
Actual params: [0.8399, 0.4321]
-Original Grad: -0.007, -lr * Pred Grad:  -0.012, New P: 0.828
-Original Grad: -0.111, -lr * Pred Grad:  -0.006, New P: 0.426
iter 10 loss: 0.297
Actual params: [0.8281, 0.4257]
-Original Grad: 0.017, -lr * Pred Grad:  0.010, New P: 0.838
-Original Grad: -0.021, -lr * Pred Grad:  -0.000, New P: 0.426
iter 11 loss: 0.299
Actual params: [0.8379, 0.4257]
-Original Grad: 0.053, -lr * Pred Grad:  0.028, New P: 0.866
-Original Grad: -0.058, -lr * Pred Grad:  0.000, New P: 0.426
iter 12 loss: 0.310
Actual params: [0.8658, 0.4259]
-Original Grad: -0.025, -lr * Pred Grad:  -0.019, New P: 0.847
-Original Grad: -0.125, -lr * Pred Grad:  -0.009, New P: 0.417
iter 13 loss: 0.295
Actual params: [0.8465, 0.4172]
-Original Grad: 0.054, -lr * Pred Grad:  0.024, New P: 0.871
-Original Grad: -0.055, -lr * Pred Grad:  -0.001, New P: 0.416
iter 14 loss: 0.303
Actual params: [0.8709, 0.4164]
-Original Grad: -0.050, -lr * Pred Grad:  -0.022, New P: 0.849
-Original Grad: -0.097, -lr * Pred Grad:  -0.007, New P: 0.409
iter 15 loss: 0.290
Actual params: [0.8488, 0.4093]
-Original Grad: 0.025, -lr * Pred Grad:  0.009, New P: 0.858
-Original Grad: -0.054, -lr * Pred Grad:  -0.003, New P: 0.406
iter 16 loss: 0.290
Actual params: [0.8583, 0.4062]
-Original Grad: 0.015, -lr * Pred Grad:  0.006, New P: 0.864
-Original Grad: -0.037, -lr * Pred Grad:  -0.002, New P: 0.404
iter 17 loss: 0.291
Actual params: [0.8642, 0.4038]
-Original Grad: 0.022, -lr * Pred Grad:  0.010, New P: 0.874
-Original Grad: -0.042, -lr * Pred Grad:  -0.003, New P: 0.401
iter 18 loss: 0.292
Actual params: [0.874 , 0.4011]
-Original Grad: 0.001, -lr * Pred Grad:  -0.001, New P: 0.873
-Original Grad: -0.025, -lr * Pred Grad:  -0.002, New P: 0.399
iter 19 loss: 0.290
Actual params: [0.8734, 0.3989]
-Original Grad: 0.017, -lr * Pred Grad:  0.009, New P: 0.882
-Original Grad: -0.032, -lr * Pred Grad:  -0.002, New P: 0.396
iter 20 loss: 0.292
Actual params: [0.8822, 0.3964]
-Original Grad: -0.041, -lr * Pred Grad:  -0.019, New P: 0.864
-Original Grad: -0.080, -lr * Pred Grad:  -0.007, New P: 0.389
Target params: [1.1812, 0.2779]
iter 0 loss: 0.288
Actual params: [0.5941, 0.5941]
-Original Grad: 0.772, -lr * Pred Grad:  0.022, New P: 0.616
-Original Grad: -0.482, -lr * Pred Grad:  -0.103, New P: 0.491
iter 1 loss: 0.264
Actual params: [0.6157, 0.4908]
-Original Grad: 0.428, -lr * Pred Grad:  0.052, New P: 0.668
-Original Grad: -0.195, -lr * Pred Grad:  0.052, New P: 0.543
iter 2 loss: 0.258
Actual params: [0.6681, 0.5426]
-Original Grad: 0.189, -lr * Pred Grad:  -0.020, New P: 0.648
-Original Grad: -0.179, -lr * Pred Grad:  -0.056, New P: 0.486
iter 3 loss: 0.257
Actual params: [0.648 , 0.4862]
-Original Grad: 0.384, -lr * Pred Grad:  0.015, New P: 0.663
-Original Grad: -0.196, -lr * Pred Grad:  -0.001, New P: 0.485
iter 4 loss: 0.254
Actual params: [0.6632, 0.4852]
-Original Grad: 0.222, -lr * Pred Grad:  0.015, New P: 0.678
-Original Grad: -0.075, -lr * Pred Grad:  0.016, New P: 0.501
iter 5 loss: 0.253
Actual params: [0.6783, 0.5011]
-Original Grad: 0.214, -lr * Pred Grad:  -0.001, New P: 0.677
-Original Grad: -0.148, -lr * Pred Grad:  -0.017, New P: 0.484
iter 6 loss: 0.252
Actual params: [0.6773, 0.4839]
-Original Grad: 0.100, -lr * Pred Grad:  -0.000, New P: 0.677
-Original Grad: -0.071, -lr * Pred Grad:  -0.008, New P: 0.476
iter 7 loss: 0.251
Actual params: [0.6769, 0.4759]
-Original Grad: 0.015, -lr * Pred Grad:  -0.002, New P: 0.675
-Original Grad: -0.024, -lr * Pred Grad:  -0.005, New P: 0.471
iter 8 loss: 0.251
Actual params: [0.6752, 0.4711]
-Original Grad: 0.100, -lr * Pred Grad:  -0.001, New P: 0.674
-Original Grad: -0.085, -lr * Pred Grad:  -0.010, New P: 0.461
iter 9 loss: 0.251
Actual params: [0.6742, 0.4612]
-Original Grad: 0.147, -lr * Pred Grad:  0.001, New P: 0.676
-Original Grad: -0.104, -lr * Pred Grad:  -0.008, New P: 0.453
iter 10 loss: 0.250
Actual params: [0.6756, 0.4534]
-Original Grad: 0.071, -lr * Pred Grad:  0.003, New P: 0.679
-Original Grad: -0.026, -lr * Pred Grad:  0.001, New P: 0.455
iter 11 loss: 0.250
Actual params: [0.6786, 0.4547]
-Original Grad: 0.094, -lr * Pred Grad:  0.004, New P: 0.682
-Original Grad: -0.038, -lr * Pred Grad:  0.001, New P: 0.456
iter 12 loss: 0.249
Actual params: [0.6823, 0.4557]
-Original Grad: 0.063, -lr * Pred Grad:  0.001, New P: 0.683
-Original Grad: -0.043, -lr * Pred Grad:  -0.004, New P: 0.452
iter 13 loss: 0.249
Actual params: [0.6829, 0.452 ]
-Original Grad: 0.094, -lr * Pred Grad:  0.002, New P: 0.685
-Original Grad: -0.056, -lr * Pred Grad:  -0.003, New P: 0.449
iter 14 loss: 0.249
Actual params: [0.6848, 0.4488]
-Original Grad: 0.079, -lr * Pred Grad:  0.002, New P: 0.687
-Original Grad: -0.043, -lr * Pred Grad:  -0.002, New P: 0.447
iter 15 loss: 0.248
Actual params: [0.6869, 0.447 ]
-Original Grad: 0.053, -lr * Pred Grad:  0.001, New P: 0.688
-Original Grad: -0.032, -lr * Pred Grad:  -0.002, New P: 0.445
iter 16 loss: 0.248
Actual params: [0.688 , 0.4451]
-Original Grad: 0.038, -lr * Pred Grad:  0.003, New P: 0.691
-Original Grad: -0.007, -lr * Pred Grad:  0.003, New P: 0.448
iter 17 loss: 0.248
Actual params: [0.6908, 0.448 ]
-Original Grad: 0.021, -lr * Pred Grad:  0.002, New P: 0.693
-Original Grad: -0.001, -lr * Pred Grad:  0.003, New P: 0.451
iter 18 loss: 0.248
Actual params: [0.6929, 0.4505]
-Original Grad: 0.081, -lr * Pred Grad:  0.005, New P: 0.698
-Original Grad: -0.025, -lr * Pred Grad:  0.004, New P: 0.454
iter 19 loss: 0.248
Actual params: [0.698 , 0.4541]
-Original Grad: 0.028, -lr * Pred Grad:  -0.000, New P: 0.698
-Original Grad: -0.023, -lr * Pred Grad:  -0.003, New P: 0.451
iter 20 loss: 0.247
Actual params: [0.6978, 0.4508]
-Original Grad: 0.015, -lr * Pred Grad:  -0.000, New P: 0.698
-Original Grad: -0.014, -lr * Pred Grad:  -0.002, New P: 0.449
Target params: [1.1812, 0.2779]
iter 0 loss: 0.718
Actual params: [0.5941, 0.5941]
-Original Grad: 0.568, -lr * Pred Grad:  -0.001, New P: 0.593
-Original Grad: -0.700, -lr * Pred Grad:  -0.147, New P: 0.447
iter 1 loss: 0.592
Actual params: [0.593 , 0.4466]
-Original Grad: 0.375, -lr * Pred Grad:  0.105, New P: 0.698
-Original Grad: -0.373, -lr * Pred Grad:  0.036, New P: 0.483
iter 2 loss: 0.517
Actual params: [0.6978, 0.4826]
-Original Grad: 0.426, -lr * Pred Grad:  0.075, New P: 0.773
-Original Grad: -0.417, -lr * Pred Grad:  0.023, New P: 0.506
iter 3 loss: 0.477
Actual params: [0.7725, 0.5061]
-Original Grad: 0.222, -lr * Pred Grad:  0.036, New P: 0.808
-Original Grad: -0.196, -lr * Pred Grad:  0.015, New P: 0.521
iter 4 loss: 0.467
Actual params: [0.8084, 0.5215]
-Original Grad: 0.315, -lr * Pred Grad:  0.048, New P: 0.857
-Original Grad: -0.256, -lr * Pred Grad:  0.029, New P: 0.551
iter 5 loss: 0.462
Actual params: [0.8568, 0.5507]
-Original Grad: 0.266, -lr * Pred Grad:  -0.021, New P: 0.836
-Original Grad: -0.311, -lr * Pred Grad:  -0.041, New P: 0.510
iter 6 loss: 0.444
Actual params: [0.8358, 0.5097]
-Original Grad: 0.290, -lr * Pred Grad:  0.036, New P: 0.871
-Original Grad: -0.220, -lr * Pred Grad:  0.022, New P: 0.532
iter 7 loss: 0.446
Actual params: [0.8714, 0.5319]
-Original Grad: 0.215, -lr * Pred Grad:  0.003, New P: 0.874
-Original Grad: -0.205, -lr * Pred Grad:  -0.009, New P: 0.523
iter 8 loss: 0.440
Actual params: [0.8744, 0.5228]
-Original Grad: 0.096, -lr * Pred Grad:  0.018, New P: 0.893
-Original Grad: -0.062, -lr * Pred Grad:  0.015, New P: 0.538
iter 9 loss: 0.446
Actual params: [0.8928, 0.5379]
-Original Grad: 0.180, -lr * Pred Grad:  0.003, New P: 0.896
-Original Grad: -0.168, -lr * Pred Grad:  -0.007, New P: 0.531
iter 10 loss: 0.441
Actual params: [0.8962, 0.5312]
-Original Grad: 0.099, -lr * Pred Grad:  0.014, New P: 0.910
-Original Grad: -0.074, -lr * Pred Grad:  0.009, New P: 0.541
iter 11 loss: 0.446
Actual params: [0.9097, 0.5407]
-Original Grad: 0.136, -lr * Pred Grad:  -0.011, New P: 0.898
-Original Grad: -0.149, -lr * Pred Grad:  -0.021, New P: 0.520
iter 12 loss: 0.435
Actual params: [0.8985, 0.52  ]
-Original Grad: 0.113, -lr * Pred Grad:  0.021, New P: 0.920
-Original Grad: -0.077, -lr * Pred Grad:  0.017, New P: 0.537
iter 13 loss: 0.443
Actual params: [0.9197, 0.5374]
-Original Grad: 0.056, -lr * Pred Grad:  -0.005, New P: 0.915
-Original Grad: -0.063, -lr * Pred Grad:  -0.009, New P: 0.528
iter 14 loss: 0.438
Actual params: [0.9148, 0.5281]
-Original Grad: 0.107, -lr * Pred Grad:  -0.000, New P: 0.915
-Original Grad: -0.104, -lr * Pred Grad:  -0.007, New P: 0.521
iter 15 loss: 0.434
Actual params: [0.9148, 0.521 ]
-Original Grad: 0.151, -lr * Pred Grad:  0.011, New P: 0.925
-Original Grad: -0.131, -lr * Pred Grad:  0.002, New P: 0.523
iter 16 loss: 0.435
Actual params: [0.9253, 0.5229]
-Original Grad: 0.064, -lr * Pred Grad:  -0.004, New P: 0.921
-Original Grad: -0.071, -lr * Pred Grad:  -0.009, New P: 0.514
iter 17 loss: 0.431
Actual params: [0.9215, 0.5139]
-Original Grad: 0.043, -lr * Pred Grad:  0.010, New P: 0.932
-Original Grad: -0.025, -lr * Pred Grad:  0.008, New P: 0.522
iter 18 loss: 0.435
Actual params: [0.9316, 0.5223]
-Original Grad: 0.086, -lr * Pred Grad:  0.004, New P: 0.936
-Original Grad: -0.077, -lr * Pred Grad:  -0.002, New P: 0.520
iter 19 loss: 0.434
Actual params: [0.9357, 0.5201]
-Original Grad: 0.033, -lr * Pred Grad:  -0.003, New P: 0.933
-Original Grad: -0.038, -lr * Pred Grad:  -0.006, New P: 0.514
iter 20 loss: 0.431
Actual params: [0.9332, 0.5142]
-Original Grad: 0.057, -lr * Pred Grad:  0.005, New P: 0.938
-Original Grad: -0.047, -lr * Pred Grad:  0.001, New P: 0.515
Target params: [1.1812, 0.2779]
iter 0 loss: 0.727
Actual params: [0.5941, 0.5941]
-Original Grad: 0.979, -lr * Pred Grad:  0.189, New P: 0.783
-Original Grad: 0.047, -lr * Pred Grad:  0.033, New P: 0.627
iter 1 loss: 0.680
Actual params: [0.7835, 0.6267]
-Original Grad: 0.476, -lr * Pred Grad:  0.080, New P: 0.863
-Original Grad: -0.014, -lr * Pred Grad:  -0.260, New P: 0.366
iter 2 loss: 0.638
Actual params: [0.8631, 0.3664]
-Original Grad: 0.513, -lr * Pred Grad:  0.023, New P: 0.886
-Original Grad: 0.118, -lr * Pred Grad:  0.368, New P: 0.734
iter 3 loss: 0.646
Actual params: [0.886 , 0.7345]
-Original Grad: -0.277, -lr * Pred Grad:  0.005, New P: 0.891
-Original Grad: -1.350, -lr * Pred Grad:  -0.102, New P: 0.632
iter 4 loss: 0.640
Actual params: [0.8908, 0.6325]
-Original Grad: 0.230, -lr * Pred Grad:  0.027, New P: 0.918
-Original Grad: -0.036, -lr * Pred Grad:  -0.009, New P: 0.623
iter 5 loss: 0.629
Actual params: [0.9183, 0.6233]
-Original Grad: 0.253, -lr * Pred Grad:  0.031, New P: 0.949
-Original Grad: -0.074, -lr * Pred Grad:  -0.013, New P: 0.610
iter 6 loss: 0.617
Actual params: [0.9495, 0.6101]
-Original Grad: 0.210, -lr * Pred Grad:  0.026, New P: 0.975
-Original Grad: -0.137, -lr * Pred Grad:  -0.018, New P: 0.592
iter 7 loss: 0.606
Actual params: [0.9753, 0.5924]
-Original Grad: 0.210, -lr * Pred Grad:  0.024, New P: 0.999
-Original Grad: -0.045, -lr * Pred Grad:  -0.009, New P: 0.583
iter 8 loss: 0.597
Actual params: [0.9992, 0.5835]
-Original Grad: 0.128, -lr * Pred Grad:  0.014, New P: 1.014
-Original Grad: -0.061, -lr * Pred Grad:  -0.009, New P: 0.575
iter 9 loss: 0.593
Actual params: [1.0136, 0.5747]
-Original Grad: 0.160, -lr * Pred Grad:  0.018, New P: 1.031
-Original Grad: -0.054, -lr * Pred Grad:  -0.008, New P: 0.566
iter 10 loss: 0.588
Actual params: [1.0312, 0.5663]
-Original Grad: 0.060, -lr * Pred Grad:  0.007, New P: 1.038
-Original Grad: -0.043, -lr * Pred Grad:  -0.006, New P: 0.560
iter 11 loss: 0.584
Actual params: [1.0382, 0.56  ]
-Original Grad: 0.055, -lr * Pred Grad:  0.006, New P: 1.044
-Original Grad: -0.056, -lr * Pred Grad:  -0.008, New P: 0.552
iter 12 loss: 0.579
Actual params: [1.0442, 0.5517]
-Original Grad: 0.012, -lr * Pred Grad:  0.002, New P: 1.046
-Original Grad: -0.070, -lr * Pred Grad:  -0.011, New P: 0.541
iter 13 loss: 0.573
Actual params: [1.0458, 0.5408]
-Original Grad: 0.074, -lr * Pred Grad:  0.007, New P: 1.053
-Original Grad: -0.036, -lr * Pred Grad:  -0.006, New P: 0.535
iter 14 loss: 0.568
Actual params: [1.0532, 0.5348]
-Original Grad: -0.137, -lr * Pred Grad:  -0.015, New P: 1.039
-Original Grad: 0.026, -lr * Pred Grad:  0.005, New P: 0.539
iter 15 loss: 0.574
Actual params: [1.0386, 0.5395]
-Original Grad: -0.069, -lr * Pred Grad:  -0.008, New P: 1.031
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: 0.539
iter 16 loss: 0.575
Actual params: [1.0309, 0.5388]
-Original Grad: -0.071, -lr * Pred Grad:  -0.008, New P: 1.023
-Original Grad: -0.027, -lr * Pred Grad:  -0.007, New P: 0.532
iter 17 loss: 0.570
Actual params: [1.023 , 0.5321]
-Original Grad: 0.023, -lr * Pred Grad:  0.002, New P: 1.025
-Original Grad: -0.032, -lr * Pred Grad:  -0.007, New P: 0.525
iter 18 loss: 0.567
Actual params: [1.0252, 0.5247]
-Original Grad: -0.009, -lr * Pred Grad:  -0.001, New P: 1.024
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: 0.524
iter 19 loss: 0.567
Actual params: [1.024 , 0.5241]
-Original Grad: 0.070, -lr * Pred Grad:  0.008, New P: 1.032
-Original Grad: -0.022, -lr * Pred Grad:  -0.004, New P: 0.520
iter 20 loss: 0.564
Actual params: [1.0318, 0.5197]
-Original Grad: 0.146, -lr * Pred Grad:  0.015, New P: 1.047
-Original Grad: -0.046, -lr * Pred Grad:  -0.009, New P: 0.511
Target params: [1.1812, 0.2779]
iter 0 loss: 0.636
Actual params: [0.5941, 0.5941]
-Original Grad: 0.254, -lr * Pred Grad:  0.509, New P: 1.103
-Original Grad: -0.086, -lr * Pred Grad:  -0.163, New P: 0.431
iter 1 loss: 0.346
Actual params: [1.1027, 0.4307]
-Original Grad: 0.109, -lr * Pred Grad:  0.071, New P: 1.174
-Original Grad: -0.527, -lr * Pred Grad:  -0.045, New P: 0.385
iter 2 loss: 0.340
Actual params: [1.1737, 0.3854]
-Original Grad: 0.055, -lr * Pred Grad:  0.042, New P: 1.216
-Original Grad: -0.053, -lr * Pred Grad:  -0.001, New P: 0.385
iter 3 loss: 0.343
Actual params: [1.2159, 0.3846]
-Original Grad: 0.015, -lr * Pred Grad:  0.009, New P: 1.225
-Original Grad: -0.031, -lr * Pred Grad:  -0.001, New P: 0.383
iter 4 loss: 0.343
Actual params: [1.2254, 0.3834]
-Original Grad: -0.038, -lr * Pred Grad:  -0.022, New P: 1.203
-Original Grad: 0.058, -lr * Pred Grad:  0.002, New P: 0.386
iter 5 loss: 0.342
Actual params: [1.203 , 0.3856]
-Original Grad: 0.017, -lr * Pred Grad:  0.011, New P: 1.214
-Original Grad: 0.010, -lr * Pred Grad:  0.001, New P: 0.387
iter 6 loss: 0.343
Actual params: [1.2142, 0.3871]
-Original Grad: 0.085, -lr * Pred Grad:  0.048, New P: 1.262
-Original Grad: -0.047, -lr * Pred Grad:  0.002, New P: 0.389
iter 7 loss: 0.341
Actual params: [1.2619, 0.3886]
-Original Grad: -0.011, -lr * Pred Grad:  0.003, New P: 1.264
-Original Grad: 0.153, -lr * Pred Grad:  0.007, New P: 0.396
iter 8 loss: 0.341
Actual params: [1.2645, 0.3959]
-Original Grad: -0.005, -lr * Pred Grad:  -0.001, New P: 1.264
-Original Grad: 0.038, -lr * Pred Grad:  0.002, New P: 0.398
iter 9 loss: 0.341
Actual params: [1.264 , 0.3976]
-Original Grad: -0.014, -lr * Pred Grad:  -0.009, New P: 1.255
-Original Grad: -0.025, -lr * Pred Grad:  -0.002, New P: 0.395
iter 10 loss: 0.340
Actual params: [1.2548, 0.3953]
-Original Grad: -0.036, -lr * Pred Grad:  -0.017, New P: 1.238
-Original Grad: 0.009, -lr * Pred Grad:  -0.002, New P: 0.393
iter 11 loss: 0.344
Actual params: [1.2378, 0.3934]
-Original Grad: 0.012, -lr * Pred Grad:  0.004, New P: 1.242
-Original Grad: -0.028, -lr * Pred Grad:  -0.001, New P: 0.393
iter 12 loss: 0.343
Actual params: [1.2419, 0.3928]
-Original Grad: -0.001, -lr * Pred Grad:  0.004, New P: 1.246
-Original Grad: 0.077, -lr * Pred Grad:  0.004, New P: 0.396
iter 13 loss: 0.342
Actual params: [1.2458, 0.3965]
-Original Grad: 0.012, -lr * Pred Grad:  0.007, New P: 1.253
-Original Grad: 0.017, -lr * Pred Grad:  0.002, New P: 0.398
iter 14 loss: 0.340
Actual params: [1.2528, 0.398 ]
-Original Grad: 0.001, -lr * Pred Grad:  0.004, New P: 1.256
-Original Grad: 0.060, -lr * Pred Grad:  0.003, New P: 0.401
iter 15 loss: 0.340
Actual params: [1.2564, 0.4007]
-Original Grad: -0.008, -lr * Pred Grad:  -0.007, New P: 1.249
-Original Grad: -0.065, -lr * Pred Grad:  -0.003, New P: 0.397
iter 16 loss: 0.340
Actual params: [1.2492, 0.3973]
-Original Grad: 0.008, -lr * Pred Grad:  0.003, New P: 1.252
-Original Grad: -0.017, -lr * Pred Grad:  -0.000, New P: 0.397
iter 17 loss: 0.340
Actual params: [1.2518, 0.3969]
-Original Grad: -0.023, -lr * Pred Grad:  -0.002, New P: 1.249
-Original Grad: 0.157, -lr * Pred Grad:  0.006, New P: 0.403
iter 18 loss: 0.341
Actual params: [1.2495, 0.4028]
-Original Grad: 0.045, -lr * Pred Grad:  0.014, New P: 1.263
-Original Grad: -0.104, -lr * Pred Grad:  -0.002, New P: 0.401
iter 19 loss: 0.341
Actual params: [1.263, 0.401]
-Original Grad: 0.050, -lr * Pred Grad:  0.015, New P: 1.278
-Original Grad: -0.099, -lr * Pred Grad:  -0.001, New P: 0.400
iter 20 loss: 0.345
Actual params: [1.2781, 0.4003]
-Original Grad: -0.036, -lr * Pred Grad:  -0.012, New P: 1.266
-Original Grad: 0.047, -lr * Pred Grad:  -0.001, New P: 0.400
Target params: [1.1812, 0.2779]
iter 0 loss: 0.473
Actual params: [0.5941, 0.5941]
-Original Grad: 0.148, -lr * Pred Grad:  0.133, New P: 0.727
-Original Grad: -0.565, -lr * Pred Grad:  -0.186, New P: 0.408
iter 1 loss: 0.374
Actual params: [0.7274, 0.4081]
-Original Grad: 0.284, -lr * Pred Grad:  0.075, New P: 0.803
-Original Grad: -0.238, -lr * Pred Grad:  -0.007, New P: 0.401
iter 2 loss: 0.334
Actual params: [0.8028, 0.4014]
-Original Grad: 0.144, -lr * Pred Grad:  0.019, New P: 0.821
-Original Grad: -0.130, -lr * Pred Grad:  -0.009, New P: 0.393
iter 3 loss: 0.328
Actual params: [0.8214, 0.3925]
-Original Grad: 0.155, -lr * Pred Grad:  0.007, New P: 0.828
-Original Grad: -0.167, -lr * Pred Grad:  -0.016, New P: 0.376
iter 4 loss: 0.325
Actual params: [0.8281, 0.3763]
-Original Grad: 0.211, -lr * Pred Grad:  0.008, New P: 0.836
-Original Grad: -0.217, -lr * Pred Grad:  -0.011, New P: 0.366
iter 5 loss: 0.322
Actual params: [0.8362, 0.3655]
-Original Grad: 0.033, -lr * Pred Grad:  0.007, New P: 0.843
-Original Grad: -0.013, -lr * Pred Grad:  0.004, New P: 0.370
iter 6 loss: 0.321
Actual params: [0.8428, 0.3699]
-Original Grad: 0.025, -lr * Pred Grad:  0.002, New P: 0.845
-Original Grad: -0.021, -lr * Pred Grad:  -0.000, New P: 0.370
iter 7 loss: 0.321
Actual params: [0.8448, 0.3696]
-Original Grad: 0.032, -lr * Pred Grad:  0.006, New P: 0.851
-Original Grad: -0.015, -lr * Pred Grad:  0.003, New P: 0.373
iter 8 loss: 0.320
Actual params: [0.8507, 0.373 ]
-Original Grad: -0.007, -lr * Pred Grad:  -0.004, New P: 0.847
-Original Grad: -0.007, -lr * Pred Grad:  -0.004, New P: 0.369
iter 9 loss: 0.321
Actual params: [0.8468, 0.3688]
-Original Grad: -0.005, -lr * Pred Grad:  -0.003, New P: 0.844
-Original Grad: -0.007, -lr * Pred Grad:  -0.004, New P: 0.365
iter 10 loss: 0.321
Actual params: [0.8436, 0.3652]
-Original Grad: -0.041, -lr * Pred Grad:  -0.012, New P: 0.831
-Original Grad: -0.006, -lr * Pred Grad:  -0.011, New P: 0.354
iter 11 loss: 0.322
Actual params: [0.8315, 0.3541]
-Original Grad: 0.030, -lr * Pred Grad:  0.007, New P: 0.838
-Original Grad: -0.006, -lr * Pred Grad:  0.005, New P: 0.359
iter 12 loss: 0.321
Actual params: [0.8381, 0.3588]
-Original Grad: 0.029, -lr * Pred Grad:  0.008, New P: 0.846
-Original Grad: 0.004, -lr * Pred Grad:  0.008, New P: 0.367
iter 13 loss: 0.320
Actual params: [0.8465, 0.3666]
-Original Grad: 0.084, -lr * Pred Grad:  0.019, New P: 0.866
-Original Grad: -0.008, -lr * Pred Grad:  0.015, New P: 0.381
iter 14 loss: 0.315
Actual params: [0.8657, 0.3815]
-Original Grad: 0.048, -lr * Pred Grad:  0.008, New P: 0.873
-Original Grad: -0.019, -lr * Pred Grad:  0.002, New P: 0.384
iter 15 loss: 0.314
Actual params: [0.8734, 0.384 ]
-Original Grad: 0.052, -lr * Pred Grad:  0.007, New P: 0.880
-Original Grad: -0.028, -lr * Pred Grad:  -0.000, New P: 0.384
iter 16 loss: 0.312
Actual params: [0.8805, 0.3837]
-Original Grad: 0.031, -lr * Pred Grad:  0.003, New P: 0.883
-Original Grad: -0.023, -lr * Pred Grad:  -0.004, New P: 0.380
iter 17 loss: 0.311
Actual params: [0.8831, 0.38  ]
-Original Grad: 0.011, -lr * Pred Grad:  -0.001, New P: 0.882
-Original Grad: -0.016, -lr * Pred Grad:  -0.006, New P: 0.374
iter 18 loss: 0.311
Actual params: [0.882 , 0.3745]
-Original Grad: 0.063, -lr * Pred Grad:  0.012, New P: 0.894
-Original Grad: -0.016, -lr * Pred Grad:  0.006, New P: 0.381
iter 19 loss: 0.309
Actual params: [0.8942, 0.3807]
-Original Grad: -0.022, -lr * Pred Grad:  -0.008, New P: 0.886
-Original Grad: -0.008, -lr * Pred Grad:  -0.010, New P: 0.371
iter 20 loss: 0.310
Actual params: [0.8864, 0.3707]
-Original Grad: 0.049, -lr * Pred Grad:  0.009, New P: 0.895
-Original Grad: -0.013, -lr * Pred Grad:  0.003, New P: 0.374
Target params: [1.1812, 0.2779]
iter 0 loss: 0.459
Actual params: [0.5941, 0.5941]
-Original Grad: 0.221, -lr * Pred Grad:  0.102, New P: 0.696
-Original Grad: -0.667, -lr * Pred Grad:  -0.110, New P: 0.484
iter 1 loss: 0.307
Actual params: [0.6956, 0.4839]
-Original Grad: 0.417, -lr * Pred Grad:  0.057, New P: 0.752
-Original Grad: -0.360, -lr * Pred Grad:  -0.015, New P: 0.469
iter 2 loss: 0.276
Actual params: [0.7522, 0.469 ]
-Original Grad: 0.236, -lr * Pred Grad:  -0.002, New P: 0.750
-Original Grad: -0.535, -lr * Pred Grad:  -0.034, New P: 0.435
iter 3 loss: 0.263
Actual params: [0.7497, 0.4346]
-Original Grad: 0.102, -lr * Pred Grad:  0.010, New P: 0.760
-Original Grad: -0.135, -lr * Pred Grad:  -0.003, New P: 0.431
iter 4 loss: 0.257
Actual params: [0.7598, 0.4313]
-Original Grad: 0.083, -lr * Pred Grad:  0.004, New P: 0.763
-Original Grad: -0.147, -lr * Pred Grad:  -0.007, New P: 0.425
iter 5 loss: 0.252
Actual params: [0.7633, 0.4246]
-Original Grad: 0.119, -lr * Pred Grad:  0.007, New P: 0.770
-Original Grad: -0.188, -lr * Pred Grad:  -0.007, New P: 0.418
iter 6 loss: 0.247
Actual params: [0.7702, 0.4179]
-Original Grad: 0.054, -lr * Pred Grad:  0.003, New P: 0.773
-Original Grad: -0.089, -lr * Pred Grad:  -0.004, New P: 0.414
iter 7 loss: 0.244
Actual params: [0.7729, 0.4143]
-Original Grad: 0.037, -lr * Pred Grad:  0.004, New P: 0.777
-Original Grad: -0.050, -lr * Pred Grad:  -0.001, New P: 0.413
iter 8 loss: 0.243
Actual params: [0.7769, 0.4133]
-Original Grad: 0.116, -lr * Pred Grad:  0.014, New P: 0.791
-Original Grad: -0.138, -lr * Pred Grad:  -0.000, New P: 0.413
iter 9 loss: 0.237
Actual params: [0.7906, 0.413 ]
-Original Grad: 0.046, -lr * Pred Grad:  0.009, New P: 0.799
-Original Grad: -0.040, -lr * Pred Grad:  0.002, New P: 0.415
iter 10 loss: 0.235
Actual params: [0.7995, 0.4154]
-Original Grad: 0.042, -lr * Pred Grad:  0.002, New P: 0.801
-Original Grad: -0.070, -lr * Pred Grad:  -0.004, New P: 0.412
iter 11 loss: 0.233
Actual params: [0.8011, 0.4117]
-Original Grad: 0.043, -lr * Pred Grad:  0.005, New P: 0.806
-Original Grad: -0.060, -lr * Pred Grad:  -0.002, New P: 0.410
iter 12 loss: 0.231
Actual params: [0.806 , 0.4101]
-Original Grad: 0.038, -lr * Pred Grad:  0.013, New P: 0.819
-Original Grad: -0.019, -lr * Pred Grad:  0.005, New P: 0.415
iter 13 loss: 0.229
Actual params: [0.8186, 0.4154]
-Original Grad: 0.045, -lr * Pred Grad:  0.009, New P: 0.827
-Original Grad: -0.052, -lr * Pred Grad:  0.000, New P: 0.416
iter 14 loss: 0.226
Actual params: [0.8271, 0.4156]
-Original Grad: 0.019, -lr * Pred Grad:  0.002, New P: 0.829
-Original Grad: -0.026, -lr * Pred Grad:  -0.001, New P: 0.415
iter 15 loss: 0.225
Actual params: [0.8295, 0.4145]
-Original Grad: 0.042, -lr * Pred Grad:  0.004, New P: 0.833
-Original Grad: -0.065, -lr * Pred Grad:  -0.004, New P: 0.411
iter 16 loss: 0.224
Actual params: [0.8334, 0.4106]
-Original Grad: 0.033, -lr * Pred Grad:  0.009, New P: 0.842
-Original Grad: -0.034, -lr * Pred Grad:  0.001, New P: 0.412
iter 17 loss: 0.222
Actual params: [0.8421, 0.4117]
-Original Grad: 0.019, -lr * Pred Grad:  0.000, New P: 0.842
-Original Grad: -0.036, -lr * Pred Grad:  -0.004, New P: 0.408
iter 18 loss: 0.220
Actual params: [0.8423, 0.4081]
-Original Grad: 0.010, -lr * Pred Grad:  0.001, New P: 0.843
-Original Grad: -0.016, -lr * Pred Grad:  -0.001, New P: 0.407
iter 19 loss: 0.219
Actual params: [0.843 , 0.4066]
-Original Grad: 0.011, -lr * Pred Grad:  0.006, New P: 0.849
-Original Grad: -0.001, -lr * Pred Grad:  0.003, New P: 0.410
iter 20 loss: 0.218
Actual params: [0.849 , 0.4098]
-Original Grad: 0.014, -lr * Pred Grad:  -0.000, New P: 0.849
-Original Grad: -0.025, -lr * Pred Grad:  -0.003, New P: 0.406
Target params: [1.1812, 0.2779]
iter 0 loss: 0.606
Actual params: [0.5941, 0.5941]
-Original Grad: 0.199, -lr * Pred Grad:  0.137, New P: 0.731
-Original Grad: -0.953, -lr * Pred Grad:  -0.167, New P: 0.427
iter 1 loss: 0.376
Actual params: [0.7313, 0.4267]
-Original Grad: 0.324, -lr * Pred Grad:  0.216, New P: 0.948
-Original Grad: -0.478, -lr * Pred Grad:  -0.002, New P: 0.425
iter 2 loss: 0.332
Actual params: [0.9477, 0.4248]
-Original Grad: 0.273, -lr * Pred Grad:  0.120, New P: 1.068
-Original Grad: -0.341, -lr * Pred Grad:  0.007, New P: 0.432
iter 3 loss: 0.329
Actual params: [1.0679, 0.4318]
-Original Grad: -0.018, -lr * Pred Grad:  -0.059, New P: 1.009
-Original Grad: -0.239, -lr * Pred Grad:  -0.047, New P: 0.385
iter 4 loss: 0.328
Actual params: [1.0088, 0.3849]
-Original Grad: 0.140, -lr * Pred Grad:  0.047, New P: 1.056
-Original Grad: -0.170, -lr * Pred Grad:  -0.003, New P: 0.382
iter 5 loss: 0.327
Actual params: [1.0562, 0.3819]
-Original Grad: 0.037, -lr * Pred Grad:  -0.004, New P: 1.052
-Original Grad: -0.153, -lr * Pred Grad:  -0.018, New P: 0.364
iter 6 loss: 0.326
Actual params: [1.0524, 0.3642]
-Original Grad: 0.060, -lr * Pred Grad:  0.015, New P: 1.067
-Original Grad: -0.108, -lr * Pred Grad:  -0.008, New P: 0.356
iter 7 loss: 0.329
Actual params: [1.0672, 0.3564]
-Original Grad: -0.043, -lr * Pred Grad:  -0.043, New P: 1.024
-Original Grad: -0.281, -lr * Pred Grad:  -0.035, New P: 0.322
iter 8 loss: 0.327
Actual params: [1.0242, 0.3217]
-Original Grad: 0.120, -lr * Pred Grad:  0.039, New P: 1.063
-Original Grad: -0.127, -lr * Pred Grad:  -0.003, New P: 0.318
iter 9 loss: 0.328
Actual params: [1.0632, 0.3185]
-Original Grad: 0.011, -lr * Pred Grad:  -0.008, New P: 1.055
-Original Grad: -0.168, -lr * Pred Grad:  -0.016, New P: 0.303
iter 10 loss: 0.327
Actual params: [1.0554, 0.3025]
-Original Grad: 0.066, -lr * Pred Grad:  0.021, New P: 1.077
-Original Grad: -0.078, -lr * Pred Grad:  -0.003, New P: 0.299
iter 11 loss: 0.330
Actual params: [1.0767, 0.2992]
-Original Grad: -0.029, -lr * Pred Grad:  -0.021, New P: 1.056
-Original Grad: -0.186, -lr * Pred Grad:  -0.018, New P: 0.281
iter 12 loss: 0.327
Actual params: [1.0559, 0.281 ]
-Original Grad: 0.068, -lr * Pred Grad:  0.024, New P: 1.079
-Original Grad: -0.053, -lr * Pred Grad:  -0.002, New P: 0.279
iter 13 loss: 0.330
Actual params: [1.0795, 0.2789]
-Original Grad: 0.038, -lr * Pred Grad:  0.012, New P: 1.092
-Original Grad: -0.052, -lr * Pred Grad:  -0.004, New P: 0.275
iter 14 loss: 0.332
Actual params: [1.0918, 0.2753]
-Original Grad: 0.027, -lr * Pred Grad:  0.008, New P: 1.099
-Original Grad: -0.080, -lr * Pred Grad:  -0.007, New P: 0.268
iter 15 loss: 0.333
Actual params: [1.0993, 0.268 ]
-Original Grad: 0.039, -lr * Pred Grad:  0.013, New P: 1.112
-Original Grad: -0.066, -lr * Pred Grad:  -0.006, New P: 0.262
iter 16 loss: 0.330
Actual params: [1.112 , 0.2622]
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 1.111
-Original Grad: -0.133, -lr * Pred Grad:  -0.013, New P: 0.250
iter 17 loss: 0.330
Actual params: [1.1106, 0.2495]
-Original Grad: 0.030, -lr * Pred Grad:  0.011, New P: 1.121
-Original Grad: -0.088, -lr * Pred Grad:  -0.008, New P: 0.241
iter 18 loss: 0.331
Actual params: [1.1214, 0.2414]
-Original Grad: 0.022, -lr * Pred Grad:  0.009, New P: 1.130
-Original Grad: -0.114, -lr * Pred Grad:  -0.010, New P: 0.232
iter 19 loss: 0.333
Actual params: [1.1305, 0.2316]
-Original Grad: -0.019, -lr * Pred Grad:  -0.005, New P: 1.125
-Original Grad: -0.183, -lr * Pred Grad:  -0.014, New P: 0.218
iter 20 loss: 0.331
Actual params: [1.125 , 0.2175]
-Original Grad: 0.042, -lr * Pred Grad:  0.018, New P: 1.143
-Original Grad: -0.038, -lr * Pred Grad:  -0.004, New P: 0.214
Target params: [1.1812, 0.2779]
iter 0 loss: 0.419
Actual params: [0.5941, 0.5941]
-Original Grad: 0.118, -lr * Pred Grad:  0.086, New P: 0.680
-Original Grad: -0.587, -lr * Pred Grad:  -0.084, New P: 0.510
iter 1 loss: 0.345
Actual params: [0.6804, 0.5096]
-Original Grad: 0.101, -lr * Pred Grad:  0.046, New P: 0.726
-Original Grad: -0.207, -lr * Pred Grad:  -0.021, New P: 0.489
iter 2 loss: 0.330
Actual params: [0.7262, 0.4887]
-Original Grad: 0.051, -lr * Pred Grad:  0.019, New P: 0.745
-Original Grad: -0.081, -lr * Pred Grad:  -0.008, New P: 0.481
iter 3 loss: 0.325
Actual params: [0.7453, 0.4809]
-Original Grad: 0.025, -lr * Pred Grad:  0.003, New P: 0.749
-Original Grad: -0.293, -lr * Pred Grad:  -0.024, New P: 0.457
iter 4 loss: 0.321
Actual params: [0.7485, 0.4571]
-Original Grad: 0.042, -lr * Pred Grad:  0.013, New P: 0.761
-Original Grad: -0.088, -lr * Pred Grad:  -0.006, New P: 0.451
iter 5 loss: 0.318
Actual params: [0.7615, 0.4507]
-Original Grad: 0.013, -lr * Pred Grad:  0.002, New P: 0.763
-Original Grad: -0.181, -lr * Pred Grad:  -0.013, New P: 0.438
iter 6 loss: 0.317
Actual params: [0.763 , 0.4381]
-Original Grad: 0.023, -lr * Pred Grad:  0.005, New P: 0.768
-Original Grad: -0.186, -lr * Pred Grad:  -0.011, New P: 0.427
iter 7 loss: 0.316
Actual params: [0.7681, 0.4271]
-Original Grad: 0.005, -lr * Pred Grad:  0.001, New P: 0.769
-Original Grad: -0.074, -lr * Pred Grad:  -0.004, New P: 0.423
iter 8 loss: 0.315
Actual params: [0.7687, 0.4227]
-Original Grad: 0.017, -lr * Pred Grad:  0.005, New P: 0.774
-Original Grad: -0.062, -lr * Pred Grad:  -0.004, New P: 0.419
iter 9 loss: 0.315
Actual params: [0.7739, 0.4191]
-Original Grad: 0.002, -lr * Pred Grad:  -0.002, New P: 0.772
-Original Grad: -0.266, -lr * Pred Grad:  -0.013, New P: 0.406
iter 10 loss: 0.314
Actual params: [0.7717, 0.4056]
-Original Grad: 0.023, -lr * Pred Grad:  0.008, New P: 0.779
-Original Grad: -0.065, -lr * Pred Grad:  -0.003, New P: 0.403
iter 11 loss: 0.315
Actual params: [0.7793, 0.4025]
-Original Grad: 0.024, -lr * Pred Grad:  0.007, New P: 0.786
-Original Grad: -0.101, -lr * Pred Grad:  -0.005, New P: 0.398
iter 12 loss: 0.313
Actual params: [0.7865, 0.3979]
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: 0.786
-Original Grad: -0.036, -lr * Pred Grad:  -0.002, New P: 0.396
iter 13 loss: 0.313
Actual params: [0.7856, 0.3961]
-Original Grad: 0.003, -lr * Pred Grad:  0.001, New P: 0.786
-Original Grad: -0.092, -lr * Pred Grad:  -0.004, New P: 0.392
iter 14 loss: 0.312
Actual params: [0.7864, 0.3918]
-Original Grad: 0.011, -lr * Pred Grad:  0.004, New P: 0.791
-Original Grad: 0.019, -lr * Pred Grad:  0.001, New P: 0.393
iter 15 loss: 0.312
Actual params: [0.7908, 0.3928]
-Original Grad: 0.004, -lr * Pred Grad:  0.002, New P: 0.793
-Original Grad: -0.052, -lr * Pred Grad:  -0.003, New P: 0.390
iter 16 loss: 0.311
Actual params: [0.7925, 0.3902]
-Original Grad: 0.003, -lr * Pred Grad:  0.001, New P: 0.794
-Original Grad: -0.042, -lr * Pred Grad:  -0.002, New P: 0.388
iter 17 loss: 0.311
Actual params: [0.7939, 0.3881]
-Original Grad: 0.002, -lr * Pred Grad:  0.001, New P: 0.795
-Original Grad: -0.052, -lr * Pred Grad:  -0.003, New P: 0.385
iter 18 loss: 0.310
Actual params: [0.7947, 0.3854]
-Original Grad: 0.004, -lr * Pred Grad:  0.002, New P: 0.797
-Original Grad: -0.060, -lr * Pred Grad:  -0.003, New P: 0.382
iter 19 loss: 0.310
Actual params: [0.7968, 0.3824]
-Original Grad: 0.003, -lr * Pred Grad:  0.002, New P: 0.799
-Original Grad: -0.046, -lr * Pred Grad:  -0.002, New P: 0.380
iter 20 loss: 0.309
Actual params: [0.7986, 0.38  ]
-Original Grad: 0.005, -lr * Pred Grad:  0.004, New P: 0.802
-Original Grad: -0.012, -lr * Pred Grad:  -0.001, New P: 0.379
Target params: [1.1812, 0.2779]
iter 0 loss: 0.388
Actual params: [0.5941, 0.5941]
-Original Grad: -0.045, -lr * Pred Grad:  -0.120, New P: 0.474
-Original Grad: -0.150, -lr * Pred Grad:  -0.166, New P: 0.428
iter 1 loss: 0.336
Actual params: [0.474 , 0.4285]
-Original Grad: -0.008, -lr * Pred Grad:  0.090, New P: 0.564
-Original Grad: -0.114, -lr * Pred Grad:  -0.108, New P: 0.320
iter 2 loss: 0.242
Actual params: [0.5645, 0.3205]
-Original Grad: 0.070, -lr * Pred Grad:  0.102, New P: 0.666
-Original Grad: -0.125, -lr * Pred Grad:  -0.041, New P: 0.279
iter 3 loss: 0.174
Actual params: [0.6664, 0.2793]
-Original Grad: 0.028, -lr * Pred Grad:  0.029, New P: 0.696
-Original Grad: -0.079, -lr * Pred Grad:  -0.013, New P: 0.266
iter 4 loss: 0.158
Actual params: [0.6956, 0.2663]
-Original Grad: 0.022, -lr * Pred Grad:  0.032, New P: 0.727
-Original Grad: -0.013, -lr * Pred Grad:  0.002, New P: 0.269
iter 5 loss: 0.154
Actual params: [0.7272, 0.2685]
-Original Grad: 0.022, -lr * Pred Grad:  0.024, New P: 0.751
-Original Grad: -0.041, -lr * Pred Grad:  -0.002, New P: 0.266
iter 6 loss: 0.151
Actual params: [0.7513, 0.2661]
-Original Grad: 0.005, -lr * Pred Grad:  0.009, New P: 0.760
-Original Grad: 0.007, -lr * Pred Grad:  0.003, New P: 0.269
iter 7 loss: 0.151
Actual params: [0.7601, 0.2688]
-Original Grad: 0.011, -lr * Pred Grad:  0.015, New P: 0.775
-Original Grad: 0.006, -lr * Pred Grad:  0.004, New P: 0.273
iter 8 loss: 0.151
Actual params: [0.7752, 0.2725]
-Original Grad: 0.016, -lr * Pred Grad:  0.019, New P: 0.794
-Original Grad: 0.006, -lr * Pred Grad:  0.004, New P: 0.277
iter 9 loss: 0.151
Actual params: [0.7942, 0.277 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.001, New P: 0.795
-Original Grad: -0.006, -lr * Pred Grad:  -0.001, New P: 0.276
iter 10 loss: 0.151
Actual params: [0.7947, 0.2759]
-Original Grad: -0.008, -lr * Pred Grad:  -0.003, New P: 0.791
-Original Grad: 0.010, -lr * Pred Grad:  0.001, New P: 0.277
iter 11 loss: 0.151
Actual params: [0.7913, 0.2773]
-Original Grad: 0.011, -lr * Pred Grad:  0.003, New P: 0.795
-Original Grad: -0.023, -lr * Pred Grad:  -0.003, New P: 0.274
iter 12 loss: 0.150
Actual params: [0.7947, 0.2743]
-Original Grad: 0.017, -lr * Pred Grad:  0.011, New P: 0.805
-Original Grad: 0.007, -lr * Pred Grad:  0.004, New P: 0.278
iter 13 loss: 0.151
Actual params: [0.8053, 0.2779]
-Original Grad: 0.010, -lr * Pred Grad:  0.006, New P: 0.811
-Original Grad: 0.008, -lr * Pred Grad:  0.003, New P: 0.281
iter 14 loss: 0.151
Actual params: [0.8112, 0.2809]
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: 0.811
-Original Grad: -0.004, -lr * Pred Grad:  -0.001, New P: 0.280
iter 15 loss: 0.151
Actual params: [0.8109, 0.2801]
-Original Grad: 0.020, -lr * Pred Grad:  0.011, New P: 0.821
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: 0.283
iter 16 loss: 0.151
Actual params: [0.8215, 0.2828]
-Original Grad: 0.026, -lr * Pred Grad:  0.014, New P: 0.835
-Original Grad: 0.010, -lr * Pred Grad:  0.005, New P: 0.288
iter 17 loss: 0.151
Actual params: [0.835 , 0.2877]
-Original Grad: 0.028, -lr * Pred Grad:  0.010, New P: 0.845
-Original Grad: -0.035, -lr * Pred Grad:  -0.004, New P: 0.284
iter 18 loss: 0.150
Actual params: [0.8453, 0.2841]
-Original Grad: 0.012, -lr * Pred Grad:  0.006, New P: 0.851
-Original Grad: 0.007, -lr * Pred Grad:  0.002, New P: 0.286
iter 19 loss: 0.151
Actual params: [0.8515, 0.2862]
-Original Grad: 0.004, -lr * Pred Grad:  -0.000, New P: 0.851
-Original Grad: -0.023, -lr * Pred Grad:  -0.003, New P: 0.283
iter 20 loss: 0.150
Actual params: [0.8512, 0.2834]
-Original Grad: 0.017, -lr * Pred Grad:  0.008, New P: 0.859
-Original Grad: 0.009, -lr * Pred Grad:  0.003, New P: 0.286
Target params: [1.1812, 0.2779]
iter 0 loss: 1.328
Actual params: [0.5941, 0.5941]
-Original Grad: 0.387, -lr * Pred Grad:  0.409, New P: 1.003
-Original Grad: -0.041, -lr * Pred Grad:  -0.092, New P: 0.502
iter 1 loss: 0.867
Actual params: [1.003 , 0.5023]
-Original Grad: 2.802, -lr * Pred Grad:  0.029, New P: 1.032
-Original Grad: 0.544, -lr * Pred Grad:  0.147, New P: 0.649
iter 2 loss: 0.692
Actual params: [1.0319, 0.6488]
-Original Grad: 1.388, -lr * Pred Grad:  -0.024, New P: 1.008
-Original Grad: 0.418, -lr * Pred Grad:  0.194, New P: 0.843
iter 3 loss: 0.523
Actual params: [1.0075, 0.8426]
-Original Grad: 0.323, -lr * Pred Grad:  0.029, New P: 1.036
-Original Grad: -0.189, -lr * Pred Grad:  -0.105, New P: 0.737
iter 4 loss: 0.518
Actual params: [1.0361, 0.7373]
-Original Grad: 0.840, -lr * Pred Grad:  0.010, New P: 1.046
-Original Grad: 0.202, -lr * Pred Grad:  -0.002, New P: 0.736
iter 5 loss: 0.485
Actual params: [1.046 , 0.7357]
-Original Grad: 0.802, -lr * Pred Grad:  -0.001, New P: 1.045
-Original Grad: 0.326, -lr * Pred Grad:  0.034, New P: 0.770
iter 6 loss: 0.440
Actual params: [1.0449, 0.7699]
-Original Grad: 0.246, -lr * Pred Grad:  0.013, New P: 1.057
-Original Grad: -0.095, -lr * Pred Grad:  -0.035, New P: 0.735
iter 7 loss: 0.446
Actual params: [1.0574, 0.7345]
-Original Grad: 0.316, -lr * Pred Grad:  0.007, New P: 1.065
-Original Grad: 0.010, -lr * Pred Grad:  -0.014, New P: 0.720
iter 8 loss: 0.444
Actual params: [1.0649, 0.7202]
-Original Grad: 0.374, -lr * Pred Grad:  0.008, New P: 1.073
-Original Grad: 0.024, -lr * Pred Grad:  -0.014, New P: 0.706
iter 9 loss: 0.442
Actual params: [1.073 , 0.7062]
-Original Grad: 0.410, -lr * Pred Grad:  0.008, New P: 1.081
-Original Grad: 0.056, -lr * Pred Grad:  -0.010, New P: 0.696
iter 10 loss: 0.437
Actual params: [1.0806, 0.6958]
-Original Grad: 0.291, -lr * Pred Grad:  0.007, New P: 1.087
-Original Grad: 0.008, -lr * Pred Grad:  -0.012, New P: 0.684
iter 11 loss: 0.439
Actual params: [1.0874, 0.6841]
-Original Grad: 0.412, -lr * Pred Grad:  0.003, New P: 1.090
-Original Grad: 0.169, -lr * Pred Grad:  0.006, New P: 0.690
iter 12 loss: 0.415
Actual params: [1.0902, 0.6897]
-Original Grad: 0.384, -lr * Pred Grad:  0.004, New P: 1.095
-Original Grad: 0.126, -lr * Pred Grad:  -0.000, New P: 0.690
iter 13 loss: 0.405
Actual params: [1.0946, 0.6896]
-Original Grad: 0.312, -lr * Pred Grad:  0.004, New P: 1.099
-Original Grad: 0.091, -lr * Pred Grad:  -0.002, New P: 0.687
iter 14 loss: 0.396
Actual params: [1.099 , 0.6872]
-Original Grad: 0.150, -lr * Pred Grad:  0.006, New P: 1.105
-Original Grad: -0.043, -lr * Pred Grad:  -0.013, New P: 0.674
iter 15 loss: 0.400
Actual params: [1.1053, 0.6744]
-Original Grad: 0.220, -lr * Pred Grad:  0.006, New P: 1.111
-Original Grad: 0.006, -lr * Pred Grad:  -0.009, New P: 0.665
iter 16 loss: 0.403
Actual params: [1.1114, 0.6652]
-Original Grad: 0.254, -lr * Pred Grad:  0.006, New P: 1.117
-Original Grad: 0.044, -lr * Pred Grad:  -0.006, New P: 0.659
iter 17 loss: 0.401
Actual params: [1.1169, 0.6592]
-Original Grad: 0.222, -lr * Pred Grad:  0.004, New P: 1.121
-Original Grad: 0.070, -lr * Pred Grad:  -0.002, New P: 0.658
iter 18 loss: 0.394
Actual params: [1.1205, 0.6577]
-Original Grad: 0.221, -lr * Pred Grad:  0.004, New P: 1.124
-Original Grad: 0.073, -lr * Pred Grad:  -0.001, New P: 0.656
iter 19 loss: 0.388
Actual params: [1.1243, 0.6562]
-Original Grad: 0.119, -lr * Pred Grad:  0.007, New P: 1.131
-Original Grad: -0.069, -lr * Pred Grad:  -0.013, New P: 0.643
iter 20 loss: 0.402
Actual params: [1.1312, 0.6428]
-Original Grad: 0.132, -lr * Pred Grad:  0.005, New P: 1.136
-Original Grad: -0.011, -lr * Pred Grad:  -0.007, New P: 0.636
Target params: [1.1812, 0.2779]
iter 0 loss: 0.321
Actual params: [0.5941, 0.5941]
-Original Grad: 0.039, -lr * Pred Grad:  -0.040, New P: 0.554
-Original Grad: -0.122, -lr * Pred Grad:  -0.094, New P: 0.500
iter 1 loss: 0.325
Actual params: [0.5539, 0.5   ]
-Original Grad: 0.022, -lr * Pred Grad:  0.033, New P: 0.587
-Original Grad: -0.029, -lr * Pred Grad:  -0.004, New P: 0.496
iter 2 loss: 0.327
Actual params: [0.5867, 0.4958]
-Original Grad: 0.005, -lr * Pred Grad:  -0.019, New P: 0.567
-Original Grad: -0.027, -lr * Pred Grad:  -0.024, New P: 0.472
iter 3 loss: 0.327
Actual params: [0.5675, 0.4721]
-Original Grad: 0.010, -lr * Pred Grad:  0.050, New P: 0.617
-Original Grad: 0.013, -lr * Pred Grad:  0.026, New P: 0.499
iter 4 loss: 0.328
Actual params: [0.6175, 0.4986]
-Original Grad: 0.003, -lr * Pred Grad:  -0.038, New P: 0.580
-Original Grad: -0.046, -lr * Pred Grad:  -0.039, New P: 0.460
iter 5 loss: 0.328
Actual params: [0.5799, 0.4599]
-Original Grad: 0.001, -lr * Pred Grad:  0.025, New P: 0.605
-Original Grad: 0.019, -lr * Pred Grad:  0.018, New P: 0.478
iter 6 loss: 0.328
Actual params: [0.605 , 0.4778]
-Original Grad: 0.002, -lr * Pred Grad:  -0.014, New P: 0.591
-Original Grad: -0.020, -lr * Pred Grad:  -0.015, New P: 0.463
iter 7 loss: 0.328
Actual params: [0.5908, 0.4629]
-Original Grad: 0.004, -lr * Pred Grad:  0.009, New P: 0.600
-Original Grad: -0.007, -lr * Pred Grad:  -0.001, New P: 0.462
iter 8 loss: 0.328
Actual params: [0.5996, 0.4618]
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.614
-Original Grad: 0.010, -lr * Pred Grad:  0.009, New P: 0.471
iter 9 loss: 0.329
Actual params: [0.6135, 0.4711]
-Original Grad: 0.004, -lr * Pred Grad:  0.042, New P: 0.655
-Original Grad: 0.024, -lr * Pred Grad:  0.023, New P: 0.494
iter 10 loss: 0.329
Actual params: [0.6553, 0.4944]
-Original Grad: 0.009, -lr * Pred Grad:  0.005, New P: 0.660
-Original Grad: -0.033, -lr * Pred Grad:  -0.017, New P: 0.477
iter 11 loss: 0.329
Actual params: [0.6601, 0.4773]
-Original Grad: 0.008, -lr * Pred Grad:  0.020, New P: 0.680
-Original Grad: -0.016, -lr * Pred Grad:  -0.005, New P: 0.472
iter 12 loss: 0.329
Actual params: [0.68  , 0.4725]
-Original Grad: 0.011, -lr * Pred Grad:  0.066, New P: 0.746
-Original Grad: 0.019, -lr * Pred Grad:  0.024, New P: 0.497
iter 13 loss: 0.326
Actual params: [0.7465, 0.4965]
-Original Grad: 0.013, -lr * Pred Grad:  0.040, New P: 0.786
-Original Grad: -0.019, -lr * Pred Grad:  -0.004, New P: 0.493
iter 14 loss: 0.324
Actual params: [0.7862, 0.4927]
-Original Grad: 0.013, -lr * Pred Grad:  0.042, New P: 0.829
-Original Grad: -0.018, -lr * Pred Grad:  -0.003, New P: 0.490
iter 15 loss: 0.322
Actual params: [0.8286, 0.49  ]
-Original Grad: 0.013, -lr * Pred Grad:  0.051, New P: 0.880
-Original Grad: 0.008, -lr * Pred Grad:  0.011, New P: 0.501
iter 16 loss: 0.318
Actual params: [0.8798, 0.501 ]
-Original Grad: 0.009, -lr * Pred Grad:  0.025, New P: 0.904
-Original Grad: -0.020, -lr * Pred Grad:  -0.008, New P: 0.493
iter 17 loss: 0.315
Actual params: [0.9044, 0.4934]
-Original Grad: 0.017, -lr * Pred Grad:  0.053, New P: 0.957
-Original Grad: -0.019, -lr * Pred Grad:  -0.003, New P: 0.491
iter 18 loss: 0.314
Actual params: [0.957 , 0.4905]
-Original Grad: 0.011, -lr * Pred Grad:  0.028, New P: 0.985
-Original Grad: -0.025, -lr * Pred Grad:  -0.009, New P: 0.481
iter 19 loss: 0.314
Actual params: [0.9849, 0.4813]
-Original Grad: 0.009, -lr * Pred Grad:  0.028, New P: 1.013
-Original Grad: -0.008, -lr * Pred Grad:  -0.001, New P: 0.480
iter 20 loss: 0.314
Actual params: [1.0127, 0.4799]
-Original Grad: 0.001, -lr * Pred Grad:  0.003, New P: 1.016
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: 0.479
