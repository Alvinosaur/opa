Target params: [1.1812, 0.2779]
Actual params: [0.4128, 0.9469]
-Original Grad: 0.087, -lr * Pred Grad:  0.009, New P: 0.421
-Original Grad: -0.142, -lr * Pred Grad:  -0.014, New P: 0.933
iter 0 loss: 0.419
Actual params: [0.4214, 0.9328]
-Original Grad: 0.092, -lr * Pred Grad:  0.017, New P: 0.438
-Original Grad: -0.145, -lr * Pred Grad:  -0.027, New P: 0.906
iter 1 loss: 0.417
Actual params: [0.4385, 0.9056]
-Original Grad: 0.091, -lr * Pred Grad:  0.024, New P: 0.463
-Original Grad: -0.168, -lr * Pred Grad:  -0.041, New P: 0.864
iter 2 loss: 0.410
Actual params: [0.4629, 0.8643]
-Original Grad: 0.104, -lr * Pred Grad:  0.032, New P: 0.495
-Original Grad: -0.195, -lr * Pred Grad:  -0.057, New P: 0.808
iter 3 loss: 0.401
Actual params: [0.4953, 0.8075]
-Original Grad: 0.146, -lr * Pred Grad:  0.044, New P: 0.539
-Original Grad: -0.263, -lr * Pred Grad:  -0.077, New P: 0.730
iter 4 loss: 0.383
Actual params: [0.539 , 0.7302]
-Original Grad: 0.330, -lr * Pred Grad:  0.072, New P: 0.611
-Original Grad: -0.044, -lr * Pred Grad:  -0.074, New P: 0.656
iter 5 loss: 0.367
Actual params: [0.6113, 0.6562]
-Original Grad: 0.403, -lr * Pred Grad:  0.105, New P: 0.717
-Original Grad: -0.151, -lr * Pred Grad:  -0.082, New P: 0.574
iter 6 loss: 0.336
Actual params: [0.7168, 0.5745]
-Original Grad: 0.822, -lr * Pred Grad:  0.177, New P: 0.894
-Original Grad: 0.286, -lr * Pred Grad:  -0.045, New P: 0.529
iter 7 loss: 0.287
Actual params: [0.8938, 0.5294]
-Original Grad: 1.128, -lr * Pred Grad:  0.272, New P: 1.166
-Original Grad: 0.610, -lr * Pred Grad:  0.020, New P: 0.550
iter 8 loss: 0.069
Actual params: [1.1659, 0.5499]
-Original Grad: 0.504, -lr * Pred Grad:  0.295, New P: 1.461
-Original Grad: -0.867, -lr * Pred Grad:  -0.068, New P: 0.482
iter 9 loss: 0.130
Actual params: [1.4612, 0.4817]
-Original Grad: -0.626, -lr * Pred Grad:  0.203, New P: 1.664
-Original Grad: 0.814, -lr * Pred Grad:  0.020, New P: 0.502
iter 10 loss: 0.187
Actual params: [1.6644, 0.5017]
-Original Grad: -0.271, -lr * Pred Grad:  0.156, New P: 1.820
-Original Grad: 0.688, -lr * Pred Grad:  0.087, New P: 0.588
iter 11 loss: 0.265
Actual params: [1.8202, 0.5884]
-Original Grad: -0.253, -lr * Pred Grad:  0.115, New P: 1.935
-Original Grad: 0.493, -lr * Pred Grad:  0.127, New P: 0.716
iter 12 loss: 0.260
Actual params: [1.9351, 0.7158]
-Original Grad: -0.322, -lr * Pred Grad:  0.071, New P: 2.006
-Original Grad: -0.026, -lr * Pred Grad:  0.112, New P: 0.828
iter 13 loss: 0.257
Actual params: [2.0064, 0.8278]
-Original Grad: -0.237, -lr * Pred Grad:  0.040, New P: 2.047
-Original Grad: -0.681, -lr * Pred Grad:  0.033, New P: 0.861
iter 14 loss: 0.343
Actual params: [2.0468, 0.8606]
-Original Grad: -0.244, -lr * Pred Grad:  0.012, New P: 2.059
-Original Grad: -0.807, -lr * Pred Grad:  -0.051, New P: 0.809
iter 15 loss: 0.381
Actual params: [2.0587, 0.8094]
-Original Grad: -0.078, -lr * Pred Grad:  0.003, New P: 2.062
-Original Grad: -0.118, -lr * Pred Grad:  -0.058, New P: 0.751
iter 16 loss: 0.342
Actual params: [2.0616, 0.7515]
-Original Grad: -0.285, -lr * Pred Grad:  -0.026, New P: 2.036
-Original Grad: -0.413, -lr * Pred Grad:  -0.093, New P: 0.658
iter 17 loss: 0.303
Actual params: [2.0358, 0.6581]
-Original Grad: 2.662, -lr * Pred Grad:  0.243, New P: 2.279
-Original Grad: 6.545, -lr * Pred Grad:  0.570, New P: 1.229
iter 18 loss: 0.293
Actual params: [2.2787, 1.2286]
-Original Grad: -2.232, -lr * Pred Grad:  -0.005, New P: 2.274
-Original Grad: 4.156, -lr * Pred Grad:  0.929, New P: 2.158
iter 19 loss: 0.425
Actual params: [2.2741, 2.1576]
-Original Grad: -0.015, -lr * Pred Grad:  -0.006, New P: 2.268
-Original Grad: 0.161, -lr * Pred Grad:  0.852, New P: 3.010
iter 20 loss: 0.414
Actual params: [2.2684, 3.0098]
Target params: [1.1812, 0.2779]
Actual params: [0.8379, 0.635 ]
-Original Grad: -0.102, -lr * Pred Grad:  -0.010, New P: 0.828
-Original Grad: -0.004, -lr * Pred Grad:  -0.000, New P: 0.635
iter 0 loss: 0.121
Actual params: [0.8276, 0.6346]
-Original Grad: 0.106, -lr * Pred Grad:  0.001, New P: 0.829
-Original Grad: -0.157, -lr * Pred Grad:  -0.016, New P: 0.619
iter 1 loss: 0.124
Actual params: [0.829 , 0.6186]
-Original Grad: 0.997, -lr * Pred Grad:  0.101, New P: 0.930
-Original Grad: -0.840, -lr * Pred Grad:  -0.098, New P: 0.520
iter 2 loss: 0.119
Actual params: [0.9299, 0.5202]
-Original Grad: 0.002, -lr * Pred Grad:  0.091, New P: 1.021
-Original Grad: 0.025, -lr * Pred Grad:  -0.086, New P: 0.434
iter 3 loss: 0.136
Actual params: [1.0209, 0.434 ]
-Original Grad: 0.255, -lr * Pred Grad:  0.107, New P: 1.128
-Original Grad: 0.271, -lr * Pred Grad:  -0.050, New P: 0.384
iter 4 loss: 0.153
Actual params: [1.1284, 0.3836]
-Original Grad: -0.622, -lr * Pred Grad:  0.034, New P: 1.163
-Original Grad: -0.501, -lr * Pred Grad:  -0.096, New P: 0.288
iter 5 loss: 0.141
Actual params: [1.1629, 0.2881]
-Original Grad: 0.255, -lr * Pred Grad:  0.057, New P: 1.219
-Original Grad: 0.124, -lr * Pred Grad:  -0.074, New P: 0.215
iter 6 loss: 0.150
Actual params: [1.2194, 0.2145]
-Original Grad: -4.025, -lr * Pred Grad:  -0.352, New P: 0.868
-Original Grad: -1.059, -lr * Pred Grad:  -0.172, New P: 0.042
iter 7 loss: 0.143
Actual params: [0.8678, 0.0424]
-Original Grad: -0.173, -lr * Pred Grad:  -0.334, New P: 0.534
-Original Grad: 0.014, -lr * Pred Grad:  -0.154, New P: -0.111
iter 8 loss: 0.186
Actual params: [ 0.5341, -0.1111]
-Original Grad: 0.011, -lr * Pred Grad:  -0.299, New P: 0.235
-Original Grad: 0.000, -lr * Pred Grad:  -0.138, New P: -0.249
iter 9 loss: 0.116
Actual params: [ 0.2348, -0.2492]
-Original Grad: 0.445, -lr * Pred Grad:  -0.225, New P: 0.010
-Original Grad: 0.078, -lr * Pred Grad:  -0.117, New P: -0.366
iter 10 loss: 0.236
Actual params: [ 0.01  , -0.3657]
-Original Grad: 0.173, -lr * Pred Grad:  -0.185, New P: -0.175
-Original Grad: 0.086, -lr * Pred Grad:  -0.096, New P: -0.462
iter 11 loss: 0.320
Actual params: [-0.1751, -0.462 ]
-Original Grad: 0.186, -lr * Pred Grad:  -0.148, New P: -0.323
-Original Grad: 0.081, -lr * Pred Grad:  -0.079, New P: -0.541
iter 12 loss: 0.363
Actual params: [-0.3231, -0.5406]
-Original Grad: 0.171, -lr * Pred Grad:  -0.116, New P: -0.439
-Original Grad: 0.068, -lr * Pred Grad:  -0.064, New P: -0.605
iter 13 loss: 0.392
Actual params: [-0.4392, -0.6045]
-Original Grad: 0.187, -lr * Pred Grad:  -0.086, New P: -0.525
-Original Grad: 0.077, -lr * Pred Grad:  -0.050, New P: -0.654
iter 14 loss: 0.417
Actual params: [-0.525 , -0.6545]
-Original Grad: 0.298, -lr * Pred Grad:  -0.047, New P: -0.572
-Original Grad: 0.090, -lr * Pred Grad:  -0.036, New P: -0.690
iter 15 loss: 0.437
Actual params: [-0.5723, -0.6904]
-Original Grad: 0.199, -lr * Pred Grad:  -0.023, New P: -0.595
-Original Grad: 0.079, -lr * Pred Grad:  -0.025, New P: -0.715
iter 16 loss: 0.450
Actual params: [-0.5951, -0.7149]
-Original Grad: 0.233, -lr * Pred Grad:  0.003, New P: -0.592
-Original Grad: 0.099, -lr * Pred Grad:  -0.012, New P: -0.727
iter 17 loss: 0.457
Actual params: [-0.5922, -0.7271]
-Original Grad: 0.234, -lr * Pred Grad:  0.026, New P: -0.566
-Original Grad: 0.107, -lr * Pred Grad:  -0.000, New P: -0.727
iter 18 loss: 0.457
Actual params: [-0.5662, -0.7274]
-Original Grad: 0.200, -lr * Pred Grad:  0.043, New P: -0.523
-Original Grad: 0.097, -lr * Pred Grad:  0.009, New P: -0.718
iter 19 loss: 0.453
Actual params: [-0.5228, -0.718 ]
-Original Grad: 0.106, -lr * Pred Grad:  0.050, New P: -0.473
-Original Grad: 0.068, -lr * Pred Grad:  0.015, New P: -0.703
iter 20 loss: 0.442
Actual params: [-0.4731, -0.7027]
Target params: [1.1812, 0.2779]
Actual params: [0.5397, 0.7849]
-Original Grad: -0.985, -lr * Pred Grad:  -0.099, New P: 0.441
-Original Grad: -1.213, -lr * Pred Grad:  -0.121, New P: 0.664
iter 0 loss: 0.717
Actual params: [0.4412, 0.6636]
-Original Grad: -1.562, -lr * Pred Grad:  -0.245, New P: 0.196
-Original Grad: -2.949, -lr * Pred Grad:  -0.404, New P: 0.260
iter 1 loss: 0.372
Actual params: [0.1963, 0.2595]
-Original Grad: -0.001, -lr * Pred Grad:  -0.220, New P: -0.024
-Original Grad: 0.047, -lr * Pred Grad:  -0.359, New P: -0.099
iter 2 loss: 0.212
Actual params: [-0.0241, -0.0994]
-Original Grad: -0.005, -lr * Pred Grad:  -0.199, New P: -0.223
-Original Grad: -0.067, -lr * Pred Grad:  -0.330, New P: -0.429
iter 3 loss: 0.200
Actual params: [-0.223 , -0.4291]
-Original Grad: 0.148, -lr * Pred Grad:  -0.164, New P: -0.387
-Original Grad: 0.059, -lr * Pred Grad:  -0.291, New P: -0.720
iter 4 loss: 0.225
Actual params: [-0.3872, -0.7199]
-Original Grad: 0.207, -lr * Pred Grad:  -0.127, New P: -0.514
-Original Grad: 0.098, -lr * Pred Grad:  -0.252, New P: -0.972
iter 5 loss: 0.280
Actual params: [-0.5143, -0.9719]
-Original Grad: 0.689, -lr * Pred Grad:  -0.046, New P: -0.560
-Original Grad: 0.409, -lr * Pred Grad:  -0.186, New P: -1.158
iter 6 loss: 0.374
Actual params: [-0.5598, -1.1578]
-Original Grad: 0.899, -lr * Pred Grad:  0.049, New P: -0.511
-Original Grad: 0.531, -lr * Pred Grad:  -0.114, New P: -1.272
iter 7 loss: 0.506
Actual params: [-0.5109, -1.272 ]
-Original Grad: -0.219, -lr * Pred Grad:  0.022, New P: -0.489
-Original Grad: 0.132, -lr * Pred Grad:  -0.090, New P: -1.362
iter 8 loss: 1.161
Actual params: [-0.4887, -1.3615]
-Original Grad: -22.740, -lr * Pred Grad:  -2.254, New P: -2.743
-Original Grad: -22.354, -lr * Pred Grad:  -2.316, New P: -3.678
iter 9 loss: 1.189
Actual params: [-2.7427, -3.6776]
-Original Grad: -0.087, -lr * Pred Grad:  -2.037, New P: -4.780
-Original Grad: -0.025, -lr * Pred Grad:  -2.087, New P: -5.764
iter 10 loss: 0.268
Actual params: [-4.7801, -5.7644]
-Original Grad: -0.001, -lr * Pred Grad:  -1.834, New P: -6.614
-Original Grad: 0.002, -lr * Pred Grad:  -1.878, New P: -7.642
iter 11 loss: 0.180
Actual params: [-6.6137, -7.6424]
-Original Grad: 0.008, -lr * Pred Grad:  -1.650, New P: -8.263
-Original Grad: 0.001, -lr * Pred Grad:  -1.690, New P: -9.332
iter 12 loss: 0.195
Actual params: [-8.2633, -9.3325]
-Original Grad: 0.003, -lr * Pred Grad:  -1.484, New P: -9.748
-Original Grad: 0.002, -lr * Pred Grad:  -1.521, New P: -10.853
iter 13 loss: 0.208
Actual params: [ -9.7475, -10.8533]
-Original Grad: 0.000, -lr * Pred Grad:  -1.336, New P: -11.083
-Original Grad: 0.002, -lr * Pred Grad:  -1.368, New P: -12.222
iter 14 loss: 0.214
Actual params: [-11.0833, -12.2218]
-Original Grad: -0.001, -lr * Pred Grad:  -1.202, New P: -12.286
-Original Grad: 0.002, -lr * Pred Grad:  -1.231, New P: -13.453
iter 15 loss: 0.217
Actual params: [-12.2856, -13.4532]
-Original Grad: -0.001, -lr * Pred Grad:  -1.082, New P: -13.368
-Original Grad: 0.002, -lr * Pred Grad:  -1.108, New P: -14.561
iter 16 loss: 0.218
Actual params: [-13.3678, -14.5612]
-Original Grad: -0.002, -lr * Pred Grad:  -0.974, New P: -14.342
-Original Grad: 0.002, -lr * Pred Grad:  -0.997, New P: -15.558
iter 17 loss: 0.218
Actual params: [-14.342 , -15.5583]
-Original Grad: -0.002, -lr * Pred Grad:  -0.877, New P: -15.219
-Original Grad: 0.002, -lr * Pred Grad:  -0.897, New P: -16.455
iter 18 loss: 0.219
Actual params: [-15.2189, -16.4555]
-Original Grad: -0.002, -lr * Pred Grad:  -0.789, New P: -16.008
-Original Grad: 0.002, -lr * Pred Grad:  -0.807, New P: -17.263
iter 19 loss: 0.219
Actual params: [-16.0083, -17.2628]
-Original Grad: -0.002, -lr * Pred Grad:  -0.711, New P: -16.719
-Original Grad: 0.002, -lr * Pred Grad:  -0.726, New P: -17.989
iter 20 loss: 0.219
Actual params: [-16.7188, -17.9892]
Target params: [1.1812, 0.2779]
Actual params: [0.5515, 0.3482]
-Original Grad: 0.248, -lr * Pred Grad:  0.025, New P: 0.576
-Original Grad: -0.125, -lr * Pred Grad:  -0.013, New P: 0.336
iter 0 loss: 0.697
Actual params: [0.5763, 0.3356]
-Original Grad: 0.287, -lr * Pred Grad:  0.051, New P: 0.627
-Original Grad: -0.140, -lr * Pred Grad:  -0.025, New P: 0.310
iter 1 loss: 0.689
Actual params: [0.6274, 0.3104]
-Original Grad: 0.421, -lr * Pred Grad:  0.088, New P: 0.715
-Original Grad: -0.171, -lr * Pred Grad:  -0.040, New P: 0.271
iter 2 loss: 0.667
Actual params: [0.7154, 0.2706]
-Original Grad: 1.123, -lr * Pred Grad:  0.192, New P: 0.907
-Original Grad: -0.532, -lr * Pred Grad:  -0.089, New P: 0.182
iter 3 loss: 0.583
Actual params: [0.907 , 0.1816]
-Original Grad: 1.121, -lr * Pred Grad:  0.284, New P: 1.191
-Original Grad: -0.292, -lr * Pred Grad:  -0.109, New P: 0.072
iter 4 loss: 0.219
Actual params: [1.1915, 0.0723]
-Original Grad: -0.173, -lr * Pred Grad:  0.239, New P: 1.430
-Original Grad: 0.017, -lr * Pred Grad:  -0.097, New P: -0.024
iter 5 loss: 0.124
Actual params: [ 1.4302, -0.0244]
-Original Grad: -0.547, -lr * Pred Grad:  0.160, New P: 1.590
-Original Grad: -0.014, -lr * Pred Grad:  -0.088, New P: -0.113
iter 6 loss: 0.242
Actual params: [ 1.5904, -0.1128]
-Original Grad: -0.351, -lr * Pred Grad:  0.109, New P: 1.699
-Original Grad: 0.114, -lr * Pred Grad:  -0.068, New P: -0.181
iter 7 loss: 0.332
Actual params: [ 1.6994, -0.1809]
-Original Grad: -0.404, -lr * Pred Grad:  0.058, New P: 1.757
-Original Grad: 0.177, -lr * Pred Grad:  -0.044, New P: -0.225
iter 8 loss: 0.393
Actual params: [ 1.7571, -0.2246]
-Original Grad: -0.346, -lr * Pred Grad:  0.017, New P: 1.774
-Original Grad: 0.182, -lr * Pred Grad:  -0.021, New P: -0.246
iter 9 loss: 0.423
Actual params: [ 1.7744, -0.2457]
-Original Grad: -0.318, -lr * Pred Grad:  -0.016, New P: 1.758
-Original Grad: 0.181, -lr * Pred Grad:  -0.001, New P: -0.246
iter 10 loss: 0.433
Actual params: [ 1.7583, -0.2465]
-Original Grad: -0.355, -lr * Pred Grad:  -0.050, New P: 1.708
-Original Grad: 0.186, -lr * Pred Grad:  0.018, New P: -0.229
iter 11 loss: 0.428
Actual params: [ 1.7082, -0.2286]
-Original Grad: -0.415, -lr * Pred Grad:  -0.087, New P: 1.622
-Original Grad: 0.186, -lr * Pred Grad:  0.035, New P: -0.194
iter 12 loss: 0.405
Actual params: [ 1.6217, -0.1938]
-Original Grad: -0.479, -lr * Pred Grad:  -0.126, New P: 1.496
-Original Grad: 0.171, -lr * Pred Grad:  0.048, New P: -0.145
iter 13 loss: 0.360
Actual params: [ 1.4959, -0.1454]
-Original Grad: -0.691, -lr * Pred Grad:  -0.182, New P: 1.314
-Original Grad: 0.166, -lr * Pred Grad:  0.060, New P: -0.085
iter 14 loss: 0.288
Actual params: [ 1.3136, -0.0853]
-Original Grad: -0.631, -lr * Pred Grad:  -0.227, New P: 1.087
-Original Grad: 0.090, -lr * Pred Grad:  0.063, New P: -0.022
iter 15 loss: 0.189
Actual params: [ 1.0865, -0.0222]
-Original Grad: 0.075, -lr * Pred Grad:  -0.197, New P: 0.890
-Original Grad: 0.040, -lr * Pred Grad:  0.061, New P: 0.039
iter 16 loss: 0.117
Actual params: [0.8896, 0.0386]
-Original Grad: 0.818, -lr * Pred Grad:  -0.095, New P: 0.794
-Original Grad: -0.009, -lr * Pred Grad:  0.054, New P: 0.092
iter 17 loss: 0.210
Actual params: [0.7942, 0.0923]
-Original Grad: 3.869, -lr * Pred Grad:  0.301, New P: 1.095
-Original Grad: -0.761, -lr * Pred Grad:  -0.028, New P: 0.065
iter 18 loss: 0.355
Actual params: [1.0953, 0.0646]
-Original Grad: 0.053, -lr * Pred Grad:  0.276, New P: 1.372
-Original Grad: 0.007, -lr * Pred Grad:  -0.024, New P: 0.040
iter 19 loss: 0.115
Actual params: [1.3715, 0.0404]
-Original Grad: -0.482, -lr * Pred Grad:  0.200, New P: 1.572
-Original Grad: 0.000, -lr * Pred Grad:  -0.022, New P: 0.019
iter 20 loss: 0.215
Actual params: [1.572 , 0.0186]
Target params: [1.1812, 0.2779]
Actual params: [1.0909, 1.0332]
-Original Grad: -0.160, -lr * Pred Grad:  -0.016, New P: 1.075
-Original Grad: 0.334, -lr * Pred Grad:  0.033, New P: 1.067
iter 0 loss: 0.400
Actual params: [1.0749, 1.0666]
-Original Grad: -0.042, -lr * Pred Grad:  -0.019, New P: 1.056
-Original Grad: 0.267, -lr * Pred Grad:  0.057, New P: 1.123
iter 1 loss: 0.386
Actual params: [1.0562, 1.1234]
-Original Grad: 0.047, -lr * Pred Grad:  -0.012, New P: 1.044
-Original Grad: 0.352, -lr * Pred Grad:  0.086, New P: 1.210
iter 2 loss: 0.376
Actual params: [1.0442, 1.2097]
-Original Grad: 0.200, -lr * Pred Grad:  0.009, New P: 1.053
-Original Grad: 0.286, -lr * Pred Grad:  0.106, New P: 1.316
iter 3 loss: 0.360
Actual params: [1.0533, 1.316 ]
-Original Grad: 0.062, -lr * Pred Grad:  0.014, New P: 1.068
-Original Grad: 0.359, -lr * Pred Grad:  0.132, New P: 1.448
iter 4 loss: 0.338
Actual params: [1.0677, 1.4476]
-Original Grad: -0.257, -lr * Pred Grad:  -0.013, New P: 1.055
-Original Grad: 0.607, -lr * Pred Grad:  0.179, New P: 1.627
iter 5 loss: 0.290
Actual params: [1.055 , 1.6267]
-Original Grad: -0.943, -lr * Pred Grad:  -0.106, New P: 0.949
-Original Grad: 2.777, -lr * Pred Grad:  0.439, New P: 2.066
iter 6 loss: 0.233
Actual params: [0.9493, 2.0656]
-Original Grad: -0.180, -lr * Pred Grad:  -0.113, New P: 0.836
-Original Grad: -0.178, -lr * Pred Grad:  0.377, New P: 2.443
iter 7 loss: 0.299
Actual params: [0.8361, 2.4427]
-Original Grad: -0.624, -lr * Pred Grad:  -0.164, New P: 0.672
-Original Grad: -0.215, -lr * Pred Grad:  0.318, New P: 2.761
iter 8 loss: 0.330
Actual params: [0.6718, 2.7606]
-Original Grad: 0.725, -lr * Pred Grad:  -0.075, New P: 0.596
-Original Grad: 0.181, -lr * Pred Grad:  0.304, New P: 3.065
iter 9 loss: 0.277
Actual params: [0.5965, 3.0648]
-Original Grad: -2.946, -lr * Pred Grad:  -0.362, New P: 0.234
-Original Grad: -0.432, -lr * Pred Grad:  0.231, New P: 3.295
iter 10 loss: 0.275
Actual params: [0.2341, 3.2953]
-Original Grad: 0.457, -lr * Pred Grad:  -0.280, New P: -0.046
-Original Grad: 0.114, -lr * Pred Grad:  0.219, New P: 3.514
iter 11 loss: 0.555
Actual params: [-0.0464,  3.5143]
-Original Grad: 0.140, -lr * Pred Grad:  -0.238, New P: -0.285
-Original Grad: 0.105, -lr * Pred Grad:  0.208, New P: 3.722
iter 12 loss: 0.602
Actual params: [-0.2848,  3.7218]
-Original Grad: 0.050, -lr * Pred Grad:  -0.210, New P: -0.494
-Original Grad: 0.102, -lr * Pred Grad:  0.197, New P: 3.919
iter 13 loss: 0.599
Actual params: [-0.4944,  3.9187]
-Original Grad: 0.070, -lr * Pred Grad:  -0.182, New P: -0.676
-Original Grad: 0.101, -lr * Pred Grad:  0.187, New P: 4.106
iter 14 loss: 0.590
Actual params: [-0.676 ,  4.1061]
-Original Grad: 0.130, -lr * Pred Grad:  -0.150, New P: -0.826
-Original Grad: 0.120, -lr * Pred Grad:  0.181, New P: 4.287
iter 15 loss: 0.587
Actual params: [-0.8265,  4.2867]
-Original Grad: 0.166, -lr * Pred Grad:  -0.119, New P: -0.945
-Original Grad: 0.132, -lr * Pred Grad:  0.176, New P: 4.463
iter 16 loss: 0.586
Actual params: [-0.9453,  4.4625]
-Original Grad: 0.201, -lr * Pred Grad:  -0.087, New P: -1.032
-Original Grad: 0.146, -lr * Pred Grad:  0.173, New P: 4.635
iter 17 loss: 0.584
Actual params: [-1.032 ,  4.6353]
-Original Grad: 0.212, -lr * Pred Grad:  -0.057, New P: -1.089
-Original Grad: 0.143, -lr * Pred Grad:  0.170, New P: 4.805
iter 18 loss: 0.577
Actual params: [-1.0889,  4.8051]
-Original Grad: 0.204, -lr * Pred Grad:  -0.031, New P: -1.120
-Original Grad: 0.131, -lr * Pred Grad:  0.166, New P: 4.971
iter 19 loss: 0.566
Actual params: [-1.1197,  4.9709]
-Original Grad: 0.198, -lr * Pred Grad:  -0.008, New P: -1.128
-Original Grad: 0.127, -lr * Pred Grad:  0.162, New P: 5.133
iter 20 loss: 0.551
Actual params: [-1.1276,  5.1329]
Target params: [1.1812, 0.2779]
Actual params: [1.1249, 0.5545]
-Original Grad: 0.214, -lr * Pred Grad:  0.021, New P: 1.146
-Original Grad: 0.731, -lr * Pred Grad:  0.073, New P: 0.628
iter 0 loss: 0.149
Actual params: [1.1463, 0.6277]
-Original Grad: 0.124, -lr * Pred Grad:  0.032, New P: 1.178
-Original Grad: 0.365, -lr * Pred Grad:  0.102, New P: 0.730
iter 1 loss: 0.120
Actual params: [1.178, 0.73 ]
-Original Grad: 1.209, -lr * Pred Grad:  0.149, New P: 1.327
-Original Grad: -5.327, -lr * Pred Grad:  -0.441, New P: 0.289
iter 2 loss: 0.761
Actual params: [1.3274, 0.2894]
-Original Grad: -0.281, -lr * Pred Grad:  0.106, New P: 1.434
-Original Grad: -0.071, -lr * Pred Grad:  -0.404, New P: -0.114
iter 3 loss: 0.184
Actual params: [ 1.4338, -0.1144]
-Original Grad: -0.308, -lr * Pred Grad:  0.065, New P: 1.499
-Original Grad: 0.080, -lr * Pred Grad:  -0.355, New P: -0.470
iter 4 loss: 0.208
Actual params: [ 1.4988, -0.4697]
-Original Grad: -0.206, -lr * Pred Grad:  0.038, New P: 1.537
-Original Grad: -0.018, -lr * Pred Grad:  -0.322, New P: -0.791
iter 5 loss: 0.219
Actual params: [ 1.5368, -0.7912]
-Original Grad: -0.172, -lr * Pred Grad:  0.017, New P: 1.554
-Original Grad: 0.047, -lr * Pred Grad:  -0.285, New P: -1.076
iter 6 loss: 0.230
Actual params: [ 1.5537, -1.0759]
-Original Grad: -0.134, -lr * Pred Grad:  0.002, New P: 1.556
-Original Grad: 0.134, -lr * Pred Grad:  -0.243, New P: -1.319
iter 7 loss: 0.253
Actual params: [ 1.5556, -1.3187]
-Original Grad: -0.132, -lr * Pred Grad:  -0.011, New P: 1.544
-Original Grad: 0.206, -lr * Pred Grad:  -0.198, New P: -1.517
iter 8 loss: 0.285
Actual params: [ 1.5442, -1.5166]
-Original Grad: 1.007, -lr * Pred Grad:  0.090, New P: 1.635
-Original Grad: -0.969, -lr * Pred Grad:  -0.275, New P: -1.792
iter 9 loss: 0.314
Actual params: [ 1.6346, -1.7917]
-Original Grad: -0.091, -lr * Pred Grad:  0.072, New P: 1.707
-Original Grad: 0.146, -lr * Pred Grad:  -0.233, New P: -2.025
iter 10 loss: 0.363
Actual params: [ 1.7068, -2.0246]
-Original Grad: -0.042, -lr * Pred Grad:  0.061, New P: 1.768
-Original Grad: 0.116, -lr * Pred Grad:  -0.198, New P: -2.223
iter 11 loss: 0.387
Actual params: [ 1.7677, -2.2227]
-Original Grad: -0.020, -lr * Pred Grad:  0.053, New P: 1.820
-Original Grad: 0.089, -lr * Pred Grad:  -0.169, New P: -2.392
iter 12 loss: 0.408
Actual params: [ 1.8205, -2.3921]
-Original Grad: -0.006, -lr * Pred Grad:  0.047, New P: 1.867
-Original Grad: 0.079, -lr * Pred Grad:  -0.144, New P: -2.537
iter 13 loss: 0.423
Actual params: [ 1.8674, -2.5366]
-Original Grad: 0.030, -lr * Pred Grad:  0.045, New P: 1.913
-Original Grad: 0.055, -lr * Pred Grad:  -0.125, New P: -2.661
iter 14 loss: 0.430
Actual params: [ 1.9125, -2.6612]
-Original Grad: 0.167, -lr * Pred Grad:  0.057, New P: 1.970
-Original Grad: 0.038, -lr * Pred Grad:  -0.108, New P: -2.769
iter 15 loss: 0.436
Actual params: [ 1.9698, -2.7695]
-Original Grad: 0.010, -lr * Pred Grad:  0.053, New P: 2.022
-Original Grad: 0.060, -lr * Pred Grad:  -0.092, New P: -2.861
iter 16 loss: 0.441
Actual params: [ 2.0225, -2.861 ]
-Original Grad: 0.009, -lr * Pred Grad:  0.048, New P: 2.071
-Original Grad: 0.056, -lr * Pred Grad:  -0.077, New P: -2.938
iter 17 loss: 0.446
Actual params: [ 2.0708, -2.9377]
-Original Grad: 0.016, -lr * Pred Grad:  0.045, New P: 2.116
-Original Grad: 0.056, -lr * Pred Grad:  -0.063, New P: -3.001
iter 18 loss: 0.450
Actual params: [ 2.1159, -3.0011]
-Original Grad: 0.005, -lr * Pred Grad:  0.041, New P: 2.157
-Original Grad: 0.042, -lr * Pred Grad:  -0.053, New P: -3.054
iter 19 loss: 0.452
Actual params: [ 2.157 , -3.0539]
-Original Grad: 0.020, -lr * Pred Grad:  0.039, New P: 2.196
-Original Grad: 0.051, -lr * Pred Grad:  -0.042, New P: -3.096
iter 20 loss: 0.454
Actual params: [ 2.1959, -3.0963]
Target params: [1.1812, 0.2779]
Actual params: [0.6859, 0.3761]
-Original Grad: 0.112, -lr * Pred Grad:  0.011, New P: 0.697
-Original Grad: -0.053, -lr * Pred Grad:  -0.005, New P: 0.371
iter 0 loss: 0.244
Actual params: [0.6971, 0.3708]
-Original Grad: 0.175, -lr * Pred Grad:  0.028, New P: 0.725
-Original Grad: 0.001, -lr * Pred Grad:  -0.005, New P: 0.366
iter 1 loss: 0.248
Actual params: [0.7247, 0.3661]
-Original Grad: 0.215, -lr * Pred Grad:  0.046, New P: 0.771
-Original Grad: -0.023, -lr * Pred Grad:  -0.006, New P: 0.360
iter 2 loss: 0.241
Actual params: [0.771 , 0.3597]
-Original Grad: 0.192, -lr * Pred Grad:  0.061, New P: 0.832
-Original Grad: -0.007, -lr * Pred Grad:  -0.007, New P: 0.353
iter 3 loss: 0.232
Actual params: [0.8319, 0.3531]
-Original Grad: 0.136, -lr * Pred Grad:  0.068, New P: 0.900
-Original Grad: 0.008, -lr * Pred Grad:  -0.005, New P: 0.348
iter 4 loss: 0.222
Actual params: [0.9004, 0.348 ]
-Original Grad: 0.061, -lr * Pred Grad:  0.068, New P: 0.968
-Original Grad: 0.018, -lr * Pred Grad:  -0.003, New P: 0.345
iter 5 loss: 0.215
Actual params: [0.9681, 0.3453]
-Original Grad: -0.004, -lr * Pred Grad:  0.061, New P: 1.029
-Original Grad: 0.013, -lr * Pred Grad:  -0.001, New P: 0.344
iter 6 loss: 0.213
Actual params: [1.0286, 0.3441]
-Original Grad: -0.002, -lr * Pred Grad:  0.054, New P: 1.083
-Original Grad: 0.015, -lr * Pred Grad:  0.000, New P: 0.345
iter 7 loss: 0.214
Actual params: [1.0829, 0.3446]
-Original Grad: 0.002, -lr * Pred Grad:  0.049, New P: 1.132
-Original Grad: 0.014, -lr * Pred Grad:  0.002, New P: 0.346
iter 8 loss: 0.214
Actual params: [1.132 , 0.3464]
-Original Grad: -0.013, -lr * Pred Grad:  0.043, New P: 1.175
-Original Grad: 0.015, -lr * Pred Grad:  0.003, New P: 0.349
iter 9 loss: 0.214
Actual params: [1.1749, 0.3495]
-Original Grad: -0.020, -lr * Pred Grad:  0.037, New P: 1.211
-Original Grad: 0.010, -lr * Pred Grad:  0.004, New P: 0.353
iter 10 loss: 0.215
Actual params: [1.2114, 0.3532]
-Original Grad: -0.030, -lr * Pred Grad:  0.030, New P: 1.241
-Original Grad: 0.001, -lr * Pred Grad:  0.003, New P: 0.357
iter 11 loss: 0.216
Actual params: [1.2414, 0.3566]
-Original Grad: -0.037, -lr * Pred Grad:  0.023, New P: 1.265
-Original Grad: 0.007, -lr * Pred Grad:  0.004, New P: 0.360
iter 12 loss: 0.217
Actual params: [1.2646, 0.3605]
-Original Grad: -0.034, -lr * Pred Grad:  0.018, New P: 1.282
-Original Grad: 0.004, -lr * Pred Grad:  0.004, New P: 0.364
iter 13 loss: 0.218
Actual params: [1.2821, 0.3643]
-Original Grad: -0.030, -lr * Pred Grad:  0.013, New P: 1.295
-Original Grad: -0.006, -lr * Pred Grad:  0.003, New P: 0.367
iter 14 loss: 0.218
Actual params: [1.2949, 0.3671]
-Original Grad: -0.041, -lr * Pred Grad:  0.007, New P: 1.302
-Original Grad: -0.010, -lr * Pred Grad:  0.002, New P: 0.369
iter 15 loss: 0.219
Actual params: [1.3023, 0.3687]
-Original Grad: -0.046, -lr * Pred Grad:  0.002, New P: 1.304
-Original Grad: -0.016, -lr * Pred Grad:  -0.000, New P: 0.368
iter 16 loss: 0.219
Actual params: [1.3043, 0.3685]
-Original Grad: -0.046, -lr * Pred Grad:  -0.003, New P: 1.301
-Original Grad: -0.016, -lr * Pred Grad:  -0.002, New P: 0.367
iter 17 loss: 0.219
Actual params: [1.3015, 0.3667]
-Original Grad: -0.046, -lr * Pred Grad:  -0.007, New P: 1.294
-Original Grad: -0.013, -lr * Pred Grad:  -0.003, New P: 0.364
iter 18 loss: 0.219
Actual params: [1.2944, 0.3637]
-Original Grad: -0.041, -lr * Pred Grad:  -0.010, New P: 1.284
-Original Grad: -0.009, -lr * Pred Grad:  -0.004, New P: 0.360
iter 19 loss: 0.218
Actual params: [1.2839, 0.3601]
-Original Grad: -0.030, -lr * Pred Grad:  -0.012, New P: 1.271
-Original Grad: -0.002, -lr * Pred Grad:  -0.003, New P: 0.357
iter 20 loss: 0.218
Actual params: [1.2715, 0.3566]
Target params: [1.1812, 0.2779]
Actual params: [0.7976, 0.9218]
-Original Grad: -1.224, -lr * Pred Grad:  -0.122, New P: 0.675
-Original Grad: -1.434, -lr * Pred Grad:  -0.143, New P: 0.778
iter 0 loss: 0.486
Actual params: [0.6752, 0.7784]
-Original Grad: -0.067, -lr * Pred Grad:  -0.117, New P: 0.558
-Original Grad: -0.468, -lr * Pred Grad:  -0.176, New P: 0.603
iter 1 loss: 0.296
Actual params: [0.5583, 0.6025]
-Original Grad: 0.209, -lr * Pred Grad:  -0.084, New P: 0.474
-Original Grad: 0.145, -lr * Pred Grad:  -0.144, New P: 0.459
iter 2 loss: 0.338
Actual params: [0.474 , 0.4588]
-Original Grad: 0.127, -lr * Pred Grad:  -0.063, New P: 0.411
-Original Grad: 0.058, -lr * Pred Grad:  -0.124, New P: 0.335
iter 3 loss: 0.365
Actual params: [0.4108, 0.3352]
-Original Grad: 0.152, -lr * Pred Grad:  -0.042, New P: 0.369
-Original Grad: 0.021, -lr * Pred Grad:  -0.109, New P: 0.226
iter 4 loss: 0.376
Actual params: [0.3692, 0.2261]
-Original Grad: 0.095, -lr * Pred Grad:  -0.028, New P: 0.341
-Original Grad: -0.043, -lr * Pred Grad:  -0.103, New P: 0.123
iter 5 loss: 0.378
Actual params: [0.3411, 0.1235]
-Original Grad: 0.084, -lr * Pred Grad:  -0.017, New P: 0.324
-Original Grad: -0.058, -lr * Pred Grad:  -0.098, New P: 0.025
iter 6 loss: 0.375
Actual params: [0.3243, 0.0254]
-Original Grad: 0.086, -lr * Pred Grad:  -0.007, New P: 0.318
-Original Grad: -0.061, -lr * Pred Grad:  -0.094, New P: -0.069
iter 7 loss: 0.371
Actual params: [ 0.3178, -0.069 ]
-Original Grad: 0.091, -lr * Pred Grad:  0.003, New P: 0.321
-Original Grad: -0.030, -lr * Pred Grad:  -0.088, New P: -0.157
iter 8 loss: 0.368
Actual params: [ 0.3211, -0.1569]
-Original Grad: 0.095, -lr * Pred Grad:  0.012, New P: 0.334
-Original Grad: -0.015, -lr * Pred Grad:  -0.081, New P: -0.238
iter 9 loss: 0.366
Actual params: [ 0.3336, -0.2375]
-Original Grad: 0.099, -lr * Pred Grad:  0.021, New P: 0.355
-Original Grad: -0.012, -lr * Pred Grad:  -0.074, New P: -0.311
iter 10 loss: 0.364
Actual params: [ 0.3547, -0.3113]
-Original Grad: 0.098, -lr * Pred Grad:  0.029, New P: 0.384
-Original Grad: -0.014, -lr * Pred Grad:  -0.068, New P: -0.379
iter 11 loss: 0.360
Actual params: [ 0.3836, -0.3792]
-Original Grad: 0.101, -lr * Pred Grad:  0.036, New P: 0.420
-Original Grad: -0.016, -lr * Pred Grad:  -0.063, New P: -0.442
iter 12 loss: 0.356
Actual params: [ 0.4196, -0.4419]
-Original Grad: 0.434, -lr * Pred Grad:  0.076, New P: 0.495
-Original Grad: -0.110, -lr * Pred Grad:  -0.067, New P: -0.509
iter 13 loss: 0.350
Actual params: [ 0.4954, -0.5093]
-Original Grad: 0.129, -lr * Pred Grad:  0.081, New P: 0.577
-Original Grad: -0.032, -lr * Pred Grad:  -0.064, New P: -0.573
iter 14 loss: 0.339
Actual params: [ 0.5765, -0.5732]
-Original Grad: 0.171, -lr * Pred Grad:  0.090, New P: 0.667
-Original Grad: -0.041, -lr * Pred Grad:  -0.062, New P: -0.635
iter 15 loss: 0.324
Actual params: [ 0.6667, -0.6347]
-Original Grad: 0.297, -lr * Pred Grad:  0.111, New P: 0.778
-Original Grad: -0.044, -lr * Pred Grad:  -0.060, New P: -0.694
iter 16 loss: 0.302
Actual params: [ 0.7776, -0.6945]
-Original Grad: 0.606, -lr * Pred Grad:  0.160, New P: 0.938
-Original Grad: -0.012, -lr * Pred Grad:  -0.055, New P: -0.749
iter 17 loss: 0.251
Actual params: [ 0.938 , -0.7495]
-Original Grad: 0.585, -lr * Pred Grad:  0.203, New P: 1.141
-Original Grad: 0.022, -lr * Pred Grad:  -0.047, New P: -0.797
iter 18 loss: 0.147
Actual params: [ 1.1408, -0.7968]
-Original Grad: -0.101, -lr * Pred Grad:  0.173, New P: 1.313
-Original Grad: 0.059, -lr * Pred Grad:  -0.037, New P: -0.833
iter 19 loss: 0.112
Actual params: [ 1.3133, -0.8335]
-Original Grad: 3.688, -lr * Pred Grad:  0.524, New P: 1.837
-Original Grad: -1.471, -lr * Pred Grad:  -0.180, New P: -1.014
iter 20 loss: 0.137
Actual params: [ 1.8374, -1.0136]
Target params: [1.1812, 0.2779]
Actual params: [0.7765, 0.9   ]
-Original Grad: -0.989, -lr * Pred Grad:  -0.099, New P: 0.678
-Original Grad: -0.227, -lr * Pred Grad:  -0.023, New P: 0.877
iter 0 loss: 0.363
Actual params: [0.6776, 0.8773]
-Original Grad: -0.197, -lr * Pred Grad:  -0.109, New P: 0.569
-Original Grad: -0.099, -lr * Pred Grad:  -0.030, New P: 0.847
iter 1 loss: 0.360
Actual params: [0.5689, 0.847 ]
-Original Grad: -0.126, -lr * Pred Grad:  -0.110, New P: 0.458
-Original Grad: -0.001, -lr * Pred Grad:  -0.027, New P: 0.820
iter 2 loss: 0.372
Actual params: [0.4584, 0.8196]
-Original Grad: 0.247, -lr * Pred Grad:  -0.075, New P: 0.384
-Original Grad: -0.244, -lr * Pred Grad:  -0.049, New P: 0.771
iter 3 loss: 0.367
Actual params: [0.3838, 0.7706]
-Original Grad: 0.024, -lr * Pred Grad:  -0.065, New P: 0.319
-Original Grad: -0.059, -lr * Pred Grad:  -0.050, New P: 0.721
iter 4 loss: 0.378
Actual params: [0.3189, 0.7205]
-Original Grad: 0.010, -lr * Pred Grad:  -0.057, New P: 0.262
-Original Grad: -0.030, -lr * Pred Grad:  -0.048, New P: 0.672
iter 5 loss: 0.377
Actual params: [0.2616, 0.6725]
-Original Grad: 0.022, -lr * Pred Grad:  -0.049, New P: 0.212
-Original Grad: -0.020, -lr * Pred Grad:  -0.045, New P: 0.627
iter 6 loss: 0.377
Actual params: [0.2122, 0.6273]
-Original Grad: 0.026, -lr * Pred Grad:  -0.042, New P: 0.170
-Original Grad: -0.011, -lr * Pred Grad:  -0.042, New P: 0.585
iter 7 loss: 0.377
Actual params: [0.1704, 0.5854]
-Original Grad: 0.029, -lr * Pred Grad:  -0.035, New P: 0.136
-Original Grad: 0.006, -lr * Pred Grad:  -0.037, New P: 0.548
iter 8 loss: 0.378
Actual params: [0.1357, 0.5484]
-Original Grad: 0.037, -lr * Pred Grad:  -0.028, New P: 0.108
-Original Grad: 0.013, -lr * Pred Grad:  -0.032, New P: 0.516
iter 9 loss: 0.380
Actual params: [0.1082, 0.5163]
-Original Grad: 0.035, -lr * Pred Grad:  -0.021, New P: 0.087
-Original Grad: 0.019, -lr * Pred Grad:  -0.027, New P: 0.489
iter 10 loss: 0.381
Actual params: [0.0869, 0.4894]
-Original Grad: 0.037, -lr * Pred Grad:  -0.015, New P: 0.071
-Original Grad: 0.026, -lr * Pred Grad:  -0.022, New P: 0.468
iter 11 loss: 0.383
Actual params: [0.0714, 0.4677]
-Original Grad: 0.033, -lr * Pred Grad:  -0.011, New P: 0.061
-Original Grad: 0.031, -lr * Pred Grad:  -0.016, New P: 0.451
iter 12 loss: 0.384
Actual params: [0.0608, 0.4512]
-Original Grad: 0.030, -lr * Pred Grad:  -0.007, New P: 0.054
-Original Grad: 0.033, -lr * Pred Grad:  -0.012, New P: 0.440
iter 13 loss: 0.385
Actual params: [0.0543, 0.4397]
-Original Grad: 0.031, -lr * Pred Grad:  -0.003, New P: 0.052
-Original Grad: 0.039, -lr * Pred Grad:  -0.006, New P: 0.433
iter 14 loss: 0.386
Actual params: [0.0516, 0.4332]
-Original Grad: 0.029, -lr * Pred Grad:  0.000, New P: 0.052
-Original Grad: 0.039, -lr * Pred Grad:  -0.002, New P: 0.431
iter 15 loss: 0.386
Actual params: [0.052 , 0.4313]
-Original Grad: 0.030, -lr * Pred Grad:  0.003, New P: 0.055
-Original Grad: 0.040, -lr * Pred Grad:  0.002, New P: 0.434
iter 16 loss: 0.386
Actual params: [0.0554, 0.4335]
-Original Grad: 0.032, -lr * Pred Grad:  0.006, New P: 0.062
-Original Grad: 0.038, -lr * Pred Grad:  0.006, New P: 0.439
iter 17 loss: 0.386
Actual params: [0.0616, 0.4394]
-Original Grad: 0.032, -lr * Pred Grad:  0.009, New P: 0.070
-Original Grad: 0.038, -lr * Pred Grad:  0.009, New P: 0.449
iter 18 loss: 0.385
Actual params: [0.0704, 0.4485]
-Original Grad: 0.034, -lr * Pred Grad:  0.011, New P: 0.082
-Original Grad: 0.034, -lr * Pred Grad:  0.012, New P: 0.460
iter 19 loss: 0.385
Actual params: [0.0818, 0.4601]
-Original Grad: 0.040, -lr * Pred Grad:  0.014, New P: 0.096
-Original Grad: 0.032, -lr * Pred Grad:  0.014, New P: 0.474
iter 20 loss: 0.384
Actual params: [0.096 , 0.4738]
Target params: [1.1812, 0.2779]
Actual params: [0.9202, 0.6989]
-Original Grad: -0.135, -lr * Pred Grad:  -0.014, New P: 0.907
-Original Grad: -1.176, -lr * Pred Grad:  -0.118, New P: 0.581
iter 0 loss: 0.624
Actual params: [0.9066, 0.5813]
-Original Grad: -0.466, -lr * Pred Grad:  -0.059, New P: 0.848
-Original Grad: -2.672, -lr * Pred Grad:  -0.373, New P: 0.208
iter 1 loss: 0.380
Actual params: [0.8478, 0.2082]
-Original Grad: 0.130, -lr * Pred Grad:  -0.040, New P: 0.808
-Original Grad: 0.105, -lr * Pred Grad:  -0.325, New P: -0.117
iter 2 loss: 0.142
Actual params: [ 0.8078, -0.117 ]
-Original Grad: 0.080, -lr * Pred Grad:  -0.028, New P: 0.780
-Original Grad: 0.237, -lr * Pred Grad:  -0.269, New P: -0.386
iter 3 loss: 0.217
Actual params: [ 0.7798, -0.386 ]
-Original Grad: 0.035, -lr * Pred Grad:  -0.022, New P: 0.758
-Original Grad: 0.089, -lr * Pred Grad:  -0.233, New P: -0.619
iter 4 loss: 0.257
Actual params: [ 0.7581, -0.6191]
-Original Grad: 0.016, -lr * Pred Grad:  -0.018, New P: 0.740
-Original Grad: 0.133, -lr * Pred Grad:  -0.197, New P: -0.816
iter 5 loss: 0.268
Actual params: [ 0.7402, -0.8157]
-Original Grad: -0.052, -lr * Pred Grad:  -0.021, New P: 0.719
-Original Grad: 0.458, -lr * Pred Grad:  -0.131, New P: -0.947
iter 6 loss: 0.329
Actual params: [ 0.7189, -0.9467]
-Original Grad: -0.427, -lr * Pred Grad:  -0.062, New P: 0.657
-Original Grad: 0.888, -lr * Pred Grad:  -0.029, New P: -0.976
iter 7 loss: 0.298
Actual params: [ 0.657 , -0.9759]
-Original Grad: -2.213, -lr * Pred Grad:  -0.277, New P: 0.380
-Original Grad: 9.453, -lr * Pred Grad:  0.919, New P: -0.057
iter 8 loss: 0.270
Actual params: [ 0.3801, -0.0568]
-Original Grad: -0.122, -lr * Pred Grad:  -0.261, New P: 0.119
-Original Grad: 0.181, -lr * Pred Grad:  0.845, New P: 0.788
iter 9 loss: 0.186
Actual params: [0.1186, 0.7885]
-Original Grad: -0.058, -lr * Pred Grad:  -0.241, New P: -0.122
-Original Grad: -0.141, -lr * Pred Grad:  0.747, New P: 1.535
iter 10 loss: 0.805
Actual params: [-0.1225,  1.5352]
-Original Grad: -0.282, -lr * Pred Grad:  -0.245, New P: -0.368
-Original Grad: 0.571, -lr * Pred Grad:  0.729, New P: 2.264
iter 11 loss: 0.530
Actual params: [-0.3677,  2.2643]
-Original Grad: -0.055, -lr * Pred Grad:  -0.226, New P: -0.594
-Original Grad: 0.067, -lr * Pred Grad:  0.663, New P: 2.927
iter 12 loss: 0.346
Actual params: [-0.5939,  2.9273]
-Original Grad: -0.067, -lr * Pred Grad:  -0.210, New P: -0.804
-Original Grad: 0.039, -lr * Pred Grad:  0.601, New P: 3.528
iter 13 loss: 0.298
Actual params: [-0.8042,  3.5278]
-Original Grad: -0.038, -lr * Pred Grad:  -0.193, New P: -0.997
-Original Grad: 0.021, -lr * Pred Grad:  0.543, New P: 4.070
iter 14 loss: 0.269
Actual params: [-0.9972,  4.0704]
-Original Grad: -0.041, -lr * Pred Grad:  -0.178, New P: -1.175
-Original Grad: 0.014, -lr * Pred Grad:  0.490, New P: 4.560
iter 15 loss: 0.250
Actual params: [-1.175,  4.56 ]
-Original Grad: -0.114, -lr * Pred Grad:  -0.171, New P: -1.346
-Original Grad: 0.001, -lr * Pred Grad:  0.441, New P: 5.001
iter 16 loss: 0.232
Actual params: [-1.3464,  5.0008]
-Original Grad: -0.164, -lr * Pred Grad:  -0.171, New P: -1.517
-Original Grad: -0.004, -lr * Pred Grad:  0.396, New P: 5.397
iter 17 loss: 0.211
Actual params: [-1.5172,  5.3972]
-Original Grad: -0.134, -lr * Pred Grad:  -0.167, New P: -1.684
-Original Grad: -0.005, -lr * Pred Grad:  0.356, New P: 5.753
iter 18 loss: 0.187
Actual params: [-1.6842,  5.7534]
-Original Grad: -0.071, -lr * Pred Grad:  -0.157, New P: -1.842
-Original Grad: -0.003, -lr * Pred Grad:  0.320, New P: 6.074
iter 19 loss: 0.175
Actual params: [-1.8417,  6.0737]
-Original Grad: -0.042, -lr * Pred Grad:  -0.146, New P: -1.988
-Original Grad: -0.002, -lr * Pred Grad:  0.288, New P: 6.362
iter 20 loss: 0.165
Actual params: [-1.9876,  6.3617]
