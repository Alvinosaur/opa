Target params: [1.3344, 1.5708]
iter 0 loss: 0.077
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.044, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.074
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.016, -lr * Pred Grad:  -0.089, New P: -0.661
-Original Grad: -0.000, -lr * Pred Grad:  -0.091, New P: -0.188
iter 2 loss: 0.073
Actual params: [-0.6609, -0.1875]
-Original Grad: -0.012, -lr * Pred Grad:  -0.083, New P: -0.743
-Original Grad: 0.000, -lr * Pred Grad:  -0.050, New P: -0.237
iter 3 loss: 0.072
Actual params: [-0.7434, -0.2374]
-Original Grad: -0.005, -lr * Pred Grad:  -0.074, New P: -0.817
-Original Grad: 0.000, -lr * Pred Grad:  -0.025, New P: -0.262
iter 4 loss: 0.072
Actual params: [-0.817 , -0.2621]
-Original Grad: -0.005, -lr * Pred Grad:  -0.067, New P: -0.884
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: -0.264
iter 5 loss: 0.071
Actual params: [-0.8841, -0.2642]
-Original Grad: -0.003, -lr * Pred Grad:  -0.061, New P: -0.945
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: -0.254
iter 6 loss: 0.071
Actual params: [-0.9451, -0.2537]
-Original Grad: -0.002, -lr * Pred Grad:  -0.056, New P: -1.001
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: -0.238
iter 7 loss: 0.071
Actual params: [-1.0007, -0.238 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.050, New P: -1.051
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: -0.215
iter 8 loss: 0.071
Actual params: [-1.0508, -0.2154]
-Original Grad: -0.001, -lr * Pred Grad:  -0.046, New P: -1.096
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: -0.185
iter 9 loss: 0.071
Actual params: [-1.0965, -0.1853]
-Original Grad: -0.001, -lr * Pred Grad:  -0.041, New P: -1.138
-Original Grad: 0.000, -lr * Pred Grad:  0.034, New P: -0.151
iter 10 loss: 0.071
Actual params: [-1.1379, -0.1514]
-Original Grad: -0.001, -lr * Pred Grad:  -0.038, New P: -1.176
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: -0.116
iter 11 loss: 0.071
Actual params: [-1.1755, -0.1162]
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -1.210
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: -0.081
iter 12 loss: 0.071
Actual params: [-1.2097, -0.0813]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.241
-Original Grad: 0.000, -lr * Pred Grad:  0.034, New P: -0.047
iter 13 loss: 0.071
Actual params: [-1.2407, -0.0469]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.269
-Original Grad: 0.000, -lr * Pred Grad:  0.033, New P: -0.014
iter 14 loss: 0.071
Actual params: [-1.269 , -0.0138]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.295
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: 0.018
iter 15 loss: 0.071
Actual params: [-1.2947,  0.0181]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.318
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: 0.048
iter 16 loss: 0.071
Actual params: [-1.3182,  0.0482]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.340
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 0.077
iter 17 loss: 0.071
Actual params: [-1.3396,  0.0766]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.359
-Original Grad: 0.000, -lr * Pred Grad:  0.027, New P: 0.103
iter 18 loss: 0.071
Actual params: [-1.3591,  0.1033]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.377
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.128
iter 19 loss: 0.071
Actual params: [-1.3769,  0.1283]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.393
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.152
iter 20 loss: 0.071
Actual params: [-1.3931,  0.1517]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.408
-Original Grad: 0.000, -lr * Pred Grad:  0.022, New P: 0.174
iter 21 loss: 0.071
Actual params: [-1.408 ,  0.1739]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.422
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.195
iter 22 loss: 0.071
Actual params: [-1.4215,  0.1947]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.434
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.214
iter 23 loss: 0.071
Actual params: [-1.4339,  0.2142]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.445
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.233
iter 24 loss: 0.071
Actual params: [-1.4452,  0.2327]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.456
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.250
iter 25 loss: 0.071
Actual params: [-1.4556,  0.2499]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.465
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.266
iter 26 loss: 0.071
Actual params: [-1.465,  0.266]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.474
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.281
iter 27 loss: 0.071
Actual params: [-1.4736,  0.281 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.482
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.295
iter 28 loss: 0.071
Actual params: [-1.4815,  0.2951]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.489
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.308
iter 29 loss: 0.071
Actual params: [-1.4887,  0.3082]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.495
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.320
iter 30 loss: 0.071
Actual params: [-1.4953,  0.3205]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.501
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.332
Target params: [1.3344, 1.5708]
iter 0 loss: 0.443
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.443
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.003, -lr * Pred Grad:  0.088, New P: -0.285
-Original Grad: 0.002, -lr * Pred Grad:  0.091, New P: 0.194
iter 2 loss: 0.442
Actual params: [-0.2845,  0.1944]
-Original Grad: 0.012, -lr * Pred Grad:  0.079, New P: -0.206
-Original Grad: 0.007, -lr * Pred Grad:  0.080, New P: 0.274
iter 3 loss: 0.439
Actual params: [-0.2056,  0.2741]
-Original Grad: 0.044, -lr * Pred Grad:  0.074, New P: -0.132
-Original Grad: 0.027, -lr * Pred Grad:  0.073, New P: 0.347
iter 4 loss: 0.434
Actual params: [-0.132 ,  0.3473]
-Original Grad: 0.069, -lr * Pred Grad:  0.080, New P: -0.052
-Original Grad: 0.044, -lr * Pred Grad:  0.079, New P: 0.427
iter 5 loss: 0.421
Actual params: [-0.0522,  0.4268]
-Original Grad: 0.194, -lr * Pred Grad:  0.075, New P: 0.023
-Original Grad: 0.123, -lr * Pred Grad:  0.075, New P: 0.501
iter 6 loss: 0.400
Actual params: [0.0227, 0.5015]
-Original Grad: 0.197, -lr * Pred Grad:  0.082, New P: 0.105
-Original Grad: 0.141, -lr * Pred Grad:  0.082, New P: 0.583
iter 7 loss: 0.373
Actual params: [0.1052, 0.5831]
-Original Grad: 0.159, -lr * Pred Grad:  0.088, New P: 0.193
-Original Grad: 0.115, -lr * Pred Grad:  0.087, New P: 0.670
iter 8 loss: 0.337
Actual params: [0.1927, 0.6703]
-Original Grad: 0.292, -lr * Pred Grad:  0.091, New P: 0.283
-Original Grad: 0.197, -lr * Pred Grad:  0.090, New P: 0.761
iter 9 loss: 0.295
Actual params: [0.2833, 0.7608]
-Original Grad: 0.387, -lr * Pred Grad:  0.093, New P: 0.376
-Original Grad: 0.228, -lr * Pred Grad:  0.094, New P: 0.854
iter 10 loss: 0.247
Actual params: [0.376 , 0.8543]
-Original Grad: 0.354, -lr * Pred Grad:  0.096, New P: 0.472
-Original Grad: 0.204, -lr * Pred Grad:  0.097, New P: 0.951
iter 11 loss: 0.195
Actual params: [0.472, 0.951]
-Original Grad: 0.349, -lr * Pred Grad:  0.099, New P: 0.571
-Original Grad: 0.223, -lr * Pred Grad:  0.099, New P: 1.050
iter 12 loss: 0.140
Actual params: [0.5708, 1.0504]
-Original Grad: 0.323, -lr * Pred Grad:  0.101, New P: 0.672
-Original Grad: 0.212, -lr * Pred Grad:  0.101, New P: 1.152
iter 13 loss: 0.087
Actual params: [0.6716, 1.1518]
-Original Grad: 0.299, -lr * Pred Grad:  0.102, New P: 0.774
-Original Grad: 0.269, -lr * Pred Grad:  0.104, New P: 1.255
iter 14 loss: 0.043
Actual params: [0.7736, 1.2553]
-Original Grad: 0.094, -lr * Pred Grad:  0.097, New P: 0.871
-Original Grad: 0.141, -lr * Pred Grad:  0.102, New P: 1.358
iter 15 loss: 0.018
Actual params: [0.8706, 1.3578]
-Original Grad: 0.052, -lr * Pred Grad:  0.091, New P: 0.961
-Original Grad: 0.083, -lr * Pred Grad:  0.099, New P: 1.456
iter 16 loss: 0.009
Actual params: [0.9612, 1.4564]
-Original Grad: -0.004, -lr * Pred Grad:  0.082, New P: 1.043
-Original Grad: 0.024, -lr * Pred Grad:  0.091, New P: 1.548
iter 17 loss: 0.007
Actual params: [1.0432, 1.5478]
-Original Grad: -0.014, -lr * Pred Grad:  0.074, New P: 1.117
-Original Grad: 0.007, -lr * Pred Grad:  0.084, New P: 1.631
iter 18 loss: 0.010
Actual params: [1.1169, 1.6314]
-Original Grad: 0.008, -lr * Pred Grad:  0.067, New P: 1.184
-Original Grad: -0.036, -lr * Pred Grad:  0.073, New P: 1.704
iter 19 loss: 0.014
Actual params: [1.1843, 1.7042]
-Original Grad: 0.014, -lr * Pred Grad:  0.062, New P: 1.246
-Original Grad: -0.057, -lr * Pred Grad:  0.061, New P: 1.766
iter 20 loss: 0.017
Actual params: [1.2464, 1.7655]
-Original Grad: 0.039, -lr * Pred Grad:  0.059, New P: 1.305
-Original Grad: -0.091, -lr * Pred Grad:  0.048, New P: 1.813
iter 21 loss: 0.019
Actual params: [1.3051, 1.8132]
-Original Grad: 0.036, -lr * Pred Grad:  0.056, New P: 1.361
-Original Grad: -0.062, -lr * Pred Grad:  0.038, New P: 1.851
iter 22 loss: 0.020
Actual params: [1.3606, 1.8512]
-Original Grad: 0.061, -lr * Pred Grad:  0.054, New P: 1.415
-Original Grad: -0.071, -lr * Pred Grad:  0.028, New P: 1.880
iter 23 loss: 0.019
Actual params: [1.4146, 1.8797]
-Original Grad: 0.035, -lr * Pred Grad:  0.051, New P: 1.466
-Original Grad: -0.053, -lr * Pred Grad:  0.021, New P: 1.901
iter 24 loss: 0.017
Actual params: [1.4659, 1.9011]
-Original Grad: 0.046, -lr * Pred Grad:  0.049, New P: 1.515
-Original Grad: -0.053, -lr * Pred Grad:  0.015, New P: 1.916
iter 25 loss: 0.016
Actual params: [1.5152, 1.9159]
-Original Grad: 0.034, -lr * Pred Grad:  0.047, New P: 1.562
-Original Grad: -0.066, -lr * Pred Grad:  0.008, New P: 1.924
iter 26 loss: 0.014
Actual params: [1.5622, 1.9238]
-Original Grad: 0.032, -lr * Pred Grad:  0.045, New P: 1.607
-Original Grad: -0.053, -lr * Pred Grad:  0.003, New P: 1.926
iter 27 loss: 0.013
Actual params: [1.607 , 1.9264]
-Original Grad: 0.023, -lr * Pred Grad:  0.042, New P: 1.649
-Original Grad: -0.023, -lr * Pred Grad:  0.000, New P: 1.927
iter 28 loss: 0.011
Actual params: [1.6492, 1.9268]
-Original Grad: 0.022, -lr * Pred Grad:  0.040, New P: 1.689
-Original Grad: -0.031, -lr * Pred Grad:  -0.002, New P: 1.924
iter 29 loss: 0.010
Actual params: [1.689 , 1.9244]
-Original Grad: 0.020, -lr * Pred Grad:  0.038, New P: 1.727
-Original Grad: -0.021, -lr * Pred Grad:  -0.004, New P: 1.920
iter 30 loss: 0.009
Actual params: [1.7266, 1.9203]
-Original Grad: 0.024, -lr * Pred Grad:  0.036, New P: 1.762
-Original Grad: -0.025, -lr * Pred Grad:  -0.006, New P: 1.914
Target params: [1.3344, 1.5708]
iter 0 loss: 0.725
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.016, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.019, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.720
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.029, -lr * Pred Grad:  0.097, New P: -0.275
-Original Grad: 0.034, -lr * Pred Grad:  0.098, New P: 0.201
iter 2 loss: 0.710
Actual params: [-0.2752,  0.2011]
-Original Grad: 0.051, -lr * Pred Grad:  0.094, New P: -0.181
-Original Grad: 0.060, -lr * Pred Grad:  0.095, New P: 0.296
iter 3 loss: 0.697
Actual params: [-0.1808,  0.2959]
-Original Grad: 0.077, -lr * Pred Grad:  0.094, New P: -0.087
-Original Grad: 0.080, -lr * Pred Grad:  0.095, New P: 0.391
iter 4 loss: 0.682
Actual params: [-0.0871,  0.391 ]
-Original Grad: 0.104, -lr * Pred Grad:  0.094, New P: 0.007
-Original Grad: 0.098, -lr * Pred Grad:  0.096, New P: 0.487
iter 5 loss: 0.661
Actual params: [0.0069, 0.4872]
-Original Grad: 0.124, -lr * Pred Grad:  0.095, New P: 0.102
-Original Grad: 0.109, -lr * Pred Grad:  0.098, New P: 0.585
iter 6 loss: 0.634
Actual params: [0.1023, 0.5848]
-Original Grad: 0.191, -lr * Pred Grad:  0.095, New P: 0.197
-Original Grad: 0.146, -lr * Pred Grad:  0.098, New P: 0.683
iter 7 loss: 0.596
Actual params: [0.1969, 0.6829]
-Original Grad: 0.261, -lr * Pred Grad:  0.094, New P: 0.291
-Original Grad: 0.177, -lr * Pred Grad:  0.099, New P: 0.782
iter 8 loss: 0.541
Actual params: [0.2912, 0.7818]
-Original Grad: 0.470, -lr * Pred Grad:  0.090, New P: 0.382
-Original Grad: 0.262, -lr * Pred Grad:  0.098, New P: 0.880
iter 9 loss: 0.477
Actual params: [0.3816, 0.8797]
-Original Grad: 0.433, -lr * Pred Grad:  0.094, New P: 0.475
-Original Grad: 0.217, -lr * Pred Grad:  0.100, New P: 0.980
iter 10 loss: 0.415
Actual params: [0.4754, 0.9797]
-Original Grad: 0.436, -lr * Pred Grad:  0.097, New P: 0.572
-Original Grad: 0.173, -lr * Pred Grad:  0.101, New P: 1.081
iter 11 loss: 0.354
Actual params: [0.5723, 1.0806]
-Original Grad: 0.532, -lr * Pred Grad:  0.099, New P: 0.672
-Original Grad: 0.172, -lr * Pred Grad:  0.102, New P: 1.182
iter 12 loss: 0.296
Actual params: [0.6717, 1.1823]
-Original Grad: 0.483, -lr * Pred Grad:  0.102, New P: 0.773
-Original Grad: 0.110, -lr * Pred Grad:  0.100, New P: 1.282
iter 13 loss: 0.243
Actual params: [0.7733, 1.282 ]
-Original Grad: 0.534, -lr * Pred Grad:  0.104, New P: 0.877
-Original Grad: 0.115, -lr * Pred Grad:  0.098, New P: 1.380
iter 14 loss: 0.187
Actual params: [0.877 , 1.3805]
-Original Grad: 0.400, -lr * Pred Grad:  0.104, New P: 0.981
-Original Grad: 0.172, -lr * Pred Grad:  0.100, New P: 1.480
iter 15 loss: 0.140
Actual params: [0.9813, 1.4802]
-Original Grad: 0.337, -lr * Pred Grad:  0.104, New P: 1.085
-Original Grad: 0.174, -lr * Pred Grad:  0.101, New P: 1.581
iter 16 loss: 0.106
Actual params: [1.085, 1.581]
-Original Grad: 0.162, -lr * Pred Grad:  0.099, New P: 1.184
-Original Grad: 0.150, -lr * Pred Grad:  0.101, New P: 1.682
iter 17 loss: 0.082
Actual params: [1.1843, 1.6818]
-Original Grad: -0.074, -lr * Pred Grad:  0.087, New P: 1.272
-Original Grad: 0.194, -lr * Pred Grad:  0.102, New P: 1.784
iter 18 loss: 0.067
Actual params: [1.2717, 1.7841]
-Original Grad: -0.081, -lr * Pred Grad:  0.076, New P: 1.348
-Original Grad: 0.185, -lr * Pred Grad:  0.103, New P: 1.888
iter 19 loss: 0.055
Actual params: [1.3479, 1.8875]
-Original Grad: -0.042, -lr * Pred Grad:  0.068, New P: 1.416
-Original Grad: 0.074, -lr * Pred Grad:  0.099, New P: 1.987
iter 20 loss: 0.048
Actual params: [1.4157, 1.9866]
-Original Grad: -0.143, -lr * Pred Grad:  0.056, New P: 1.472
-Original Grad: 0.143, -lr * Pred Grad:  0.099, New P: 2.085
iter 21 loss: 0.042
Actual params: [1.4716, 2.0854]
-Original Grad: -0.315, -lr * Pred Grad:  0.038, New P: 1.509
-Original Grad: 0.220, -lr * Pred Grad:  0.102, New P: 2.187
iter 22 loss: 0.037
Actual params: [1.5095, 2.187 ]
-Original Grad: -0.235, -lr * Pred Grad:  0.025, New P: 1.535
-Original Grad: 0.165, -lr * Pred Grad:  0.102, New P: 2.289
iter 23 loss: 0.034
Actual params: [1.5348, 2.2889]
-Original Grad: -0.359, -lr * Pred Grad:  0.009, New P: 1.544
-Original Grad: 0.124, -lr * Pred Grad:  0.100, New P: 2.389
iter 24 loss: 0.036
Actual params: [1.5442, 2.3893]
-Original Grad: -0.141, -lr * Pred Grad:  0.003, New P: 1.548
-Original Grad: -0.005, -lr * Pred Grad:  0.091, New P: 2.480
iter 25 loss: 0.054
Actual params: [1.5476, 2.4803]
-Original Grad: -0.030, -lr * Pred Grad:  0.002, New P: 1.550
-Original Grad: -0.378, -lr * Pred Grad:  0.049, New P: 2.530
iter 26 loss: 0.076
Actual params: [1.5496, 2.5297]
-Original Grad: 0.282, -lr * Pred Grad:  0.012, New P: 1.562
-Original Grad: -0.536, -lr * Pred Grad:  0.008, New P: 2.538
iter 27 loss: 0.081
Actual params: [1.5617, 2.5378]
-Original Grad: 0.133, -lr * Pred Grad:  0.016, New P: 1.578
-Original Grad: -0.636, -lr * Pred Grad:  -0.024, New P: 2.514
iter 28 loss: 0.071
Actual params: [1.5776, 2.5141]
-Original Grad: -0.331, -lr * Pred Grad:  0.002, New P: 1.580
-Original Grad: -0.437, -lr * Pred Grad:  -0.040, New P: 2.474
iter 29 loss: 0.059
Actual params: [1.5796, 2.4742]
-Original Grad: -0.603, -lr * Pred Grad:  -0.019, New P: 1.561
-Original Grad: -0.196, -lr * Pred Grad:  -0.045, New P: 2.430
iter 30 loss: 0.044
Actual params: [1.5605, 2.4296]
-Original Grad: -0.271, -lr * Pred Grad:  -0.026, New P: 1.534
-Original Grad: -0.131, -lr * Pred Grad:  -0.046, New P: 2.383
Target params: [1.3344, 1.5708]
iter 0 loss: 0.228
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.228
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.006, -lr * Pred Grad:  0.086, New P: -0.286
-Original Grad: 0.002, -lr * Pred Grad:  0.085, New P: 0.189
iter 2 loss: 0.227
Actual params: [-0.2864,  0.189 ]
-Original Grad: 0.015, -lr * Pred Grad:  0.084, New P: -0.203
-Original Grad: 0.004, -lr * Pred Grad:  0.083, New P: 0.272
iter 3 loss: 0.224
Actual params: [-0.2025,  0.2723]
-Original Grad: 0.028, -lr * Pred Grad:  0.085, New P: -0.118
-Original Grad: 0.009, -lr * Pred Grad:  0.083, New P: 0.356
iter 4 loss: 0.217
Actual params: [-0.1176,  0.3557]
-Original Grad: 0.092, -lr * Pred Grad:  0.075, New P: -0.043
-Original Grad: 0.031, -lr * Pred Grad:  0.073, New P: 0.429
iter 5 loss: 0.203
Actual params: [-0.0425,  0.4291]
-Original Grad: 0.206, -lr * Pred Grad:  0.075, New P: 0.032
-Original Grad: 0.076, -lr * Pred Grad:  0.073, New P: 0.502
iter 6 loss: 0.174
Actual params: [0.0324, 0.5021]
-Original Grad: 0.275, -lr * Pred Grad:  0.081, New P: 0.113
-Original Grad: 0.095, -lr * Pred Grad:  0.080, New P: 0.582
iter 7 loss: 0.138
Actual params: [0.1131, 0.5822]
-Original Grad: 0.324, -lr * Pred Grad:  0.086, New P: 0.199
-Original Grad: 0.090, -lr * Pred Grad:  0.086, New P: 0.668
iter 8 loss: 0.104
Actual params: [0.1991, 0.6684]
-Original Grad: 0.316, -lr * Pred Grad:  0.091, New P: 0.290
-Original Grad: 0.067, -lr * Pred Grad:  0.090, New P: 0.758
iter 9 loss: 0.077
Actual params: [0.2898, 0.7581]
-Original Grad: 0.184, -lr * Pred Grad:  0.092, New P: 0.382
-Original Grad: 0.010, -lr * Pred Grad:  0.083, New P: 0.841
iter 10 loss: 0.062
Actual params: [0.3816, 0.8408]
-Original Grad: 0.085, -lr * Pred Grad:  0.088, New P: 0.470
-Original Grad: 0.031, -lr * Pred Grad:  0.082, New P: 0.922
iter 11 loss: 0.054
Actual params: [0.4699, 0.9225]
-Original Grad: 0.002, -lr * Pred Grad:  0.079, New P: 0.549
-Original Grad: 0.056, -lr * Pred Grad:  0.085, New P: 1.007
iter 12 loss: 0.048
Actual params: [0.5493, 1.0072]
-Original Grad: -0.031, -lr * Pred Grad:  0.069, New P: 0.618
-Original Grad: 0.075, -lr * Pred Grad:  0.089, New P: 1.096
iter 13 loss: 0.043
Actual params: [0.6185, 1.0962]
-Original Grad: 0.024, -lr * Pred Grad:  0.064, New P: 0.683
-Original Grad: 0.037, -lr * Pred Grad:  0.088, New P: 1.184
iter 14 loss: 0.036
Actual params: [0.6828, 1.1843]
-Original Grad: 0.045, -lr * Pred Grad:  0.062, New P: 0.745
-Original Grad: 0.053, -lr * Pred Grad:  0.090, New P: 1.274
iter 15 loss: 0.028
Actual params: [0.7445, 1.274 ]
-Original Grad: 0.017, -lr * Pred Grad:  0.057, New P: 0.802
-Original Grad: 0.057, -lr * Pred Grad:  0.091, New P: 1.365
iter 16 loss: 0.023
Actual params: [0.8017, 1.3654]
-Original Grad: 0.021, -lr * Pred Grad:  0.054, New P: 0.855
-Original Grad: 0.055, -lr * Pred Grad:  0.093, New P: 1.458
iter 17 loss: 0.019
Actual params: [0.8553, 1.458 ]
-Original Grad: 0.034, -lr * Pred Grad:  0.051, New P: 0.907
-Original Grad: 0.007, -lr * Pred Grad:  0.086, New P: 1.544
iter 18 loss: 0.017
Actual params: [0.9066, 1.5435]
-Original Grad: 0.033, -lr * Pred Grad:  0.049, New P: 0.956
-Original Grad: -0.027, -lr * Pred Grad:  0.071, New P: 1.615
iter 19 loss: 0.017
Actual params: [0.9559, 1.6146]
-Original Grad: 0.002, -lr * Pred Grad:  0.045, New P: 1.001
-Original Grad: -0.035, -lr * Pred Grad:  0.056, New P: 1.671
iter 20 loss: 0.019
Actual params: [1.0009, 1.6705]
-Original Grad: 0.019, -lr * Pred Grad:  0.042, New P: 1.043
-Original Grad: -0.077, -lr * Pred Grad:  0.032, New P: 1.702
iter 21 loss: 0.021
Actual params: [1.0433, 1.702 ]
-Original Grad: 0.007, -lr * Pred Grad:  0.039, New P: 1.083
-Original Grad: -0.075, -lr * Pred Grad:  0.012, New P: 1.714
iter 22 loss: 0.021
Actual params: [1.0826, 1.7137]
-Original Grad: -0.007, -lr * Pred Grad:  0.035, New P: 1.118
-Original Grad: -0.078, -lr * Pred Grad:  -0.006, New P: 1.708
iter 23 loss: 0.021
Actual params: [1.1177, 1.7081]
-Original Grad: -0.016, -lr * Pred Grad:  0.031, New P: 1.148
-Original Grad: -0.088, -lr * Pred Grad:  -0.022, New P: 1.686
iter 24 loss: 0.020
Actual params: [1.1483, 1.6863]
-Original Grad: -0.017, -lr * Pred Grad:  0.026, New P: 1.175
-Original Grad: -0.073, -lr * Pred Grad:  -0.033, New P: 1.653
iter 25 loss: 0.018
Actual params: [1.1747, 1.6535]
-Original Grad: -0.027, -lr * Pred Grad:  0.022, New P: 1.196
-Original Grad: -0.066, -lr * Pred Grad:  -0.041, New P: 1.612
iter 26 loss: 0.017
Actual params: [1.1964, 1.6121]
-Original Grad: -0.021, -lr * Pred Grad:  0.018, New P: 1.214
-Original Grad: -0.046, -lr * Pred Grad:  -0.046, New P: 1.566
iter 27 loss: 0.015
Actual params: [1.2142, 1.5664]
-Original Grad: -0.023, -lr * Pred Grad:  0.014, New P: 1.228
-Original Grad: -0.037, -lr * Pred Grad:  -0.048, New P: 1.518
iter 28 loss: 0.014
Actual params: [1.2285, 1.5181]
-Original Grad: -0.014, -lr * Pred Grad:  0.012, New P: 1.240
-Original Grad: -0.012, -lr * Pred Grad:  -0.046, New P: 1.472
iter 29 loss: 0.014
Actual params: [1.2401, 1.4718]
-Original Grad: -0.022, -lr * Pred Grad:  0.009, New P: 1.249
-Original Grad: -0.012, -lr * Pred Grad:  -0.044, New P: 1.427
iter 30 loss: 0.013
Actual params: [1.2487, 1.4274]
-Original Grad: -0.020, -lr * Pred Grad:  0.006, New P: 1.255
-Original Grad: -0.014, -lr * Pred Grad:  -0.043, New P: 1.384
Target params: [1.3344, 1.5708]
iter 0 loss: 0.466
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.466
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.087, New P: -0.660
-Original Grad: 0.000, -lr * Pred Grad:  0.093, New P: 0.197
iter 2 loss: 0.466
Actual params: [-0.6596,  0.1966]
-Original Grad: -0.000, -lr * Pred Grad:  -0.073, New P: -0.733
-Original Grad: 0.000, -lr * Pred Grad:  0.086, New P: 0.282
iter 3 loss: 0.466
Actual params: [-0.7328,  0.2823]
-Original Grad: -0.000, -lr * Pred Grad:  -0.061, New P: -0.793
-Original Grad: 0.000, -lr * Pred Grad:  0.077, New P: 0.359
iter 4 loss: 0.466
Actual params: [-0.7934,  0.3593]
-Original Grad: 0.000, -lr * Pred Grad:  -0.050, New P: -0.844
-Original Grad: 0.000, -lr * Pred Grad:  0.071, New P: 0.431
iter 5 loss: 0.466
Actual params: [-0.8438,  0.4307]
-Original Grad: 0.000, -lr * Pred Grad:  -0.042, New P: -0.885
-Original Grad: 0.000, -lr * Pred Grad:  0.070, New P: 0.501
iter 6 loss: 0.466
Actual params: [-0.8854,  0.5008]
-Original Grad: 0.000, -lr * Pred Grad:  -0.034, New P: -0.920
-Original Grad: 0.000, -lr * Pred Grad:  0.069, New P: 0.570
iter 7 loss: 0.466
Actual params: [-0.9197,  0.5696]
-Original Grad: 0.000, -lr * Pred Grad:  -0.026, New P: -0.945
-Original Grad: 0.000, -lr * Pred Grad:  0.073, New P: 0.643
iter 8 loss: 0.466
Actual params: [-0.9454,  0.643 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -0.962
-Original Grad: 0.000, -lr * Pred Grad:  0.079, New P: 0.723
iter 9 loss: 0.466
Actual params: [-0.9621,  0.7225]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -0.966
-Original Grad: 0.000, -lr * Pred Grad:  0.086, New P: 0.808
iter 10 loss: 0.466
Actual params: [-0.9656,  0.8084]
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: -0.951
-Original Grad: 0.001, -lr * Pred Grad:  0.088, New P: 0.897
iter 11 loss: 0.466
Actual params: [-0.9507,  0.8965]
-Original Grad: 0.001, -lr * Pred Grad:  0.040, New P: -0.911
-Original Grad: 0.001, -lr * Pred Grad:  0.086, New P: 0.983
iter 12 loss: 0.466
Actual params: [-0.9106,  0.9826]
-Original Grad: 0.002, -lr * Pred Grad:  0.060, New P: -0.851
-Original Grad: 0.002, -lr * Pred Grad:  0.077, New P: 1.060
iter 13 loss: 0.465
Actual params: [-0.8508,  1.0596]
-Original Grad: 0.006, -lr * Pred Grad:  0.066, New P: -0.785
-Original Grad: 0.007, -lr * Pred Grad:  0.072, New P: 1.131
iter 14 loss: 0.464
Actual params: [-0.7852,  1.1312]
-Original Grad: 0.009, -lr * Pred Grad:  0.074, New P: -0.711
-Original Grad: 0.009, -lr * Pred Grad:  0.078, New P: 1.209
iter 15 loss: 0.461
Actual params: [-0.7112,  1.209 ]
-Original Grad: 0.026, -lr * Pred Grad:  0.072, New P: -0.640
-Original Grad: 0.025, -lr * Pred Grad:  0.074, New P: 1.283
iter 16 loss: 0.455
Actual params: [-0.6395,  1.2833]
-Original Grad: 0.072, -lr * Pred Grad:  0.069, New P: -0.570
-Original Grad: 0.066, -lr * Pred Grad:  0.072, New P: 1.355
iter 17 loss: 0.443
Actual params: [-0.57  ,  1.3549]
-Original Grad: 0.117, -lr * Pred Grad:  0.076, New P: -0.494
-Original Grad: 0.104, -lr * Pred Grad:  0.078, New P: 1.433
iter 18 loss: 0.420
Actual params: [-0.4938,  1.4326]
-Original Grad: 0.204, -lr * Pred Grad:  0.081, New P: -0.413
-Original Grad: 0.155, -lr * Pred Grad:  0.083, New P: 1.516
iter 19 loss: 0.390
Actual params: [-0.4132,  1.5161]
-Original Grad: 0.213, -lr * Pred Grad:  0.089, New P: -0.325
-Original Grad: 0.129, -lr * Pred Grad:  0.091, New P: 1.607
iter 20 loss: 0.359
Actual params: [-0.3246,  1.6074]
-Original Grad: 0.257, -lr * Pred Grad:  0.095, New P: -0.230
-Original Grad: 0.106, -lr * Pred Grad:  0.097, New P: 1.704
iter 21 loss: 0.326
Actual params: [-0.2296,  1.7041]
-Original Grad: 0.283, -lr * Pred Grad:  0.101, New P: -0.129
-Original Grad: 0.066, -lr * Pred Grad:  0.098, New P: 1.802
iter 22 loss: 0.292
Actual params: [-0.1289,  1.8022]
-Original Grad: 0.349, -lr * Pred Grad:  0.105, New P: -0.024
-Original Grad: -0.008, -lr * Pred Grad:  0.088, New P: 1.890
iter 23 loss: 0.259
Actual params: [-0.0239,  1.8899]
-Original Grad: 0.378, -lr * Pred Grad:  0.109, New P: 0.085
-Original Grad: -0.044, -lr * Pred Grad:  0.070, New P: 1.960
iter 24 loss: 0.225
Actual params: [0.0854, 1.9603]
-Original Grad: 0.381, -lr * Pred Grad:  0.113, New P: 0.198
-Original Grad: -0.099, -lr * Pred Grad:  0.042, New P: 2.002
iter 25 loss: 0.189
Actual params: [0.1984, 2.0022]
-Original Grad: 0.443, -lr * Pred Grad:  0.116, New P: 0.315
-Original Grad: -0.104, -lr * Pred Grad:  0.018, New P: 2.020
iter 26 loss: 0.153
Actual params: [0.3147, 2.0199]
-Original Grad: 0.269, -lr * Pred Grad:  0.117, New P: 0.432
-Original Grad: -0.142, -lr * Pred Grad:  -0.008, New P: 2.012
iter 27 loss: 0.120
Actual params: [0.4318, 2.0116]
-Original Grad: 0.209, -lr * Pred Grad:  0.116, New P: 0.548
-Original Grad: -0.144, -lr * Pred Grad:  -0.029, New P: 1.983
iter 28 loss: 0.090
Actual params: [0.5478, 1.9829]
-Original Grad: 0.137, -lr * Pred Grad:  0.112, New P: 0.660
-Original Grad: -0.127, -lr * Pred Grad:  -0.043, New P: 1.940
iter 29 loss: 0.063
Actual params: [0.6602, 1.9398]
-Original Grad: 0.187, -lr * Pred Grad:  0.111, New P: 0.771
-Original Grad: -0.181, -lr * Pred Grad:  -0.060, New P: 1.880
iter 30 loss: 0.042
Actual params: [0.7713, 1.8803]
-Original Grad: 0.056, -lr * Pred Grad:  0.104, New P: 0.876
-Original Grad: -0.096, -lr * Pred Grad:  -0.066, New P: 1.815
Target params: [1.3344, 1.5708]
iter 0 loss: 0.228
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.228
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.006, -lr * Pred Grad:  0.092, New P: -0.281
-Original Grad: 0.002, -lr * Pred Grad:  0.091, New P: 0.194
iter 2 loss: 0.227
Actual params: [-0.2807,  0.1942]
-Original Grad: 0.016, -lr * Pred Grad:  0.085, New P: -0.196
-Original Grad: 0.005, -lr * Pred Grad:  0.084, New P: 0.278
iter 3 loss: 0.224
Actual params: [-0.196 ,  0.2784]
-Original Grad: 0.046, -lr * Pred Grad:  0.079, New P: -0.117
-Original Grad: 0.014, -lr * Pred Grad:  0.078, New P: 0.356
iter 4 loss: 0.217
Actual params: [-0.1173,  0.356 ]
-Original Grad: 0.091, -lr * Pred Grad:  0.079, New P: -0.038
-Original Grad: 0.031, -lr * Pred Grad:  0.078, New P: 0.434
iter 5 loss: 0.201
Actual params: [-0.0378,  0.4336]
-Original Grad: 0.280, -lr * Pred Grad:  0.073, New P: 0.035
-Original Grad: 0.100, -lr * Pred Grad:  0.071, New P: 0.505
iter 6 loss: 0.173
Actual params: [0.0349, 0.5047]
-Original Grad: 0.429, -lr * Pred Grad:  0.078, New P: 0.113
-Original Grad: 0.152, -lr * Pred Grad:  0.077, New P: 0.582
iter 7 loss: 0.138
Actual params: [0.1128, 0.5818]
-Original Grad: 0.283, -lr * Pred Grad:  0.084, New P: 0.197
-Original Grad: 0.078, -lr * Pred Grad:  0.082, New P: 0.664
iter 8 loss: 0.105
Actual params: [0.1967, 0.6637]
-Original Grad: 0.234, -lr * Pred Grad:  0.087, New P: 0.284
-Original Grad: 0.041, -lr * Pred Grad:  0.081, New P: 0.745
iter 9 loss: 0.079
Actual params: [0.284 , 0.7448]
-Original Grad: 0.209, -lr * Pred Grad:  0.089, New P: 0.373
-Original Grad: 0.019, -lr * Pred Grad:  0.076, New P: 0.821
iter 10 loss: 0.063
Actual params: [0.373 , 0.8212]
-Original Grad: 0.087, -lr * Pred Grad:  0.085, New P: 0.458
-Original Grad: 0.030, -lr * Pred Grad:  0.075, New P: 0.896
iter 11 loss: 0.055
Actual params: [0.4584, 0.896 ]
-Original Grad: 0.035, -lr * Pred Grad:  0.079, New P: 0.538
-Original Grad: 0.043, -lr * Pred Grad:  0.076, New P: 0.972
iter 12 loss: 0.050
Actual params: [0.5375, 0.9715]
-Original Grad: -0.053, -lr * Pred Grad:  0.067, New P: 0.605
-Original Grad: 0.081, -lr * Pred Grad:  0.081, New P: 1.052
iter 13 loss: 0.046
Actual params: [0.6049, 1.0523]
-Original Grad: -0.069, -lr * Pred Grad:  0.056, New P: 0.661
-Original Grad: 0.109, -lr * Pred Grad:  0.087, New P: 1.139
iter 14 loss: 0.039
Actual params: [0.6606, 1.1391]
-Original Grad: -0.015, -lr * Pred Grad:  0.049, New P: 0.710
-Original Grad: 0.078, -lr * Pred Grad:  0.089, New P: 1.228
iter 15 loss: 0.032
Actual params: [0.7098, 1.2284]
-Original Grad: 0.068, -lr * Pred Grad:  0.049, New P: 0.759
-Original Grad: 0.055, -lr * Pred Grad:  0.089, New P: 1.318
iter 16 loss: 0.026
Actual params: [0.7591, 1.3177]
-Original Grad: 0.029, -lr * Pred Grad:  0.047, New P: 0.806
-Original Grad: 0.042, -lr * Pred Grad:  0.088, New P: 1.405
iter 17 loss: 0.022
Actual params: [0.8058, 1.4053]
-Original Grad: 0.031, -lr * Pred Grad:  0.045, New P: 0.850
-Original Grad: 0.028, -lr * Pred Grad:  0.084, New P: 1.490
iter 18 loss: 0.019
Actual params: [0.8504, 1.4896]
-Original Grad: 0.035, -lr * Pred Grad:  0.043, New P: 0.893
-Original Grad: -0.003, -lr * Pred Grad:  0.076, New P: 1.566
iter 19 loss: 0.018
Actual params: [0.8935, 1.5656]
-Original Grad: 0.027, -lr * Pred Grad:  0.041, New P: 0.935
-Original Grad: -0.021, -lr * Pred Grad:  0.065, New P: 1.631
iter 20 loss: 0.019
Actual params: [0.9346, 1.6306]
-Original Grad: 0.014, -lr * Pred Grad:  0.038, New P: 0.973
-Original Grad: -0.036, -lr * Pred Grad:  0.052, New P: 1.683
iter 21 loss: 0.020
Actual params: [0.9731, 1.6825]
-Original Grad: 0.026, -lr * Pred Grad:  0.037, New P: 1.010
-Original Grad: -0.053, -lr * Pred Grad:  0.037, New P: 1.719
iter 22 loss: 0.022
Actual params: [1.0101, 1.7192]
-Original Grad: 0.005, -lr * Pred Grad:  0.034, New P: 1.044
-Original Grad: -0.081, -lr * Pred Grad:  0.017, New P: 1.737
iter 23 loss: 0.023
Actual params: [1.0442, 1.7366]
-Original Grad: -0.014, -lr * Pred Grad:  0.030, New P: 1.074
-Original Grad: -0.118, -lr * Pred Grad:  -0.005, New P: 1.732
iter 24 loss: 0.023
Actual params: [1.0741, 1.7315]
-Original Grad: 0.003, -lr * Pred Grad:  0.028, New P: 1.102
-Original Grad: -0.101, -lr * Pred Grad:  -0.021, New P: 1.711
iter 25 loss: 0.021
Actual params: [1.1017, 1.7107]
-Original Grad: -0.008, -lr * Pred Grad:  0.024, New P: 1.126
-Original Grad: -0.071, -lr * Pred Grad:  -0.030, New P: 1.681
iter 26 loss: 0.019
Actual params: [1.1261, 1.6808]
-Original Grad: -0.014, -lr * Pred Grad:  0.021, New P: 1.147
-Original Grad: -0.064, -lr * Pred Grad:  -0.037, New P: 1.644
iter 27 loss: 0.017
Actual params: [1.1473, 1.6438]
-Original Grad: -0.016, -lr * Pred Grad:  0.018, New P: 1.165
-Original Grad: -0.052, -lr * Pred Grad:  -0.042, New P: 1.602
iter 28 loss: 0.016
Actual params: [1.1652, 1.6022]
-Original Grad: -0.017, -lr * Pred Grad:  0.015, New P: 1.180
-Original Grad: -0.054, -lr * Pred Grad:  -0.046, New P: 1.556
iter 29 loss: 0.014
Actual params: [1.1801, 1.5561]
-Original Grad: -0.015, -lr * Pred Grad:  0.012, New P: 1.192
-Original Grad: -0.027, -lr * Pred Grad:  -0.046, New P: 1.510
iter 30 loss: 0.013
Actual params: [1.1924, 1.5099]
-Original Grad: -0.011, -lr * Pred Grad:  0.010, New P: 1.203
-Original Grad: -0.010, -lr * Pred Grad:  -0.044, New P: 1.466
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.054, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.006, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.107
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.063, -lr * Pred Grad:  0.100, New P: -0.272
-Original Grad: 0.001, -lr * Pred Grad:  0.074, New P: 0.177
iter 2 loss: 0.096
Actual params: [-0.2722,  0.1773]
-Original Grad: 0.062, -lr * Pred Grad:  0.100, New P: -0.172
-Original Grad: -0.003, -lr * Pred Grad:  0.017, New P: 0.195
iter 3 loss: 0.082
Actual params: [-0.172 ,  0.1947]
-Original Grad: 0.110, -lr * Pred Grad:  0.099, New P: -0.073
-Original Grad: -0.005, -lr * Pred Grad:  -0.022, New P: 0.173
iter 4 loss: 0.069
Actual params: [-0.0733,  0.1731]
-Original Grad: 0.076, -lr * Pred Grad:  0.099, New P: 0.026
-Original Grad: -0.003, -lr * Pred Grad:  -0.035, New P: 0.138
iter 5 loss: 0.055
Actual params: [0.0257, 0.1385]
-Original Grad: 0.092, -lr * Pred Grad:  0.100, New P: 0.126
-Original Grad: 0.012, -lr * Pred Grad:  0.024, New P: 0.162
iter 6 loss: 0.042
Actual params: [0.1256, 0.1623]
-Original Grad: 0.090, -lr * Pred Grad:  0.101, New P: 0.226
-Original Grad: 0.004, -lr * Pred Grad:  0.034, New P: 0.197
iter 7 loss: 0.033
Actual params: [0.2261, 0.1968]
-Original Grad: 0.041, -lr * Pred Grad:  0.096, New P: 0.322
-Original Grad: 0.009, -lr * Pred Grad:  0.052, New P: 0.248
iter 8 loss: 0.038
Actual params: [0.3225, 0.2485]
-Original Grad: -0.123, -lr * Pred Grad:  0.050, New P: 0.373
-Original Grad: -0.049, -lr * Pred Grad:  -0.030, New P: 0.218
iter 9 loss: 0.044
Actual params: [0.3727, 0.2182]
-Original Grad: -0.036, -lr * Pred Grad:  0.037, New P: 0.410
-Original Grad: -0.021, -lr * Pred Grad:  -0.043, New P: 0.175
iter 10 loss: 0.046
Actual params: [0.41  , 0.1748]
-Original Grad: -0.020, -lr * Pred Grad:  0.029, New P: 0.439
-Original Grad: -0.004, -lr * Pred Grad:  -0.042, New P: 0.133
iter 11 loss: 0.048
Actual params: [0.4394, 0.1326]
-Original Grad: -0.153, -lr * Pred Grad:  -0.002, New P: 0.437
-Original Grad: -0.106, -lr * Pred Grad:  -0.060, New P: 0.072
iter 12 loss: 0.044
Actual params: [0.4369, 0.0722]
-Original Grad: -0.126, -lr * Pred Grad:  -0.021, New P: 0.416
-Original Grad: -0.027, -lr * Pred Grad:  -0.064, New P: 0.008
iter 13 loss: 0.040
Actual params: [0.4159, 0.0085]
-Original Grad: -0.057, -lr * Pred Grad:  -0.027, New P: 0.389
-Original Grad: 0.001, -lr * Pred Grad:  -0.057, New P: -0.049
iter 14 loss: 0.039
Actual params: [ 0.3887, -0.0485]
-Original Grad: 0.011, -lr * Pred Grad:  -0.023, New P: 0.366
-Original Grad: 0.027, -lr * Pred Grad:  -0.040, New P: -0.088
iter 15 loss: 0.039
Actual params: [ 0.3657, -0.0885]
-Original Grad: -0.030, -lr * Pred Grad:  -0.025, New P: 0.341
-Original Grad: 0.005, -lr * Pred Grad:  -0.034, New P: -0.123
iter 16 loss: 0.039
Actual params: [ 0.3405, -0.1227]
-Original Grad: -0.010, -lr * Pred Grad:  -0.024, New P: 0.316
-Original Grad: 0.015, -lr * Pred Grad:  -0.025, New P: -0.148
iter 17 loss: 0.040
Actual params: [ 0.3162, -0.1476]
-Original Grad: -0.007, -lr * Pred Grad:  -0.023, New P: 0.293
-Original Grad: 0.017, -lr * Pred Grad:  -0.016, New P: -0.164
iter 18 loss: 0.041
Actual params: [ 0.2931, -0.1636]
-Original Grad: 0.002, -lr * Pred Grad:  -0.021, New P: 0.272
-Original Grad: 0.016, -lr * Pred Grad:  -0.008, New P: -0.172
iter 19 loss: 0.042
Actual params: [ 0.2724, -0.1715]
-Original Grad: 0.056, -lr * Pred Grad:  -0.010, New P: 0.262
-Original Grad: 0.054, -lr * Pred Grad:  0.013, New P: -0.159
iter 20 loss: 0.042
Actual params: [ 0.2625, -0.1585]
-Original Grad: 0.039, -lr * Pred Grad:  -0.003, New P: 0.260
-Original Grad: 0.038, -lr * Pred Grad:  0.025, New P: -0.134
iter 21 loss: 0.041
Actual params: [ 0.2596, -0.1336]
-Original Grad: 0.063, -lr * Pred Grad:  0.007, New P: 0.267
-Original Grad: 0.034, -lr * Pred Grad:  0.034, New P: -0.100
iter 22 loss: 0.040
Actual params: [ 0.2666, -0.0998]
-Original Grad: 0.034, -lr * Pred Grad:  0.012, New P: 0.278
-Original Grad: 0.029, -lr * Pred Grad:  0.040, New P: -0.059
iter 23 loss: 0.038
Actual params: [ 0.2782, -0.0594]
-Original Grad: 0.047, -lr * Pred Grad:  0.018, New P: 0.296
-Original Grad: 0.022, -lr * Pred Grad:  0.044, New P: -0.015
iter 24 loss: 0.036
Actual params: [ 0.2959, -0.0153]
-Original Grad: 0.022, -lr * Pred Grad:  0.019, New P: 0.315
-Original Grad: 0.026, -lr * Pred Grad:  0.049, New P: 0.033
iter 25 loss: 0.035
Actual params: [0.3153, 0.0332]
-Original Grad: 0.050, -lr * Pred Grad:  0.025, New P: 0.341
-Original Grad: 0.026, -lr * Pred Grad:  0.052, New P: 0.086
iter 26 loss: 0.035
Actual params: [0.3407, 0.0857]
-Original Grad: -0.021, -lr * Pred Grad:  0.020, New P: 0.360
-Original Grad: 0.014, -lr * Pred Grad:  0.053, New P: 0.138
iter 27 loss: 0.036
Actual params: [0.3605, 0.1383]
-Original Grad: -0.147, -lr * Pred Grad:  -0.005, New P: 0.355
-Original Grad: -0.050, -lr * Pred Grad:  0.029, New P: 0.167
iter 28 loss: 0.037
Actual params: [0.3554, 0.1673]
-Original Grad: 0.023, -lr * Pred Grad:  -0.001, New P: 0.354
-Original Grad: 0.015, -lr * Pred Grad:  0.031, New P: 0.199
iter 29 loss: 0.039
Actual params: [0.3543, 0.1987]
-Original Grad: -0.082, -lr * Pred Grad:  -0.013, New P: 0.341
-Original Grad: -0.034, -lr * Pred Grad:  0.017, New P: 0.216
iter 30 loss: 0.039
Actual params: [0.3412, 0.2155]
-Original Grad: -0.067, -lr * Pred Grad:  -0.022, New P: 0.320
-Original Grad: -0.033, -lr * Pred Grad:  0.004, New P: 0.220
Target params: [1.3344, 1.5708]
iter 0 loss: 0.008
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.015, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.004, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.006
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.027, -lr * Pred Grad:  0.098, New P: -0.275
-Original Grad: -0.007, -lr * Pred Grad:  -0.098, New P: -0.195
iter 2 loss: 0.003
Actual params: [-0.2746, -0.1946]
-Original Grad: 0.025, -lr * Pred Grad:  0.099, New P: -0.176
-Original Grad: -0.000, -lr * Pred Grad:  -0.078, New P: -0.273
iter 3 loss: 0.002
Actual params: [-0.1757, -0.2728]
-Original Grad: 0.008, -lr * Pred Grad:  0.091, New P: -0.084
-Original Grad: 0.001, -lr * Pred Grad:  -0.058, New P: -0.331
iter 4 loss: 0.003
Actual params: [-0.0843, -0.3312]
-Original Grad: -0.028, -lr * Pred Grad:  0.032, New P: -0.052
-Original Grad: 0.001, -lr * Pred Grad:  -0.040, New P: -0.372
iter 5 loss: 0.004
Actual params: [-0.0523, -0.3717]
-Original Grad: -0.127, -lr * Pred Grad:  -0.039, New P: -0.091
-Original Grad: -0.032, -lr * Pred Grad:  -0.060, New P: -0.431
iter 6 loss: 0.002
Actual params: [-0.0911, -0.4312]
-Original Grad: -0.019, -lr * Pred Grad:  -0.041, New P: -0.132
-Original Grad: -0.003, -lr * Pred Grad:  -0.056, New P: -0.487
iter 7 loss: 0.002
Actual params: [-0.1318, -0.4875]
-Original Grad: 0.001, -lr * Pred Grad:  -0.035, New P: -0.167
-Original Grad: 0.004, -lr * Pred Grad:  -0.044, New P: -0.532
iter 8 loss: 0.002
Actual params: [-0.1672, -0.5317]
-Original Grad: 0.011, -lr * Pred Grad:  -0.027, New P: -0.195
-Original Grad: 0.007, -lr * Pred Grad:  -0.028, New P: -0.560
iter 9 loss: 0.003
Actual params: [-0.1946, -0.5601]
-Original Grad: 0.020, -lr * Pred Grad:  -0.017, New P: -0.212
-Original Grad: 0.013, -lr * Pred Grad:  -0.006, New P: -0.566
iter 10 loss: 0.003
Actual params: [-0.2119, -0.5664]
-Original Grad: 0.013, -lr * Pred Grad:  -0.011, New P: -0.223
-Original Grad: 0.008, -lr * Pred Grad:  0.005, New P: -0.562
iter 11 loss: 0.003
Actual params: [-0.2229, -0.5618]
-Original Grad: 0.018, -lr * Pred Grad:  -0.003, New P: -0.226
-Original Grad: 0.010, -lr * Pred Grad:  0.016, New P: -0.545
iter 12 loss: 0.003
Actual params: [-0.2263, -0.5453]
-Original Grad: 0.022, -lr * Pred Grad:  0.004, New P: -0.222
-Original Grad: 0.012, -lr * Pred Grad:  0.029, New P: -0.517
iter 13 loss: 0.003
Actual params: [-0.2219, -0.5165]
-Original Grad: 0.018, -lr * Pred Grad:  0.010, New P: -0.212
-Original Grad: 0.009, -lr * Pred Grad:  0.036, New P: -0.481
iter 14 loss: 0.003
Actual params: [-0.2118, -0.4807]
-Original Grad: 0.017, -lr * Pred Grad:  0.015, New P: -0.197
-Original Grad: 0.008, -lr * Pred Grad:  0.041, New P: -0.440
iter 15 loss: 0.002
Actual params: [-0.1971, -0.4399]
-Original Grad: 0.012, -lr * Pred Grad:  0.017, New P: -0.180
-Original Grad: 0.005, -lr * Pred Grad:  0.043, New P: -0.397
iter 16 loss: 0.002
Actual params: [-0.1798, -0.3972]
-Original Grad: 0.011, -lr * Pred Grad:  0.019, New P: -0.160
-Original Grad: 0.004, -lr * Pred Grad:  0.043, New P: -0.354
iter 17 loss: 0.002
Actual params: [-0.1604, -0.3539]
-Original Grad: 0.002, -lr * Pred Grad:  0.018, New P: -0.142
-Original Grad: 0.002, -lr * Pred Grad:  0.042, New P: -0.312
iter 18 loss: 0.002
Actual params: [-0.1421, -0.3123]
-Original Grad: -0.006, -lr * Pred Grad:  0.015, New P: -0.127
-Original Grad: 0.000, -lr * Pred Grad:  0.038, New P: -0.274
iter 19 loss: 0.002
Actual params: [-0.1274, -0.2743]
-Original Grad: -0.006, -lr * Pred Grad:  0.011, New P: -0.116
-Original Grad: 0.001, -lr * Pred Grad:  0.036, New P: -0.238
iter 20 loss: 0.002
Actual params: [-0.116 , -0.2384]
-Original Grad: -0.014, -lr * Pred Grad:  0.005, New P: -0.111
-Original Grad: 0.002, -lr * Pred Grad:  0.035, New P: -0.203
iter 21 loss: 0.002
Actual params: [-0.1106, -0.2031]
-Original Grad: -0.013, -lr * Pred Grad:  0.000, New P: -0.110
-Original Grad: 0.002, -lr * Pred Grad:  0.035, New P: -0.168
iter 22 loss: 0.002
Actual params: [-0.1104, -0.1683]
-Original Grad: -0.012, -lr * Pred Grad:  -0.004, New P: -0.115
-Original Grad: 0.002, -lr * Pred Grad:  0.034, New P: -0.134
iter 23 loss: 0.002
Actual params: [-0.1147, -0.1341]
-Original Grad: -0.011, -lr * Pred Grad:  -0.008, New P: -0.122
-Original Grad: 0.002, -lr * Pred Grad:  0.033, New P: -0.101
iter 24 loss: 0.002
Actual params: [-0.1224, -0.1009]
-Original Grad: -0.002, -lr * Pred Grad:  -0.008, New P: -0.130
-Original Grad: 0.000, -lr * Pred Grad:  0.031, New P: -0.070
iter 25 loss: 0.002
Actual params: [-0.1302, -0.0702]
-Original Grad: 0.003, -lr * Pred Grad:  -0.006, New P: -0.136
-Original Grad: -0.001, -lr * Pred Grad:  0.027, New P: -0.044
iter 26 loss: 0.002
Actual params: [-0.1363, -0.0435]
-Original Grad: 0.002, -lr * Pred Grad:  -0.005, New P: -0.141
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: -0.019
iter 27 loss: 0.002
Actual params: [-0.1411, -0.0191]
-Original Grad: 0.008, -lr * Pred Grad:  -0.001, New P: -0.143
-Original Grad: -0.002, -lr * Pred Grad:  0.019, New P: 0.000
iter 28 loss: 0.002
Actual params: [-0.1426,  0.0002]
-Original Grad: 0.007, -lr * Pred Grad:  0.001, New P: -0.141
-Original Grad: -0.002, -lr * Pred Grad:  0.015, New P: 0.015
iter 29 loss: 0.002
Actual params: [-0.1413,  0.0152]
-Original Grad: 0.006, -lr * Pred Grad:  0.003, New P: -0.138
-Original Grad: -0.002, -lr * Pred Grad:  0.011, New P: 0.026
iter 30 loss: 0.002
Actual params: [-0.1378,  0.0261]
-Original Grad: 0.005, -lr * Pred Grad:  0.005, New P: -0.133
-Original Grad: -0.002, -lr * Pred Grad:  0.008, New P: 0.034
Target params: [1.3344, 1.5708]
iter 0 loss: 0.537
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.036, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.535
Actual params: [-0.5723,  0.1035]
-Original Grad: 0.006, -lr * Pred Grad:  0.032, New P: -0.541
-Original Grad: 0.037, -lr * Pred Grad:  0.100, New P: 0.204
iter 2 loss: 0.531
Actual params: [-0.5406,  0.2036]
-Original Grad: 0.014, -lr * Pred Grad:  0.068, New P: -0.472
-Original Grad: 0.045, -lr * Pred Grad:  0.100, New P: 0.304
iter 3 loss: 0.527
Actual params: [-0.4723,  0.3038]
-Original Grad: 0.027, -lr * Pred Grad:  0.078, New P: -0.394
-Original Grad: 0.057, -lr * Pred Grad:  0.100, New P: 0.404
iter 4 loss: 0.520
Actual params: [-0.3941,  0.4041]
-Original Grad: 0.038, -lr * Pred Grad:  0.084, New P: -0.310
-Original Grad: 0.056, -lr * Pred Grad:  0.101, New P: 0.505
iter 5 loss: 0.512
Actual params: [-0.3099,  0.5049]
-Original Grad: 0.073, -lr * Pred Grad:  0.084, New P: -0.226
-Original Grad: 0.087, -lr * Pred Grad:  0.100, New P: 0.605
iter 6 loss: 0.502
Actual params: [-0.226 ,  0.6052]
-Original Grad: 0.097, -lr * Pred Grad:  0.087, New P: -0.139
-Original Grad: 0.083, -lr * Pred Grad:  0.101, New P: 0.706
iter 7 loss: 0.489
Actual params: [-0.1393,  0.7063]
-Original Grad: 0.141, -lr * Pred Grad:  0.088, New P: -0.051
-Original Grad: 0.090, -lr * Pred Grad:  0.102, New P: 0.808
iter 8 loss: 0.474
Actual params: [-0.051 ,  0.8084]
-Original Grad: 0.164, -lr * Pred Grad:  0.091, New P: 0.040
-Original Grad: 0.082, -lr * Pred Grad:  0.103, New P: 0.911
iter 9 loss: 0.458
Actual params: [0.0404, 0.911 ]
-Original Grad: 0.189, -lr * Pred Grad:  0.094, New P: 0.135
-Original Grad: 0.058, -lr * Pred Grad:  0.101, New P: 1.012
iter 10 loss: 0.439
Actual params: [0.1347, 1.0124]
-Original Grad: 0.220, -lr * Pred Grad:  0.097, New P: 0.232
-Original Grad: 0.043, -lr * Pred Grad:  0.099, New P: 1.111
iter 11 loss: 0.415
Actual params: [0.2316, 1.1112]
-Original Grad: 0.452, -lr * Pred Grad:  0.093, New P: 0.325
-Original Grad: 0.124, -lr * Pred Grad:  0.101, New P: 1.212
iter 12 loss: 0.376
Actual params: [0.3247, 1.2121]
-Original Grad: 0.460, -lr * Pred Grad:  0.096, New P: 0.421
-Original Grad: 0.119, -lr * Pred Grad:  0.103, New P: 1.315
iter 13 loss: 0.329
Actual params: [0.4206, 1.315 ]
-Original Grad: 0.501, -lr * Pred Grad:  0.099, New P: 0.520
-Original Grad: 0.153, -lr * Pred Grad:  0.105, New P: 1.420
iter 14 loss: 0.285
Actual params: [0.5195, 1.4198]
-Original Grad: 0.392, -lr * Pred Grad:  0.101, New P: 0.621
-Original Grad: 0.145, -lr * Pred Grad:  0.106, New P: 1.526
iter 15 loss: 0.247
Actual params: [0.621 , 1.5262]
-Original Grad: 0.339, -lr * Pred Grad:  0.103, New P: 0.724
-Original Grad: 0.110, -lr * Pred Grad:  0.107, New P: 1.633
iter 16 loss: 0.213
Actual params: [0.7239, 1.633 ]
-Original Grad: 0.433, -lr * Pred Grad:  0.105, New P: 0.829
-Original Grad: 0.068, -lr * Pred Grad:  0.104, New P: 1.737
iter 17 loss: 0.185
Actual params: [0.829 , 1.7373]
-Original Grad: 0.382, -lr * Pred Grad:  0.106, New P: 0.935
-Original Grad: 0.024, -lr * Pred Grad:  0.098, New P: 1.835
iter 18 loss: 0.150
Actual params: [0.9354, 1.8351]
-Original Grad: 0.516, -lr * Pred Grad:  0.109, New P: 1.044
-Original Grad: 0.020, -lr * Pred Grad:  0.092, New P: 1.927
iter 19 loss: 0.119
Actual params: [1.044 , 1.9266]
-Original Grad: 0.235, -lr * Pred Grad:  0.106, New P: 1.150
-Original Grad: 0.009, -lr * Pred Grad:  0.084, New P: 2.011
iter 20 loss: 0.096
Actual params: [1.1503, 2.0109]
-Original Grad: 0.150, -lr * Pred Grad:  0.102, New P: 1.252
-Original Grad: 0.037, -lr * Pred Grad:  0.081, New P: 2.092
iter 21 loss: 0.084
Actual params: [1.2522, 2.0924]
-Original Grad: 0.107, -lr * Pred Grad:  0.097, New P: 1.349
-Original Grad: 0.018, -lr * Pred Grad:  0.077, New P: 2.169
iter 22 loss: 0.080
Actual params: [1.3489, 2.1689]
-Original Grad: 0.094, -lr * Pred Grad:  0.091, New P: 1.440
-Original Grad: 0.029, -lr * Pred Grad:  0.074, New P: 2.243
iter 23 loss: 0.077
Actual params: [1.4403, 2.2425]
-Original Grad: 0.008, -lr * Pred Grad:  0.084, New P: 1.524
-Original Grad: 0.039, -lr * Pred Grad:  0.072, New P: 2.315
iter 24 loss: 0.073
Actual params: [1.5239, 2.3147]
-Original Grad: 0.003, -lr * Pred Grad:  0.076, New P: 1.600
-Original Grad: 0.056, -lr * Pred Grad:  0.073, New P: 2.388
iter 25 loss: 0.070
Actual params: [1.6001, 2.3877]
-Original Grad: -0.129, -lr * Pred Grad:  0.064, New P: 1.664
-Original Grad: 0.068, -lr * Pred Grad:  0.075, New P: 2.463
iter 26 loss: 0.076
Actual params: [1.6641, 2.4628]
-Original Grad: -0.171, -lr * Pred Grad:  0.051, New P: 1.715
-Original Grad: 0.067, -lr * Pred Grad:  0.077, New P: 2.540
iter 27 loss: 0.083
Actual params: [1.7149, 2.5396]
-Original Grad: -0.225, -lr * Pred Grad:  0.036, New P: 1.751
-Original Grad: 0.025, -lr * Pred Grad:  0.073, New P: 2.613
iter 28 loss: 0.092
Actual params: [1.7513, 2.6131]
-Original Grad: -0.106, -lr * Pred Grad:  0.029, New P: 1.780
-Original Grad: -0.135, -lr * Pred Grad:  0.045, New P: 2.658
iter 29 loss: 0.103
Actual params: [1.7801, 2.6578]
-Original Grad: -0.291, -lr * Pred Grad:  0.014, New P: 1.794
-Original Grad: -0.060, -lr * Pred Grad:  0.032, New P: 2.690
iter 30 loss: 0.111
Actual params: [1.7939, 2.69  ]
-Original Grad: -0.218, -lr * Pred Grad:  0.004, New P: 1.797
-Original Grad: -0.271, -lr * Pred Grad:  -0.007, New P: 2.683
Target params: [1.3344, 1.5708]
iter 0 loss: 0.577
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.005, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.006, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.574
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.024, -lr * Pred Grad:  0.087, New P: -0.285
-Original Grad: 0.023, -lr * Pred Grad:  0.089, New P: 0.192
iter 2 loss: 0.569
Actual params: [-0.285 ,  0.1924]
-Original Grad: 0.058, -lr * Pred Grad:  0.085, New P: -0.200
-Original Grad: 0.054, -lr * Pred Grad:  0.087, New P: 0.279
iter 3 loss: 0.558
Actual params: [-0.2  ,  0.279]
-Original Grad: 0.115, -lr * Pred Grad:  0.084, New P: -0.116
-Original Grad: 0.101, -lr * Pred Grad:  0.086, New P: 0.365
iter 4 loss: 0.543
Actual params: [-0.1157,  0.3648]
-Original Grad: 0.139, -lr * Pred Grad:  0.089, New P: -0.027
-Original Grad: 0.116, -lr * Pred Grad:  0.090, New P: 0.455
iter 5 loss: 0.523
Actual params: [-0.0271,  0.4548]
-Original Grad: 0.219, -lr * Pred Grad:  0.089, New P: 0.062
-Original Grad: 0.151, -lr * Pred Grad:  0.092, New P: 0.547
iter 6 loss: 0.499
Actual params: [0.0624, 0.5472]
-Original Grad: 0.219, -lr * Pred Grad:  0.093, New P: 0.155
-Original Grad: 0.132, -lr * Pred Grad:  0.095, New P: 0.642
iter 7 loss: 0.470
Actual params: [0.1553, 0.6424]
-Original Grad: 0.258, -lr * Pred Grad:  0.095, New P: 0.251
-Original Grad: 0.119, -lr * Pred Grad:  0.097, New P: 0.739
iter 8 loss: 0.439
Actual params: [0.2507, 0.7394]
-Original Grad: 0.295, -lr * Pred Grad:  0.098, New P: 0.348
-Original Grad: 0.119, -lr * Pred Grad:  0.098, New P: 0.838
iter 9 loss: 0.408
Actual params: [0.3484, 0.8377]
-Original Grad: 0.275, -lr * Pred Grad:  0.100, New P: 0.448
-Original Grad: 0.082, -lr * Pred Grad:  0.097, New P: 0.935
iter 10 loss: 0.386
Actual params: [0.448 , 0.9349]
-Original Grad: 0.121, -lr * Pred Grad:  0.097, New P: 0.545
-Original Grad: 0.032, -lr * Pred Grad:  0.092, New P: 1.026
iter 11 loss: 0.371
Actual params: [0.5451, 1.0264]
-Original Grad: 0.135, -lr * Pred Grad:  0.096, New P: 0.641
-Original Grad: 0.050, -lr * Pred Grad:  0.089, New P: 1.115
iter 12 loss: 0.354
Actual params: [0.6407, 1.1152]
-Original Grad: 0.057, -lr * Pred Grad:  0.090, New P: 0.731
-Original Grad: 0.077, -lr * Pred Grad:  0.089, New P: 1.204
iter 13 loss: 0.329
Actual params: [0.7309, 1.2041]
-Original Grad: 0.246, -lr * Pred Grad:  0.094, New P: 0.824
-Original Grad: 0.050, -lr * Pred Grad:  0.087, New P: 1.291
iter 14 loss: 0.296
Actual params: [0.8244, 1.2909]
-Original Grad: 0.379, -lr * Pred Grad:  0.098, New P: 0.922
-Original Grad: -0.016, -lr * Pred Grad:  0.076, New P: 1.367
iter 15 loss: 0.261
Actual params: [0.922 , 1.3669]
-Original Grad: 0.229, -lr * Pred Grad:  0.099, New P: 1.021
-Original Grad: -0.054, -lr * Pred Grad:  0.060, New P: 1.427
iter 16 loss: 0.236
Actual params: [1.0207, 1.4272]
-Original Grad: 0.168, -lr * Pred Grad:  0.098, New P: 1.118
-Original Grad: -0.128, -lr * Pred Grad:  0.034, New P: 1.461
iter 17 loss: 0.223
Actual params: [1.1184, 1.461 ]
-Original Grad: 0.108, -lr * Pred Grad:  0.094, New P: 1.213
-Original Grad: -0.304, -lr * Pred Grad:  -0.008, New P: 1.453
iter 18 loss: 0.215
Actual params: [1.2128, 1.4528]
-Original Grad: 0.016, -lr * Pred Grad:  0.087, New P: 1.300
-Original Grad: -0.271, -lr * Pred Grad:  -0.031, New P: 1.421
iter 19 loss: 0.205
Actual params: [1.2996, 1.4214]
-Original Grad: -0.041, -lr * Pred Grad:  0.076, New P: 1.376
-Original Grad: -0.350, -lr * Pred Grad:  -0.051, New P: 1.370
iter 20 loss: 0.188
Actual params: [1.3759, 1.37  ]
-Original Grad: -0.020, -lr * Pred Grad:  0.068, New P: 1.444
-Original Grad: -0.341, -lr * Pred Grad:  -0.065, New P: 1.305
iter 21 loss: 0.167
Actual params: [1.4441, 1.3047]
-Original Grad: -0.036, -lr * Pred Grad:  0.060, New P: 1.504
-Original Grad: -0.221, -lr * Pred Grad:  -0.072, New P: 1.233
iter 22 loss: 0.149
Actual params: [1.5039, 1.2329]
-Original Grad: -0.045, -lr * Pred Grad:  0.051, New P: 1.555
-Original Grad: -0.172, -lr * Pred Grad:  -0.075, New P: 1.158
iter 23 loss: 0.133
Actual params: [1.5554, 1.1577]
-Original Grad: -0.045, -lr * Pred Grad:  0.044, New P: 1.599
-Original Grad: -0.141, -lr * Pred Grad:  -0.077, New P: 1.081
iter 24 loss: 0.120
Actual params: [1.5994, 1.0809]
-Original Grad: -0.046, -lr * Pred Grad:  0.037, New P: 1.636
-Original Grad: -0.123, -lr * Pred Grad:  -0.077, New P: 1.004
iter 25 loss: 0.111
Actual params: [1.6364, 1.0035]
-Original Grad: -0.053, -lr * Pred Grad:  0.030, New P: 1.667
-Original Grad: -0.122, -lr * Pred Grad:  -0.078, New P: 0.926
iter 26 loss: 0.104
Actual params: [1.6667, 0.9257]
-Original Grad: -0.061, -lr * Pred Grad:  0.023, New P: 1.690
-Original Grad: -0.051, -lr * Pred Grad:  -0.074, New P: 0.852
iter 27 loss: 0.101
Actual params: [1.6901, 0.8516]
-Original Grad: -0.064, -lr * Pred Grad:  0.017, New P: 1.707
-Original Grad: -0.056, -lr * Pred Grad:  -0.071, New P: 0.780
iter 28 loss: 0.100
Actual params: [1.7072, 0.7804]
-Original Grad: -0.087, -lr * Pred Grad:  0.010, New P: 1.717
-Original Grad: -0.011, -lr * Pred Grad:  -0.066, New P: 0.715
iter 29 loss: 0.098
Actual params: [1.717 , 0.7147]
-Original Grad: -0.085, -lr * Pred Grad:  0.003, New P: 1.720
-Original Grad: -0.060, -lr * Pred Grad:  -0.064, New P: 0.651
iter 30 loss: 0.097
Actual params: [1.72 , 0.651]
-Original Grad: -0.075, -lr * Pred Grad:  -0.002, New P: 1.718
-Original Grad: -0.024, -lr * Pred Grad:  -0.060, New P: 0.591
Target params: [1.3344, 1.5708]
iter 0 loss: 0.486
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.020, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.035, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.476
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.050, -lr * Pred Grad:  0.094, New P: -0.278
-Original Grad: 0.075, -lr * Pred Grad:  0.096, New P: 0.199
iter 2 loss: 0.462
Actual params: [-0.2785,  0.1991]
-Original Grad: 0.091, -lr * Pred Grad:  0.092, New P: -0.187
-Original Grad: 0.128, -lr * Pred Grad:  0.094, New P: 0.293
iter 3 loss: 0.443
Actual params: [-0.1867,  0.293 ]
-Original Grad: 0.114, -lr * Pred Grad:  0.094, New P: -0.093
-Original Grad: 0.129, -lr * Pred Grad:  0.096, New P: 0.389
iter 4 loss: 0.412
Actual params: [-0.0929,  0.3892]
-Original Grad: 0.232, -lr * Pred Grad:  0.089, New P: -0.003
-Original Grad: 0.202, -lr * Pred Grad:  0.096, New P: 0.485
iter 5 loss: 0.370
Actual params: [-0.0035,  0.4851]
-Original Grad: 0.305, -lr * Pred Grad:  0.091, New P: 0.087
-Original Grad: 0.192, -lr * Pred Grad:  0.098, New P: 0.583
iter 6 loss: 0.318
Actual params: [0.0871, 0.5829]
-Original Grad: 0.370, -lr * Pred Grad:  0.093, New P: 0.180
-Original Grad: 0.160, -lr * Pred Grad:  0.099, New P: 0.682
iter 7 loss: 0.255
Actual params: [0.1797, 0.6816]
-Original Grad: 0.407, -lr * Pred Grad:  0.095, New P: 0.275
-Original Grad: 0.118, -lr * Pred Grad:  0.098, New P: 0.779
iter 8 loss: 0.204
Actual params: [0.2749, 0.7795]
-Original Grad: 0.427, -lr * Pred Grad:  0.098, New P: 0.373
-Original Grad: 0.079, -lr * Pred Grad:  0.095, New P: 0.874
iter 9 loss: 0.170
Actual params: [0.3725, 0.8742]
-Original Grad: 0.310, -lr * Pred Grad:  0.099, New P: 0.471
-Original Grad: 0.003, -lr * Pred Grad:  0.085, New P: 0.959
iter 10 loss: 0.149
Actual params: [0.4711, 0.9591]
-Original Grad: 0.138, -lr * Pred Grad:  0.095, New P: 0.566
-Original Grad: 0.011, -lr * Pred Grad:  0.077, New P: 1.036
iter 11 loss: 0.135
Actual params: [0.566 , 1.0365]
-Original Grad: 0.159, -lr * Pred Grad:  0.093, New P: 0.659
-Original Grad: 0.001, -lr * Pred Grad:  0.070, New P: 1.106
iter 12 loss: 0.119
Actual params: [0.6585, 1.1062]
-Original Grad: 0.196, -lr * Pred Grad:  0.092, New P: 0.750
-Original Grad: -0.001, -lr * Pred Grad:  0.063, New P: 1.169
iter 13 loss: 0.100
Actual params: [0.7504, 1.1688]
-Original Grad: 0.259, -lr * Pred Grad:  0.093, New P: 0.843
-Original Grad: -0.026, -lr * Pred Grad:  0.053, New P: 1.222
iter 14 loss: 0.079
Actual params: [0.8434, 1.2222]
-Original Grad: 0.269, -lr * Pred Grad:  0.094, New P: 0.938
-Original Grad: -0.038, -lr * Pred Grad:  0.044, New P: 1.266
iter 15 loss: 0.061
Actual params: [0.9376, 1.2657]
-Original Grad: 0.214, -lr * Pred Grad:  0.094, New P: 1.032
-Original Grad: -0.046, -lr * Pred Grad:  0.034, New P: 1.299
iter 16 loss: 0.049
Actual params: [1.0315, 1.2993]
-Original Grad: 0.069, -lr * Pred Grad:  0.088, New P: 1.120
-Original Grad: -0.013, -lr * Pred Grad:  0.029, New P: 1.328
iter 17 loss: 0.043
Actual params: [1.1199, 1.3283]
-Original Grad: 0.019, -lr * Pred Grad:  0.081, New P: 1.201
-Original Grad: -0.049, -lr * Pred Grad:  0.020, New P: 1.348
iter 18 loss: 0.044
Actual params: [1.201 , 1.3485]
-Original Grad: -0.027, -lr * Pred Grad:  0.072, New P: 1.273
-Original Grad: -0.106, -lr * Pred Grad:  0.005, New P: 1.354
iter 19 loss: 0.048
Actual params: [1.2734, 1.3537]
-Original Grad: -0.070, -lr * Pred Grad:  0.062, New P: 1.336
-Original Grad: -0.103, -lr * Pred Grad:  -0.007, New P: 1.346
iter 20 loss: 0.053
Actual params: [1.3356, 1.3463]
-Original Grad: -0.131, -lr * Pred Grad:  0.050, New P: 1.385
-Original Grad: -0.088, -lr * Pred Grad:  -0.017, New P: 1.330
iter 21 loss: 0.056
Actual params: [1.3851, 1.3296]
-Original Grad: -0.053, -lr * Pred Grad:  0.042, New P: 1.427
-Original Grad: -0.140, -lr * Pred Grad:  -0.030, New P: 1.300
iter 22 loss: 0.054
Actual params: [1.4274, 1.2995]
-Original Grad: -0.060, -lr * Pred Grad:  0.035, New P: 1.463
-Original Grad: -0.144, -lr * Pred Grad:  -0.042, New P: 1.258
iter 23 loss: 0.050
Actual params: [1.4628, 1.2579]
-Original Grad: -0.056, -lr * Pred Grad:  0.029, New P: 1.492
-Original Grad: -0.140, -lr * Pred Grad:  -0.051, New P: 1.207
iter 24 loss: 0.044
Actual params: [1.4921, 1.2068]
-Original Grad: -0.042, -lr * Pred Grad:  0.024, New P: 1.517
-Original Grad: -0.127, -lr * Pred Grad:  -0.058, New P: 1.149
iter 25 loss: 0.038
Actual params: [1.5166, 1.1487]
-Original Grad: -0.055, -lr * Pred Grad:  0.019, New P: 1.536
-Original Grad: -0.134, -lr * Pred Grad:  -0.065, New P: 1.084
iter 26 loss: 0.032
Actual params: [1.5359, 1.0839]
-Original Grad: -0.057, -lr * Pred Grad:  0.015, New P: 1.550
-Original Grad: -0.102, -lr * Pred Grad:  -0.068, New P: 1.016
iter 27 loss: 0.027
Actual params: [1.5504, 1.0156]
-Original Grad: -0.042, -lr * Pred Grad:  0.011, New P: 1.561
-Original Grad: -0.068, -lr * Pred Grad:  -0.069, New P: 0.947
iter 28 loss: 0.024
Actual params: [1.5613, 0.9471]
-Original Grad: -0.028, -lr * Pred Grad:  0.008, New P: 1.570
-Original Grad: -0.027, -lr * Pred Grad:  -0.065, New P: 0.882
iter 29 loss: 0.023
Actual params: [1.5698, 0.882 ]
-Original Grad: -0.027, -lr * Pred Grad:  0.006, New P: 1.576
-Original Grad: -0.019, -lr * Pred Grad:  -0.061, New P: 0.821
iter 30 loss: 0.022
Actual params: [1.576 , 0.8207]
-Original Grad: -0.014, -lr * Pred Grad:  0.005, New P: 1.581
-Original Grad: -0.014, -lr * Pred Grad:  -0.057, New P: 0.763
Target params: [1.3344, 1.5708]
iter 0 loss: 0.170
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.073, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.013, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.158
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.096, -lr * Pred Grad:  0.100, New P: -0.273
-Original Grad: 0.008, -lr * Pred Grad:  -0.015, New P: -0.111
iter 2 loss: 0.146
Actual params: [-0.2726, -0.1114]
-Original Grad: 0.092, -lr * Pred Grad:  0.100, New P: -0.173
-Original Grad: 0.024, -lr * Pred Grad:  0.048, New P: -0.063
iter 3 loss: 0.131
Actual params: [-0.1725, -0.0635]
-Original Grad: 0.101, -lr * Pred Grad:  0.100, New P: -0.072
-Original Grad: 0.019, -lr * Pred Grad:  0.065, New P: 0.001
iter 4 loss: 0.114
Actual params: [-0.0721,  0.0012]
-Original Grad: 0.109, -lr * Pred Grad:  0.101, New P: 0.029
-Original Grad: 0.025, -lr * Pred Grad:  0.076, New P: 0.078
iter 5 loss: 0.097
Actual params: [0.0288, 0.0775]
-Original Grad: 0.099, -lr * Pred Grad:  0.101, New P: 0.130
-Original Grad: 0.002, -lr * Pred Grad:  0.068, New P: 0.145
iter 6 loss: 0.075
Actual params: [0.1296, 0.1452]
-Original Grad: 0.193, -lr * Pred Grad:  0.100, New P: 0.230
-Original Grad: 0.060, -lr * Pred Grad:  0.076, New P: 0.221
iter 7 loss: 0.046
Actual params: [0.2298, 0.2207]
-Original Grad: 0.190, -lr * Pred Grad:  0.101, New P: 0.331
-Original Grad: 0.043, -lr * Pred Grad:  0.083, New P: 0.303
iter 8 loss: 0.021
Actual params: [0.3309, 0.3033]
-Original Grad: 0.134, -lr * Pred Grad:  0.101, New P: 0.432
-Original Grad: 0.033, -lr * Pred Grad:  0.086, New P: 0.389
iter 9 loss: 0.007
Actual params: [0.4322, 0.3893]
-Original Grad: 0.019, -lr * Pred Grad:  0.093, New P: 0.525
-Original Grad: 0.004, -lr * Pred Grad:  0.079, New P: 0.468
iter 10 loss: 0.007
Actual params: [0.5248, 0.4679]
-Original Grad: -0.011, -lr * Pred Grad:  0.082, New P: 0.606
-Original Grad: -0.025, -lr * Pred Grad:  0.055, New P: 0.523
iter 11 loss: 0.014
Actual params: [0.6064, 0.5233]
-Original Grad: -0.064, -lr * Pred Grad:  0.064, New P: 0.671
-Original Grad: -0.074, -lr * Pred Grad:  0.010, New P: 0.533
iter 12 loss: 0.020
Actual params: [0.6709, 0.5328]
-Original Grad: -0.075, -lr * Pred Grad:  0.048, New P: 0.719
-Original Grad: -0.083, -lr * Pred Grad:  -0.020, New P: 0.512
iter 13 loss: 0.023
Actual params: [0.7187, 0.5124]
-Original Grad: -0.041, -lr * Pred Grad:  0.038, New P: 0.757
-Original Grad: -0.062, -lr * Pred Grad:  -0.036, New P: 0.477
iter 14 loss: 0.023
Actual params: [0.7567, 0.4765]
-Original Grad: -0.112, -lr * Pred Grad:  0.020, New P: 0.777
-Original Grad: -0.104, -lr * Pred Grad:  -0.054, New P: 0.423
iter 15 loss: 0.019
Actual params: [0.7766, 0.4227]
-Original Grad: -0.066, -lr * Pred Grad:  0.010, New P: 0.787
-Original Grad: -0.067, -lr * Pred Grad:  -0.062, New P: 0.360
iter 16 loss: 0.015
Actual params: [0.7866, 0.3604]
-Original Grad: -0.063, -lr * Pred Grad:  0.002, New P: 0.788
-Original Grad: -0.041, -lr * Pred Grad:  -0.065, New P: 0.295
iter 17 loss: 0.012
Actual params: [0.7883, 0.2951]
-Original Grad: -0.048, -lr * Pred Grad:  -0.004, New P: 0.784
-Original Grad: -0.034, -lr * Pred Grad:  -0.067, New P: 0.228
iter 18 loss: 0.009
Actual params: [0.7842, 0.2285]
-Original Grad: -0.042, -lr * Pred Grad:  -0.009, New P: 0.776
-Original Grad: -0.022, -lr * Pred Grad:  -0.065, New P: 0.163
iter 19 loss: 0.007
Actual params: [0.7756, 0.1631]
-Original Grad: -0.044, -lr * Pred Grad:  -0.013, New P: 0.763
-Original Grad: -0.004, -lr * Pred Grad:  -0.061, New P: 0.102
iter 20 loss: 0.007
Actual params: [0.7626, 0.1025]
-Original Grad: -0.029, -lr * Pred Grad:  -0.015, New P: 0.747
-Original Grad: 0.006, -lr * Pred Grad:  -0.054, New P: 0.049
iter 21 loss: 0.008
Actual params: [0.7474, 0.049 ]
-Original Grad: -0.009, -lr * Pred Grad:  -0.015, New P: 0.732
-Original Grad: 0.045, -lr * Pred Grad:  -0.037, New P: 0.012
iter 22 loss: 0.009
Actual params: [0.7325, 0.0122]
-Original Grad: -0.008, -lr * Pred Grad:  -0.015, New P: 0.718
-Original Grad: 0.036, -lr * Pred Grad:  -0.024, New P: -0.012
iter 23 loss: 0.010
Actual params: [ 0.7179, -0.0122]
-Original Grad: 0.012, -lr * Pred Grad:  -0.012, New P: 0.706
-Original Grad: 0.043, -lr * Pred Grad:  -0.011, New P: -0.024
iter 24 loss: 0.011
Actual params: [ 0.7061, -0.0235]
-Original Grad: 0.017, -lr * Pred Grad:  -0.009, New P: 0.698
-Original Grad: 0.057, -lr * Pred Grad:  0.004, New P: -0.020
iter 25 loss: 0.011
Actual params: [ 0.6976, -0.02  ]
-Original Grad: 0.002, -lr * Pred Grad:  -0.008, New P: 0.690
-Original Grad: 0.015, -lr * Pred Grad:  0.007, New P: -0.013
iter 26 loss: 0.011
Actual params: [ 0.69  , -0.0132]
-Original Grad: 0.008, -lr * Pred Grad:  -0.006, New P: 0.684
-Original Grad: 0.051, -lr * Pred Grad:  0.018, New P: 0.005
iter 27 loss: 0.010
Actual params: [0.684 , 0.0048]
-Original Grad: 0.029, -lr * Pred Grad:  -0.002, New P: 0.682
-Original Grad: 0.041, -lr * Pred Grad:  0.026, New P: 0.031
iter 28 loss: 0.009
Actual params: [0.6824, 0.0305]
-Original Grad: 0.012, -lr * Pred Grad:  0.000, New P: 0.682
-Original Grad: 0.020, -lr * Pred Grad:  0.028, New P: 0.059
iter 29 loss: 0.007
Actual params: [0.6824, 0.0586]
-Original Grad: 0.007, -lr * Pred Grad:  0.001, New P: 0.683
-Original Grad: 0.006, -lr * Pred Grad:  0.027, New P: 0.086
iter 30 loss: 0.007
Actual params: [0.6834, 0.0857]
-Original Grad: 0.011, -lr * Pred Grad:  0.002, New P: 0.686
-Original Grad: 0.029, -lr * Pred Grad:  0.032, New P: 0.117
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.363
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.002, -lr * Pred Grad:  0.094, New P: -0.278
-Original Grad: 0.000, -lr * Pred Grad:  0.094, New P: 0.198
iter 2 loss: 0.363
Actual params: [-0.278,  0.198]
-Original Grad: 0.006, -lr * Pred Grad:  0.083, New P: -0.195
-Original Grad: 0.001, -lr * Pred Grad:  0.082, New P: 0.280
iter 3 loss: 0.362
Actual params: [-0.1955,  0.2804]
-Original Grad: 0.012, -lr * Pred Grad:  0.082, New P: -0.113
-Original Grad: 0.003, -lr * Pred Grad:  0.081, New P: 0.361
iter 4 loss: 0.360
Actual params: [-0.1131,  0.3613]
-Original Grad: 0.021, -lr * Pred Grad:  0.083, New P: -0.030
-Original Grad: 0.006, -lr * Pred Grad:  0.081, New P: 0.442
iter 5 loss: 0.355
Actual params: [-0.0303,  0.4423]
-Original Grad: 0.078, -lr * Pred Grad:  0.071, New P: 0.041
-Original Grad: 0.023, -lr * Pred Grad:  0.069, New P: 0.511
iter 6 loss: 0.347
Actual params: [0.041 , 0.5114]
-Original Grad: 0.138, -lr * Pred Grad:  0.075, New P: 0.116
-Original Grad: 0.044, -lr * Pred Grad:  0.074, New P: 0.585
iter 7 loss: 0.328
Actual params: [0.1164, 0.585 ]
-Original Grad: 0.270, -lr * Pred Grad:  0.077, New P: 0.193
-Original Grad: 0.091, -lr * Pred Grad:  0.075, New P: 0.660
iter 8 loss: 0.300
Actual params: [0.193 , 0.6597]
-Original Grad: 0.285, -lr * Pred Grad:  0.083, New P: 0.276
-Original Grad: 0.088, -lr * Pred Grad:  0.082, New P: 0.742
iter 9 loss: 0.260
Actual params: [0.2761, 0.7419]
-Original Grad: 0.533, -lr * Pred Grad:  0.084, New P: 0.360
-Original Grad: 0.137, -lr * Pred Grad:  0.086, New P: 0.828
iter 10 loss: 0.206
Actual params: [0.3603, 0.828 ]
-Original Grad: 0.547, -lr * Pred Grad:  0.089, New P: 0.449
-Original Grad: 0.080, -lr * Pred Grad:  0.090, New P: 0.918
iter 11 loss: 0.142
Actual params: [0.4492, 0.9177]
-Original Grad: 0.644, -lr * Pred Grad:  0.093, New P: 0.542
-Original Grad: 0.060, -lr * Pred Grad:  0.091, New P: 1.009
iter 12 loss: 0.100
Actual params: [0.5422, 1.0086]
-Original Grad: 0.439, -lr * Pred Grad:  0.096, New P: 0.638
-Original Grad: -0.014, -lr * Pred Grad:  0.078, New P: 1.087
iter 13 loss: 0.077
Actual params: [0.6379, 1.0871]
-Original Grad: 0.125, -lr * Pred Grad:  0.091, New P: 0.729
-Original Grad: 0.002, -lr * Pred Grad:  0.071, New P: 1.158
iter 14 loss: 0.060
Actual params: [0.7291, 1.1584]
-Original Grad: 0.209, -lr * Pred Grad:  0.090, New P: 0.819
-Original Grad: -0.002, -lr * Pred Grad:  0.064, New P: 1.223
iter 15 loss: 0.046
Actual params: [0.8188, 1.2226]
-Original Grad: 0.107, -lr * Pred Grad:  0.085, New P: 0.904
-Original Grad: 0.030, -lr * Pred Grad:  0.064, New P: 1.287
iter 16 loss: 0.034
Actual params: [0.9041, 1.2869]
-Original Grad: 0.133, -lr * Pred Grad:  0.082, New P: 0.987
-Original Grad: 0.022, -lr * Pred Grad:  0.063, New P: 1.350
iter 17 loss: 0.023
Actual params: [0.9865, 1.3499]
-Original Grad: 0.086, -lr * Pred Grad:  0.078, New P: 1.065
-Original Grad: -0.036, -lr * Pred Grad:  0.048, New P: 1.398
iter 18 loss: 0.020
Actual params: [1.0647, 1.3983]
-Original Grad: 0.021, -lr * Pred Grad:  0.072, New P: 1.137
-Original Grad: -0.065, -lr * Pred Grad:  0.028, New P: 1.427
iter 19 loss: 0.020
Actual params: [1.1366, 1.4265]
-Original Grad: 0.013, -lr * Pred Grad:  0.066, New P: 1.203
-Original Grad: -0.068, -lr * Pred Grad:  0.010, New P: 1.437
iter 20 loss: 0.020
Actual params: [1.2025, 1.4368]
-Original Grad: -0.011, -lr * Pred Grad:  0.060, New P: 1.262
-Original Grad: -0.122, -lr * Pred Grad:  -0.015, New P: 1.422
iter 21 loss: 0.019
Actual params: [1.2621, 1.4221]
-Original Grad: -0.016, -lr * Pred Grad:  0.054, New P: 1.316
-Original Grad: -0.091, -lr * Pred Grad:  -0.029, New P: 1.393
iter 22 loss: 0.017
Actual params: [1.3156, 1.3929]
-Original Grad: -0.012, -lr * Pred Grad:  0.048, New P: 1.364
-Original Grad: -0.068, -lr * Pred Grad:  -0.038, New P: 1.355
iter 23 loss: 0.016
Actual params: [1.3638, 1.355 ]
-Original Grad: -0.015, -lr * Pred Grad:  0.043, New P: 1.407
-Original Grad: -0.032, -lr * Pred Grad:  -0.040, New P: 1.315
iter 24 loss: 0.015
Actual params: [1.407 , 1.3149]
-Original Grad: -0.061, -lr * Pred Grad:  0.037, New P: 1.444
-Original Grad: -0.064, -lr * Pred Grad:  -0.047, New P: 1.268
iter 25 loss: 0.016
Actual params: [1.4436, 1.2678]
-Original Grad: -0.050, -lr * Pred Grad:  0.031, New P: 1.475
-Original Grad: -0.040, -lr * Pred Grad:  -0.050, New P: 1.218
iter 26 loss: 0.016
Actual params: [1.4747, 1.2182]
-Original Grad: -0.081, -lr * Pred Grad:  0.025, New P: 1.499
-Original Grad: -0.041, -lr * Pred Grad:  -0.052, New P: 1.166
iter 27 loss: 0.017
Actual params: [1.4993, 1.1661]
-Original Grad: -0.094, -lr * Pred Grad:  0.018, New P: 1.517
-Original Grad: -0.009, -lr * Pred Grad:  -0.049, New P: 1.117
iter 28 loss: 0.019
Actual params: [1.5174, 1.1171]
-Original Grad: -0.113, -lr * Pred Grad:  0.011, New P: 1.528
-Original Grad: 0.010, -lr * Pred Grad:  -0.043, New P: 1.074
iter 29 loss: 0.020
Actual params: [1.5285, 1.0743]
-Original Grad: -0.124, -lr * Pred Grad:  0.004, New P: 1.533
-Original Grad: 0.005, -lr * Pred Grad:  -0.038, New P: 1.036
iter 30 loss: 0.021
Actual params: [1.5328, 1.0361]
-Original Grad: -0.103, -lr * Pred Grad:  -0.001, New P: 1.532
-Original Grad: 0.010, -lr * Pred Grad:  -0.033, New P: 1.003
Target params: [1.3344, 1.5708]
iter 0 loss: 0.320
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.025, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.005, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.317
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.005, -lr * Pred Grad:  -0.080, New P: -0.652
-Original Grad: 0.004, -lr * Pred Grad:  0.099, New P: 0.202
iter 2 loss: 0.316
Actual params: [-0.6519,  0.2024]
-Original Grad: -0.001, -lr * Pred Grad:  -0.065, New P: -0.717
-Original Grad: 0.003, -lr * Pred Grad:  0.097, New P: 0.299
iter 3 loss: 0.315
Actual params: [-0.7169,  0.2989]
-Original Grad: -0.002, -lr * Pred Grad:  -0.058, New P: -0.775
-Original Grad: 0.003, -lr * Pred Grad:  0.095, New P: 0.394
iter 4 loss: 0.315
Actual params: [-0.775,  0.394]
-Original Grad: 0.000, -lr * Pred Grad:  -0.048, New P: -0.823
-Original Grad: 0.001, -lr * Pred Grad:  0.088, New P: 0.482
iter 5 loss: 0.315
Actual params: [-0.8232,  0.482 ]
-Original Grad: 0.001, -lr * Pred Grad:  -0.039, New P: -0.863
-Original Grad: 0.000, -lr * Pred Grad:  0.079, New P: 0.561
iter 6 loss: 0.315
Actual params: [-0.8626,  0.561 ]
-Original Grad: 0.001, -lr * Pred Grad:  -0.032, New P: -0.895
-Original Grad: 0.000, -lr * Pred Grad:  0.071, New P: 0.632
iter 7 loss: 0.315
Actual params: [-0.8949,  0.6321]
-Original Grad: 0.001, -lr * Pred Grad:  -0.027, New P: -0.922
-Original Grad: 0.000, -lr * Pred Grad:  0.064, New P: 0.696
iter 8 loss: 0.315
Actual params: [-0.9219,  0.6959]
-Original Grad: 0.000, -lr * Pred Grad:  -0.023, New P: -0.945
-Original Grad: 0.000, -lr * Pred Grad:  0.057, New P: 0.753
iter 9 loss: 0.315
Actual params: [-0.9451,  0.7529]
-Original Grad: 0.000, -lr * Pred Grad:  -0.020, New P: -0.965
-Original Grad: 0.000, -lr * Pred Grad:  0.051, New P: 0.804
iter 10 loss: 0.315
Actual params: [-0.965 ,  0.8037]
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -0.982
-Original Grad: -0.000, -lr * Pred Grad:  0.045, New P: 0.849
iter 11 loss: 0.315
Actual params: [-0.9821,  0.8491]
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -0.997
-Original Grad: -0.000, -lr * Pred Grad:  0.041, New P: 0.890
iter 12 loss: 0.315
Actual params: [-0.9968,  0.8898]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -1.009
-Original Grad: -0.000, -lr * Pred Grad:  0.036, New P: 0.926
iter 13 loss: 0.315
Actual params: [-1.0095,  0.9262]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.020
-Original Grad: -0.000, -lr * Pred Grad:  0.033, New P: 0.959
iter 14 loss: 0.315
Actual params: [-1.0204,  0.9591]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.030
-Original Grad: -0.000, -lr * Pred Grad:  0.029, New P: 0.989
iter 15 loss: 0.315
Actual params: [-1.03  ,  0.9886]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.038
-Original Grad: -0.000, -lr * Pred Grad:  0.027, New P: 1.015
iter 16 loss: 0.315
Actual params: [-1.0382,  1.0152]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.045
-Original Grad: -0.000, -lr * Pred Grad:  0.024, New P: 1.039
iter 17 loss: 0.315
Actual params: [-1.0453,  1.0392]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.051
-Original Grad: -0.000, -lr * Pred Grad:  0.022, New P: 1.061
iter 18 loss: 0.315
Actual params: [-1.0514,  1.0607]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.057
-Original Grad: -0.000, -lr * Pred Grad:  0.019, New P: 1.080
iter 19 loss: 0.315
Actual params: [-1.0567,  1.08  ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.061
-Original Grad: -0.000, -lr * Pred Grad:  0.017, New P: 1.097
iter 20 loss: 0.315
Actual params: [-1.0611,  1.0973]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.065
-Original Grad: -0.000, -lr * Pred Grad:  0.015, New P: 1.113
iter 21 loss: 0.315
Actual params: [-1.065 ,  1.1128]
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -1.068
-Original Grad: -0.000, -lr * Pred Grad:  0.014, New P: 1.127
iter 22 loss: 0.315
Actual params: [-1.0682,  1.1266]
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -1.071
-Original Grad: -0.000, -lr * Pred Grad:  0.012, New P: 1.139
iter 23 loss: 0.315
Actual params: [-1.0708,  1.139 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: -1.073
-Original Grad: -0.000, -lr * Pred Grad:  0.011, New P: 1.150
iter 24 loss: 0.315
Actual params: [-1.0731,  1.1501]
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: -1.075
-Original Grad: -0.000, -lr * Pred Grad:  0.010, New P: 1.160
iter 25 loss: 0.315
Actual params: [-1.0749,  1.16  ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -1.076
-Original Grad: -0.000, -lr * Pred Grad:  0.009, New P: 1.169
iter 26 loss: 0.315
Actual params: [-1.0763,  1.1686]
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -1.077
-Original Grad: -0.000, -lr * Pred Grad:  0.008, New P: 1.176
iter 27 loss: 0.315
Actual params: [-1.0774,  1.1763]
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -1.078
-Original Grad: -0.000, -lr * Pred Grad:  0.007, New P: 1.183
iter 28 loss: 0.315
Actual params: [-1.0782,  1.1831]
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -1.079
-Original Grad: -0.000, -lr * Pred Grad:  0.006, New P: 1.189
iter 29 loss: 0.315
Actual params: [-1.0787,  1.189 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: -1.079
-Original Grad: -0.000, -lr * Pred Grad:  0.005, New P: 1.194
iter 30 loss: 0.315
Actual params: [-1.079 ,  1.1942]
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: -1.079
-Original Grad: -0.000, -lr * Pred Grad:  0.004, New P: 1.199
Target params: [1.3344, 1.5708]
iter 0 loss: 0.583
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.019, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.005, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.578
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.043, -lr * Pred Grad:  0.095, New P: -0.277
-Original Grad: 0.010, -lr * Pred Grad:  0.096, New P: 0.200
iter 2 loss: 0.571
Actual params: [-0.2774,  0.1997]
-Original Grad: 0.104, -lr * Pred Grad:  0.088, New P: -0.189
-Original Grad: 0.012, -lr * Pred Grad:  0.097, New P: 0.297
iter 3 loss: 0.563
Actual params: [-0.189 ,  0.2971]
-Original Grad: 0.114, -lr * Pred Grad:  0.092, New P: -0.097
-Original Grad: -0.000, -lr * Pred Grad:  0.079, New P: 0.376
iter 4 loss: 0.554
Actual params: [-0.0967,  0.3762]
-Original Grad: 0.102, -lr * Pred Grad:  0.095, New P: -0.002
-Original Grad: 0.009, -lr * Pred Grad:  0.085, New P: 0.461
iter 5 loss: 0.541
Actual params: [-0.0016,  0.4607]
-Original Grad: 0.149, -lr * Pred Grad:  0.097, New P: 0.095
-Original Grad: 0.020, -lr * Pred Grad:  0.088, New P: 0.548
iter 6 loss: 0.524
Actual params: [0.0951, 0.5484]
-Original Grad: 0.166, -lr * Pred Grad:  0.098, New P: 0.193
-Original Grad: 0.019, -lr * Pred Grad:  0.092, New P: 0.640
iter 7 loss: 0.498
Actual params: [0.1934, 0.6402]
-Original Grad: 0.292, -lr * Pred Grad:  0.096, New P: 0.290
-Original Grad: 0.009, -lr * Pred Grad:  0.091, New P: 0.732
iter 8 loss: 0.464
Actual params: [0.2898, 0.7316]
-Original Grad: 0.362, -lr * Pred Grad:  0.097, New P: 0.386
-Original Grad: 0.031, -lr * Pred Grad:  0.093, New P: 0.825
iter 9 loss: 0.419
Actual params: [0.3864, 0.8248]
-Original Grad: 0.523, -lr * Pred Grad:  0.096, New P: 0.482
-Original Grad: 0.049, -lr * Pred Grad:  0.092, New P: 0.917
iter 10 loss: 0.358
Actual params: [0.4822, 0.9169]
-Original Grad: 0.607, -lr * Pred Grad:  0.097, New P: 0.579
-Original Grad: 0.143, -lr * Pred Grad:  0.079, New P: 0.996
iter 11 loss: 0.296
Actual params: [0.5793, 0.9957]
-Original Grad: 0.432, -lr * Pred Grad:  0.099, New P: 0.679
-Original Grad: 0.120, -lr * Pred Grad:  0.086, New P: 1.081
iter 12 loss: 0.238
Actual params: [0.6787, 1.0812]
-Original Grad: 0.413, -lr * Pred Grad:  0.101, New P: 0.780
-Original Grad: 0.134, -lr * Pred Grad:  0.091, New P: 1.172
iter 13 loss: 0.189
Actual params: [0.7797, 1.1721]
-Original Grad: 0.368, -lr * Pred Grad:  0.102, New P: 0.881
-Original Grad: 0.138, -lr * Pred Grad:  0.095, New P: 1.267
iter 14 loss: 0.146
Actual params: [0.8814, 1.2674]
-Original Grad: 0.253, -lr * Pred Grad:  0.100, New P: 0.981
-Original Grad: 0.073, -lr * Pred Grad:  0.096, New P: 1.363
iter 15 loss: 0.110
Actual params: [0.9814, 1.3632]
-Original Grad: 0.360, -lr * Pred Grad:  0.101, New P: 1.082
-Original Grad: 0.063, -lr * Pred Grad:  0.095, New P: 1.458
iter 16 loss: 0.081
Actual params: [1.0822, 1.4585]
-Original Grad: 0.299, -lr * Pred Grad:  0.100, New P: 1.183
-Original Grad: 0.047, -lr * Pred Grad:  0.093, New P: 1.552
iter 17 loss: 0.061
Actual params: [1.1826, 1.5517]
-Original Grad: 0.105, -lr * Pred Grad:  0.095, New P: 1.277
-Original Grad: 0.020, -lr * Pred Grad:  0.088, New P: 1.639
iter 18 loss: 0.048
Actual params: [1.2774, 1.6394]
-Original Grad: 0.119, -lr * Pred Grad:  0.090, New P: 1.368
-Original Grad: 0.015, -lr * Pred Grad:  0.082, New P: 1.722
iter 19 loss: 0.039
Actual params: [1.3677, 1.7215]
-Original Grad: 0.000, -lr * Pred Grad:  0.082, New P: 1.450
-Original Grad: 0.070, -lr * Pred Grad:  0.084, New P: 1.806
iter 20 loss: 0.034
Actual params: [1.4499, 1.8059]
-Original Grad: 0.060, -lr * Pred Grad:  0.077, New P: 1.527
-Original Grad: -0.006, -lr * Pred Grad:  0.076, New P: 1.882
iter 21 loss: 0.032
Actual params: [1.5269, 1.8817]
-Original Grad: -0.011, -lr * Pred Grad:  0.070, New P: 1.597
-Original Grad: 0.034, -lr * Pred Grad:  0.074, New P: 1.956
iter 22 loss: 0.032
Actual params: [1.5965, 1.956 ]
-Original Grad: -0.047, -lr * Pred Grad:  0.061, New P: 1.658
-Original Grad: 0.043, -lr * Pred Grad:  0.074, New P: 2.030
iter 23 loss: 0.034
Actual params: [1.658 , 2.0303]
-Original Grad: -0.142, -lr * Pred Grad:  0.050, New P: 1.708
-Original Grad: 0.064, -lr * Pred Grad:  0.077, New P: 2.107
iter 24 loss: 0.037
Actual params: [1.708 , 2.1074]
-Original Grad: -0.117, -lr * Pred Grad:  0.041, New P: 1.749
-Original Grad: 0.043, -lr * Pred Grad:  0.077, New P: 2.184
iter 25 loss: 0.041
Actual params: [1.7486, 2.1842]
-Original Grad: -0.120, -lr * Pred Grad:  0.032, New P: 1.781
-Original Grad: -0.009, -lr * Pred Grad:  0.068, New P: 2.253
iter 26 loss: 0.048
Actual params: [1.7806, 2.2525]
-Original Grad: -0.192, -lr * Pred Grad:  0.021, New P: 1.802
-Original Grad: -0.028, -lr * Pred Grad:  0.057, New P: 2.310
iter 27 loss: 0.055
Actual params: [1.8017, 2.3097]
-Original Grad: -0.160, -lr * Pred Grad:  0.012, New P: 1.814
-Original Grad: -0.099, -lr * Pred Grad:  0.033, New P: 2.343
iter 28 loss: 0.061
Actual params: [1.8141, 2.343 ]
-Original Grad: -0.141, -lr * Pred Grad:  0.005, New P: 1.820
-Original Grad: -0.133, -lr * Pred Grad:  0.007, New P: 2.350
iter 29 loss: 0.063
Actual params: [1.8196, 2.3505]
-Original Grad: -0.138, -lr * Pred Grad:  -0.001, New P: 1.819
-Original Grad: -0.132, -lr * Pred Grad:  -0.013, New P: 2.337
iter 30 loss: 0.061
Actual params: [1.8189, 2.3372]
-Original Grad: -0.082, -lr * Pred Grad:  -0.004, New P: 1.815
-Original Grad: -0.109, -lr * Pred Grad:  -0.027, New P: 2.310
Target params: [1.3344, 1.5708]
iter 0 loss: 0.018
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.015, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.004, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.016
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.014, -lr * Pred Grad:  0.100, New P: -0.272
-Original Grad: -0.001, -lr * Pred Grad:  -0.088, New P: -0.185
iter 2 loss: 0.014
Actual params: [-0.2724, -0.1846]
-Original Grad: 0.016, -lr * Pred Grad:  0.100, New P: -0.172
-Original Grad: 0.001, -lr * Pred Grad:  -0.058, New P: -0.242
iter 3 loss: 0.012
Actual params: [-0.1723, -0.2425]
-Original Grad: 0.012, -lr * Pred Grad:  0.099, New P: -0.073
-Original Grad: 0.001, -lr * Pred Grad:  -0.030, New P: -0.273
iter 4 loss: 0.011
Actual params: [-0.0732, -0.2726]
-Original Grad: 0.005, -lr * Pred Grad:  0.092, New P: 0.019
-Original Grad: 0.001, -lr * Pred Grad:  -0.018, New P: -0.290
iter 5 loss: 0.013
Actual params: [ 0.0186, -0.2901]
-Original Grad: -0.033, -lr * Pred Grad:  0.013, New P: 0.032
-Original Grad: 0.003, -lr * Pred Grad:  0.019, New P: -0.271
iter 6 loss: 0.013
Actual params: [ 0.0318, -0.2709]
-Original Grad: -0.049, -lr * Pred Grad:  -0.030, New P: 0.002
-Original Grad: 0.006, -lr * Pred Grad:  0.049, New P: -0.222
iter 7 loss: 0.012
Actual params: [ 0.0019, -0.2217]
-Original Grad: -0.014, -lr * Pred Grad:  -0.036, New P: -0.034
-Original Grad: 0.001, -lr * Pred Grad:  0.049, New P: -0.172
iter 8 loss: 0.011
Actual params: [-0.0344, -0.1723]
-Original Grad: -0.018, -lr * Pred Grad:  -0.044, New P: -0.078
-Original Grad: 0.002, -lr * Pred Grad:  0.055, New P: -0.117
iter 9 loss: 0.011
Actual params: [-0.0783, -0.1169]
-Original Grad: -0.003, -lr * Pred Grad:  -0.041, New P: -0.119
-Original Grad: 0.001, -lr * Pred Grad:  0.053, New P: -0.064
iter 10 loss: 0.012
Actual params: [-0.1195, -0.064 ]
-Original Grad: 0.010, -lr * Pred Grad:  -0.030, New P: -0.149
-Original Grad: -0.003, -lr * Pred Grad:  0.027, New P: -0.038
iter 11 loss: 0.012
Actual params: [-0.1494, -0.0375]
-Original Grad: 0.012, -lr * Pred Grad:  -0.018, New P: -0.168
-Original Grad: -0.002, -lr * Pred Grad:  0.010, New P: -0.027
iter 12 loss: 0.012
Actual params: [-0.1678, -0.0273]
-Original Grad: 0.014, -lr * Pred Grad:  -0.007, New P: -0.175
-Original Grad: -0.005, -lr * Pred Grad:  -0.015, New P: -0.043
iter 13 loss: 0.012
Actual params: [-0.1746, -0.0427]
-Original Grad: 0.010, -lr * Pred Grad:  0.000, New P: -0.174
-Original Grad: -0.002, -lr * Pred Grad:  -0.025, New P: -0.067
iter 14 loss: 0.012
Actual params: [-0.1742, -0.0673]
-Original Grad: 0.015, -lr * Pred Grad:  0.010, New P: -0.164
-Original Grad: -0.002, -lr * Pred Grad:  -0.031, New P: -0.098
iter 15 loss: 0.012
Actual params: [-0.1641, -0.0981]
-Original Grad: 0.009, -lr * Pred Grad:  0.015, New P: -0.149
-Original Grad: -0.001, -lr * Pred Grad:  -0.032, New P: -0.130
iter 16 loss: 0.012
Actual params: [-0.1493, -0.1297]
-Original Grad: 0.011, -lr * Pred Grad:  0.020, New P: -0.129
-Original Grad: -0.001, -lr * Pred Grad:  -0.033, New P: -0.163
iter 17 loss: 0.011
Actual params: [-0.129 , -0.1629]
-Original Grad: 0.004, -lr * Pred Grad:  0.021, New P: -0.108
-Original Grad: -0.001, -lr * Pred Grad:  -0.036, New P: -0.199
iter 18 loss: 0.011
Actual params: [-0.1078, -0.199 ]
-Original Grad: 0.007, -lr * Pred Grad:  0.024, New P: -0.084
-Original Grad: -0.002, -lr * Pred Grad:  -0.039, New P: -0.238
iter 19 loss: 0.011
Actual params: [-0.0839, -0.2385]
-Original Grad: 0.005, -lr * Pred Grad:  0.025, New P: -0.059
-Original Grad: 0.002, -lr * Pred Grad:  -0.029, New P: -0.267
iter 20 loss: 0.011
Actual params: [-0.0586, -0.2671]
-Original Grad: 0.005, -lr * Pred Grad:  0.026, New P: -0.032
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -0.295
iter 21 loss: 0.012
Actual params: [-0.0323, -0.2949]
-Original Grad: -0.006, -lr * Pred Grad:  0.020, New P: -0.013
-Original Grad: 0.001, -lr * Pred Grad:  -0.018, New P: -0.313
iter 22 loss: 0.012
Actual params: [-0.0128, -0.3134]
-Original Grad: -0.015, -lr * Pred Grad:  0.007, New P: -0.006
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.330
iter 23 loss: 0.012
Actual params: [-0.0055, -0.3303]
-Original Grad: -0.038, -lr * Pred Grad:  -0.017, New P: -0.023
-Original Grad: 0.001, -lr * Pred Grad:  -0.009, New P: -0.339
iter 24 loss: 0.012
Actual params: [-0.0228, -0.3393]
-Original Grad: -0.015, -lr * Pred Grad:  -0.025, New P: -0.047
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -0.346
iter 25 loss: 0.011
Actual params: [-0.0474, -0.3457]
-Original Grad: -0.002, -lr * Pred Grad:  -0.024, New P: -0.071
-Original Grad: 0.001, -lr * Pred Grad:  -0.002, New P: -0.348
iter 26 loss: 0.011
Actual params: [-0.0712, -0.3481]
-Original Grad: 0.001, -lr * Pred Grad:  -0.021, New P: -0.092
-Original Grad: 0.001, -lr * Pred Grad:  0.004, New P: -0.344
iter 27 loss: 0.011
Actual params: [-0.0923, -0.3443]
-Original Grad: 0.002, -lr * Pred Grad:  -0.018, New P: -0.110
-Original Grad: 0.002, -lr * Pred Grad:  0.013, New P: -0.331
iter 28 loss: 0.011
Actual params: [-0.1101, -0.3311]
-Original Grad: 0.004, -lr * Pred Grad:  -0.014, New P: -0.124
-Original Grad: 0.002, -lr * Pred Grad:  0.021, New P: -0.310
iter 29 loss: 0.011
Actual params: [-0.1237, -0.3102]
-Original Grad: 0.007, -lr * Pred Grad:  -0.008, New P: -0.132
-Original Grad: 0.002, -lr * Pred Grad:  0.029, New P: -0.282
iter 30 loss: 0.012
Actual params: [-0.1316, -0.2816]
-Original Grad: 0.004, -lr * Pred Grad:  -0.005, New P: -0.137
-Original Grad: 0.001, -lr * Pred Grad:  0.030, New P: -0.251
Target params: [1.3344, 1.5708]
iter 0 loss: 0.612
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.027, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.018, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.605
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.045, -lr * Pred Grad:  0.098, New P: -0.274
-Original Grad: 0.024, -lr * Pred Grad:  0.100, New P: 0.203
iter 2 loss: 0.597
Actual params: [-0.274 ,  0.2034]
-Original Grad: 0.027, -lr * Pred Grad:  0.097, New P: -0.177
-Original Grad: 0.012, -lr * Pred Grad:  0.095, New P: 0.298
iter 3 loss: 0.587
Actual params: [-0.1774,  0.2984]
-Original Grad: 0.057, -lr * Pred Grad:  0.097, New P: -0.080
-Original Grad: 0.018, -lr * Pred Grad:  0.096, New P: 0.395
iter 4 loss: 0.573
Actual params: [-0.08  ,  0.3948]
-Original Grad: 0.071, -lr * Pred Grad:  0.098, New P: 0.018
-Original Grad: 0.017, -lr * Pred Grad:  0.097, New P: 0.492
iter 5 loss: 0.556
Actual params: [0.0179, 0.4918]
-Original Grad: 0.098, -lr * Pred Grad:  0.098, New P: 0.115
-Original Grad: 0.017, -lr * Pred Grad:  0.097, New P: 0.589
iter 6 loss: 0.537
Actual params: [0.1155, 0.5891]
-Original Grad: 0.179, -lr * Pred Grad:  0.093, New P: 0.208
-Original Grad: -0.002, -lr * Pred Grad:  0.083, New P: 0.672
iter 7 loss: 0.515
Actual params: [0.2085, 0.6718]
-Original Grad: 0.239, -lr * Pred Grad:  0.093, New P: 0.301
-Original Grad: -0.023, -lr * Pred Grad:  0.042, New P: 0.713
iter 8 loss: 0.486
Actual params: [0.3011, 0.7135]
-Original Grad: 0.171, -lr * Pred Grad:  0.095, New P: 0.397
-Original Grad: -0.023, -lr * Pred Grad:  0.013, New P: 0.727
iter 9 loss: 0.446
Actual params: [0.3965, 0.7269]
-Original Grad: 0.514, -lr * Pred Grad:  0.089, New P: 0.486
-Original Grad: 0.112, -lr * Pred Grad:  0.049, New P: 0.776
iter 10 loss: 0.397
Actual params: [0.4858, 0.7757]
-Original Grad: 0.247, -lr * Pred Grad:  0.092, New P: 0.578
-Original Grad: 0.067, -lr * Pred Grad:  0.061, New P: 0.837
iter 11 loss: 0.354
Actual params: [0.5778, 0.8369]
-Original Grad: 0.327, -lr * Pred Grad:  0.095, New P: 0.673
-Original Grad: 0.047, -lr * Pred Grad:  0.067, New P: 0.904
iter 12 loss: 0.316
Actual params: [0.6732, 0.9043]
-Original Grad: 0.285, -lr * Pred Grad:  0.098, New P: 0.771
-Original Grad: 0.061, -lr * Pred Grad:  0.075, New P: 0.979
iter 13 loss: 0.275
Actual params: [0.7708, 0.9789]
-Original Grad: 0.276, -lr * Pred Grad:  0.099, New P: 0.870
-Original Grad: 0.072, -lr * Pred Grad:  0.081, New P: 1.060
iter 14 loss: 0.232
Actual params: [0.8698, 1.06  ]
-Original Grad: 0.246, -lr * Pred Grad:  0.100, New P: 0.969
-Original Grad: 0.036, -lr * Pred Grad:  0.082, New P: 1.142
iter 15 loss: 0.195
Actual params: [0.9693, 1.1418]
-Original Grad: 0.239, -lr * Pred Grad:  0.100, New P: 1.069
-Original Grad: 0.035, -lr * Pred Grad:  0.082, New P: 1.224
iter 16 loss: 0.166
Actual params: [1.0692, 1.2238]
-Original Grad: 0.070, -lr * Pred Grad:  0.094, New P: 1.163
-Original Grad: 0.058, -lr * Pred Grad:  0.086, New P: 1.310
iter 17 loss: 0.148
Actual params: [1.1632, 1.3096]
-Original Grad: 0.026, -lr * Pred Grad:  0.087, New P: 1.250
-Original Grad: 0.022, -lr * Pred Grad:  0.083, New P: 1.393
iter 18 loss: 0.141
Actual params: [1.2501, 1.3927]
-Original Grad: -0.088, -lr * Pred Grad:  0.074, New P: 1.324
-Original Grad: 0.082, -lr * Pred Grad:  0.089, New P: 1.482
iter 19 loss: 0.143
Actual params: [1.3238, 1.4818]
-Original Grad: -0.221, -lr * Pred Grad:  0.053, New P: 1.377
-Original Grad: 0.094, -lr * Pred Grad:  0.095, New P: 1.576
iter 20 loss: 0.149
Actual params: [1.3773, 1.5764]
-Original Grad: -0.232, -lr * Pred Grad:  0.035, New P: 1.412
-Original Grad: 0.088, -lr * Pred Grad:  0.099, New P: 1.675
iter 21 loss: 0.161
Actual params: [1.4124, 1.6752]
-Original Grad: -0.236, -lr * Pred Grad:  0.019, New P: 1.431
-Original Grad: -0.048, -lr * Pred Grad:  0.078, New P: 1.754
iter 22 loss: 0.179
Actual params: [1.4312, 1.7535]
-Original Grad: -0.089, -lr * Pred Grad:  0.012, New P: 1.444
-Original Grad: -0.230, -lr * Pred Grad:  0.017, New P: 1.771
iter 23 loss: 0.186
Actual params: [1.4437, 1.7705]
-Original Grad: -0.087, -lr * Pred Grad:  0.007, New P: 1.451
-Original Grad: -0.160, -lr * Pred Grad:  -0.009, New P: 1.762
iter 24 loss: 0.185
Actual params: [1.4505, 1.7619]
-Original Grad: -0.160, -lr * Pred Grad:  -0.002, New P: 1.448
-Original Grad: -0.183, -lr * Pred Grad:  -0.031, New P: 1.731
iter 25 loss: 0.179
Actual params: [1.4483, 1.7314]
-Original Grad: -0.106, -lr * Pred Grad:  -0.008, New P: 1.441
-Original Grad: -0.099, -lr * Pred Grad:  -0.040, New P: 1.692
iter 26 loss: 0.172
Actual params: [1.4406, 1.6919]
-Original Grad: -0.234, -lr * Pred Grad:  -0.019, New P: 1.422
-Original Grad: -0.058, -lr * Pred Grad:  -0.043, New P: 1.649
iter 27 loss: 0.163
Actual params: [1.4217, 1.6489]
-Original Grad: -0.259, -lr * Pred Grad:  -0.030, New P: 1.392
-Original Grad: 0.029, -lr * Pred Grad:  -0.035, New P: 1.614
iter 28 loss: 0.152
Actual params: [1.3917, 1.6136]
-Original Grad: -0.319, -lr * Pred Grad:  -0.042, New P: 1.350
-Original Grad: 0.031, -lr * Pred Grad:  -0.028, New P: 1.585
iter 29 loss: 0.144
Actual params: [1.3497, 1.5855]
-Original Grad: -0.048, -lr * Pred Grad:  -0.041, New P: 1.309
-Original Grad: -0.054, -lr * Pred Grad:  -0.032, New P: 1.553
iter 30 loss: 0.141
Actual params: [1.309, 1.553]
-Original Grad: -0.013, -lr * Pred Grad:  -0.038, New P: 1.271
-Original Grad: -0.036, -lr * Pred Grad:  -0.034, New P: 1.519
Target params: [1.3344, 1.5708]
iter 0 loss: 0.082
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.039, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.030, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.076
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.015, -lr * Pred Grad:  -0.090, New P: -0.662
-Original Grad: 0.014, -lr * Pred Grad:  0.092, New P: 0.196
iter 2 loss: 0.073
Actual params: [-0.662,  0.196]
-Original Grad: -0.009, -lr * Pred Grad:  -0.082, New P: -0.744
-Original Grad: 0.010, -lr * Pred Grad:  0.086, New P: 0.282
iter 3 loss: 0.071
Actual params: [-0.7437,  0.2823]
-Original Grad: -0.005, -lr * Pred Grad:  -0.073, New P: -0.817
-Original Grad: 0.005, -lr * Pred Grad:  0.079, New P: 0.361
iter 4 loss: 0.070
Actual params: [-0.817 ,  0.3608]
-Original Grad: -0.003, -lr * Pred Grad:  -0.066, New P: -0.882
-Original Grad: 0.003, -lr * Pred Grad:  0.071, New P: 0.432
iter 5 loss: 0.070
Actual params: [-0.8825,  0.4317]
-Original Grad: -0.002, -lr * Pred Grad:  -0.058, New P: -0.941
-Original Grad: 0.002, -lr * Pred Grad:  0.064, New P: 0.495
iter 6 loss: 0.070
Actual params: [-0.941 ,  0.4953]
-Original Grad: -0.001, -lr * Pred Grad:  -0.052, New P: -0.993
-Original Grad: 0.001, -lr * Pred Grad:  0.057, New P: 0.552
iter 7 loss: 0.070
Actual params: [-0.993,  0.552]
-Original Grad: -0.001, -lr * Pred Grad:  -0.047, New P: -1.040
-Original Grad: 0.000, -lr * Pred Grad:  0.051, New P: 0.603
iter 8 loss: 0.070
Actual params: [-1.0395,  0.6027]
-Original Grad: -0.000, -lr * Pred Grad:  -0.042, New P: -1.081
-Original Grad: 0.000, -lr * Pred Grad:  0.045, New P: 0.648
iter 9 loss: 0.070
Actual params: [-1.0813,  0.6481]
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -1.119
-Original Grad: 0.000, -lr * Pred Grad:  0.041, New P: 0.689
iter 10 loss: 0.070
Actual params: [-1.1188,  0.689 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -1.153
-Original Grad: 0.000, -lr * Pred Grad:  0.037, New P: 0.726
iter 11 loss: 0.070
Actual params: [-1.1527,  0.7258]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.183
-Original Grad: 0.000, -lr * Pred Grad:  0.033, New P: 0.759
iter 12 loss: 0.070
Actual params: [-1.1833,  0.7592]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.211
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: 0.789
iter 13 loss: 0.070
Actual params: [-1.2111,  0.7894]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.236
-Original Grad: 0.000, -lr * Pred Grad:  0.027, New P: 0.817
iter 14 loss: 0.070
Actual params: [-1.2363,  0.8168]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.259
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.842
iter 15 loss: 0.070
Actual params: [-1.2592,  0.8417]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.280
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.864
iter 16 loss: 0.070
Actual params: [-1.2801,  0.8643]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.299
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.885
iter 17 loss: 0.070
Actual params: [-1.2991,  0.885 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.316
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.904
iter 18 loss: 0.070
Actual params: [-1.3163,  0.9037]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.332
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.921
iter 19 loss: 0.070
Actual params: [-1.3321,  0.9209]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.346
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.936
iter 20 loss: 0.070
Actual params: [-1.3464,  0.9365]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.360
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.951
iter 21 loss: 0.070
Actual params: [-1.3595,  0.9507]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.371
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.964
iter 22 loss: 0.070
Actual params: [-1.3715,  0.9637]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.382
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.976
iter 23 loss: 0.070
Actual params: [-1.3823,  0.9756]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.392
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.986
iter 24 loss: 0.070
Actual params: [-1.3923,  0.9865]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.401
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.996
iter 25 loss: 0.070
Actual params: [-1.4014,  0.9964]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.410
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 1.005
iter 26 loss: 0.070
Actual params: [-1.4097,  1.0055]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.417
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 1.014
iter 27 loss: 0.070
Actual params: [-1.4172,  1.0138]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.424
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 1.021
iter 28 loss: 0.070
Actual params: [-1.4242,  1.0213]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.430
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 1.028
iter 29 loss: 0.070
Actual params: [-1.4305,  1.0283]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.436
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 1.035
iter 30 loss: 0.070
Actual params: [-1.4363,  1.0346]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.442
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 1.040
Target params: [1.3344, 1.5708]
iter 0 loss: 0.049
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.020, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.010, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.047
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.004, -lr * Pred Grad:  -0.082, New P: -0.654
-Original Grad: 0.003, -lr * Pred Grad:  0.083, New P: 0.187
iter 2 loss: 0.047
Actual params: [-0.6539,  0.1866]
-Original Grad: -0.003, -lr * Pred Grad:  -0.071, New P: -0.725
-Original Grad: 0.001, -lr * Pred Grad:  0.073, New P: 0.259
iter 3 loss: 0.046
Actual params: [-0.7247,  0.2591]
-Original Grad: -0.001, -lr * Pred Grad:  -0.060, New P: -0.785
-Original Grad: 0.000, -lr * Pred Grad:  0.062, New P: 0.321
iter 4 loss: 0.046
Actual params: [-0.785,  0.321]
-Original Grad: -0.000, -lr * Pred Grad:  -0.052, New P: -0.837
-Original Grad: 0.000, -lr * Pred Grad:  0.054, New P: 0.375
iter 5 loss: 0.046
Actual params: [-0.8372,  0.3745]
-Original Grad: -0.000, -lr * Pred Grad:  -0.046, New P: -0.883
-Original Grad: 0.000, -lr * Pred Grad:  0.047, New P: 0.422
iter 6 loss: 0.046
Actual params: [-0.8828,  0.4215]
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -0.923
-Original Grad: 0.000, -lr * Pred Grad:  0.041, New P: 0.463
iter 7 loss: 0.046
Actual params: [-0.923 ,  0.4629]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -0.959
-Original Grad: 0.000, -lr * Pred Grad:  0.037, New P: 0.500
iter 8 loss: 0.046
Actual params: [-0.9586,  0.4996]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -0.990
-Original Grad: 0.000, -lr * Pred Grad:  0.033, New P: 0.532
iter 9 loss: 0.046
Actual params: [-0.9903,  0.5323]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.019
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: 0.562
iter 10 loss: 0.046
Actual params: [-1.0187,  0.5616]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.044
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 0.588
iter 11 loss: 0.046
Actual params: [-1.0441,  0.5878]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.067
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.611
iter 12 loss: 0.046
Actual params: [-1.067 ,  0.6115]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.088
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.633
iter 13 loss: 0.046
Actual params: [-1.0877,  0.6328]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.106
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.652
iter 14 loss: 0.046
Actual params: [-1.1064,  0.6522]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.123
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.670
iter 15 loss: 0.046
Actual params: [-1.1233,  0.6697]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.139
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.686
iter 16 loss: 0.046
Actual params: [-1.1387,  0.6855]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.153
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.700
iter 17 loss: 0.046
Actual params: [-1.1526,  0.6999]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.165
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.713
iter 18 loss: 0.046
Actual params: [-1.1653,  0.7131]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.177
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.725
iter 19 loss: 0.046
Actual params: [-1.1768,  0.725 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.187
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.736
iter 20 loss: 0.046
Actual params: [-1.1873,  0.7358]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.197
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.746
iter 21 loss: 0.046
Actual params: [-1.1969,  0.7457]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.206
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.755
iter 22 loss: 0.046
Actual params: [-1.2056,  0.7547]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.213
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.763
iter 23 loss: 0.046
Actual params: [-1.2135,  0.7629]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.221
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.770
iter 24 loss: 0.046
Actual params: [-1.2207,  0.7704]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.227
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.777
iter 25 loss: 0.046
Actual params: [-1.2273,  0.7772]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.233
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.783
iter 26 loss: 0.046
Actual params: [-1.2333,  0.7834]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.239
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.789
iter 27 loss: 0.046
Actual params: [-1.2388,  0.7891]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.244
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.794
iter 28 loss: 0.046
Actual params: [-1.2438,  0.7943]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.248
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.799
iter 29 loss: 0.046
Actual params: [-1.2483,  0.799 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.252
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.803
iter 30 loss: 0.046
Actual params: [-1.2524,  0.8033]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.256
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.807
Target params: [1.3344, 1.5708]
iter 0 loss: 0.194
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.055, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.027, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.187
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.027, -lr * Pred Grad:  -0.093, New P: -0.665
-Original Grad: 0.019, -lr * Pred Grad:  0.097, New P: 0.201
iter 2 loss: 0.183
Actual params: [-0.6655,  0.2009]
-Original Grad: -0.019, -lr * Pred Grad:  -0.088, New P: -0.753
-Original Grad: 0.016, -lr * Pred Grad:  0.095, New P: 0.296
iter 3 loss: 0.181
Actual params: [-0.7531,  0.2963]
-Original Grad: -0.012, -lr * Pred Grad:  -0.081, New P: -0.834
-Original Grad: 0.012, -lr * Pred Grad:  0.092, New P: 0.388
iter 4 loss: 0.179
Actual params: [-0.8345,  0.3885]
-Original Grad: -0.005, -lr * Pred Grad:  -0.073, New P: -0.907
-Original Grad: 0.005, -lr * Pred Grad:  0.084, New P: 0.473
iter 5 loss: 0.179
Actual params: [-0.9072,  0.4729]
-Original Grad: -0.002, -lr * Pred Grad:  -0.065, New P: -0.972
-Original Grad: 0.003, -lr * Pred Grad:  0.076, New P: 0.549
iter 6 loss: 0.179
Actual params: [-0.9717,  0.5491]
-Original Grad: -0.002, -lr * Pred Grad:  -0.058, New P: -1.030
-Original Grad: 0.002, -lr * Pred Grad:  0.069, New P: 0.618
iter 7 loss: 0.178
Actual params: [-1.0295,  0.6185]
-Original Grad: -0.001, -lr * Pred Grad:  -0.052, New P: -1.081
-Original Grad: 0.001, -lr * Pred Grad:  0.063, New P: 0.681
iter 8 loss: 0.178
Actual params: [-1.0814,  0.6812]
-Original Grad: -0.001, -lr * Pred Grad:  -0.047, New P: -1.128
-Original Grad: 0.001, -lr * Pred Grad:  0.057, New P: 0.738
iter 9 loss: 0.178
Actual params: [-1.128 ,  0.7382]
-Original Grad: -0.000, -lr * Pred Grad:  -0.042, New P: -1.170
-Original Grad: 0.000, -lr * Pred Grad:  0.051, New P: 0.790
iter 10 loss: 0.178
Actual params: [-1.1699,  0.7895]
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -1.208
-Original Grad: 0.000, -lr * Pred Grad:  0.046, New P: 0.836
iter 11 loss: 0.178
Actual params: [-1.2077,  0.836 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -1.242
-Original Grad: 0.000, -lr * Pred Grad:  0.042, New P: 0.878
iter 12 loss: 0.178
Actual params: [-1.242 ,  0.8782]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.273
-Original Grad: 0.000, -lr * Pred Grad:  0.038, New P: 0.916
iter 13 loss: 0.178
Actual params: [-1.273 ,  0.9165]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.301
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: 0.951
iter 14 loss: 0.178
Actual params: [-1.3012,  0.9513]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.327
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: 0.983
iter 15 loss: 0.178
Actual params: [-1.3268,  0.9829]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.350
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: 1.012
iter 16 loss: 0.178
Actual params: [-1.3501,  1.0117]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.371
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 1.038
iter 17 loss: 0.178
Actual params: [-1.3713,  1.038 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.391
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 1.062
iter 18 loss: 0.178
Actual params: [-1.3906,  1.0619]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.408
-Original Grad: 0.000, -lr * Pred Grad:  0.022, New P: 1.084
iter 19 loss: 0.178
Actual params: [-1.4083,  1.0838]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.424
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 1.104
iter 20 loss: 0.178
Actual params: [-1.4243,  1.1037]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.439
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 1.122
iter 21 loss: 0.178
Actual params: [-1.439 ,  1.1219]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.452
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 1.139
iter 22 loss: 0.178
Actual params: [-1.4524,  1.1386]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.465
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 1.154
iter 23 loss: 0.178
Actual params: [-1.4646,  1.1538]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.476
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 1.168
iter 24 loss: 0.178
Actual params: [-1.4758,  1.1677]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.486
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 1.180
iter 25 loss: 0.178
Actual params: [-1.486 ,  1.1804]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.495
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 1.192
iter 26 loss: 0.178
Actual params: [-1.4953,  1.192 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.504
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 1.203
iter 27 loss: 0.178
Actual params: [-1.5038,  1.2026]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.512
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 1.212
iter 28 loss: 0.178
Actual params: [-1.5116,  1.2124]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.519
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 1.221
iter 29 loss: 0.178
Actual params: [-1.5187,  1.2213]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.525
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 1.229
iter 30 loss: 0.178
Actual params: [-1.5252,  1.2294]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.531
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 1.237
Target params: [1.3344, 1.5708]
iter 0 loss: 0.029
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.033, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.003, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.025
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.023, -lr * Pred Grad:  -0.097, New P: -0.670
-Original Grad: 0.004, -lr * Pred Grad:  0.100, New P: 0.204
iter 2 loss: 0.021
Actual params: [-0.6698,  0.2035]
-Original Grad: -0.024, -lr * Pred Grad:  -0.097, New P: -0.767
-Original Grad: 0.007, -lr * Pred Grad:  0.098, New P: 0.302
iter 3 loss: 0.019
Actual params: [-0.7673,  0.3017]
-Original Grad: -0.014, -lr * Pred Grad:  -0.093, New P: -0.861
-Original Grad: 0.005, -lr * Pred Grad:  0.099, New P: 0.401
iter 4 loss: 0.017
Actual params: [-0.8605,  0.4006]
-Original Grad: -0.010, -lr * Pred Grad:  -0.088, New P: -0.948
-Original Grad: 0.005, -lr * Pred Grad:  0.099, New P: 0.500
iter 5 loss: 0.016
Actual params: [-0.9483,  0.5   ]
-Original Grad: -0.005, -lr * Pred Grad:  -0.080, New P: -1.028
-Original Grad: 0.003, -lr * Pred Grad:  0.097, New P: 0.597
iter 6 loss: 0.015
Actual params: [-1.0285,  0.5965]
-Original Grad: -0.002, -lr * Pred Grad:  -0.072, New P: -1.100
-Original Grad: 0.002, -lr * Pred Grad:  0.090, New P: 0.687
iter 7 loss: 0.015
Actual params: [-1.1005,  0.6869]
-Original Grad: -0.002, -lr * Pred Grad:  -0.065, New P: -1.165
-Original Grad: 0.001, -lr * Pred Grad:  0.085, New P: 0.772
iter 8 loss: 0.015
Actual params: [-1.1654,  0.7717]
-Original Grad: -0.001, -lr * Pred Grad:  -0.058, New P: -1.224
-Original Grad: 0.001, -lr * Pred Grad:  0.078, New P: 0.850
iter 9 loss: 0.015
Actual params: [-1.2236,  0.8498]
-Original Grad: -0.000, -lr * Pred Grad:  -0.052, New P: -1.276
-Original Grad: 0.000, -lr * Pred Grad:  0.072, New P: 0.921
iter 10 loss: 0.015
Actual params: [-1.276 ,  0.9213]
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -1.323
-Original Grad: 0.000, -lr * Pred Grad:  0.065, New P: 0.986
iter 11 loss: 0.015
Actual params: [-1.3231,  0.9864]
-Original Grad: -0.000, -lr * Pred Grad:  -0.042, New P: -1.366
-Original Grad: 0.000, -lr * Pred Grad:  0.059, New P: 1.046
iter 12 loss: 0.015
Actual params: [-1.3656,  1.0455]
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -1.404
-Original Grad: 0.000, -lr * Pred Grad:  0.054, New P: 1.099
iter 13 loss: 0.015
Actual params: [-1.404 ,  1.0993]
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -1.439
-Original Grad: 0.000, -lr * Pred Grad:  0.049, New P: 1.148
iter 14 loss: 0.015
Actual params: [-1.4387,  1.1482]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.470
-Original Grad: 0.000, -lr * Pred Grad:  0.044, New P: 1.193
iter 15 loss: 0.015
Actual params: [-1.4701,  1.1926]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.499
-Original Grad: 0.000, -lr * Pred Grad:  0.040, New P: 1.233
iter 16 loss: 0.015
Actual params: [-1.4986,  1.233 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.524
-Original Grad: 0.000, -lr * Pred Grad:  0.037, New P: 1.270
iter 17 loss: 0.015
Actual params: [-1.5244,  1.2698]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.548
-Original Grad: 0.000, -lr * Pred Grad:  0.033, New P: 1.303
iter 18 loss: 0.015
Actual params: [-1.5479,  1.3032]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.569
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: 1.334
iter 19 loss: 0.015
Actual params: [-1.5693,  1.3336]
-Original Grad: 0.000, -lr * Pred Grad:  -0.019, New P: -1.589
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 1.361
iter 20 loss: 0.015
Actual params: [-1.5887,  1.3613]
-Original Grad: 0.000, -lr * Pred Grad:  -0.018, New P: -1.606
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 1.387
iter 21 loss: 0.015
Actual params: [-1.6064,  1.3865]
-Original Grad: 0.000, -lr * Pred Grad:  -0.016, New P: -1.622
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 1.410
iter 22 loss: 0.015
Actual params: [-1.6224,  1.4095]
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -1.637
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 1.430
iter 23 loss: 0.015
Actual params: [-1.6371,  1.4305]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -1.650
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 1.450
iter 24 loss: 0.015
Actual params: [-1.6504,  1.4496]
-Original Grad: 0.000, -lr * Pred Grad:  -0.012, New P: -1.663
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 1.467
iter 25 loss: 0.015
Actual params: [-1.6625,  1.4669]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.674
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 1.483
iter 26 loss: 0.015
Actual params: [-1.6736,  1.4828]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.684
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 1.497
iter 27 loss: 0.015
Actual params: [-1.6836,  1.4972]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.693
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 1.510
iter 28 loss: 0.015
Actual params: [-1.6928,  1.5104]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.701
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 1.522
iter 29 loss: 0.015
Actual params: [-1.7012,  1.5224]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.709
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 1.533
iter 30 loss: 0.015
Actual params: [-1.7087,  1.5333]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.716
-Original Grad: -0.000, -lr * Pred Grad:  0.010, New P: 1.543
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.363
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.002, -lr * Pred Grad:  0.091, New P: -0.281
-Original Grad: 0.000, -lr * Pred Grad:  0.091, New P: 0.195
iter 2 loss: 0.363
Actual params: [-0.2812,  0.195 ]
-Original Grad: 0.005, -lr * Pred Grad:  0.085, New P: -0.196
-Original Grad: 0.001, -lr * Pred Grad:  0.085, New P: 0.280
iter 3 loss: 0.362
Actual params: [-0.1962,  0.2799]
-Original Grad: 0.009, -lr * Pred Grad:  0.085, New P: -0.111
-Original Grad: 0.002, -lr * Pred Grad:  0.084, New P: 0.364
iter 4 loss: 0.360
Actual params: [-0.1109,  0.3639]
-Original Grad: 0.036, -lr * Pred Grad:  0.073, New P: -0.038
-Original Grad: 0.010, -lr * Pred Grad:  0.071, New P: 0.434
iter 5 loss: 0.356
Actual params: [-0.0384,  0.4344]
-Original Grad: 0.072, -lr * Pred Grad:  0.075, New P: 0.037
-Original Grad: 0.022, -lr * Pred Grad:  0.073, New P: 0.508
iter 6 loss: 0.347
Actual params: [0.0367, 0.5075]
-Original Grad: 0.120, -lr * Pred Grad:  0.079, New P: 0.115
-Original Grad: 0.038, -lr * Pred Grad:  0.077, New P: 0.584
iter 7 loss: 0.329
Actual params: [0.1153, 0.5844]
-Original Grad: 0.275, -lr * Pred Grad:  0.076, New P: 0.191
-Original Grad: 0.089, -lr * Pred Grad:  0.075, New P: 0.659
iter 8 loss: 0.301
Actual params: [0.1915, 0.6593]
-Original Grad: 0.321, -lr * Pred Grad:  0.082, New P: 0.274
-Original Grad: 0.102, -lr * Pred Grad:  0.081, New P: 0.741
iter 9 loss: 0.261
Actual params: [0.2736, 0.7408]
-Original Grad: 0.428, -lr * Pred Grad:  0.086, New P: 0.360
-Original Grad: 0.108, -lr * Pred Grad:  0.087, New P: 0.828
iter 10 loss: 0.206
Actual params: [0.36 , 0.828]
-Original Grad: 0.545, -lr * Pred Grad:  0.090, New P: 0.450
-Original Grad: 0.099, -lr * Pred Grad:  0.092, New P: 0.920
iter 11 loss: 0.141
Actual params: [0.45  , 0.9198]
-Original Grad: 0.488, -lr * Pred Grad:  0.094, New P: 0.544
-Original Grad: 0.036, -lr * Pred Grad:  0.090, New P: 1.009
iter 12 loss: 0.099
Actual params: [0.5442, 1.0094]
-Original Grad: 0.170, -lr * Pred Grad:  0.092, New P: 0.636
-Original Grad: 0.004, -lr * Pred Grad:  0.082, New P: 1.091
iter 13 loss: 0.078
Actual params: [0.6362, 1.0911]
-Original Grad: 0.194, -lr * Pred Grad:  0.091, New P: 0.727
-Original Grad: 0.005, -lr * Pred Grad:  0.075, New P: 1.166
iter 14 loss: 0.060
Actual params: [0.7272, 1.1661]
-Original Grad: 0.182, -lr * Pred Grad:  0.090, New P: 0.817
-Original Grad: 0.020, -lr * Pred Grad:  0.072, New P: 1.238
iter 15 loss: 0.046
Actual params: [0.817 , 1.2384]
-Original Grad: 0.140, -lr * Pred Grad:  0.087, New P: 0.904
-Original Grad: -0.025, -lr * Pred Grad:  0.059, New P: 1.298
iter 16 loss: 0.034
Actual params: [0.9043, 1.2977]
-Original Grad: 0.115, -lr * Pred Grad:  0.084, New P: 0.989
-Original Grad: 0.010, -lr * Pred Grad:  0.056, New P: 1.354
iter 17 loss: 0.023
Actual params: [0.9885, 1.3537]
-Original Grad: 0.093, -lr * Pred Grad:  0.081, New P: 1.069
-Original Grad: -0.033, -lr * Pred Grad:  0.042, New P: 1.396
iter 18 loss: 0.020
Actual params: [1.0692, 1.3961]
-Original Grad: 0.055, -lr * Pred Grad:  0.076, New P: 1.145
-Original Grad: -0.051, -lr * Pred Grad:  0.026, New P: 1.422
iter 19 loss: 0.019
Actual params: [1.1452, 1.4219]
-Original Grad: 0.015, -lr * Pred Grad:  0.070, New P: 1.215
-Original Grad: -0.063, -lr * Pred Grad:  0.008, New P: 1.430
iter 20 loss: 0.019
Actual params: [1.2149, 1.4303]
-Original Grad: -0.013, -lr * Pred Grad:  0.063, New P: 1.278
-Original Grad: -0.098, -lr * Pred Grad:  -0.013, New P: 1.417
iter 21 loss: 0.018
Actual params: [1.2778, 1.4169]
-Original Grad: -0.008, -lr * Pred Grad:  0.057, New P: 1.335
-Original Grad: -0.070, -lr * Pred Grad:  -0.026, New P: 1.391
iter 22 loss: 0.017
Actual params: [1.3346, 1.391 ]
-Original Grad: -0.027, -lr * Pred Grad:  0.050, New P: 1.385
-Original Grad: -0.072, -lr * Pred Grad:  -0.037, New P: 1.354
iter 23 loss: 0.017
Actual params: [1.3849, 1.3543]
-Original Grad: -0.028, -lr * Pred Grad:  0.044, New P: 1.429
-Original Grad: -0.058, -lr * Pred Grad:  -0.044, New P: 1.310
iter 24 loss: 0.016
Actual params: [1.4292, 1.3102]
-Original Grad: -0.068, -lr * Pred Grad:  0.037, New P: 1.466
-Original Grad: -0.058, -lr * Pred Grad:  -0.050, New P: 1.260
iter 25 loss: 0.017
Actual params: [1.466 , 1.2598]
-Original Grad: -0.055, -lr * Pred Grad:  0.031, New P: 1.497
-Original Grad: -0.015, -lr * Pred Grad:  -0.049, New P: 1.211
iter 26 loss: 0.018
Actual params: [1.4966, 1.211 ]
-Original Grad: -0.087, -lr * Pred Grad:  0.023, New P: 1.520
-Original Grad: -0.019, -lr * Pred Grad:  -0.048, New P: 1.163
iter 27 loss: 0.019
Actual params: [1.5198, 1.1628]
-Original Grad: -0.096, -lr * Pred Grad:  0.016, New P: 1.536
-Original Grad: -0.003, -lr * Pred Grad:  -0.044, New P: 1.118
iter 28 loss: 0.021
Actual params: [1.5356, 1.1184]
-Original Grad: -0.121, -lr * Pred Grad:  0.008, New P: 1.543
-Original Grad: -0.009, -lr * Pred Grad:  -0.042, New P: 1.076
iter 29 loss: 0.022
Actual params: [1.5434, 1.076 ]
-Original Grad: -0.121, -lr * Pred Grad:  0.000, New P: 1.544
-Original Grad: 0.019, -lr * Pred Grad:  -0.035, New P: 1.041
iter 30 loss: 0.022
Actual params: [1.5439, 1.0414]
-Original Grad: -0.094, -lr * Pred Grad:  -0.005, New P: 1.539
-Original Grad: 0.002, -lr * Pred Grad:  -0.031, New P: 1.010
Target params: [1.3344, 1.5708]
iter 0 loss: 0.829
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.002, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.000, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.829
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.000, -lr * Pred Grad:  -0.078, New P: -0.650
-Original Grad: 0.000, -lr * Pred Grad:  -0.042, New P: -0.139
iter 2 loss: 0.829
Actual params: [-0.6499, -0.1388]
-Original Grad: -0.000, -lr * Pred Grad:  -0.066, New P: -0.716
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -0.148
iter 3 loss: 0.829
Actual params: [-0.7157, -0.1477]
-Original Grad: -0.000, -lr * Pred Grad:  -0.060, New P: -0.776
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -0.148
iter 4 loss: 0.829
Actual params: [-0.7761, -0.1483]
-Original Grad: -0.000, -lr * Pred Grad:  -0.053, New P: -0.829
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.146
iter 5 loss: 0.829
Actual params: [-0.8292, -0.1465]
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -0.877
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: -0.145
iter 6 loss: 0.829
Actual params: [-0.8767, -0.1454]
-Original Grad: -0.000, -lr * Pred Grad:  -0.042, New P: -0.919
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.140
iter 7 loss: 0.829
Actual params: [-0.919 , -0.1401]
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -0.957
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: -0.132
iter 8 loss: 0.829
Actual params: [-0.9568, -0.1319]
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -0.991
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: -0.120
iter 9 loss: 0.829
Actual params: [-0.9906, -0.1202]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -1.021
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: -0.107
iter 10 loss: 0.829
Actual params: [-1.0208, -0.1069]
-Original Grad: 0.000, -lr * Pred Grad:  -0.027, New P: -1.048
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: -0.091
iter 11 loss: 0.829
Actual params: [-1.0477, -0.0912]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.072
-Original Grad: -0.000, -lr * Pred Grad:  0.014, New P: -0.077
iter 12 loss: 0.829
Actual params: [-1.0722, -0.0773]
-Original Grad: 0.000, -lr * Pred Grad:  -0.022, New P: -1.094
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: -0.063
iter 13 loss: 0.829
Actual params: [-1.0941, -0.0632]
-Original Grad: 0.000, -lr * Pred Grad:  -0.020, New P: -1.114
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: -0.049
iter 14 loss: 0.829
Actual params: [-1.1139, -0.049 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.018, New P: -1.132
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: -0.035
iter 15 loss: 0.829
Actual params: [-1.1317, -0.035 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.148
-Original Grad: -0.000, -lr * Pred Grad:  0.011, New P: -0.024
iter 16 loss: 0.829
Actual params: [-1.148 , -0.0243]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.163
-Original Grad: -0.000, -lr * Pred Grad:  0.009, New P: -0.016
iter 17 loss: 0.829
Actual params: [-1.1628, -0.0155]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.176
-Original Grad: -0.000, -lr * Pred Grad:  0.007, New P: -0.008
iter 18 loss: 0.829
Actual params: [-1.1763, -0.0081]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.189
-Original Grad: -0.000, -lr * Pred Grad:  0.006, New P: -0.002
iter 19 loss: 0.829
Actual params: [-1.1887, -0.0016]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.200
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.004
iter 20 loss: 0.829
Actual params: [-1.1999,  0.0042]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.210
-Original Grad: -0.000, -lr * Pred Grad:  0.004, New P: 0.008
iter 21 loss: 0.829
Actual params: [-1.2102,  0.0084]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.220
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.013
iter 22 loss: 0.829
Actual params: [-1.2195,  0.0133]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.228
-Original Grad: -0.000, -lr * Pred Grad:  0.004, New P: 0.017
iter 23 loss: 0.829
Actual params: [-1.228 ,  0.0174]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.236
-Original Grad: -0.000, -lr * Pred Grad:  0.003, New P: 0.020
iter 24 loss: 0.829
Actual params: [-1.2358,  0.0203]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.243
-Original Grad: -0.000, -lr * Pred Grad:  0.003, New P: 0.023
iter 25 loss: 0.829
Actual params: [-1.2429,  0.0229]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.249
-Original Grad: -0.000, -lr * Pred Grad:  0.002, New P: 0.024
iter 26 loss: 0.829
Actual params: [-1.2494,  0.0244]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.255
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: 0.026
iter 27 loss: 0.829
Actual params: [-1.2553,  0.0259]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.261
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: 0.027
iter 28 loss: 0.829
Actual params: [-1.2607,  0.0266]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.266
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: 0.027
iter 29 loss: 0.829
Actual params: [-1.2657,  0.0269]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.270
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.026
iter 30 loss: 0.829
Actual params: [-1.2703,  0.0263]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.274
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.025
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.008, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.005, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.118
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.004, -lr * Pred Grad:  -0.091, New P: -0.663
-Original Grad: -0.002, -lr * Pred Grad:  -0.090, New P: -0.186
iter 2 loss: 0.118
Actual params: [-0.6629, -0.1863]
-Original Grad: -0.001, -lr * Pred Grad:  -0.077, New P: -0.740
-Original Grad: -0.001, -lr * Pred Grad:  -0.077, New P: -0.263
iter 3 loss: 0.118
Actual params: [-0.7398, -0.2632]
-Original Grad: -0.000, -lr * Pred Grad:  -0.065, New P: -0.805
-Original Grad: -0.000, -lr * Pred Grad:  -0.065, New P: -0.328
iter 4 loss: 0.118
Actual params: [-0.8048, -0.3283]
-Original Grad: -0.000, -lr * Pred Grad:  -0.056, New P: -0.861
-Original Grad: -0.000, -lr * Pred Grad:  -0.056, New P: -0.384
iter 5 loss: 0.118
Actual params: [-0.8607, -0.3843]
-Original Grad: -0.000, -lr * Pred Grad:  -0.049, New P: -0.909
-Original Grad: -0.000, -lr * Pred Grad:  -0.049, New P: -0.433
iter 6 loss: 0.118
Actual params: [-0.9095, -0.4331]
-Original Grad: -0.000, -lr * Pred Grad:  -0.043, New P: -0.952
-Original Grad: -0.000, -lr * Pred Grad:  -0.043, New P: -0.476
iter 7 loss: 0.118
Actual params: [-0.9524, -0.476 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -0.991
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -0.514
iter 8 loss: 0.118
Actual params: [-0.9905, -0.5141]
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -1.024
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -0.548
iter 9 loss: 0.118
Actual params: [-1.0245, -0.548 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -1.055
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -0.578
iter 10 loss: 0.118
Actual params: [-1.0549, -0.5784]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.082
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -0.606
iter 11 loss: 0.118
Actual params: [-1.0823, -0.6058]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.107
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -0.630
iter 12 loss: 0.118
Actual params: [-1.107 , -0.6304]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.129
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -0.653
iter 13 loss: 0.118
Actual params: [-1.1293, -0.6526]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.150
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -0.673
iter 14 loss: 0.118
Actual params: [-1.1495, -0.6727]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.168
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -0.691
iter 15 loss: 0.118
Actual params: [-1.1678, -0.6909]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.184
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.707
iter 16 loss: 0.118
Actual params: [-1.1845, -0.7074]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.200
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.722
iter 17 loss: 0.118
Actual params: [-1.1996, -0.7225]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.213
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.736
iter 18 loss: 0.118
Actual params: [-1.2133, -0.7361]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.226
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.749
iter 19 loss: 0.118
Actual params: [-1.2258, -0.7485]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.237
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.760
iter 20 loss: 0.118
Actual params: [-1.2372, -0.7598]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.248
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.770
iter 21 loss: 0.118
Actual params: [-1.2476, -0.7701]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.257
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.779
iter 22 loss: 0.118
Actual params: [-1.2571, -0.7795]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.266
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.788
iter 23 loss: 0.118
Actual params: [-1.2657, -0.788 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.274
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.796
iter 24 loss: 0.118
Actual params: [-1.2736, -0.7958]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.281
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.803
iter 25 loss: 0.118
Actual params: [-1.2808, -0.8029]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.287
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.809
iter 26 loss: 0.118
Actual params: [-1.2874, -0.8094]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.293
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.815
iter 27 loss: 0.118
Actual params: [-1.2934, -0.8153]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.299
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.821
iter 28 loss: 0.118
Actual params: [-1.2988, -0.8206]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.304
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.826
iter 29 loss: 0.118
Actual params: [-1.3038, -0.8256]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.308
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.830
iter 30 loss: 0.118
Actual params: [-1.3084, -0.83  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.313
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.834
Target params: [1.3344, 1.5708]
iter 0 loss: 0.081
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.005, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.003, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.080
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.085, New P: -0.657
-Original Grad: 0.001, -lr * Pred Grad:  0.082, New P: 0.186
iter 2 loss: 0.080
Actual params: [-0.6573,  0.1856]
-Original Grad: -0.001, -lr * Pred Grad:  -0.074, New P: -0.731
-Original Grad: 0.000, -lr * Pred Grad:  0.071, New P: 0.257
iter 3 loss: 0.080
Actual params: [-0.7311,  0.2566]
-Original Grad: -0.000, -lr * Pred Grad:  -0.062, New P: -0.793
-Original Grad: 0.000, -lr * Pred Grad:  0.060, New P: 0.317
iter 4 loss: 0.080
Actual params: [-0.7935,  0.3167]
-Original Grad: -0.000, -lr * Pred Grad:  -0.054, New P: -0.847
-Original Grad: 0.000, -lr * Pred Grad:  0.052, New P: 0.369
iter 5 loss: 0.080
Actual params: [-0.8475,  0.3688]
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -0.894
-Original Grad: 0.000, -lr * Pred Grad:  0.045, New P: 0.414
iter 6 loss: 0.080
Actual params: [-0.8942,  0.414 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.041, New P: -0.935
-Original Grad: 0.000, -lr * Pred Grad:  0.040, New P: 0.454
iter 7 loss: 0.080
Actual params: [-0.9352,  0.4538]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -0.971
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: 0.489
iter 8 loss: 0.080
Actual params: [-0.9714,  0.489 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -1.004
-Original Grad: 0.000, -lr * Pred Grad:  0.031, New P: 0.520
iter 9 loss: 0.080
Actual params: [-1.0037,  0.5203]
-Original Grad: 0.000, -lr * Pred Grad:  -0.029, New P: -1.032
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 0.548
iter 10 loss: 0.080
Actual params: [-1.0325,  0.5483]
-Original Grad: 0.000, -lr * Pred Grad:  -0.026, New P: -1.058
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.573
iter 11 loss: 0.080
Actual params: [-1.0583,  0.5735]
-Original Grad: 0.000, -lr * Pred Grad:  -0.023, New P: -1.081
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.596
iter 12 loss: 0.080
Actual params: [-1.0814,  0.5961]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.102
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.617
iter 13 loss: 0.080
Actual params: [-1.1024,  0.6165]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.121
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.635
iter 14 loss: 0.080
Actual params: [-1.1213,  0.635 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.138
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.652
iter 15 loss: 0.080
Actual params: [-1.1384,  0.6518]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.154
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.667
iter 16 loss: 0.080
Actual params: [-1.154,  0.667]
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -1.168
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.681
iter 17 loss: 0.080
Actual params: [-1.1681,  0.6808]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.181
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.693
iter 18 loss: 0.080
Actual params: [-1.1809,  0.6933]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.193
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.705
iter 19 loss: 0.080
Actual params: [-1.1925,  0.7047]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.203
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.715
iter 20 loss: 0.080
Actual params: [-1.2031,  0.7151]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.213
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.725
iter 21 loss: 0.080
Actual params: [-1.2127,  0.7245]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.221
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.733
iter 22 loss: 0.080
Actual params: [-1.2215,  0.7331]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.229
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.741
iter 23 loss: 0.080
Actual params: [-1.2294,  0.741 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.237
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.748
iter 24 loss: 0.080
Actual params: [-1.2367,  0.7481]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.243
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.755
iter 25 loss: 0.080
Actual params: [-1.2433,  0.7546]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.249
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.761
iter 26 loss: 0.080
Actual params: [-1.2493,  0.7606]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.255
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.766
iter 27 loss: 0.080
Actual params: [-1.2547,  0.766 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.260
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.771
iter 28 loss: 0.080
Actual params: [-1.2597,  0.7709]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.264
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.775
iter 29 loss: 0.080
Actual params: [-1.2642,  0.7754]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.268
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.780
iter 30 loss: 0.080
Actual params: [-1.2684,  0.7795]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.272
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.783
Target params: [1.3344, 1.5708]
iter 0 loss: 0.492
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.040, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.082, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.479
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.058, -lr * Pred Grad:  0.099, New P: -0.273
-Original Grad: 0.116, -lr * Pred Grad:  0.099, New P: 0.203
iter 2 loss: 0.463
Actual params: [-0.2731,  0.2029]
-Original Grad: 0.035, -lr * Pred Grad:  0.097, New P: -0.176
-Original Grad: 0.114, -lr * Pred Grad:  0.100, New P: 0.303
iter 3 loss: 0.443
Actual params: [-0.1758,  0.3029]
-Original Grad: 0.073, -lr * Pred Grad:  0.098, New P: -0.078
-Original Grad: 0.103, -lr * Pred Grad:  0.100, New P: 0.403
iter 4 loss: 0.416
Actual params: [-0.0778,  0.4028]
-Original Grad: 0.128, -lr * Pred Grad:  0.095, New P: 0.017
-Original Grad: 0.210, -lr * Pred Grad:  0.098, New P: 0.501
iter 5 loss: 0.380
Actual params: [0.0172, 0.501 ]
-Original Grad: 0.114, -lr * Pred Grad:  0.097, New P: 0.114
-Original Grad: 0.228, -lr * Pred Grad:  0.099, New P: 0.600
iter 6 loss: 0.328
Actual params: [0.1143, 0.5999]
-Original Grad: 0.236, -lr * Pred Grad:  0.094, New P: 0.208
-Original Grad: 0.214, -lr * Pred Grad:  0.100, New P: 0.700
iter 7 loss: 0.254
Actual params: [0.2079, 0.7001]
-Original Grad: 0.199, -lr * Pred Grad:  0.096, New P: 0.304
-Original Grad: 0.208, -lr * Pred Grad:  0.101, New P: 0.801
iter 8 loss: 0.202
Actual params: [0.3041, 0.8011]
-Original Grad: 0.223, -lr * Pred Grad:  0.098, New P: 0.403
-Original Grad: 0.289, -lr * Pred Grad:  0.102, New P: 0.903
iter 9 loss: 0.155
Actual params: [0.4025, 0.9032]
-Original Grad: 0.120, -lr * Pred Grad:  0.098, New P: 0.500
-Original Grad: 0.138, -lr * Pred Grad:  0.100, New P: 1.003
iter 10 loss: 0.111
Actual params: [0.5001, 1.0033]
-Original Grad: 0.135, -lr * Pred Grad:  0.098, New P: 0.598
-Original Grad: 0.279, -lr * Pred Grad:  0.102, New P: 1.105
iter 11 loss: 0.076
Actual params: [0.5977, 1.105 ]
-Original Grad: 0.036, -lr * Pred Grad:  0.091, New P: 0.689
-Original Grad: 0.140, -lr * Pred Grad:  0.100, New P: 1.205
iter 12 loss: 0.056
Actual params: [0.689 , 1.2047]
-Original Grad: -0.043, -lr * Pred Grad:  0.078, New P: 0.767
-Original Grad: 0.139, -lr * Pred Grad:  0.098, New P: 1.303
iter 13 loss: 0.046
Actual params: [0.7666, 1.3026]
-Original Grad: -0.108, -lr * Pred Grad:  0.057, New P: 0.824
-Original Grad: 0.134, -lr * Pred Grad:  0.096, New P: 1.399
iter 14 loss: 0.043
Actual params: [0.8239, 1.3988]
-Original Grad: -0.070, -lr * Pred Grad:  0.044, New P: 0.868
-Original Grad: 0.014, -lr * Pred Grad:  0.088, New P: 1.487
iter 15 loss: 0.043
Actual params: [0.8683, 1.4869]
-Original Grad: -0.147, -lr * Pred Grad:  0.024, New P: 0.893
-Original Grad: -0.012, -lr * Pred Grad:  0.079, New P: 1.566
iter 16 loss: 0.045
Actual params: [0.8926, 1.5659]
-Original Grad: -0.088, -lr * Pred Grad:  0.013, New P: 0.906
-Original Grad: -0.013, -lr * Pred Grad:  0.071, New P: 1.637
iter 17 loss: 0.047
Actual params: [0.906 , 1.6366]
-Original Grad: -0.100, -lr * Pred Grad:  0.002, New P: 0.908
-Original Grad: -0.056, -lr * Pred Grad:  0.060, New P: 1.697
iter 18 loss: 0.049
Actual params: [0.9084, 1.6965]
-Original Grad: -0.147, -lr * Pred Grad:  -0.011, New P: 0.897
-Original Grad: -0.047, -lr * Pred Grad:  0.051, New P: 1.747
iter 19 loss: 0.050
Actual params: [0.897 , 1.7474]
-Original Grad: -0.086, -lr * Pred Grad:  -0.018, New P: 0.879
-Original Grad: -0.037, -lr * Pred Grad:  0.043, New P: 1.791
iter 20 loss: 0.050
Actual params: [0.8789, 1.7909]
-Original Grad: -0.118, -lr * Pred Grad:  -0.027, New P: 0.852
-Original Grad: -0.060, -lr * Pred Grad:  0.035, New P: 1.826
iter 21 loss: 0.050
Actual params: [0.8521, 1.8259]
-Original Grad: -0.108, -lr * Pred Grad:  -0.034, New P: 0.818
-Original Grad: -0.037, -lr * Pred Grad:  0.029, New P: 1.855
iter 22 loss: 0.049
Actual params: [0.8183, 1.8549]
-Original Grad: -0.029, -lr * Pred Grad:  -0.033, New P: 0.785
-Original Grad: -0.054, -lr * Pred Grad:  0.022, New P: 1.877
iter 23 loss: 0.047
Actual params: [0.7849, 1.8771]
-Original Grad: -0.007, -lr * Pred Grad:  -0.031, New P: 0.754
-Original Grad: -0.077, -lr * Pred Grad:  0.014, New P: 1.891
iter 24 loss: 0.045
Actual params: [0.7539, 1.8913]
-Original Grad: -0.033, -lr * Pred Grad:  -0.031, New P: 0.722
-Original Grad: -0.084, -lr * Pred Grad:  0.006, New P: 1.898
iter 25 loss: 0.042
Actual params: [0.7225, 1.8978]
-Original Grad: -0.072, -lr * Pred Grad:  -0.035, New P: 0.687
-Original Grad: -0.150, -lr * Pred Grad:  -0.006, New P: 1.892
iter 26 loss: 0.038
Actual params: [0.6874, 1.8921]
-Original Grad: -0.145, -lr * Pred Grad:  -0.044, New P: 0.643
-Original Grad: -0.130, -lr * Pred Grad:  -0.015, New P: 1.877
iter 27 loss: 0.032
Actual params: [0.643 , 1.8772]
-Original Grad: -0.134, -lr * Pred Grad:  -0.052, New P: 0.591
-Original Grad: -0.090, -lr * Pred Grad:  -0.020, New P: 1.857
iter 28 loss: 0.026
Actual params: [0.5914, 1.8569]
-Original Grad: 0.016, -lr * Pred Grad:  -0.046, New P: 0.546
-Original Grad: -0.044, -lr * Pred Grad:  -0.022, New P: 1.835
iter 29 loss: 0.022
Actual params: [0.5458, 1.8351]
-Original Grad: -0.040, -lr * Pred Grad:  -0.045, New P: 0.501
-Original Grad: -0.037, -lr * Pred Grad:  -0.023, New P: 1.812
iter 30 loss: 0.019
Actual params: [0.5006, 1.8123]
-Original Grad: 0.001, -lr * Pred Grad:  -0.041, New P: 0.460
-Original Grad: -0.047, -lr * Pred Grad:  -0.024, New P: 1.788
Target params: [1.3344, 1.5708]
iter 0 loss: 0.239
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.011, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.003, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.237
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.028, -lr * Pred Grad:  0.094, New P: -0.278
-Original Grad: -0.002, -lr * Pred Grad:  -0.100, New P: -0.196
iter 2 loss: 0.234
Actual params: [-0.2782, -0.1962]
-Original Grad: 0.026, -lr * Pred Grad:  0.097, New P: -0.181
-Original Grad: 0.003, -lr * Pred Grad:  -0.011, New P: -0.208
iter 3 loss: 0.232
Actual params: [-0.1814, -0.2076]
-Original Grad: 0.030, -lr * Pred Grad:  0.098, New P: -0.083
-Original Grad: 0.010, -lr * Pred Grad:  0.048, New P: -0.160
iter 4 loss: 0.228
Actual params: [-0.0831, -0.1597]
-Original Grad: 0.051, -lr * Pred Grad:  0.097, New P: 0.014
-Original Grad: 0.012, -lr * Pred Grad:  0.068, New P: -0.092
iter 5 loss: 0.222
Actual params: [ 0.0139, -0.0922]
-Original Grad: 0.072, -lr * Pred Grad:  0.096, New P: 0.110
-Original Grad: 0.006, -lr * Pred Grad:  0.072, New P: -0.020
iter 6 loss: 0.213
Actual params: [ 0.1098, -0.02  ]
-Original Grad: 0.065, -lr * Pred Grad:  0.098, New P: 0.208
-Original Grad: 0.003, -lr * Pred Grad:  0.071, New P: 0.051
iter 7 loss: 0.202
Actual params: [0.2076, 0.0514]
-Original Grad: 0.114, -lr * Pred Grad:  0.097, New P: 0.304
-Original Grad: 0.010, -lr * Pred Grad:  0.079, New P: 0.131
iter 8 loss: 0.188
Actual params: [0.3044, 0.1305]
-Original Grad: 0.108, -lr * Pred Grad:  0.099, New P: 0.403
-Original Grad: 0.011, -lr * Pred Grad:  0.085, New P: 0.216
iter 9 loss: 0.174
Actual params: [0.403 , 0.2156]
-Original Grad: 0.127, -lr * Pred Grad:  0.100, New P: 0.503
-Original Grad: 0.017, -lr * Pred Grad:  0.090, New P: 0.305
iter 10 loss: 0.169
Actual params: [0.5032, 0.3054]
-Original Grad: -0.160, -lr * Pred Grad:  0.047, New P: 0.550
-Original Grad: 0.065, -lr * Pred Grad:  0.076, New P: 0.382
iter 11 loss: 0.169
Actual params: [0.5502, 0.3817]
-Original Grad: 0.046, -lr * Pred Grad:  0.049, New P: 0.600
-Original Grad: 0.019, -lr * Pred Grad:  0.079, New P: 0.460
iter 12 loss: 0.170
Actual params: [0.5996, 0.4604]
-Original Grad: -0.050, -lr * Pred Grad:  0.036, New P: 0.635
-Original Grad: 0.021, -lr * Pred Grad:  0.081, New P: 0.542
iter 13 loss: 0.171
Actual params: [0.6351, 0.5417]
-Original Grad: -0.019, -lr * Pred Grad:  0.029, New P: 0.664
-Original Grad: 0.009, -lr * Pred Grad:  0.079, New P: 0.620
iter 14 loss: 0.171
Actual params: [0.6641, 0.6204]
-Original Grad: -0.060, -lr * Pred Grad:  0.016, New P: 0.680
-Original Grad: 0.066, -lr * Pred Grad:  0.086, New P: 0.706
iter 15 loss: 0.166
Actual params: [0.6801, 0.7061]
-Original Grad: -0.026, -lr * Pred Grad:  0.010, New P: 0.690
-Original Grad: 0.048, -lr * Pred Grad:  0.091, New P: 0.797
iter 16 loss: 0.162
Actual params: [0.6903, 0.7972]
-Original Grad: -0.010, -lr * Pred Grad:  0.007, New P: 0.698
-Original Grad: 0.077, -lr * Pred Grad:  0.096, New P: 0.893
iter 17 loss: 0.157
Actual params: [0.6978, 0.8933]
-Original Grad: 0.147, -lr * Pred Grad:  0.028, New P: 0.726
-Original Grad: -0.013, -lr * Pred Grad:  0.082, New P: 0.975
iter 18 loss: 0.153
Actual params: [0.7258, 0.9752]
-Original Grad: 0.022, -lr * Pred Grad:  0.029, New P: 0.754
-Original Grad: 0.029, -lr * Pred Grad:  0.083, New P: 1.059
iter 19 loss: 0.148
Actual params: [0.7545, 1.0586]
-Original Grad: 0.115, -lr * Pred Grad:  0.041, New P: 0.796
-Original Grad: 0.028, -lr * Pred Grad:  0.084, New P: 1.143
iter 20 loss: 0.141
Actual params: [0.7956, 1.1429]
-Original Grad: 0.143, -lr * Pred Grad:  0.054, New P: 0.850
-Original Grad: 0.008, -lr * Pred Grad:  0.080, New P: 1.222
iter 21 loss: 0.134
Actual params: [0.8495, 1.2225]
-Original Grad: 0.116, -lr * Pred Grad:  0.062, New P: 0.912
-Original Grad: 0.016, -lr * Pred Grad:  0.078, New P: 1.300
iter 22 loss: 0.127
Actual params: [0.9116, 1.3003]
-Original Grad: 0.133, -lr * Pred Grad:  0.070, New P: 0.982
-Original Grad: -0.029, -lr * Pred Grad:  0.059, New P: 1.359
iter 23 loss: 0.122
Actual params: [0.9818, 1.3594]
-Original Grad: 0.070, -lr * Pred Grad:  0.072, New P: 1.054
-Original Grad: -0.036, -lr * Pred Grad:  0.040, New P: 1.399
iter 24 loss: 0.118
Actual params: [1.0536, 1.3992]
-Original Grad: 0.047, -lr * Pred Grad:  0.071, New P: 1.125
-Original Grad: -0.035, -lr * Pred Grad:  0.023, New P: 1.422
iter 25 loss: 0.114
Actual params: [1.1245, 1.4224]
-Original Grad: 0.057, -lr * Pred Grad:  0.071, New P: 1.196
-Original Grad: -0.072, -lr * Pred Grad:  -0.004, New P: 1.418
iter 26 loss: 0.112
Actual params: [1.1958, 1.4185]
-Original Grad: -0.005, -lr * Pred Grad:  0.064, New P: 1.260
-Original Grad: -0.051, -lr * Pred Grad:  -0.019, New P: 1.399
iter 27 loss: 0.110
Actual params: [1.26  , 1.3993]
-Original Grad: 0.003, -lr * Pred Grad:  0.059, New P: 1.319
-Original Grad: -0.049, -lr * Pred Grad:  -0.032, New P: 1.368
iter 28 loss: 0.109
Actual params: [1.3189, 1.3677]
-Original Grad: -0.017, -lr * Pred Grad:  0.051, New P: 1.370
-Original Grad: -0.062, -lr * Pred Grad:  -0.045, New P: 1.323
iter 29 loss: 0.109
Actual params: [1.3703, 1.3225]
-Original Grad: -0.047, -lr * Pred Grad:  0.040, New P: 1.411
-Original Grad: -0.056, -lr * Pred Grad:  -0.055, New P: 1.267
iter 30 loss: 0.108
Actual params: [1.4106, 1.2672]
-Original Grad: -0.071, -lr * Pred Grad:  0.027, New P: 1.438
-Original Grad: -0.060, -lr * Pred Grad:  -0.065, New P: 1.202
Target params: [1.3344, 1.5708]
iter 0 loss: 0.065
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.058, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.013, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.057
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.073, -lr * Pred Grad:  0.100, New P: -0.272
-Original Grad: 0.001, -lr * Pred Grad:  -0.059, New P: -0.155
iter 2 loss: 0.048
Actual params: [-0.2724, -0.1552]
-Original Grad: 0.052, -lr * Pred Grad:  0.099, New P: -0.174
-Original Grad: 0.004, -lr * Pred Grad:  -0.026, New P: -0.181
iter 3 loss: 0.041
Actual params: [-0.1738, -0.1815]
-Original Grad: 0.063, -lr * Pred Grad:  0.099, New P: -0.075
-Original Grad: 0.011, -lr * Pred Grad:  0.019, New P: -0.162
iter 4 loss: 0.036
Actual params: [-0.0748, -0.162 ]
-Original Grad: 0.017, -lr * Pred Grad:  0.091, New P: 0.016
-Original Grad: 0.016, -lr * Pred Grad:  0.049, New P: -0.113
iter 5 loss: 0.040
Actual params: [ 0.0157, -0.113 ]
-Original Grad: -0.159, -lr * Pred Grad:  0.007, New P: 0.023
-Original Grad: 0.049, -lr * Pred Grad:  0.065, New P: -0.048
iter 6 loss: 0.039
Actual params: [ 0.023 , -0.0478]
-Original Grad: -0.095, -lr * Pred Grad:  -0.016, New P: 0.007
-Original Grad: 0.047, -lr * Pred Grad:  0.076, New P: 0.028
iter 7 loss: 0.034
Actual params: [0.0073, 0.0284]
-Original Grad: -0.037, -lr * Pred Grad:  -0.022, New P: -0.014
-Original Grad: 0.033, -lr * Pred Grad:  0.082, New P: 0.110
iter 8 loss: 0.032
Actual params: [-0.0144,  0.1101]
-Original Grad: 0.030, -lr * Pred Grad:  -0.013, New P: -0.027
-Original Grad: 0.009, -lr * Pred Grad:  0.078, New P: 0.188
iter 9 loss: 0.033
Actual params: [-0.0271,  0.1879]
-Original Grad: 0.037, -lr * Pred Grad:  -0.003, New P: -0.031
-Original Grad: -0.005, -lr * Pred Grad:  0.067, New P: 0.254
iter 10 loss: 0.033
Actual params: [-0.0305,  0.2544]
-Original Grad: 0.061, -lr * Pred Grad:  0.009, New P: -0.021
-Original Grad: -0.013, -lr * Pred Grad:  0.051, New P: 0.305
iter 11 loss: 0.033
Actual params: [-0.0212,  0.3054]
-Original Grad: 0.076, -lr * Pred Grad:  0.023, New P: 0.001
-Original Grad: -0.013, -lr * Pred Grad:  0.038, New P: 0.343
iter 12 loss: 0.033
Actual params: [0.0013, 0.3432]
-Original Grad: 0.075, -lr * Pred Grad:  0.033, New P: 0.035
-Original Grad: -0.020, -lr * Pred Grad:  0.022, New P: 0.365
iter 13 loss: 0.031
Actual params: [0.0345, 0.3648]
-Original Grad: 0.043, -lr * Pred Grad:  0.037, New P: 0.072
-Original Grad: -0.009, -lr * Pred Grad:  0.014, New P: 0.379
iter 14 loss: 0.030
Actual params: [0.072 , 0.3793]
-Original Grad: 0.023, -lr * Pred Grad:  0.038, New P: 0.110
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 0.393
iter 15 loss: 0.029
Actual params: [0.1101, 0.393 ]
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: 0.145
-Original Grad: 0.006, -lr * Pred Grad:  0.016, New P: 0.409
iter 16 loss: 0.029
Actual params: [0.1447, 0.4086]
-Original Grad: -0.042, -lr * Pred Grad:  0.023, New P: 0.168
-Original Grad: 0.027, -lr * Pred Grad:  0.029, New P: 0.437
iter 17 loss: 0.029
Actual params: [0.168 , 0.4373]
-Original Grad: -0.048, -lr * Pred Grad:  0.012, New P: 0.180
-Original Grad: 0.030, -lr * Pred Grad:  0.041, New P: 0.478
iter 18 loss: 0.029
Actual params: [0.1801, 0.478 ]
-Original Grad: -0.016, -lr * Pred Grad:  0.008, New P: 0.188
-Original Grad: 0.016, -lr * Pred Grad:  0.045, New P: 0.523
iter 19 loss: 0.028
Actual params: [0.1882, 0.5228]
-Original Grad: -0.046, -lr * Pred Grad:  -0.001, New P: 0.187
-Original Grad: 0.028, -lr * Pred Grad:  0.053, New P: 0.576
iter 20 loss: 0.028
Actual params: [0.1869, 0.5762]
-Original Grad: 0.006, -lr * Pred Grad:  0.000, New P: 0.187
-Original Grad: 0.004, -lr * Pred Grad:  0.051, New P: 0.627
iter 21 loss: 0.028
Actual params: [0.1869, 0.6271]
-Original Grad: 0.025, -lr * Pred Grad:  0.005, New P: 0.192
-Original Grad: -0.006, -lr * Pred Grad:  0.043, New P: 0.670
iter 22 loss: 0.028
Actual params: [0.1916, 0.6704]
-Original Grad: 0.028, -lr * Pred Grad:  0.009, New P: 0.201
-Original Grad: -0.005, -lr * Pred Grad:  0.037, New P: 0.707
iter 23 loss: 0.028
Actual params: [0.201 , 0.7071]
-Original Grad: 0.033, -lr * Pred Grad:  0.015, New P: 0.216
-Original Grad: -0.007, -lr * Pred Grad:  0.030, New P: 0.737
iter 24 loss: 0.027
Actual params: [0.2157, 0.7368]
-Original Grad: 0.023, -lr * Pred Grad:  0.018, New P: 0.233
-Original Grad: -0.003, -lr * Pred Grad:  0.025, New P: 0.762
iter 25 loss: 0.027
Actual params: [0.2334, 0.762 ]
-Original Grad: 0.040, -lr * Pred Grad:  0.024, New P: 0.257
-Original Grad: -0.011, -lr * Pred Grad:  0.017, New P: 0.779
iter 26 loss: 0.027
Actual params: [0.257, 0.779]
-Original Grad: 0.027, -lr * Pred Grad:  0.027, New P: 0.284
-Original Grad: -0.007, -lr * Pred Grad:  0.012, New P: 0.791
iter 27 loss: 0.026
Actual params: [0.2835, 0.7905]
-Original Grad: 0.003, -lr * Pred Grad:  0.025, New P: 0.308
-Original Grad: 0.007, -lr * Pred Grad:  0.014, New P: 0.805
iter 28 loss: 0.026
Actual params: [0.3082, 0.8049]
-Original Grad: -0.000, -lr * Pred Grad:  0.022, New P: 0.331
-Original Grad: 0.020, -lr * Pred Grad:  0.024, New P: 0.829
iter 29 loss: 0.026
Actual params: [0.3306, 0.8289]
-Original Grad: -0.010, -lr * Pred Grad:  0.018, New P: 0.349
-Original Grad: 0.015, -lr * Pred Grad:  0.030, New P: 0.858
iter 30 loss: 0.026
Actual params: [0.3489, 0.8585]
-Original Grad: -0.003, -lr * Pred Grad:  0.016, New P: 0.365
-Original Grad: 0.011, -lr * Pred Grad:  0.033, New P: 0.891
Target params: [1.3344, 1.5708]
iter 0 loss: 0.575
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.064, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.098, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.562
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.034, -lr * Pred Grad:  -0.094, New P: -0.667
-Original Grad: 0.072, -lr * Pred Grad:  0.098, New P: 0.202
iter 2 loss: 0.553
Actual params: [-0.6666,  0.2016]
-Original Grad: -0.017, -lr * Pred Grad:  -0.086, New P: -0.752
-Original Grad: 0.057, -lr * Pred Grad:  0.096, New P: 0.297
iter 3 loss: 0.547
Actual params: [-0.7524,  0.2974]
-Original Grad: -0.006, -lr * Pred Grad:  -0.075, New P: -0.827
-Original Grad: 0.054, -lr * Pred Grad:  0.094, New P: 0.392
iter 4 loss: 0.543
Actual params: [-0.8269,  0.3918]
-Original Grad: 0.004, -lr * Pred Grad:  -0.060, New P: -0.887
-Original Grad: 0.028, -lr * Pred Grad:  0.089, New P: 0.480
iter 5 loss: 0.542
Actual params: [-0.8867,  0.4805]
-Original Grad: 0.012, -lr * Pred Grad:  -0.043, New P: -0.930
-Original Grad: 0.017, -lr * Pred Grad:  0.082, New P: 0.562
iter 6 loss: 0.541
Actual params: [-0.9296,  0.5623]
-Original Grad: 0.015, -lr * Pred Grad:  -0.027, New P: -0.956
-Original Grad: 0.008, -lr * Pred Grad:  0.074, New P: 0.636
iter 7 loss: 0.542
Actual params: [-0.9561,  0.6364]
-Original Grad: 0.012, -lr * Pred Grad:  -0.015, New P: -0.971
-Original Grad: 0.001, -lr * Pred Grad:  0.066, New P: 0.702
iter 8 loss: 0.542
Actual params: [-0.9714,  0.7021]
-Original Grad: 0.012, -lr * Pred Grad:  -0.006, New P: -0.977
-Original Grad: -0.001, -lr * Pred Grad:  0.058, New P: 0.760
iter 9 loss: 0.542
Actual params: [-0.9772,  0.76  ]
-Original Grad: 0.012, -lr * Pred Grad:  0.002, New P: -0.975
-Original Grad: -0.003, -lr * Pred Grad:  0.051, New P: 0.811
iter 10 loss: 0.542
Actual params: [-0.9751,  0.8109]
-Original Grad: 0.014, -lr * Pred Grad:  0.010, New P: -0.965
-Original Grad: -0.006, -lr * Pred Grad:  0.044, New P: 0.855
iter 11 loss: 0.542
Actual params: [-0.965 ,  0.8546]
-Original Grad: 0.012, -lr * Pred Grad:  0.016, New P: -0.949
-Original Grad: -0.006, -lr * Pred Grad:  0.037, New P: 0.892
iter 12 loss: 0.542
Actual params: [-0.9489,  0.8921]
-Original Grad: 0.011, -lr * Pred Grad:  0.021, New P: -0.928
-Original Grad: -0.005, -lr * Pred Grad:  0.032, New P: 0.924
iter 13 loss: 0.542
Actual params: [-0.9279,  0.9242]
-Original Grad: 0.015, -lr * Pred Grad:  0.027, New P: -0.901
-Original Grad: -0.008, -lr * Pred Grad:  0.026, New P: 0.951
iter 14 loss: 0.542
Actual params: [-0.9005,  0.9506]
-Original Grad: 0.012, -lr * Pred Grad:  0.031, New P: -0.869
-Original Grad: -0.006, -lr * Pred Grad:  0.022, New P: 0.973
iter 15 loss: 0.542
Actual params: [-0.8691,  0.9725]
-Original Grad: 0.013, -lr * Pred Grad:  0.035, New P: -0.834
-Original Grad: -0.006, -lr * Pred Grad:  0.018, New P: 0.990
iter 16 loss: 0.541
Actual params: [-0.8337,  0.9903]
-Original Grad: 0.015, -lr * Pred Grad:  0.040, New P: -0.794
-Original Grad: -0.008, -lr * Pred Grad:  0.014, New P: 1.004
iter 17 loss: 0.541
Actual params: [-0.7938,  1.0038]
-Original Grad: 0.019, -lr * Pred Grad:  0.046, New P: -0.748
-Original Grad: -0.010, -lr * Pred Grad:  0.009, New P: 1.013
iter 18 loss: 0.540
Actual params: [-0.7477,  1.0127]
-Original Grad: 0.013, -lr * Pred Grad:  0.049, New P: -0.699
-Original Grad: -0.007, -lr * Pred Grad:  0.006, New P: 1.018
iter 19 loss: 0.539
Actual params: [-0.6991,  1.0185]
-Original Grad: 0.021, -lr * Pred Grad:  0.055, New P: -0.645
-Original Grad: -0.011, -lr * Pred Grad:  0.002, New P: 1.020
iter 20 loss: 0.538
Actual params: [-0.6446,  1.02  ]
-Original Grad: 0.029, -lr * Pred Grad:  0.063, New P: -0.582
-Original Grad: -0.015, -lr * Pred Grad:  -0.004, New P: 1.016
iter 21 loss: 0.535
Actual params: [-0.5818,  1.0163]
-Original Grad: 0.032, -lr * Pred Grad:  0.070, New P: -0.511
-Original Grad: -0.017, -lr * Pred Grad:  -0.009, New P: 1.007
iter 22 loss: 0.532
Actual params: [-0.5113,  1.0073]
-Original Grad: 0.062, -lr * Pred Grad:  0.082, New P: -0.429
-Original Grad: -0.031, -lr * Pred Grad:  -0.018, New P: 0.989
iter 23 loss: 0.526
Actual params: [-0.4293,  0.9889]
-Original Grad: 0.079, -lr * Pred Grad:  0.092, New P: -0.338
-Original Grad: -0.034, -lr * Pred Grad:  -0.028, New P: 0.961
iter 24 loss: 0.517
Actual params: [-0.3377,  0.9612]
-Original Grad: 0.103, -lr * Pred Grad:  0.099, New P: -0.239
-Original Grad: -0.023, -lr * Pred Grad:  -0.033, New P: 0.928
iter 25 loss: 0.506
Actual params: [-0.2388,  0.9284]
-Original Grad: 0.122, -lr * Pred Grad:  0.105, New P: -0.134
-Original Grad: -0.017, -lr * Pred Grad:  -0.036, New P: 0.893
iter 26 loss: 0.493
Actual params: [-0.1339,  0.8928]
-Original Grad: 0.133, -lr * Pred Grad:  0.110, New P: -0.024
-Original Grad: 0.054, -lr * Pred Grad:  -0.013, New P: 0.880
iter 27 loss: 0.480
Actual params: [-0.0237,  0.8795]
-Original Grad: 0.129, -lr * Pred Grad:  0.115, New P: 0.091
-Original Grad: 0.076, -lr * Pred Grad:  0.012, New P: 0.891
iter 28 loss: 0.460
Actual params: [0.0909, 0.8912]
-Original Grad: 0.160, -lr * Pred Grad:  0.119, New P: 0.209
-Original Grad: 0.109, -lr * Pred Grad:  0.038, New P: 0.929
iter 29 loss: 0.430
Actual params: [0.2094, 0.9288]
-Original Grad: 0.253, -lr * Pred Grad:  0.120, New P: 0.329
-Original Grad: 0.148, -lr * Pred Grad:  0.060, New P: 0.989
iter 30 loss: 0.385
Actual params: [0.3294, 0.9893]
-Original Grad: 0.286, -lr * Pred Grad:  0.123, New P: 0.452
-Original Grad: 0.190, -lr * Pred Grad:  0.078, New P: 1.068
Target params: [1.3344, 1.5708]
iter 0 loss: 0.309
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.309
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.079, New P: -0.651
-Original Grad: 0.000, -lr * Pred Grad:  0.097, New P: 0.200
iter 2 loss: 0.309
Actual params: [-0.6509,  0.2004]
-Original Grad: 0.000, -lr * Pred Grad:  -0.058, New P: -0.709
-Original Grad: 0.000, -lr * Pred Grad:  0.084, New P: 0.285
iter 3 loss: 0.309
Actual params: [-0.7092,  0.2847]
-Original Grad: 0.000, -lr * Pred Grad:  -0.036, New P: -0.745
-Original Grad: 0.000, -lr * Pred Grad:  0.086, New P: 0.370
iter 4 loss: 0.309
Actual params: [-0.7448,  0.3702]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -0.758
-Original Grad: 0.000, -lr * Pred Grad:  0.089, New P: 0.459
iter 5 loss: 0.309
Actual params: [-0.7577,  0.4593]
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: -0.741
-Original Grad: 0.000, -lr * Pred Grad:  0.092, New P: 0.552
iter 6 loss: 0.309
Actual params: [-0.7415,  0.5518]
-Original Grad: 0.000, -lr * Pred Grad:  0.041, New P: -0.700
-Original Grad: 0.000, -lr * Pred Grad:  0.093, New P: 0.645
iter 7 loss: 0.308
Actual params: [-0.7002,  0.6449]
-Original Grad: 0.001, -lr * Pred Grad:  0.061, New P: -0.639
-Original Grad: 0.001, -lr * Pred Grad:  0.082, New P: 0.727
iter 8 loss: 0.308
Actual params: [-0.6391,  0.7267]
-Original Grad: 0.003, -lr * Pred Grad:  0.064, New P: -0.575
-Original Grad: 0.004, -lr * Pred Grad:  0.070, New P: 0.797
iter 9 loss: 0.307
Actual params: [-0.5752,  0.7972]
-Original Grad: 0.011, -lr * Pred Grad:  0.063, New P: -0.512
-Original Grad: 0.012, -lr * Pred Grad:  0.066, New P: 0.863
iter 10 loss: 0.305
Actual params: [-0.5122,  0.8631]
-Original Grad: 0.020, -lr * Pred Grad:  0.070, New P: -0.442
-Original Grad: 0.020, -lr * Pred Grad:  0.072, New P: 0.935
iter 11 loss: 0.300
Actual params: [-0.4418,  0.9353]
-Original Grad: 0.044, -lr * Pred Grad:  0.072, New P: -0.370
-Original Grad: 0.042, -lr * Pred Grad:  0.074, New P: 1.009
iter 12 loss: 0.288
Actual params: [-0.3697,  1.0089]
-Original Grad: 0.123, -lr * Pred Grad:  0.069, New P: -0.301
-Original Grad: 0.102, -lr * Pred Grad:  0.072, New P: 1.081
iter 13 loss: 0.268
Actual params: [-0.3008,  1.0807]
-Original Grad: 0.120, -lr * Pred Grad:  0.079, New P: -0.222
-Original Grad: 0.103, -lr * Pred Grad:  0.080, New P: 1.161
iter 14 loss: 0.240
Actual params: [-0.2222,  1.1611]
-Original Grad: 0.271, -lr * Pred Grad:  0.080, New P: -0.142
-Original Grad: 0.174, -lr * Pred Grad:  0.085, New P: 1.246
iter 15 loss: 0.210
Actual params: [-0.1425,  1.2457]
-Original Grad: 0.253, -lr * Pred Grad:  0.087, New P: -0.055
-Original Grad: 0.140, -lr * Pred Grad:  0.091, New P: 1.337
iter 16 loss: 0.178
Actual params: [-0.0554,  1.3366]
-Original Grad: 0.280, -lr * Pred Grad:  0.093, New P: 0.038
-Original Grad: 0.100, -lr * Pred Grad:  0.094, New P: 1.431
iter 17 loss: 0.143
Actual params: [0.0377, 1.4311]
-Original Grad: 0.169, -lr * Pred Grad:  0.096, New P: 0.134
-Original Grad: 0.046, -lr * Pred Grad:  0.093, New P: 1.524
iter 18 loss: 0.115
Actual params: [0.1337, 1.5237]
-Original Grad: 0.212, -lr * Pred Grad:  0.100, New P: 0.233
-Original Grad: 0.020, -lr * Pred Grad:  0.087, New P: 1.611
iter 19 loss: 0.087
Actual params: [0.2334, 1.6111]
-Original Grad: 0.217, -lr * Pred Grad:  0.103, New P: 0.336
-Original Grad: 0.000, -lr * Pred Grad:  0.080, New P: 1.691
iter 20 loss: 0.064
Actual params: [0.3362, 1.6906]
-Original Grad: 0.091, -lr * Pred Grad:  0.100, New P: 0.436
-Original Grad: -0.009, -lr * Pred Grad:  0.071, New P: 1.761
iter 21 loss: 0.047
Actual params: [0.4363, 1.7614]
-Original Grad: 0.086, -lr * Pred Grad:  0.097, New P: 0.534
-Original Grad: -0.040, -lr * Pred Grad:  0.057, New P: 1.818
iter 22 loss: 0.038
Actual params: [0.5337, 1.8182]
-Original Grad: 0.084, -lr * Pred Grad:  0.095, New P: 0.629
-Original Grad: -0.050, -lr * Pred Grad:  0.042, New P: 1.860
iter 23 loss: 0.030
Actual params: [0.6287, 1.8604]
-Original Grad: 0.053, -lr * Pred Grad:  0.091, New P: 0.719
-Original Grad: -0.056, -lr * Pred Grad:  0.028, New P: 1.888
iter 24 loss: 0.026
Actual params: [0.7193, 1.8884]
-Original Grad: 0.023, -lr * Pred Grad:  0.084, New P: 0.804
-Original Grad: -0.047, -lr * Pred Grad:  0.017, New P: 1.905
iter 25 loss: 0.024
Actual params: [0.8037, 1.9055]
-Original Grad: 0.006, -lr * Pred Grad:  0.077, New P: 0.881
-Original Grad: -0.042, -lr * Pred Grad:  0.008, New P: 1.914
iter 26 loss: 0.023
Actual params: [0.8812, 1.9135]
-Original Grad: -0.020, -lr * Pred Grad:  0.069, New P: 0.950
-Original Grad: -0.038, -lr * Pred Grad:  0.001, New P: 1.914
iter 27 loss: 0.022
Actual params: [0.95  , 1.9141]
-Original Grad: 0.018, -lr * Pred Grad:  0.064, New P: 1.014
-Original Grad: 0.004, -lr * Pred Grad:  0.001, New P: 1.915
iter 28 loss: 0.022
Actual params: [1.0142, 1.9155]
-Original Grad: -0.016, -lr * Pred Grad:  0.057, New P: 1.071
-Original Grad: -0.033, -lr * Pred Grad:  -0.005, New P: 1.911
iter 29 loss: 0.023
Actual params: [1.0713, 1.9107]
-Original Grad: -0.002, -lr * Pred Grad:  0.052, New P: 1.123
-Original Grad: -0.011, -lr * Pred Grad:  -0.006, New P: 1.904
iter 30 loss: 0.023
Actual params: [1.1231, 1.9044]
-Original Grad: -0.024, -lr * Pred Grad:  0.045, New P: 1.168
-Original Grad: -0.010, -lr * Pred Grad:  -0.008, New P: 1.897
