Target params: [1.3344, 1.5708]
iter 0 loss: 0.077
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.035, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.000, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.074
Actual params: [-0.5723, -0.0964]
-Original Grad: -0.014, -lr * Pred Grad:  -0.090, New P: -0.662
-Original Grad: 0.000, -lr * Pred Grad:  -0.042, New P: -0.139
iter 2 loss: 0.073
Actual params: [-0.662 , -0.1388]
-Original Grad: -0.013, -lr * Pred Grad:  -0.087, New P: -0.749
-Original Grad: 0.000, -lr * Pred Grad:  0.061, New P: -0.078
iter 3 loss: 0.072
Actual params: [-0.7486, -0.0781]
-Original Grad: -0.007, -lr * Pred Grad:  -0.080, New P: -0.829
-Original Grad: 0.000, -lr * Pred Grad:  0.071, New P: -0.008
iter 4 loss: 0.071
Actual params: [-0.8288, -0.0075]
-Original Grad: -0.004, -lr * Pred Grad:  -0.073, New P: -0.902
-Original Grad: 0.001, -lr * Pred Grad:  0.076, New P: 0.068
iter 5 loss: 0.071
Actual params: [-0.9022,  0.0682]
-Original Grad: -0.002, -lr * Pred Grad:  -0.066, New P: -0.968
-Original Grad: 0.000, -lr * Pred Grad:  0.083, New P: 0.151
iter 6 loss: 0.071
Actual params: [-0.9678,  0.1512]
-Original Grad: -0.001, -lr * Pred Grad:  -0.059, New P: -1.027
-Original Grad: 0.000, -lr * Pred Grad:  0.087, New P: 0.238
iter 7 loss: 0.071
Actual params: [-1.0266,  0.2381]
-Original Grad: -0.000, -lr * Pred Grad:  -0.052, New P: -1.079
-Original Grad: 0.000, -lr * Pred Grad:  0.086, New P: 0.324
iter 8 loss: 0.071
Actual params: [-1.0791,  0.3239]
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -1.126
-Original Grad: 0.000, -lr * Pred Grad:  0.083, New P: 0.407
iter 9 loss: 0.071
Actual params: [-1.126 ,  0.4074]
-Original Grad: -0.000, -lr * Pred Grad:  -0.042, New P: -1.168
-Original Grad: 0.000, -lr * Pred Grad:  0.078, New P: 0.486
iter 10 loss: 0.071
Actual params: [-1.168 ,  0.4856]
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -1.206
-Original Grad: 0.000, -lr * Pred Grad:  0.073, New P: 0.558
iter 11 loss: 0.071
Actual params: [-1.2058,  0.5582]
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -1.240
-Original Grad: 0.000, -lr * Pred Grad:  0.067, New P: 0.625
iter 12 loss: 0.071
Actual params: [-1.2398,  0.6247]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.270
-Original Grad: 0.000, -lr * Pred Grad:  0.061, New P: 0.686
iter 13 loss: 0.071
Actual params: [-1.2705,  0.6856]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.298
-Original Grad: 0.000, -lr * Pred Grad:  0.055, New P: 0.741
iter 14 loss: 0.071
Actual params: [-1.2982,  0.741 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.323
-Original Grad: 0.000, -lr * Pred Grad:  0.050, New P: 0.791
iter 15 loss: 0.071
Actual params: [-1.3233,  0.7915]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.346
-Original Grad: 0.000, -lr * Pred Grad:  0.046, New P: 0.837
iter 16 loss: 0.071
Actual params: [-1.346 ,  0.8374]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.367
-Original Grad: 0.000, -lr * Pred Grad:  0.042, New P: 0.879
iter 17 loss: 0.071
Actual params: [-1.3667,  0.8792]
-Original Grad: 0.000, -lr * Pred Grad:  -0.019, New P: -1.385
-Original Grad: 0.000, -lr * Pred Grad:  0.038, New P: 0.917
iter 18 loss: 0.071
Actual params: [-1.3854,  0.9172]
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -1.402
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: 0.952
iter 19 loss: 0.071
Actual params: [-1.4024,  0.9518]
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -1.418
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: 0.983
iter 20 loss: 0.071
Actual params: [-1.4179,  0.9834]
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -1.432
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: 1.012
iter 21 loss: 0.071
Actual params: [-1.432 ,  1.0121]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -1.445
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 1.038
iter 22 loss: 0.071
Actual params: [-1.4448,  1.0383]
-Original Grad: 0.000, -lr * Pred Grad:  -0.012, New P: -1.457
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 1.062
iter 23 loss: 0.071
Actual params: [-1.4565,  1.0622]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.467
-Original Grad: 0.000, -lr * Pred Grad:  0.022, New P: 1.084
iter 24 loss: 0.071
Actual params: [-1.4672,  1.0839]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.477
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 1.104
iter 25 loss: 0.071
Actual params: [-1.4768,  1.1038]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.486
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 1.122
iter 26 loss: 0.071
Actual params: [-1.4857,  1.1219]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.494
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 1.138
iter 27 loss: 0.071
Actual params: [-1.4937,  1.1383]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.501
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 1.153
iter 28 loss: 0.071
Actual params: [-1.501 ,  1.1533]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.508
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 1.167
iter 29 loss: 0.071
Actual params: [-1.5077,  1.167 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.514
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 1.179
iter 30 loss: 0.071
Actual params: [-1.5138,  1.1795]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.519
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 1.191
Target params: [1.3344, 1.5708]
iter 0 loss: 0.443
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.443
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.003, -lr * Pred Grad:  0.085, New P: -0.287
-Original Grad: 0.002, -lr * Pred Grad:  0.088, New P: 0.191
iter 2 loss: 0.442
Actual params: [-0.2872,  0.1913]
-Original Grad: 0.013, -lr * Pred Grad:  0.079, New P: -0.209
-Original Grad: 0.008, -lr * Pred Grad:  0.079, New P: 0.270
iter 3 loss: 0.440
Actual params: [-0.2086,  0.2699]
-Original Grad: 0.024, -lr * Pred Grad:  0.083, New P: -0.126
-Original Grad: 0.014, -lr * Pred Grad:  0.083, New P: 0.353
iter 4 loss: 0.433
Actual params: [-0.126 ,  0.3527]
-Original Grad: 0.078, -lr * Pred Grad:  0.075, New P: -0.051
-Original Grad: 0.050, -lr * Pred Grad:  0.074, New P: 0.427
iter 5 loss: 0.421
Actual params: [-0.0515,  0.4266]
-Original Grad: 0.193, -lr * Pred Grad:  0.073, New P: 0.022
-Original Grad: 0.123, -lr * Pred Grad:  0.073, New P: 0.500
iter 6 loss: 0.400
Actual params: [0.0218, 0.4997]
-Original Grad: 0.193, -lr * Pred Grad:  0.081, New P: 0.103
-Original Grad: 0.137, -lr * Pred Grad:  0.081, New P: 0.581
iter 7 loss: 0.373
Actual params: [0.1031, 0.5805]
-Original Grad: 0.266, -lr * Pred Grad:  0.086, New P: 0.189
-Original Grad: 0.190, -lr * Pred Grad:  0.085, New P: 0.666
iter 8 loss: 0.338
Actual params: [0.1894, 0.666 ]
-Original Grad: 0.271, -lr * Pred Grad:  0.091, New P: 0.280
-Original Grad: 0.181, -lr * Pred Grad:  0.090, New P: 0.756
iter 9 loss: 0.297
Actual params: [0.2802, 0.7563]
-Original Grad: 0.341, -lr * Pred Grad:  0.094, New P: 0.374
-Original Grad: 0.204, -lr * Pred Grad:  0.094, New P: 0.850
iter 10 loss: 0.248
Actual params: [0.3742, 0.8503]
-Original Grad: 0.378, -lr * Pred Grad:  0.097, New P: 0.471
-Original Grad: 0.211, -lr * Pred Grad:  0.097, New P: 0.947
iter 11 loss: 0.196
Actual params: [0.4712, 0.9473]
-Original Grad: 0.339, -lr * Pred Grad:  0.099, New P: 0.571
-Original Grad: 0.206, -lr * Pred Grad:  0.099, New P: 1.047
iter 12 loss: 0.141
Actual params: [0.5706, 1.0467]
-Original Grad: 0.336, -lr * Pred Grad:  0.101, New P: 0.672
-Original Grad: 0.165, -lr * Pred Grad:  0.100, New P: 1.147
iter 13 loss: 0.088
Actual params: [0.6721, 1.1472]
-Original Grad: 0.217, -lr * Pred Grad:  0.101, New P: 0.773
-Original Grad: 0.196, -lr * Pred Grad:  0.102, New P: 1.249
iter 14 loss: 0.044
Actual params: [0.7729, 1.2492]
-Original Grad: 0.171, -lr * Pred Grad:  0.099, New P: 0.872
-Original Grad: 0.164, -lr * Pred Grad:  0.102, New P: 1.352
iter 15 loss: 0.018
Actual params: [0.8718, 1.3516]
-Original Grad: 0.038, -lr * Pred Grad:  0.092, New P: 0.963
-Original Grad: 0.113, -lr * Pred Grad:  0.101, New P: 1.452
iter 16 loss: 0.009
Actual params: [0.9635, 1.4521]
-Original Grad: -0.018, -lr * Pred Grad:  0.082, New P: 1.046
-Original Grad: 0.033, -lr * Pred Grad:  0.094, New P: 1.546
iter 17 loss: 0.007
Actual params: [1.0456, 1.546 ]
-Original Grad: -0.004, -lr * Pred Grad:  0.074, New P: 1.120
-Original Grad: 0.012, -lr * Pred Grad:  0.086, New P: 1.632
iter 18 loss: 0.010
Actual params: [1.12  , 1.6323]
-Original Grad: 0.008, -lr * Pred Grad:  0.068, New P: 1.188
-Original Grad: -0.034, -lr * Pred Grad:  0.075, New P: 1.708
iter 19 loss: 0.014
Actual params: [1.188 , 1.7076]
-Original Grad: 0.014, -lr * Pred Grad:  0.063, New P: 1.251
-Original Grad: -0.044, -lr * Pred Grad:  0.064, New P: 1.772
iter 20 loss: 0.018
Actual params: [1.2507, 1.7721]
-Original Grad: 0.040, -lr * Pred Grad:  0.059, New P: 1.310
-Original Grad: -0.086, -lr * Pred Grad:  0.051, New P: 1.823
iter 21 loss: 0.020
Actual params: [1.31  , 1.8227]
-Original Grad: 0.041, -lr * Pred Grad:  0.056, New P: 1.366
-Original Grad: -0.076, -lr * Pred Grad:  0.039, New P: 1.862
iter 22 loss: 0.020
Actual params: [1.3663, 1.8617]
-Original Grad: 0.050, -lr * Pred Grad:  0.054, New P: 1.420
-Original Grad: -0.067, -lr * Pred Grad:  0.029, New P: 1.891
iter 23 loss: 0.019
Actual params: [1.4205, 1.8912]
-Original Grad: 0.063, -lr * Pred Grad:  0.053, New P: 1.473
-Original Grad: -0.081, -lr * Pred Grad:  0.019, New P: 1.911
iter 24 loss: 0.018
Actual params: [1.4734, 1.9107]
-Original Grad: 0.049, -lr * Pred Grad:  0.051, New P: 1.525
-Original Grad: -0.051, -lr * Pred Grad:  0.013, New P: 1.924
iter 25 loss: 0.016
Actual params: [1.5245, 1.9239]
-Original Grad: 0.026, -lr * Pred Grad:  0.048, New P: 1.573
-Original Grad: -0.038, -lr * Pred Grad:  0.009, New P: 1.932
iter 26 loss: 0.014
Actual params: [1.5727, 1.9325]
-Original Grad: 0.031, -lr * Pred Grad:  0.046, New P: 1.618
-Original Grad: -0.032, -lr * Pred Grad:  0.005, New P: 1.937
iter 27 loss: 0.013
Actual params: [1.6184, 1.9374]
-Original Grad: 0.030, -lr * Pred Grad:  0.044, New P: 1.662
-Original Grad: -0.048, -lr * Pred Grad:  0.000, New P: 1.937
iter 28 loss: 0.011
Actual params: [1.6619, 1.9375]
-Original Grad: 0.028, -lr * Pred Grad:  0.041, New P: 1.703
-Original Grad: -0.035, -lr * Pred Grad:  -0.003, New P: 1.934
iter 29 loss: 0.010
Actual params: [1.7033, 1.9343]
-Original Grad: 0.026, -lr * Pred Grad:  0.039, New P: 1.743
-Original Grad: -0.027, -lr * Pred Grad:  -0.005, New P: 1.929
iter 30 loss: 0.008
Actual params: [1.7426, 1.9289]
-Original Grad: 0.008, -lr * Pred Grad:  0.036, New P: 1.779
-Original Grad: -0.016, -lr * Pred Grad:  -0.006, New P: 1.922
Target params: [1.3344, 1.5708]
iter 0 loss: 0.725
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.018, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.023, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.720
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.044, -lr * Pred Grad:  0.094, New P: -0.278
-Original Grad: 0.050, -lr * Pred Grad:  0.096, New P: 0.199
iter 2 loss: 0.710
Actual params: [-0.2779,  0.1991]
-Original Grad: 0.050, -lr * Pred Grad:  0.097, New P: -0.181
-Original Grad: 0.059, -lr * Pred Grad:  0.097, New P: 0.296
iter 3 loss: 0.697
Actual params: [-0.1814,  0.2962]
-Original Grad: 0.063, -lr * Pred Grad:  0.098, New P: -0.084
-Original Grad: 0.068, -lr * Pred Grad:  0.098, New P: 0.394
iter 4 loss: 0.682
Actual params: [-0.0838,  0.3944]
-Original Grad: 0.083, -lr * Pred Grad:  0.098, New P: 0.014
-Original Grad: 0.078, -lr * Pred Grad:  0.099, New P: 0.494
iter 5 loss: 0.659
Actual params: [0.0141, 0.4936]
-Original Grad: 0.130, -lr * Pred Grad:  0.096, New P: 0.110
-Original Grad: 0.112, -lr * Pred Grad:  0.099, New P: 0.593
iter 6 loss: 0.631
Actual params: [0.1103, 0.5926]
-Original Grad: 0.211, -lr * Pred Grad:  0.093, New P: 0.203
-Original Grad: 0.167, -lr * Pred Grad:  0.097, New P: 0.690
iter 7 loss: 0.592
Actual params: [0.2032, 0.6898]
-Original Grad: 0.290, -lr * Pred Grad:  0.092, New P: 0.296
-Original Grad: 0.182, -lr * Pred Grad:  0.098, New P: 0.788
iter 8 loss: 0.538
Actual params: [0.2957, 0.7881]
-Original Grad: 0.333, -lr * Pred Grad:  0.094, New P: 0.390
-Original Grad: 0.186, -lr * Pred Grad:  0.100, New P: 0.888
iter 9 loss: 0.471
Actual params: [0.3901, 0.888 ]
-Original Grad: 0.455, -lr * Pred Grad:  0.095, New P: 0.485
-Original Grad: 0.221, -lr * Pred Grad:  0.101, New P: 0.989
iter 10 loss: 0.408
Actual params: [0.4854, 0.9893]
-Original Grad: 0.555, -lr * Pred Grad:  0.097, New P: 0.582
-Original Grad: 0.228, -lr * Pred Grad:  0.103, New P: 1.092
iter 11 loss: 0.348
Actual params: [0.5822, 1.092 ]
-Original Grad: 0.447, -lr * Pred Grad:  0.099, New P: 0.682
-Original Grad: 0.138, -lr * Pred Grad:  0.102, New P: 1.194
iter 12 loss: 0.291
Actual params: [0.6816, 1.1942]
-Original Grad: 0.527, -lr * Pred Grad:  0.102, New P: 0.783
-Original Grad: 0.127, -lr * Pred Grad:  0.101, New P: 1.295
iter 13 loss: 0.237
Actual params: [0.7834, 1.2954]
-Original Grad: 0.601, -lr * Pred Grad:  0.104, New P: 0.887
-Original Grad: 0.123, -lr * Pred Grad:  0.100, New P: 1.396
iter 14 loss: 0.181
Actual params: [0.8872, 1.3956]
-Original Grad: 0.494, -lr * Pred Grad:  0.105, New P: 0.992
-Original Grad: 0.156, -lr * Pred Grad:  0.101, New P: 1.496
iter 15 loss: 0.135
Actual params: [0.9924, 1.4964]
-Original Grad: 0.161, -lr * Pred Grad:  0.100, New P: 1.093
-Original Grad: 0.169, -lr * Pred Grad:  0.102, New P: 1.598
iter 16 loss: 0.103
Actual params: [1.0929, 1.5981]
-Original Grad: -0.031, -lr * Pred Grad:  0.090, New P: 1.183
-Original Grad: 0.148, -lr * Pred Grad:  0.102, New P: 1.700
iter 17 loss: 0.080
Actual params: [1.1829, 1.6998]
-Original Grad: -0.031, -lr * Pred Grad:  0.081, New P: 1.263
-Original Grad: 0.154, -lr * Pred Grad:  0.102, New P: 1.802
iter 18 loss: 0.065
Actual params: [1.2634, 1.8017]
-Original Grad: -0.039, -lr * Pred Grad:  0.072, New P: 1.335
-Original Grad: 0.212, -lr * Pred Grad:  0.104, New P: 1.906
iter 19 loss: 0.054
Actual params: [1.3352, 1.9057]
-Original Grad: -0.063, -lr * Pred Grad:  0.063, New P: 1.398
-Original Grad: 0.095, -lr * Pred Grad:  0.101, New P: 2.007
iter 20 loss: 0.046
Actual params: [1.398 , 2.0066]
-Original Grad: -0.024, -lr * Pred Grad:  0.056, New P: 1.454
-Original Grad: 0.113, -lr * Pred Grad:  0.099, New P: 2.106
iter 21 loss: 0.038
Actual params: [1.4543, 2.1058]
-Original Grad: -0.178, -lr * Pred Grad:  0.044, New P: 1.498
-Original Grad: 0.178, -lr * Pred Grad:  0.101, New P: 2.206
iter 22 loss: 0.033
Actual params: [1.4984, 2.2064]
-Original Grad: -0.258, -lr * Pred Grad:  0.030, New P: 1.528
-Original Grad: 0.126, -lr * Pred Grad:  0.100, New P: 2.306
iter 23 loss: 0.032
Actual params: [1.5284, 2.306 ]
-Original Grad: -0.222, -lr * Pred Grad:  0.019, New P: 1.547
-Original Grad: 0.051, -lr * Pred Grad:  0.094, New P: 2.400
iter 24 loss: 0.038
Actual params: [1.547 , 2.4003]
-Original Grad: -0.240, -lr * Pred Grad:  0.008, New P: 1.555
-Original Grad: -0.080, -lr * Pred Grad:  0.079, New P: 2.480
iter 25 loss: 0.054
Actual params: [1.5549, 2.4796]
-Original Grad: 0.092, -lr * Pred Grad:  0.011, New P: 1.565
-Original Grad: -0.315, -lr * Pred Grad:  0.044, New P: 2.523
iter 26 loss: 0.073
Actual params: [1.5654, 2.5232]
-Original Grad: -0.072, -lr * Pred Grad:  0.007, New P: 1.572
-Original Grad: -0.569, -lr * Pred Grad:  -0.001, New P: 2.523
iter 27 loss: 0.074
Actual params: [1.5723, 2.5226]
-Original Grad: -0.512, -lr * Pred Grad:  -0.013, New P: 1.560
-Original Grad: -0.393, -lr * Pred Grad:  -0.022, New P: 2.501
iter 28 loss: 0.062
Actual params: [1.5598, 2.501 ]
-Original Grad: -0.343, -lr * Pred Grad:  -0.023, New P: 1.536
-Original Grad: -0.321, -lr * Pred Grad:  -0.035, New P: 2.466
iter 29 loss: 0.050
Actual params: [1.5363, 2.4655]
-Original Grad: 0.061, -lr * Pred Grad:  -0.019, New P: 1.517
-Original Grad: -0.241, -lr * Pred Grad:  -0.044, New P: 2.422
iter 30 loss: 0.040
Actual params: [1.5172, 2.4216]
-Original Grad: 0.044, -lr * Pred Grad:  -0.016, New P: 1.501
-Original Grad: -0.087, -lr * Pred Grad:  -0.044, New P: 2.377
Target params: [1.3344, 1.5708]
iter 0 loss: 0.228
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.228
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.006, -lr * Pred Grad:  0.092, New P: -0.280
-Original Grad: 0.002, -lr * Pred Grad:  0.091, New P: 0.195
iter 2 loss: 0.227
Actual params: [-0.2802,  0.195 ]
-Original Grad: 0.018, -lr * Pred Grad:  0.083, New P: -0.197
-Original Grad: 0.005, -lr * Pred Grad:  0.082, New P: 0.277
iter 3 loss: 0.224
Actual params: [-0.1972,  0.2771]
-Original Grad: 0.034, -lr * Pred Grad:  0.084, New P: -0.113
-Original Grad: 0.010, -lr * Pred Grad:  0.083, New P: 0.360
iter 4 loss: 0.216
Actual params: [-0.1133,  0.36  ]
-Original Grad: 0.112, -lr * Pred Grad:  0.075, New P: -0.038
-Original Grad: 0.037, -lr * Pred Grad:  0.073, New P: 0.433
iter 5 loss: 0.201
Actual params: [-0.0384,  0.4332]
-Original Grad: 0.196, -lr * Pred Grad:  0.078, New P: 0.040
-Original Grad: 0.072, -lr * Pred Grad:  0.076, New P: 0.509
iter 6 loss: 0.171
Actual params: [0.0398, 0.5092]
-Original Grad: 0.364, -lr * Pred Grad:  0.079, New P: 0.119
-Original Grad: 0.127, -lr * Pred Grad:  0.079, New P: 0.588
iter 7 loss: 0.135
Actual params: [0.119 , 0.5877]
-Original Grad: 0.324, -lr * Pred Grad:  0.086, New P: 0.205
-Original Grad: 0.091, -lr * Pred Grad:  0.085, New P: 0.673
iter 8 loss: 0.102
Actual params: [0.2046, 0.6726]
-Original Grad: 0.296, -lr * Pred Grad:  0.090, New P: 0.295
-Original Grad: 0.057, -lr * Pred Grad:  0.087, New P: 0.759
iter 9 loss: 0.076
Actual params: [0.2947, 0.7594]
-Original Grad: 0.216, -lr * Pred Grad:  0.092, New P: 0.387
-Original Grad: 0.032, -lr * Pred Grad:  0.085, New P: 0.844
iter 10 loss: 0.062
Actual params: [0.3867, 0.844 ]
-Original Grad: 0.054, -lr * Pred Grad:  0.086, New P: 0.473
-Original Grad: 0.038, -lr * Pred Grad:  0.084, New P: 0.928
iter 11 loss: 0.053
Actual params: [0.4728, 0.9279]
-Original Grad: -0.003, -lr * Pred Grad:  0.077, New P: 0.550
-Original Grad: 0.056, -lr * Pred Grad:  0.086, New P: 1.014
iter 12 loss: 0.048
Actual params: [0.55  , 1.0138]
-Original Grad: 0.020, -lr * Pred Grad:  0.071, New P: 0.621
-Original Grad: 0.033, -lr * Pred Grad:  0.084, New P: 1.098
iter 13 loss: 0.043
Actual params: [0.6211, 1.0981]
-Original Grad: 0.039, -lr * Pred Grad:  0.067, New P: 0.688
-Original Grad: 0.039, -lr * Pred Grad:  0.084, New P: 1.182
iter 14 loss: 0.036
Actual params: [0.688, 1.182]
-Original Grad: 0.008, -lr * Pred Grad:  0.061, New P: 0.749
-Original Grad: 0.088, -lr * Pred Grad:  0.089, New P: 1.271
iter 15 loss: 0.028
Actual params: [0.7492, 1.2709]
-Original Grad: 0.033, -lr * Pred Grad:  0.058, New P: 0.807
-Original Grad: 0.080, -lr * Pred Grad:  0.092, New P: 1.363
iter 16 loss: 0.023
Actual params: [0.807 , 1.3632]
-Original Grad: 0.007, -lr * Pred Grad:  0.053, New P: 0.860
-Original Grad: 0.037, -lr * Pred Grad:  0.090, New P: 1.454
iter 17 loss: 0.019
Actual params: [0.86  , 1.4536]
-Original Grad: 0.070, -lr * Pred Grad:  0.053, New P: 0.913
-Original Grad: 0.002, -lr * Pred Grad:  0.082, New P: 1.536
iter 18 loss: 0.016
Actual params: [0.9132, 1.5361]
-Original Grad: 0.019, -lr * Pred Grad:  0.050, New P: 0.963
-Original Grad: -0.013, -lr * Pred Grad:  0.072, New P: 1.608
iter 19 loss: 0.017
Actual params: [0.9629, 1.6083]
-Original Grad: 0.026, -lr * Pred Grad:  0.047, New P: 1.010
-Original Grad: -0.051, -lr * Pred Grad:  0.054, New P: 1.662
iter 20 loss: 0.018
Actual params: [1.01  , 1.6619]
-Original Grad: 0.013, -lr * Pred Grad:  0.044, New P: 1.054
-Original Grad: -0.060, -lr * Pred Grad:  0.035, New P: 1.697
iter 21 loss: 0.020
Actual params: [1.0539, 1.6973]
-Original Grad: -0.004, -lr * Pred Grad:  0.040, New P: 1.094
-Original Grad: -0.068, -lr * Pred Grad:  0.018, New P: 1.715
iter 22 loss: 0.022
Actual params: [1.0936, 1.7148]
-Original Grad: -0.008, -lr * Pred Grad:  0.035, New P: 1.129
-Original Grad: -0.096, -lr * Pred Grad:  -0.003, New P: 1.712
iter 23 loss: 0.022
Actual params: [1.129 , 1.7118]
-Original Grad: -0.014, -lr * Pred Grad:  0.031, New P: 1.160
-Original Grad: -0.082, -lr * Pred Grad:  -0.018, New P: 1.694
iter 24 loss: 0.021
Actual params: [1.1601, 1.6943]
-Original Grad: -0.023, -lr * Pred Grad:  0.026, New P: 1.187
-Original Grad: -0.081, -lr * Pred Grad:  -0.030, New P: 1.664
iter 25 loss: 0.019
Actual params: [1.1866, 1.6644]
-Original Grad: -0.021, -lr * Pred Grad:  0.022, New P: 1.209
-Original Grad: -0.063, -lr * Pred Grad:  -0.038, New P: 1.627
iter 26 loss: 0.018
Actual params: [1.209 , 1.6266]
-Original Grad: -0.023, -lr * Pred Grad:  0.019, New P: 1.228
-Original Grad: -0.055, -lr * Pred Grad:  -0.044, New P: 1.583
iter 27 loss: 0.016
Actual params: [1.2276, 1.5831]
-Original Grad: -0.016, -lr * Pred Grad:  0.015, New P: 1.243
-Original Grad: -0.031, -lr * Pred Grad:  -0.045, New P: 1.538
iter 28 loss: 0.015
Actual params: [1.243, 1.538]
-Original Grad: -0.022, -lr * Pred Grad:  0.012, New P: 1.255
-Original Grad: -0.026, -lr * Pred Grad:  -0.045, New P: 1.493
iter 29 loss: 0.014
Actual params: [1.2553, 1.4925]
-Original Grad: -0.029, -lr * Pred Grad:  0.009, New P: 1.264
-Original Grad: -0.013, -lr * Pred Grad:  -0.044, New P: 1.449
iter 30 loss: 0.014
Actual params: [1.2639, 1.4487]
-Original Grad: -0.023, -lr * Pred Grad:  0.006, New P: 1.270
-Original Grad: -0.014, -lr * Pred Grad:  -0.042, New P: 1.406
Target params: [1.3344, 1.5708]
iter 0 loss: 0.466
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.466
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.084, New P: -0.656
-Original Grad: 0.000, -lr * Pred Grad:  0.089, New P: 0.193
iter 2 loss: 0.466
Actual params: [-0.6559,  0.1929]
-Original Grad: -0.000, -lr * Pred Grad:  -0.070, New P: -0.725
-Original Grad: 0.000, -lr * Pred Grad:  0.081, New P: 0.274
iter 3 loss: 0.466
Actual params: [-0.7254,  0.2736]
-Original Grad: -0.000, -lr * Pred Grad:  -0.058, New P: -0.783
-Original Grad: 0.000, -lr * Pred Grad:  0.074, New P: 0.347
iter 4 loss: 0.466
Actual params: [-0.7832,  0.3471]
-Original Grad: 0.000, -lr * Pred Grad:  -0.048, New P: -0.832
-Original Grad: 0.000, -lr * Pred Grad:  0.067, New P: 0.414
iter 5 loss: 0.466
Actual params: [-0.8317,  0.414 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.040, New P: -0.872
-Original Grad: 0.000, -lr * Pred Grad:  0.064, New P: 0.478
iter 6 loss: 0.466
Actual params: [-0.8722,  0.4782]
-Original Grad: 0.000, -lr * Pred Grad:  -0.033, New P: -0.905
-Original Grad: 0.000, -lr * Pred Grad:  0.065, New P: 0.543
iter 7 loss: 0.466
Actual params: [-0.9053,  0.5433]
-Original Grad: 0.000, -lr * Pred Grad:  -0.027, New P: -0.932
-Original Grad: 0.000, -lr * Pred Grad:  0.066, New P: 0.609
iter 8 loss: 0.466
Actual params: [-0.932 ,  0.6094]
-Original Grad: 0.000, -lr * Pred Grad:  -0.021, New P: -0.953
-Original Grad: 0.000, -lr * Pred Grad:  0.069, New P: 0.678
iter 9 loss: 0.466
Actual params: [-0.9526,  0.678 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -0.964
-Original Grad: 0.000, -lr * Pred Grad:  0.077, New P: 0.755
iter 10 loss: 0.466
Actual params: [-0.964 ,  0.7546]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.962
-Original Grad: 0.000, -lr * Pred Grad:  0.084, New P: 0.838
iter 11 loss: 0.466
Actual params: [-0.9616,  0.8382]
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: -0.941
-Original Grad: 0.001, -lr * Pred Grad:  0.087, New P: 0.926
iter 12 loss: 0.466
Actual params: [-0.9414,  0.9255]
-Original Grad: 0.001, -lr * Pred Grad:  0.047, New P: -0.895
-Original Grad: 0.001, -lr * Pred Grad:  0.082, New P: 1.008
iter 13 loss: 0.466
Actual params: [-0.8945,  1.0078]
-Original Grad: 0.002, -lr * Pred Grad:  0.064, New P: -0.831
-Original Grad: 0.003, -lr * Pred Grad:  0.080, New P: 1.088
iter 14 loss: 0.465
Actual params: [-0.8307,  1.0875]
-Original Grad: 0.006, -lr * Pred Grad:  0.069, New P: -0.761
-Original Grad: 0.006, -lr * Pred Grad:  0.076, New P: 1.164
iter 15 loss: 0.463
Actual params: [-0.7615,  1.1636]
-Original Grad: 0.018, -lr * Pred Grad:  0.067, New P: -0.695
-Original Grad: 0.019, -lr * Pred Grad:  0.069, New P: 1.233
iter 16 loss: 0.460
Actual params: [-0.6947,  1.233 ]
-Original Grad: 0.029, -lr * Pred Grad:  0.075, New P: -0.620
-Original Grad: 0.028, -lr * Pred Grad:  0.077, New P: 1.310
iter 17 loss: 0.452
Actual params: [-0.6198,  1.3097]
-Original Grad: 0.076, -lr * Pred Grad:  0.073, New P: -0.546
-Original Grad: 0.067, -lr * Pred Grad:  0.076, New P: 1.386
iter 18 loss: 0.437
Actual params: [-0.5464,  1.3856]
-Original Grad: 0.168, -lr * Pred Grad:  0.075, New P: -0.472
-Original Grad: 0.136, -lr * Pred Grad:  0.077, New P: 1.463
iter 19 loss: 0.410
Actual params: [-0.4717,  1.463 ]
-Original Grad: 0.164, -lr * Pred Grad:  0.084, New P: -0.387
-Original Grad: 0.125, -lr * Pred Grad:  0.087, New P: 1.550
iter 20 loss: 0.380
Actual params: [-0.3873,  1.5495]
-Original Grad: 0.264, -lr * Pred Grad:  0.090, New P: -0.297
-Original Grad: 0.141, -lr * Pred Grad:  0.094, New P: 1.643
iter 21 loss: 0.349
Actual params: [-0.2973,  1.6434]
-Original Grad: 0.278, -lr * Pred Grad:  0.097, New P: -0.201
-Original Grad: 0.100, -lr * Pred Grad:  0.099, New P: 1.742
iter 22 loss: 0.315
Actual params: [-0.2008,  1.7421]
-Original Grad: 0.283, -lr * Pred Grad:  0.102, New P: -0.099
-Original Grad: 0.038, -lr * Pred Grad:  0.096, New P: 1.838
iter 23 loss: 0.283
Actual params: [-0.0985,  1.8385]
-Original Grad: 0.490, -lr * Pred Grad:  0.104, New P: 0.006
-Original Grad: -0.071, -lr * Pred Grad:  0.071, New P: 1.910
iter 24 loss: 0.250
Actual params: [0.006 , 1.9097]
-Original Grad: 0.377, -lr * Pred Grad:  0.109, New P: 0.115
-Original Grad: -0.040, -lr * Pred Grad:  0.057, New P: 1.966
iter 25 loss: 0.215
Actual params: [0.1151, 1.9662]
-Original Grad: 0.377, -lr * Pred Grad:  0.113, New P: 0.228
-Original Grad: -0.075, -lr * Pred Grad:  0.036, New P: 2.002
iter 26 loss: 0.179
Actual params: [0.2281, 2.002 ]
-Original Grad: 0.371, -lr * Pred Grad:  0.116, New P: 0.344
-Original Grad: -0.133, -lr * Pred Grad:  0.007, New P: 2.009
iter 27 loss: 0.143
Actual params: [0.3441, 2.0085]
-Original Grad: 0.252, -lr * Pred Grad:  0.116, New P: 0.460
-Original Grad: -0.136, -lr * Pred Grad:  -0.016, New P: 1.992
iter 28 loss: 0.110
Actual params: [0.4603, 1.9921]
-Original Grad: 0.240, -lr * Pred Grad:  0.116, New P: 0.576
-Original Grad: -0.198, -lr * Pred Grad:  -0.041, New P: 1.951
iter 29 loss: 0.080
Actual params: [0.5764, 1.9512]
-Original Grad: 0.165, -lr * Pred Grad:  0.113, New P: 0.690
-Original Grad: -0.136, -lr * Pred Grad:  -0.054, New P: 1.898
iter 30 loss: 0.053
Actual params: [0.6898, 1.8976]
-Original Grad: 0.168, -lr * Pred Grad:  0.111, New P: 0.801
-Original Grad: -0.151, -lr * Pred Grad:  -0.065, New P: 1.832
Target params: [1.3344, 1.5708]
iter 0 loss: 0.228
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.228
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.006, -lr * Pred Grad:  0.092, New P: -0.280
-Original Grad: 0.002, -lr * Pred Grad:  0.091, New P: 0.195
iter 2 loss: 0.227
Actual params: [-0.2803,  0.1946]
-Original Grad: 0.019, -lr * Pred Grad:  0.082, New P: -0.198
-Original Grad: 0.006, -lr * Pred Grad:  0.082, New P: 0.276
iter 3 loss: 0.224
Actual params: [-0.1981,  0.2763]
-Original Grad: 0.043, -lr * Pred Grad:  0.081, New P: -0.117
-Original Grad: 0.013, -lr * Pred Grad:  0.080, New P: 0.357
iter 4 loss: 0.217
Actual params: [-0.117 ,  0.3565]
-Original Grad: 0.132, -lr * Pred Grad:  0.075, New P: -0.042
-Original Grad: 0.044, -lr * Pred Grad:  0.073, New P: 0.429
iter 5 loss: 0.202
Actual params: [-0.0424,  0.4295]
-Original Grad: 0.255, -lr * Pred Grad:  0.077, New P: 0.034
-Original Grad: 0.090, -lr * Pred Grad:  0.075, New P: 0.505
iter 6 loss: 0.174
Actual params: [0.0342, 0.5046]
-Original Grad: 0.314, -lr * Pred Grad:  0.083, New P: 0.117
-Original Grad: 0.108, -lr * Pred Grad:  0.082, New P: 0.586
iter 7 loss: 0.136
Actual params: [0.1169, 0.5864]
-Original Grad: 0.292, -lr * Pred Grad:  0.088, New P: 0.205
-Original Grad: 0.076, -lr * Pred Grad:  0.087, New P: 0.673
iter 8 loss: 0.102
Actual params: [0.205 , 0.6732]
-Original Grad: 0.261, -lr * Pred Grad:  0.092, New P: 0.297
-Original Grad: 0.052, -lr * Pred Grad:  0.088, New P: 0.761
iter 9 loss: 0.075
Actual params: [0.2969, 0.7613]
-Original Grad: 0.186, -lr * Pred Grad:  0.093, New P: 0.390
-Original Grad: 0.020, -lr * Pred Grad:  0.084, New P: 0.845
iter 10 loss: 0.061
Actual params: [0.3898, 0.845 ]
-Original Grad: 0.114, -lr * Pred Grad:  0.091, New P: 0.481
-Original Grad: 0.026, -lr * Pred Grad:  0.081, New P: 0.926
iter 11 loss: 0.053
Actual params: [0.4805, 0.9261]
-Original Grad: -0.024, -lr * Pred Grad:  0.080, New P: 0.560
-Original Grad: 0.069, -lr * Pred Grad:  0.085, New P: 1.012
iter 12 loss: 0.048
Actual params: [0.5601, 1.0116]
-Original Grad: -0.081, -lr * Pred Grad:  0.065, New P: 0.625
-Original Grad: 0.095, -lr * Pred Grad:  0.090, New P: 1.102
iter 13 loss: 0.042
Actual params: [0.625 , 1.1021]
-Original Grad: -0.027, -lr * Pred Grad:  0.057, New P: 0.682
-Original Grad: 0.072, -lr * Pred Grad:  0.093, New P: 1.195
iter 14 loss: 0.035
Actual params: [0.6815, 1.195 ]
-Original Grad: 0.057, -lr * Pred Grad:  0.055, New P: 0.737
-Original Grad: 0.058, -lr * Pred Grad:  0.094, New P: 1.289
iter 15 loss: 0.028
Actual params: [0.7369, 1.2886]
-Original Grad: 0.034, -lr * Pred Grad:  0.053, New P: 0.790
-Original Grad: 0.045, -lr * Pred Grad:  0.093, New P: 1.381
iter 16 loss: 0.023
Actual params: [0.7897, 1.3812]
-Original Grad: 0.034, -lr * Pred Grad:  0.050, New P: 0.840
-Original Grad: 0.035, -lr * Pred Grad:  0.090, New P: 1.471
iter 17 loss: 0.019
Actual params: [0.8401, 1.4715]
-Original Grad: 0.026, -lr * Pred Grad:  0.048, New P: 0.888
-Original Grad: -0.001, -lr * Pred Grad:  0.082, New P: 1.553
iter 18 loss: 0.018
Actual params: [0.8881, 1.5532]
-Original Grad: 0.028, -lr * Pred Grad:  0.046, New P: 0.934
-Original Grad: -0.015, -lr * Pred Grad:  0.071, New P: 1.624
iter 19 loss: 0.018
Actual params: [0.9338, 1.6241]
-Original Grad: 0.014, -lr * Pred Grad:  0.043, New P: 0.977
-Original Grad: -0.033, -lr * Pred Grad:  0.057, New P: 1.681
iter 20 loss: 0.020
Actual params: [0.9766, 1.6809]
-Original Grad: 0.026, -lr * Pred Grad:  0.041, New P: 1.018
-Original Grad: -0.053, -lr * Pred Grad:  0.040, New P: 1.720
iter 21 loss: 0.022
Actual params: [1.0176, 1.7205]
-Original Grad: 0.000, -lr * Pred Grad:  0.037, New P: 1.055
-Original Grad: -0.081, -lr * Pred Grad:  0.018, New P: 1.738
iter 22 loss: 0.023
Actual params: [1.0549, 1.7383]
-Original Grad: -0.014, -lr * Pred Grad:  0.033, New P: 1.088
-Original Grad: -0.120, -lr * Pred Grad:  -0.007, New P: 1.731
iter 23 loss: 0.023
Actual params: [1.0877, 1.7311]
-Original Grad: -0.000, -lr * Pred Grad:  0.030, New P: 1.118
-Original Grad: -0.101, -lr * Pred Grad:  -0.024, New P: 1.707
iter 24 loss: 0.021
Actual params: [1.1176, 1.7072]
-Original Grad: -0.012, -lr * Pred Grad:  0.026, New P: 1.144
-Original Grad: -0.070, -lr * Pred Grad:  -0.033, New P: 1.674
iter 25 loss: 0.019
Actual params: [1.1437, 1.674 ]
-Original Grad: -0.016, -lr * Pred Grad:  0.022, New P: 1.166
-Original Grad: -0.063, -lr * Pred Grad:  -0.040, New P: 1.634
iter 26 loss: 0.017
Actual params: [1.1661, 1.6336]
-Original Grad: -0.017, -lr * Pred Grad:  0.019, New P: 1.185
-Original Grad: -0.051, -lr * Pred Grad:  -0.045, New P: 1.589
iter 27 loss: 0.015
Actual params: [1.185 , 1.5885]
-Original Grad: -0.022, -lr * Pred Grad:  0.015, New P: 1.200
-Original Grad: -0.049, -lr * Pred Grad:  -0.049, New P: 1.540
iter 28 loss: 0.014
Actual params: [1.2003, 1.5395]
-Original Grad: -0.017, -lr * Pred Grad:  0.012, New P: 1.213
-Original Grad: -0.023, -lr * Pred Grad:  -0.048, New P: 1.491
iter 29 loss: 0.013
Actual params: [1.2128, 1.4911]
-Original Grad: -0.014, -lr * Pred Grad:  0.010, New P: 1.223
-Original Grad: -0.006, -lr * Pred Grad:  -0.045, New P: 1.446
iter 30 loss: 0.013
Actual params: [1.2228, 1.4459]
-Original Grad: -0.014, -lr * Pred Grad:  0.008, New P: 1.231
-Original Grad: -0.010, -lr * Pred Grad:  -0.043, New P: 1.403
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.054, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.003, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.107
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.074, -lr * Pred Grad:  0.100, New P: -0.273
-Original Grad: 0.003, -lr * Pred Grad:  0.098, New P: 0.202
iter 2 loss: 0.096
Actual params: [-0.2727,  0.202 ]
-Original Grad: 0.089, -lr * Pred Grad:  0.100, New P: -0.173
-Original Grad: -0.009, -lr * Pred Grad:  -0.025, New P: 0.176
iter 3 loss: 0.083
Actual params: [-0.173 ,  0.1765]
-Original Grad: 0.092, -lr * Pred Grad:  0.100, New P: -0.073
-Original Grad: -0.004, -lr * Pred Grad:  -0.040, New P: 0.137
iter 4 loss: 0.069
Actual params: [-0.0728,  0.1366]
-Original Grad: 0.087, -lr * Pred Grad:  0.101, New P: 0.028
-Original Grad: 0.005, -lr * Pred Grad:  -0.005, New P: 0.131
iter 5 loss: 0.055
Actual params: [0.0277, 0.1315]
-Original Grad: 0.093, -lr * Pred Grad:  0.101, New P: 0.129
-Original Grad: 0.005, -lr * Pred Grad:  0.018, New P: 0.149
iter 6 loss: 0.042
Actual params: [0.1287, 0.1493]
-Original Grad: 0.062, -lr * Pred Grad:  0.099, New P: 0.228
-Original Grad: 0.003, -lr * Pred Grad:  0.026, New P: 0.175
iter 7 loss: 0.033
Actual params: [0.2278, 0.1753]
-Original Grad: 0.008, -lr * Pred Grad:  0.089, New P: 0.317
-Original Grad: 0.013, -lr * Pred Grad:  0.051, New P: 0.226
iter 8 loss: 0.036
Actual params: [0.317 , 0.2264]
-Original Grad: -0.002, -lr * Pred Grad:  0.079, New P: 0.396
-Original Grad: -0.010, -lr * Pred Grad:  0.016, New P: 0.243
iter 9 loss: 0.051
Actual params: [0.3957, 0.2427]
-Original Grad: -0.023, -lr * Pred Grad:  0.065, New P: 0.460
-Original Grad: -0.009, -lr * Pred Grad:  -0.006, New P: 0.237
iter 10 loss: 0.065
Actual params: [0.4603, 0.237 ]
-Original Grad: -0.181, -lr * Pred Grad:  0.013, New P: 0.473
-Original Grad: -0.121, -lr * Pred Grad:  -0.048, New P: 0.189
iter 11 loss: 0.062
Actual params: [0.4731, 0.1886]
-Original Grad: -0.173, -lr * Pred Grad:  -0.016, New P: 0.457
-Original Grad: -0.106, -lr * Pred Grad:  -0.064, New P: 0.124
iter 12 loss: 0.050
Actual params: [0.4575, 0.1243]
-Original Grad: -0.139, -lr * Pred Grad:  -0.032, New P: 0.426
-Original Grad: -0.065, -lr * Pred Grad:  -0.072, New P: 0.053
iter 13 loss: 0.042
Actual params: [0.4256, 0.0526]
-Original Grad: -0.028, -lr * Pred Grad:  -0.032, New P: 0.393
-Original Grad: 0.008, -lr * Pred Grad:  -0.063, New P: -0.010
iter 14 loss: 0.039
Actual params: [ 0.3932, -0.0101]
-Original Grad: -0.055, -lr * Pred Grad:  -0.036, New P: 0.357
-Original Grad: -0.004, -lr * Pred Grad:  -0.058, New P: -0.068
iter 15 loss: 0.038
Actual params: [ 0.3567, -0.0679]
-Original Grad: -0.022, -lr * Pred Grad:  -0.036, New P: 0.321
-Original Grad: 0.012, -lr * Pred Grad:  -0.049, New P: -0.117
iter 16 loss: 0.039
Actual params: [ 0.3208, -0.1171]
-Original Grad: -0.014, -lr * Pred Grad:  -0.035, New P: 0.286
-Original Grad: 0.015, -lr * Pred Grad:  -0.040, New P: -0.157
iter 17 loss: 0.041
Actual params: [ 0.2863, -0.1571]
-Original Grad: 0.003, -lr * Pred Grad:  -0.031, New P: 0.255
-Original Grad: 0.017, -lr * Pred Grad:  -0.031, New P: -0.188
iter 18 loss: 0.044
Actual params: [ 0.2553, -0.1884]
-Original Grad: 0.064, -lr * Pred Grad:  -0.019, New P: 0.236
-Original Grad: 0.056, -lr * Pred Grad:  -0.012, New P: -0.200
iter 19 loss: 0.046
Actual params: [ 0.2363, -0.2002]
-Original Grad: 0.047, -lr * Pred Grad:  -0.011, New P: 0.226
-Original Grad: 0.044, -lr * Pred Grad:  0.001, New P: -0.199
iter 20 loss: 0.046
Actual params: [ 0.2257, -0.1989]
-Original Grad: 0.065, -lr * Pred Grad:  -0.001, New P: 0.225
-Original Grad: 0.045, -lr * Pred Grad:  0.013, New P: -0.186
iter 21 loss: 0.046
Actual params: [ 0.2251, -0.1862]
-Original Grad: 0.049, -lr * Pred Grad:  0.006, New P: 0.231
-Original Grad: 0.041, -lr * Pred Grad:  0.022, New P: -0.164
iter 22 loss: 0.044
Actual params: [ 0.2312, -0.1641]
-Original Grad: 0.046, -lr * Pred Grad:  0.012, New P: 0.243
-Original Grad: 0.034, -lr * Pred Grad:  0.029, New P: -0.136
iter 23 loss: 0.042
Actual params: [ 0.2431, -0.1356]
-Original Grad: 0.044, -lr * Pred Grad:  0.017, New P: 0.260
-Original Grad: 0.041, -lr * Pred Grad:  0.036, New P: -0.100
iter 24 loss: 0.040
Actual params: [ 0.2599, -0.0995]
-Original Grad: 0.070, -lr * Pred Grad:  0.025, New P: 0.285
-Original Grad: 0.038, -lr * Pred Grad:  0.042, New P: -0.058
iter 25 loss: 0.038
Actual params: [ 0.2847, -0.0575]
-Original Grad: 0.026, -lr * Pred Grad:  0.026, New P: 0.311
-Original Grad: 0.021, -lr * Pred Grad:  0.044, New P: -0.014
iter 26 loss: 0.036
Actual params: [ 0.3108, -0.014 ]
-Original Grad: -0.051, -lr * Pred Grad:  0.016, New P: 0.327
-Original Grad: 0.006, -lr * Pred Grad:  0.041, New P: 0.027
iter 27 loss: 0.036
Actual params: [0.3272, 0.0274]
-Original Grad: 0.044, -lr * Pred Grad:  0.021, New P: 0.348
-Original Grad: 0.035, -lr * Pred Grad:  0.046, New P: 0.074
iter 28 loss: 0.035
Actual params: [0.3483, 0.0737]
-Original Grad: -0.027, -lr * Pred Grad:  0.015, New P: 0.364
-Original Grad: 0.015, -lr * Pred Grad:  0.046, New P: 0.120
iter 29 loss: 0.036
Actual params: [0.3637, 0.1196]
-Original Grad: -0.041, -lr * Pred Grad:  0.008, New P: 0.372
-Original Grad: -0.018, -lr * Pred Grad:  0.037, New P: 0.157
iter 30 loss: 0.039
Actual params: [0.3719, 0.1566]
-Original Grad: -0.127, -lr * Pred Grad:  -0.010, New P: 0.361
-Original Grad: -0.066, -lr * Pred Grad:  0.015, New P: 0.172
Target params: [1.3344, 1.5708]
iter 0 loss: 0.008
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.015, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.005, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.006
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.033, -lr * Pred Grad:  0.096, New P: -0.277
-Original Grad: -0.004, -lr * Pred Grad:  -0.097, New P: -0.194
iter 2 loss: 0.003
Actual params: [-0.2765, -0.1935]
-Original Grad: 0.027, -lr * Pred Grad:  0.098, New P: -0.179
-Original Grad: -0.000, -lr * Pred Grad:  -0.079, New P: -0.273
iter 3 loss: 0.002
Actual params: [-0.1789, -0.2728]
-Original Grad: 0.008, -lr * Pred Grad:  0.089, New P: -0.090
-Original Grad: 0.001, -lr * Pred Grad:  -0.052, New P: -0.325
iter 4 loss: 0.002
Actual params: [-0.0901, -0.3247]
-Original Grad: -0.053, -lr * Pred Grad:  0.007, New P: -0.083
-Original Grad: 0.002, -lr * Pred Grad:  -0.026, New P: -0.351
iter 5 loss: 0.003
Actual params: [-0.0826, -0.3512]
-Original Grad: -0.044, -lr * Pred Grad:  -0.022, New P: -0.105
-Original Grad: -0.003, -lr * Pred Grad:  -0.042, New P: -0.393
iter 6 loss: 0.002
Actual params: [-0.1049, -0.393 ]
-Original Grad: -0.020, -lr * Pred Grad:  -0.031, New P: -0.136
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -0.433
iter 7 loss: 0.002
Actual params: [-0.1358, -0.4327]
-Original Grad: -0.004, -lr * Pred Grad:  -0.030, New P: -0.165
-Original Grad: 0.001, -lr * Pred Grad:  -0.029, New P: -0.462
iter 8 loss: 0.002
Actual params: [-0.1654, -0.462 ]
-Original Grad: 0.010, -lr * Pred Grad:  -0.020, New P: -0.186
-Original Grad: 0.006, -lr * Pred Grad:  0.008, New P: -0.454
iter 9 loss: 0.002
Actual params: [-0.1855, -0.4536]
-Original Grad: 0.007, -lr * Pred Grad:  -0.014, New P: -0.199
-Original Grad: 0.004, -lr * Pred Grad:  0.026, New P: -0.428
iter 10 loss: 0.002
Actual params: [-0.1993, -0.4276]
-Original Grad: 0.015, -lr * Pred Grad:  -0.004, New P: -0.203
-Original Grad: 0.004, -lr * Pred Grad:  0.039, New P: -0.389
iter 11 loss: 0.002
Actual params: [-0.2033, -0.3889]
-Original Grad: 0.018, -lr * Pred Grad:  0.006, New P: -0.197
-Original Grad: 0.004, -lr * Pred Grad:  0.049, New P: -0.339
iter 12 loss: 0.002
Actual params: [-0.1971, -0.3395]
-Original Grad: 0.009, -lr * Pred Grad:  0.010, New P: -0.187
-Original Grad: 0.002, -lr * Pred Grad:  0.053, New P: -0.286
iter 13 loss: 0.002
Actual params: [-0.1867, -0.286 ]
-Original Grad: 0.010, -lr * Pred Grad:  0.015, New P: -0.172
-Original Grad: 0.002, -lr * Pred Grad:  0.057, New P: -0.229
iter 14 loss: 0.002
Actual params: [-0.1719, -0.2293]
-Original Grad: 0.004, -lr * Pred Grad:  0.016, New P: -0.156
-Original Grad: -0.000, -lr * Pred Grad:  0.049, New P: -0.180
iter 15 loss: 0.002
Actual params: [-0.1561, -0.1799]
-Original Grad: 0.005, -lr * Pred Grad:  0.017, New P: -0.139
-Original Grad: -0.000, -lr * Pred Grad:  0.043, New P: -0.137
iter 16 loss: 0.002
Actual params: [-0.1391, -0.1366]
-Original Grad: 0.003, -lr * Pred Grad:  0.017, New P: -0.122
-Original Grad: -0.001, -lr * Pred Grad:  0.037, New P: -0.100
iter 17 loss: 0.002
Actual params: [-0.1222, -0.0999]
-Original Grad: -0.004, -lr * Pred Grad:  0.013, New P: -0.109
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: -0.065
iter 18 loss: 0.002
Actual params: [-0.1089, -0.0646]
-Original Grad: -0.001, -lr * Pred Grad:  0.011, New P: -0.098
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: -0.033
iter 19 loss: 0.002
Actual params: [-0.0975, -0.0325]
-Original Grad: -0.008, -lr * Pred Grad:  0.006, New P: -0.092
-Original Grad: 0.002, -lr * Pred Grad:  0.038, New P: 0.005
iter 20 loss: 0.002
Actual params: [-0.0917,  0.0052]
-Original Grad: -0.005, -lr * Pred Grad:  0.003, New P: -0.089
-Original Grad: 0.001, -lr * Pred Grad:  0.038, New P: 0.044
iter 21 loss: 0.002
Actual params: [-0.0892,  0.0435]
-Original Grad: -0.006, -lr * Pred Grad:  -0.001, New P: -0.090
-Original Grad: 0.001, -lr * Pred Grad:  0.040, New P: 0.084
iter 22 loss: 0.002
Actual params: [-0.0902,  0.0838]
-Original Grad: -0.004, -lr * Pred Grad:  -0.003, New P: -0.094
-Original Grad: 0.001, -lr * Pred Grad:  0.041, New P: 0.125
iter 23 loss: 0.002
Actual params: [-0.0936,  0.1248]
-Original Grad: 0.005, -lr * Pred Grad:  -0.000, New P: -0.094
-Original Grad: -0.002, -lr * Pred Grad:  0.031, New P: 0.155
iter 24 loss: 0.002
Actual params: [-0.0937,  0.1554]
-Original Grad: 0.007, -lr * Pred Grad:  0.004, New P: -0.089
-Original Grad: -0.002, -lr * Pred Grad:  0.018, New P: 0.173
iter 25 loss: 0.002
Actual params: [-0.0895,  0.1729]
-Original Grad: 0.005, -lr * Pred Grad:  0.007, New P: -0.082
-Original Grad: -0.002, -lr * Pred Grad:  0.008, New P: 0.181
iter 26 loss: 0.002
Actual params: [-0.0825,  0.1806]
-Original Grad: 0.010, -lr * Pred Grad:  0.012, New P: -0.070
-Original Grad: -0.004, -lr * Pred Grad:  -0.008, New P: 0.173
iter 27 loss: 0.002
Actual params: [-0.0702,  0.173 ]
-Original Grad: 0.003, -lr * Pred Grad:  0.013, New P: -0.057
-Original Grad: -0.001, -lr * Pred Grad:  -0.012, New P: 0.161
iter 28 loss: 0.002
Actual params: [-0.0573,  0.1609]
-Original Grad: -0.003, -lr * Pred Grad:  0.010, New P: -0.047
-Original Grad: 0.001, -lr * Pred Grad:  -0.008, New P: 0.153
iter 29 loss: 0.002
Actual params: [-0.0474,  0.1531]
-Original Grad: -0.014, -lr * Pred Grad:  0.000, New P: -0.047
-Original Grad: 0.003, -lr * Pred Grad:  0.005, New P: 0.158
iter 30 loss: 0.002
Actual params: [-0.047 ,  0.1581]
-Original Grad: -0.005, -lr * Pred Grad:  -0.003, New P: -0.050
-Original Grad: 0.001, -lr * Pred Grad:  0.009, New P: 0.167
Target params: [1.3344, 1.5708]
iter 0 loss: 0.537
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.039, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.535
Actual params: [-0.5723,  0.1035]
-Original Grad: 0.006, -lr * Pred Grad:  0.033, New P: -0.539
-Original Grad: 0.041, -lr * Pred Grad:  0.100, New P: 0.204
iter 2 loss: 0.531
Actual params: [-0.5395,  0.2036]
-Original Grad: 0.016, -lr * Pred Grad:  0.069, New P: -0.471
-Original Grad: 0.049, -lr * Pred Grad:  0.100, New P: 0.304
iter 3 loss: 0.527
Actual params: [-0.4708,  0.304 ]
-Original Grad: 0.024, -lr * Pred Grad:  0.080, New P: -0.391
-Original Grad: 0.049, -lr * Pred Grad:  0.101, New P: 0.405
iter 4 loss: 0.520
Actual params: [-0.3907,  0.4045]
-Original Grad: 0.048, -lr * Pred Grad:  0.082, New P: -0.309
-Original Grad: 0.076, -lr * Pred Grad:  0.100, New P: 0.505
iter 5 loss: 0.512
Actual params: [-0.309 ,  0.5046]
-Original Grad: 0.066, -lr * Pred Grad:  0.086, New P: -0.223
-Original Grad: 0.075, -lr * Pred Grad:  0.101, New P: 0.605
iter 6 loss: 0.502
Actual params: [-0.2235,  0.6053]
-Original Grad: 0.101, -lr * Pred Grad:  0.087, New P: -0.136
-Original Grad: 0.089, -lr * Pred Grad:  0.101, New P: 0.707
iter 7 loss: 0.489
Actual params: [-0.1364,  0.7068]
-Original Grad: 0.130, -lr * Pred Grad:  0.090, New P: -0.047
-Original Grad: 0.090, -lr * Pred Grad:  0.102, New P: 0.809
iter 8 loss: 0.474
Actual params: [-0.0468,  0.809 ]
-Original Grad: 0.165, -lr * Pred Grad:  0.092, New P: 0.045
-Original Grad: 0.072, -lr * Pred Grad:  0.102, New P: 0.911
iter 9 loss: 0.457
Actual params: [0.0451, 0.9112]
-Original Grad: 0.186, -lr * Pred Grad:  0.095, New P: 0.140
-Original Grad: 0.063, -lr * Pred Grad:  0.102, New P: 1.013
iter 10 loss: 0.438
Actual params: [0.1398, 1.0128]
-Original Grad: 0.279, -lr * Pred Grad:  0.095, New P: 0.235
-Original Grad: 0.041, -lr * Pred Grad:  0.099, New P: 1.111
iter 11 loss: 0.414
Actual params: [0.2351, 1.1113]
-Original Grad: 0.338, -lr * Pred Grad:  0.097, New P: 0.332
-Original Grad: 0.069, -lr * Pred Grad:  0.099, New P: 1.210
iter 12 loss: 0.373
Actual params: [0.3319, 1.2105]
-Original Grad: 0.490, -lr * Pred Grad:  0.097, New P: 0.429
-Original Grad: 0.163, -lr * Pred Grad:  0.101, New P: 1.311
iter 13 loss: 0.326
Actual params: [0.4289, 1.3111]
-Original Grad: 0.487, -lr * Pred Grad:  0.100, New P: 0.529
-Original Grad: 0.163, -lr * Pred Grad:  0.103, New P: 1.414
iter 14 loss: 0.283
Actual params: [0.5286, 1.4141]
-Original Grad: 0.363, -lr * Pred Grad:  0.102, New P: 0.631
-Original Grad: 0.155, -lr * Pred Grad:  0.105, New P: 1.519
iter 15 loss: 0.245
Actual params: [0.6306, 1.5191]
-Original Grad: 0.403, -lr * Pred Grad:  0.104, New P: 0.735
-Original Grad: 0.077, -lr * Pred Grad:  0.103, New P: 1.622
iter 16 loss: 0.211
Actual params: [0.7348, 1.6224]
-Original Grad: 0.299, -lr * Pred Grad:  0.105, New P: 0.839
-Original Grad: 0.076, -lr * Pred Grad:  0.102, New P: 1.724
iter 17 loss: 0.182
Actual params: [0.8393, 1.7243]
-Original Grad: 0.484, -lr * Pred Grad:  0.107, New P: 0.946
-Original Grad: 0.044, -lr * Pred Grad:  0.098, New P: 1.822
iter 18 loss: 0.147
Actual params: [0.9463, 1.822 ]
-Original Grad: 0.215, -lr * Pred Grad:  0.105, New P: 1.051
-Original Grad: 0.008, -lr * Pred Grad:  0.090, New P: 1.912
iter 19 loss: 0.117
Actual params: [1.051 , 1.9117]
-Original Grad: 0.208, -lr * Pred Grad:  0.103, New P: 1.154
-Original Grad: 0.031, -lr * Pred Grad:  0.086, New P: 1.997
iter 20 loss: 0.096
Actual params: [1.1535, 1.9973]
-Original Grad: 0.245, -lr * Pred Grad:  0.102, New P: 1.255
-Original Grad: 0.015, -lr * Pred Grad:  0.080, New P: 2.077
iter 21 loss: 0.084
Actual params: [1.2551, 2.077 ]
-Original Grad: 0.148, -lr * Pred Grad:  0.098, New P: 1.353
-Original Grad: 0.032, -lr * Pred Grad:  0.077, New P: 2.154
iter 22 loss: 0.079
Actual params: [1.353 , 2.1537]
-Original Grad: 0.031, -lr * Pred Grad:  0.090, New P: 1.444
-Original Grad: 0.022, -lr * Pred Grad:  0.073, New P: 2.226
iter 23 loss: 0.077
Actual params: [1.4435, 2.2264]
-Original Grad: -0.017, -lr * Pred Grad:  0.082, New P: 1.525
-Original Grad: 0.046, -lr * Pred Grad:  0.072, New P: 2.299
iter 24 loss: 0.073
Actual params: [1.5251, 2.2986]
-Original Grad: 0.009, -lr * Pred Grad:  0.075, New P: 1.600
-Original Grad: 0.053, -lr * Pred Grad:  0.072, New P: 2.371
iter 25 loss: 0.071
Actual params: [1.5999, 2.3711]
-Original Grad: -0.115, -lr * Pred Grad:  0.063, New P: 1.663
-Original Grad: 0.109, -lr * Pred Grad:  0.078, New P: 2.449
iter 26 loss: 0.076
Actual params: [1.6627, 2.4494]
-Original Grad: -0.212, -lr * Pred Grad:  0.047, New P: 1.710
-Original Grad: 0.076, -lr * Pred Grad:  0.080, New P: 2.530
iter 27 loss: 0.082
Actual params: [1.71  , 2.5298]
-Original Grad: -0.134, -lr * Pred Grad:  0.037, New P: 1.747
-Original Grad: -0.001, -lr * Pred Grad:  0.073, New P: 2.603
iter 28 loss: 0.091
Actual params: [1.7469, 2.6029]
-Original Grad: -0.245, -lr * Pred Grad:  0.022, New P: 1.769
-Original Grad: -0.004, -lr * Pred Grad:  0.066, New P: 2.669
iter 29 loss: 0.103
Actual params: [1.7694, 2.669 ]
-Original Grad: -0.127, -lr * Pred Grad:  0.015, New P: 1.784
-Original Grad: -0.232, -lr * Pred Grad:  0.024, New P: 2.693
iter 30 loss: 0.110
Actual params: [1.7841, 2.6931]
-Original Grad: 0.001, -lr * Pred Grad:  0.014, New P: 1.798
-Original Grad: -0.523, -lr * Pred Grad:  -0.028, New P: 2.665
Target params: [1.3344, 1.5708]
iter 0 loss: 0.577
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.006, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.007, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.574
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.024, -lr * Pred Grad:  0.089, New P: -0.283
-Original Grad: 0.024, -lr * Pred Grad:  0.090, New P: 0.194
iter 2 loss: 0.569
Actual params: [-0.2834,  0.1936]
-Original Grad: 0.059, -lr * Pred Grad:  0.086, New P: -0.198
-Original Grad: 0.055, -lr * Pred Grad:  0.087, New P: 0.280
iter 3 loss: 0.558
Actual params: [-0.1976,  0.2803]
-Original Grad: 0.095, -lr * Pred Grad:  0.087, New P: -0.110
-Original Grad: 0.084, -lr * Pred Grad:  0.089, New P: 0.369
iter 4 loss: 0.542
Actual params: [-0.1102,  0.3688]
-Original Grad: 0.140, -lr * Pred Grad:  0.089, New P: -0.021
-Original Grad: 0.116, -lr * Pred Grad:  0.091, New P: 0.459
iter 5 loss: 0.522
Actual params: [-0.0213,  0.4594]
-Original Grad: 0.149, -lr * Pred Grad:  0.092, New P: 0.071
-Original Grad: 0.102, -lr * Pred Grad:  0.094, New P: 0.553
iter 6 loss: 0.497
Actual params: [0.0711, 0.5533]
-Original Grad: 0.222, -lr * Pred Grad:  0.093, New P: 0.164
-Original Grad: 0.126, -lr * Pred Grad:  0.096, New P: 0.650
iter 7 loss: 0.467
Actual params: [0.1645, 0.6495]
-Original Grad: 0.237, -lr * Pred Grad:  0.096, New P: 0.260
-Original Grad: 0.123, -lr * Pred Grad:  0.098, New P: 0.748
iter 8 loss: 0.435
Actual params: [0.2602, 0.7478]
-Original Grad: 0.347, -lr * Pred Grad:  0.096, New P: 0.356
-Original Grad: 0.119, -lr * Pred Grad:  0.100, New P: 0.847
iter 9 loss: 0.406
Actual params: [0.3564, 0.8475]
-Original Grad: 0.297, -lr * Pred Grad:  0.099, New P: 0.455
-Original Grad: 0.090, -lr * Pred Grad:  0.100, New P: 0.947
iter 10 loss: 0.385
Actual params: [0.455, 0.947]
-Original Grad: 0.150, -lr * Pred Grad:  0.097, New P: 0.552
-Original Grad: 0.043, -lr * Pred Grad:  0.095, New P: 1.042
iter 11 loss: 0.370
Actual params: [0.5523, 1.0422]
-Original Grad: 0.070, -lr * Pred Grad:  0.092, New P: 0.645
-Original Grad: 0.066, -lr * Pred Grad:  0.094, New P: 1.136
iter 12 loss: 0.352
Actual params: [0.6447, 1.1363]
-Original Grad: 0.143, -lr * Pred Grad:  0.092, New P: 0.737
-Original Grad: 0.070, -lr * Pred Grad:  0.093, New P: 1.230
iter 13 loss: 0.326
Actual params: [0.7365, 1.2296]
-Original Grad: 0.249, -lr * Pred Grad:  0.095, New P: 0.831
-Original Grad: 0.020, -lr * Pred Grad:  0.087, New P: 1.317
iter 14 loss: 0.294
Actual params: [0.8314, 1.3168]
-Original Grad: 0.265, -lr * Pred Grad:  0.098, New P: 0.929
-Original Grad: -0.014, -lr * Pred Grad:  0.077, New P: 1.393
iter 15 loss: 0.260
Actual params: [0.9289, 1.3934]
-Original Grad: 0.315, -lr * Pred Grad:  0.101, New P: 1.029
-Original Grad: -0.079, -lr * Pred Grad:  0.056, New P: 1.449
iter 16 loss: 0.237
Actual params: [1.0294, 1.4489]
-Original Grad: 0.267, -lr * Pred Grad:  0.102, New P: 1.131
-Original Grad: -0.203, -lr * Pred Grad:  0.016, New P: 1.465
iter 17 loss: 0.223
Actual params: [1.1315, 1.4654]
-Original Grad: 0.144, -lr * Pred Grad:  0.100, New P: 1.231
-Original Grad: -0.236, -lr * Pred Grad:  -0.013, New P: 1.452
iter 18 loss: 0.214
Actual params: [1.2312, 1.452 ]
-Original Grad: -0.034, -lr * Pred Grad:  0.089, New P: 1.320
-Original Grad: -0.312, -lr * Pred Grad:  -0.039, New P: 1.413
iter 19 loss: 0.203
Actual params: [1.3199, 1.4134]
-Original Grad: -0.022, -lr * Pred Grad:  0.079, New P: 1.399
-Original Grad: -0.341, -lr * Pred Grad:  -0.057, New P: 1.357
iter 20 loss: 0.183
Actual params: [1.3992, 1.3567]
-Original Grad: -0.044, -lr * Pred Grad:  0.069, New P: 1.469
-Original Grad: -0.291, -lr * Pred Grad:  -0.068, New P: 1.289
iter 21 loss: 0.163
Actual params: [1.4686, 1.2887]
-Original Grad: -0.048, -lr * Pred Grad:  0.060, New P: 1.529
-Original Grad: -0.189, -lr * Pred Grad:  -0.073, New P: 1.215
iter 22 loss: 0.145
Actual params: [1.5288, 1.2155]
-Original Grad: -0.041, -lr * Pred Grad:  0.052, New P: 1.581
-Original Grad: -0.156, -lr * Pred Grad:  -0.076, New P: 1.139
iter 23 loss: 0.130
Actual params: [1.5811, 1.1393]
-Original Grad: -0.043, -lr * Pred Grad:  0.045, New P: 1.626
-Original Grad: -0.136, -lr * Pred Grad:  -0.078, New P: 1.062
iter 24 loss: 0.118
Actual params: [1.626 , 1.0617]
-Original Grad: -0.047, -lr * Pred Grad:  0.038, New P: 1.664
-Original Grad: -0.152, -lr * Pred Grad:  -0.080, New P: 0.982
iter 25 loss: 0.109
Actual params: [1.6639, 0.9818]
-Original Grad: -0.060, -lr * Pred Grad:  0.031, New P: 1.695
-Original Grad: -0.115, -lr * Pred Grad:  -0.080, New P: 0.902
iter 26 loss: 0.104
Actual params: [1.6946, 0.9019]
-Original Grad: -0.060, -lr * Pred Grad:  0.024, New P: 1.719
-Original Grad: -0.064, -lr * Pred Grad:  -0.077, New P: 0.825
iter 27 loss: 0.102
Actual params: [1.7186, 0.8248]
-Original Grad: -0.089, -lr * Pred Grad:  0.016, New P: 1.735
-Original Grad: -0.012, -lr * Pred Grad:  -0.071, New P: 0.754
iter 28 loss: 0.100
Actual params: [1.7348, 0.7537]
-Original Grad: -0.081, -lr * Pred Grad:  0.009, New P: 1.744
-Original Grad: -0.053, -lr * Pred Grad:  -0.068, New P: 0.685
iter 29 loss: 0.099
Actual params: [1.7442, 0.6853]
-Original Grad: -0.068, -lr * Pred Grad:  0.004, New P: 1.748
-Original Grad: -0.022, -lr * Pred Grad:  -0.064, New P: 0.621
iter 30 loss: 0.098
Actual params: [1.7483, 0.6214]
-Original Grad: -0.091, -lr * Pred Grad:  -0.002, New P: 1.746
-Original Grad: -0.024, -lr * Pred Grad:  -0.060, New P: 0.562
Target params: [1.3344, 1.5708]
iter 0 loss: 0.486
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.019, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.033, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.476
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.062, -lr * Pred Grad:  0.091, New P: -0.282
-Original Grad: 0.097, -lr * Pred Grad:  0.092, New P: 0.196
iter 2 loss: 0.463
Actual params: [-0.2817,  0.1957]
-Original Grad: 0.069, -lr * Pred Grad:  0.094, New P: -0.187
-Original Grad: 0.096, -lr * Pred Grad:  0.096, New P: 0.291
iter 3 loss: 0.443
Actual params: [-0.1872,  0.2913]
-Original Grad: 0.139, -lr * Pred Grad:  0.092, New P: -0.096
-Original Grad: 0.155, -lr * Pred Grad:  0.096, New P: 0.387
iter 4 loss: 0.413
Actual params: [-0.0956,  0.387 ]
-Original Grad: 0.202, -lr * Pred Grad:  0.091, New P: -0.004
-Original Grad: 0.178, -lr * Pred Grad:  0.097, New P: 0.484
iter 5 loss: 0.371
Actual params: [-0.0041,  0.484 ]
-Original Grad: 0.260, -lr * Pred Grad:  0.093, New P: 0.089
-Original Grad: 0.169, -lr * Pred Grad:  0.099, New P: 0.583
iter 6 loss: 0.317
Actual params: [0.0886, 0.5825]
-Original Grad: 0.429, -lr * Pred Grad:  0.091, New P: 0.180
-Original Grad: 0.152, -lr * Pred Grad:  0.099, New P: 0.682
iter 7 loss: 0.255
Actual params: [0.1798, 0.6819]
-Original Grad: 0.496, -lr * Pred Grad:  0.093, New P: 0.273
-Original Grad: 0.148, -lr * Pred Grad:  0.100, New P: 0.782
iter 8 loss: 0.205
Actual params: [0.273 , 0.7819]
-Original Grad: 0.347, -lr * Pred Grad:  0.096, New P: 0.369
-Original Grad: 0.074, -lr * Pred Grad:  0.096, New P: 0.878
iter 9 loss: 0.171
Actual params: [0.3685, 0.8783]
-Original Grad: 0.225, -lr * Pred Grad:  0.095, New P: 0.464
-Original Grad: 0.022, -lr * Pred Grad:  0.089, New P: 0.967
iter 10 loss: 0.151
Actual params: [0.4635, 0.9669]
-Original Grad: 0.178, -lr * Pred Grad:  0.093, New P: 0.557
-Original Grad: 0.006, -lr * Pred Grad:  0.080, New P: 1.047
iter 11 loss: 0.137
Actual params: [0.5567, 1.0471]
-Original Grad: 0.188, -lr * Pred Grad:  0.092, New P: 0.649
-Original Grad: -0.034, -lr * Pred Grad:  0.068, New P: 1.115
iter 12 loss: 0.121
Actual params: [0.6487, 1.1149]
-Original Grad: 0.242, -lr * Pred Grad:  0.093, New P: 0.741
-Original Grad: -0.017, -lr * Pred Grad:  0.059, New P: 1.174
iter 13 loss: 0.102
Actual params: [0.7413, 1.1739]
-Original Grad: 0.258, -lr * Pred Grad:  0.094, New P: 0.835
-Original Grad: -0.028, -lr * Pred Grad:  0.050, New P: 1.224
iter 14 loss: 0.082
Actual params: [0.8349, 1.2236]
-Original Grad: 0.400, -lr * Pred Grad:  0.097, New P: 0.932
-Original Grad: -0.078, -lr * Pred Grad:  0.035, New P: 1.258
iter 15 loss: 0.061
Actual params: [0.9318, 1.2583]
-Original Grad: 0.189, -lr * Pred Grad:  0.095, New P: 1.027
-Original Grad: -0.004, -lr * Pred Grad:  0.031, New P: 1.289
iter 16 loss: 0.049
Actual params: [1.0269, 1.2893]
-Original Grad: 0.067, -lr * Pred Grad:  0.089, New P: 1.116
-Original Grad: -0.012, -lr * Pred Grad:  0.027, New P: 1.316
iter 17 loss: 0.042
Actual params: [1.1162, 1.3159]
-Original Grad: 0.040, -lr * Pred Grad:  0.083, New P: 1.199
-Original Grad: -0.072, -lr * Pred Grad:  0.015, New P: 1.331
iter 18 loss: 0.042
Actual params: [1.199 , 1.3309]
-Original Grad: -0.017, -lr * Pred Grad:  0.074, New P: 1.274
-Original Grad: -0.083, -lr * Pred Grad:  0.003, New P: 1.334
iter 19 loss: 0.046
Actual params: [1.2735, 1.3343]
-Original Grad: -0.095, -lr * Pred Grad:  0.063, New P: 1.336
-Original Grad: -0.083, -lr * Pred Grad:  -0.007, New P: 1.327
iter 20 loss: 0.051
Actual params: [1.3364, 1.3274]
-Original Grad: -0.083, -lr * Pred Grad:  0.053, New P: 1.390
-Original Grad: -0.123, -lr * Pred Grad:  -0.020, New P: 1.307
iter 21 loss: 0.053
Actual params: [1.3896, 1.3071]
-Original Grad: -0.078, -lr * Pred Grad:  0.044, New P: 1.434
-Original Grad: -0.121, -lr * Pred Grad:  -0.032, New P: 1.276
iter 22 loss: 0.051
Actual params: [1.434 , 1.2755]
-Original Grad: -0.056, -lr * Pred Grad:  0.038, New P: 1.472
-Original Grad: -0.142, -lr * Pred Grad:  -0.043, New P: 1.233
iter 23 loss: 0.047
Actual params: [1.4717, 1.2326]
-Original Grad: -0.036, -lr * Pred Grad:  0.033, New P: 1.504
-Original Grad: -0.132, -lr * Pred Grad:  -0.052, New P: 1.181
iter 24 loss: 0.042
Actual params: [1.5042, 1.1809]
-Original Grad: -0.060, -lr * Pred Grad:  0.027, New P: 1.531
-Original Grad: -0.145, -lr * Pred Grad:  -0.060, New P: 1.121
iter 25 loss: 0.036
Actual params: [1.5308, 1.1206]
-Original Grad: -0.062, -lr * Pred Grad:  0.021, New P: 1.552
-Original Grad: -0.113, -lr * Pred Grad:  -0.065, New P: 1.055
iter 26 loss: 0.030
Actual params: [1.5519, 1.0554]
-Original Grad: -0.049, -lr * Pred Grad:  0.017, New P: 1.568
-Original Grad: -0.086, -lr * Pred Grad:  -0.067, New P: 0.988
iter 27 loss: 0.026
Actual params: [1.5685, 0.988 ]
-Original Grad: -0.038, -lr * Pred Grad:  0.013, New P: 1.582
-Original Grad: -0.052, -lr * Pred Grad:  -0.066, New P: 0.921
iter 28 loss: 0.024
Actual params: [1.5817, 0.9215]
-Original Grad: -0.041, -lr * Pred Grad:  0.010, New P: 1.592
-Original Grad: -0.032, -lr * Pred Grad:  -0.064, New P: 0.858
iter 29 loss: 0.023
Actual params: [1.5915, 0.8577]
-Original Grad: -0.028, -lr * Pred Grad:  0.008, New P: 1.599
-Original Grad: -0.025, -lr * Pred Grad:  -0.061, New P: 0.797
iter 30 loss: 0.022
Actual params: [1.599 , 0.7969]
-Original Grad: -0.045, -lr * Pred Grad:  0.004, New P: 1.603
-Original Grad: -0.033, -lr * Pred Grad:  -0.059, New P: 0.738
Target params: [1.3344, 1.5708]
iter 0 loss: 0.170
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.095, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.008, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.158
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.096, -lr * Pred Grad:  0.100, New P: -0.272
-Original Grad: 0.019, -lr * Pred Grad:  0.044, New P: -0.053
iter 2 loss: 0.144
Actual params: [-0.2723, -0.0529]
-Original Grad: 0.096, -lr * Pred Grad:  0.100, New P: -0.172
-Original Grad: 0.006, -lr * Pred Grad:  0.051, New P: -0.002
iter 3 loss: 0.129
Actual params: [-0.1723, -0.0017]
-Original Grad: 0.111, -lr * Pred Grad:  0.100, New P: -0.072
-Original Grad: 0.020, -lr * Pred Grad:  0.070, New P: 0.069
iter 4 loss: 0.113
Actual params: [-0.0719,  0.0686]
-Original Grad: 0.094, -lr * Pred Grad:  0.100, New P: 0.028
-Original Grad: -0.001, -lr * Pred Grad:  0.058, New P: 0.127
iter 5 loss: 0.096
Actual params: [0.0282, 0.1266]
-Original Grad: 0.152, -lr * Pred Grad:  0.101, New P: 0.129
-Original Grad: 0.013, -lr * Pred Grad:  0.067, New P: 0.194
iter 6 loss: 0.073
Actual params: [0.1288, 0.1936]
-Original Grad: 0.141, -lr * Pred Grad:  0.101, New P: 0.230
-Original Grad: 0.034, -lr * Pred Grad:  0.077, New P: 0.271
iter 7 loss: 0.043
Actual params: [0.2302, 0.2706]
-Original Grad: 0.205, -lr * Pred Grad:  0.102, New P: 0.332
-Original Grad: 0.054, -lr * Pred Grad:  0.082, New P: 0.353
iter 8 loss: 0.019
Actual params: [0.3319, 0.3526]
-Original Grad: 0.107, -lr * Pred Grad:  0.100, New P: 0.432
-Original Grad: 0.032, -lr * Pred Grad:  0.086, New P: 0.439
iter 9 loss: 0.007
Actual params: [0.4324, 0.439 ]
-Original Grad: 0.012, -lr * Pred Grad:  0.091, New P: 0.524
-Original Grad: -0.006, -lr * Pred Grad:  0.073, New P: 0.512
iter 10 loss: 0.009
Actual params: [0.5236, 0.5124]
-Original Grad: -0.064, -lr * Pred Grad:  0.073, New P: 0.596
-Original Grad: -0.057, -lr * Pred Grad:  0.025, New P: 0.537
iter 11 loss: 0.014
Actual params: [0.5962, 0.5374]
-Original Grad: -0.086, -lr * Pred Grad:  0.053, New P: 0.649
-Original Grad: -0.082, -lr * Pred Grad:  -0.014, New P: 0.524
iter 12 loss: 0.017
Actual params: [0.6494, 0.5236]
-Original Grad: -0.030, -lr * Pred Grad:  0.044, New P: 0.694
-Original Grad: -0.049, -lr * Pred Grad:  -0.029, New P: 0.495
iter 13 loss: 0.019
Actual params: [0.6936, 0.4945]
-Original Grad: -0.068, -lr * Pred Grad:  0.031, New P: 0.725
-Original Grad: -0.070, -lr * Pred Grad:  -0.045, New P: 0.449
iter 14 loss: 0.017
Actual params: [0.7247, 0.449 ]
-Original Grad: -0.058, -lr * Pred Grad:  0.021, New P: 0.746
-Original Grad: -0.070, -lr * Pred Grad:  -0.058, New P: 0.391
iter 15 loss: 0.014
Actual params: [0.7457, 0.3914]
-Original Grad: -0.062, -lr * Pred Grad:  0.011, New P: 0.757
-Original Grad: -0.042, -lr * Pred Grad:  -0.063, New P: 0.329
iter 16 loss: 0.011
Actual params: [0.757 , 0.3288]
-Original Grad: -0.056, -lr * Pred Grad:  0.004, New P: 0.761
-Original Grad: -0.037, -lr * Pred Grad:  -0.066, New P: 0.263
iter 17 loss: 0.008
Actual params: [0.7606, 0.2631]
-Original Grad: -0.036, -lr * Pred Grad:  -0.001, New P: 0.759
-Original Grad: -0.028, -lr * Pred Grad:  -0.067, New P: 0.196
iter 18 loss: 0.007
Actual params: [0.7594, 0.1964]
-Original Grad: -0.042, -lr * Pred Grad:  -0.006, New P: 0.753
-Original Grad: -0.016, -lr * Pred Grad:  -0.065, New P: 0.132
iter 19 loss: 0.006
Actual params: [0.7533, 0.1317]
-Original Grad: -0.023, -lr * Pred Grad:  -0.008, New P: 0.745
-Original Grad: 0.001, -lr * Pred Grad:  -0.059, New P: 0.073
iter 20 loss: 0.007
Actual params: [0.7449, 0.073 ]
-Original Grad: -0.010, -lr * Pred Grad:  -0.009, New P: 0.736
-Original Grad: 0.039, -lr * Pred Grad:  -0.041, New P: 0.032
iter 21 loss: 0.008
Actual params: [0.7361, 0.0316]
-Original Grad: -0.010, -lr * Pred Grad:  -0.009, New P: 0.727
-Original Grad: 0.027, -lr * Pred Grad:  -0.030, New P: 0.002
iter 22 loss: 0.010
Actual params: [0.7268, 0.0019]
-Original Grad: 0.007, -lr * Pred Grad:  -0.008, New P: 0.719
-Original Grad: 0.041, -lr * Pred Grad:  -0.015, New P: -0.013
iter 23 loss: 0.010
Actual params: [ 0.7192, -0.0133]
-Original Grad: 0.011, -lr * Pred Grad:  -0.005, New P: 0.714
-Original Grad: 0.053, -lr * Pred Grad:  0.001, New P: -0.012
iter 24 loss: 0.010
Actual params: [ 0.7137, -0.0123]
-Original Grad: -0.006, -lr * Pred Grad:  -0.006, New P: 0.708
-Original Grad: 0.011, -lr * Pred Grad:  0.004, New P: -0.008
iter 25 loss: 0.010
Actual params: [ 0.7079, -0.0083]
-Original Grad: 0.001, -lr * Pred Grad:  -0.005, New P: 0.703
-Original Grad: 0.049, -lr * Pred Grad:  0.017, New P: 0.008
iter 26 loss: 0.009
Actual params: [0.7028, 0.0083]
-Original Grad: 0.019, -lr * Pred Grad:  -0.002, New P: 0.701
-Original Grad: 0.037, -lr * Pred Grad:  0.025, New P: 0.033
iter 27 loss: 0.008
Actual params: [0.7006, 0.0331]
-Original Grad: 0.009, -lr * Pred Grad:  -0.001, New P: 0.700
-Original Grad: 0.017, -lr * Pred Grad:  0.027, New P: 0.060
iter 28 loss: 0.007
Actual params: [0.6999, 0.0602]
-Original Grad: 0.001, -lr * Pred Grad:  -0.001, New P: 0.699
-Original Grad: 0.003, -lr * Pred Grad:  0.026, New P: 0.086
iter 29 loss: 0.006
Actual params: [0.6994, 0.0859]
-Original Grad: 0.006, -lr * Pred Grad:  0.000, New P: 0.700
-Original Grad: 0.027, -lr * Pred Grad:  0.031, New P: 0.116
iter 30 loss: 0.006
Actual params: [0.6997, 0.1164]
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: 0.700
-Original Grad: 0.016, -lr * Pred Grad:  0.032, New P: 0.149
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.363
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.002, -lr * Pred Grad:  0.090, New P: -0.282
-Original Grad: 0.000, -lr * Pred Grad:  0.090, New P: 0.194
iter 2 loss: 0.363
Actual params: [-0.2824,  0.1938]
-Original Grad: 0.004, -lr * Pred Grad:  0.088, New P: -0.195
-Original Grad: 0.001, -lr * Pred Grad:  0.087, New P: 0.281
iter 3 loss: 0.362
Actual params: [-0.1949,  0.2811]
-Original Grad: 0.009, -lr * Pred Grad:  0.085, New P: -0.110
-Original Grad: 0.002, -lr * Pred Grad:  0.084, New P: 0.365
iter 4 loss: 0.360
Actual params: [-0.11  ,  0.3646]
-Original Grad: 0.034, -lr * Pred Grad:  0.072, New P: -0.038
-Original Grad: 0.009, -lr * Pred Grad:  0.071, New P: 0.435
iter 5 loss: 0.356
Actual params: [-0.0375,  0.4353]
-Original Grad: 0.059, -lr * Pred Grad:  0.077, New P: 0.039
-Original Grad: 0.018, -lr * Pred Grad:  0.075, New P: 0.510
iter 6 loss: 0.347
Actual params: [0.0392, 0.5099]
-Original Grad: 0.152, -lr * Pred Grad:  0.074, New P: 0.113
-Original Grad: 0.047, -lr * Pred Grad:  0.072, New P: 0.582
iter 7 loss: 0.329
Actual params: [0.1131, 0.5823]
-Original Grad: 0.205, -lr * Pred Grad:  0.080, New P: 0.193
-Original Grad: 0.067, -lr * Pred Grad:  0.078, New P: 0.661
iter 8 loss: 0.300
Actual params: [0.1926, 0.6606]
-Original Grad: 0.368, -lr * Pred Grad:  0.081, New P: 0.273
-Original Grad: 0.114, -lr * Pred Grad:  0.081, New P: 0.741
iter 9 loss: 0.261
Actual params: [0.2735, 0.7413]
-Original Grad: 0.469, -lr * Pred Grad:  0.085, New P: 0.358
-Original Grad: 0.114, -lr * Pred Grad:  0.086, New P: 0.828
iter 10 loss: 0.207
Actual params: [0.3584, 0.8277]
-Original Grad: 0.749, -lr * Pred Grad:  0.086, New P: 0.445
-Original Grad: 0.148, -lr * Pred Grad:  0.091, New P: 0.918
iter 11 loss: 0.144
Actual params: [0.4448, 0.9182]
-Original Grad: 0.606, -lr * Pred Grad:  0.091, New P: 0.536
-Original Grad: 0.061, -lr * Pred Grad:  0.091, New P: 1.009
iter 12 loss: 0.102
Actual params: [0.5361, 1.0091]
-Original Grad: 0.353, -lr * Pred Grad:  0.093, New P: 0.629
-Original Grad: -0.031, -lr * Pred Grad:  0.075, New P: 1.084
iter 13 loss: 0.079
Actual params: [0.629 , 1.0843]
-Original Grad: 0.169, -lr * Pred Grad:  0.090, New P: 0.719
-Original Grad: -0.020, -lr * Pred Grad:  0.064, New P: 1.148
iter 14 loss: 0.062
Actual params: [0.7187, 1.1481]
-Original Grad: 0.134, -lr * Pred Grad:  0.086, New P: 0.805
-Original Grad: 0.026, -lr * Pred Grad:  0.062, New P: 1.211
iter 15 loss: 0.048
Actual params: [0.8047, 1.2106]
-Original Grad: 0.088, -lr * Pred Grad:  0.081, New P: 0.886
-Original Grad: 0.042, -lr * Pred Grad:  0.064, New P: 1.275
iter 16 loss: 0.036
Actual params: [0.8859, 1.2746]
-Original Grad: 0.085, -lr * Pred Grad:  0.077, New P: 0.963
-Original Grad: 0.011, -lr * Pred Grad:  0.060, New P: 1.335
iter 17 loss: 0.025
Actual params: [0.9629, 1.3347]
-Original Grad: 0.090, -lr * Pred Grad:  0.073, New P: 1.036
-Original Grad: -0.029, -lr * Pred Grad:  0.048, New P: 1.383
iter 18 loss: 0.021
Actual params: [1.0362, 1.3831]
-Original Grad: 0.038, -lr * Pred Grad:  0.068, New P: 1.104
-Original Grad: -0.043, -lr * Pred Grad:  0.035, New P: 1.418
iter 19 loss: 0.020
Actual params: [1.1043, 1.418 ]
-Original Grad: 0.004, -lr * Pred Grad:  0.062, New P: 1.166
-Original Grad: -0.088, -lr * Pred Grad:  0.013, New P: 1.431
iter 20 loss: 0.020
Actual params: [1.1664, 1.4314]
-Original Grad: 0.017, -lr * Pred Grad:  0.057, New P: 1.224
-Original Grad: -0.091, -lr * Pred Grad:  -0.005, New P: 1.427
iter 21 loss: 0.019
Actual params: [1.2236, 1.4266]
-Original Grad: -0.003, -lr * Pred Grad:  0.052, New P: 1.276
-Original Grad: -0.071, -lr * Pred Grad:  -0.017, New P: 1.410
iter 22 loss: 0.018
Actual params: [1.2756, 1.4097]
-Original Grad: -0.004, -lr * Pred Grad:  0.047, New P: 1.323
-Original Grad: -0.046, -lr * Pred Grad:  -0.023, New P: 1.386
iter 23 loss: 0.017
Actual params: [1.3228, 1.3865]
-Original Grad: -0.030, -lr * Pred Grad:  0.042, New P: 1.364
-Original Grad: -0.090, -lr * Pred Grad:  -0.036, New P: 1.351
iter 24 loss: 0.016
Actual params: [1.3644, 1.3508]
-Original Grad: -0.031, -lr * Pred Grad:  0.037, New P: 1.401
-Original Grad: -0.061, -lr * Pred Grad:  -0.042, New P: 1.309
iter 25 loss: 0.015
Actual params: [1.401 , 1.3086]
-Original Grad: -0.046, -lr * Pred Grad:  0.031, New P: 1.432
-Original Grad: -0.056, -lr * Pred Grad:  -0.047, New P: 1.261
iter 26 loss: 0.015
Actual params: [1.4323, 1.2612]
-Original Grad: -0.044, -lr * Pred Grad:  0.027, New P: 1.459
-Original Grad: -0.033, -lr * Pred Grad:  -0.049, New P: 1.213
iter 27 loss: 0.015
Actual params: [1.4588, 1.2126]
-Original Grad: -0.078, -lr * Pred Grad:  0.021, New P: 1.479
-Original Grad: -0.016, -lr * Pred Grad:  -0.047, New P: 1.166
iter 28 loss: 0.016
Actual params: [1.4795, 1.1657]
-Original Grad: -0.084, -lr * Pred Grad:  0.015, New P: 1.495
-Original Grad: -0.012, -lr * Pred Grad:  -0.045, New P: 1.121
iter 29 loss: 0.016
Actual params: [1.4945, 1.1208]
-Original Grad: -0.071, -lr * Pred Grad:  0.010, New P: 1.505
-Original Grad: -0.014, -lr * Pred Grad:  -0.043, New P: 1.078
iter 30 loss: 0.017
Actual params: [1.5049, 1.0776]
-Original Grad: -0.111, -lr * Pred Grad:  0.004, New P: 1.509
-Original Grad: 0.010, -lr * Pred Grad:  -0.038, New P: 1.040
Target params: [1.3344, 1.5708]
iter 0 loss: 0.320
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.020, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.005, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.317
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.004, -lr * Pred Grad:  -0.080, New P: -0.652
-Original Grad: 0.004, -lr * Pred Grad:  0.099, New P: 0.203
iter 2 loss: 0.316
Actual params: [-0.6524,  0.2026]
-Original Grad: -0.007, -lr * Pred Grad:  -0.080, New P: -0.732
-Original Grad: 0.004, -lr * Pred Grad:  0.097, New P: 0.300
iter 3 loss: 0.315
Actual params: [-0.7321,  0.3   ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.068, New P: -0.800
-Original Grad: 0.002, -lr * Pred Grad:  0.092, New P: 0.392
iter 4 loss: 0.315
Actual params: [-0.8   ,  0.3919]
-Original Grad: 0.000, -lr * Pred Grad:  -0.056, New P: -0.856
-Original Grad: 0.001, -lr * Pred Grad:  0.083, New P: 0.475
iter 5 loss: 0.315
Actual params: [-0.8562,  0.4753]
-Original Grad: 0.000, -lr * Pred Grad:  -0.047, New P: -0.904
-Original Grad: 0.001, -lr * Pred Grad:  0.075, New P: 0.550
iter 6 loss: 0.315
Actual params: [-0.9037,  0.5502]
-Original Grad: 0.001, -lr * Pred Grad:  -0.040, New P: -0.944
-Original Grad: 0.000, -lr * Pred Grad:  0.067, New P: 0.617
iter 7 loss: 0.315
Actual params: [-0.9437,  0.6169]
-Original Grad: 0.000, -lr * Pred Grad:  -0.035, New P: -0.978
-Original Grad: 0.000, -lr * Pred Grad:  0.059, New P: 0.676
iter 8 loss: 0.315
Actual params: [-0.9782,  0.6764]
-Original Grad: 0.000, -lr * Pred Grad:  -0.030, New P: -1.008
-Original Grad: 0.000, -lr * Pred Grad:  0.053, New P: 0.730
iter 9 loss: 0.315
Actual params: [-1.008 ,  0.7297]
-Original Grad: 0.000, -lr * Pred Grad:  -0.026, New P: -1.034
-Original Grad: 0.000, -lr * Pred Grad:  0.048, New P: 0.777
iter 10 loss: 0.315
Actual params: [-1.0338,  0.7775]
-Original Grad: 0.000, -lr * Pred Grad:  -0.023, New P: -1.057
-Original Grad: -0.000, -lr * Pred Grad:  0.043, New P: 0.820
iter 11 loss: 0.315
Actual params: [-1.0565,  0.8203]
-Original Grad: 0.000, -lr * Pred Grad:  -0.020, New P: -1.077
-Original Grad: 0.000, -lr * Pred Grad:  0.039, New P: 0.859
iter 12 loss: 0.315
Actual params: [-1.0765,  0.8589]
-Original Grad: 0.000, -lr * Pred Grad:  -0.018, New P: -1.094
-Original Grad: -0.000, -lr * Pred Grad:  0.035, New P: 0.894
iter 13 loss: 0.315
Actual params: [-1.0943,  0.8936]
-Original Grad: 0.000, -lr * Pred Grad:  -0.016, New P: -1.110
-Original Grad: -0.000, -lr * Pred Grad:  0.031, New P: 0.925
iter 14 loss: 0.315
Actual params: [-1.11  ,  0.9249]
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -1.124
-Original Grad: -0.000, -lr * Pred Grad:  0.028, New P: 0.953
iter 15 loss: 0.315
Actual params: [-1.124 ,  0.9533]
-Original Grad: 0.000, -lr * Pred Grad:  -0.012, New P: -1.136
-Original Grad: -0.000, -lr * Pred Grad:  0.026, New P: 0.979
iter 16 loss: 0.315
Actual params: [-1.1365,  0.9789]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.148
-Original Grad: -0.000, -lr * Pred Grad:  0.023, New P: 1.002
iter 17 loss: 0.315
Actual params: [-1.1476,  1.0021]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.158
-Original Grad: -0.000, -lr * Pred Grad:  0.021, New P: 1.023
iter 18 loss: 0.315
Actual params: [-1.1575,  1.0231]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.166
-Original Grad: -0.000, -lr * Pred Grad:  0.019, New P: 1.042
iter 19 loss: 0.315
Actual params: [-1.1664,  1.0422]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.174
-Original Grad: -0.000, -lr * Pred Grad:  0.017, New P: 1.060
iter 20 loss: 0.315
Actual params: [-1.1742,  1.0595]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.181
-Original Grad: -0.000, -lr * Pred Grad:  0.016, New P: 1.075
iter 21 loss: 0.315
Actual params: [-1.1813,  1.0752]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.188
-Original Grad: -0.000, -lr * Pred Grad:  0.014, New P: 1.089
iter 22 loss: 0.315
Actual params: [-1.1876,  1.0895]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.193
-Original Grad: -0.000, -lr * Pred Grad:  0.013, New P: 1.102
iter 23 loss: 0.315
Actual params: [-1.1932,  1.1024]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.198
-Original Grad: -0.000, -lr * Pred Grad:  0.012, New P: 1.114
iter 24 loss: 0.315
Actual params: [-1.1982,  1.1141]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.203
-Original Grad: -0.000, -lr * Pred Grad:  0.011, New P: 1.125
iter 25 loss: 0.315
Actual params: [-1.2026,  1.1246]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.207
-Original Grad: -0.000, -lr * Pred Grad:  0.010, New P: 1.134
iter 26 loss: 0.315
Actual params: [-1.2066,  1.1342]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.210
-Original Grad: -0.000, -lr * Pred Grad:  0.009, New P: 1.143
iter 27 loss: 0.315
Actual params: [-1.2101,  1.1429]
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -1.213
-Original Grad: -0.000, -lr * Pred Grad:  0.008, New P: 1.151
iter 28 loss: 0.315
Actual params: [-1.2133,  1.1508]
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -1.216
-Original Grad: -0.000, -lr * Pred Grad:  0.007, New P: 1.158
iter 29 loss: 0.315
Actual params: [-1.216 ,  1.1579]
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: -1.218
-Original Grad: -0.000, -lr * Pred Grad:  0.006, New P: 1.164
iter 30 loss: 0.315
Actual params: [-1.2185,  1.1644]
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: -1.221
-Original Grad: -0.000, -lr * Pred Grad:  0.006, New P: 1.170
Target params: [1.3344, 1.5708]
iter 0 loss: 0.583
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.023, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.007, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.578
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.080, -lr * Pred Grad:  0.090, New P: -0.282
-Original Grad: 0.012, -lr * Pred Grad:  0.098, New P: 0.202
iter 2 loss: 0.571
Actual params: [-0.2822,  0.2019]
-Original Grad: 0.081, -lr * Pred Grad:  0.094, New P: -0.188
-Original Grad: 0.011, -lr * Pred Grad:  0.099, New P: 0.301
iter 3 loss: 0.563
Actual params: [-0.1877,  0.3011]
-Original Grad: 0.090, -lr * Pred Grad:  0.097, New P: -0.091
-Original Grad: 0.003, -lr * Pred Grad:  0.090, New P: 0.391
iter 4 loss: 0.553
Actual params: [-0.091 ,  0.3908]
-Original Grad: 0.120, -lr * Pred Grad:  0.098, New P: 0.007
-Original Grad: 0.013, -lr * Pred Grad:  0.093, New P: 0.484
iter 5 loss: 0.539
Actual params: [0.0069, 0.4842]
-Original Grad: 0.118, -lr * Pred Grad:  0.099, New P: 0.106
-Original Grad: 0.014, -lr * Pred Grad:  0.096, New P: 0.580
iter 6 loss: 0.521
Actual params: [0.106 , 0.5802]
-Original Grad: 0.202, -lr * Pred Grad:  0.098, New P: 0.204
-Original Grad: 0.013, -lr * Pred Grad:  0.098, New P: 0.678
iter 7 loss: 0.495
Actual params: [0.2042, 0.6778]
-Original Grad: 0.310, -lr * Pred Grad:  0.096, New P: 0.300
-Original Grad: 0.007, -lr * Pred Grad:  0.095, New P: 0.773
iter 8 loss: 0.459
Actual params: [0.3001, 0.7729]
-Original Grad: 0.383, -lr * Pred Grad:  0.096, New P: 0.396
-Original Grad: 0.028, -lr * Pred Grad:  0.095, New P: 0.868
iter 9 loss: 0.412
Actual params: [0.3962, 0.8678]
-Original Grad: 0.653, -lr * Pred Grad:  0.093, New P: 0.489
-Original Grad: 0.047, -lr * Pred Grad:  0.092, New P: 0.960
iter 10 loss: 0.350
Actual params: [0.4893, 0.9603]
-Original Grad: 0.518, -lr * Pred Grad:  0.096, New P: 0.586
-Original Grad: 0.083, -lr * Pred Grad:  0.088, New P: 1.049
iter 11 loss: 0.286
Actual params: [0.5856, 1.0486]
-Original Grad: 0.556, -lr * Pred Grad:  0.099, New P: 0.685
-Original Grad: 0.106, -lr * Pred Grad:  0.090, New P: 1.139
iter 12 loss: 0.229
Actual params: [0.6847, 1.1386]
-Original Grad: 0.391, -lr * Pred Grad:  0.100, New P: 0.785
-Original Grad: 0.140, -lr * Pred Grad:  0.092, New P: 1.231
iter 13 loss: 0.182
Actual params: [0.7848, 1.2306]
-Original Grad: 0.402, -lr * Pred Grad:  0.101, New P: 0.886
-Original Grad: 0.083, -lr * Pred Grad:  0.095, New P: 1.326
iter 14 loss: 0.141
Actual params: [0.8858, 1.3258]
-Original Grad: 0.326, -lr * Pred Grad:  0.101, New P: 0.986
-Original Grad: 0.061, -lr * Pred Grad:  0.096, New P: 1.422
iter 15 loss: 0.106
Actual params: [0.9864, 1.4219]
-Original Grad: 0.220, -lr * Pred Grad:  0.098, New P: 1.084
-Original Grad: 0.043, -lr * Pred Grad:  0.095, New P: 1.517
iter 16 loss: 0.080
Actual params: [1.0843, 1.5165]
-Original Grad: 0.153, -lr * Pred Grad:  0.094, New P: 1.178
-Original Grad: 0.008, -lr * Pred Grad:  0.087, New P: 1.604
iter 17 loss: 0.061
Actual params: [1.1782, 1.604 ]
-Original Grad: 0.156, -lr * Pred Grad:  0.090, New P: 1.269
-Original Grad: -0.015, -lr * Pred Grad:  0.076, New P: 1.680
iter 18 loss: 0.048
Actual params: [1.2686, 1.68  ]
-Original Grad: 0.089, -lr * Pred Grad:  0.085, New P: 1.354
-Original Grad: 0.038, -lr * Pred Grad:  0.076, New P: 1.756
iter 19 loss: 0.039
Actual params: [1.3539, 1.7564]
-Original Grad: 0.056, -lr * Pred Grad:  0.080, New P: 1.433
-Original Grad: -0.023, -lr * Pred Grad:  0.064, New P: 1.821
iter 20 loss: 0.034
Actual params: [1.4335, 1.8206]
-Original Grad: 0.044, -lr * Pred Grad:  0.074, New P: 1.507
-Original Grad: 0.016, -lr * Pred Grad:  0.062, New P: 1.882
iter 21 loss: 0.031
Actual params: [1.5075, 1.8822]
-Original Grad: -0.029, -lr * Pred Grad:  0.066, New P: 1.574
-Original Grad: 0.027, -lr * Pred Grad:  0.062, New P: 1.944
iter 22 loss: 0.031
Actual params: [1.5737, 1.9439]
-Original Grad: -0.084, -lr * Pred Grad:  0.057, New P: 1.631
-Original Grad: 0.068, -lr * Pred Grad:  0.068, New P: 2.012
iter 23 loss: 0.032
Actual params: [1.6307, 2.0122]
-Original Grad: -0.040, -lr * Pred Grad:  0.050, New P: 1.681
-Original Grad: 0.047, -lr * Pred Grad:  0.071, New P: 2.083
iter 24 loss: 0.035
Actual params: [1.681 , 2.0832]
-Original Grad: -0.112, -lr * Pred Grad:  0.041, New P: 1.722
-Original Grad: 0.015, -lr * Pred Grad:  0.068, New P: 2.151
iter 25 loss: 0.038
Actual params: [1.7222, 2.1509]
-Original Grad: -0.142, -lr * Pred Grad:  0.032, New P: 1.754
-Original Grad: 0.024, -lr * Pred Grad:  0.067, New P: 2.217
iter 26 loss: 0.042
Actual params: [1.754 , 2.2175]
-Original Grad: -0.145, -lr * Pred Grad:  0.023, New P: 1.777
-Original Grad: -0.027, -lr * Pred Grad:  0.054, New P: 2.272
iter 27 loss: 0.048
Actual params: [1.777 , 2.2719]
-Original Grad: -0.160, -lr * Pred Grad:  0.014, New P: 1.791
-Original Grad: -0.064, -lr * Pred Grad:  0.034, New P: 2.306
iter 28 loss: 0.053
Actual params: [1.7915, 2.3063]
-Original Grad: -0.127, -lr * Pred Grad:  0.008, New P: 1.799
-Original Grad: -0.094, -lr * Pred Grad:  0.011, New P: 2.317
iter 29 loss: 0.056
Actual params: [1.7995, 2.3171]
-Original Grad: -0.071, -lr * Pred Grad:  0.004, New P: 1.804
-Original Grad: -0.097, -lr * Pred Grad:  -0.009, New P: 2.308
iter 30 loss: 0.056
Actual params: [1.8039, 2.3076]
-Original Grad: -0.169, -lr * Pred Grad:  -0.003, New P: 1.801
-Original Grad: -0.081, -lr * Pred Grad:  -0.024, New P: 2.284
Target params: [1.3344, 1.5708]
iter 0 loss: 0.018
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.014, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.003, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.016
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.017, -lr * Pred Grad:  0.100, New P: -0.272
-Original Grad: -0.001, -lr * Pred Grad:  -0.085, New P: -0.181
iter 2 loss: 0.014
Actual params: [-0.2723, -0.1814]
-Original Grad: 0.016, -lr * Pred Grad:  0.100, New P: -0.172
-Original Grad: 0.001, -lr * Pred Grad:  -0.050, New P: -0.232
iter 3 loss: 0.012
Actual params: [-0.1721, -0.2319]
-Original Grad: 0.012, -lr * Pred Grad:  0.099, New P: -0.073
-Original Grad: 0.001, -lr * Pred Grad:  -0.030, New P: -0.262
iter 4 loss: 0.011
Actual params: [-0.0733, -0.2623]
-Original Grad: 0.001, -lr * Pred Grad:  0.085, New P: 0.012
-Original Grad: 0.001, -lr * Pred Grad:  -0.012, New P: -0.274
iter 5 loss: 0.013
Actual params: [ 0.0121, -0.2743]
-Original Grad: -0.036, -lr * Pred Grad:  0.006, New P: 0.018
-Original Grad: 0.005, -lr * Pred Grad:  0.038, New P: -0.236
iter 6 loss: 0.013
Actual params: [ 0.0184, -0.2364]
-Original Grad: -0.022, -lr * Pred Grad:  -0.016, New P: 0.002
-Original Grad: 0.003, -lr * Pred Grad:  0.050, New P: -0.186
iter 7 loss: 0.012
Actual params: [ 0.002 , -0.1863]
-Original Grad: -0.033, -lr * Pred Grad:  -0.039, New P: -0.037
-Original Grad: 0.005, -lr * Pred Grad:  0.064, New P: -0.122
iter 8 loss: 0.011
Actual params: [-0.0367, -0.1219]
-Original Grad: -0.011, -lr * Pred Grad:  -0.042, New P: -0.079
-Original Grad: 0.002, -lr * Pred Grad:  0.067, New P: -0.055
iter 9 loss: 0.011
Actual params: [-0.0789, -0.0546]
-Original Grad: 0.007, -lr * Pred Grad:  -0.032, New P: -0.111
-Original Grad: -0.003, -lr * Pred Grad:  0.040, New P: -0.014
iter 10 loss: 0.012
Actual params: [-0.1107, -0.0144]
-Original Grad: 0.010, -lr * Pred Grad:  -0.021, New P: -0.132
-Original Grad: -0.002, -lr * Pred Grad:  0.024, New P: 0.009
iter 11 loss: 0.012
Actual params: [-0.1315,  0.0093]
-Original Grad: 0.014, -lr * Pred Grad:  -0.008, New P: -0.139
-Original Grad: -0.006, -lr * Pred Grad:  -0.010, New P: -0.000
iter 12 loss: 0.012
Actual params: [-0.1392, -0.0004]
-Original Grad: 0.008, -lr * Pred Grad:  -0.001, New P: -0.140
-Original Grad: -0.003, -lr * Pred Grad:  -0.020, New P: -0.020
iter 13 loss: 0.012
Actual params: [-0.1404, -0.0203]
-Original Grad: 0.013, -lr * Pred Grad:  0.008, New P: -0.132
-Original Grad: -0.003, -lr * Pred Grad:  -0.029, New P: -0.050
iter 14 loss: 0.012
Actual params: [-0.1324, -0.0497]
-Original Grad: 0.007, -lr * Pred Grad:  0.012, New P: -0.120
-Original Grad: -0.001, -lr * Pred Grad:  -0.032, New P: -0.081
iter 15 loss: 0.011
Actual params: [-0.1202, -0.0814]
-Original Grad: 0.009, -lr * Pred Grad:  0.017, New P: -0.103
-Original Grad: -0.002, -lr * Pred Grad:  -0.036, New P: -0.117
iter 16 loss: 0.011
Actual params: [-0.1029, -0.1171]
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: -0.087
-Original Grad: -0.002, -lr * Pred Grad:  -0.038, New P: -0.156
iter 17 loss: 0.011
Actual params: [-0.0866, -0.1555]
-Original Grad: 0.005, -lr * Pred Grad:  0.019, New P: -0.068
-Original Grad: -0.002, -lr * Pred Grad:  -0.043, New P: -0.199
iter 18 loss: 0.011
Actual params: [-0.0678, -0.1986]
-Original Grad: 0.002, -lr * Pred Grad:  0.019, New P: -0.049
-Original Grad: 0.001, -lr * Pred Grad:  -0.036, New P: -0.234
iter 19 loss: 0.011
Actual params: [-0.0491, -0.2341]
-Original Grad: 0.004, -lr * Pred Grad:  0.020, New P: -0.029
-Original Grad: -0.001, -lr * Pred Grad:  -0.036, New P: -0.270
iter 20 loss: 0.012
Actual params: [-0.0293, -0.2697]
-Original Grad: -0.006, -lr * Pred Grad:  0.013, New P: -0.016
-Original Grad: 0.002, -lr * Pred Grad:  -0.025, New P: -0.295
iter 21 loss: 0.012
Actual params: [-0.0162, -0.295 ]
-Original Grad: -0.013, -lr * Pred Grad:  0.002, New P: -0.014
-Original Grad: 0.000, -lr * Pred Grad:  -0.021, New P: -0.316
iter 22 loss: 0.012
Actual params: [-0.0145, -0.3163]
-Original Grad: -0.033, -lr * Pred Grad:  -0.021, New P: -0.036
-Original Grad: 0.001, -lr * Pred Grad:  -0.014, New P: -0.330
iter 23 loss: 0.012
Actual params: [-0.0359, -0.3301]
-Original Grad: -0.010, -lr * Pred Grad:  -0.026, New P: -0.062
-Original Grad: 0.001, -lr * Pred Grad:  -0.010, New P: -0.340
iter 24 loss: 0.011
Actual params: [-0.0621, -0.3402]
-Original Grad: 0.002, -lr * Pred Grad:  -0.022, New P: -0.085
-Original Grad: 0.001, -lr * Pred Grad:  -0.004, New P: -0.344
iter 25 loss: 0.011
Actual params: [-0.0846, -0.3445]
-Original Grad: 0.004, -lr * Pred Grad:  -0.018, New P: -0.103
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: -0.342
iter 26 loss: 0.011
Actual params: [-0.1025, -0.3417]
-Original Grad: 0.004, -lr * Pred Grad:  -0.013, New P: -0.116
-Original Grad: 0.002, -lr * Pred Grad:  0.013, New P: -0.329
iter 27 loss: 0.011
Actual params: [-0.1159, -0.3289]
-Original Grad: 0.004, -lr * Pred Grad:  -0.009, New P: -0.125
-Original Grad: 0.002, -lr * Pred Grad:  0.021, New P: -0.308
iter 28 loss: 0.012
Actual params: [-0.1254, -0.3083]
-Original Grad: 0.007, -lr * Pred Grad:  -0.003, New P: -0.129
-Original Grad: 0.002, -lr * Pred Grad:  0.027, New P: -0.281
iter 29 loss: 0.011
Actual params: [-0.1288, -0.281 ]
-Original Grad: 0.003, -lr * Pred Grad:  -0.001, New P: -0.130
-Original Grad: 0.001, -lr * Pred Grad:  0.029, New P: -0.252
iter 30 loss: 0.011
Actual params: [-0.1298, -0.2519]
-Original Grad: 0.013, -lr * Pred Grad:  0.008, New P: -0.121
-Original Grad: 0.001, -lr * Pred Grad:  0.030, New P: -0.222
Target params: [1.3344, 1.5708]
iter 0 loss: 0.612
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.033, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.022, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.605
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.016, -lr * Pred Grad:  0.092, New P: -0.280
-Original Grad: 0.009, -lr * Pred Grad:  0.090, New P: 0.194
iter 2 loss: 0.598
Actual params: [-0.2799,  0.194 ]
-Original Grad: 0.039, -lr * Pred Grad:  0.096, New P: -0.184
-Original Grad: 0.017, -lr * Pred Grad:  0.094, New P: 0.288
iter 3 loss: 0.587
Actual params: [-0.1844,  0.2878]
-Original Grad: 0.050, -lr * Pred Grad:  0.097, New P: -0.088
-Original Grad: 0.018, -lr * Pred Grad:  0.096, New P: 0.384
iter 4 loss: 0.574
Actual params: [-0.0876,  0.3838]
-Original Grad: 0.070, -lr * Pred Grad:  0.097, New P: 0.009
-Original Grad: 0.020, -lr * Pred Grad:  0.098, New P: 0.481
iter 5 loss: 0.558
Actual params: [0.0092, 0.4814]
-Original Grad: 0.145, -lr * Pred Grad:  0.091, New P: 0.100
-Original Grad: 0.017, -lr * Pred Grad:  0.098, New P: 0.579
iter 6 loss: 0.541
Actual params: [0.0999, 0.5792]
-Original Grad: 0.189, -lr * Pred Grad:  0.091, New P: 0.191
-Original Grad: -0.002, -lr * Pred Grad:  0.083, New P: 0.662
iter 7 loss: 0.520
Actual params: [0.1909, 0.6625]
-Original Grad: 0.167, -lr * Pred Grad:  0.094, New P: 0.285
-Original Grad: -0.016, -lr * Pred Grad:  0.052, New P: 0.714
iter 8 loss: 0.491
Actual params: [0.2853, 0.7144]
-Original Grad: 0.331, -lr * Pred Grad:  0.093, New P: 0.378
-Original Grad: -0.034, -lr * Pred Grad:  0.008, New P: 0.722
iter 9 loss: 0.455
Actual params: [0.3781, 0.7223]
-Original Grad: 0.325, -lr * Pred Grad:  0.096, New P: 0.474
-Original Grad: 0.043, -lr * Pred Grad:  0.035, New P: 0.757
iter 10 loss: 0.405
Actual params: [0.4737, 0.7571]
-Original Grad: 0.250, -lr * Pred Grad:  0.098, New P: 0.572
-Original Grad: 0.047, -lr * Pred Grad:  0.053, New P: 0.810
iter 11 loss: 0.359
Actual params: [0.5716, 0.8097]
-Original Grad: 0.373, -lr * Pred Grad:  0.100, New P: 0.672
-Original Grad: 0.093, -lr * Pred Grad:  0.067, New P: 0.877
iter 12 loss: 0.319
Actual params: [0.6717, 0.8771]
-Original Grad: 0.279, -lr * Pred Grad:  0.102, New P: 0.773
-Original Grad: 0.064, -lr * Pred Grad:  0.076, New P: 0.953
iter 13 loss: 0.276
Actual params: [0.7734, 0.9532]
-Original Grad: 0.190, -lr * Pred Grad:  0.101, New P: 0.874
-Original Grad: 0.049, -lr * Pred Grad:  0.081, New P: 1.034
iter 14 loss: 0.233
Actual params: [0.8742, 1.0339]
-Original Grad: 0.141, -lr * Pred Grad:  0.098, New P: 0.973
-Original Grad: 0.038, -lr * Pred Grad:  0.083, New P: 1.117
iter 15 loss: 0.196
Actual params: [0.9725, 1.1167]
-Original Grad: 0.215, -lr * Pred Grad:  0.099, New P: 1.071
-Original Grad: 0.067, -lr * Pred Grad:  0.088, New P: 1.205
iter 16 loss: 0.167
Actual params: [1.0713, 1.2049]
-Original Grad: 0.093, -lr * Pred Grad:  0.095, New P: 1.166
-Original Grad: 0.053, -lr * Pred Grad:  0.091, New P: 1.296
iter 17 loss: 0.148
Actual params: [1.1658, 1.2961]
-Original Grad: 0.122, -lr * Pred Grad:  0.092, New P: 1.258
-Original Grad: 0.070, -lr * Pred Grad:  0.095, New P: 1.391
iter 18 loss: 0.142
Actual params: [1.258 , 1.3914]
-Original Grad: -0.109, -lr * Pred Grad:  0.077, New P: 1.335
-Original Grad: 0.081, -lr * Pred Grad:  0.099, New P: 1.491
iter 19 loss: 0.145
Actual params: [1.3346, 1.4907]
-Original Grad: -0.232, -lr * Pred Grad:  0.054, New P: 1.388
-Original Grad: 0.101, -lr * Pred Grad:  0.103, New P: 1.594
iter 20 loss: 0.152
Actual params: [1.3885, 1.5942]
-Original Grad: -0.303, -lr * Pred Grad:  0.030, New P: 1.418
-Original Grad: 0.065, -lr * Pred Grad:  0.104, New P: 1.699
iter 21 loss: 0.165
Actual params: [1.4181, 1.6987]
-Original Grad: -0.077, -lr * Pred Grad:  0.023, New P: 1.441
-Original Grad: -0.164, -lr * Pred Grad:  0.049, New P: 1.748
iter 22 loss: 0.180
Actual params: [1.4408, 1.7475]
-Original Grad: -0.088, -lr * Pred Grad:  0.016, New P: 1.456
-Original Grad: -0.136, -lr * Pred Grad:  0.018, New P: 1.765
iter 23 loss: 0.188
Actual params: [1.4564, 1.7654]
-Original Grad: -0.143, -lr * Pred Grad:  0.006, New P: 1.462
-Original Grad: -0.174, -lr * Pred Grad:  -0.011, New P: 1.754
iter 24 loss: 0.188
Actual params: [1.4625, 1.7544]
-Original Grad: -0.125, -lr * Pred Grad:  -0.002, New P: 1.461
-Original Grad: -0.125, -lr * Pred Grad:  -0.027, New P: 1.728
iter 25 loss: 0.183
Actual params: [1.461 , 1.7275]
-Original Grad: -0.316, -lr * Pred Grad:  -0.018, New P: 1.443
-Original Grad: -0.057, -lr * Pred Grad:  -0.032, New P: 1.695
iter 26 loss: 0.173
Actual params: [1.4427, 1.6953]
-Original Grad: -0.287, -lr * Pred Grad:  -0.031, New P: 1.412
-Original Grad: 0.015, -lr * Pred Grad:  -0.027, New P: 1.668
iter 27 loss: 0.161
Actual params: [1.4116, 1.6682]
-Original Grad: -0.330, -lr * Pred Grad:  -0.044, New P: 1.368
-Original Grad: -0.024, -lr * Pred Grad:  -0.028, New P: 1.640
iter 28 loss: 0.150
Actual params: [1.3678, 1.64  ]
-Original Grad: -0.016, -lr * Pred Grad:  -0.041, New P: 1.327
-Original Grad: -0.106, -lr * Pred Grad:  -0.040, New P: 1.601
iter 29 loss: 0.144
Actual params: [1.3272, 1.6005]
-Original Grad: -0.012, -lr * Pred Grad:  -0.038, New P: 1.290
-Original Grad: -0.054, -lr * Pred Grad:  -0.043, New P: 1.557
iter 30 loss: 0.142
Actual params: [1.2896, 1.5572]
-Original Grad: 0.030, -lr * Pred Grad:  -0.033, New P: 1.257
-Original Grad: -0.026, -lr * Pred Grad:  -0.043, New P: 1.514
Target params: [1.3344, 1.5708]
iter 0 loss: 0.082
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.030, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.024, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.076
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.017, -lr * Pred Grad:  -0.095, New P: -0.667
-Original Grad: 0.015, -lr * Pred Grad:  0.097, New P: 0.200
iter 2 loss: 0.072
Actual params: [-0.6672,  0.2001]
-Original Grad: -0.010, -lr * Pred Grad:  -0.088, New P: -0.755
-Original Grad: 0.010, -lr * Pred Grad:  0.091, New P: 0.291
iter 3 loss: 0.071
Actual params: [-0.7552,  0.2914]
-Original Grad: -0.005, -lr * Pred Grad:  -0.080, New P: -0.835
-Original Grad: 0.005, -lr * Pred Grad:  0.084, New P: 0.375
iter 4 loss: 0.070
Actual params: [-0.8347,  0.3753]
-Original Grad: -0.002, -lr * Pred Grad:  -0.071, New P: -0.906
-Original Grad: 0.003, -lr * Pred Grad:  0.075, New P: 0.451
iter 5 loss: 0.070
Actual params: [-0.9056,  0.4508]
-Original Grad: -0.001, -lr * Pred Grad:  -0.063, New P: -0.968
-Original Grad: 0.001, -lr * Pred Grad:  0.067, New P: 0.518
iter 6 loss: 0.070
Actual params: [-0.9684,  0.5178]
-Original Grad: -0.001, -lr * Pred Grad:  -0.056, New P: -1.024
-Original Grad: 0.001, -lr * Pred Grad:  0.060, New P: 0.577
iter 7 loss: 0.070
Actual params: [-1.0242,  0.5773]
-Original Grad: -0.000, -lr * Pred Grad:  -0.050, New P: -1.074
-Original Grad: 0.000, -lr * Pred Grad:  0.053, New P: 0.630
iter 8 loss: 0.070
Actual params: [-1.074 ,  0.6305]
-Original Grad: -0.000, -lr * Pred Grad:  -0.045, New P: -1.119
-Original Grad: 0.000, -lr * Pred Grad:  0.048, New P: 0.678
iter 9 loss: 0.070
Actual params: [-1.1187,  0.6781]
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -1.159
-Original Grad: 0.000, -lr * Pred Grad:  0.043, New P: 0.721
iter 10 loss: 0.070
Actual params: [-1.1588,  0.7209]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -1.195
-Original Grad: 0.000, -lr * Pred Grad:  0.039, New P: 0.759
iter 11 loss: 0.070
Actual params: [-1.195 ,  0.7594]
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -1.228
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: 0.794
iter 12 loss: 0.070
Actual params: [-1.2277,  0.7942]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -1.257
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: 0.826
iter 13 loss: 0.070
Actual params: [-1.2574,  0.8258]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.284
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: 0.854
iter 14 loss: 0.070
Actual params: [-1.2842,  0.8544]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.309
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 0.880
iter 15 loss: 0.070
Actual params: [-1.3086,  0.8803]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.331
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.904
iter 16 loss: 0.070
Actual params: [-1.3307,  0.9039]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.351
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.925
iter 17 loss: 0.070
Actual params: [-1.3509,  0.9253]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.369
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.945
iter 18 loss: 0.070
Actual params: [-1.3692,  0.9449]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.386
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.963
iter 19 loss: 0.070
Actual params: [-1.3859,  0.9627]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.401
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.979
iter 20 loss: 0.070
Actual params: [-1.4011,  0.9789]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.415
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.994
iter 21 loss: 0.070
Actual params: [-1.415 ,  0.9937]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.428
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 1.007
iter 22 loss: 0.070
Actual params: [-1.4276,  1.0071]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.439
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 1.019
iter 23 loss: 0.070
Actual params: [-1.4391,  1.0195]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.450
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 1.031
iter 24 loss: 0.070
Actual params: [-1.4496,  1.0307]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.459
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 1.041
iter 25 loss: 0.070
Actual params: [-1.4592,  1.041 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.468
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 1.050
iter 26 loss: 0.070
Actual params: [-1.468 ,  1.0503]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.476
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 1.059
iter 27 loss: 0.070
Actual params: [-1.476 ,  1.0589]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.483
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 1.067
iter 28 loss: 0.070
Actual params: [-1.4833,  1.0667]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.490
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 1.074
iter 29 loss: 0.070
Actual params: [-1.4899,  1.0739]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.496
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 1.080
iter 30 loss: 0.070
Actual params: [-1.496 ,  1.0804]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.502
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 1.086
Target params: [1.3344, 1.5708]
iter 0 loss: 0.049
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.014, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.008, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.047
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.007, -lr * Pred Grad:  -0.094, New P: -0.666
-Original Grad: 0.004, -lr * Pred Grad:  0.094, New P: 0.197
iter 2 loss: 0.047
Actual params: [-0.6661,  0.1973]
-Original Grad: -0.002, -lr * Pred Grad:  -0.079, New P: -0.745
-Original Grad: 0.001, -lr * Pred Grad:  0.079, New P: 0.276
iter 3 loss: 0.046
Actual params: [-0.7451,  0.2764]
-Original Grad: -0.001, -lr * Pred Grad:  -0.067, New P: -0.812
-Original Grad: 0.000, -lr * Pred Grad:  0.067, New P: 0.344
iter 4 loss: 0.046
Actual params: [-0.8122,  0.3438]
-Original Grad: -0.000, -lr * Pred Grad:  -0.058, New P: -0.870
-Original Grad: 0.000, -lr * Pred Grad:  0.058, New P: 0.402
iter 5 loss: 0.046
Actual params: [-0.8704,  0.4022]
-Original Grad: -0.000, -lr * Pred Grad:  -0.051, New P: -0.921
-Original Grad: 0.000, -lr * Pred Grad:  0.051, New P: 0.453
iter 6 loss: 0.046
Actual params: [-0.9209,  0.453 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.044, New P: -0.965
-Original Grad: 0.000, -lr * Pred Grad:  0.045, New P: 0.498
iter 7 loss: 0.046
Actual params: [-0.9653,  0.4976]
-Original Grad: -0.000, -lr * Pred Grad:  -0.039, New P: -1.005
-Original Grad: 0.000, -lr * Pred Grad:  0.040, New P: 0.537
iter 8 loss: 0.046
Actual params: [-1.0046,  0.5372]
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -1.039
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: 0.572
iter 9 loss: 0.046
Actual params: [-1.0395,  0.5723]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.071
-Original Grad: 0.000, -lr * Pred Grad:  0.031, New P: 0.604
iter 10 loss: 0.046
Actual params: [-1.0706,  0.6037]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.099
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 0.632
iter 11 loss: 0.046
Actual params: [-1.0986,  0.6319]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.124
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.657
iter 12 loss: 0.046
Actual params: [-1.1238,  0.6573]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.146
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.680
iter 13 loss: 0.046
Actual params: [-1.1465,  0.6801]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.167
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.701
iter 14 loss: 0.046
Actual params: [-1.167 ,  0.7008]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.186
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.720
iter 15 loss: 0.046
Actual params: [-1.1856,  0.7196]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.202
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.737
iter 16 loss: 0.046
Actual params: [-1.2025,  0.7366]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.218
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.752
iter 17 loss: 0.046
Actual params: [-1.2178,  0.752 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.232
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.766
iter 18 loss: 0.046
Actual params: [-1.2317,  0.766 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.244
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.779
iter 19 loss: 0.046
Actual params: [-1.2443,  0.7788]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.256
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.790
iter 20 loss: 0.046
Actual params: [-1.2558,  0.7904]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.266
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.801
iter 21 loss: 0.046
Actual params: [-1.2663,  0.8009]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.276
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.811
iter 22 loss: 0.046
Actual params: [-1.2758,  0.8105]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.284
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.819
iter 23 loss: 0.046
Actual params: [-1.2845,  0.8193]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.292
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.827
iter 24 loss: 0.046
Actual params: [-1.2924,  0.8273]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.300
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.835
iter 25 loss: 0.046
Actual params: [-1.2996,  0.8345]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.306
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.841
iter 26 loss: 0.046
Actual params: [-1.3062,  0.8412]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.312
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.847
iter 27 loss: 0.046
Actual params: [-1.3121,  0.8472]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.318
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.853
iter 28 loss: 0.046
Actual params: [-1.3176,  0.8527]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.323
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.858
iter 29 loss: 0.046
Actual params: [-1.3226,  0.8577]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.327
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.862
iter 30 loss: 0.046
Actual params: [-1.3271,  0.8623]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.331
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.866
Target params: [1.3344, 1.5708]
iter 0 loss: 0.194
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.038, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.021, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.187
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.036, -lr * Pred Grad:  -0.100, New P: -0.672
-Original Grad: 0.024, -lr * Pred Grad:  0.100, New P: 0.204
iter 2 loss: 0.183
Actual params: [-0.6721,  0.2036]
-Original Grad: -0.022, -lr * Pred Grad:  -0.096, New P: -0.768
-Original Grad: 0.017, -lr * Pred Grad:  0.098, New P: 0.302
iter 3 loss: 0.180
Actual params: [-0.7681,  0.3021]
-Original Grad: -0.010, -lr * Pred Grad:  -0.087, New P: -0.855
-Original Grad: 0.009, -lr * Pred Grad:  0.093, New P: 0.395
iter 4 loss: 0.179
Actual params: [-0.8553,  0.3948]
-Original Grad: -0.004, -lr * Pred Grad:  -0.077, New P: -0.932
-Original Grad: 0.004, -lr * Pred Grad:  0.084, New P: 0.479
iter 5 loss: 0.179
Actual params: [-0.9324,  0.4785]
-Original Grad: -0.003, -lr * Pred Grad:  -0.069, New P: -1.001
-Original Grad: 0.003, -lr * Pred Grad:  0.076, New P: 0.555
iter 6 loss: 0.178
Actual params: [-1.0013,  0.5548]
-Original Grad: -0.001, -lr * Pred Grad:  -0.061, New P: -1.063
-Original Grad: 0.002, -lr * Pred Grad:  0.069, New P: 0.623
iter 7 loss: 0.178
Actual params: [-1.0627,  0.6235]
-Original Grad: -0.001, -lr * Pred Grad:  -0.055, New P: -1.118
-Original Grad: 0.001, -lr * Pred Grad:  0.062, New P: 0.686
iter 8 loss: 0.178
Actual params: [-1.1178,  0.6858]
-Original Grad: -0.000, -lr * Pred Grad:  -0.049, New P: -1.167
-Original Grad: 0.000, -lr * Pred Grad:  0.056, New P: 0.742
iter 9 loss: 0.178
Actual params: [-1.1671,  0.7416]
-Original Grad: -0.000, -lr * Pred Grad:  -0.044, New P: -1.211
-Original Grad: 0.000, -lr * Pred Grad:  0.050, New P: 0.792
iter 10 loss: 0.178
Actual params: [-1.2115,  0.7921]
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -1.252
-Original Grad: 0.000, -lr * Pred Grad:  0.046, New P: 0.838
iter 11 loss: 0.178
Actual params: [-1.2516,  0.8379]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -1.288
-Original Grad: 0.000, -lr * Pred Grad:  0.041, New P: 0.879
iter 12 loss: 0.178
Actual params: [-1.2878,  0.8793]
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -1.321
-Original Grad: 0.000, -lr * Pred Grad:  0.038, New P: 0.917
iter 13 loss: 0.178
Actual params: [-1.3206,  0.9168]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -1.350
-Original Grad: 0.000, -lr * Pred Grad:  0.034, New P: 0.951
iter 14 loss: 0.178
Actual params: [-1.3504,  0.951 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.377
-Original Grad: 0.000, -lr * Pred Grad:  0.031, New P: 0.982
iter 15 loss: 0.178
Actual params: [-1.3775,  0.982 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.402
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 1.010
iter 16 loss: 0.178
Actual params: [-1.4021,  1.0102]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.424
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 1.036
iter 17 loss: 0.178
Actual params: [-1.4245,  1.0359]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.445
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 1.059
iter 18 loss: 0.178
Actual params: [-1.4449,  1.0594]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.463
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 1.081
iter 19 loss: 0.178
Actual params: [-1.4634,  1.0807]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.480
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 1.100
iter 20 loss: 0.178
Actual params: [-1.4804,  1.1002]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.496
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 1.118
iter 21 loss: 0.178
Actual params: [-1.4959,  1.118 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.510
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 1.134
iter 22 loss: 0.178
Actual params: [-1.51  ,  1.1342]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.523
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 1.149
iter 23 loss: 0.178
Actual params: [-1.5229,  1.1491]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.535
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 1.163
iter 24 loss: 0.178
Actual params: [-1.5346,  1.1626]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.545
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 1.175
iter 25 loss: 0.178
Actual params: [-1.5454,  1.175 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.555
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 1.186
iter 26 loss: 0.178
Actual params: [-1.5552,  1.1863]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.564
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 1.197
iter 27 loss: 0.178
Actual params: [-1.5641,  1.1967]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.572
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 1.206
iter 28 loss: 0.178
Actual params: [-1.5723,  1.2061]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.580
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 1.215
iter 29 loss: 0.178
Actual params: [-1.5798,  1.2148]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.587
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 1.223
iter 30 loss: 0.178
Actual params: [-1.5866,  1.2227]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.593
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 1.230
Target params: [1.3344, 1.5708]
iter 0 loss: 0.029
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.034, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.003, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.025
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.036, -lr * Pred Grad:  -0.100, New P: -0.672
-Original Grad: 0.007, -lr * Pred Grad:  0.095, New P: 0.199
iter 2 loss: 0.021
Actual params: [-0.6724,  0.1987]
-Original Grad: -0.020, -lr * Pred Grad:  -0.096, New P: -0.768
-Original Grad: 0.005, -lr * Pred Grad:  0.097, New P: 0.296
iter 3 loss: 0.019
Actual params: [-0.7683,  0.2956]
-Original Grad: -0.013, -lr * Pred Grad:  -0.090, New P: -0.858
-Original Grad: 0.004, -lr * Pred Grad:  0.097, New P: 0.392
iter 4 loss: 0.017
Actual params: [-0.8581,  0.3925]
-Original Grad: -0.008, -lr * Pred Grad:  -0.083, New P: -0.941
-Original Grad: 0.004, -lr * Pred Grad:  0.096, New P: 0.489
iter 5 loss: 0.016
Actual params: [-0.941 ,  0.4886]
-Original Grad: -0.004, -lr * Pred Grad:  -0.075, New P: -1.016
-Original Grad: 0.002, -lr * Pred Grad:  0.092, New P: 0.581
iter 6 loss: 0.016
Actual params: [-1.0162,  0.5809]
-Original Grad: -0.003, -lr * Pred Grad:  -0.068, New P: -1.084
-Original Grad: 0.002, -lr * Pred Grad:  0.089, New P: 0.670
iter 7 loss: 0.015
Actual params: [-1.0843,  0.6703]
-Original Grad: -0.001, -lr * Pred Grad:  -0.061, New P: -1.146
-Original Grad: 0.001, -lr * Pred Grad:  0.084, New P: 0.754
iter 8 loss: 0.015
Actual params: [-1.1456,  0.7541]
-Original Grad: -0.001, -lr * Pred Grad:  -0.055, New P: -1.201
-Original Grad: 0.001, -lr * Pred Grad:  0.078, New P: 0.832
iter 9 loss: 0.015
Actual params: [-1.2008,  0.8319]
-Original Grad: -0.000, -lr * Pred Grad:  -0.050, New P: -1.250
-Original Grad: 0.000, -lr * Pred Grad:  0.072, New P: 0.903
iter 10 loss: 0.015
Actual params: [-1.2504,  0.9035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.045, New P: -1.295
-Original Grad: 0.000, -lr * Pred Grad:  0.066, New P: 0.969
iter 11 loss: 0.015
Actual params: [-1.2952,  0.9691]
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -1.336
-Original Grad: 0.000, -lr * Pred Grad:  0.060, New P: 1.029
iter 12 loss: 0.015
Actual params: [-1.3356,  1.0291]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -1.372
-Original Grad: 0.000, -lr * Pred Grad:  0.055, New P: 1.084
iter 13 loss: 0.015
Actual params: [-1.3721,  1.0837]
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -1.405
-Original Grad: 0.000, -lr * Pred Grad:  0.050, New P: 1.133
iter 14 loss: 0.015
Actual params: [-1.4051,  1.1334]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -1.435
-Original Grad: 0.000, -lr * Pred Grad:  0.045, New P: 1.179
iter 15 loss: 0.015
Actual params: [-1.435 ,  1.1786]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.462
-Original Grad: 0.000, -lr * Pred Grad:  0.041, New P: 1.220
iter 16 loss: 0.015
Actual params: [-1.4621,  1.2197]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.487
-Original Grad: 0.000, -lr * Pred Grad:  0.037, New P: 1.257
iter 17 loss: 0.015
Actual params: [-1.4867,  1.2571]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.509
-Original Grad: 0.000, -lr * Pred Grad:  0.034, New P: 1.291
iter 18 loss: 0.015
Actual params: [-1.5091,  1.2912]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.529
-Original Grad: 0.000, -lr * Pred Grad:  0.031, New P: 1.322
iter 19 loss: 0.015
Actual params: [-1.5294,  1.3222]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.548
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 1.350
iter 20 loss: 0.015
Actual params: [-1.5479,  1.3505]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.565
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 1.376
iter 21 loss: 0.015
Actual params: [-1.5647,  1.3762]
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -1.580
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 1.400
iter 22 loss: 0.015
Actual params: [-1.58  ,  1.3997]
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -1.594
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 1.421
iter 23 loss: 0.015
Actual params: [-1.5939,  1.4211]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -1.607
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 1.441
iter 24 loss: 0.015
Actual params: [-1.6066,  1.4406]
-Original Grad: 0.000, -lr * Pred Grad:  -0.012, New P: -1.618
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 1.458
iter 25 loss: 0.015
Actual params: [-1.6182,  1.4584]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.629
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 1.475
iter 26 loss: 0.015
Actual params: [-1.6287,  1.4746]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.638
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 1.489
iter 27 loss: 0.015
Actual params: [-1.6383,  1.4894]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.647
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 1.503
iter 28 loss: 0.015
Actual params: [-1.647 ,  1.5028]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.655
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 1.515
iter 29 loss: 0.015
Actual params: [-1.6549,  1.5151]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.662
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 1.526
iter 30 loss: 0.015
Actual params: [-1.6622,  1.5263]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.669
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 1.536
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.363
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.002, -lr * Pred Grad:  0.092, New P: -0.280
-Original Grad: 0.000, -lr * Pred Grad:  0.093, New P: 0.196
iter 2 loss: 0.363
Actual params: [-0.28  ,  0.1964]
-Original Grad: 0.004, -lr * Pred Grad:  0.090, New P: -0.190
-Original Grad: 0.001, -lr * Pred Grad:  0.090, New P: 0.286
iter 3 loss: 0.362
Actual params: [-0.1902,  0.2859]
-Original Grad: 0.016, -lr * Pred Grad:  0.075, New P: -0.116
-Original Grad: 0.004, -lr * Pred Grad:  0.073, New P: 0.359
iter 4 loss: 0.360
Actual params: [-0.1157,  0.3592]
-Original Grad: 0.034, -lr * Pred Grad:  0.076, New P: -0.039
-Original Grad: 0.010, -lr * Pred Grad:  0.075, New P: 0.434
iter 5 loss: 0.356
Actual params: [-0.0393,  0.4338]
-Original Grad: 0.056, -lr * Pred Grad:  0.080, New P: 0.041
-Original Grad: 0.017, -lr * Pred Grad:  0.078, New P: 0.512
iter 6 loss: 0.347
Actual params: [0.0408, 0.5121]
-Original Grad: 0.143, -lr * Pred Grad:  0.076, New P: 0.116
-Original Grad: 0.046, -lr * Pred Grad:  0.074, New P: 0.586
iter 7 loss: 0.328
Actual params: [0.1165, 0.5857]
-Original Grad: 0.245, -lr * Pred Grad:  0.078, New P: 0.194
-Original Grad: 0.082, -lr * Pred Grad:  0.076, New P: 0.662
iter 8 loss: 0.299
Actual params: [0.1943, 0.6616]
-Original Grad: 0.334, -lr * Pred Grad:  0.082, New P: 0.277
-Original Grad: 0.107, -lr * Pred Grad:  0.081, New P: 0.743
iter 9 loss: 0.260
Actual params: [0.2766, 0.7431]
-Original Grad: 0.447, -lr * Pred Grad:  0.086, New P: 0.363
-Original Grad: 0.109, -lr * Pred Grad:  0.087, New P: 0.830
iter 10 loss: 0.204
Actual params: [0.3627, 0.8303]
-Original Grad: 0.550, -lr * Pred Grad:  0.090, New P: 0.453
-Original Grad: 0.114, -lr * Pred Grad:  0.092, New P: 0.922
iter 11 loss: 0.140
Actual params: [0.4525, 0.9221]
-Original Grad: 0.463, -lr * Pred Grad:  0.094, New P: 0.546
-Original Grad: 0.043, -lr * Pred Grad:  0.090, New P: 1.012
iter 12 loss: 0.098
Actual params: [0.5465, 1.0125]
-Original Grad: 0.308, -lr * Pred Grad:  0.095, New P: 0.642
-Original Grad: -0.013, -lr * Pred Grad:  0.078, New P: 1.091
iter 13 loss: 0.076
Actual params: [0.6419, 1.0909]
-Original Grad: 0.139, -lr * Pred Grad:  0.092, New P: 0.734
-Original Grad: 0.010, -lr * Pred Grad:  0.073, New P: 1.164
iter 14 loss: 0.059
Actual params: [0.734 , 1.1638]
-Original Grad: 0.236, -lr * Pred Grad:  0.092, New P: 0.826
-Original Grad: -0.043, -lr * Pred Grad:  0.055, New P: 1.219
iter 15 loss: 0.045
Actual params: [0.8261, 1.2193]
-Original Grad: 0.095, -lr * Pred Grad:  0.088, New P: 0.914
-Original Grad: 0.037, -lr * Pred Grad:  0.058, New P: 1.277
iter 16 loss: 0.032
Actual params: [0.9137, 1.277 ]
-Original Grad: 0.122, -lr * Pred Grad:  0.085, New P: 0.998
-Original Grad: 0.012, -lr * Pred Grad:  0.055, New P: 1.332
iter 17 loss: 0.021
Actual params: [0.9984, 1.3318]
-Original Grad: 0.132, -lr * Pred Grad:  0.083, New P: 1.081
-Original Grad: -0.022, -lr * Pred Grad:  0.045, New P: 1.377
iter 18 loss: 0.018
Actual params: [1.081 , 1.3765]
-Original Grad: 0.035, -lr * Pred Grad:  0.077, New P: 1.158
-Original Grad: -0.044, -lr * Pred Grad:  0.030, New P: 1.407
iter 19 loss: 0.018
Actual params: [1.1576, 1.4068]
-Original Grad: 0.012, -lr * Pred Grad:  0.070, New P: 1.228
-Original Grad: -0.080, -lr * Pred Grad:  0.009, New P: 1.416
iter 20 loss: 0.018
Actual params: [1.2279, 1.4162]
-Original Grad: 0.005, -lr * Pred Grad:  0.064, New P: 1.292
-Original Grad: -0.072, -lr * Pred Grad:  -0.006, New P: 1.410
iter 21 loss: 0.018
Actual params: [1.2921, 1.4099]
-Original Grad: -0.016, -lr * Pred Grad:  0.058, New P: 1.350
-Original Grad: -0.082, -lr * Pred Grad:  -0.021, New P: 1.389
iter 22 loss: 0.018
Actual params: [1.3498, 1.3886]
-Original Grad: -0.023, -lr * Pred Grad:  0.051, New P: 1.401
-Original Grad: -0.060, -lr * Pred Grad:  -0.030, New P: 1.358
iter 23 loss: 0.018
Actual params: [1.4011, 1.3582]
-Original Grad: -0.051, -lr * Pred Grad:  0.044, New P: 1.445
-Original Grad: -0.093, -lr * Pred Grad:  -0.043, New P: 1.315
iter 24 loss: 0.018
Actual params: [1.4452, 1.3149]
-Original Grad: -0.052, -lr * Pred Grad:  0.038, New P: 1.483
-Original Grad: -0.028, -lr * Pred Grad:  -0.044, New P: 1.271
iter 25 loss: 0.018
Actual params: [1.4828, 1.2706]
-Original Grad: -0.092, -lr * Pred Grad:  0.029, New P: 1.512
-Original Grad: -0.036, -lr * Pred Grad:  -0.047, New P: 1.224
iter 26 loss: 0.020
Actual params: [1.5121, 1.2239]
-Original Grad: -0.082, -lr * Pred Grad:  0.022, New P: 1.535
-Original Grad: -0.026, -lr * Pred Grad:  -0.047, New P: 1.177
iter 27 loss: 0.021
Actual params: [1.5345, 1.1765]
-Original Grad: -0.123, -lr * Pred Grad:  0.014, New P: 1.548
-Original Grad: -0.023, -lr * Pred Grad:  -0.047, New P: 1.129
iter 28 loss: 0.022
Actual params: [1.5485, 1.1291]
-Original Grad: -0.123, -lr * Pred Grad:  0.006, New P: 1.555
-Original Grad: 0.008, -lr * Pred Grad:  -0.042, New P: 1.087
iter 29 loss: 0.023
Actual params: [1.5546, 1.0874]
-Original Grad: -0.121, -lr * Pred Grad:  -0.001, New P: 1.554
-Original Grad: 0.015, -lr * Pred Grad:  -0.035, New P: 1.052
iter 30 loss: 0.023
Actual params: [1.5539, 1.0523]
-Original Grad: -0.118, -lr * Pred Grad:  -0.007, New P: 1.547
-Original Grad: -0.019, -lr * Pred Grad:  -0.036, New P: 1.017
Target params: [1.3344, 1.5708]
iter 0 loss: 0.829
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.103
iter 1 loss: 0.829
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.071, New P: -0.644
-Original Grad: 0.000, -lr * Pred Grad:  0.083, New P: 0.186
iter 2 loss: 0.829
Actual params: [-0.6438,  0.1865]
-Original Grad: -0.000, -lr * Pred Grad:  -0.079, New P: -0.723
-Original Grad: -0.000, -lr * Pred Grad:  0.063, New P: 0.249
iter 3 loss: 0.829
Actual params: [-0.7229,  0.2492]
-Original Grad: -0.000, -lr * Pred Grad:  -0.067, New P: -0.790
-Original Grad: 0.000, -lr * Pred Grad:  0.056, New P: 0.306
iter 4 loss: 0.829
Actual params: [-0.7901,  0.3056]
-Original Grad: 0.000, -lr * Pred Grad:  -0.054, New P: -0.844
-Original Grad: 0.000, -lr * Pred Grad:  0.052, New P: 0.358
iter 5 loss: 0.829
Actual params: [-0.844 ,  0.3577]
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -0.891
-Original Grad: 0.000, -lr * Pred Grad:  0.049, New P: 0.406
iter 6 loss: 0.829
Actual params: [-0.8911,  0.4064]
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -0.938
-Original Grad: -0.000, -lr * Pred Grad:  0.029, New P: 0.436
iter 7 loss: 0.829
Actual params: [-0.9379,  0.4355]
-Original Grad: 0.000, -lr * Pred Grad:  -0.040, New P: -0.978
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: 0.468
iter 8 loss: 0.829
Actual params: [-0.9783,  0.4678]
-Original Grad: 0.000, -lr * Pred Grad:  -0.034, New P: -1.013
-Original Grad: 0.000, -lr * Pred Grad:  0.038, New P: 0.506
iter 9 loss: 0.829
Actual params: [-1.0126,  0.506 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.028, New P: -1.041
-Original Grad: 0.000, -lr * Pred Grad:  0.044, New P: 0.550
iter 10 loss: 0.829
Actual params: [-1.0411,  0.5497]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -1.073
-Original Grad: -0.000, -lr * Pred Grad:  0.018, New P: 0.568
iter 11 loss: 0.829
Actual params: [-1.073 ,  0.5681]
-Original Grad: 0.000, -lr * Pred Grad:  -0.027, New P: -1.100
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.591
iter 12 loss: 0.829
Actual params: [-1.0997,  0.5907]
-Original Grad: 0.000, -lr * Pred Grad:  -0.021, New P: -1.121
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: 0.623
iter 13 loss: 0.829
Actual params: [-1.121 ,  0.6228]
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -1.138
-Original Grad: 0.000, -lr * Pred Grad:  0.040, New P: 0.662
iter 14 loss: 0.829
Actual params: [-1.1376,  0.6624]
-Original Grad: 0.000, -lr * Pred Grad:  -0.012, New P: -1.149
-Original Grad: 0.000, -lr * Pred Grad:  0.050, New P: 0.712
iter 15 loss: 0.829
Actual params: [-1.1492,  0.7119]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.157
-Original Grad: 0.000, -lr * Pred Grad:  0.058, New P: 0.770
iter 16 loss: 0.829
Actual params: [-1.1568,  0.7702]
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -1.155
-Original Grad: 0.000, -lr * Pred Grad:  0.072, New P: 0.842
iter 17 loss: 0.829
Actual params: [-1.1553,  0.842 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: -1.157
-Original Grad: 0.000, -lr * Pred Grad:  0.073, New P: 0.915
iter 18 loss: 0.829
Actual params: [-1.1574,  0.9145]
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: -1.146
-Original Grad: 0.000, -lr * Pred Grad:  0.080, New P: 0.995
iter 19 loss: 0.829
Actual params: [-1.1463,  0.9948]
-Original Grad: 0.001, -lr * Pred Grad:  0.041, New P: -1.105
-Original Grad: 0.001, -lr * Pred Grad:  0.082, New P: 1.077
iter 20 loss: 0.829
Actual params: [-1.1048,  1.077 ]
-Original Grad: 0.002, -lr * Pred Grad:  0.062, New P: -1.042
-Original Grad: 0.002, -lr * Pred Grad:  0.075, New P: 1.152
iter 21 loss: 0.829
Actual params: [-1.0425,  1.152 ]
-Original Grad: 0.001, -lr * Pred Grad:  0.076, New P: -0.967
-Original Grad: 0.002, -lr * Pred Grad:  0.086, New P: 1.238
iter 22 loss: 0.828
Actual params: [-0.9665,  1.2377]
-Original Grad: 0.015, -lr * Pred Grad:  0.063, New P: -0.903
-Original Grad: 0.012, -lr * Pred Grad:  0.070, New P: 1.308
iter 23 loss: 0.828
Actual params: [-0.9031,  1.3077]
-Original Grad: 0.004, -lr * Pred Grad:  0.069, New P: -0.834
-Original Grad: 0.004, -lr * Pred Grad:  0.078, New P: 1.385
iter 24 loss: 0.827
Actual params: [-0.8337,  1.3853]
-Original Grad: 0.005, -lr * Pred Grad:  0.077, New P: -0.757
-Original Grad: 0.006, -lr * Pred Grad:  0.087, New P: 1.472
iter 25 loss: 0.825
Actual params: [-0.7572,  1.4722]
-Original Grad: 0.011, -lr * Pred Grad:  0.088, New P: -0.669
-Original Grad: 0.014, -lr * Pred Grad:  0.095, New P: 1.567
iter 26 loss: 0.820
Actual params: [-0.669 ,  1.5667]
-Original Grad: 0.018, -lr * Pred Grad:  0.096, New P: -0.573
-Original Grad: 0.026, -lr * Pred Grad:  0.096, New P: 1.662
iter 27 loss: 0.812
Actual params: [-0.5729,  1.6624]
-Original Grad: 0.029, -lr * Pred Grad:  0.100, New P: -0.473
-Original Grad: 0.029, -lr * Pred Grad:  0.102, New P: 1.765
iter 28 loss: 0.798
Actual params: [-0.4727,  1.7646]
-Original Grad: 0.074, -lr * Pred Grad:  0.093, New P: -0.380
-Original Grad: 0.061, -lr * Pred Grad:  0.100, New P: 1.865
iter 29 loss: 0.781
Actual params: [-0.3802,  1.8646]
-Original Grad: 0.091, -lr * Pred Grad:  0.099, New P: -0.281
-Original Grad: 0.041, -lr * Pred Grad:  0.107, New P: 1.972
iter 30 loss: 0.752
Actual params: [-0.2812,  1.9717]
-Original Grad: 0.184, -lr * Pred Grad:  0.098, New P: -0.183
-Original Grad: 0.071, -lr * Pred Grad:  0.112, New P: 2.084
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.008, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.005, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.118
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.001, -lr * Pred Grad:  -0.078, New P: -0.650
-Original Grad: -0.001, -lr * Pred Grad:  -0.080, New P: -0.176
iter 2 loss: 0.118
Actual params: [-0.6503, -0.1762]
-Original Grad: -0.002, -lr * Pred Grad:  -0.072, New P: -0.722
-Original Grad: -0.001, -lr * Pred Grad:  -0.073, New P: -0.250
iter 3 loss: 0.118
Actual params: [-0.7222, -0.2497]
-Original Grad: -0.000, -lr * Pred Grad:  -0.061, New P: -0.784
-Original Grad: -0.000, -lr * Pred Grad:  -0.063, New P: -0.312
iter 4 loss: 0.118
Actual params: [-0.7836, -0.3125]
-Original Grad: -0.000, -lr * Pred Grad:  -0.053, New P: -0.837
-Original Grad: -0.000, -lr * Pred Grad:  -0.055, New P: -0.367
iter 5 loss: 0.118
Actual params: [-0.837 , -0.3671]
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -0.884
-Original Grad: -0.000, -lr * Pred Grad:  -0.048, New P: -0.415
iter 6 loss: 0.118
Actual params: [-0.884 , -0.4151]
-Original Grad: -0.000, -lr * Pred Grad:  -0.042, New P: -0.926
-Original Grad: -0.000, -lr * Pred Grad:  -0.042, New P: -0.458
iter 7 loss: 0.118
Actual params: [-0.9256, -0.4576]
-Original Grad: -0.000, -lr * Pred Grad:  -0.037, New P: -0.963
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -0.495
iter 8 loss: 0.118
Actual params: [-0.9626, -0.4953]
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -0.996
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -0.529
iter 9 loss: 0.118
Actual params: [-0.9956, -0.529 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -1.025
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -0.559
iter 10 loss: 0.118
Actual params: [-1.0253, -0.5592]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.052
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -0.586
iter 11 loss: 0.118
Actual params: [-1.0521, -0.5863]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.076
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -0.611
iter 12 loss: 0.118
Actual params: [-1.0762, -0.6109]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.098
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -0.633
iter 13 loss: 0.118
Actual params: [-1.098, -0.633]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.118
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -0.653
iter 14 loss: 0.118
Actual params: [-1.1178, -0.6531]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.136
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -0.671
iter 15 loss: 0.118
Actual params: [-1.1358, -0.6713]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.152
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.688
iter 16 loss: 0.118
Actual params: [-1.1521, -0.6878]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.167
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.703
iter 17 loss: 0.118
Actual params: [-1.167 , -0.7028]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.180
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.716
iter 18 loss: 0.118
Actual params: [-1.1805, -0.7165]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.193
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.729
iter 19 loss: 0.118
Actual params: [-1.1928, -0.7289]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.204
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.740
iter 20 loss: 0.118
Actual params: [-1.2041, -0.7402]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.214
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.751
iter 21 loss: 0.118
Actual params: [-1.2143, -0.7505]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.224
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.760
iter 22 loss: 0.118
Actual params: [-1.2237, -0.7599]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.232
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.768
iter 23 loss: 0.118
Actual params: [-1.2322, -0.7685]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.240
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.776
iter 24 loss: 0.118
Actual params: [-1.2401, -0.7763]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.247
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.783
iter 25 loss: 0.118
Actual params: [-1.2472, -0.7834]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.254
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.790
iter 26 loss: 0.118
Actual params: [-1.2537, -0.79  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.260
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.796
iter 27 loss: 0.118
Actual params: [-1.2597, -0.7959]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.265
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.801
iter 28 loss: 0.118
Actual params: [-1.2651, -0.8013]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.270
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.806
iter 29 loss: 0.118
Actual params: [-1.2701, -0.8063]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.275
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.811
iter 30 loss: 0.118
Actual params: [-1.2747, -0.8108]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.279
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.815
Target params: [1.3344, 1.5708]
iter 0 loss: 0.081
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.008, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.004, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.080
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.077, New P: -0.649
-Original Grad: 0.001, -lr * Pred Grad:  0.078, New P: 0.182
iter 2 loss: 0.080
Actual params: [-0.6489,  0.1818]
-Original Grad: -0.001, -lr * Pred Grad:  -0.065, New P: -0.714
-Original Grad: 0.000, -lr * Pred Grad:  0.067, New P: 0.249
iter 3 loss: 0.080
Actual params: [-0.7141,  0.2488]
-Original Grad: -0.000, -lr * Pred Grad:  -0.054, New P: -0.768
-Original Grad: 0.000, -lr * Pred Grad:  0.056, New P: 0.305
iter 4 loss: 0.080
Actual params: [-0.768,  0.305]
-Original Grad: 0.000, -lr * Pred Grad:  -0.045, New P: -0.813
-Original Grad: 0.000, -lr * Pred Grad:  0.048, New P: 0.353
iter 5 loss: 0.080
Actual params: [-0.8133,  0.353 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.039, New P: -0.853
-Original Grad: 0.000, -lr * Pred Grad:  0.042, New P: 0.395
iter 6 loss: 0.080
Actual params: [-0.8528,  0.3948]
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -0.888
-Original Grad: 0.000, -lr * Pred Grad:  0.037, New P: 0.432
iter 7 loss: 0.080
Actual params: [-0.8875,  0.4317]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -0.918
-Original Grad: 0.000, -lr * Pred Grad:  0.033, New P: 0.464
iter 8 loss: 0.080
Actual params: [-0.9183,  0.4644]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -0.946
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: 0.494
iter 9 loss: 0.080
Actual params: [-0.9456,  0.4936]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -0.970
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 0.520
iter 10 loss: 0.080
Actual params: [-0.97  ,  0.5196]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -0.992
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.543
iter 11 loss: 0.080
Actual params: [-0.9919,  0.5431]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.012
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.564
iter 12 loss: 0.080
Actual params: [-1.0116,  0.5642]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.030
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.583
iter 13 loss: 0.080
Actual params: [-1.0295,  0.5832]
-Original Grad: 0.000, -lr * Pred Grad:  -0.016, New P: -1.046
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.601
iter 14 loss: 0.080
Actual params: [-1.0456,  0.6005]
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -1.060
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.616
iter 15 loss: 0.080
Actual params: [-1.0602,  0.6161]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -1.073
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.630
iter 16 loss: 0.080
Actual params: [-1.0734,  0.6303]
-Original Grad: 0.000, -lr * Pred Grad:  -0.012, New P: -1.085
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.643
iter 17 loss: 0.080
Actual params: [-1.0853,  0.6432]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.096
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.655
iter 18 loss: 0.080
Actual params: [-1.0962,  0.655 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.106
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.666
iter 19 loss: 0.080
Actual params: [-1.106 ,  0.6656]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.115
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.675
iter 20 loss: 0.080
Actual params: [-1.115 ,  0.6753]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.123
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.684
iter 21 loss: 0.080
Actual params: [-1.1231,  0.6842]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.131
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.692
iter 22 loss: 0.080
Actual params: [-1.1306,  0.6922]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.137
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.700
iter 23 loss: 0.080
Actual params: [-1.1373,  0.6996]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.143
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.706
iter 24 loss: 0.080
Actual params: [-1.1435,  0.7063]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.149
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.712
iter 25 loss: 0.080
Actual params: [-1.149 ,  0.7124]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.154
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.718
iter 26 loss: 0.080
Actual params: [-1.1541,  0.718 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.159
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.723
iter 27 loss: 0.080
Actual params: [-1.1587,  0.7231]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.163
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.728
iter 28 loss: 0.080
Actual params: [-1.163 ,  0.7277]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.167
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.732
iter 29 loss: 0.080
Actual params: [-1.1668,  0.7319]
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -1.170
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.736
iter 30 loss: 0.080
Actual params: [-1.1703,  0.7358]
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -1.173
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.739
Target params: [1.3344, 1.5708]
iter 0 loss: 0.492
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.026, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.060, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.479
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.022, -lr * Pred Grad:  0.099, New P: -0.273
-Original Grad: 0.092, -lr * Pred Grad:  0.099, New P: 0.203
iter 2 loss: 0.463
Actual params: [-0.2729,  0.2025]
-Original Grad: 0.062, -lr * Pred Grad:  0.093, New P: -0.180
-Original Grad: 0.102, -lr * Pred Grad:  0.100, New P: 0.302
iter 3 loss: 0.443
Actual params: [-0.1798,  0.3021]
-Original Grad: 0.100, -lr * Pred Grad:  0.091, New P: -0.088
-Original Grad: 0.134, -lr * Pred Grad:  0.100, New P: 0.402
iter 4 loss: 0.417
Actual params: [-0.0884,  0.4016]
-Original Grad: 0.094, -lr * Pred Grad:  0.094, New P: 0.006
-Original Grad: 0.164, -lr * Pred Grad:  0.100, New P: 0.501
iter 5 loss: 0.382
Actual params: [0.0061, 0.5013]
-Original Grad: 0.141, -lr * Pred Grad:  0.095, New P: 0.101
-Original Grad: 0.200, -lr * Pred Grad:  0.100, New P: 0.601
iter 6 loss: 0.331
Actual params: [0.1014, 0.6012]
-Original Grad: 0.239, -lr * Pred Grad:  0.093, New P: 0.194
-Original Grad: 0.235, -lr * Pred Grad:  0.100, New P: 0.702
iter 7 loss: 0.256
Actual params: [0.1943, 0.7016]
-Original Grad: 0.289, -lr * Pred Grad:  0.094, New P: 0.288
-Original Grad: 0.260, -lr * Pred Grad:  0.101, New P: 0.803
iter 8 loss: 0.205
Actual params: [0.2882, 0.8028]
-Original Grad: 0.180, -lr * Pred Grad:  0.096, New P: 0.384
-Original Grad: 0.141, -lr * Pred Grad:  0.100, New P: 0.903
iter 9 loss: 0.159
Actual params: [0.384 , 0.9029]
-Original Grad: 0.231, -lr * Pred Grad:  0.098, New P: 0.482
-Original Grad: 0.298, -lr * Pred Grad:  0.101, New P: 1.004
iter 10 loss: 0.113
Actual params: [0.4821, 1.0044]
-Original Grad: 0.153, -lr * Pred Grad:  0.098, New P: 0.580
-Original Grad: 0.185, -lr * Pred Grad:  0.101, New P: 1.106
iter 11 loss: 0.077
Actual params: [0.5801, 1.1056]
-Original Grad: 0.010, -lr * Pred Grad:  0.089, New P: 0.669
-Original Grad: 0.183, -lr * Pred Grad:  0.101, New P: 1.207
iter 12 loss: 0.055
Actual params: [0.6691, 1.2067]
-Original Grad: -0.064, -lr * Pred Grad:  0.074, New P: 0.743
-Original Grad: 0.162, -lr * Pred Grad:  0.100, New P: 1.307
iter 13 loss: 0.044
Actual params: [0.7431, 1.3069]
-Original Grad: -0.047, -lr * Pred Grad:  0.062, New P: 0.806
-Original Grad: 0.064, -lr * Pred Grad:  0.095, New P: 1.402
iter 14 loss: 0.040
Actual params: [0.8056, 1.4017]
-Original Grad: -0.165, -lr * Pred Grad:  0.040, New P: 0.846
-Original Grad: 0.021, -lr * Pred Grad:  0.087, New P: 1.489
iter 15 loss: 0.040
Actual params: [0.8457, 1.489 ]
-Original Grad: -0.060, -lr * Pred Grad:  0.031, New P: 0.877
-Original Grad: 0.010, -lr * Pred Grad:  0.080, New P: 1.569
iter 16 loss: 0.042
Actual params: [0.8766, 1.5689]
-Original Grad: -0.093, -lr * Pred Grad:  0.020, New P: 0.896
-Original Grad: -0.036, -lr * Pred Grad:  0.070, New P: 1.638
iter 17 loss: 0.045
Actual params: [0.8965, 1.6385]
-Original Grad: -0.150, -lr * Pred Grad:  0.005, New P: 0.901
-Original Grad: -0.025, -lr * Pred Grad:  0.061, New P: 1.700
iter 18 loss: 0.048
Actual params: [0.9014, 1.6998]
-Original Grad: -0.072, -lr * Pred Grad:  -0.001, New P: 0.900
-Original Grad: -0.026, -lr * Pred Grad:  0.054, New P: 1.753
iter 19 loss: 0.051
Actual params: [0.8999, 1.7535]
-Original Grad: -0.070, -lr * Pred Grad:  -0.007, New P: 0.893
-Original Grad: -0.048, -lr * Pred Grad:  0.045, New P: 1.799
iter 20 loss: 0.053
Actual params: [0.8928, 1.7985]
-Original Grad: -0.150, -lr * Pred Grad:  -0.019, New P: 0.874
-Original Grad: -0.028, -lr * Pred Grad:  0.039, New P: 1.837
iter 21 loss: 0.054
Actual params: [0.8742, 1.8373]
-Original Grad: -0.126, -lr * Pred Grad:  -0.027, New P: 0.847
-Original Grad: -0.050, -lr * Pred Grad:  0.031, New P: 1.869
iter 22 loss: 0.054
Actual params: [0.8473, 1.8686]
-Original Grad: -0.021, -lr * Pred Grad:  -0.026, New P: 0.821
-Original Grad: -0.069, -lr * Pred Grad:  0.023, New P: 1.891
iter 23 loss: 0.053
Actual params: [0.8212, 1.8915]
-Original Grad: -0.091, -lr * Pred Grad:  -0.031, New P: 0.790
-Original Grad: -0.083, -lr * Pred Grad:  0.014, New P: 1.906
iter 24 loss: 0.051
Actual params: [0.7901, 1.9056]
-Original Grad: -0.094, -lr * Pred Grad:  -0.036, New P: 0.754
-Original Grad: -0.156, -lr * Pred Grad:  0.000, New P: 1.906
iter 25 loss: 0.047
Actual params: [0.7543, 1.9061]
-Original Grad: -0.104, -lr * Pred Grad:  -0.041, New P: 0.714
-Original Grad: -0.140, -lr * Pred Grad:  -0.010, New P: 1.896
iter 26 loss: 0.041
Actual params: [0.7136, 1.8957]
-Original Grad: -0.143, -lr * Pred Grad:  -0.048, New P: 0.666
-Original Grad: -0.110, -lr * Pred Grad:  -0.018, New P: 1.878
iter 27 loss: 0.034
Actual params: [0.6656, 1.8779]
-Original Grad: -0.021, -lr * Pred Grad:  -0.045, New P: 0.620
-Original Grad: -0.048, -lr * Pred Grad:  -0.020, New P: 1.858
iter 28 loss: 0.028
Actual params: [0.6203, 1.858 ]
-Original Grad: -0.118, -lr * Pred Grad:  -0.050, New P: 0.570
-Original Grad: -0.069, -lr * Pred Grad:  -0.023, New P: 1.834
iter 29 loss: 0.023
Actual params: [0.5699, 1.8345]
-Original Grad: -0.022, -lr * Pred Grad:  -0.048, New P: 0.522
-Original Grad: -0.057, -lr * Pred Grad:  -0.026, New P: 1.809
iter 30 loss: 0.020
Actual params: [0.5221, 1.8086]
-Original Grad: -0.046, -lr * Pred Grad:  -0.047, New P: 0.475
-Original Grad: -0.047, -lr * Pred Grad:  -0.027, New P: 1.781
Target params: [1.3344, 1.5708]
iter 0 loss: 0.239
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.013, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.003, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.237
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.021, -lr * Pred Grad:  0.098, New P: -0.274
-Original Grad: -0.002, -lr * Pred Grad:  -0.099, New P: -0.195
iter 2 loss: 0.234
Actual params: [-0.2738, -0.1955]
-Original Grad: 0.027, -lr * Pred Grad:  0.098, New P: -0.175
-Original Grad: 0.005, -lr * Pred Grad:  -0.001, New P: -0.196
iter 3 loss: 0.231
Actual params: [-0.1754, -0.1963]
-Original Grad: 0.041, -lr * Pred Grad:  0.097, New P: -0.078
-Original Grad: 0.009, -lr * Pred Grad:  0.048, New P: -0.148
iter 4 loss: 0.227
Actual params: [-0.0783, -0.1483]
-Original Grad: 0.053, -lr * Pred Grad:  0.097, New P: 0.019
-Original Grad: 0.011, -lr * Pred Grad:  0.067, New P: -0.081
iter 5 loss: 0.221
Actual params: [ 0.0186, -0.0811]
-Original Grad: 0.045, -lr * Pred Grad:  0.098, New P: 0.117
-Original Grad: 0.004, -lr * Pred Grad:  0.069, New P: -0.012
iter 6 loss: 0.212
Actual params: [ 0.1171, -0.0121]
-Original Grad: 0.096, -lr * Pred Grad:  0.096, New P: 0.213
-Original Grad: 0.008, -lr * Pred Grad:  0.077, New P: 0.065
iter 7 loss: 0.201
Actual params: [0.2127, 0.065 ]
-Original Grad: 0.090, -lr * Pred Grad:  0.097, New P: 0.310
-Original Grad: 0.008, -lr * Pred Grad:  0.082, New P: 0.147
iter 8 loss: 0.187
Actual params: [0.3102, 0.1473]
-Original Grad: 0.129, -lr * Pred Grad:  0.098, New P: 0.408
-Original Grad: 0.018, -lr * Pred Grad:  0.087, New P: 0.234
iter 9 loss: 0.173
Actual params: [0.4081, 0.2342]
-Original Grad: 0.011, -lr * Pred Grad:  0.090, New P: 0.498
-Original Grad: 0.017, -lr * Pred Grad:  0.091, New P: 0.326
iter 10 loss: 0.168
Actual params: [0.4981, 0.3256]
-Original Grad: 0.098, -lr * Pred Grad:  0.094, New P: 0.592
-Original Grad: 0.020, -lr * Pred Grad:  0.095, New P: 0.421
iter 11 loss: 0.171
Actual params: [0.5917, 0.4206]
-Original Grad: -0.084, -lr * Pred Grad:  0.062, New P: 0.654
-Original Grad: 0.046, -lr * Pred Grad:  0.091, New P: 0.512
iter 12 loss: 0.175
Actual params: [0.6537, 0.5117]
-Original Grad: -0.026, -lr * Pred Grad:  0.050, New P: 0.704
-Original Grad: 0.015, -lr * Pred Grad:  0.091, New P: 0.603
iter 13 loss: 0.174
Actual params: [0.7041, 0.6032]
-Original Grad: -0.022, -lr * Pred Grad:  0.041, New P: 0.745
-Original Grad: 0.076, -lr * Pred Grad:  0.090, New P: 0.693
iter 14 loss: 0.169
Actual params: [0.7451, 0.6928]
-Original Grad: 0.022, -lr * Pred Grad:  0.041, New P: 0.786
-Original Grad: 0.053, -lr * Pred Grad:  0.094, New P: 0.787
iter 15 loss: 0.160
Actual params: [0.7865, 0.7873]
-Original Grad: 0.119, -lr * Pred Grad:  0.055, New P: 0.842
-Original Grad: 0.083, -lr * Pred Grad:  0.098, New P: 0.885
iter 16 loss: 0.148
Actual params: [0.8416, 0.8851]
-Original Grad: 0.140, -lr * Pred Grad:  0.067, New P: 0.909
-Original Grad: 0.018, -lr * Pred Grad:  0.094, New P: 0.980
iter 17 loss: 0.134
Actual params: [0.9085, 0.9796]
-Original Grad: 0.103, -lr * Pred Grad:  0.074, New P: 0.982
-Original Grad: 0.041, -lr * Pred Grad:  0.096, New P: 1.076
iter 18 loss: 0.122
Actual params: [0.9821, 1.0759]
-Original Grad: 0.118, -lr * Pred Grad:  0.080, New P: 1.062
-Original Grad: 0.014, -lr * Pred Grad:  0.092, New P: 1.168
iter 19 loss: 0.113
Actual params: [1.062 , 1.1678]
-Original Grad: 0.051, -lr * Pred Grad:  0.079, New P: 1.141
-Original Grad: -0.000, -lr * Pred Grad:  0.083, New P: 1.251
iter 20 loss: 0.108
Actual params: [1.1414, 1.2513]
-Original Grad: 0.053, -lr * Pred Grad:  0.079, New P: 1.220
-Original Grad: -0.005, -lr * Pred Grad:  0.074, New P: 1.325
iter 21 loss: 0.107
Actual params: [1.2205, 1.3253]
-Original Grad: 0.052, -lr * Pred Grad:  0.079, New P: 1.299
-Original Grad: -0.057, -lr * Pred Grad:  0.044, New P: 1.369
iter 22 loss: 0.109
Actual params: [1.2993, 1.3692]
-Original Grad: -0.000, -lr * Pred Grad:  0.072, New P: 1.371
-Original Grad: -0.063, -lr * Pred Grad:  0.018, New P: 1.387
iter 23 loss: 0.113
Actual params: [1.3711, 1.3869]
-Original Grad: -0.054, -lr * Pred Grad:  0.057, New P: 1.428
-Original Grad: -0.063, -lr * Pred Grad:  -0.003, New P: 1.384
iter 24 loss: 0.116
Actual params: [1.4278, 1.3836]
-Original Grad: -0.086, -lr * Pred Grad:  0.038, New P: 1.465
-Original Grad: -0.092, -lr * Pred Grad:  -0.027, New P: 1.356
iter 25 loss: 0.117
Actual params: [1.4654, 1.3565]
-Original Grad: -0.080, -lr * Pred Grad:  0.022, New P: 1.487
-Original Grad: -0.069, -lr * Pred Grad:  -0.041, New P: 1.316
iter 26 loss: 0.115
Actual params: [1.4875, 1.3156]
-Original Grad: -0.078, -lr * Pred Grad:  0.009, New P: 1.496
-Original Grad: -0.056, -lr * Pred Grad:  -0.050, New P: 1.266
iter 27 loss: 0.113
Actual params: [1.496 , 1.2657]
-Original Grad: -0.065, -lr * Pred Grad:  -0.002, New P: 1.494
-Original Grad: -0.061, -lr * Pred Grad:  -0.059, New P: 1.207
iter 28 loss: 0.109
Actual params: [1.4945, 1.2071]
-Original Grad: -0.069, -lr * Pred Grad:  -0.011, New P: 1.483
-Original Grad: -0.055, -lr * Pred Grad:  -0.065, New P: 1.142
iter 29 loss: 0.105
Actual params: [1.4833, 1.1421]
-Original Grad: -0.071, -lr * Pred Grad:  -0.020, New P: 1.463
-Original Grad: -0.052, -lr * Pred Grad:  -0.070, New P: 1.072
iter 30 loss: 0.101
Actual params: [1.4633, 1.072 ]
-Original Grad: -0.053, -lr * Pred Grad:  -0.026, New P: 1.438
-Original Grad: -0.034, -lr * Pred Grad:  -0.071, New P: 1.001
Target params: [1.3344, 1.5708]
iter 0 loss: 0.065
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.050, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.008, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.057
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.082, -lr * Pred Grad:  0.098, New P: -0.274
-Original Grad: -0.009, -lr * Pred Grad:  -0.100, New P: -0.197
iter 2 loss: 0.048
Actual params: [-0.2739, -0.1966]
-Original Grad: 0.078, -lr * Pred Grad:  0.099, New P: -0.175
-Original Grad: 0.011, -lr * Pred Grad:  -0.014, New P: -0.210
iter 3 loss: 0.041
Actual params: [-0.1746, -0.2104]
-Original Grad: 0.063, -lr * Pred Grad:  0.099, New P: -0.076
-Original Grad: 0.011, -lr * Pred Grad:  0.023, New P: -0.187
iter 4 loss: 0.037
Actual params: [-0.0757, -0.1874]
-Original Grad: 0.002, -lr * Pred Grad:  0.084, New P: 0.009
-Original Grad: 0.005, -lr * Pred Grad:  0.033, New P: -0.155
iter 5 loss: 0.040
Actual params: [ 0.0086, -0.1547]
-Original Grad: -0.104, -lr * Pred Grad:  0.027, New P: 0.035
-Original Grad: -0.008, -lr * Pred Grad:  0.005, New P: -0.149
iter 6 loss: 0.043
Actual params: [ 0.0355, -0.1492]
-Original Grad: -0.092, -lr * Pred Grad:  -0.003, New P: 0.033
-Original Grad: 0.003, -lr * Pred Grad:  0.012, New P: -0.137
iter 7 loss: 0.043
Actual params: [ 0.0325, -0.1369]
-Original Grad: -0.013, -lr * Pred Grad:  -0.006, New P: 0.027
-Original Grad: 0.019, -lr * Pred Grad:  0.041, New P: -0.096
iter 8 loss: 0.041
Actual params: [ 0.0266, -0.0961]
-Original Grad: -0.243, -lr * Pred Grad:  -0.041, New P: -0.015
-Original Grad: 0.062, -lr * Pred Grad:  0.060, New P: -0.037
iter 9 loss: 0.035
Actual params: [-0.0147, -0.0365]
-Original Grad: -0.033, -lr * Pred Grad:  -0.042, New P: -0.056
-Original Grad: 0.030, -lr * Pred Grad:  0.068, New P: 0.032
iter 10 loss: 0.034
Actual params: [-0.0564,  0.0316]
-Original Grad: 0.043, -lr * Pred Grad:  -0.031, New P: -0.087
-Original Grad: 0.008, -lr * Pred Grad:  0.066, New P: 0.098
iter 11 loss: 0.035
Actual params: [-0.087 ,  0.0976]
-Original Grad: 0.057, -lr * Pred Grad:  -0.019, New P: -0.106
-Original Grad: -0.006, -lr * Pred Grad:  0.055, New P: 0.152
iter 12 loss: 0.037
Actual params: [-0.1055,  0.1525]
-Original Grad: 0.066, -lr * Pred Grad:  -0.007, New P: -0.112
-Original Grad: -0.012, -lr * Pred Grad:  0.041, New P: 0.194
iter 13 loss: 0.038
Actual params: [-0.1123,  0.1939]
-Original Grad: 0.081, -lr * Pred Grad:  0.006, New P: -0.107
-Original Grad: -0.017, -lr * Pred Grad:  0.026, New P: 0.220
iter 14 loss: 0.038
Actual params: [-0.1067,  0.2198]
-Original Grad: 0.043, -lr * Pred Grad:  0.011, New P: -0.096
-Original Grad: -0.009, -lr * Pred Grad:  0.018, New P: 0.238
iter 15 loss: 0.037
Actual params: [-0.0956,  0.2377]
-Original Grad: 0.084, -lr * Pred Grad:  0.022, New P: -0.074
-Original Grad: -0.027, -lr * Pred Grad:  -0.000, New P: 0.237
iter 16 loss: 0.036
Actual params: [-0.0741,  0.2372]
-Original Grad: 0.066, -lr * Pred Grad:  0.028, New P: -0.046
-Original Grad: -0.015, -lr * Pred Grad:  -0.009, New P: 0.228
iter 17 loss: 0.034
Actual params: [-0.0458,  0.228 ]
-Original Grad: 0.068, -lr * Pred Grad:  0.035, New P: -0.011
-Original Grad: -0.012, -lr * Pred Grad:  -0.015, New P: 0.213
iter 18 loss: 0.032
Actual params: [-0.0112,  0.2129]
-Original Grad: 0.041, -lr * Pred Grad:  0.037, New P: 0.026
-Original Grad: -0.002, -lr * Pred Grad:  -0.015, New P: 0.198
iter 19 loss: 0.031
Actual params: [0.0256, 0.1981]
-Original Grad: 0.018, -lr * Pred Grad:  0.036, New P: 0.062
-Original Grad: 0.007, -lr * Pred Grad:  -0.009, New P: 0.189
iter 20 loss: 0.032
Actual params: [0.0617, 0.1887]
-Original Grad: -0.009, -lr * Pred Grad:  0.032, New P: 0.093
-Original Grad: 0.016, -lr * Pred Grad:  0.001, New P: 0.190
iter 21 loss: 0.034
Actual params: [0.0932, 0.1901]
-Original Grad: -0.070, -lr * Pred Grad:  0.018, New P: 0.112
-Original Grad: 0.041, -lr * Pred Grad:  0.023, New P: 0.214
iter 22 loss: 0.034
Actual params: [0.1116, 0.2135]
-Original Grad: -0.136, -lr * Pred Grad:  -0.002, New P: 0.109
-Original Grad: 0.063, -lr * Pred Grad:  0.047, New P: 0.260
iter 23 loss: 0.032
Actual params: [0.1093, 0.2601]
-Original Grad: -0.064, -lr * Pred Grad:  -0.010, New P: 0.099
-Original Grad: 0.037, -lr * Pred Grad:  0.057, New P: 0.317
iter 24 loss: 0.030
Actual params: [0.0989, 0.3167]
-Original Grad: -0.033, -lr * Pred Grad:  -0.014, New P: 0.085
-Original Grad: 0.021, -lr * Pred Grad:  0.060, New P: 0.377
iter 25 loss: 0.029
Actual params: [0.085 , 0.3767]
-Original Grad: 0.028, -lr * Pred Grad:  -0.009, New P: 0.076
-Original Grad: -0.001, -lr * Pred Grad:  0.054, New P: 0.431
iter 26 loss: 0.030
Actual params: [0.0762, 0.4306]
-Original Grad: 0.046, -lr * Pred Grad:  -0.002, New P: 0.074
-Original Grad: -0.008, -lr * Pred Grad:  0.045, New P: 0.476
iter 27 loss: 0.030
Actual params: [0.0743, 0.4759]
-Original Grad: 0.065, -lr * Pred Grad:  0.007, New P: 0.082
-Original Grad: -0.018, -lr * Pred Grad:  0.033, New P: 0.509
iter 28 loss: 0.031
Actual params: [0.0816, 0.5088]
-Original Grad: 0.059, -lr * Pred Grad:  0.015, New P: 0.096
-Original Grad: -0.020, -lr * Pred Grad:  0.021, New P: 0.529
iter 29 loss: 0.030
Actual params: [0.0961, 0.5295]
-Original Grad: 0.071, -lr * Pred Grad:  0.023, New P: 0.119
-Original Grad: -0.022, -lr * Pred Grad:  0.009, New P: 0.538
iter 30 loss: 0.029
Actual params: [0.1187, 0.5381]
-Original Grad: 0.030, -lr * Pred Grad:  0.025, New P: 0.143
-Original Grad: -0.005, -lr * Pred Grad:  0.006, New P: 0.544
Target params: [1.3344, 1.5708]
iter 0 loss: 0.575
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.056, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.083, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.562
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.033, -lr * Pred Grad:  -0.096, New P: -0.668
-Original Grad: 0.070, -lr * Pred Grad:  0.099, New P: 0.203
iter 2 loss: 0.553
Actual params: [-0.6679,  0.2028]
-Original Grad: -0.018, -lr * Pred Grad:  -0.088, New P: -0.756
-Original Grad: 0.067, -lr * Pred Grad:  0.099, New P: 0.302
iter 3 loss: 0.547
Actual params: [-0.7563,  0.3017]
-Original Grad: -0.004, -lr * Pred Grad:  -0.075, New P: -0.832
-Original Grad: 0.037, -lr * Pred Grad:  0.094, New P: 0.396
iter 4 loss: 0.543
Actual params: [-0.8316,  0.3956]
-Original Grad: 0.006, -lr * Pred Grad:  -0.059, New P: -0.891
-Original Grad: 0.028, -lr * Pred Grad:  0.089, New P: 0.485
iter 5 loss: 0.542
Actual params: [-0.8906,  0.4846]
-Original Grad: 0.013, -lr * Pred Grad:  -0.040, New P: -0.931
-Original Grad: 0.016, -lr * Pred Grad:  0.082, New P: 0.567
iter 6 loss: 0.541
Actual params: [-0.9307,  0.5669]
-Original Grad: 0.011, -lr * Pred Grad:  -0.027, New P: -0.958
-Original Grad: 0.007, -lr * Pred Grad:  0.074, New P: 0.641
iter 7 loss: 0.542
Actual params: [-0.9576,  0.6411]
-Original Grad: 0.013, -lr * Pred Grad:  -0.014, New P: -0.972
-Original Grad: 0.001, -lr * Pred Grad:  0.066, New P: 0.707
iter 8 loss: 0.542
Actual params: [-0.9721,  0.7068]
-Original Grad: 0.013, -lr * Pred Grad:  -0.004, New P: -0.976
-Original Grad: -0.001, -lr * Pred Grad:  0.058, New P: 0.765
iter 9 loss: 0.542
Actual params: [-0.9759,  0.7646]
-Original Grad: 0.013, -lr * Pred Grad:  0.005, New P: -0.971
-Original Grad: -0.004, -lr * Pred Grad:  0.050, New P: 0.815
iter 10 loss: 0.542
Actual params: [-0.9706,  0.8149]
-Original Grad: 0.014, -lr * Pred Grad:  0.014, New P: -0.957
-Original Grad: -0.006, -lr * Pred Grad:  0.043, New P: 0.858
iter 11 loss: 0.542
Actual params: [-0.957 ,  0.8578]
-Original Grad: 0.012, -lr * Pred Grad:  0.020, New P: -0.937
-Original Grad: -0.005, -lr * Pred Grad:  0.037, New P: 0.894
iter 12 loss: 0.542
Actual params: [-0.9373,  0.8944]
-Original Grad: 0.016, -lr * Pred Grad:  0.027, New P: -0.910
-Original Grad: -0.008, -lr * Pred Grad:  0.030, New P: 0.925
iter 13 loss: 0.542
Actual params: [-0.9101,  0.9246]
-Original Grad: 0.010, -lr * Pred Grad:  0.031, New P: -0.879
-Original Grad: -0.005, -lr * Pred Grad:  0.026, New P: 0.950
iter 14 loss: 0.542
Actual params: [-0.8794,  0.9502]
-Original Grad: 0.014, -lr * Pred Grad:  0.036, New P: -0.844
-Original Grad: -0.007, -lr * Pred Grad:  0.021, New P: 0.971
iter 15 loss: 0.541
Actual params: [-0.8436,  0.971 ]
-Original Grad: 0.015, -lr * Pred Grad:  0.041, New P: -0.803
-Original Grad: -0.008, -lr * Pred Grad:  0.016, New P: 0.987
iter 16 loss: 0.541
Actual params: [-0.8026,  0.987 ]
-Original Grad: 0.019, -lr * Pred Grad:  0.047, New P: -0.755
-Original Grad: -0.010, -lr * Pred Grad:  0.011, New P: 0.998
iter 17 loss: 0.540
Actual params: [-0.7553,  0.998 ]
-Original Grad: 0.013, -lr * Pred Grad:  0.050, New P: -0.705
-Original Grad: -0.007, -lr * Pred Grad:  0.007, New P: 1.005
iter 18 loss: 0.539
Actual params: [-0.705 ,  1.0053]
-Original Grad: 0.020, -lr * Pred Grad:  0.056, New P: -0.649
-Original Grad: -0.011, -lr * Pred Grad:  0.003, New P: 1.008
iter 19 loss: 0.538
Actual params: [-0.6487,  1.0081]
-Original Grad: 0.028, -lr * Pred Grad:  0.064, New P: -0.584
-Original Grad: -0.014, -lr * Pred Grad:  -0.003, New P: 1.006
iter 20 loss: 0.535
Actual params: [-0.5844,  1.0055]
-Original Grad: 0.035, -lr * Pred Grad:  0.073, New P: -0.511
-Original Grad: -0.018, -lr * Pred Grad:  -0.009, New P: 0.997
iter 21 loss: 0.532
Actual params: [-0.5115,  0.9966]
-Original Grad: 0.058, -lr * Pred Grad:  0.084, New P: -0.428
-Original Grad: -0.030, -lr * Pred Grad:  -0.019, New P: 0.978
iter 22 loss: 0.525
Actual params: [-0.4279,  0.9776]
-Original Grad: 0.075, -lr * Pred Grad:  0.092, New P: -0.336
-Original Grad: -0.031, -lr * Pred Grad:  -0.028, New P: 0.950
iter 23 loss: 0.517
Actual params: [-0.3355,  0.9496]
-Original Grad: 0.091, -lr * Pred Grad:  0.099, New P: -0.236
-Original Grad: -0.022, -lr * Pred Grad:  -0.033, New P: 0.917
iter 24 loss: 0.505
Actual params: [-0.2362,  0.9166]
-Original Grad: 0.110, -lr * Pred Grad:  0.105, New P: -0.131
-Original Grad: -0.009, -lr * Pred Grad:  -0.033, New P: 0.883
iter 25 loss: 0.493
Actual params: [-0.1312,  0.8833]
-Original Grad: 0.149, -lr * Pred Grad:  0.109, New P: -0.022
-Original Grad: 0.057, -lr * Pred Grad:  -0.009, New P: 0.875
iter 26 loss: 0.480
Actual params: [-0.0223,  0.8745]
-Original Grad: 0.132, -lr * Pred Grad:  0.113, New P: 0.091
-Original Grad: 0.073, -lr * Pred Grad:  0.016, New P: 0.890
iter 27 loss: 0.460
Actual params: [0.0911, 0.8903]
-Original Grad: 0.168, -lr * Pred Grad:  0.117, New P: 0.208
-Original Grad: 0.117, -lr * Pred Grad:  0.043, New P: 0.933
iter 28 loss: 0.429
Actual params: [0.2082, 0.9331]
-Original Grad: 0.200, -lr * Pred Grad:  0.120, New P: 0.328
-Original Grad: 0.142, -lr * Pred Grad:  0.064, New P: 0.997
iter 29 loss: 0.384
Actual params: [0.3284, 0.9969]
-Original Grad: 0.264, -lr * Pred Grad:  0.122, New P: 0.451
-Original Grad: 0.177, -lr * Pred Grad:  0.080, New P: 1.077
iter 30 loss: 0.329
Actual params: [0.4509, 1.0771]
-Original Grad: 0.291, -lr * Pred Grad:  0.125, New P: 0.576
-Original Grad: 0.274, -lr * Pred Grad:  0.093, New P: 1.170
Target params: [1.3344, 1.5708]
iter 0 loss: 0.309
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.309
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.080, New P: -0.652
-Original Grad: 0.000, -lr * Pred Grad:  0.090, New P: 0.193
iter 2 loss: 0.309
Actual params: [-0.6521,  0.1931]
-Original Grad: 0.000, -lr * Pred Grad:  -0.059, New P: -0.712
-Original Grad: 0.000, -lr * Pred Grad:  0.083, New P: 0.276
iter 3 loss: 0.309
Actual params: [-0.7115,  0.2763]
-Original Grad: 0.000, -lr * Pred Grad:  -0.042, New P: -0.754
-Original Grad: 0.000, -lr * Pred Grad:  0.079, New P: 0.356
iter 4 loss: 0.309
Actual params: [-0.754 ,  0.3556]
-Original Grad: 0.000, -lr * Pred Grad:  -0.016, New P: -0.770
-Original Grad: 0.000, -lr * Pred Grad:  0.085, New P: 0.441
iter 5 loss: 0.309
Actual params: [-0.7703,  0.4406]
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: -0.760
-Original Grad: 0.000, -lr * Pred Grad:  0.090, New P: 0.531
iter 6 loss: 0.309
Actual params: [-0.7601,  0.5305]
-Original Grad: 0.000, -lr * Pred Grad:  0.034, New P: -0.726
-Original Grad: 0.000, -lr * Pred Grad:  0.093, New P: 0.624
iter 7 loss: 0.308
Actual params: [-0.7261,  0.6237]
-Original Grad: 0.001, -lr * Pred Grad:  0.057, New P: -0.669
-Original Grad: 0.001, -lr * Pred Grad:  0.072, New P: 0.696
iter 8 loss: 0.308
Actual params: [-0.6694,  0.6955]
-Original Grad: 0.003, -lr * Pred Grad:  0.065, New P: -0.605
-Original Grad: 0.003, -lr * Pred Grad:  0.071, New P: 0.766
iter 9 loss: 0.308
Actual params: [-0.6046,  0.7662]
-Original Grad: 0.005, -lr * Pred Grad:  0.072, New P: -0.532
-Original Grad: 0.005, -lr * Pred Grad:  0.076, New P: 0.842
iter 10 loss: 0.306
Actual params: [-0.5321,  0.8422]
-Original Grad: 0.021, -lr * Pred Grad:  0.064, New P: -0.468
-Original Grad: 0.020, -lr * Pred Grad:  0.066, New P: 0.909
iter 11 loss: 0.302
Actual params: [-0.4683,  0.9087]
-Original Grad: 0.046, -lr * Pred Grad:  0.068, New P: -0.400
-Original Grad: 0.041, -lr * Pred Grad:  0.070, New P: 0.979
iter 12 loss: 0.294
Actual params: [-0.4004,  0.9788]
-Original Grad: 0.061, -lr * Pred Grad:  0.076, New P: -0.324
-Original Grad: 0.056, -lr * Pred Grad:  0.077, New P: 1.056
iter 13 loss: 0.276
Actual params: [-0.3242,  1.056 ]
-Original Grad: 0.268, -lr * Pred Grad:  0.066, New P: -0.258
-Original Grad: 0.213, -lr * Pred Grad:  0.068, New P: 1.124
iter 14 loss: 0.253
Actual params: [-0.2583,  1.1243]
-Original Grad: 0.154, -lr * Pred Grad:  0.076, New P: -0.183
-Original Grad: 0.117, -lr * Pred Grad:  0.077, New P: 1.202
iter 15 loss: 0.226
Actual params: [-0.1826,  1.2015]
-Original Grad: 0.264, -lr * Pred Grad:  0.084, New P: -0.099
-Original Grad: 0.166, -lr * Pred Grad:  0.085, New P: 1.287
iter 16 loss: 0.194
Actual params: [-0.0986,  1.2869]
-Original Grad: 0.186, -lr * Pred Grad:  0.090, New P: -0.009
-Original Grad: 0.081, -lr * Pred Grad:  0.088, New P: 1.374
iter 17 loss: 0.161
Actual params: [-0.0088,  1.3744]
-Original Grad: 0.256, -lr * Pred Grad:  0.095, New P: 0.087
-Original Grad: 0.074, -lr * Pred Grad:  0.089, New P: 1.463
iter 18 loss: 0.129
Actual params: [0.0866, 1.4633]
-Original Grad: 0.219, -lr * Pred Grad:  0.099, New P: 0.186
-Original Grad: 0.013, -lr * Pred Grad:  0.083, New P: 1.546
iter 19 loss: 0.100
Actual params: [0.186 , 1.5459]
-Original Grad: 0.136, -lr * Pred Grad:  0.100, New P: 0.286
-Original Grad: 0.011, -lr * Pred Grad:  0.077, New P: 1.623
iter 20 loss: 0.075
Actual params: [0.2857, 1.6228]
-Original Grad: 0.147, -lr * Pred Grad:  0.101, New P: 0.386
-Original Grad: -0.008, -lr * Pred Grad:  0.069, New P: 1.691
iter 21 loss: 0.054
Actual params: [0.3863, 1.6914]
-Original Grad: 0.130, -lr * Pred Grad:  0.100, New P: 0.487
-Original Grad: -0.017, -lr * Pred Grad:  0.060, New P: 1.751
iter 22 loss: 0.040
Actual params: [0.4867, 1.751 ]
-Original Grad: 0.087, -lr * Pred Grad:  0.098, New P: 0.585
-Original Grad: -0.042, -lr * Pred Grad:  0.047, New P: 1.798
iter 23 loss: 0.031
Actual params: [0.5846, 1.7981]
-Original Grad: 0.049, -lr * Pred Grad:  0.093, New P: 0.678
-Original Grad: -0.035, -lr * Pred Grad:  0.037, New P: 1.835
iter 24 loss: 0.025
Actual params: [0.6777, 1.835 ]
-Original Grad: 0.049, -lr * Pred Grad:  0.089, New P: 0.766
-Original Grad: -0.041, -lr * Pred Grad:  0.027, New P: 1.862
iter 25 loss: 0.023
Actual params: [0.7664, 1.8616]
-Original Grad: 0.001, -lr * Pred Grad:  0.081, New P: 0.847
-Original Grad: -0.040, -lr * Pred Grad:  0.018, New P: 1.879
iter 26 loss: 0.022
Actual params: [0.8472, 1.8792]
-Original Grad: 0.004, -lr * Pred Grad:  0.074, New P: 0.921
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 1.896
iter 27 loss: 0.022
Actual params: [0.9212, 1.8955]
-Original Grad: 0.009, -lr * Pred Grad:  0.068, New P: 0.989
-Original Grad: -0.035, -lr * Pred Grad:  0.009, New P: 1.904
iter 28 loss: 0.022
Actual params: [0.9894, 1.9045]
-Original Grad: -0.000, -lr * Pred Grad:  0.062, New P: 1.052
-Original Grad: -0.011, -lr * Pred Grad:  0.006, New P: 1.911
iter 29 loss: 0.022
Actual params: [1.0516, 1.9108]
-Original Grad: -0.020, -lr * Pred Grad:  0.055, New P: 1.106
-Original Grad: -0.010, -lr * Pred Grad:  0.004, New P: 1.915
iter 30 loss: 0.023
Actual params: [1.1062, 1.9149]
-Original Grad: -0.009, -lr * Pred Grad:  0.049, New P: 1.155
-Original Grad: -0.005, -lr * Pred Grad:  0.003, New P: 1.918
Target params: [1.3344, 1.5708]
iter 0 loss: 0.049
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.021, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.011, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.047
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.009, -lr * Pred Grad:  -0.090, New P: -0.662
-Original Grad: 0.005, -lr * Pred Grad:  0.091, New P: 0.195
iter 2 loss: 0.047
Actual params: [-0.6625,  0.1946]
-Original Grad: -0.002, -lr * Pred Grad:  -0.074, New P: -0.737
-Original Grad: 0.001, -lr * Pred Grad:  0.075, New P: 0.270
iter 3 loss: 0.046
Actual params: [-0.7366,  0.2698]
-Original Grad: -0.001, -lr * Pred Grad:  -0.063, New P: -0.800
-Original Grad: 0.001, -lr * Pred Grad:  0.064, New P: 0.334
iter 4 loss: 0.046
Actual params: [-0.7996,  0.334 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.054, New P: -0.854
-Original Grad: 0.000, -lr * Pred Grad:  0.055, New P: 0.389
iter 5 loss: 0.046
Actual params: [-0.8538,  0.3891]
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -0.901
-Original Grad: 0.000, -lr * Pred Grad:  0.048, New P: 0.437
iter 6 loss: 0.046
Actual params: [-0.901 ,  0.4371]
-Original Grad: -0.000, -lr * Pred Grad:  -0.041, New P: -0.942
-Original Grad: 0.000, -lr * Pred Grad:  0.042, New P: 0.479
iter 7 loss: 0.046
Actual params: [-0.9423,  0.4793]
-Original Grad: -0.000, -lr * Pred Grad:  -0.037, New P: -0.979
-Original Grad: 0.000, -lr * Pred Grad:  0.037, New P: 0.517
iter 8 loss: 0.046
Actual params: [-0.9789,  0.5167]
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -1.011
-Original Grad: 0.000, -lr * Pred Grad:  0.033, New P: 0.550
iter 9 loss: 0.046
Actual params: [-1.0115,  0.5499]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.041
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: 0.580
iter 10 loss: 0.046
Actual params: [-1.0406,  0.5797]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.067
-Original Grad: 0.000, -lr * Pred Grad:  0.027, New P: 0.606
iter 11 loss: 0.046
Actual params: [-1.0667,  0.6064]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.090
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.630
iter 12 loss: 0.046
Actual params: [-1.0902,  0.6304]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.111
-Original Grad: 0.000, -lr * Pred Grad:  0.022, New P: 0.652
iter 13 loss: 0.046
Actual params: [-1.1115,  0.6521]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.131
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.672
iter 14 loss: 0.046
Actual params: [-1.1306,  0.6717]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.148
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.689
iter 15 loss: 0.046
Actual params: [-1.148 ,  0.6894]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.164
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.706
iter 16 loss: 0.046
Actual params: [-1.1638,  0.7055]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.178
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.720
iter 17 loss: 0.046
Actual params: [-1.1781,  0.7202]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.191
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.733
iter 18 loss: 0.046
Actual params: [-1.191 ,  0.7334]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.203
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.746
iter 19 loss: 0.046
Actual params: [-1.2029,  0.7455]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.214
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.757
iter 20 loss: 0.046
Actual params: [-1.2136,  0.7565]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.223
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.767
iter 21 loss: 0.046
Actual params: [-1.2234,  0.7665]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.232
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.776
iter 22 loss: 0.046
Actual params: [-1.2323,  0.7756]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.240
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.784
iter 23 loss: 0.046
Actual params: [-1.2404,  0.7839]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.248
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.792
iter 24 loss: 0.046
Actual params: [-1.2478,  0.7915]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.255
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.798
iter 25 loss: 0.046
Actual params: [-1.2545,  0.7984]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.261
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.805
iter 26 loss: 0.046
Actual params: [-1.2607,  0.8047]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.266
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.810
iter 27 loss: 0.046
Actual params: [-1.2663,  0.8104]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.271
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.816
iter 28 loss: 0.046
Actual params: [-1.2714,  0.8156]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.276
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.820
iter 29 loss: 0.046
Actual params: [-1.276 ,  0.8204]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.280
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.825
iter 30 loss: 0.046
Actual params: [-1.2802,  0.8247]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.284
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.829
Target params: [1.3344, 1.5708]
iter 0 loss: 0.431
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.000, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.430
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.005, -lr * Pred Grad:  0.088, New P: -0.284
-Original Grad: -0.000, -lr * Pred Grad:  -0.087, New P: -0.183
iter 2 loss: 0.430
Actual params: [-0.2843, -0.1831]
-Original Grad: 0.009, -lr * Pred Grad:  0.090, New P: -0.194
-Original Grad: -0.001, -lr * Pred Grad:  -0.087, New P: -0.270
iter 3 loss: 0.427
Actual params: [-0.1942, -0.2704]
-Original Grad: 0.015, -lr * Pred Grad:  0.090, New P: -0.104
-Original Grad: -0.001, -lr * Pred Grad:  -0.090, New P: -0.360
iter 4 loss: 0.423
Actual params: [-0.1042, -0.3602]
-Original Grad: 0.037, -lr * Pred Grad:  0.083, New P: -0.022
-Original Grad: -0.002, -lr * Pred Grad:  -0.090, New P: -0.450
iter 5 loss: 0.416
Actual params: [-0.0215, -0.4502]
-Original Grad: 0.034, -lr * Pred Grad:  0.088, New P: 0.067
-Original Grad: 0.002, -lr * Pred Grad:  -0.025, New P: -0.475
iter 6 loss: 0.407
Actual params: [ 0.0667, -0.475 ]
-Original Grad: 0.080, -lr * Pred Grad:  0.085, New P: 0.152
-Original Grad: 0.014, -lr * Pred Grad:  0.043, New P: -0.432
iter 7 loss: 0.395
Actual params: [ 0.1515, -0.4316]
-Original Grad: 0.076, -lr * Pred Grad:  0.089, New P: 0.241
-Original Grad: 0.034, -lr * Pred Grad:  0.061, New P: -0.371
iter 8 loss: 0.379
Actual params: [ 0.2409, -0.371 ]
-Original Grad: 0.075, -lr * Pred Grad:  0.093, New P: 0.334
-Original Grad: 0.030, -lr * Pred Grad:  0.073, New P: -0.298
iter 9 loss: 0.354
Actual params: [ 0.3342, -0.2985]
-Original Grad: 0.140, -lr * Pred Grad:  0.093, New P: 0.428
-Original Grad: 0.061, -lr * Pred Grad:  0.078, New P: -0.221
iter 10 loss: 0.319
Actual params: [ 0.4275, -0.2206]
-Original Grad: 0.182, -lr * Pred Grad:  0.094, New P: 0.522
-Original Grad: 0.069, -lr * Pred Grad:  0.084, New P: -0.136
iter 11 loss: 0.270
Actual params: [ 0.5219, -0.1364]
-Original Grad: 0.360, -lr * Pred Grad:  0.089, New P: 0.611
-Original Grad: 0.121, -lr * Pred Grad:  0.086, New P: -0.051
iter 12 loss: 0.217
Actual params: [ 0.6114, -0.0507]
-Original Grad: 0.248, -lr * Pred Grad:  0.094, New P: 0.705
-Original Grad: 0.145, -lr * Pred Grad:  0.090, New P: 0.039
iter 13 loss: 0.164
Actual params: [0.7053, 0.0389]
-Original Grad: 0.163, -lr * Pred Grad:  0.095, New P: 0.801
-Original Grad: 0.195, -lr * Pred Grad:  0.092, New P: 0.131
iter 14 loss: 0.121
Actual params: [0.8008, 0.1314]
-Original Grad: 0.097, -lr * Pred Grad:  0.094, New P: 0.894
-Original Grad: 0.119, -lr * Pred Grad:  0.096, New P: 0.227
iter 15 loss: 0.091
Actual params: [0.8944, 0.2273]
-Original Grad: 0.009, -lr * Pred Grad:  0.086, New P: 0.980
-Original Grad: 0.162, -lr * Pred Grad:  0.100, New P: 0.327
iter 16 loss: 0.069
Actual params: [0.9801, 0.3269]
-Original Grad: 0.020, -lr * Pred Grad:  0.080, New P: 1.060
-Original Grad: 0.109, -lr * Pred Grad:  0.101, New P: 0.428
iter 17 loss: 0.053
Actual params: [1.0597, 0.4278]
-Original Grad: -0.020, -lr * Pred Grad:  0.070, New P: 1.130
-Original Grad: 0.067, -lr * Pred Grad:  0.099, New P: 0.527
iter 18 loss: 0.040
Actual params: [1.13  , 0.5268]
-Original Grad: -0.018, -lr * Pred Grad:  0.062, New P: 1.192
-Original Grad: 0.106, -lr * Pred Grad:  0.100, New P: 0.627
iter 19 loss: 0.032
Actual params: [1.1923, 0.627 ]
-Original Grad: -0.037, -lr * Pred Grad:  0.053, New P: 1.245
-Original Grad: 0.044, -lr * Pred Grad:  0.096, New P: 0.723
iter 20 loss: 0.027
Actual params: [1.2453, 0.7233]
-Original Grad: -0.035, -lr * Pred Grad:  0.045, New P: 1.290
-Original Grad: 0.041, -lr * Pred Grad:  0.092, New P: 0.816
iter 21 loss: 0.024
Actual params: [1.2902, 0.8157]
-Original Grad: -0.036, -lr * Pred Grad:  0.037, New P: 1.328
-Original Grad: 0.020, -lr * Pred Grad:  0.087, New P: 0.902
iter 22 loss: 0.021
Actual params: [1.3276, 0.9023]
-Original Grad: -0.029, -lr * Pred Grad:  0.031, New P: 1.359
-Original Grad: 0.029, -lr * Pred Grad:  0.082, New P: 0.985
iter 23 loss: 0.019
Actual params: [1.3588, 0.9848]
-Original Grad: -0.009, -lr * Pred Grad:  0.028, New P: 1.386
-Original Grad: 0.018, -lr * Pred Grad:  0.077, New P: 1.062
iter 24 loss: 0.019
Actual params: [1.3864, 1.0623]
-Original Grad: 0.013, -lr * Pred Grad:  0.026, New P: 1.413
-Original Grad: -0.031, -lr * Pred Grad:  0.066, New P: 1.128
iter 25 loss: 0.017
Actual params: [1.4129, 1.1284]
-Original Grad: 0.014, -lr * Pred Grad:  0.025, New P: 1.438
-Original Grad: -0.010, -lr * Pred Grad:  0.059, New P: 1.187
iter 26 loss: 0.018
Actual params: [1.4383, 1.1873]
-Original Grad: 0.027, -lr * Pred Grad:  0.026, New P: 1.464
-Original Grad: -0.011, -lr * Pred Grad:  0.052, New P: 1.239
iter 27 loss: 0.021
Actual params: [1.4641, 1.2395]
-Original Grad: 0.067, -lr * Pred Grad:  0.030, New P: 1.494
-Original Grad: -0.098, -lr * Pred Grad:  0.033, New P: 1.272
iter 28 loss: 0.022
Actual params: [1.494 , 1.2724]
-Original Grad: 0.042, -lr * Pred Grad:  0.031, New P: 1.526
-Original Grad: -0.053, -lr * Pred Grad:  0.023, New P: 1.295
iter 29 loss: 0.022
Actual params: [1.5255, 1.2949]
-Original Grad: 0.044, -lr * Pred Grad:  0.033, New P: 1.559
-Original Grad: -0.046, -lr * Pred Grad:  0.014, New P: 1.309
iter 30 loss: 0.021
Actual params: [1.5585, 1.309 ]
-Original Grad: 0.053, -lr * Pred Grad:  0.035, New P: 1.594
-Original Grad: -0.073, -lr * Pred Grad:  0.003, New P: 1.312
Target params: [1.3344, 1.5708]
iter 0 loss: 0.731
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.008, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.003, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.729
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.030, -lr * Pred Grad:  0.090, New P: -0.283
-Original Grad: -0.009, -lr * Pred Grad:  -0.090, New P: -0.187
iter 2 loss: 0.724
Actual params: [-0.2827, -0.1868]
-Original Grad: 0.045, -lr * Pred Grad:  0.092, New P: -0.191
-Original Grad: -0.012, -lr * Pred Grad:  -0.093, New P: -0.280
iter 3 loss: 0.714
Actual params: [-0.1907, -0.28  ]
-Original Grad: 0.108, -lr * Pred Grad:  0.086, New P: -0.105
-Original Grad: -0.022, -lr * Pred Grad:  -0.092, New P: -0.372
iter 4 loss: 0.703
Actual params: [-0.1049, -0.3717]
-Original Grad: 0.119, -lr * Pred Grad:  0.090, New P: -0.015
-Original Grad: 0.007, -lr * Pred Grad:  -0.062, New P: -0.434
iter 5 loss: 0.695
Actual params: [-0.0149, -0.4342]
-Original Grad: 0.116, -lr * Pred Grad:  0.093, New P: 0.079
-Original Grad: 0.045, -lr * Pred Grad:  0.016, New P: -0.418
iter 6 loss: 0.683
Actual params: [ 0.0786, -0.4179]
-Original Grad: 0.127, -lr * Pred Grad:  0.096, New P: 0.175
-Original Grad: 0.059, -lr * Pred Grad:  0.047, New P: -0.371
iter 7 loss: 0.664
Actual params: [ 0.1747, -0.3708]
-Original Grad: 0.179, -lr * Pred Grad:  0.098, New P: 0.272
-Original Grad: 0.080, -lr * Pred Grad:  0.064, New P: -0.306
iter 8 loss: 0.638
Actual params: [ 0.2722, -0.3063]
-Original Grad: 0.225, -lr * Pred Grad:  0.099, New P: 0.371
-Original Grad: 0.122, -lr * Pred Grad:  0.075, New P: -0.232
iter 9 loss: 0.603
Actual params: [ 0.3708, -0.2316]
-Original Grad: 0.331, -lr * Pred Grad:  0.098, New P: 0.469
-Original Grad: 0.148, -lr * Pred Grad:  0.082, New P: -0.150
iter 10 loss: 0.559
Actual params: [ 0.4689, -0.1496]
-Original Grad: 0.388, -lr * Pred Grad:  0.099, New P: 0.568
-Original Grad: 0.259, -lr * Pred Grad:  0.084, New P: -0.065
iter 11 loss: 0.507
Actual params: [ 0.5679, -0.0651]
-Original Grad: 0.308, -lr * Pred Grad:  0.101, New P: 0.669
-Original Grad: 0.256, -lr * Pred Grad:  0.090, New P: 0.025
iter 12 loss: 0.449
Actual params: [0.669 , 0.0245]
-Original Grad: 0.322, -lr * Pred Grad:  0.103, New P: 0.772
-Original Grad: 0.294, -lr * Pred Grad:  0.094, New P: 0.118
iter 13 loss: 0.390
Actual params: [0.7719, 0.1184]
-Original Grad: 0.287, -lr * Pred Grad:  0.104, New P: 0.876
-Original Grad: 0.320, -lr * Pred Grad:  0.098, New P: 0.216
iter 14 loss: 0.337
Actual params: [0.8759, 0.216 ]
-Original Grad: 0.171, -lr * Pred Grad:  0.102, New P: 0.978
-Original Grad: 0.191, -lr * Pred Grad:  0.099, New P: 0.315
iter 15 loss: 0.297
Actual params: [0.9779, 0.3148]
-Original Grad: 0.012, -lr * Pred Grad:  0.093, New P: 1.071
-Original Grad: 0.378, -lr * Pred Grad:  0.102, New P: 0.417
iter 16 loss: 0.263
Actual params: [1.0711, 0.4168]
-Original Grad: -0.035, -lr * Pred Grad:  0.082, New P: 1.153
-Original Grad: 0.331, -lr * Pred Grad:  0.105, New P: 0.521
iter 17 loss: 0.230
Actual params: [1.1535, 0.5213]
-Original Grad: -0.030, -lr * Pred Grad:  0.073, New P: 1.226
-Original Grad: 0.416, -lr * Pred Grad:  0.107, New P: 0.629
iter 18 loss: 0.199
Actual params: [1.2265, 0.6285]
-Original Grad: -0.046, -lr * Pred Grad:  0.064, New P: 1.290
-Original Grad: 0.301, -lr * Pred Grad:  0.108, New P: 0.737
iter 19 loss: 0.172
Actual params: [1.29  , 0.7368]
-Original Grad: -0.057, -lr * Pred Grad:  0.054, New P: 1.344
-Original Grad: 0.253, -lr * Pred Grad:  0.108, New P: 0.845
iter 20 loss: 0.149
Actual params: [1.3443, 0.8449]
-Original Grad: -0.089, -lr * Pred Grad:  0.044, New P: 1.388
-Original Grad: 0.270, -lr * Pred Grad:  0.108, New P: 0.953
iter 21 loss: 0.128
Actual params: [1.388 , 0.9533]
-Original Grad: -0.056, -lr * Pred Grad:  0.036, New P: 1.424
-Original Grad: 0.197, -lr * Pred Grad:  0.107, New P: 1.060
iter 22 loss: 0.110
Actual params: [1.4243, 1.0599]
-Original Grad: -0.033, -lr * Pred Grad:  0.031, New P: 1.455
-Original Grad: 0.155, -lr * Pred Grad:  0.104, New P: 1.164
iter 23 loss: 0.094
Actual params: [1.4553, 1.1636]
-Original Grad: -0.087, -lr * Pred Grad:  0.023, New P: 1.478
-Original Grad: 0.202, -lr * Pred Grad:  0.103, New P: 1.266
iter 24 loss: 0.079
Actual params: [1.4781, 1.2663]
-Original Grad: 0.001, -lr * Pred Grad:  0.021, New P: 1.499
-Original Grad: 0.090, -lr * Pred Grad:  0.098, New P: 1.364
iter 25 loss: 0.067
Actual params: [1.4988, 1.364 ]
-Original Grad: -0.032, -lr * Pred Grad:  0.017, New P: 1.516
-Original Grad: 0.120, -lr * Pred Grad:  0.094, New P: 1.458
iter 26 loss: 0.057
Actual params: [1.5157, 1.4584]
-Original Grad: -0.017, -lr * Pred Grad:  0.014, New P: 1.530
-Original Grad: 0.059, -lr * Pred Grad:  0.089, New P: 1.547
iter 27 loss: 0.051
Actual params: [1.53  , 1.5473]
-Original Grad: -0.078, -lr * Pred Grad:  0.008, New P: 1.538
-Original Grad: 0.080, -lr * Pred Grad:  0.085, New P: 1.632
iter 28 loss: 0.044
Actual params: [1.5379, 1.6322]
-Original Grad: -0.003, -lr * Pred Grad:  0.007, New P: 1.545
-Original Grad: 0.039, -lr * Pred Grad:  0.079, New P: 1.711
iter 29 loss: 0.053
Actual params: [1.5449, 1.7114]
-Original Grad: 0.070, -lr * Pred Grad:  0.011, New P: 1.556
-Original Grad: -0.254, -lr * Pred Grad:  0.057, New P: 1.769
iter 30 loss: 0.066
Actual params: [1.556 , 1.7689]
-Original Grad: 0.091, -lr * Pred Grad:  0.016, New P: 1.572
-Original Grad: -0.346, -lr * Pred Grad:  0.033, New P: 1.802
Target params: [1.3344, 1.5708]
iter 0 loss: 0.563
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.002, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.563
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.084, New P: -0.657
-Original Grad: 0.000, -lr * Pred Grad:  0.084, New P: 0.187
iter 2 loss: 0.563
Actual params: [-0.6568,  0.1872]
-Original Grad: -0.000, -lr * Pred Grad:  -0.072, New P: -0.729
-Original Grad: 0.000, -lr * Pred Grad:  0.070, New P: 0.258
iter 3 loss: 0.563
Actual params: [-0.7288,  0.2577]
-Original Grad: -0.000, -lr * Pred Grad:  -0.061, New P: -0.789
-Original Grad: 0.000, -lr * Pred Grad:  0.060, New P: 0.318
iter 4 loss: 0.563
Actual params: [-0.7893,  0.3176]
-Original Grad: -0.000, -lr * Pred Grad:  -0.053, New P: -0.842
-Original Grad: 0.000, -lr * Pred Grad:  0.052, New P: 0.369
iter 5 loss: 0.563
Actual params: [-0.8419,  0.3694]
-Original Grad: -0.000, -lr * Pred Grad:  -0.046, New P: -0.888
-Original Grad: 0.000, -lr * Pred Grad:  0.046, New P: 0.415
iter 6 loss: 0.563
Actual params: [-0.8876,  0.415 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -0.928
-Original Grad: 0.000, -lr * Pred Grad:  0.040, New P: 0.455
iter 7 loss: 0.563
Actual params: [-0.9277,  0.4553]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -0.963
-Original Grad: 0.000, -lr * Pred Grad:  0.036, New P: 0.491
iter 8 loss: 0.563
Actual params: [-0.9632,  0.4914]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -0.995
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: 0.524
iter 9 loss: 0.563
Actual params: [-0.9949,  0.5237]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.023
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: 0.553
iter 10 loss: 0.563
Actual params: [-1.0232,  0.5529]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.049
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 0.579
iter 11 loss: 0.563
Actual params: [-1.0486,  0.5793]
-Original Grad: 0.000, -lr * Pred Grad:  -0.023, New P: -1.071
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.603
iter 12 loss: 0.563
Actual params: [-1.0714,  0.6033]
-Original Grad: 0.000, -lr * Pred Grad:  -0.021, New P: -1.092
-Original Grad: 0.000, -lr * Pred Grad:  0.022, New P: 0.625
iter 13 loss: 0.563
Actual params: [-1.0919,  0.6251]
-Original Grad: 0.000, -lr * Pred Grad:  -0.018, New P: -1.110
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.645
iter 14 loss: 0.563
Actual params: [-1.1104,  0.6451]
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -1.127
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.663
iter 15 loss: 0.563
Actual params: [-1.1271,  0.6634]
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -1.142
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.680
iter 16 loss: 0.563
Actual params: [-1.1422,  0.6803]
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -1.156
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.696
iter 17 loss: 0.563
Actual params: [-1.1559,  0.6958]
-Original Grad: 0.000, -lr * Pred Grad:  -0.012, New P: -1.168
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.710
iter 18 loss: 0.563
Actual params: [-1.1683,  0.7101]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.179
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.723
iter 19 loss: 0.563
Actual params: [-1.1794,  0.7234]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.190
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.736
iter 20 loss: 0.563
Actual params: [-1.1896,  0.7357]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.199
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.747
iter 21 loss: 0.563
Actual params: [-1.1987,  0.7473]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.207
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.758
iter 22 loss: 0.563
Actual params: [-1.207 ,  0.7582]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.214
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.768
iter 23 loss: 0.563
Actual params: [-1.2144,  0.7684]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.221
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.778
iter 24 loss: 0.563
Actual params: [-1.2211,  0.778 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.227
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.787
iter 25 loss: 0.563
Actual params: [-1.2272,  0.787 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.233
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.796
iter 26 loss: 0.563
Actual params: [-1.2327,  0.7956]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.238
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.804
iter 27 loss: 0.563
Actual params: [-1.2375,  0.8037]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.242
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.812
iter 28 loss: 0.563
Actual params: [-1.2419,  0.8116]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.246
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.819
iter 29 loss: 0.563
Actual params: [-1.2458,  0.8191]
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -1.249
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.826
iter 30 loss: 0.563
Actual params: [-1.2493,  0.8264]
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -1.252
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.834
Target params: [1.3344, 1.5708]
iter 0 loss: 0.495
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.495
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.001, -lr * Pred Grad:  0.092, New P: -0.281
-Original Grad: 0.001, -lr * Pred Grad:  0.096, New P: 0.199
iter 2 loss: 0.495
Actual params: [-0.2807,  0.1992]
-Original Grad: 0.004, -lr * Pred Grad:  0.080, New P: -0.201
-Original Grad: 0.002, -lr * Pred Grad:  0.087, New P: 0.286
iter 3 loss: 0.494
Actual params: [-0.2009,  0.2861]
-Original Grad: 0.009, -lr * Pred Grad:  0.081, New P: -0.120
-Original Grad: 0.004, -lr * Pred Grad:  0.086, New P: 0.372
iter 4 loss: 0.491
Actual params: [-0.1197,  0.372 ]
-Original Grad: 0.033, -lr * Pred Grad:  0.072, New P: -0.048
-Original Grad: 0.014, -lr * Pred Grad:  0.074, New P: 0.446
iter 5 loss: 0.487
Actual params: [-0.0476,  0.4462]
-Original Grad: 0.062, -lr * Pred Grad:  0.076, New P: 0.028
-Original Grad: 0.027, -lr * Pred Grad:  0.076, New P: 0.522
iter 6 loss: 0.476
Actual params: [0.0283, 0.5224]
-Original Grad: 0.140, -lr * Pred Grad:  0.075, New P: 0.103
-Original Grad: 0.063, -lr * Pred Grad:  0.074, New P: 0.597
iter 7 loss: 0.457
Actual params: [0.1033, 0.5968]
-Original Grad: 0.217, -lr * Pred Grad:  0.079, New P: 0.182
-Original Grad: 0.106, -lr * Pred Grad:  0.077, New P: 0.674
iter 8 loss: 0.429
Actual params: [0.1823, 0.6743]
-Original Grad: 0.288, -lr * Pred Grad:  0.084, New P: 0.266
-Original Grad: 0.151, -lr * Pred Grad:  0.082, New P: 0.756
iter 9 loss: 0.393
Actual params: [0.2659, 0.7559]
-Original Grad: 0.304, -lr * Pred Grad:  0.089, New P: 0.354
-Original Grad: 0.156, -lr * Pred Grad:  0.087, New P: 0.843
iter 10 loss: 0.342
Actual params: [0.3545, 0.8431]
-Original Grad: 0.475, -lr * Pred Grad:  0.091, New P: 0.445
-Original Grad: 0.252, -lr * Pred Grad:  0.089, New P: 0.932
iter 11 loss: 0.274
Actual params: [0.4452, 0.9325]
-Original Grad: 0.460, -lr * Pred Grad:  0.095, New P: 0.540
-Original Grad: 0.245, -lr * Pred Grad:  0.093, New P: 1.026
iter 12 loss: 0.205
Actual params: [0.5398, 1.0259]
-Original Grad: 0.458, -lr * Pred Grad:  0.098, New P: 0.638
-Original Grad: 0.141, -lr * Pred Grad:  0.095, New P: 1.121
iter 13 loss: 0.144
Actual params: [0.6377, 1.121 ]
-Original Grad: 0.416, -lr * Pred Grad:  0.100, New P: 0.738
-Original Grad: 0.121, -lr * Pred Grad:  0.095, New P: 1.216
iter 14 loss: 0.093
Actual params: [0.7381, 1.2164]
-Original Grad: 0.342, -lr * Pred Grad:  0.102, New P: 0.840
-Original Grad: 0.090, -lr * Pred Grad:  0.094, New P: 1.310
iter 15 loss: 0.060
Actual params: [0.8397, 1.3104]
-Original Grad: 0.120, -lr * Pred Grad:  0.097, New P: 0.937
-Original Grad: 0.101, -lr * Pred Grad:  0.093, New P: 1.404
iter 16 loss: 0.036
Actual params: [0.9366, 1.4038]
-Original Grad: 0.063, -lr * Pred Grad:  0.091, New P: 1.027
-Original Grad: 0.118, -lr * Pred Grad:  0.094, New P: 1.498
iter 17 loss: 0.023
Actual params: [1.0272, 1.4978]
-Original Grad: 0.028, -lr * Pred Grad:  0.083, New P: 1.111
-Original Grad: 0.083, -lr * Pred Grad:  0.092, New P: 1.590
iter 18 loss: 0.015
Actual params: [1.1107, 1.5902]
-Original Grad: -0.021, -lr * Pred Grad:  0.075, New P: 1.186
-Original Grad: 0.051, -lr * Pred Grad:  0.089, New P: 1.679
iter 19 loss: 0.013
Actual params: [1.1856, 1.6787]
-Original Grad: -0.109, -lr * Pred Grad:  0.063, New P: 1.248
-Original Grad: 0.112, -lr * Pred Grad:  0.090, New P: 1.768
iter 20 loss: 0.007
Actual params: [1.2483, 1.7682]
-Original Grad: -0.054, -lr * Pred Grad:  0.054, New P: 1.303
-Original Grad: 0.078, -lr * Pred Grad:  0.088, New P: 1.856
iter 21 loss: 0.005
Actual params: [1.3027, 1.8563]
-Original Grad: -0.005, -lr * Pred Grad:  0.049, New P: 1.352
-Original Grad: 0.008, -lr * Pred Grad:  0.081, New P: 1.937
iter 22 loss: 0.006
Actual params: [1.3521, 1.9372]
-Original Grad: 0.030, -lr * Pred Grad:  0.046, New P: 1.398
-Original Grad: -0.061, -lr * Pred Grad:  0.067, New P: 2.004
iter 23 loss: 0.009
Actual params: [1.3984, 2.0045]
-Original Grad: 0.047, -lr * Pred Grad:  0.044, New P: 1.443
-Original Grad: -0.086, -lr * Pred Grad:  0.052, New P: 2.056
iter 24 loss: 0.011
Actual params: [1.4428, 2.0564]
-Original Grad: 0.057, -lr * Pred Grad:  0.043, New P: 1.486
-Original Grad: -0.093, -lr * Pred Grad:  0.038, New P: 2.094
iter 25 loss: 0.012
Actual params: [1.486, 2.094]
-Original Grad: 0.030, -lr * Pred Grad:  0.041, New P: 1.527
-Original Grad: -0.049, -lr * Pred Grad:  0.029, New P: 2.123
iter 26 loss: 0.013
Actual params: [1.5268, 2.1232]
-Original Grad: 0.025, -lr * Pred Grad:  0.038, New P: 1.565
-Original Grad: -0.075, -lr * Pred Grad:  0.019, New P: 2.142
iter 27 loss: 0.014
Actual params: [1.5652, 2.1421]
-Original Grad: 0.030, -lr * Pred Grad:  0.037, New P: 1.602
-Original Grad: -0.086, -lr * Pred Grad:  0.008, New P: 2.151
iter 28 loss: 0.014
Actual params: [1.6017, 2.1506]
-Original Grad: 0.023, -lr * Pred Grad:  0.034, New P: 1.636
-Original Grad: -0.089, -lr * Pred Grad:  -0.001, New P: 2.149
iter 29 loss: 0.013
Actual params: [1.6361, 2.1493]
-Original Grad: 0.018, -lr * Pred Grad:  0.032, New P: 1.668
-Original Grad: -0.078, -lr * Pred Grad:  -0.009, New P: 2.140
iter 30 loss: 0.012
Actual params: [1.6685, 2.1404]
-Original Grad: -0.014, -lr * Pred Grad:  0.029, New P: 1.697
-Original Grad: -0.025, -lr * Pred Grad:  -0.011, New P: 2.130
Target params: [1.3344, 1.5708]
iter 0 loss: 0.118
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.052, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.005, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.112
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.041, -lr * Pred Grad:  -0.099, New P: -0.671
-Original Grad: -0.003, -lr * Pred Grad:  -0.098, New P: -0.194
iter 2 loss: 0.109
Actual params: [-0.6711, -0.1944]
-Original Grad: -0.020, -lr * Pred Grad:  -0.092, New P: -0.763
-Original Grad: -0.001, -lr * Pred Grad:  -0.083, New P: -0.277
iter 3 loss: 0.107
Actual params: [-0.7628, -0.2771]
-Original Grad: -0.013, -lr * Pred Grad:  -0.085, New P: -0.847
-Original Grad: -0.000, -lr * Pred Grad:  -0.070, New P: -0.347
iter 4 loss: 0.107
Actual params: [-0.8474, -0.3466]
-Original Grad: -0.006, -lr * Pred Grad:  -0.076, New P: -0.923
-Original Grad: 0.000, -lr * Pred Grad:  -0.058, New P: -0.405
iter 5 loss: 0.106
Actual params: [-0.9233, -0.405 ]
-Original Grad: -0.004, -lr * Pred Grad:  -0.068, New P: -0.992
-Original Grad: 0.001, -lr * Pred Grad:  -0.044, New P: -0.449
iter 6 loss: 0.106
Actual params: [-0.9916, -0.4493]
-Original Grad: -0.003, -lr * Pred Grad:  -0.062, New P: -1.053
-Original Grad: 0.001, -lr * Pred Grad:  -0.031, New P: -0.481
iter 7 loss: 0.106
Actual params: [-1.0533, -0.4806]
-Original Grad: -0.002, -lr * Pred Grad:  -0.056, New P: -1.109
-Original Grad: 0.001, -lr * Pred Grad:  -0.021, New P: -0.502
iter 8 loss: 0.106
Actual params: [-1.1093, -0.5017]
-Original Grad: -0.001, -lr * Pred Grad:  -0.051, New P: -1.160
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -0.517
iter 9 loss: 0.106
Actual params: [-1.1599, -0.5165]
-Original Grad: -0.001, -lr * Pred Grad:  -0.046, New P: -1.206
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -0.526
iter 10 loss: 0.106
Actual params: [-1.2059, -0.5258]
-Original Grad: -0.001, -lr * Pred Grad:  -0.042, New P: -1.248
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -0.531
iter 11 loss: 0.106
Actual params: [-1.2477, -0.5313]
-Original Grad: -0.001, -lr * Pred Grad:  -0.038, New P: -1.286
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -0.534
iter 12 loss: 0.106
Actual params: [-1.2858, -0.5339]
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -1.320
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -0.535
iter 13 loss: 0.106
Actual params: [-1.3205, -0.5347]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -1.352
-Original Grad: 0.000, -lr * Pred Grad:  0.001, New P: -0.534
iter 14 loss: 0.106
Actual params: [-1.3521, -0.5336]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.381
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.531
iter 15 loss: 0.106
Actual params: [-1.381 , -0.5314]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.407
-Original Grad: 0.000, -lr * Pred Grad:  0.003, New P: -0.528
iter 16 loss: 0.106
Actual params: [-1.4074, -0.5284]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.431
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.525
iter 17 loss: 0.106
Actual params: [-1.4315, -0.5247]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.453
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.521
iter 18 loss: 0.106
Actual params: [-1.4534, -0.5207]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.474
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.516
iter 19 loss: 0.106
Actual params: [-1.4736, -0.5163]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.492
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.512
iter 20 loss: 0.106
Actual params: [-1.492 , -0.5118]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.509
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.507
iter 21 loss: 0.106
Actual params: [-1.5088, -0.5073]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.524
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.503
iter 22 loss: 0.106
Actual params: [-1.5242, -0.5027]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.538
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: -0.498
iter 23 loss: 0.106
Actual params: [-1.5382, -0.4982]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.551
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.494
iter 24 loss: 0.106
Actual params: [-1.5511, -0.4937]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.563
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.489
iter 25 loss: 0.106
Actual params: [-1.5629, -0.4893]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.574
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.485
iter 26 loss: 0.106
Actual params: [-1.5737, -0.485 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.584
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.481
iter 27 loss: 0.106
Actual params: [-1.5836, -0.4807]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.593
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.477
iter 28 loss: 0.106
Actual params: [-1.5927, -0.4765]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.601
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.473
iter 29 loss: 0.106
Actual params: [-1.601 , -0.4725]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.609
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.469
iter 30 loss: 0.106
Actual params: [-1.6086, -0.4687]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.616
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: -0.465
Target params: [1.3344, 1.5708]
iter 0 loss: 0.583
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.582
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.012, -lr * Pred Grad:  0.078, New P: -0.294
-Original Grad: 0.010, -lr * Pred Grad:  0.081, New P: 0.185
iter 2 loss: 0.580
Actual params: [-0.2939,  0.185 ]
-Original Grad: 0.022, -lr * Pred Grad:  0.085, New P: -0.209
-Original Grad: 0.022, -lr * Pred Grad:  0.085, New P: 0.270
iter 3 loss: 0.572
Actual params: [-0.209 ,  0.2696]
-Original Grad: 0.063, -lr * Pred Grad:  0.080, New P: -0.129
-Original Grad: 0.056, -lr * Pred Grad:  0.081, New P: 0.350
iter 4 loss: 0.556
Actual params: [-0.1293,  0.3502]
-Original Grad: 0.147, -lr * Pred Grad:  0.078, New P: -0.052
-Original Grad: 0.132, -lr * Pred Grad:  0.078, New P: 0.428
iter 5 loss: 0.533
Actual params: [-0.0516,  0.4282]
-Original Grad: 0.153, -lr * Pred Grad:  0.085, New P: 0.033
-Original Grad: 0.178, -lr * Pred Grad:  0.083, New P: 0.511
iter 6 loss: 0.507
Actual params: [0.033 , 0.5111]
-Original Grad: 0.120, -lr * Pred Grad:  0.089, New P: 0.122
-Original Grad: 0.152, -lr * Pred Grad:  0.088, New P: 0.599
iter 7 loss: 0.472
Actual params: [0.1219, 0.5994]
-Original Grad: 0.211, -lr * Pred Grad:  0.092, New P: 0.214
-Original Grad: 0.261, -lr * Pred Grad:  0.091, New P: 0.690
iter 8 loss: 0.430
Actual params: [0.2139, 0.69  ]
-Original Grad: 0.272, -lr * Pred Grad:  0.094, New P: 0.308
-Original Grad: 0.318, -lr * Pred Grad:  0.093, New P: 0.783
iter 9 loss: 0.380
Actual params: [0.308 , 0.7831]
-Original Grad: 0.304, -lr * Pred Grad:  0.096, New P: 0.405
-Original Grad: 0.272, -lr * Pred Grad:  0.096, New P: 0.879
iter 10 loss: 0.322
Actual params: [0.4045, 0.8792]
-Original Grad: 0.377, -lr * Pred Grad:  0.098, New P: 0.503
-Original Grad: 0.401, -lr * Pred Grad:  0.098, New P: 0.977
iter 11 loss: 0.256
Actual params: [0.5029, 0.9774]
-Original Grad: 0.356, -lr * Pred Grad:  0.101, New P: 0.604
-Original Grad: 0.357, -lr * Pred Grad:  0.100, New P: 1.078
iter 12 loss: 0.183
Actual params: [0.6036, 1.0779]
-Original Grad: 0.240, -lr * Pred Grad:  0.101, New P: 0.705
-Original Grad: 0.322, -lr * Pred Grad:  0.102, New P: 1.180
iter 13 loss: 0.120
Actual params: [0.7048, 1.18  ]
-Original Grad: 0.232, -lr * Pred Grad:  0.102, New P: 0.806
-Original Grad: 0.291, -lr * Pred Grad:  0.103, New P: 1.283
iter 14 loss: 0.076
Actual params: [0.8063, 1.283 ]
-Original Grad: 0.107, -lr * Pred Grad:  0.097, New P: 0.904
-Original Grad: 0.217, -lr * Pred Grad:  0.102, New P: 1.385
iter 15 loss: 0.048
Actual params: [0.9038, 1.3851]
-Original Grad: 0.046, -lr * Pred Grad:  0.091, New P: 0.995
-Original Grad: 0.201, -lr * Pred Grad:  0.101, New P: 1.486
iter 16 loss: 0.032
Actual params: [0.9947, 1.486 ]
-Original Grad: -0.009, -lr * Pred Grad:  0.082, New P: 1.077
-Original Grad: 0.100, -lr * Pred Grad:  0.096, New P: 1.582
iter 17 loss: 0.022
Actual params: [1.0767, 1.5822]
-Original Grad: -0.016, -lr * Pred Grad:  0.073, New P: 1.150
-Original Grad: 0.088, -lr * Pred Grad:  0.092, New P: 1.674
iter 18 loss: 0.016
Actual params: [1.1501, 1.6738]
-Original Grad: -0.009, -lr * Pred Grad:  0.066, New P: 1.216
-Original Grad: 0.083, -lr * Pred Grad:  0.087, New P: 1.761
iter 19 loss: 0.012
Actual params: [1.2163, 1.7611]
-Original Grad: -0.001, -lr * Pred Grad:  0.060, New P: 1.276
-Original Grad: 0.028, -lr * Pred Grad:  0.081, New P: 1.842
iter 20 loss: 0.009
Actual params: [1.2764, 1.842 ]
-Original Grad: -0.003, -lr * Pred Grad:  0.055, New P: 1.331
-Original Grad: 0.035, -lr * Pred Grad:  0.075, New P: 1.917
iter 21 loss: 0.008
Actual params: [1.331 , 1.9174]
-Original Grad: 0.009, -lr * Pred Grad:  0.050, New P: 1.381
-Original Grad: -0.002, -lr * Pred Grad:  0.069, New P: 1.986
iter 22 loss: 0.009
Actual params: [1.3812, 1.9859]
-Original Grad: 0.002, -lr * Pred Grad:  0.046, New P: 1.427
-Original Grad: -0.030, -lr * Pred Grad:  0.061, New P: 2.047
iter 23 loss: 0.011
Actual params: [1.4271, 2.0466]
-Original Grad: 0.001, -lr * Pred Grad:  0.042, New P: 1.469
-Original Grad: -0.048, -lr * Pred Grad:  0.053, New P: 2.099
iter 24 loss: 0.015
Actual params: [1.469 , 2.0991]
-Original Grad: -0.008, -lr * Pred Grad:  0.038, New P: 1.507
-Original Grad: -0.049, -lr * Pred Grad:  0.045, New P: 2.144
iter 25 loss: 0.019
Actual params: [1.5066, 2.1442]
-Original Grad: -0.001, -lr * Pred Grad:  0.034, New P: 1.541
-Original Grad: -0.067, -lr * Pred Grad:  0.037, New P: 2.181
iter 26 loss: 0.023
Actual params: [1.5407, 2.1813]
-Original Grad: 0.020, -lr * Pred Grad:  0.032, New P: 1.573
-Original Grad: -0.118, -lr * Pred Grad:  0.027, New P: 2.208
iter 27 loss: 0.026
Actual params: [1.5732, 2.2082]
-Original Grad: 0.014, -lr * Pred Grad:  0.031, New P: 1.604
-Original Grad: -0.192, -lr * Pred Grad:  0.013, New P: 2.221
iter 28 loss: 0.027
Actual params: [1.6037, 2.2212]
-Original Grad: 0.005, -lr * Pred Grad:  0.028, New P: 1.632
-Original Grad: -0.108, -lr * Pred Grad:  0.006, New P: 2.227
iter 29 loss: 0.028
Actual params: [1.6319, 2.2269]
-Original Grad: 0.006, -lr * Pred Grad:  0.026, New P: 1.658
-Original Grad: -0.142, -lr * Pred Grad:  -0.003, New P: 2.224
iter 30 loss: 0.028
Actual params: [1.658 , 2.2239]
-Original Grad: -0.010, -lr * Pred Grad:  0.023, New P: 1.681
-Original Grad: -0.176, -lr * Pred Grad:  -0.013, New P: 2.211
Target params: [1.3344, 1.5708]
iter 0 loss: 0.267
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.012, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.006, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.266
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.003, -lr * Pred Grad:  -0.083, New P: -0.656
-Original Grad: -0.002, -lr * Pred Grad:  -0.085, New P: -0.182
iter 2 loss: 0.266
Actual params: [-0.6558, -0.1817]
-Original Grad: -0.001, -lr * Pred Grad:  -0.069, New P: -0.724
-Original Grad: -0.000, -lr * Pred Grad:  -0.069, New P: -0.251
iter 3 loss: 0.266
Actual params: [-0.7244, -0.2511]
-Original Grad: -0.001, -lr * Pred Grad:  -0.059, New P: -0.783
-Original Grad: -0.000, -lr * Pred Grad:  -0.059, New P: -0.310
iter 4 loss: 0.266
Actual params: [-0.7832, -0.31  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.051, New P: -0.834
-Original Grad: -0.000, -lr * Pred Grad:  -0.051, New P: -0.361
iter 5 loss: 0.266
Actual params: [-0.8344, -0.3614]
-Original Grad: -0.000, -lr * Pred Grad:  -0.045, New P: -0.880
-Original Grad: -0.000, -lr * Pred Grad:  -0.045, New P: -0.406
iter 6 loss: 0.266
Actual params: [-0.8795, -0.4063]
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -0.920
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -0.446
iter 7 loss: 0.266
Actual params: [-0.9196, -0.446 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -0.955
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -0.481
iter 8 loss: 0.266
Actual params: [-0.9554, -0.4814]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -0.987
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -0.513
iter 9 loss: 0.266
Actual params: [-0.9875, -0.5129]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.016
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -0.541
iter 10 loss: 0.266
Actual params: [-1.0163, -0.5411]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.042
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -0.567
iter 11 loss: 0.266
Actual params: [-1.0423, -0.5665]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.066
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -0.589
iter 12 loss: 0.266
Actual params: [-1.0658, -0.5894]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.087
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -0.610
iter 13 loss: 0.266
Actual params: [-1.0872, -0.6102]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.107
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -0.629
iter 14 loss: 0.266
Actual params: [-1.1066, -0.6289]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.124
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.646
iter 15 loss: 0.266
Actual params: [-1.1242, -0.646 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.140
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.661
iter 16 loss: 0.266
Actual params: [-1.1402, -0.6614]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.155
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.675
iter 17 loss: 0.266
Actual params: [-1.1548, -0.6754]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.168
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.688
iter 18 loss: 0.266
Actual params: [-1.1681, -0.6882]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.180
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.700
iter 19 loss: 0.266
Actual params: [-1.1803, -0.6998]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.191
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.710
iter 20 loss: 0.266
Actual params: [-1.1914, -0.7103]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.202
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.720
iter 21 loss: 0.266
Actual params: [-1.2016, -0.72  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.211
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.729
iter 22 loss: 0.266
Actual params: [-1.2108, -0.7287]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.219
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.737
iter 23 loss: 0.266
Actual params: [-1.2193, -0.7367]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.227
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.744
iter 24 loss: 0.266
Actual params: [-1.2271, -0.744 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.234
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.751
iter 25 loss: 0.266
Actual params: [-1.2342, -0.7507]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.241
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.757
iter 26 loss: 0.266
Actual params: [-1.2407, -0.7567]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.247
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.762
iter 27 loss: 0.266
Actual params: [-1.2467, -0.7622]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.252
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.767
iter 28 loss: 0.266
Actual params: [-1.2521, -0.7673]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.257
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.772
iter 29 loss: 0.266
Actual params: [-1.2571, -0.7719]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.262
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.776
iter 30 loss: 0.266
Actual params: [-1.2617, -0.7761]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.266
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.780
Target params: [1.3344, 1.5708]
iter 0 loss: 0.048
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.051, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.031, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.045
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.009, -lr * Pred Grad:  -0.079, New P: -0.651
-Original Grad: 0.005, -lr * Pred Grad:  0.079, New P: 0.182
iter 2 loss: 0.044
Actual params: [-0.6512,  0.1823]
-Original Grad: -0.007, -lr * Pred Grad:  -0.069, New P: -0.720
-Original Grad: 0.003, -lr * Pred Grad:  0.067, New P: 0.249
iter 3 loss: 0.043
Actual params: [-0.7199,  0.2492]
-Original Grad: -0.002, -lr * Pred Grad:  -0.058, New P: -0.778
-Original Grad: 0.001, -lr * Pred Grad:  0.057, New P: 0.306
iter 4 loss: 0.043
Actual params: [-0.7784,  0.3062]
-Original Grad: -0.003, -lr * Pred Grad:  -0.053, New P: -0.831
-Original Grad: 0.001, -lr * Pred Grad:  0.050, New P: 0.357
iter 5 loss: 0.043
Actual params: [-0.8312,  0.3567]
-Original Grad: -0.002, -lr * Pred Grad:  -0.047, New P: -0.879
-Original Grad: 0.001, -lr * Pred Grad:  0.045, New P: 0.402
iter 6 loss: 0.043
Actual params: [-0.8787,  0.4017]
-Original Grad: -0.001, -lr * Pred Grad:  -0.042, New P: -0.921
-Original Grad: 0.001, -lr * Pred Grad:  0.040, New P: 0.442
iter 7 loss: 0.043
Actual params: [-0.9209,  0.4419]
-Original Grad: -0.001, -lr * Pred Grad:  -0.038, New P: -0.959
-Original Grad: 0.000, -lr * Pred Grad:  0.036, New P: 0.478
iter 8 loss: 0.043
Actual params: [-0.9589,  0.4781]
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -0.993
-Original Grad: 0.000, -lr * Pred Grad:  0.033, New P: 0.511
iter 9 loss: 0.043
Actual params: [-0.9932,  0.5106]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.024
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: 0.540
iter 10 loss: 0.043
Actual params: [-1.0242,  0.5401]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.052
-Original Grad: 0.000, -lr * Pred Grad:  0.027, New P: 0.567
iter 11 loss: 0.043
Actual params: [-1.0522,  0.5668]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.078
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.591
iter 12 loss: 0.043
Actual params: [-1.0776,  0.5911]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.101
-Original Grad: 0.000, -lr * Pred Grad:  0.022, New P: 0.613
iter 13 loss: 0.043
Actual params: [-1.1007,  0.6131]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.122
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.633
iter 14 loss: 0.043
Actual params: [-1.1217,  0.6333]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.141
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.652
iter 15 loss: 0.043
Actual params: [-1.1408,  0.6517]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.158
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.669
iter 16 loss: 0.043
Actual params: [-1.1582,  0.6685]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.174
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.684
iter 17 loss: 0.043
Actual params: [-1.174 ,  0.6839]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.188
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.698
iter 18 loss: 0.043
Actual params: [-1.1884,  0.6979]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.202
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.711
iter 19 loss: 0.043
Actual params: [-1.2016,  0.7109]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.214
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.723
iter 20 loss: 0.043
Actual params: [-1.2136,  0.7227]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.225
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.733
iter 21 loss: 0.043
Actual params: [-1.2246,  0.7335]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.235
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.743
iter 22 loss: 0.043
Actual params: [-1.2346,  0.7434]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.244
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.752
iter 23 loss: 0.043
Actual params: [-1.2437,  0.7525]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.252
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.761
iter 24 loss: 0.043
Actual params: [-1.252 ,  0.7608]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.260
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.768
iter 25 loss: 0.043
Actual params: [-1.2596,  0.7684]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.267
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.775
iter 26 loss: 0.043
Actual params: [-1.2666,  0.7755]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.273
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.782
iter 27 loss: 0.043
Actual params: [-1.2729,  0.7819]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.279
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.788
iter 28 loss: 0.043
Actual params: [-1.2787,  0.7879]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.284
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.793
iter 29 loss: 0.043
Actual params: [-1.284 ,  0.7933]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.289
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.798
iter 30 loss: 0.043
Actual params: [-1.2889,  0.7983]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.293
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.803
Target params: [1.3344, 1.5708]
iter 0 loss: 0.548
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.002, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.548
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.091, New P: -0.664
-Original Grad: -0.000, -lr * Pred Grad:  0.063, New P: 0.166
iter 2 loss: 0.548
Actual params: [-0.6637,  0.1661]
-Original Grad: -0.000, -lr * Pred Grad:  -0.077, New P: -0.740
-Original Grad: 0.000, -lr * Pred Grad:  0.066, New P: 0.232
iter 3 loss: 0.548
Actual params: [-0.7403,  0.2319]
-Original Grad: -0.000, -lr * Pred Grad:  -0.066, New P: -0.806
-Original Grad: 0.000, -lr * Pred Grad:  0.060, New P: 0.292
iter 4 loss: 0.548
Actual params: [-0.8065,  0.2916]
-Original Grad: -0.000, -lr * Pred Grad:  -0.059, New P: -0.865
-Original Grad: -0.000, -lr * Pred Grad:  0.033, New P: 0.325
iter 5 loss: 0.548
Actual params: [-0.8655,  0.3247]
-Original Grad: -0.000, -lr * Pred Grad:  -0.054, New P: -0.919
-Original Grad: -0.000, -lr * Pred Grad:  0.015, New P: 0.340
iter 6 loss: 0.548
Actual params: [-0.919 ,  0.3402]
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -0.966
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 0.366
iter 7 loss: 0.548
Actual params: [-0.9662,  0.3659]
-Original Grad: -0.000, -lr * Pred Grad:  -0.042, New P: -1.008
-Original Grad: 0.000, -lr * Pred Grad:  0.031, New P: 0.397
iter 8 loss: 0.548
Actual params: [-1.0083,  0.3966]
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -1.046
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: 0.429
iter 9 loss: 0.548
Actual params: [-1.0463,  0.4291]
-Original Grad: 0.000, -lr * Pred Grad:  -0.034, New P: -1.080
-Original Grad: 0.000, -lr * Pred Grad:  0.049, New P: 0.478
iter 10 loss: 0.548
Actual params: [-1.0799,  0.4779]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.110
-Original Grad: 0.000, -lr * Pred Grad:  0.051, New P: 0.529
iter 11 loss: 0.548
Actual params: [-1.1104,  0.5292]
-Original Grad: 0.000, -lr * Pred Grad:  -0.027, New P: -1.138
-Original Grad: 0.000, -lr * Pred Grad:  0.063, New P: 0.592
iter 12 loss: 0.548
Actual params: [-1.1375,  0.5918]
-Original Grad: 0.000, -lr * Pred Grad:  -0.024, New P: -1.162
-Original Grad: 0.000, -lr * Pred Grad:  0.073, New P: 0.665
iter 13 loss: 0.548
Actual params: [-1.1616,  0.6646]
-Original Grad: 0.000, -lr * Pred Grad:  -0.019, New P: -1.181
-Original Grad: 0.000, -lr * Pred Grad:  0.080, New P: 0.745
iter 14 loss: 0.548
Actual params: [-1.1808,  0.7449]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -1.194
-Original Grad: 0.000, -lr * Pred Grad:  0.084, New P: 0.829
iter 15 loss: 0.548
Actual params: [-1.1942,  0.8289]
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -1.198
-Original Grad: 0.000, -lr * Pred Grad:  0.083, New P: 0.912
iter 16 loss: 0.547
Actual params: [-1.1976,  0.9123]
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: -1.192
-Original Grad: 0.000, -lr * Pred Grad:  0.089, New P: 1.002
iter 17 loss: 0.547
Actual params: [-1.1916,  1.0018]
-Original Grad: 0.001, -lr * Pred Grad:  0.029, New P: -1.162
-Original Grad: 0.001, -lr * Pred Grad:  0.083, New P: 1.084
iter 18 loss: 0.547
Actual params: [-1.1621,  1.0843]
-Original Grad: 0.002, -lr * Pred Grad:  0.053, New P: -1.109
-Original Grad: 0.002, -lr * Pred Grad:  0.084, New P: 1.168
iter 19 loss: 0.547
Actual params: [-1.1093,  1.1684]
-Original Grad: 0.006, -lr * Pred Grad:  0.067, New P: -1.042
-Original Grad: 0.006, -lr * Pred Grad:  0.078, New P: 1.247
iter 20 loss: 0.545
Actual params: [-1.042 ,  1.2466]
-Original Grad: 0.013, -lr * Pred Grad:  0.074, New P: -0.968
-Original Grad: 0.012, -lr * Pred Grad:  0.078, New P: 1.325
iter 21 loss: 0.542
Actual params: [-0.9685,  1.3249]
-Original Grad: 0.035, -lr * Pred Grad:  0.074, New P: -0.895
-Original Grad: 0.030, -lr * Pred Grad:  0.077, New P: 1.402
iter 22 loss: 0.536
Actual params: [-0.8948,  1.402 ]
-Original Grad: 0.061, -lr * Pred Grad:  0.080, New P: -0.815
-Original Grad: 0.049, -lr * Pred Grad:  0.083, New P: 1.485
iter 23 loss: 0.524
Actual params: [-0.8151,  1.4847]
-Original Grad: 0.132, -lr * Pred Grad:  0.081, New P: -0.734
-Original Grad: 0.093, -lr * Pred Grad:  0.085, New P: 1.570
iter 24 loss: 0.503
Actual params: [-0.734 ,  1.5697]
-Original Grad: 0.183, -lr * Pred Grad:  0.088, New P: -0.646
-Original Grad: 0.098, -lr * Pred Grad:  0.093, New P: 1.663
iter 25 loss: 0.481
Actual params: [-0.6457,  1.6631]
-Original Grad: 0.203, -lr * Pred Grad:  0.096, New P: -0.549
-Original Grad: 0.078, -lr * Pred Grad:  0.101, New P: 1.764
iter 26 loss: 0.455
Actual params: [-0.5494,  1.7636]
-Original Grad: 0.211, -lr * Pred Grad:  0.103, New P: -0.446
-Original Grad: 0.019, -lr * Pred Grad:  0.097, New P: 1.861
iter 27 loss: 0.429
Actual params: [-0.446 ,  1.8608]
-Original Grad: 0.245, -lr * Pred Grad:  0.109, New P: -0.337
-Original Grad: 0.005, -lr * Pred Grad:  0.090, New P: 1.951
iter 28 loss: 0.399
Actual params: [-0.3367,  1.951 ]
-Original Grad: 0.194, -lr * Pred Grad:  0.114, New P: -0.223
-Original Grad: -0.021, -lr * Pred Grad:  0.075, New P: 2.026
iter 29 loss: 0.363
Actual params: [-0.2231,  2.0255]
-Original Grad: 0.357, -lr * Pred Grad:  0.117, New P: -0.106
-Original Grad: -0.058, -lr * Pred Grad:  0.046, New P: 2.071
iter 30 loss: 0.321
Actual params: [-0.1061,  2.0712]
-Original Grad: 0.378, -lr * Pred Grad:  0.121, New P: 0.015
-Original Grad: -0.134, -lr * Pred Grad:  -0.001, New P: 2.070
Target params: [1.3344, 1.5708]
iter 0 loss: 0.498
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.041, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.495
Actual params: [-0.5723,  0.1035]
-Original Grad: 0.012, -lr * Pred Grad:  0.071, New P: -0.501
-Original Grad: 0.043, -lr * Pred Grad:  0.100, New P: 0.204
iter 2 loss: 0.486
Actual params: [-0.5012,  0.2036]
-Original Grad: 0.033, -lr * Pred Grad:  0.079, New P: -0.422
-Original Grad: 0.084, -lr * Pred Grad:  0.097, New P: 0.300
iter 3 loss: 0.474
Actual params: [-0.4222,  0.3003]
-Original Grad: 0.057, -lr * Pred Grad:  0.084, New P: -0.339
-Original Grad: 0.119, -lr * Pred Grad:  0.096, New P: 0.396
iter 4 loss: 0.458
Actual params: [-0.3386,  0.3961]
-Original Grad: 0.070, -lr * Pred Grad:  0.088, New P: -0.250
-Original Grad: 0.138, -lr * Pred Grad:  0.097, New P: 0.493
iter 5 loss: 0.438
Actual params: [-0.2504,  0.4929]
-Original Grad: 0.078, -lr * Pred Grad:  0.092, New P: -0.158
-Original Grad: 0.131, -lr * Pred Grad:  0.098, New P: 0.591
iter 6 loss: 0.413
Actual params: [-0.1584,  0.5914]
-Original Grad: 0.121, -lr * Pred Grad:  0.093, New P: -0.066
-Original Grad: 0.167, -lr * Pred Grad:  0.100, New P: 0.691
iter 7 loss: 0.382
Actual params: [-0.0655,  0.691 ]
-Original Grad: 0.197, -lr * Pred Grad:  0.091, New P: 0.026
-Original Grad: 0.203, -lr * Pred Grad:  0.100, New P: 0.791
iter 8 loss: 0.344
Actual params: [0.0258, 0.7914]
-Original Grad: 0.246, -lr * Pred Grad:  0.093, New P: 0.118
-Original Grad: 0.229, -lr * Pred Grad:  0.101, New P: 0.893
iter 9 loss: 0.300
Actual params: [0.1184, 0.8929]
-Original Grad: 0.190, -lr * Pred Grad:  0.096, New P: 0.214
-Original Grad: 0.158, -lr * Pred Grad:  0.102, New P: 0.995
iter 10 loss: 0.254
Actual params: [0.2141, 0.9947]
-Original Grad: 0.292, -lr * Pred Grad:  0.098, New P: 0.312
-Original Grad: 0.186, -lr * Pred Grad:  0.103, New P: 1.097
iter 11 loss: 0.207
Actual params: [0.3119, 1.0973]
-Original Grad: 0.302, -lr * Pred Grad:  0.100, New P: 0.412
-Original Grad: 0.172, -lr * Pred Grad:  0.103, New P: 1.200
iter 12 loss: 0.162
Actual params: [0.412 , 1.2003]
-Original Grad: 0.312, -lr * Pred Grad:  0.102, New P: 0.514
-Original Grad: 0.185, -lr * Pred Grad:  0.104, New P: 1.304
iter 13 loss: 0.112
Actual params: [0.5143, 1.304 ]
-Original Grad: 0.212, -lr * Pred Grad:  0.103, New P: 0.617
-Original Grad: 0.192, -lr * Pred Grad:  0.104, New P: 1.408
iter 14 loss: 0.068
Actual params: [0.6171, 1.4083]
-Original Grad: 0.278, -lr * Pred Grad:  0.104, New P: 0.722
-Original Grad: 0.193, -lr * Pred Grad:  0.105, New P: 1.513
iter 15 loss: 0.033
Actual params: [0.7216, 1.5132]
-Original Grad: 0.107, -lr * Pred Grad:  0.101, New P: 0.822
-Original Grad: 0.092, -lr * Pred Grad:  0.101, New P: 1.614
iter 16 loss: 0.012
Actual params: [0.8222, 1.6145]
-Original Grad: 0.027, -lr * Pred Grad:  0.093, New P: 0.915
-Original Grad: 0.029, -lr * Pred Grad:  0.094, New P: 1.709
iter 17 loss: 0.006
Actual params: [0.9152, 1.7085]
-Original Grad: 0.007, -lr * Pred Grad:  0.085, New P: 1.000
-Original Grad: -0.012, -lr * Pred Grad:  0.084, New P: 1.793
iter 18 loss: 0.010
Actual params: [1.0001, 1.793 ]
-Original Grad: -0.036, -lr * Pred Grad:  0.075, New P: 1.075
-Original Grad: -0.069, -lr * Pred Grad:  0.071, New P: 1.864
iter 19 loss: 0.017
Actual params: [1.0749, 1.8638]
-Original Grad: -0.008, -lr * Pred Grad:  0.067, New P: 1.142
-Original Grad: -0.079, -lr * Pred Grad:  0.057, New P: 1.921
iter 20 loss: 0.024
Actual params: [1.1423, 1.9212]
-Original Grad: -0.003, -lr * Pred Grad:  0.061, New P: 1.203
-Original Grad: -0.113, -lr * Pred Grad:  0.042, New P: 1.964
iter 21 loss: 0.029
Actual params: [1.2034, 1.9636]
-Original Grad: 0.011, -lr * Pred Grad:  0.056, New P: 1.260
-Original Grad: -0.117, -lr * Pred Grad:  0.029, New P: 1.992
iter 22 loss: 0.032
Actual params: [1.2598, 1.9923]
-Original Grad: 0.029, -lr * Pred Grad:  0.053, New P: 1.313
-Original Grad: -0.116, -lr * Pred Grad:  0.017, New P: 2.009
iter 23 loss: 0.032
Actual params: [1.3131, 2.0088]
-Original Grad: 0.114, -lr * Pred Grad:  0.056, New P: 1.369
-Original Grad: -0.160, -lr * Pred Grad:  0.002, New P: 2.011
iter 24 loss: 0.028
Actual params: [1.369 , 2.0109]
-Original Grad: 0.087, -lr * Pred Grad:  0.057, New P: 1.426
-Original Grad: -0.128, -lr * Pred Grad:  -0.008, New P: 2.003
iter 25 loss: 0.021
Actual params: [1.4257, 2.0029]
-Original Grad: 0.108, -lr * Pred Grad:  0.059, New P: 1.484
-Original Grad: -0.109, -lr * Pred Grad:  -0.016, New P: 1.987
iter 26 loss: 0.016
Actual params: [1.4845, 1.9872]
-Original Grad: 0.052, -lr * Pred Grad:  0.057, New P: 1.542
-Original Grad: -0.064, -lr * Pred Grad:  -0.019, New P: 1.968
iter 27 loss: 0.013
Actual params: [1.5416, 1.968 ]
-Original Grad: 0.022, -lr * Pred Grad:  0.054, New P: 1.595
-Original Grad: -0.023, -lr * Pred Grad:  -0.019, New P: 1.949
iter 28 loss: 0.013
Actual params: [1.5952, 1.9488]
-Original Grad: -0.048, -lr * Pred Grad:  0.045, New P: 1.640
-Original Grad: 0.029, -lr * Pred Grad:  -0.015, New P: 1.934
iter 29 loss: 0.014
Actual params: [1.6404, 1.9335]
-Original Grad: -0.061, -lr * Pred Grad:  0.037, New P: 1.677
-Original Grad: 0.044, -lr * Pred Grad:  -0.010, New P: 1.923
iter 30 loss: 0.017
Actual params: [1.6771, 1.9232]
-Original Grad: -0.045, -lr * Pred Grad:  0.030, New P: 1.707
-Original Grad: 0.032, -lr * Pred Grad:  -0.007, New P: 1.916
Target params: [1.3344, 1.5708]
iter 0 loss: 0.170
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.112, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.015, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.158
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.088, -lr * Pred Grad:  0.099, New P: -0.274
-Original Grad: 0.011, -lr * Pred Grad:  -0.011, New P: -0.108
iter 2 loss: 0.146
Actual params: [-0.2737, -0.1078]
-Original Grad: 0.094, -lr * Pred Grad:  0.099, New P: -0.175
-Original Grad: 0.015, -lr * Pred Grad:  0.033, New P: -0.075
iter 3 loss: 0.131
Actual params: [-0.1749, -0.0751]
-Original Grad: 0.093, -lr * Pred Grad:  0.099, New P: -0.076
-Original Grad: 0.015, -lr * Pred Grad:  0.054, New P: -0.021
iter 4 loss: 0.115
Actual params: [-0.076 , -0.0215]
-Original Grad: 0.102, -lr * Pred Grad:  0.099, New P: 0.023
-Original Grad: 0.021, -lr * Pred Grad:  0.069, New P: 0.048
iter 5 loss: 0.098
Actual params: [0.0234, 0.0475]
-Original Grad: 0.096, -lr * Pred Grad:  0.099, New P: 0.123
-Original Grad: 0.003, -lr * Pred Grad:  0.063, New P: 0.111
iter 6 loss: 0.078
Actual params: [0.1228, 0.1107]
-Original Grad: 0.166, -lr * Pred Grad:  0.100, New P: 0.223
-Original Grad: 0.033, -lr * Pred Grad:  0.075, New P: 0.186
iter 7 loss: 0.051
Actual params: [0.223 , 0.1856]
-Original Grad: 0.162, -lr * Pred Grad:  0.101, New P: 0.324
-Original Grad: 0.039, -lr * Pred Grad:  0.083, New P: 0.268
iter 8 loss: 0.024
Actual params: [0.3243, 0.2681]
-Original Grad: 0.106, -lr * Pred Grad:  0.100, New P: 0.425
-Original Grad: 0.046, -lr * Pred Grad:  0.088, New P: 0.356
iter 9 loss: 0.008
Actual params: [0.4248, 0.356 ]
-Original Grad: 0.070, -lr * Pred Grad:  0.097, New P: 0.522
-Original Grad: 0.025, -lr * Pred Grad:  0.090, New P: 0.446
iter 10 loss: 0.006
Actual params: [0.5222, 0.4456]
-Original Grad: -0.001, -lr * Pred Grad:  0.087, New P: 0.609
-Original Grad: -0.021, -lr * Pred Grad:  0.066, New P: 0.511
iter 11 loss: 0.013
Actual params: [0.6094, 0.5111]
-Original Grad: -0.046, -lr * Pred Grad:  0.072, New P: 0.681
-Original Grad: -0.049, -lr * Pred Grad:  0.027, New P: 0.538
iter 12 loss: 0.022
Actual params: [0.6809, 0.538 ]
-Original Grad: -0.076, -lr * Pred Grad:  0.053, New P: 0.734
-Original Grad: -0.077, -lr * Pred Grad:  -0.011, New P: 0.527
iter 13 loss: 0.026
Actual params: [0.734 , 0.5273]
-Original Grad: -0.056, -lr * Pred Grad:  0.040, New P: 0.774
-Original Grad: -0.081, -lr * Pred Grad:  -0.035, New P: 0.493
iter 14 loss: 0.026
Actual params: [0.7741, 0.4927]
-Original Grad: -0.079, -lr * Pred Grad:  0.025, New P: 0.799
-Original Grad: -0.093, -lr * Pred Grad:  -0.052, New P: 0.440
iter 15 loss: 0.023
Actual params: [0.7994, 0.4403]
-Original Grad: -0.101, -lr * Pred Grad:  0.010, New P: 0.809
-Original Grad: -0.084, -lr * Pred Grad:  -0.064, New P: 0.376
iter 16 loss: 0.019
Actual params: [0.8089, 0.3763]
-Original Grad: -0.079, -lr * Pred Grad:  -0.001, New P: 0.808
-Original Grad: -0.058, -lr * Pred Grad:  -0.070, New P: 0.307
iter 17 loss: 0.014
Actual params: [0.8077, 0.3066]
-Original Grad: -0.075, -lr * Pred Grad:  -0.010, New P: 0.798
-Original Grad: -0.057, -lr * Pred Grad:  -0.074, New P: 0.232
iter 18 loss: 0.010
Actual params: [0.7975, 0.2321]
-Original Grad: -0.054, -lr * Pred Grad:  -0.016, New P: 0.782
-Original Grad: -0.039, -lr * Pred Grad:  -0.076, New P: 0.156
iter 19 loss: 0.007
Actual params: [0.7817, 0.1564]
-Original Grad: -0.045, -lr * Pred Grad:  -0.020, New P: 0.762
-Original Grad: -0.009, -lr * Pred Grad:  -0.071, New P: 0.085
iter 20 loss: 0.007
Actual params: [0.762 , 0.0853]
-Original Grad: -0.026, -lr * Pred Grad:  -0.021, New P: 0.741
-Original Grad: -0.001, -lr * Pred Grad:  -0.065, New P: 0.020
iter 21 loss: 0.009
Actual params: [0.7408, 0.0203]
-Original Grad: -0.005, -lr * Pred Grad:  -0.020, New P: 0.721
-Original Grad: 0.020, -lr * Pred Grad:  -0.054, New P: -0.034
iter 22 loss: 0.011
Actual params: [ 0.721 , -0.0336]
-Original Grad: 0.004, -lr * Pred Grad:  -0.018, New P: 0.703
-Original Grad: 0.044, -lr * Pred Grad:  -0.037, New P: -0.071
iter 23 loss: 0.014
Actual params: [ 0.7034, -0.0711]
-Original Grad: 0.012, -lr * Pred Grad:  -0.014, New P: 0.689
-Original Grad: 0.058, -lr * Pred Grad:  -0.019, New P: -0.090
iter 24 loss: 0.016
Actual params: [ 0.6889, -0.0904]
-Original Grad: 0.022, -lr * Pred Grad:  -0.010, New P: 0.679
-Original Grad: 0.055, -lr * Pred Grad:  -0.005, New P: -0.095
iter 25 loss: 0.017
Actual params: [ 0.6787, -0.095 ]
-Original Grad: 0.021, -lr * Pred Grad:  -0.007, New P: 0.672
-Original Grad: 0.042, -lr * Pred Grad:  0.006, New P: -0.089
iter 26 loss: 0.016
Actual params: [ 0.6721, -0.0894]
-Original Grad: 0.026, -lr * Pred Grad:  -0.002, New P: 0.670
-Original Grad: 0.043, -lr * Pred Grad:  0.015, New P: -0.075
iter 27 loss: 0.015
Actual params: [ 0.6696, -0.0747]
-Original Grad: 0.038, -lr * Pred Grad:  0.003, New P: 0.672
-Original Grad: 0.065, -lr * Pred Grad:  0.028, New P: -0.047
iter 28 loss: 0.013
Actual params: [ 0.6724, -0.0471]
-Original Grad: 0.014, -lr * Pred Grad:  0.004, New P: 0.677
-Original Grad: 0.042, -lr * Pred Grad:  0.034, New P: -0.013
iter 29 loss: 0.011
Actual params: [ 0.6769, -0.0132]
-Original Grad: 0.005, -lr * Pred Grad:  0.005, New P: 0.682
-Original Grad: 0.043, -lr * Pred Grad:  0.040, New P: 0.027
iter 30 loss: 0.009
Actual params: [0.6818, 0.0268]
-Original Grad: 0.023, -lr * Pred Grad:  0.008, New P: 0.689
-Original Grad: 0.037, -lr * Pred Grad:  0.044, New P: 0.071
Target params: [1.3344, 1.5708]
iter 0 loss: 0.514
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.054, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.010, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.501
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.115, -lr * Pred Grad:  0.096, New P: -0.277
-Original Grad: 0.012, -lr * Pred Grad:  0.100, New P: 0.204
iter 2 loss: 0.482
Actual params: [-0.2765,  0.2036]
-Original Grad: 0.064, -lr * Pred Grad:  0.095, New P: -0.182
-Original Grad: 0.005, -lr * Pred Grad:  0.093, New P: 0.296
iter 3 loss: 0.463
Actual params: [-0.1817,  0.2963]
-Original Grad: 0.155, -lr * Pred Grad:  0.095, New P: -0.086
-Original Grad: 0.008, -lr * Pred Grad:  0.094, New P: 0.390
iter 4 loss: 0.439
Actual params: [-0.0863,  0.3903]
-Original Grad: 0.183, -lr * Pred Grad:  0.097, New P: 0.010
-Original Grad: -0.023, -lr * Pred Grad:  0.005, New P: 0.395
iter 5 loss: 0.401
Actual params: [0.0103, 0.3954]
-Original Grad: 0.301, -lr * Pred Grad:  0.095, New P: 0.105
-Original Grad: -0.019, -lr * Pred Grad:  -0.025, New P: 0.371
iter 6 loss: 0.355
Actual params: [0.1052, 0.3708]
-Original Grad: 0.339, -lr * Pred Grad:  0.096, New P: 0.201
-Original Grad: 0.068, -lr * Pred Grad:  0.036, New P: 0.406
iter 7 loss: 0.289
Actual params: [0.2014, 0.4064]
-Original Grad: 0.272, -lr * Pred Grad:  0.098, New P: 0.299
-Original Grad: 0.289, -lr * Pred Grad:  0.056, New P: 0.462
iter 8 loss: 0.228
Actual params: [0.2994, 0.4623]
-Original Grad: 0.255, -lr * Pred Grad:  0.099, New P: 0.399
-Original Grad: 0.222, -lr * Pred Grad:  0.069, New P: 0.531
iter 9 loss: 0.165
Actual params: [0.3986, 0.5313]
-Original Grad: 0.329, -lr * Pred Grad:  0.101, New P: 0.499
-Original Grad: 0.243, -lr * Pred Grad:  0.078, New P: 0.609
iter 10 loss: 0.109
Actual params: [0.4995, 0.6094]
-Original Grad: 0.279, -lr * Pred Grad:  0.102, New P: 0.601
-Original Grad: 0.190, -lr * Pred Grad:  0.083, New P: 0.693
iter 11 loss: 0.071
Actual params: [0.6012, 0.6927]
-Original Grad: 0.095, -lr * Pred Grad:  0.097, New P: 0.698
-Original Grad: 0.168, -lr * Pred Grad:  0.087, New P: 0.779
iter 12 loss: 0.054
Actual params: [0.6978, 0.7792]
-Original Grad: -0.006, -lr * Pred Grad:  0.087, New P: 0.784
-Original Grad: 0.034, -lr * Pred Grad:  0.081, New P: 0.860
iter 13 loss: 0.058
Actual params: [0.7845, 0.8602]
-Original Grad: -0.151, -lr * Pred Grad:  0.068, New P: 0.852
-Original Grad: 0.047, -lr * Pred Grad:  0.077, New P: 0.938
iter 14 loss: 0.077
Actual params: [0.8523, 0.9376]
-Original Grad: -0.325, -lr * Pred Grad:  0.038, New P: 0.891
-Original Grad: -0.043, -lr * Pred Grad:  0.066, New P: 1.003
iter 15 loss: 0.093
Actual params: [0.8907, 1.0033]
-Original Grad: -0.207, -lr * Pred Grad:  0.022, New P: 0.913
-Original Grad: -0.002, -lr * Pred Grad:  0.059, New P: 1.063
iter 16 loss: 0.106
Actual params: [0.9131, 1.0626]
-Original Grad: -0.122, -lr * Pred Grad:  0.013, New P: 0.927
-Original Grad: -0.098, -lr * Pred Grad:  0.044, New P: 1.106
iter 17 loss: 0.115
Actual params: [0.9266, 1.1063]
-Original Grad: -0.161, -lr * Pred Grad:  0.003, New P: 0.930
-Original Grad: -0.034, -lr * Pred Grad:  0.036, New P: 1.143
iter 18 loss: 0.120
Actual params: [0.9297, 1.1427]
-Original Grad: -0.063, -lr * Pred Grad:  -0.001, New P: 0.929
-Original Grad: -0.078, -lr * Pred Grad:  0.025, New P: 1.168
iter 19 loss: 0.125
Actual params: [0.9291, 1.168 ]
-Original Grad: 0.034, -lr * Pred Grad:  0.001, New P: 0.930
-Original Grad: -0.121, -lr * Pred Grad:  0.011, New P: 1.179
iter 20 loss: 0.127
Actual params: [0.9305, 1.1793]
-Original Grad: -0.016, -lr * Pred Grad:  0.000, New P: 0.931
-Original Grad: -0.124, -lr * Pred Grad:  -0.001, New P: 1.178
iter 21 loss: 0.127
Actual params: [0.9308, 1.1779]
-Original Grad: 0.050, -lr * Pred Grad:  0.003, New P: 0.934
-Original Grad: -0.131, -lr * Pred Grad:  -0.013, New P: 1.165
iter 22 loss: 0.124
Actual params: [0.9339, 1.1648]
-Original Grad: 0.097, -lr * Pred Grad:  0.008, New P: 0.942
-Original Grad: -0.199, -lr * Pred Grad:  -0.028, New P: 1.136
iter 23 loss: 0.119
Actual params: [0.9422, 1.1364]
-Original Grad: -0.001, -lr * Pred Grad:  0.008, New P: 0.950
-Original Grad: -0.127, -lr * Pred Grad:  -0.036, New P: 1.100
iter 24 loss: 0.114
Actual params: [0.9498, 1.1001]
-Original Grad: -0.040, -lr * Pred Grad:  0.005, New P: 0.954
-Original Grad: -0.057, -lr * Pred Grad:  -0.038, New P: 1.062
iter 25 loss: 0.110
Actual params: [0.9544, 1.0623]
-Original Grad: -0.021, -lr * Pred Grad:  0.003, New P: 0.957
-Original Grad: -0.030, -lr * Pred Grad:  -0.037, New P: 1.025
iter 26 loss: 0.106
Actual params: [0.9573, 1.0253]
-Original Grad: -0.014, -lr * Pred Grad:  0.002, New P: 0.959
-Original Grad: -0.041, -lr * Pred Grad:  -0.037, New P: 0.988
iter 27 loss: 0.103
Actual params: [0.9591, 0.988 ]
-Original Grad: -0.068, -lr * Pred Grad:  -0.002, New P: 0.957
-Original Grad: -0.060, -lr * Pred Grad:  -0.039, New P: 0.949
iter 28 loss: 0.100
Actual params: [0.9566, 0.9488]
-Original Grad: -0.044, -lr * Pred Grad:  -0.005, New P: 0.952
-Original Grad: -0.028, -lr * Pred Grad:  -0.038, New P: 0.911
iter 29 loss: 0.098
Actual params: [0.9517, 0.9106]
-Original Grad: -0.012, -lr * Pred Grad:  -0.005, New P: 0.946
-Original Grad: -0.056, -lr * Pred Grad:  -0.040, New P: 0.871
iter 30 loss: 0.096
Actual params: [0.9464, 0.8708]
-Original Grad: -0.049, -lr * Pred Grad:  -0.008, New P: 0.939
-Original Grad: -0.044, -lr * Pred Grad:  -0.040, New P: 0.831
Target params: [1.3344, 1.5708]
iter 0 loss: 0.542
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.031, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.012, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.531
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.072, -lr * Pred Grad:  0.095, New P: -0.277
-Original Grad: 0.017, -lr * Pred Grad:  0.099, New P: 0.203
iter 2 loss: 0.514
Actual params: [-0.2773,  0.2029]
-Original Grad: 0.097, -lr * Pred Grad:  0.096, New P: -0.181
-Original Grad: 0.024, -lr * Pred Grad:  0.099, New P: 0.301
iter 3 loss: 0.495
Actual params: [-0.1814,  0.3014]
-Original Grad: 0.112, -lr * Pred Grad:  0.097, New P: -0.084
-Original Grad: 0.017, -lr * Pred Grad:  0.099, New P: 0.400
iter 4 loss: 0.472
Actual params: [-0.0842,  0.4001]
-Original Grad: 0.134, -lr * Pred Grad:  0.098, New P: 0.014
-Original Grad: 0.018, -lr * Pred Grad:  0.099, New P: 0.499
iter 5 loss: 0.442
Actual params: [0.014, 0.499]
-Original Grad: 0.209, -lr * Pred Grad:  0.097, New P: 0.111
-Original Grad: 0.002, -lr * Pred Grad:  0.088, New P: 0.587
iter 6 loss: 0.406
Actual params: [0.1111, 0.5866]
-Original Grad: 0.171, -lr * Pred Grad:  0.099, New P: 0.210
-Original Grad: 0.019, -lr * Pred Grad:  0.091, New P: 0.677
iter 7 loss: 0.361
Actual params: [0.2098, 0.6774]
-Original Grad: 0.185, -lr * Pred Grad:  0.100, New P: 0.310
-Original Grad: 0.013, -lr * Pred Grad:  0.091, New P: 0.768
iter 8 loss: 0.305
Actual params: [0.31 , 0.768]
-Original Grad: 0.338, -lr * Pred Grad:  0.099, New P: 0.409
-Original Grad: 0.072, -lr * Pred Grad:  0.085, New P: 0.853
iter 9 loss: 0.242
Actual params: [0.4092, 0.8526]
-Original Grad: 0.281, -lr * Pred Grad:  0.101, New P: 0.510
-Original Grad: 0.083, -lr * Pred Grad:  0.088, New P: 0.941
iter 10 loss: 0.178
Actual params: [0.5101, 0.9406]
-Original Grad: 0.205, -lr * Pred Grad:  0.101, New P: 0.611
-Original Grad: 0.061, -lr * Pred Grad:  0.092, New P: 1.033
iter 11 loss: 0.124
Actual params: [0.6114, 1.0327]
-Original Grad: 0.276, -lr * Pred Grad:  0.103, New P: 0.714
-Original Grad: 0.100, -lr * Pred Grad:  0.095, New P: 1.128
iter 12 loss: 0.088
Actual params: [0.7142, 1.128 ]
-Original Grad: 0.180, -lr * Pred Grad:  0.102, New P: 0.816
-Original Grad: 0.096, -lr * Pred Grad:  0.098, New P: 1.226
iter 13 loss: 0.074
Actual params: [0.8161, 1.2264]
-Original Grad: -0.129, -lr * Pred Grad:  0.082, New P: 0.898
-Original Grad: -0.048, -lr * Pred Grad:  0.075, New P: 1.301
iter 14 loss: 0.081
Actual params: [0.8979, 1.3011]
-Original Grad: -0.038, -lr * Pred Grad:  0.071, New P: 0.969
-Original Grad: -0.057, -lr * Pred Grad:  0.052, New P: 1.353
iter 15 loss: 0.082
Actual params: [0.9693, 1.3526]
-Original Grad: 0.113, -lr * Pred Grad:  0.072, New P: 1.041
-Original Grad: -0.072, -lr * Pred Grad:  0.028, New P: 1.381
iter 16 loss: 0.075
Actual params: [1.0409, 1.3807]
-Original Grad: 0.077, -lr * Pred Grad:  0.070, New P: 1.111
-Original Grad: -0.054, -lr * Pred Grad:  0.013, New P: 1.394
iter 17 loss: 0.065
Actual params: [1.1107, 1.3936]
-Original Grad: 0.097, -lr * Pred Grad:  0.069, New P: 1.180
-Original Grad: -0.121, -lr * Pred Grad:  -0.013, New P: 1.380
iter 18 loss: 0.058
Actual params: [1.18  , 1.3803]
-Original Grad: 0.013, -lr * Pred Grad:  0.064, New P: 1.244
-Original Grad: 0.052, -lr * Pred Grad:  -0.002, New P: 1.379
iter 19 loss: 0.057
Actual params: [1.2439, 1.3786]
-Original Grad: 0.015, -lr * Pred Grad:  0.059, New P: 1.303
-Original Grad: -0.016, -lr * Pred Grad:  -0.005, New P: 1.374
iter 20 loss: 0.059
Actual params: [1.303, 1.374]
-Original Grad: -0.014, -lr * Pred Grad:  0.053, New P: 1.356
-Original Grad: 0.048, -lr * Pred Grad:  0.005, New P: 1.379
iter 21 loss: 0.063
Actual params: [1.3558, 1.3791]
-Original Grad: -0.047, -lr * Pred Grad:  0.045, New P: 1.400
-Original Grad: 0.096, -lr * Pred Grad:  0.022, New P: 1.401
iter 22 loss: 0.064
Actual params: [1.4004, 1.4012]
-Original Grad: -0.078, -lr * Pred Grad:  0.035, New P: 1.435
-Original Grad: 0.115, -lr * Pred Grad:  0.038, New P: 1.440
iter 23 loss: 0.061
Actual params: [1.4353, 1.4396]
-Original Grad: -0.096, -lr * Pred Grad:  0.025, New P: 1.460
-Original Grad: 0.174, -lr * Pred Grad:  0.057, New P: 1.496
iter 24 loss: 0.057
Actual params: [1.46  , 1.4963]
-Original Grad: -0.052, -lr * Pred Grad:  0.019, New P: 1.479
-Original Grad: 0.064, -lr * Pred Grad:  0.060, New P: 1.557
iter 25 loss: 0.056
Actual params: [1.4788, 1.5567]
-Original Grad: -0.099, -lr * Pred Grad:  0.010, New P: 1.489
-Original Grad: 0.045, -lr * Pred Grad:  0.061, New P: 1.618
iter 26 loss: 0.060
Actual params: [1.4885, 1.6181]
-Original Grad: -0.036, -lr * Pred Grad:  0.006, New P: 1.495
-Original Grad: -0.038, -lr * Pred Grad:  0.050, New P: 1.668
iter 27 loss: 0.067
Actual params: [1.4948, 1.668 ]
-Original Grad: 0.022, -lr * Pred Grad:  0.007, New P: 1.502
-Original Grad: -0.084, -lr * Pred Grad:  0.032, New P: 1.700
iter 28 loss: 0.073
Actual params: [1.5021, 1.6997]
-Original Grad: 0.135, -lr * Pred Grad:  0.016, New P: 1.519
-Original Grad: -0.181, -lr * Pred Grad:  0.001, New P: 1.701
iter 29 loss: 0.072
Actual params: [1.5186, 1.701 ]
-Original Grad: 0.051, -lr * Pred Grad:  0.019, New P: 1.537
-Original Grad: -0.132, -lr * Pred Grad:  -0.016, New P: 1.685
iter 30 loss: 0.067
Actual params: [1.5373, 1.6848]
-Original Grad: 0.111, -lr * Pred Grad:  0.025, New P: 1.563
-Original Grad: -0.131, -lr * Pred Grad:  -0.031, New P: 1.654
Target params: [1.3344, 1.5708]
iter 0 loss: 0.206
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.122, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.087, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.187
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.127, -lr * Pred Grad:  0.100, New P: -0.272
-Original Grad: 0.059, -lr * Pred Grad:  0.097, New P: 0.201
iter 2 loss: 0.165
Actual params: [-0.2722,  0.2007]
-Original Grad: 0.192, -lr * Pred Grad:  0.099, New P: -0.173
-Original Grad: 0.035, -lr * Pred Grad:  0.091, New P: 0.292
iter 3 loss: 0.143
Actual params: [-0.1729,  0.2921]
-Original Grad: 0.279, -lr * Pred Grad:  0.098, New P: -0.075
-Original Grad: -0.025, -lr * Pred Grad:  0.060, New P: 0.352
iter 4 loss: 0.122
Actual params: [-0.0748,  0.3524]
-Original Grad: 0.209, -lr * Pred Grad:  0.099, New P: 0.024
-Original Grad: -0.027, -lr * Pred Grad:  0.037, New P: 0.389
iter 5 loss: 0.099
Actual params: [0.024 , 0.3893]
-Original Grad: 0.240, -lr * Pred Grad:  0.100, New P: 0.124
-Original Grad: -0.009, -lr * Pred Grad:  0.028, New P: 0.417
iter 6 loss: 0.086
Actual params: [0.1239, 0.4169]
-Original Grad: 0.041, -lr * Pred Grad:  0.091, New P: 0.215
-Original Grad: 0.040, -lr * Pred Grad:  0.039, New P: 0.456
iter 7 loss: 0.095
Actual params: [0.215 , 0.4559]
-Original Grad: -0.179, -lr * Pred Grad:  0.059, New P: 0.274
-Original Grad: 0.083, -lr * Pred Grad:  0.056, New P: 0.512
iter 8 loss: 0.100
Actual params: [0.2738, 0.5122]
-Original Grad: -0.429, -lr * Pred Grad:  0.010, New P: 0.284
-Original Grad: 0.197, -lr * Pred Grad:  0.069, New P: 0.581
iter 9 loss: 0.093
Actual params: [0.2836, 0.5813]
-Original Grad: -0.265, -lr * Pred Grad:  -0.009, New P: 0.274
-Original Grad: 0.123, -lr * Pred Grad:  0.077, New P: 0.658
iter 10 loss: 0.083
Actual params: [0.2742, 0.6581]
-Original Grad: -0.161, -lr * Pred Grad:  -0.019, New P: 0.256
-Original Grad: 0.101, -lr * Pred Grad:  0.081, New P: 0.739
iter 11 loss: 0.071
Actual params: [0.2556, 0.7393]
-Original Grad: -0.202, -lr * Pred Grad:  -0.029, New P: 0.227
-Original Grad: 0.134, -lr * Pred Grad:  0.086, New P: 0.826
iter 12 loss: 0.065
Actual params: [0.2268, 0.8257]
-Original Grad: 0.040, -lr * Pred Grad:  -0.023, New P: 0.203
-Original Grad: 0.012, -lr * Pred Grad:  0.080, New P: 0.905
iter 13 loss: 0.068
Actual params: [0.2034, 0.9053]
-Original Grad: 0.163, -lr * Pred Grad:  -0.011, New P: 0.193
-Original Grad: -0.027, -lr * Pred Grad:  0.068, New P: 0.973
iter 14 loss: 0.072
Actual params: [0.1926, 0.973 ]
-Original Grad: 0.320, -lr * Pred Grad:  0.009, New P: 0.202
-Original Grad: -0.062, -lr * Pred Grad:  0.051, New P: 1.024
iter 15 loss: 0.072
Actual params: [0.2018, 1.0241]
-Original Grad: 0.233, -lr * Pred Grad:  0.021, New P: 0.223
-Original Grad: -0.053, -lr * Pred Grad:  0.038, New P: 1.062
iter 16 loss: 0.069
Actual params: [0.2227, 1.0619]
-Original Grad: 0.255, -lr * Pred Grad:  0.032, New P: 0.255
-Original Grad: -0.056, -lr * Pred Grad:  0.026, New P: 1.088
iter 17 loss: 0.065
Actual params: [0.2546, 1.0876]
-Original Grad: 0.126, -lr * Pred Grad:  0.035, New P: 0.290
-Original Grad: -0.027, -lr * Pred Grad:  0.019, New P: 1.107
iter 18 loss: 0.061
Actual params: [0.2901, 1.1069]
-Original Grad: 0.021, -lr * Pred Grad:  0.033, New P: 0.323
-Original Grad: -0.006, -lr * Pred Grad:  0.017, New P: 1.124
iter 19 loss: 0.061
Actual params: [0.3234, 1.1235]
-Original Grad: 0.069, -lr * Pred Grad:  0.034, New P: 0.357
-Original Grad: 0.001, -lr * Pred Grad:  0.015, New P: 1.139
iter 20 loss: 0.061
Actual params: [0.3574, 1.1388]
-Original Grad: 0.052, -lr * Pred Grad:  0.034, New P: 0.391
-Original Grad: 0.022, -lr * Pred Grad:  0.017, New P: 1.156
iter 21 loss: 0.061
Actual params: [0.3911, 1.156 ]
-Original Grad: 0.006, -lr * Pred Grad:  0.031, New P: 0.422
-Original Grad: 0.023, -lr * Pred Grad:  0.019, New P: 1.175
iter 22 loss: 0.061
Actual params: [0.4221, 1.1752]
-Original Grad: 0.063, -lr * Pred Grad:  0.032, New P: 0.454
-Original Grad: -0.009, -lr * Pred Grad:  0.016, New P: 1.191
iter 23 loss: 0.060
Actual params: [0.4538, 1.1911]
-Original Grad: 0.077, -lr * Pred Grad:  0.033, New P: 0.487
-Original Grad: 0.028, -lr * Pred Grad:  0.019, New P: 1.210
iter 24 loss: 0.059
Actual params: [0.4869, 1.21  ]
-Original Grad: -0.008, -lr * Pred Grad:  0.030, New P: 0.517
-Original Grad: 0.076, -lr * Pred Grad:  0.028, New P: 1.238
iter 25 loss: 0.056
Actual params: [0.5166, 1.2384]
-Original Grad: 0.029, -lr * Pred Grad:  0.029, New P: 0.545
-Original Grad: 0.077, -lr * Pred Grad:  0.037, New P: 1.275
iter 26 loss: 0.053
Actual params: [0.5453, 1.2755]
-Original Grad: 0.043, -lr * Pred Grad:  0.029, New P: 0.574
-Original Grad: 0.073, -lr * Pred Grad:  0.044, New P: 1.320
iter 27 loss: 0.049
Actual params: [0.574 , 1.3196]
-Original Grad: 0.039, -lr * Pred Grad:  0.028, New P: 0.602
-Original Grad: 0.056, -lr * Pred Grad:  0.048, New P: 1.368
iter 28 loss: 0.046
Actual params: [0.6024, 1.3677]
-Original Grad: 0.054, -lr * Pred Grad:  0.029, New P: 0.632
-Original Grad: 0.015, -lr * Pred Grad:  0.046, New P: 1.414
iter 29 loss: 0.042
Actual params: [0.6315, 1.4139]
-Original Grad: 0.033, -lr * Pred Grad:  0.028, New P: 0.660
-Original Grad: 0.020, -lr * Pred Grad:  0.045, New P: 1.459
iter 30 loss: 0.040
Actual params: [0.66  , 1.4589]
-Original Grad: 0.138, -lr * Pred Grad:  0.034, New P: 0.694
-Original Grad: 0.019, -lr * Pred Grad:  0.044, New P: 1.503
Target params: [1.3344, 1.5708]
iter 0 loss: 0.778
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.002, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.778
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.084, New P: -0.656
-Original Grad: 0.000, -lr * Pred Grad:  0.080, New P: 0.184
iter 2 loss: 0.778
Actual params: [-0.6565,  0.1837]
-Original Grad: -0.000, -lr * Pred Grad:  -0.075, New P: -0.731
-Original Grad: 0.000, -lr * Pred Grad:  0.068, New P: 0.251
iter 3 loss: 0.778
Actual params: [-0.7314,  0.2512]
-Original Grad: -0.000, -lr * Pred Grad:  -0.065, New P: -0.797
-Original Grad: 0.000, -lr * Pred Grad:  0.057, New P: 0.308
iter 4 loss: 0.778
Actual params: [-0.7968,  0.3081]
-Original Grad: -0.000, -lr * Pred Grad:  -0.057, New P: -0.854
-Original Grad: 0.000, -lr * Pred Grad:  0.048, New P: 0.357
iter 5 loss: 0.778
Actual params: [-0.854 ,  0.3565]
-Original Grad: -0.000, -lr * Pred Grad:  -0.050, New P: -0.904
-Original Grad: -0.000, -lr * Pred Grad:  0.041, New P: 0.398
iter 6 loss: 0.778
Actual params: [-0.9042,  0.3979]
-Original Grad: -0.000, -lr * Pred Grad:  -0.045, New P: -0.949
-Original Grad: -0.000, -lr * Pred Grad:  0.036, New P: 0.434
iter 7 loss: 0.778
Actual params: [-0.9489,  0.4338]
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -0.989
-Original Grad: -0.000, -lr * Pred Grad:  0.031, New P: 0.465
iter 8 loss: 0.778
Actual params: [-0.989 ,  0.4651]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -1.025
-Original Grad: -0.000, -lr * Pred Grad:  0.027, New P: 0.492
iter 9 loss: 0.778
Actual params: [-1.025 ,  0.4923]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -1.057
-Original Grad: -0.000, -lr * Pred Grad:  0.024, New P: 0.516
iter 10 loss: 0.778
Actual params: [-1.0575,  0.516 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.087
-Original Grad: -0.000, -lr * Pred Grad:  0.021, New P: 0.537
iter 11 loss: 0.778
Actual params: [-1.0868,  0.5368]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.113
-Original Grad: -0.000, -lr * Pred Grad:  0.018, New P: 0.555
iter 12 loss: 0.778
Actual params: [-1.1135,  0.555 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.138
-Original Grad: -0.000, -lr * Pred Grad:  0.016, New P: 0.571
iter 13 loss: 0.778
Actual params: [-1.1377,  0.571 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.160
-Original Grad: -0.000, -lr * Pred Grad:  0.014, New P: 0.585
iter 14 loss: 0.778
Actual params: [-1.1598,  0.5851]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.180
-Original Grad: -0.000, -lr * Pred Grad:  0.012, New P: 0.598
iter 15 loss: 0.778
Actual params: [-1.1799,  0.5975]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.198
-Original Grad: -0.000, -lr * Pred Grad:  0.011, New P: 0.608
iter 16 loss: 0.778
Actual params: [-1.1984,  0.6083]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.215
-Original Grad: -0.000, -lr * Pred Grad:  0.009, New P: 0.618
iter 17 loss: 0.778
Actual params: [-1.2152,  0.6176]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.231
-Original Grad: -0.000, -lr * Pred Grad:  0.008, New P: 0.626
iter 18 loss: 0.778
Actual params: [-1.2307,  0.6259]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.245
-Original Grad: -0.000, -lr * Pred Grad:  0.007, New P: 0.633
iter 19 loss: 0.778
Actual params: [-1.2449,  0.6329]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.258
-Original Grad: -0.000, -lr * Pred Grad:  0.006, New P: 0.639
iter 20 loss: 0.778
Actual params: [-1.2579,  0.6389]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.270
-Original Grad: -0.000, -lr * Pred Grad:  0.005, New P: 0.644
iter 21 loss: 0.778
Actual params: [-1.27  ,  0.6441]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.281
-Original Grad: -0.000, -lr * Pred Grad:  0.004, New P: 0.648
iter 22 loss: 0.778
Actual params: [-1.281 ,  0.6483]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.291
-Original Grad: -0.000, -lr * Pred Grad:  0.004, New P: 0.652
iter 23 loss: 0.778
Actual params: [-1.2913,  0.6519]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.301
-Original Grad: -0.000, -lr * Pred Grad:  0.003, New P: 0.655
iter 24 loss: 0.778
Actual params: [-1.3007,  0.6548]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.309
-Original Grad: -0.000, -lr * Pred Grad:  0.002, New P: 0.657
iter 25 loss: 0.778
Actual params: [-1.3094,  0.6572]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.317
-Original Grad: -0.000, -lr * Pred Grad:  0.002, New P: 0.659
iter 26 loss: 0.778
Actual params: [-1.3174,  0.6592]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.325
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: 0.661
iter 27 loss: 0.778
Actual params: [-1.3248,  0.6607]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.332
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: 0.662
iter 28 loss: 0.778
Actual params: [-1.3316,  0.6618]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.338
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: 0.663
iter 29 loss: 0.778
Actual params: [-1.3379,  0.6626]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.344
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: 0.663
iter 30 loss: 0.778
Actual params: [-1.3437,  0.6631]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.349
-Original Grad: -0.000, -lr * Pred Grad:  0.000, New P: 0.663
Target params: [1.3344, 1.5708]
iter 0 loss: 0.440
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.007, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.003, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.438
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.018, -lr * Pred Grad:  0.093, New P: -0.279
-Original Grad: 0.009, -lr * Pred Grad:  0.092, New P: 0.196
iter 2 loss: 0.433
Actual params: [-0.2794,  0.1955]
-Original Grad: 0.040, -lr * Pred Grad:  0.088, New P: -0.191
-Original Grad: 0.022, -lr * Pred Grad:  0.087, New P: 0.282
iter 3 loss: 0.419
Actual params: [-0.191,  0.282]
-Original Grad: 0.146, -lr * Pred Grad:  0.077, New P: -0.114
-Original Grad: 0.080, -lr * Pred Grad:  0.076, New P: 0.358
iter 4 loss: 0.391
Actual params: [-0.1144,  0.3577]
-Original Grad: 0.166, -lr * Pred Grad:  0.084, New P: -0.030
-Original Grad: 0.096, -lr * Pred Grad:  0.083, New P: 0.441
iter 5 loss: 0.344
Actual params: [-0.0304,  0.4409]
-Original Grad: 0.273, -lr * Pred Grad:  0.086, New P: 0.056
-Original Grad: 0.192, -lr * Pred Grad:  0.083, New P: 0.524
iter 6 loss: 0.273
Actual params: [0.0559, 0.5241]
-Original Grad: 0.758, -lr * Pred Grad:  0.078, New P: 0.134
-Original Grad: 0.299, -lr * Pred Grad:  0.084, New P: 0.609
iter 7 loss: 0.171
Actual params: [0.1337, 0.6085]
-Original Grad: 0.408, -lr * Pred Grad:  0.083, New P: 0.217
-Original Grad: 0.311, -lr * Pred Grad:  0.089, New P: 0.698
iter 8 loss: 0.103
Actual params: [0.2171, 0.6975]
-Original Grad: 0.229, -lr * Pred Grad:  0.084, New P: 0.301
-Original Grad: 0.164, -lr * Pred Grad:  0.090, New P: 0.788
iter 9 loss: 0.059
Actual params: [0.3006, 0.7879]
-Original Grad: 0.177, -lr * Pred Grad:  0.082, New P: 0.383
-Original Grad: 0.093, -lr * Pred Grad:  0.088, New P: 0.876
iter 10 loss: 0.033
Actual params: [0.3828, 0.876 ]
-Original Grad: 0.066, -lr * Pred Grad:  0.077, New P: 0.459
-Original Grad: 0.042, -lr * Pred Grad:  0.083, New P: 0.959
iter 11 loss: 0.021
Actual params: [0.4595, 0.9586]
-Original Grad: 0.031, -lr * Pred Grad:  0.070, New P: 0.530
-Original Grad: -0.011, -lr * Pred Grad:  0.073, New P: 1.032
iter 12 loss: 0.021
Actual params: [0.5299, 1.0318]
-Original Grad: 0.050, -lr * Pred Grad:  0.066, New P: 0.596
-Original Grad: -0.021, -lr * Pred Grad:  0.064, New P: 1.096
iter 13 loss: 0.031
Actual params: [0.5959, 1.0958]
-Original Grad: -0.030, -lr * Pred Grad:  0.058, New P: 0.654
-Original Grad: -0.067, -lr * Pred Grad:  0.051, New P: 1.147
iter 14 loss: 0.049
Actual params: [0.6539, 1.1471]
-Original Grad: 0.019, -lr * Pred Grad:  0.053, New P: 0.707
-Original Grad: -0.068, -lr * Pred Grad:  0.040, New P: 1.187
iter 15 loss: 0.069
Actual params: [0.7074, 1.1869]
-Original Grad: -0.033, -lr * Pred Grad:  0.047, New P: 0.754
-Original Grad: -0.107, -lr * Pred Grad:  0.026, New P: 1.213
iter 16 loss: 0.087
Actual params: [0.7542, 1.2127]
-Original Grad: -0.294, -lr * Pred Grad:  0.026, New P: 0.781
-Original Grad: -0.192, -lr * Pred Grad:  0.006, New P: 1.218
iter 17 loss: 0.095
Actual params: [0.7806, 1.2182]
-Original Grad: -0.049, -lr * Pred Grad:  0.022, New P: 0.802
-Original Grad: -0.100, -lr * Pred Grad:  -0.004, New P: 1.215
iter 18 loss: 0.100
Actual params: [0.8022, 1.2147]
-Original Grad: -0.181, -lr * Pred Grad:  0.011, New P: 0.813
-Original Grad: -0.128, -lr * Pred Grad:  -0.014, New P: 1.201
iter 19 loss: 0.099
Actual params: [0.8127, 1.2008]
-Original Grad: -0.124, -lr * Pred Grad:  0.003, New P: 0.816
-Original Grad: -0.151, -lr * Pred Grad:  -0.025, New P: 1.176
iter 20 loss: 0.095
Actual params: [0.8162, 1.1762]
-Original Grad: -0.350, -lr * Pred Grad:  -0.013, New P: 0.803
-Original Grad: -0.165, -lr * Pred Grad:  -0.035, New P: 1.141
iter 21 loss: 0.084
Actual params: [0.8029, 1.1413]
-Original Grad: -0.204, -lr * Pred Grad:  -0.021, New P: 0.781
-Original Grad: -0.117, -lr * Pred Grad:  -0.041, New P: 1.101
iter 22 loss: 0.070
Actual params: [0.7815, 1.1007]
-Original Grad: -0.104, -lr * Pred Grad:  -0.024, New P: 0.757
-Original Grad: -0.109, -lr * Pred Grad:  -0.045, New P: 1.056
iter 23 loss: 0.054
Actual params: [0.7573, 1.0556]
-Original Grad: -0.205, -lr * Pred Grad:  -0.031, New P: 0.726
-Original Grad: -0.191, -lr * Pred Grad:  -0.054, New P: 1.001
iter 24 loss: 0.037
Actual params: [0.726 , 1.0013]
-Original Grad: -0.102, -lr * Pred Grad:  -0.033, New P: 0.693
-Original Grad: -0.086, -lr * Pred Grad:  -0.056, New P: 0.946
iter 25 loss: 0.024
Actual params: [0.6929, 0.9457]
-Original Grad: -0.076, -lr * Pred Grad:  -0.034, New P: 0.659
-Original Grad: -0.037, -lr * Pred Grad:  -0.054, New P: 0.892
iter 26 loss: 0.017
Actual params: [0.6592, 0.8921]
-Original Grad: -0.044, -lr * Pred Grad:  -0.033, New P: 0.626
-Original Grad: -0.034, -lr * Pred Grad:  -0.051, New P: 0.841
iter 27 loss: 0.014
Actual params: [0.6264, 0.8406]
-Original Grad: -0.012, -lr * Pred Grad:  -0.030, New P: 0.596
-Original Grad: -0.016, -lr * Pred Grad:  -0.048, New P: 0.792
iter 28 loss: 0.015
Actual params: [0.596 , 0.7924]
-Original Grad: 0.026, -lr * Pred Grad:  -0.026, New P: 0.569
-Original Grad: 0.008, -lr * Pred Grad:  -0.043, New P: 0.749
iter 29 loss: 0.017
Actual params: [0.5695, 0.7491]
-Original Grad: 0.086, -lr * Pred Grad:  -0.020, New P: 0.550
-Original Grad: 0.021, -lr * Pred Grad:  -0.038, New P: 0.712
iter 30 loss: 0.021
Actual params: [0.5497, 0.7115]
-Original Grad: 0.061, -lr * Pred Grad:  -0.015, New P: 0.535
-Original Grad: 0.030, -lr * Pred Grad:  -0.032, New P: 0.680
Target params: [1.3344, 1.5708]
iter 0 loss: 0.586
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.012, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.010, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.586
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.083, New P: -0.655
-Original Grad: 0.003, -lr * Pred Grad:  0.085, New P: 0.189
iter 2 loss: 0.585
Actual params: [-0.6551,  0.1885]
-Original Grad: -0.001, -lr * Pred Grad:  -0.067, New P: -0.722
-Original Grad: 0.001, -lr * Pred Grad:  0.071, New P: 0.259
iter 3 loss: 0.585
Actual params: [-0.7223,  0.259 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.056, New P: -0.778
-Original Grad: 0.000, -lr * Pred Grad:  0.060, New P: 0.319
iter 4 loss: 0.585
Actual params: [-0.7782,  0.3188]
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -0.826
-Original Grad: 0.000, -lr * Pred Grad:  0.051, New P: 0.370
iter 5 loss: 0.585
Actual params: [-0.8256,  0.3701]
-Original Grad: 0.000, -lr * Pred Grad:  -0.041, New P: -0.866
-Original Grad: 0.000, -lr * Pred Grad:  0.045, New P: 0.415
iter 6 loss: 0.585
Actual params: [-0.8663,  0.4148]
-Original Grad: 0.000, -lr * Pred Grad:  -0.035, New P: -0.902
-Original Grad: 0.000, -lr * Pred Grad:  0.039, New P: 0.454
iter 7 loss: 0.585
Actual params: [-0.9017,  0.454 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.031, New P: -0.933
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: 0.489
iter 8 loss: 0.585
Actual params: [-0.9329,  0.4886]
-Original Grad: 0.000, -lr * Pred Grad:  -0.028, New P: -0.960
-Original Grad: 0.000, -lr * Pred Grad:  0.031, New P: 0.519
iter 9 loss: 0.585
Actual params: [-0.9604,  0.5194]
-Original Grad: 0.000, -lr * Pred Grad:  -0.024, New P: -0.985
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 0.547
iter 10 loss: 0.585
Actual params: [-0.9848,  0.547 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.022, New P: -1.007
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.572
iter 11 loss: 0.585
Actual params: [-1.0067,  0.5717]
-Original Grad: 0.000, -lr * Pred Grad:  -0.020, New P: -1.026
-Original Grad: 0.000, -lr * Pred Grad:  0.022, New P: 0.594
iter 12 loss: 0.585
Actual params: [-1.0262,  0.5939]
-Original Grad: 0.000, -lr * Pred Grad:  -0.018, New P: -1.044
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.614
iter 13 loss: 0.585
Actual params: [-1.0438,  0.6139]
-Original Grad: 0.000, -lr * Pred Grad:  -0.016, New P: -1.060
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.632
iter 14 loss: 0.585
Actual params: [-1.0596,  0.6321]
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -1.074
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.648
iter 15 loss: 0.585
Actual params: [-1.0739,  0.6485]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -1.087
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.663
iter 16 loss: 0.585
Actual params: [-1.0868,  0.6633]
-Original Grad: 0.000, -lr * Pred Grad:  -0.012, New P: -1.099
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.677
iter 17 loss: 0.585
Actual params: [-1.0985,  0.6768]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.109
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.689
iter 18 loss: 0.585
Actual params: [-1.1091,  0.6891]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.119
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.700
iter 19 loss: 0.585
Actual params: [-1.1186,  0.7002]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.127
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.710
iter 20 loss: 0.585
Actual params: [-1.1273,  0.7104]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.135
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.720
iter 21 loss: 0.585
Actual params: [-1.1352,  0.7196]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.142
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.728
iter 22 loss: 0.585
Actual params: [-1.1423,  0.728 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.149
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.736
iter 23 loss: 0.585
Actual params: [-1.1487,  0.7357]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.155
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.743
iter 24 loss: 0.585
Actual params: [-1.1546,  0.7426]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.160
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.749
iter 25 loss: 0.585
Actual params: [-1.1599,  0.749 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.165
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.755
iter 26 loss: 0.585
Actual params: [-1.1647,  0.7548]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.169
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.760
iter 27 loss: 0.585
Actual params: [-1.1691,  0.7601]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.173
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.765
iter 28 loss: 0.585
Actual params: [-1.173 ,  0.7649]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.177
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.769
iter 29 loss: 0.585
Actual params: [-1.1766,  0.7692]
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -1.180
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.773
iter 30 loss: 0.585
Actual params: [-1.1798,  0.7732]
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -1.183
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.777
Target params: [1.3344, 1.5708]
iter 0 loss: 0.293
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.025, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.026, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.288
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.017, -lr * Pred Grad:  -0.098, New P: -0.670
-Original Grad: 0.041, -lr * Pred Grad:  0.099, New P: 0.202
iter 2 loss: 0.285
Actual params: [-0.6699,  0.2024]
-Original Grad: -0.003, -lr * Pred Grad:  -0.081, New P: -0.751
-Original Grad: 0.040, -lr * Pred Grad:  0.100, New P: 0.302
iter 3 loss: 0.282
Actual params: [-0.7511,  0.302 ]
-Original Grad: 0.004, -lr * Pred Grad:  -0.058, New P: -0.809
-Original Grad: 0.022, -lr * Pred Grad:  0.096, New P: 0.398
iter 4 loss: 0.279
Actual params: [-0.8088,  0.3981]
-Original Grad: 0.017, -lr * Pred Grad:  -0.017, New P: -0.826
-Original Grad: 0.033, -lr * Pred Grad:  0.097, New P: 0.495
iter 5 loss: 0.277
Actual params: [-0.8259,  0.4952]
-Original Grad: 0.021, -lr * Pred Grad:  0.014, New P: -0.812
-Original Grad: 0.030, -lr * Pred Grad:  0.097, New P: 0.592
iter 6 loss: 0.274
Actual params: [-0.8122,  0.5923]
-Original Grad: 0.035, -lr * Pred Grad:  0.042, New P: -0.770
-Original Grad: 0.041, -lr * Pred Grad:  0.099, New P: 0.691
iter 7 loss: 0.269
Actual params: [-0.7701,  0.6909]
-Original Grad: 0.037, -lr * Pred Grad:  0.059, New P: -0.711
-Original Grad: 0.032, -lr * Pred Grad:  0.099, New P: 0.789
iter 8 loss: 0.264
Actual params: [-0.7114,  0.7894]
-Original Grad: 0.046, -lr * Pred Grad:  0.071, New P: -0.641
-Original Grad: 0.031, -lr * Pred Grad:  0.098, New P: 0.888
iter 9 loss: 0.258
Actual params: [-0.6407,  0.8877]
-Original Grad: 0.043, -lr * Pred Grad:  0.079, New P: -0.562
-Original Grad: 0.019, -lr * Pred Grad:  0.095, New P: 0.983
iter 10 loss: 0.251
Actual params: [-0.5621,  0.983 ]
-Original Grad: 0.055, -lr * Pred Grad:  0.085, New P: -0.477
-Original Grad: 0.021, -lr * Pred Grad:  0.093, New P: 1.076
iter 11 loss: 0.242
Actual params: [-0.4768,  1.0763]
-Original Grad: 0.097, -lr * Pred Grad:  0.089, New P: -0.388
-Original Grad: 0.011, -lr * Pred Grad:  0.089, New P: 1.165
iter 12 loss: 0.232
Actual params: [-0.3878,  1.1649]
-Original Grad: 0.078, -lr * Pred Grad:  0.094, New P: -0.294
-Original Grad: 0.002, -lr * Pred Grad:  0.081, New P: 1.246
iter 13 loss: 0.222
Actual params: [-0.2942,  1.2457]
-Original Grad: 0.126, -lr * Pred Grad:  0.096, New P: -0.198
-Original Grad: 0.010, -lr * Pred Grad:  0.077, New P: 1.323
iter 14 loss: 0.210
Actual params: [-0.1977,  1.323 ]
-Original Grad: 0.118, -lr * Pred Grad:  0.100, New P: -0.098
-Original Grad: -0.011, -lr * Pred Grad:  0.065, New P: 1.388
iter 15 loss: 0.197
Actual params: [-0.0978,  1.3875]
-Original Grad: 0.120, -lr * Pred Grad:  0.103, New P: 0.005
-Original Grad: -0.003, -lr * Pred Grad:  0.057, New P: 1.444
iter 16 loss: 0.180
Actual params: [0.0051, 1.4444]
-Original Grad: 0.287, -lr * Pred Grad:  0.100, New P: 0.105
-Original Grad: 0.019, -lr * Pred Grad:  0.060, New P: 1.504
iter 17 loss: 0.155
Actual params: [0.1048, 1.5042]
-Original Grad: 0.285, -lr * Pred Grad:  0.102, New P: 0.207
-Original Grad: 0.017, -lr * Pred Grad:  0.061, New P: 1.566
iter 18 loss: 0.122
Actual params: [0.2072, 1.5656]
-Original Grad: 0.326, -lr * Pred Grad:  0.105, New P: 0.312
-Original Grad: 0.066, -lr * Pred Grad:  0.074, New P: 1.639
iter 19 loss: 0.085
Actual params: [0.3124, 1.6394]
-Original Grad: 0.264, -lr * Pred Grad:  0.108, New P: 0.420
-Original Grad: 0.065, -lr * Pred Grad:  0.083, New P: 1.722
iter 20 loss: 0.053
Actual params: [0.4204, 1.7223]
-Original Grad: 0.218, -lr * Pred Grad:  0.110, New P: 0.530
-Original Grad: 0.041, -lr * Pred Grad:  0.087, New P: 1.809
iter 21 loss: 0.031
Actual params: [0.53  , 1.8089]
-Original Grad: 0.166, -lr * Pred Grad:  0.109, New P: 0.639
-Original Grad: 0.013, -lr * Pred Grad:  0.083, New P: 1.892
iter 22 loss: 0.018
Actual params: [0.6392, 1.892 ]
-Original Grad: 0.105, -lr * Pred Grad:  0.106, New P: 0.745
-Original Grad: -0.017, -lr * Pred Grad:  0.069, New P: 1.961
iter 23 loss: 0.014
Actual params: [0.7453, 1.9612]
-Original Grad: 0.037, -lr * Pred Grad:  0.099, New P: 0.845
-Original Grad: -0.038, -lr * Pred Grad:  0.048, New P: 2.009
iter 24 loss: 0.015
Actual params: [0.8446, 2.0091]
-Original Grad: 0.002, -lr * Pred Grad:  0.091, New P: 0.935
-Original Grad: -0.059, -lr * Pred Grad:  0.022, New P: 2.031
iter 25 loss: 0.017
Actual params: [0.9352, 2.0307]
-Original Grad: 0.001, -lr * Pred Grad:  0.083, New P: 1.018
-Original Grad: -0.078, -lr * Pred Grad:  -0.005, New P: 2.025
iter 26 loss: 0.018
Actual params: [1.0178, 2.0254]
-Original Grad: -0.004, -lr * Pred Grad:  0.075, New P: 1.093
-Original Grad: -0.086, -lr * Pred Grad:  -0.028, New P: 1.998
iter 27 loss: 0.018
Actual params: [1.0928, 1.9977]
-Original Grad: -0.027, -lr * Pred Grad:  0.066, New P: 1.159
-Original Grad: -0.046, -lr * Pred Grad:  -0.037, New P: 1.961
iter 28 loss: 0.017
Actual params: [1.1589, 1.9606]
-Original Grad: -0.024, -lr * Pred Grad:  0.058, New P: 1.217
-Original Grad: -0.049, -lr * Pred Grad:  -0.046, New P: 1.915
iter 29 loss: 0.015
Actual params: [1.2172, 1.9148]
-Original Grad: -0.023, -lr * Pred Grad:  0.051, New P: 1.268
-Original Grad: -0.052, -lr * Pred Grad:  -0.054, New P: 1.861
iter 30 loss: 0.013
Actual params: [1.2684, 1.8609]
-Original Grad: -0.005, -lr * Pred Grad:  0.046, New P: 1.315
-Original Grad: -0.047, -lr * Pred Grad:  -0.060, New P: 1.801
Target params: [1.3344, 1.5708]
iter 0 loss: 0.267
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.011, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.007, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.266
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.002, -lr * Pred Grad:  -0.080, New P: -0.652
-Original Grad: -0.001, -lr * Pred Grad:  -0.078, New P: -0.175
iter 2 loss: 0.266
Actual params: [-0.6523, -0.1747]
-Original Grad: -0.001, -lr * Pred Grad:  -0.067, New P: -0.719
-Original Grad: -0.001, -lr * Pred Grad:  -0.066, New P: -0.241
iter 3 loss: 0.266
Actual params: [-0.7194, -0.2411]
-Original Grad: -0.001, -lr * Pred Grad:  -0.059, New P: -0.778
-Original Grad: -0.000, -lr * Pred Grad:  -0.058, New P: -0.299
iter 4 loss: 0.266
Actual params: [-0.7779, -0.2989]
-Original Grad: -0.000, -lr * Pred Grad:  -0.051, New P: -0.829
-Original Grad: -0.000, -lr * Pred Grad:  -0.050, New P: -0.349
iter 5 loss: 0.266
Actual params: [-0.8286, -0.3487]
-Original Grad: -0.000, -lr * Pred Grad:  -0.045, New P: -0.873
-Original Grad: -0.000, -lr * Pred Grad:  -0.044, New P: -0.392
iter 6 loss: 0.266
Actual params: [-0.8732, -0.3924]
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -0.913
-Original Grad: -0.000, -lr * Pred Grad:  -0.039, New P: -0.431
iter 7 loss: 0.266
Actual params: [-0.9128, -0.4309]
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -0.948
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -0.465
iter 8 loss: 0.266
Actual params: [-0.9482, -0.4652]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -0.980
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -0.496
iter 9 loss: 0.266
Actual params: [-0.98  , -0.4959]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.009
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -0.523
iter 10 loss: 0.266
Actual params: [-1.0085, -0.5235]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.034
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -0.548
iter 11 loss: 0.266
Actual params: [-1.0343, -0.5482]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.058
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -0.571
iter 12 loss: 0.266
Actual params: [-1.0576, -0.5706]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.079
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -0.591
iter 13 loss: 0.266
Actual params: [-1.0788, -0.5908]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.098
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -0.609
iter 14 loss: 0.266
Actual params: [-1.098 , -0.6092]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.116
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.626
iter 15 loss: 0.266
Actual params: [-1.1156, -0.6258]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.132
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.641
iter 16 loss: 0.266
Actual params: [-1.1316, -0.6409]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.146
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.655
iter 17 loss: 0.266
Actual params: [-1.1461, -0.6546]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.159
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.667
iter 18 loss: 0.266
Actual params: [-1.1594, -0.6671]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.172
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.678
iter 19 loss: 0.266
Actual params: [-1.1715, -0.6785]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.183
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.689
iter 20 loss: 0.266
Actual params: [-1.1826, -0.6888]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.193
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.698
iter 21 loss: 0.266
Actual params: [-1.1928, -0.6983]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.202
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.707
iter 22 loss: 0.266
Actual params: [-1.2021, -0.7069]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.211
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.715
iter 23 loss: 0.266
Actual params: [-1.2106, -0.7147]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.218
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.722
iter 24 loss: 0.266
Actual params: [-1.2184, -0.7219]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.226
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.728
iter 25 loss: 0.266
Actual params: [-1.2255, -0.7284]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.232
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.734
iter 26 loss: 0.266
Actual params: [-1.2321, -0.7344]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.238
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.740
iter 27 loss: 0.266
Actual params: [-1.2381, -0.7398]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.244
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.745
iter 28 loss: 0.266
Actual params: [-1.2436, -0.7448]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.249
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.749
iter 29 loss: 0.266
Actual params: [-1.2487, -0.7493]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.253
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.753
iter 30 loss: 0.266
Actual params: [-1.2533, -0.7535]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.258
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.757
Target params: [1.3344, 1.5708]
iter 0 loss: 0.432
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.016, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.002, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.431
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.007, -lr * Pred Grad:  -0.092, New P: -0.664
-Original Grad: -0.003, -lr * Pred Grad:  -0.100, New P: -0.196
iter 2 loss: 0.430
Actual params: [-0.6639, -0.196 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.079, New P: -0.743
-Original Grad: -0.001, -lr * Pred Grad:  -0.094, New P: -0.290
iter 3 loss: 0.430
Actual params: [-0.7427, -0.29  ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.069, New P: -0.811
-Original Grad: -0.001, -lr * Pred Grad:  -0.087, New P: -0.377
iter 4 loss: 0.430
Actual params: [-0.8113, -0.3774]
-Original Grad: -0.001, -lr * Pred Grad:  -0.060, New P: -0.871
-Original Grad: -0.000, -lr * Pred Grad:  -0.079, New P: -0.456
iter 5 loss: 0.430
Actual params: [-0.8709, -0.456 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.052, New P: -0.923
-Original Grad: -0.000, -lr * Pred Grad:  -0.069, New P: -0.525
iter 6 loss: 0.430
Actual params: [-0.9226, -0.5248]
-Original Grad: -0.000, -lr * Pred Grad:  -0.046, New P: -0.968
-Original Grad: -0.000, -lr * Pred Grad:  -0.061, New P: -0.586
iter 7 loss: 0.430
Actual params: [-0.9681, -0.5859]
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -1.008
-Original Grad: -0.000, -lr * Pred Grad:  -0.055, New P: -0.640
iter 8 loss: 0.430
Actual params: [-1.0084, -0.6404]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -1.044
-Original Grad: -0.000, -lr * Pred Grad:  -0.049, New P: -0.689
iter 9 loss: 0.430
Actual params: [-1.0444, -0.6892]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -1.077
-Original Grad: -0.000, -lr * Pred Grad:  -0.044, New P: -0.733
iter 10 loss: 0.429
Actual params: [-1.0766, -0.733 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.105
-Original Grad: -0.000, -lr * Pred Grad:  -0.039, New P: -0.772
iter 11 loss: 0.429
Actual params: [-1.1055, -0.7724]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.131
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -0.808
iter 12 loss: 0.429
Actual params: [-1.1315, -0.8079]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.155
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -0.840
iter 13 loss: 0.429
Actual params: [-1.155, -0.84 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.176
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -0.869
iter 14 loss: 0.429
Actual params: [-1.1762, -0.8691]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.195
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -0.895
iter 15 loss: 0.429
Actual params: [-1.1955, -0.8954]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.213
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -0.919
iter 16 loss: 0.429
Actual params: [-1.2129, -0.9193]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.229
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -0.941
iter 17 loss: 0.429
Actual params: [-1.2288, -0.941 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.243
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -0.961
iter 18 loss: 0.429
Actual params: [-1.2432, -0.9608]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.256
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -0.979
iter 19 loss: 0.429
Actual params: [-1.2563, -0.9787]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.268
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.995
iter 20 loss: 0.429
Actual params: [-1.2683, -0.9951]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.279
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.010
iter 21 loss: 0.429
Actual params: [-1.2791, -1.01  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.289
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.024
iter 22 loss: 0.429
Actual params: [-1.289 , -1.0236]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.298
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.036
iter 23 loss: 0.429
Actual params: [-1.2981, -1.036 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.306
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.047
iter 24 loss: 0.429
Actual params: [-1.3063, -1.0473]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.314
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.058
iter 25 loss: 0.429
Actual params: [-1.3138, -1.0575]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.321
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.067
iter 26 loss: 0.429
Actual params: [-1.3206, -1.0669]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.327
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.075
iter 27 loss: 0.429
Actual params: [-1.3269, -1.0755]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.333
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.083
iter 28 loss: 0.429
Actual params: [-1.3326, -1.0833]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.338
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.090
iter 29 loss: 0.429
Actual params: [-1.3378, -1.0904]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.343
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.097
iter 30 loss: 0.429
Actual params: [-1.3425, -1.0969]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.347
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.103
Target params: [1.3344, 1.5708]
iter 0 loss: 0.292
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.004, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.010, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.290
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.019, -lr * Pred Grad:  0.086, New P: -0.287
-Original Grad: 0.020, -lr * Pred Grad:  0.097, New P: 0.200
iter 2 loss: 0.286
Actual params: [-0.2865,  0.2004]
-Original Grad: 0.026, -lr * Pred Grad:  0.091, New P: -0.196
-Original Grad: 0.019, -lr * Pred Grad:  0.098, New P: 0.299
iter 3 loss: 0.278
Actual params: [-0.1957,  0.2989]
-Original Grad: 0.061, -lr * Pred Grad:  0.086, New P: -0.110
-Original Grad: 0.045, -lr * Pred Grad:  0.093, New P: 0.392
iter 4 loss: 0.266
Actual params: [-0.1099,  0.3916]
-Original Grad: 0.107, -lr * Pred Grad:  0.085, New P: -0.025
-Original Grad: 0.068, -lr * Pred Grad:  0.091, New P: 0.483
iter 5 loss: 0.253
Actual params: [-0.0248,  0.4831]
-Original Grad: 0.110, -lr * Pred Grad:  0.090, New P: 0.065
-Original Grad: 0.087, -lr * Pred Grad:  0.093, New P: 0.576
iter 6 loss: 0.236
Actual params: [0.0648, 0.5757]
-Original Grad: 0.136, -lr * Pred Grad:  0.093, New P: 0.158
-Original Grad: 0.094, -lr * Pred Grad:  0.095, New P: 0.671
iter 7 loss: 0.214
Actual params: [0.1576, 0.6706]
-Original Grad: 0.198, -lr * Pred Grad:  0.094, New P: 0.251
-Original Grad: 0.112, -lr * Pred Grad:  0.097, New P: 0.768
iter 8 loss: 0.187
Actual params: [0.2513, 0.7675]
-Original Grad: 0.192, -lr * Pred Grad:  0.096, New P: 0.348
-Original Grad: 0.110, -lr * Pred Grad:  0.099, New P: 0.866
iter 9 loss: 0.157
Actual params: [0.3477, 0.8665]
-Original Grad: 0.241, -lr * Pred Grad:  0.098, New P: 0.446
-Original Grad: 0.086, -lr * Pred Grad:  0.100, New P: 0.966
iter 10 loss: 0.124
Actual params: [0.4461, 0.9664]
-Original Grad: 0.215, -lr * Pred Grad:  0.100, New P: 0.547
-Original Grad: 0.120, -lr * Pred Grad:  0.102, New P: 1.068
iter 11 loss: 0.088
Actual params: [0.5465, 1.068 ]
-Original Grad: 0.165, -lr * Pred Grad:  0.101, New P: 0.648
-Original Grad: 0.098, -lr * Pred Grad:  0.102, New P: 1.170
iter 12 loss: 0.055
Actual params: [0.6476, 1.1704]
-Original Grad: 0.166, -lr * Pred Grad:  0.102, New P: 0.749
-Original Grad: 0.119, -lr * Pred Grad:  0.104, New P: 1.274
iter 13 loss: 0.029
Actual params: [0.7493, 1.2742]
-Original Grad: 0.109, -lr * Pred Grad:  0.100, New P: 0.849
-Original Grad: 0.103, -lr * Pred Grad:  0.104, New P: 1.379
iter 14 loss: 0.015
Actual params: [0.849 , 1.3785]
-Original Grad: 0.032, -lr * Pred Grad:  0.093, New P: 0.942
-Original Grad: 0.025, -lr * Pred Grad:  0.098, New P: 1.476
iter 15 loss: 0.015
Actual params: [0.9419, 1.4764]
-Original Grad: -0.040, -lr * Pred Grad:  0.080, New P: 1.022
-Original Grad: -0.025, -lr * Pred Grad:  0.085, New P: 1.561
iter 16 loss: 0.021
Actual params: [1.0223, 1.5612]
-Original Grad: 0.022, -lr * Pred Grad:  0.075, New P: 1.097
-Original Grad: -0.038, -lr * Pred Grad:  0.071, New P: 1.632
iter 17 loss: 0.025
Actual params: [1.0971, 1.6319]
-Original Grad: 0.002, -lr * Pred Grad:  0.068, New P: 1.165
-Original Grad: -0.034, -lr * Pred Grad:  0.059, New P: 1.691
iter 18 loss: 0.026
Actual params: [1.1653, 1.6906]
-Original Grad: -0.004, -lr * Pred Grad:  0.062, New P: 1.227
-Original Grad: -0.029, -lr * Pred Grad:  0.049, New P: 1.739
iter 19 loss: 0.027
Actual params: [1.2269, 1.7393]
-Original Grad: 0.024, -lr * Pred Grad:  0.058, New P: 1.285
-Original Grad: -0.038, -lr * Pred Grad:  0.038, New P: 1.778
iter 20 loss: 0.027
Actual params: [1.2851, 1.7775]
-Original Grad: 0.013, -lr * Pred Grad:  0.054, New P: 1.339
-Original Grad: -0.024, -lr * Pred Grad:  0.031, New P: 1.809
iter 21 loss: 0.027
Actual params: [1.3393, 1.8085]
-Original Grad: 0.014, -lr * Pred Grad:  0.051, New P: 1.390
-Original Grad: -0.031, -lr * Pred Grad:  0.023, New P: 1.832
iter 22 loss: 0.028
Actual params: [1.3899, 1.8317]
-Original Grad: 0.005, -lr * Pred Grad:  0.047, New P: 1.436
-Original Grad: -0.049, -lr * Pred Grad:  0.013, New P: 1.845
iter 23 loss: 0.029
Actual params: [1.4365, 1.8451]
-Original Grad: -0.017, -lr * Pred Grad:  0.041, New P: 1.477
-Original Grad: -0.035, -lr * Pred Grad:  0.007, New P: 1.852
iter 24 loss: 0.030
Actual params: [1.4772, 1.8518]
-Original Grad: -0.030, -lr * Pred Grad:  0.034, New P: 1.511
-Original Grad: -0.024, -lr * Pred Grad:  0.002, New P: 1.854
iter 25 loss: 0.031
Actual params: [1.5113, 1.854 ]
-Original Grad: -0.014, -lr * Pred Grad:  0.030, New P: 1.541
-Original Grad: -0.019, -lr * Pred Grad:  -0.001, New P: 1.853
iter 26 loss: 0.032
Actual params: [1.5409, 1.853 ]
-Original Grad: -0.014, -lr * Pred Grad:  0.026, New P: 1.566
-Original Grad: -0.021, -lr * Pred Grad:  -0.004, New P: 1.849
iter 27 loss: 0.032
Actual params: [1.5665, 1.8488]
-Original Grad: -0.013, -lr * Pred Grad:  0.022, New P: 1.588
-Original Grad: -0.022, -lr * Pred Grad:  -0.007, New P: 1.841
iter 28 loss: 0.032
Actual params: [1.5884, 1.8413]
-Original Grad: -0.016, -lr * Pred Grad:  0.018, New P: 1.607
-Original Grad: -0.030, -lr * Pred Grad:  -0.012, New P: 1.830
iter 29 loss: 0.032
Actual params: [1.6067, 1.8296]
-Original Grad: -0.019, -lr * Pred Grad:  0.015, New P: 1.621
-Original Grad: -0.035, -lr * Pred Grad:  -0.016, New P: 1.813
iter 30 loss: 0.032
Actual params: [1.6213, 1.8131]
-Original Grad: -0.019, -lr * Pred Grad:  0.011, New P: 1.633
-Original Grad: -0.030, -lr * Pred Grad:  -0.020, New P: 1.793
Target params: [1.3344, 1.5708]
iter 0 loss: 0.062
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.027, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.025, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.058
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.014, -lr * Pred Grad:  -0.094, New P: -0.666
-Original Grad: 0.014, -lr * Pred Grad:  0.095, New P: 0.198
iter 2 loss: 0.056
Actual params: [-0.6664,  0.198 ]
-Original Grad: -0.007, -lr * Pred Grad:  -0.085, New P: -0.752
-Original Grad: 0.007, -lr * Pred Grad:  0.086, New P: 0.284
iter 3 loss: 0.055
Actual params: [-0.7519,  0.2836]
-Original Grad: -0.003, -lr * Pred Grad:  -0.075, New P: -0.827
-Original Grad: 0.002, -lr * Pred Grad:  0.075, New P: 0.358
iter 4 loss: 0.055
Actual params: [-0.8269,  0.3584]
-Original Grad: -0.002, -lr * Pred Grad:  -0.066, New P: -0.893
-Original Grad: 0.001, -lr * Pred Grad:  0.066, New P: 0.424
iter 5 loss: 0.054
Actual params: [-0.8933,  0.4241]
-Original Grad: -0.001, -lr * Pred Grad:  -0.059, New P: -0.953
-Original Grad: 0.001, -lr * Pred Grad:  0.058, New P: 0.482
iter 6 loss: 0.054
Actual params: [-0.9528,  0.4823]
-Original Grad: -0.001, -lr * Pred Grad:  -0.053, New P: -1.006
-Original Grad: 0.001, -lr * Pred Grad:  0.052, New P: 0.534
iter 7 loss: 0.054
Actual params: [-1.006 ,  0.5342]
-Original Grad: -0.000, -lr * Pred Grad:  -0.048, New P: -1.054
-Original Grad: 0.000, -lr * Pred Grad:  0.046, New P: 0.580
iter 8 loss: 0.054
Actual params: [-1.0537,  0.5805]
-Original Grad: -0.000, -lr * Pred Grad:  -0.043, New P: -1.097
-Original Grad: 0.000, -lr * Pred Grad:  0.041, New P: 0.622
iter 9 loss: 0.054
Actual params: [-1.0966,  0.6219]
-Original Grad: -0.000, -lr * Pred Grad:  -0.039, New P: -1.135
-Original Grad: 0.000, -lr * Pred Grad:  0.037, New P: 0.659
iter 10 loss: 0.054
Actual params: [-1.1353,  0.6592]
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -1.170
-Original Grad: 0.000, -lr * Pred Grad:  0.034, New P: 0.693
iter 11 loss: 0.054
Actual params: [-1.1704,  0.6929]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -1.202
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: 0.723
iter 12 loss: 0.054
Actual params: [-1.2021,  0.7233]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.231
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 0.751
iter 13 loss: 0.054
Actual params: [-1.2309,  0.7508]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.257
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.776
iter 14 loss: 0.054
Actual params: [-1.2571,  0.7759]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.281
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.799
iter 15 loss: 0.054
Actual params: [-1.281 ,  0.7986]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.303
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.819
iter 16 loss: 0.054
Actual params: [-1.3026,  0.8192]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.322
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.838
iter 17 loss: 0.054
Actual params: [-1.3224,  0.8381]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.340
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.855
iter 18 loss: 0.054
Actual params: [-1.3404,  0.8552]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.357
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.871
iter 19 loss: 0.054
Actual params: [-1.3568,  0.8708]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.372
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.885
iter 20 loss: 0.054
Actual params: [-1.3717,  0.8851]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.385
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.898
iter 21 loss: 0.054
Actual params: [-1.3854,  0.8981]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.398
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.910
iter 22 loss: 0.054
Actual params: [-1.3979,  0.91  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.409
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.921
iter 23 loss: 0.054
Actual params: [-1.4093,  0.9208]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.420
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.931
iter 24 loss: 0.054
Actual params: [-1.4197,  0.9307]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.429
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.940
iter 25 loss: 0.054
Actual params: [-1.4292,  0.9398]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.438
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.948
iter 26 loss: 0.054
Actual params: [-1.4379,  0.9481]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.446
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.956
iter 27 loss: 0.054
Actual params: [-1.4458,  0.9556]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.453
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.963
iter 28 loss: 0.054
Actual params: [-1.4531,  0.9625]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.460
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.969
iter 29 loss: 0.054
Actual params: [-1.4597,  0.9689]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.466
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.975
iter 30 loss: 0.054
Actual params: [-1.4658,  0.9746]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.471
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.980
Target params: [1.3344, 1.5708]
iter 0 loss: 0.481
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.013, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.004, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.478
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.037, -lr * Pred Grad:  0.093, New P: -0.279
-Original Grad: -0.008, -lr * Pred Grad:  -0.096, New P: -0.193
iter 2 loss: 0.473
Actual params: [-0.2794, -0.1928]
-Original Grad: 0.061, -lr * Pred Grad:  0.092, New P: -0.187
-Original Grad: -0.008, -lr * Pred Grad:  -0.098, New P: -0.291
iter 3 loss: 0.465
Actual params: [-0.187 , -0.2909]
-Original Grad: 0.100, -lr * Pred Grad:  0.091, New P: -0.096
-Original Grad: 0.013, -lr * Pred Grad:  -0.012, New P: -0.303
iter 4 loss: 0.458
Actual params: [-0.0955, -0.3027]
-Original Grad: 0.070, -lr * Pred Grad:  0.094, New P: -0.001
-Original Grad: 0.033, -lr * Pred Grad:  0.044, New P: -0.259
iter 5 loss: 0.448
Actual params: [-0.0014, -0.259 ]
-Original Grad: 0.088, -lr * Pred Grad:  0.096, New P: 0.095
-Original Grad: 0.056, -lr * Pred Grad:  0.064, New P: -0.195
iter 6 loss: 0.433
Actual params: [ 0.0951, -0.1947]
-Original Grad: 0.139, -lr * Pred Grad:  0.097, New P: 0.192
-Original Grad: 0.073, -lr * Pred Grad:  0.075, New P: -0.119
iter 7 loss: 0.412
Actual params: [ 0.1923, -0.1193]
-Original Grad: 0.176, -lr * Pred Grad:  0.098, New P: 0.290
-Original Grad: 0.081, -lr * Pred Grad:  0.083, New P: -0.037
iter 8 loss: 0.384
Actual params: [ 0.2901, -0.0365]
-Original Grad: 0.186, -lr * Pred Grad:  0.099, New P: 0.389
-Original Grad: 0.126, -lr * Pred Grad:  0.087, New P: 0.050
iter 9 loss: 0.348
Actual params: [0.3894, 0.0502]
-Original Grad: 0.271, -lr * Pred Grad:  0.099, New P: 0.489
-Original Grad: 0.150, -lr * Pred Grad:  0.090, New P: 0.141
iter 10 loss: 0.303
Actual params: [0.4888, 0.1406]
-Original Grad: 0.203, -lr * Pred Grad:  0.101, New P: 0.590
-Original Grad: 0.133, -lr * Pred Grad:  0.094, New P: 0.235
iter 11 loss: 0.252
Actual params: [0.5899, 0.2348]
-Original Grad: 0.259, -lr * Pred Grad:  0.103, New P: 0.693
-Original Grad: 0.235, -lr * Pred Grad:  0.096, New P: 0.330
iter 12 loss: 0.203
Actual params: [0.6928, 0.3304]
-Original Grad: 0.272, -lr * Pred Grad:  0.105, New P: 0.797
-Original Grad: 0.320, -lr * Pred Grad:  0.096, New P: 0.427
iter 13 loss: 0.159
Actual params: [0.7973, 0.4266]
-Original Grad: 0.153, -lr * Pred Grad:  0.103, New P: 0.901
-Original Grad: 0.224, -lr * Pred Grad:  0.099, New P: 0.526
iter 14 loss: 0.127
Actual params: [0.9007, 0.5259]
-Original Grad: 0.015, -lr * Pred Grad:  0.095, New P: 0.995
-Original Grad: 0.259, -lr * Pred Grad:  0.102, New P: 0.628
iter 15 loss: 0.106
Actual params: [0.9954, 0.628 ]
-Original Grad: -0.021, -lr * Pred Grad:  0.084, New P: 1.079
-Original Grad: 0.132, -lr * Pred Grad:  0.101, New P: 0.729
iter 16 loss: 0.091
Actual params: [1.0795, 0.729 ]
-Original Grad: -0.073, -lr * Pred Grad:  0.070, New P: 1.149
-Original Grad: 0.202, -lr * Pred Grad:  0.103, New P: 0.831
iter 17 loss: 0.080
Actual params: [1.1495, 0.8315]
-Original Grad: -0.052, -lr * Pred Grad:  0.059, New P: 1.209
-Original Grad: 0.212, -lr * Pred Grad:  0.104, New P: 0.936
iter 18 loss: 0.072
Actual params: [1.2087, 0.9355]
-Original Grad: -0.090, -lr * Pred Grad:  0.046, New P: 1.255
-Original Grad: 0.113, -lr * Pred Grad:  0.102, New P: 1.037
iter 19 loss: 0.068
Actual params: [1.2549, 1.0371]
-Original Grad: -0.073, -lr * Pred Grad:  0.036, New P: 1.291
-Original Grad: 0.028, -lr * Pred Grad:  0.094, New P: 1.132
iter 20 loss: 0.066
Actual params: [1.291 , 1.1315]
-Original Grad: -0.023, -lr * Pred Grad:  0.031, New P: 1.322
-Original Grad: 0.014, -lr * Pred Grad:  0.087, New P: 1.218
iter 21 loss: 0.066
Actual params: [1.3218, 1.2185]
-Original Grad: 0.022, -lr * Pred Grad:  0.030, New P: 1.352
-Original Grad: -0.011, -lr * Pred Grad:  0.078, New P: 1.297
iter 22 loss: 0.066
Actual params: [1.3517, 1.2967]
-Original Grad: 0.017, -lr * Pred Grad:  0.029, New P: 1.380
-Original Grad: 0.005, -lr * Pred Grad:  0.072, New P: 1.368
iter 23 loss: 0.067
Actual params: [1.3802, 1.3684]
-Original Grad: 0.030, -lr * Pred Grad:  0.028, New P: 1.409
-Original Grad: -0.034, -lr * Pred Grad:  0.063, New P: 1.431
iter 24 loss: 0.067
Actual params: [1.4087, 1.4309]
-Original Grad: 0.053, -lr * Pred Grad:  0.030, New P: 1.439
-Original Grad: -0.025, -lr * Pred Grad:  0.055, New P: 1.486
iter 25 loss: 0.067
Actual params: [1.439 , 1.4859]
-Original Grad: 0.115, -lr * Pred Grad:  0.037, New P: 1.476
-Original Grad: -0.023, -lr * Pred Grad:  0.048, New P: 1.534
iter 26 loss: 0.065
Actual params: [1.4757, 1.5342]
-Original Grad: 0.097, -lr * Pred Grad:  0.041, New P: 1.517
-Original Grad: -0.008, -lr * Pred Grad:  0.043, New P: 1.578
iter 27 loss: 0.062
Actual params: [1.5168, 1.5775]
-Original Grad: 0.142, -lr * Pred Grad:  0.048, New P: 1.565
-Original Grad: -0.062, -lr * Pred Grad:  0.034, New P: 1.612
iter 28 loss: 0.057
Actual params: [1.5651, 1.6118]
-Original Grad: 0.076, -lr * Pred Grad:  0.050, New P: 1.615
-Original Grad: 0.016, -lr * Pred Grad:  0.033, New P: 1.644
iter 29 loss: 0.053
Actual params: [1.6152, 1.6444]
-Original Grad: 0.062, -lr * Pred Grad:  0.051, New P: 1.666
-Original Grad: 0.001, -lr * Pred Grad:  0.030, New P: 1.674
iter 30 loss: 0.049
Actual params: [1.6658, 1.6742]
-Original Grad: 0.075, -lr * Pred Grad:  0.052, New P: 1.718
-Original Grad: -0.013, -lr * Pred Grad:  0.026, New P: 1.700
Target params: [1.3344, 1.5708]
iter 0 loss: 0.212
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.038, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.004, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.206
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.070, -lr * Pred Grad:  0.097, New P: -0.275
-Original Grad: 0.012, -lr * Pred Grad:  0.052, New P: -0.045
iter 2 loss: 0.198
Actual params: [-0.2749, -0.0446]
-Original Grad: 0.058, -lr * Pred Grad:  0.098, New P: -0.176
-Original Grad: 0.010, -lr * Pred Grad:  0.071, New P: 0.026
iter 3 loss: 0.189
Actual params: [-0.1765,  0.0265]
-Original Grad: 0.092, -lr * Pred Grad:  0.099, New P: -0.078
-Original Grad: 0.001, -lr * Pred Grad:  0.060, New P: 0.087
iter 4 loss: 0.175
Actual params: [-0.0779,  0.0869]
-Original Grad: 0.120, -lr * Pred Grad:  0.099, New P: 0.021
-Original Grad: 0.014, -lr * Pred Grad:  0.074, New P: 0.161
iter 5 loss: 0.152
Actual params: [0.0206, 0.1613]
-Original Grad: 0.240, -lr * Pred Grad:  0.093, New P: 0.113
-Original Grad: 0.020, -lr * Pred Grad:  0.083, New P: 0.244
iter 6 loss: 0.121
Actual params: [0.1134, 0.2438]
-Original Grad: 0.377, -lr * Pred Grad:  0.090, New P: 0.203
-Original Grad: 0.007, -lr * Pred Grad:  0.082, New P: 0.326
iter 7 loss: 0.087
Actual params: [0.2034, 0.3262]
-Original Grad: 0.364, -lr * Pred Grad:  0.093, New P: 0.297
-Original Grad: -0.018, -lr * Pred Grad:  0.036, New P: 0.362
iter 8 loss: 0.071
Actual params: [0.2966, 0.3623]
-Original Grad: 0.027, -lr * Pred Grad:  0.085, New P: 0.381
-Original Grad: 0.056, -lr * Pred Grad:  0.058, New P: 0.421
iter 9 loss: 0.088
Actual params: [0.3815, 0.4208]
-Original Grad: -0.101, -lr * Pred Grad:  0.067, New P: 0.448
-Original Grad: 0.077, -lr * Pred Grad:  0.071, New P: 0.492
iter 10 loss: 0.093
Actual params: [0.4482, 0.4916]
-Original Grad: -0.258, -lr * Pred Grad:  0.036, New P: 0.485
-Original Grad: 0.156, -lr * Pred Grad:  0.075, New P: 0.567
iter 11 loss: 0.087
Actual params: [0.4845, 0.5666]
-Original Grad: -0.008, -lr * Pred Grad:  0.032, New P: 0.517
-Original Grad: 0.076, -lr * Pred Grad:  0.081, New P: 0.647
iter 12 loss: 0.078
Actual params: [0.5167, 0.6473]
-Original Grad: -0.091, -lr * Pred Grad:  0.022, New P: 0.539
-Original Grad: 0.159, -lr * Pred Grad:  0.087, New P: 0.734
iter 13 loss: 0.067
Actual params: [0.5388, 0.7342]
-Original Grad: -0.010, -lr * Pred Grad:  0.019, New P: 0.558
-Original Grad: 0.161, -lr * Pred Grad:  0.092, New P: 0.826
iter 14 loss: 0.055
Actual params: [0.5581, 0.8264]
-Original Grad: 0.069, -lr * Pred Grad:  0.022, New P: 0.580
-Original Grad: 0.098, -lr * Pred Grad:  0.094, New P: 0.921
iter 15 loss: 0.046
Actual params: [0.5804, 0.9209]
-Original Grad: 0.058, -lr * Pred Grad:  0.024, New P: 0.605
-Original Grad: 0.065, -lr * Pred Grad:  0.094, New P: 1.015
iter 16 loss: 0.038
Actual params: [0.6048, 1.0145]
-Original Grad: 0.058, -lr * Pred Grad:  0.026, New P: 0.631
-Original Grad: 0.053, -lr * Pred Grad:  0.092, New P: 1.106
iter 17 loss: 0.031
Actual params: [0.631 , 1.1063]
-Original Grad: 0.113, -lr * Pred Grad:  0.032, New P: 0.663
-Original Grad: 0.019, -lr * Pred Grad:  0.086, New P: 1.192
iter 18 loss: 0.027
Actual params: [0.6627, 1.1925]
-Original Grad: 0.111, -lr * Pred Grad:  0.036, New P: 0.699
-Original Grad: -0.001, -lr * Pred Grad:  0.078, New P: 1.271
iter 19 loss: 0.024
Actual params: [0.6991, 1.2706]
-Original Grad: 0.048, -lr * Pred Grad:  0.037, New P: 0.736
-Original Grad: 0.005, -lr * Pred Grad:  0.072, New P: 1.342
iter 20 loss: 0.023
Actual params: [0.7357, 1.3423]
-Original Grad: 0.115, -lr * Pred Grad:  0.041, New P: 0.777
-Original Grad: -0.037, -lr * Pred Grad:  0.059, New P: 1.401
iter 21 loss: 0.021
Actual params: [0.7769, 1.4014]
-Original Grad: 0.103, -lr * Pred Grad:  0.045, New P: 0.821
-Original Grad: -0.037, -lr * Pred Grad:  0.048, New P: 1.449
iter 22 loss: 0.019
Actual params: [0.8214, 1.4492]
-Original Grad: 0.130, -lr * Pred Grad:  0.049, New P: 0.871
-Original Grad: -0.025, -lr * Pred Grad:  0.039, New P: 1.489
iter 23 loss: 0.017
Actual params: [0.8708, 1.4886]
-Original Grad: 0.009, -lr * Pred Grad:  0.046, New P: 0.916
-Original Grad: -0.014, -lr * Pred Grad:  0.034, New P: 1.522
iter 24 loss: 0.016
Actual params: [0.9164, 1.5222]
-Original Grad: 0.028, -lr * Pred Grad:  0.044, New P: 0.960
-Original Grad: -0.027, -lr * Pred Grad:  0.026, New P: 1.548
iter 25 loss: 0.017
Actual params: [0.9599, 1.5483]
-Original Grad: -0.005, -lr * Pred Grad:  0.039, New P: 0.999
-Original Grad: -0.030, -lr * Pred Grad:  0.019, New P: 1.567
iter 26 loss: 0.018
Actual params: [0.9992, 1.5672]
-Original Grad: -0.016, -lr * Pred Grad:  0.035, New P: 1.034
-Original Grad: -0.024, -lr * Pred Grad:  0.013, New P: 1.580
iter 27 loss: 0.019
Actual params: [1.0339, 1.5804]
-Original Grad: -0.021, -lr * Pred Grad:  0.030, New P: 1.064
-Original Grad: -0.011, -lr * Pred Grad:  0.010, New P: 1.591
iter 28 loss: 0.020
Actual params: [1.0638, 1.5906]
-Original Grad: -0.035, -lr * Pred Grad:  0.024, New P: 1.088
-Original Grad: -0.022, -lr * Pred Grad:  0.006, New P: 1.596
iter 29 loss: 0.020
Actual params: [1.0883, 1.5961]
-Original Grad: -0.073, -lr * Pred Grad:  0.017, New P: 1.105
-Original Grad: -0.017, -lr * Pred Grad:  0.002, New P: 1.598
iter 30 loss: 0.021
Actual params: [1.1048, 1.5984]
-Original Grad: -0.006, -lr * Pred Grad:  0.015, New P: 1.119
-Original Grad: -0.029, -lr * Pred Grad:  -0.003, New P: 1.595
Target params: [1.3344, 1.5708]
iter 0 loss: 0.057
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.006, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.004, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.057
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.090, New P: -0.662
-Original Grad: 0.002, -lr * Pred Grad:  0.091, New P: 0.195
iter 2 loss: 0.057
Actual params: [-0.6624,  0.1949]
-Original Grad: -0.001, -lr * Pred Grad:  -0.075, New P: -0.738
-Original Grad: 0.001, -lr * Pred Grad:  0.077, New P: 0.272
iter 3 loss: 0.057
Actual params: [-0.7375,  0.272 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.063, New P: -0.801
-Original Grad: 0.000, -lr * Pred Grad:  0.066, New P: 0.338
iter 4 loss: 0.057
Actual params: [-0.801 ,  0.3377]
-Original Grad: -0.000, -lr * Pred Grad:  -0.054, New P: -0.855
-Original Grad: 0.000, -lr * Pred Grad:  0.056, New P: 0.394
iter 5 loss: 0.057
Actual params: [-0.8553,  0.3942]
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -0.902
-Original Grad: 0.000, -lr * Pred Grad:  0.049, New P: 0.443
iter 6 loss: 0.057
Actual params: [-0.9022,  0.4432]
-Original Grad: -0.000, -lr * Pred Grad:  -0.041, New P: -0.943
-Original Grad: 0.000, -lr * Pred Grad:  0.043, New P: 0.486
iter 7 loss: 0.057
Actual params: [-0.9432,  0.4861]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -0.979
-Original Grad: 0.000, -lr * Pred Grad:  0.038, New P: 0.524
iter 8 loss: 0.057
Actual params: [-0.9793,  0.5241]
-Original Grad: 0.000, -lr * Pred Grad:  -0.032, New P: -1.011
-Original Grad: 0.000, -lr * Pred Grad:  0.034, New P: 0.558
iter 9 loss: 0.057
Actual params: [-1.0114,  0.5578]
-Original Grad: 0.000, -lr * Pred Grad:  -0.029, New P: -1.040
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: 0.588
iter 10 loss: 0.057
Actual params: [-1.0401,  0.588 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.026, New P: -1.066
-Original Grad: 0.000, -lr * Pred Grad:  0.027, New P: 0.615
iter 11 loss: 0.057
Actual params: [-1.0657,  0.615 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.023, New P: -1.089
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.639
iter 12 loss: 0.057
Actual params: [-1.0887,  0.6393]
-Original Grad: 0.000, -lr * Pred Grad:  -0.021, New P: -1.110
-Original Grad: 0.000, -lr * Pred Grad:  0.022, New P: 0.661
iter 13 loss: 0.057
Actual params: [-1.1095,  0.6613]
-Original Grad: 0.000, -lr * Pred Grad:  -0.019, New P: -1.128
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.681
iter 14 loss: 0.057
Actual params: [-1.1283,  0.6811]
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -1.145
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.699
iter 15 loss: 0.057
Actual params: [-1.1452,  0.699 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -1.161
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.715
iter 16 loss: 0.057
Actual params: [-1.1606,  0.7153]
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -1.174
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.730
iter 17 loss: 0.057
Actual params: [-1.1745,  0.7301]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -1.187
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.743
iter 18 loss: 0.057
Actual params: [-1.1871,  0.7435]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.199
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.756
iter 19 loss: 0.057
Actual params: [-1.1986,  0.7557]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.209
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.767
iter 20 loss: 0.057
Actual params: [-1.209 ,  0.7668]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.219
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.777
iter 21 loss: 0.057
Actual params: [-1.2185,  0.7769]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.227
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.786
iter 22 loss: 0.057
Actual params: [-1.2271,  0.7861]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.235
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.794
iter 23 loss: 0.057
Actual params: [-1.235 ,  0.7945]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.242
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.802
iter 24 loss: 0.057
Actual params: [-1.2421,  0.8021]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.249
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.809
iter 25 loss: 0.057
Actual params: [-1.2486,  0.809 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.254
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.815
iter 26 loss: 0.057
Actual params: [-1.2545,  0.8154]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.260
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.821
iter 27 loss: 0.057
Actual params: [-1.2599,  0.8212]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.265
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.826
iter 28 loss: 0.057
Actual params: [-1.2648,  0.8264]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.269
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.831
iter 29 loss: 0.057
Actual params: [-1.2692,  0.8312]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.273
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.836
iter 30 loss: 0.057
Actual params: [-1.2732,  0.8356]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.277
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.840
Target params: [1.3344, 1.5708]
iter 0 loss: 0.432
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.014, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.002, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.431
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.007, -lr * Pred Grad:  -0.093, New P: -0.665
-Original Grad: -0.003, -lr * Pred Grad:  -0.098, New P: -0.195
iter 2 loss: 0.430
Actual params: [-0.665 , -0.1947]
-Original Grad: -0.003, -lr * Pred Grad:  -0.084, New P: -0.749
-Original Grad: -0.002, -lr * Pred Grad:  -0.098, New P: -0.292
iter 3 loss: 0.430
Actual params: [-0.749 , -0.2925]
-Original Grad: -0.001, -lr * Pred Grad:  -0.074, New P: -0.823
-Original Grad: -0.001, -lr * Pred Grad:  -0.092, New P: -0.384
iter 4 loss: 0.430
Actual params: [-0.8229, -0.3843]
-Original Grad: -0.001, -lr * Pred Grad:  -0.065, New P: -0.888
-Original Grad: -0.000, -lr * Pred Grad:  -0.083, New P: -0.468
iter 5 loss: 0.430
Actual params: [-0.8877, -0.4677]
-Original Grad: -0.000, -lr * Pred Grad:  -0.056, New P: -0.944
-Original Grad: -0.000, -lr * Pred Grad:  -0.073, New P: -0.541
iter 6 loss: 0.430
Actual params: [-0.944 , -0.5411]
-Original Grad: -0.000, -lr * Pred Grad:  -0.050, New P: -0.994
-Original Grad: -0.000, -lr * Pred Grad:  -0.065, New P: -0.606
iter 7 loss: 0.430
Actual params: [-0.9936, -0.6063]
-Original Grad: -0.000, -lr * Pred Grad:  -0.044, New P: -1.037
-Original Grad: -0.000, -lr * Pred Grad:  -0.058, New P: -0.664
iter 8 loss: 0.430
Actual params: [-1.0375, -0.6644]
-Original Grad: -0.000, -lr * Pred Grad:  -0.039, New P: -1.077
-Original Grad: -0.000, -lr * Pred Grad:  -0.052, New P: -0.716
iter 9 loss: 0.429
Actual params: [-1.0766, -0.7162]
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -1.112
-Original Grad: -0.000, -lr * Pred Grad:  -0.046, New P: -0.763
iter 10 loss: 0.429
Actual params: [-1.1116, -0.7627]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.143
-Original Grad: -0.000, -lr * Pred Grad:  -0.042, New P: -0.804
iter 11 loss: 0.429
Actual params: [-1.1431, -0.8045]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.171
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -0.842
iter 12 loss: 0.429
Actual params: [-1.1714, -0.8422]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.197
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -0.876
iter 13 loss: 0.429
Actual params: [-1.1969, -0.8762]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.220
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -0.907
iter 14 loss: 0.429
Actual params: [-1.22 , -0.907]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.241
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -0.935
iter 15 loss: 0.429
Actual params: [-1.2409, -0.9349]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.260
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -0.960
iter 16 loss: 0.429
Actual params: [-1.2599, -0.9602]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.277
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -0.983
iter 17 loss: 0.429
Actual params: [-1.2771, -0.9832]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.293
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.004
iter 18 loss: 0.429
Actual params: [-1.2928, -1.0041]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.307
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.023
iter 19 loss: 0.429
Actual params: [-1.307 , -1.0231]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.320
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.040
iter 20 loss: 0.429
Actual params: [-1.32  , -1.0404]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.332
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.056
iter 21 loss: 0.429
Actual params: [-1.3318, -1.0561]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.343
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.070
iter 22 loss: 0.429
Actual params: [-1.3426, -1.0705]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.352
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.084
iter 23 loss: 0.429
Actual params: [-1.3523, -1.0835]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.361
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.095
iter 24 loss: 0.429
Actual params: [-1.3613, -1.0954]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.369
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.106
iter 25 loss: 0.429
Actual params: [-1.3694, -1.1063]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.377
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.116
iter 26 loss: 0.429
Actual params: [-1.3768, -1.1162]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.384
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.125
iter 27 loss: 0.429
Actual params: [-1.3836, -1.1252]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.390
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.133
iter 28 loss: 0.429
Actual params: [-1.3897, -1.1334]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.395
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.141
iter 29 loss: 0.429
Actual params: [-1.3954, -1.1409]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.400
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.148
iter 30 loss: 0.429
Actual params: [-1.4005, -1.1477]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.405
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.154
Target params: [1.3344, 1.5708]
iter 0 loss: 0.008
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.012, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.004, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.006
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.027, -lr * Pred Grad:  0.095, New P: -0.278
-Original Grad: -0.003, -lr * Pred Grad:  -0.100, New P: -0.196
iter 2 loss: 0.003
Actual params: [-0.2776, -0.1964]
-Original Grad: 0.021, -lr * Pred Grad:  0.097, New P: -0.181
-Original Grad: -0.001, -lr * Pred Grad:  -0.085, New P: -0.281
iter 3 loss: 0.002
Actual params: [-0.1808, -0.2809]
-Original Grad: 0.004, -lr * Pred Grad:  0.085, New P: -0.095
-Original Grad: 0.002, -lr * Pred Grad:  -0.042, New P: -0.323
iter 4 loss: 0.002
Actual params: [-0.0953, -0.3232]
-Original Grad: -0.026, -lr * Pred Grad:  0.027, New P: -0.069
-Original Grad: 0.002, -lr * Pred Grad:  -0.014, New P: -0.338
iter 5 loss: 0.003
Actual params: [-0.0687, -0.3377]
-Original Grad: -0.083, -lr * Pred Grad:  -0.035, New P: -0.104
-Original Grad: 0.004, -lr * Pred Grad:  0.021, New P: -0.317
iter 6 loss: 0.002
Actual params: [-0.1037, -0.3172]
-Original Grad: -0.027, -lr * Pred Grad:  -0.043, New P: -0.147
-Original Grad: 0.003, -lr * Pred Grad:  0.036, New P: -0.281
iter 7 loss: 0.002
Actual params: [-0.1471, -0.2814]
-Original Grad: 0.000, -lr * Pred Grad:  -0.038, New P: -0.185
-Original Grad: 0.001, -lr * Pred Grad:  0.038, New P: -0.244
iter 8 loss: 0.002
Actual params: [-0.1851, -0.2435]
-Original Grad: 0.008, -lr * Pred Grad:  -0.030, New P: -0.215
-Original Grad: -0.000, -lr * Pred Grad:  0.033, New P: -0.211
iter 9 loss: 0.002
Actual params: [-0.2151, -0.2109]
-Original Grad: 0.018, -lr * Pred Grad:  -0.018, New P: -0.233
-Original Grad: -0.000, -lr * Pred Grad:  0.029, New P: -0.182
iter 10 loss: 0.002
Actual params: [-0.2328, -0.1818]
-Original Grad: 0.014, -lr * Pred Grad:  -0.009, New P: -0.242
-Original Grad: -0.001, -lr * Pred Grad:  0.019, New P: -0.163
iter 11 loss: 0.003
Actual params: [-0.242 , -0.1625]
-Original Grad: 0.019, -lr * Pred Grad:  0.001, New P: -0.241
-Original Grad: -0.002, -lr * Pred Grad:  0.003, New P: -0.159
iter 12 loss: 0.003
Actual params: [-0.2411, -0.1593]
-Original Grad: 0.020, -lr * Pred Grad:  0.010, New P: -0.231
-Original Grad: -0.000, -lr * Pred Grad:  0.002, New P: -0.157
iter 13 loss: 0.002
Actual params: [-0.2309, -0.1569]
-Original Grad: 0.017, -lr * Pred Grad:  0.017, New P: -0.214
-Original Grad: -0.002, -lr * Pred Grad:  -0.008, New P: -0.165
iter 14 loss: 0.002
Actual params: [-0.2143, -0.1653]
-Original Grad: 0.013, -lr * Pred Grad:  0.021, New P: -0.194
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.175
iter 15 loss: 0.002
Actual params: [-0.1936, -0.1753]
-Original Grad: 0.011, -lr * Pred Grad:  0.024, New P: -0.170
-Original Grad: -0.001, -lr * Pred Grad:  -0.018, New P: -0.193
iter 16 loss: 0.002
Actual params: [-0.17  , -0.1929]
-Original Grad: 0.003, -lr * Pred Grad:  0.023, New P: -0.147
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.210
iter 17 loss: 0.002
Actual params: [-0.1471, -0.2096]
-Original Grad: 0.001, -lr * Pred Grad:  0.021, New P: -0.126
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -0.224
iter 18 loss: 0.002
Actual params: [-0.1256, -0.2245]
-Original Grad: -0.006, -lr * Pred Grad:  0.017, New P: -0.109
-Original Grad: 0.002, -lr * Pred Grad:  -0.002, New P: -0.227
iter 19 loss: 0.002
Actual params: [-0.1089, -0.2267]
-Original Grad: -0.013, -lr * Pred Grad:  0.009, New P: -0.100
-Original Grad: 0.001, -lr * Pred Grad:  0.006, New P: -0.221
iter 20 loss: 0.002
Actual params: [-0.0999, -0.2208]
-Original Grad: -0.016, -lr * Pred Grad:  0.001, New P: -0.099
-Original Grad: 0.003, -lr * Pred Grad:  0.024, New P: -0.197
iter 21 loss: 0.002
Actual params: [-0.0992, -0.1971]
-Original Grad: -0.012, -lr * Pred Grad:  -0.005, New P: -0.104
-Original Grad: 0.003, -lr * Pred Grad:  0.034, New P: -0.163
iter 22 loss: 0.002
Actual params: [-0.1042, -0.1627]
-Original Grad: -0.013, -lr * Pred Grad:  -0.011, New P: -0.115
-Original Grad: 0.002, -lr * Pred Grad:  0.039, New P: -0.123
iter 23 loss: 0.002
Actual params: [-0.115 , -0.1234]
-Original Grad: -0.010, -lr * Pred Grad:  -0.014, New P: -0.129
-Original Grad: 0.001, -lr * Pred Grad:  0.043, New P: -0.080
iter 24 loss: 0.002
Actual params: [-0.1293, -0.0801]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -0.142
-Original Grad: 0.000, -lr * Pred Grad:  0.040, New P: -0.040
iter 25 loss: 0.002
Actual params: [-0.1423, -0.0397]
-Original Grad: 0.006, -lr * Pred Grad:  -0.009, New P: -0.151
-Original Grad: -0.002, -lr * Pred Grad:  0.027, New P: -0.012
iter 26 loss: 0.002
Actual params: [-0.1513, -0.0123]
-Original Grad: 0.010, -lr * Pred Grad:  -0.003, New P: -0.155
-Original Grad: -0.002, -lr * Pred Grad:  0.012, New P: -0.000
iter 27 loss: 0.002
Actual params: [-1.5476e-01, -9.4136e-05]
-Original Grad: 0.012, -lr * Pred Grad:  0.003, New P: -0.152
-Original Grad: -0.003, -lr * Pred Grad:  -0.006, New P: -0.007
iter 28 loss: 0.002
Actual params: [-0.1518, -0.0065]
-Original Grad: 0.015, -lr * Pred Grad:  0.010, New P: -0.142
-Original Grad: -0.004, -lr * Pred Grad:  -0.026, New P: -0.033
iter 29 loss: 0.002
Actual params: [-0.142 , -0.0328]
-Original Grad: 0.004, -lr * Pred Grad:  0.011, New P: -0.131
-Original Grad: -0.002, -lr * Pred Grad:  -0.034, New P: -0.066
iter 30 loss: 0.002
Actual params: [-0.131 , -0.0664]
-Original Grad: 0.001, -lr * Pred Grad:  0.010, New P: -0.121
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -0.099
Target params: [1.3344, 1.5708]
iter 0 loss: 0.293
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.022, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.023, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.288
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.012, -lr * Pred Grad:  -0.095, New P: -0.667
-Original Grad: 0.028, -lr * Pred Grad:  0.100, New P: 0.204
iter 2 loss: 0.285
Actual params: [-0.6671,  0.2036]
-Original Grad: -0.003, -lr * Pred Grad:  -0.080, New P: -0.747
-Original Grad: 0.037, -lr * Pred Grad:  0.100, New P: 0.303
iter 3 loss: 0.282
Actual params: [-0.7467,  0.3033]
-Original Grad: 0.007, -lr * Pred Grad:  -0.046, New P: -0.793
-Original Grad: 0.035, -lr * Pred Grad:  0.100, New P: 0.404
iter 4 loss: 0.279
Actual params: [-0.7927,  0.4035]
-Original Grad: 0.017, -lr * Pred Grad:  -0.002, New P: -0.794
-Original Grad: 0.038, -lr * Pred Grad:  0.101, New P: 0.504
iter 5 loss: 0.276
Actual params: [-0.7943,  0.5043]
-Original Grad: 0.026, -lr * Pred Grad:  0.032, New P: -0.762
-Original Grad: 0.041, -lr * Pred Grad:  0.101, New P: 0.606
iter 6 loss: 0.272
Actual params: [-0.7618,  0.6055]
-Original Grad: 0.024, -lr * Pred Grad:  0.050, New P: -0.712
-Original Grad: 0.029, -lr * Pred Grad:  0.100, New P: 0.706
iter 7 loss: 0.267
Actual params: [-0.7117,  0.7057]
-Original Grad: 0.040, -lr * Pred Grad:  0.066, New P: -0.646
-Original Grad: 0.036, -lr * Pred Grad:  0.100, New P: 0.806
iter 8 loss: 0.261
Actual params: [-0.6458,  0.8061]
-Original Grad: 0.050, -lr * Pred Grad:  0.076, New P: -0.570
-Original Grad: 0.038, -lr * Pred Grad:  0.101, New P: 0.907
iter 9 loss: 0.253
Actual params: [-0.5696,  0.907 ]
-Original Grad: 0.050, -lr * Pred Grad:  0.083, New P: -0.486
-Original Grad: 0.025, -lr * Pred Grad:  0.099, New P: 1.006
iter 10 loss: 0.244
Actual params: [-0.4863,  1.0058]
-Original Grad: 0.095, -lr * Pred Grad:  0.087, New P: -0.399
-Original Grad: 0.030, -lr * Pred Grad:  0.098, New P: 1.104
iter 11 loss: 0.234
Actual params: [-0.3994,  1.1042]
-Original Grad: 0.089, -lr * Pred Grad:  0.092, New P: -0.308
-Original Grad: 0.023, -lr * Pred Grad:  0.096, New P: 1.200
iter 12 loss: 0.223
Actual params: [-0.3077,  1.2005]
-Original Grad: 0.087, -lr * Pred Grad:  0.096, New P: -0.212
-Original Grad: 0.008, -lr * Pred Grad:  0.090, New P: 1.290
iter 13 loss: 0.211
Actual params: [-0.2121,  1.2903]
-Original Grad: 0.111, -lr * Pred Grad:  0.099, New P: -0.113
-Original Grad: -0.004, -lr * Pred Grad:  0.080, New P: 1.370
iter 14 loss: 0.199
Actual params: [-0.1131,  1.3699]
-Original Grad: 0.111, -lr * Pred Grad:  0.102, New P: -0.011
-Original Grad: -0.006, -lr * Pred Grad:  0.069, New P: 1.439
iter 15 loss: 0.183
Actual params: [-0.0113,  1.4392]
-Original Grad: 0.170, -lr * Pred Grad:  0.104, New P: 0.092
-Original Grad: 0.000, -lr * Pred Grad:  0.063, New P: 1.502
iter 16 loss: 0.158
Actual params: [0.0924, 1.5021]
-Original Grad: 0.287, -lr * Pred Grad:  0.102, New P: 0.194
-Original Grad: 0.044, -lr * Pred Grad:  0.071, New P: 1.573
iter 17 loss: 0.126
Actual params: [0.1941, 1.5732]
-Original Grad: 0.291, -lr * Pred Grad:  0.104, New P: 0.298
-Original Grad: 0.022, -lr * Pred Grad:  0.072, New P: 1.645
iter 18 loss: 0.089
Actual params: [0.2982, 1.6455]
-Original Grad: 0.251, -lr * Pred Grad:  0.107, New P: 0.405
-Original Grad: 0.042, -lr * Pred Grad:  0.078, New P: 1.724
iter 19 loss: 0.057
Actual params: [0.4053, 1.7239]
-Original Grad: 0.205, -lr * Pred Grad:  0.109, New P: 0.514
-Original Grad: 0.034, -lr * Pred Grad:  0.082, New P: 1.806
iter 20 loss: 0.033
Actual params: [0.5142, 1.8057]
-Original Grad: 0.111, -lr * Pred Grad:  0.107, New P: 0.621
-Original Grad: -0.010, -lr * Pred Grad:  0.071, New P: 1.876
iter 21 loss: 0.019
Actual params: [0.6209, 1.8762]
-Original Grad: 0.091, -lr * Pred Grad:  0.104, New P: 0.725
-Original Grad: -0.042, -lr * Pred Grad:  0.046, New P: 1.922
iter 22 loss: 0.013
Actual params: [0.7247, 1.9218]
-Original Grad: 0.043, -lr * Pred Grad:  0.098, New P: 0.823
-Original Grad: -0.055, -lr * Pred Grad:  0.020, New P: 1.941
iter 23 loss: 0.011
Actual params: [0.8225, 1.9414]
-Original Grad: 0.023, -lr * Pred Grad:  0.091, New P: 0.914
-Original Grad: -0.054, -lr * Pred Grad:  -0.001, New P: 1.940
iter 24 loss: 0.011
Actual params: [0.9136, 1.9404]
-Original Grad: -0.006, -lr * Pred Grad:  0.082, New P: 0.996
-Original Grad: -0.039, -lr * Pred Grad:  -0.014, New P: 1.927
iter 25 loss: 0.011
Actual params: [0.996 , 1.9267]
-Original Grad: -0.008, -lr * Pred Grad:  0.074, New P: 1.070
-Original Grad: -0.049, -lr * Pred Grad:  -0.028, New P: 1.899
iter 26 loss: 0.011
Actual params: [1.0704, 1.8992]
-Original Grad: -0.025, -lr * Pred Grad:  0.065, New P: 1.136
-Original Grad: -0.038, -lr * Pred Grad:  -0.036, New P: 1.863
iter 27 loss: 0.011
Actual params: [1.1358, 1.8628]
-Original Grad: -0.020, -lr * Pred Grad:  0.058, New P: 1.194
-Original Grad: -0.044, -lr * Pred Grad:  -0.046, New P: 1.817
iter 28 loss: 0.010
Actual params: [1.1936, 1.8173]
-Original Grad: -0.019, -lr * Pred Grad:  0.051, New P: 1.244
-Original Grad: -0.030, -lr * Pred Grad:  -0.050, New P: 1.767
iter 29 loss: 0.010
Actual params: [1.2445, 1.7671]
-Original Grad: -0.025, -lr * Pred Grad:  0.044, New P: 1.288
-Original Grad: -0.025, -lr * Pred Grad:  -0.053, New P: 1.714
iter 30 loss: 0.009
Actual params: [1.2885, 1.714 ]
-Original Grad: -0.012, -lr * Pred Grad:  0.039, New P: 1.327
-Original Grad: -0.017, -lr * Pred Grad:  -0.054, New P: 1.660
Target params: [1.3344, 1.5708]
iter 0 loss: 0.103
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.023, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.015, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.099
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.014, -lr * Pred Grad:  -0.096, New P: -0.668
-Original Grad: 0.007, -lr * Pred Grad:  0.093, New P: 0.197
iter 2 loss: 0.098
Actual params: [-0.668 ,  0.1967]
-Original Grad: -0.008, -lr * Pred Grad:  -0.089, New P: -0.757
-Original Grad: 0.004, -lr * Pred Grad:  0.086, New P: 0.282
iter 3 loss: 0.098
Actual params: [-0.7565,  0.2824]
-Original Grad: -0.003, -lr * Pred Grad:  -0.078, New P: -0.834
-Original Grad: 0.002, -lr * Pred Grad:  0.075, New P: 0.357
iter 4 loss: 0.097
Actual params: [-0.8343,  0.3575]
-Original Grad: -0.002, -lr * Pred Grad:  -0.069, New P: -0.904
-Original Grad: 0.001, -lr * Pred Grad:  0.067, New P: 0.424
iter 5 loss: 0.097
Actual params: [-0.9037,  0.4243]
-Original Grad: -0.001, -lr * Pred Grad:  -0.061, New P: -0.965
-Original Grad: 0.001, -lr * Pred Grad:  0.059, New P: 0.484
iter 6 loss: 0.097
Actual params: [-0.9652,  0.4838]
-Original Grad: -0.001, -lr * Pred Grad:  -0.055, New P: -1.020
-Original Grad: 0.000, -lr * Pred Grad:  0.053, New P: 0.537
iter 7 loss: 0.097
Actual params: [-1.0197,  0.537 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.049, New P: -1.068
-Original Grad: 0.000, -lr * Pred Grad:  0.048, New P: 0.585
iter 8 loss: 0.097
Actual params: [-1.0683,  0.5847]
-Original Grad: -0.000, -lr * Pred Grad:  -0.043, New P: -1.112
-Original Grad: 0.000, -lr * Pred Grad:  0.043, New P: 0.628
iter 9 loss: 0.097
Actual params: [-1.1117,  0.6277]
-Original Grad: -0.000, -lr * Pred Grad:  -0.039, New P: -1.151
-Original Grad: 0.000, -lr * Pred Grad:  0.039, New P: 0.666
iter 10 loss: 0.097
Actual params: [-1.1505,  0.6665]
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -1.185
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: 0.701
iter 11 loss: 0.097
Actual params: [-1.1854,  0.7014]
-Original Grad: 0.000, -lr * Pred Grad:  -0.031, New P: -1.217
-Original Grad: 0.000, -lr * Pred Grad:  0.032, New P: 0.733
iter 12 loss: 0.097
Actual params: [-1.2167,  0.733 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.028, New P: -1.245
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: 0.762
iter 13 loss: 0.097
Actual params: [-1.2449,  0.7616]
-Original Grad: 0.000, -lr * Pred Grad:  -0.025, New P: -1.270
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 0.788
iter 14 loss: 0.097
Actual params: [-1.2703,  0.7876]
-Original Grad: 0.000, -lr * Pred Grad:  -0.023, New P: -1.293
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.811
iter 15 loss: 0.097
Actual params: [-1.2933,  0.8112]
-Original Grad: 0.000, -lr * Pred Grad:  -0.021, New P: -1.314
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.833
iter 16 loss: 0.097
Actual params: [-1.3141,  0.8326]
-Original Grad: 0.000, -lr * Pred Grad:  -0.019, New P: -1.333
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.852
iter 17 loss: 0.097
Actual params: [-1.3329,  0.8521]
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -1.350
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.870
iter 18 loss: 0.097
Actual params: [-1.35  ,  0.8699]
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -1.365
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.886
iter 19 loss: 0.097
Actual params: [-1.3654,  0.8861]
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -1.379
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.901
iter 20 loss: 0.097
Actual params: [-1.3795,  0.9008]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -1.392
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.914
iter 21 loss: 0.097
Actual params: [-1.3922,  0.9143]
-Original Grad: 0.000, -lr * Pred Grad:  -0.012, New P: -1.404
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.927
iter 22 loss: 0.097
Actual params: [-1.4037,  0.9266]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.414
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.938
iter 23 loss: 0.097
Actual params: [-1.4142,  0.9378]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.424
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.948
iter 24 loss: 0.097
Actual params: [-1.4236,  0.948 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.432
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.957
iter 25 loss: 0.097
Actual params: [-1.4322,  0.9573]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.440
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.966
iter 26 loss: 0.097
Actual params: [-1.44  ,  0.9658]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.447
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.974
iter 27 loss: 0.097
Actual params: [-1.4471,  0.9735]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.454
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.981
iter 28 loss: 0.097
Actual params: [-1.4535,  0.9806]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.459
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.987
iter 29 loss: 0.097
Actual params: [-1.4593,  0.9871]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.465
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.993
iter 30 loss: 0.097
Actual params: [-1.4646,  0.9929]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.469
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.998
Target params: [1.3344, 1.5708]
iter 0 loss: 0.017
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.009, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.016
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.006, -lr * Pred Grad:  -0.098, New P: -0.670
-Original Grad: 0.000, -lr * Pred Grad:  0.086, New P: 0.190
iter 2 loss: 0.016
Actual params: [-0.6702,  0.19  ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.086, New P: -0.756
-Original Grad: -0.000, -lr * Pred Grad:  0.065, New P: 0.255
iter 3 loss: 0.015
Actual params: [-0.756 ,  0.2551]
-Original Grad: -0.002, -lr * Pred Grad:  -0.078, New P: -0.834
-Original Grad: 0.000, -lr * Pred Grad:  0.058, New P: 0.313
iter 4 loss: 0.015
Actual params: [-0.8342,  0.3126]
-Original Grad: -0.001, -lr * Pred Grad:  -0.070, New P: -0.904
-Original Grad: 0.000, -lr * Pred Grad:  0.054, New P: 0.366
iter 5 loss: 0.015
Actual params: [-0.9043,  0.3661]
-Original Grad: -0.000, -lr * Pred Grad:  -0.062, New P: -0.966
-Original Grad: 0.000, -lr * Pred Grad:  0.048, New P: 0.415
iter 6 loss: 0.015
Actual params: [-0.966 ,  0.4146]
-Original Grad: -0.000, -lr * Pred Grad:  -0.055, New P: -1.021
-Original Grad: 0.000, -lr * Pred Grad:  0.044, New P: 0.458
iter 7 loss: 0.015
Actual params: [-1.0206,  0.4583]
-Original Grad: -0.000, -lr * Pred Grad:  -0.048, New P: -1.069
-Original Grad: -0.000, -lr * Pred Grad:  0.038, New P: 0.496
iter 8 loss: 0.015
Actual params: [-1.0688,  0.496 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.042, New P: -1.111
-Original Grad: -0.000, -lr * Pred Grad:  0.032, New P: 0.528
iter 9 loss: 0.015
Actual params: [-1.1112,  0.5281]
-Original Grad: 0.000, -lr * Pred Grad:  -0.038, New P: -1.149
-Original Grad: 0.000, -lr * Pred Grad:  0.029, New P: 0.557
iter 10 loss: 0.015
Actual params: [-1.149 ,  0.5571]
-Original Grad: 0.000, -lr * Pred Grad:  -0.034, New P: -1.183
-Original Grad: 0.000, -lr * Pred Grad:  0.026, New P: 0.583
iter 11 loss: 0.015
Actual params: [-1.1828,  0.5833]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -1.213
-Original Grad: 0.000, -lr * Pred Grad:  0.024, New P: 0.607
iter 12 loss: 0.015
Actual params: [-1.2132,  0.6071]
-Original Grad: 0.000, -lr * Pred Grad:  -0.027, New P: -1.241
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.629
iter 13 loss: 0.015
Actual params: [-1.2405,  0.6285]
-Original Grad: 0.000, -lr * Pred Grad:  -0.025, New P: -1.265
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.648
iter 14 loss: 0.015
Actual params: [-1.2652,  0.6481]
-Original Grad: 0.000, -lr * Pred Grad:  -0.022, New P: -1.288
-Original Grad: 0.000, -lr * Pred Grad:  0.018, New P: 0.666
iter 15 loss: 0.015
Actual params: [-1.2875,  0.6661]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.308
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.683
iter 16 loss: 0.015
Actual params: [-1.3077,  0.6827]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.326
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.698
iter 17 loss: 0.015
Actual params: [-1.3261,  0.698 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -1.343
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.712
iter 18 loss: 0.015
Actual params: [-1.3428,  0.7119]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.358
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.725
iter 19 loss: 0.015
Actual params: [-1.358 ,  0.7248]
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -1.372
-Original Grad: -0.000, -lr * Pred Grad:  0.012, New P: 0.736
iter 20 loss: 0.015
Actual params: [-1.3718,  0.7365]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -1.384
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.747
iter 21 loss: 0.015
Actual params: [-1.3844,  0.7471]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.396
-Original Grad: -0.000, -lr * Pred Grad:  0.010, New P: 0.757
iter 22 loss: 0.015
Actual params: [-1.3958,  0.7567]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.406
-Original Grad: -0.000, -lr * Pred Grad:  0.009, New P: 0.765
iter 23 loss: 0.015
Actual params: [-1.4061,  0.7654]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.416
-Original Grad: -0.000, -lr * Pred Grad:  0.008, New P: 0.773
iter 24 loss: 0.015
Actual params: [-1.4156,  0.7732]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.424
-Original Grad: -0.000, -lr * Pred Grad:  0.007, New P: 0.780
iter 25 loss: 0.015
Actual params: [-1.4241,  0.7801]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.432
-Original Grad: -0.000, -lr * Pred Grad:  0.006, New P: 0.786
iter 26 loss: 0.015
Actual params: [-1.4319,  0.7864]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.439
-Original Grad: -0.000, -lr * Pred Grad:  0.006, New P: 0.792
iter 27 loss: 0.015
Actual params: [-1.439,  0.792]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.445
-Original Grad: -0.000, -lr * Pred Grad:  0.005, New P: 0.797
iter 28 loss: 0.015
Actual params: [-1.4454,  0.7971]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.451
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.802
iter 29 loss: 0.015
Actual params: [-1.4513,  0.8017]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.457
-Original Grad: -0.000, -lr * Pred Grad:  0.004, New P: 0.806
iter 30 loss: 0.015
Actual params: [-1.4566,  0.8059]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.461
-Original Grad: -0.000, -lr * Pred Grad:  0.004, New P: 0.810
Target params: [1.3344, 1.5708]
iter 0 loss: 0.095
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.085, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.075, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.087
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.012, -lr * Pred Grad:  -0.077, New P: -0.649
-Original Grad: 0.014, -lr * Pred Grad:  0.079, New P: 0.183
iter 2 loss: 0.084
Actual params: [-0.6493,  0.1826]
-Original Grad: -0.011, -lr * Pred Grad:  -0.067, New P: -0.716
-Original Grad: 0.010, -lr * Pred Grad:  0.069, New P: 0.252
iter 3 loss: 0.083
Actual params: [-0.7162,  0.2516]
-Original Grad: -0.003, -lr * Pred Grad:  -0.057, New P: -0.773
-Original Grad: 0.004, -lr * Pred Grad:  0.059, New P: 0.311
iter 4 loss: 0.082
Actual params: [-0.7733,  0.3107]
-Original Grad: -0.003, -lr * Pred Grad:  -0.050, New P: -0.824
-Original Grad: 0.003, -lr * Pred Grad:  0.052, New P: 0.363
iter 5 loss: 0.082
Actual params: [-0.8236,  0.363 ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.045, New P: -0.869
-Original Grad: 0.002, -lr * Pred Grad:  0.047, New P: 0.410
iter 6 loss: 0.081
Actual params: [-0.8686,  0.4097]
-Original Grad: -0.001, -lr * Pred Grad:  -0.040, New P: -0.909
-Original Grad: 0.001, -lr * Pred Grad:  0.042, New P: 0.451
iter 7 loss: 0.081
Actual params: [-0.9086,  0.4513]
-Original Grad: -0.001, -lr * Pred Grad:  -0.036, New P: -0.945
-Original Grad: 0.001, -lr * Pred Grad:  0.037, New P: 0.488
iter 8 loss: 0.081
Actual params: [-0.9446,  0.4885]
-Original Grad: -0.001, -lr * Pred Grad:  -0.033, New P: -0.977
-Original Grad: 0.001, -lr * Pred Grad:  0.034, New P: 0.522
iter 9 loss: 0.081
Actual params: [-0.9772,  0.522 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.029, New P: -1.007
-Original Grad: 0.001, -lr * Pred Grad:  0.030, New P: 0.552
iter 10 loss: 0.081
Actual params: [-1.0066,  0.5523]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.033
-Original Grad: 0.000, -lr * Pred Grad:  0.027, New P: 0.580
iter 11 loss: 0.081
Actual params: [-1.0332,  0.5797]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.057
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.604
iter 12 loss: 0.081
Actual params: [-1.0574,  0.6044]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.079
-Original Grad: 0.000, -lr * Pred Grad:  0.022, New P: 0.627
iter 13 loss: 0.081
Actual params: [-1.0792,  0.6268]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.099
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.647
iter 14 loss: 0.081
Actual params: [-1.099 ,  0.6473]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.117
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.666
iter 15 loss: 0.081
Actual params: [-1.1171,  0.6658]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.134
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.683
iter 16 loss: 0.081
Actual params: [-1.1335,  0.6827]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.148
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 0.698
iter 17 loss: 0.081
Actual params: [-1.1485,  0.6982]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.162
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.712
iter 18 loss: 0.081
Actual params: [-1.1621,  0.7123]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.174
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.725
iter 19 loss: 0.081
Actual params: [-1.1744,  0.7251]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.186
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.737
iter 20 loss: 0.081
Actual params: [-1.1857,  0.7368]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.196
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.748
iter 21 loss: 0.081
Actual params: [-1.1959,  0.7476]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.205
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.757
iter 22 loss: 0.081
Actual params: [-1.2053,  0.7574]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.214
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.766
iter 23 loss: 0.081
Actual params: [-1.2138,  0.7664]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.222
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.775
iter 24 loss: 0.081
Actual params: [-1.2216,  0.7746]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.229
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.782
iter 25 loss: 0.081
Actual params: [-1.2287,  0.7821]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.235
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.789
iter 26 loss: 0.081
Actual params: [-1.2352,  0.789 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.241
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.795
iter 27 loss: 0.081
Actual params: [-1.241 ,  0.7953]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.246
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.801
iter 28 loss: 0.081
Actual params: [-1.2464,  0.8011]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.251
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.806
iter 29 loss: 0.081
Actual params: [-1.2513,  0.8064]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.256
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.811
iter 30 loss: 0.081
Actual params: [-1.2558,  0.8113]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.260
-Original Grad: 0.000, -lr * Pred Grad:  0.004, New P: 0.816
Target params: [1.3344, 1.5708]
iter 0 loss: 0.432
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.015, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.003, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.431
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.008, -lr * Pred Grad:  -0.094, New P: -0.666
-Original Grad: -0.003, -lr * Pred Grad:  -0.100, New P: -0.196
iter 2 loss: 0.430
Actual params: [-0.6664, -0.1961]
-Original Grad: -0.003, -lr * Pred Grad:  -0.082, New P: -0.749
-Original Grad: -0.002, -lr * Pred Grad:  -0.096, New P: -0.292
iter 3 loss: 0.430
Actual params: [-0.7489, -0.2918]
-Original Grad: -0.001, -lr * Pred Grad:  -0.072, New P: -0.820
-Original Grad: -0.001, -lr * Pred Grad:  -0.087, New P: -0.379
iter 4 loss: 0.430
Actual params: [-0.8204, -0.3789]
-Original Grad: -0.001, -lr * Pred Grad:  -0.062, New P: -0.883
-Original Grad: -0.000, -lr * Pred Grad:  -0.078, New P: -0.457
iter 5 loss: 0.430
Actual params: [-0.8828, -0.4573]
-Original Grad: -0.000, -lr * Pred Grad:  -0.054, New P: -0.937
-Original Grad: -0.000, -lr * Pred Grad:  -0.069, New P: -0.527
iter 6 loss: 0.430
Actual params: [-0.9373, -0.5267]
-Original Grad: -0.000, -lr * Pred Grad:  -0.048, New P: -0.985
-Original Grad: -0.000, -lr * Pred Grad:  -0.062, New P: -0.588
iter 7 loss: 0.430
Actual params: [-0.9853, -0.5884]
-Original Grad: -0.000, -lr * Pred Grad:  -0.043, New P: -1.028
-Original Grad: -0.000, -lr * Pred Grad:  -0.055, New P: -0.643
iter 8 loss: 0.430
Actual params: [-1.0279, -0.6434]
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -1.066
-Original Grad: -0.000, -lr * Pred Grad:  -0.049, New P: -0.692
iter 9 loss: 0.429
Actual params: [-1.0657, -0.6925]
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -1.100
-Original Grad: -0.000, -lr * Pred Grad:  -0.044, New P: -0.736
iter 10 loss: 0.429
Actual params: [-1.0996, -0.7364]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -1.130
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -0.776
iter 11 loss: 0.429
Actual params: [-1.13  , -0.7759]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.157
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -0.812
iter 12 loss: 0.429
Actual params: [-1.1574, -0.8115]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.182
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -0.844
iter 13 loss: 0.429
Actual params: [-1.1821, -0.8437]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.204
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -0.873
iter 14 loss: 0.429
Actual params: [-1.2045, -0.8728]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.225
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -0.899
iter 15 loss: 0.429
Actual params: [-1.2247, -0.8992]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.243
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -0.923
iter 16 loss: 0.429
Actual params: [-1.2431, -0.9232]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.260
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -0.945
iter 17 loss: 0.429
Actual params: [-1.2598, -0.9449]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.275
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -0.965
iter 18 loss: 0.429
Actual params: [-1.275 , -0.9647]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.289
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -0.983
iter 19 loss: 0.429
Actual params: [-1.2887, -0.9827]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.301
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.999
iter 20 loss: 0.429
Actual params: [-1.3013, -0.999 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.313
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.014
iter 21 loss: 0.429
Actual params: [-1.3127, -1.0139]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.323
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.028
iter 22 loss: 0.429
Actual params: [-1.3231, -1.0275]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.333
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.040
iter 23 loss: 0.429
Actual params: [-1.3326, -1.0399]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.341
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.051
iter 24 loss: 0.429
Actual params: [-1.3413, -1.0512]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.349
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.061
iter 25 loss: 0.429
Actual params: [-1.3491, -1.0614]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.356
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.071
iter 26 loss: 0.429
Actual params: [-1.3563, -1.0708]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.363
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.079
iter 27 loss: 0.429
Actual params: [-1.3629, -1.0794]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.369
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.087
iter 28 loss: 0.429
Actual params: [-1.3689, -1.0872]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.374
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.094
iter 29 loss: 0.429
Actual params: [-1.3743, -1.0943]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.379
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.101
iter 30 loss: 0.429
Actual params: [-1.3793, -1.1008]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.384
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.107
Target params: [1.3344, 1.5708]
iter 0 loss: 0.274
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.018, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.256, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.266
Actual params: [-0.5723,  0.1035]
-Original Grad: 0.031, -lr * Pred Grad:  0.030, New P: -0.542
-Original Grad: 0.116, -lr * Pred Grad:  0.092, New P: 0.195
iter 2 loss: 0.244
Actual params: [-0.5422,  0.1953]
-Original Grad: 0.121, -lr * Pred Grad:  0.068, New P: -0.474
-Original Grad: 0.058, -lr * Pred Grad:  0.082, New P: 0.278
iter 3 loss: 0.218
Actual params: [-0.4743,  0.2776]
-Original Grad: 0.220, -lr * Pred Grad:  0.078, New P: -0.396
-Original Grad: 0.135, -lr * Pred Grad:  0.086, New P: 0.363
iter 4 loss: 0.189
Actual params: [-0.3962,  0.3634]
-Original Grad: 0.144, -lr * Pred Grad:  0.084, New P: -0.312
-Original Grad: 0.112, -lr * Pred Grad:  0.087, New P: 0.450
iter 5 loss: 0.156
Actual params: [-0.3119,  0.45  ]
-Original Grad: 0.242, -lr * Pred Grad:  0.089, New P: -0.223
-Original Grad: 0.080, -lr * Pred Grad:  0.085, New P: 0.535
iter 6 loss: 0.124
Actual params: [-0.2226,  0.5346]
-Original Grad: 0.274, -lr * Pred Grad:  0.093, New P: -0.130
-Original Grad: 0.017, -lr * Pred Grad:  0.076, New P: 0.611
iter 7 loss: 0.096
Actual params: [-0.1297,  0.6109]
-Original Grad: 0.295, -lr * Pred Grad:  0.096, New P: -0.034
-Original Grad: -0.034, -lr * Pred Grad:  0.062, New P: 0.673
iter 8 loss: 0.075
Actual params: [-0.0341,  0.6729]
-Original Grad: 0.280, -lr * Pred Grad:  0.098, New P: 0.064
-Original Grad: -0.048, -lr * Pred Grad:  0.048, New P: 0.721
iter 9 loss: 0.058
Actual params: [0.0638, 0.7207]
-Original Grad: 0.089, -lr * Pred Grad:  0.093, New P: 0.157
-Original Grad: 0.000, -lr * Pred Grad:  0.043, New P: 0.763
iter 10 loss: 0.046
Actual params: [0.1571, 0.7634]
-Original Grad: 0.072, -lr * Pred Grad:  0.089, New P: 0.246
-Original Grad: 0.019, -lr * Pred Grad:  0.041, New P: 0.804
iter 11 loss: 0.042
Actual params: [0.2458, 0.8043]
-Original Grad: -0.059, -lr * Pred Grad:  0.075, New P: 0.321
-Original Grad: 0.052, -lr * Pred Grad:  0.043, New P: 0.848
iter 12 loss: 0.040
Actual params: [0.3206, 0.8477]
-Original Grad: 0.016, -lr * Pred Grad:  0.069, New P: 0.389
-Original Grad: 0.054, -lr * Pred Grad:  0.046, New P: 0.894
iter 13 loss: 0.038
Actual params: [0.3893, 0.8936]
-Original Grad: -0.000, -lr * Pred Grad:  0.062, New P: 0.451
-Original Grad: 0.105, -lr * Pred Grad:  0.053, New P: 0.947
iter 14 loss: 0.034
Actual params: [0.4514, 0.947 ]
-Original Grad: -0.035, -lr * Pred Grad:  0.053, New P: 0.505
-Original Grad: 0.091, -lr * Pred Grad:  0.058, New P: 1.005
iter 15 loss: 0.031
Actual params: [0.5048, 1.0055]
-Original Grad: -0.057, -lr * Pred Grad:  0.044, New P: 0.549
-Original Grad: 0.088, -lr * Pred Grad:  0.063, New P: 1.068
iter 16 loss: 0.026
Actual params: [0.5486, 1.0681]
-Original Grad: -0.032, -lr * Pred Grad:  0.037, New P: 0.586
-Original Grad: 0.076, -lr * Pred Grad:  0.065, New P: 1.133
iter 17 loss: 0.022
Actual params: [0.5858, 1.1333]
-Original Grad: 0.007, -lr * Pred Grad:  0.034, New P: 0.620
-Original Grad: 0.052, -lr * Pred Grad:  0.065, New P: 1.198
iter 18 loss: 0.018
Actual params: [0.6201, 1.1983]
-Original Grad: -0.018, -lr * Pred Grad:  0.030, New P: 0.650
-Original Grad: 0.055, -lr * Pred Grad:  0.065, New P: 1.264
iter 19 loss: 0.016
Actual params: [0.6499, 1.2638]
-Original Grad: -0.004, -lr * Pred Grad:  0.027, New P: 0.677
-Original Grad: 0.046, -lr * Pred Grad:  0.065, New P: 1.329
iter 20 loss: 0.014
Actual params: [0.6767, 1.3285]
-Original Grad: 0.009, -lr * Pred Grad:  0.025, New P: 0.702
-Original Grad: 0.022, -lr * Pred Grad:  0.062, New P: 1.390
iter 21 loss: 0.013
Actual params: [0.7018, 1.3901]
-Original Grad: -0.003, -lr * Pred Grad:  0.023, New P: 0.724
-Original Grad: 0.005, -lr * Pred Grad:  0.057, New P: 1.447
iter 22 loss: 0.013
Actual params: [0.7244, 1.4467]
-Original Grad: 0.002, -lr * Pred Grad:  0.021, New P: 0.745
-Original Grad: 0.010, -lr * Pred Grad:  0.053, New P: 1.499
iter 23 loss: 0.014
Actual params: [0.7451, 1.4995]
-Original Grad: -0.006, -lr * Pred Grad:  0.018, New P: 0.763
-Original Grad: -0.000, -lr * Pred Grad:  0.048, New P: 1.548
iter 24 loss: 0.016
Actual params: [0.7635, 1.5475]
-Original Grad: -0.021, -lr * Pred Grad:  0.015, New P: 0.778
-Original Grad: -0.053, -lr * Pred Grad:  0.036, New P: 1.584
iter 25 loss: 0.018
Actual params: [0.7784, 1.584 ]
-Original Grad: 0.006, -lr * Pred Grad:  0.014, New P: 0.792
-Original Grad: -0.030, -lr * Pred Grad:  0.029, New P: 1.613
iter 26 loss: 0.020
Actual params: [0.7925, 1.6132]
-Original Grad: 0.002, -lr * Pred Grad:  0.013, New P: 0.805
-Original Grad: -0.063, -lr * Pred Grad:  0.018, New P: 1.631
iter 27 loss: 0.022
Actual params: [0.8054, 1.6313]
-Original Grad: -0.002, -lr * Pred Grad:  0.012, New P: 0.817
-Original Grad: -0.070, -lr * Pred Grad:  0.007, New P: 1.638
iter 28 loss: 0.022
Actual params: [0.817 , 1.6384]
-Original Grad: -0.006, -lr * Pred Grad:  0.010, New P: 0.827
-Original Grad: -0.094, -lr * Pred Grad:  -0.006, New P: 1.633
iter 29 loss: 0.022
Actual params: [0.827 , 1.6325]
-Original Grad: -0.020, -lr * Pred Grad:  0.007, New P: 0.834
-Original Grad: -0.047, -lr * Pred Grad:  -0.011, New P: 1.621
iter 30 loss: 0.021
Actual params: [0.8343, 1.6211]
-Original Grad: -0.013, -lr * Pred Grad:  0.006, New P: 0.840
-Original Grad: -0.088, -lr * Pred Grad:  -0.022, New P: 1.600
Target params: [1.3344, 1.5708]
iter 0 loss: 0.365
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.019, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.012, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.361
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.008, -lr * Pred Grad:  -0.091, New P: -0.663
-Original Grad: -0.006, -lr * Pred Grad:  -0.094, New P: -0.190
iter 2 loss: 0.358
Actual params: [-0.6634, -0.1904]
-Original Grad: -0.005, -lr * Pred Grad:  -0.084, New P: -0.747
-Original Grad: -0.004, -lr * Pred Grad:  -0.089, New P: -0.279
iter 3 loss: 0.357
Actual params: [-0.7474, -0.2793]
-Original Grad: -0.002, -lr * Pred Grad:  -0.074, New P: -0.822
-Original Grad: -0.002, -lr * Pred Grad:  -0.080, New P: -0.359
iter 4 loss: 0.357
Actual params: [-0.8218, -0.3595]
-Original Grad: -0.001, -lr * Pred Grad:  -0.065, New P: -0.887
-Original Grad: -0.001, -lr * Pred Grad:  -0.070, New P: -0.430
iter 5 loss: 0.357
Actual params: [-0.8866, -0.4297]
-Original Grad: -0.000, -lr * Pred Grad:  -0.057, New P: -0.943
-Original Grad: -0.000, -lr * Pred Grad:  -0.062, New P: -0.491
iter 6 loss: 0.357
Actual params: [-0.9433, -0.4915]
-Original Grad: -0.000, -lr * Pred Grad:  -0.050, New P: -0.993
-Original Grad: -0.000, -lr * Pred Grad:  -0.055, New P: -0.546
iter 7 loss: 0.357
Actual params: [-0.9933, -0.546 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.044, New P: -1.038
-Original Grad: -0.000, -lr * Pred Grad:  -0.048, New P: -0.595
iter 8 loss: 0.357
Actual params: [-1.0376, -0.5945]
-Original Grad: -0.000, -lr * Pred Grad:  -0.039, New P: -1.077
-Original Grad: -0.000, -lr * Pred Grad:  -0.043, New P: -0.638
iter 9 loss: 0.357
Actual params: [-1.077 , -0.6377]
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -1.112
-Original Grad: -0.000, -lr * Pred Grad:  -0.039, New P: -0.676
iter 10 loss: 0.357
Actual params: [-1.1124, -0.6765]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -1.144
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -0.711
iter 11 loss: 0.357
Actual params: [-1.1441, -0.7113]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.173
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -0.743
iter 12 loss: 0.357
Actual params: [-1.1727, -0.7427]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.198
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -0.771
iter 13 loss: 0.357
Actual params: [-1.1985, -0.771 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.222
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -0.797
iter 14 loss: 0.357
Actual params: [-1.2218, -0.7966]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.243
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -0.820
iter 15 loss: 0.357
Actual params: [-1.2429, -0.8198]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.262
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -0.841
iter 16 loss: 0.357
Actual params: [-1.2621, -0.8408]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.280
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -0.860
iter 17 loss: 0.357
Actual params: [-1.2795, -0.8599]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.295
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.877
iter 18 loss: 0.357
Actual params: [-1.2953, -0.8773]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.310
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.893
iter 19 loss: 0.357
Actual params: [-1.3097, -0.8931]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.323
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.907
iter 20 loss: 0.357
Actual params: [-1.3228, -0.9074]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.335
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.921
iter 21 loss: 0.357
Actual params: [-1.3347, -0.9205]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.346
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.932
iter 22 loss: 0.357
Actual params: [-1.3455, -0.9324]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.355
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.943
iter 23 loss: 0.357
Actual params: [-1.3554, -0.9433]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.364
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.953
iter 24 loss: 0.357
Actual params: [-1.3644, -0.9531]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.373
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.962
iter 25 loss: 0.357
Actual params: [-1.3726, -0.9621]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.380
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.970
iter 26 loss: 0.357
Actual params: [-1.3801, -0.9704]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.387
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.978
iter 27 loss: 0.357
Actual params: [-1.3869, -0.9778]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.393
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.985
iter 28 loss: 0.357
Actual params: [-1.3931, -0.9847]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.399
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.991
iter 29 loss: 0.357
Actual params: [-1.3988, -0.9909]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.404
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.997
iter 30 loss: 0.357
Actual params: [-1.4039, -0.9965]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.409
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.002
Target params: [1.3344, 1.5708]
iter 0 loss: 0.104
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.036, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.032, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.098
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.030, -lr * Pred Grad:  0.099, New P: -0.273
-Original Grad: 0.022, -lr * Pred Grad:  0.098, New P: 0.201
iter 2 loss: 0.090
Actual params: [-0.2733,  0.2011]
-Original Grad: 0.044, -lr * Pred Grad:  0.099, New P: -0.174
-Original Grad: 0.027, -lr * Pred Grad:  0.098, New P: 0.300
iter 3 loss: 0.082
Actual params: [-0.1738,  0.2995]
-Original Grad: 0.088, -lr * Pred Grad:  0.095, New P: -0.079
-Original Grad: 0.034, -lr * Pred Grad:  0.099, New P: 0.399
iter 4 loss: 0.073
Actual params: [-0.0788,  0.3989]
-Original Grad: 0.065, -lr * Pred Grad:  0.097, New P: 0.018
-Original Grad: 0.020, -lr * Pred Grad:  0.097, New P: 0.496
iter 5 loss: 0.064
Actual params: [0.0181, 0.4959]
-Original Grad: 0.079, -lr * Pred Grad:  0.099, New P: 0.117
-Original Grad: 0.012, -lr * Pred Grad:  0.092, New P: 0.588
iter 6 loss: 0.056
Actual params: [0.1166, 0.5881]
-Original Grad: 0.066, -lr * Pred Grad:  0.099, New P: 0.216
-Original Grad: 0.004, -lr * Pred Grad:  0.083, New P: 0.672
iter 7 loss: 0.052
Actual params: [0.2158, 0.6716]
-Original Grad: 0.007, -lr * Pred Grad:  0.090, New P: 0.305
-Original Grad: 0.010, -lr * Pred Grad:  0.080, New P: 0.752
iter 8 loss: 0.050
Actual params: [0.3053, 0.752 ]
-Original Grad: -0.002, -lr * Pred Grad:  0.079, New P: 0.384
-Original Grad: 0.010, -lr * Pred Grad:  0.078, New P: 0.830
iter 9 loss: 0.049
Actual params: [0.3843, 0.8303]
-Original Grad: 0.004, -lr * Pred Grad:  0.072, New P: 0.456
-Original Grad: 0.007, -lr * Pred Grad:  0.074, New P: 0.905
iter 10 loss: 0.049
Actual params: [0.4561, 0.9047]
-Original Grad: -0.028, -lr * Pred Grad:  0.055, New P: 0.511
-Original Grad: 0.017, -lr * Pred Grad:  0.077, New P: 0.981
iter 11 loss: 0.049
Actual params: [0.5115, 0.9815]
-Original Grad: -0.009, -lr * Pred Grad:  0.047, New P: 0.559
-Original Grad: 0.011, -lr * Pred Grad:  0.076, New P: 1.057
iter 12 loss: 0.049
Actual params: [0.5587, 1.0572]
-Original Grad: 0.011, -lr * Pred Grad:  0.046, New P: 0.604
-Original Grad: -0.014, -lr * Pred Grad:  0.057, New P: 1.114
iter 13 loss: 0.047
Actual params: [0.6042, 1.1143]
-Original Grad: 0.018, -lr * Pred Grad:  0.046, New P: 0.650
-Original Grad: 0.013, -lr * Pred Grad:  0.060, New P: 1.174
iter 14 loss: 0.045
Actual params: [0.6504, 1.1741]
-Original Grad: 0.035, -lr * Pred Grad:  0.051, New P: 0.701
-Original Grad: 0.009, -lr * Pred Grad:  0.060, New P: 1.234
iter 15 loss: 0.043
Actual params: [0.7014, 1.2339]
-Original Grad: 0.052, -lr * Pred Grad:  0.058, New P: 0.760
-Original Grad: 0.001, -lr * Pred Grad:  0.055, New P: 1.289
iter 16 loss: 0.039
Actual params: [0.7597, 1.2887]
-Original Grad: 0.076, -lr * Pred Grad:  0.068, New P: 0.828
-Original Grad: -0.009, -lr * Pred Grad:  0.043, New P: 1.331
iter 17 loss: 0.035
Actual params: [0.8277, 1.3314]
-Original Grad: 0.086, -lr * Pred Grad:  0.077, New P: 0.904
-Original Grad: -0.017, -lr * Pred Grad:  0.026, New P: 1.358
iter 18 loss: 0.029
Actual params: [0.9043, 1.3576]
-Original Grad: 0.088, -lr * Pred Grad:  0.084, New P: 0.988
-Original Grad: -0.035, -lr * Pred Grad:  -0.000, New P: 1.357
iter 19 loss: 0.023
Actual params: [0.9879, 1.3573]
-Original Grad: 0.061, -lr * Pred Grad:  0.087, New P: 1.074
-Original Grad: -0.016, -lr * Pred Grad:  -0.010, New P: 1.347
iter 20 loss: 0.019
Actual params: [1.0745, 1.3472]
-Original Grad: 0.030, -lr * Pred Grad:  0.084, New P: 1.159
-Original Grad: -0.018, -lr * Pred Grad:  -0.020, New P: 1.327
iter 21 loss: 0.016
Actual params: [1.1589, 1.3273]
-Original Grad: 0.004, -lr * Pred Grad:  0.078, New P: 1.237
-Original Grad: -0.030, -lr * Pred Grad:  -0.034, New P: 1.293
iter 22 loss: 0.015
Actual params: [1.2368, 1.2931]
-Original Grad: -0.007, -lr * Pred Grad:  0.069, New P: 1.306
-Original Grad: -0.017, -lr * Pred Grad:  -0.040, New P: 1.253
iter 23 loss: 0.015
Actual params: [1.306 , 1.2528]
-Original Grad: -0.022, -lr * Pred Grad:  0.058, New P: 1.364
-Original Grad: -0.020, -lr * Pred Grad:  -0.047, New P: 1.206
iter 24 loss: 0.015
Actual params: [1.364 , 1.2057]
-Original Grad: -0.007, -lr * Pred Grad:  0.051, New P: 1.415
-Original Grad: -0.021, -lr * Pred Grad:  -0.054, New P: 1.152
iter 25 loss: 0.014
Actual params: [1.4152, 1.1521]
-Original Grad: -0.013, -lr * Pred Grad:  0.044, New P: 1.459
-Original Grad: -0.016, -lr * Pred Grad:  -0.057, New P: 1.095
iter 26 loss: 0.013
Actual params: [1.4587, 1.0953]
-Original Grad: -0.006, -lr * Pred Grad:  0.038, New P: 1.497
-Original Grad: -0.016, -lr * Pred Grad:  -0.060, New P: 1.035
iter 27 loss: 0.013
Actual params: [1.497 , 1.0351]
-Original Grad: -0.007, -lr * Pred Grad:  0.033, New P: 1.530
-Original Grad: -0.004, -lr * Pred Grad:  -0.057, New P: 0.978
iter 28 loss: 0.013
Actual params: [1.5302, 0.9778]
-Original Grad: -0.003, -lr * Pred Grad:  0.030, New P: 1.560
-Original Grad: 0.007, -lr * Pred Grad:  -0.048, New P: 0.930
iter 29 loss: 0.013
Actual params: [1.5598, 0.9295]
-Original Grad: -0.001, -lr * Pred Grad:  0.027, New P: 1.587
-Original Grad: 0.009, -lr * Pred Grad:  -0.038, New P: 0.891
iter 30 loss: 0.014
Actual params: [1.5866, 0.8911]
-Original Grad: -0.000, -lr * Pred Grad:  0.024, New P: 1.611
-Original Grad: 0.014, -lr * Pred Grad:  -0.027, New P: 0.864
Target params: [1.3344, 1.5708]
iter 0 loss: 0.044
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.006, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.043
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.004, -lr * Pred Grad:  -0.098, New P: -0.670
-Original Grad: 0.000, -lr * Pred Grad:  0.095, New P: 0.198
iter 2 loss: 0.043
Actual params: [-0.6699,  0.198 ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.094, New P: -0.764
-Original Grad: -0.000, -lr * Pred Grad:  0.066, New P: 0.264
iter 3 loss: 0.042
Actual params: [-0.7644,  0.2644]
-Original Grad: -0.001, -lr * Pred Grad:  -0.085, New P: -0.849
-Original Grad: 0.000, -lr * Pred Grad:  0.060, New P: 0.325
iter 4 loss: 0.042
Actual params: [-0.8489,  0.3248]
-Original Grad: -0.001, -lr * Pred Grad:  -0.076, New P: -0.925
-Original Grad: 0.000, -lr * Pred Grad:  0.069, New P: 0.394
iter 5 loss: 0.042
Actual params: [-0.9252,  0.3939]
-Original Grad: -0.000, -lr * Pred Grad:  -0.069, New P: -0.994
-Original Grad: 0.000, -lr * Pred Grad:  0.075, New P: 0.469
iter 6 loss: 0.042
Actual params: [-0.9939,  0.4688]
-Original Grad: -0.000, -lr * Pred Grad:  -0.063, New P: -1.057
-Original Grad: 0.000, -lr * Pred Grad:  0.079, New P: 0.548
iter 7 loss: 0.042
Actual params: [-1.0565,  0.5483]
-Original Grad: -0.000, -lr * Pred Grad:  -0.056, New P: -1.113
-Original Grad: 0.000, -lr * Pred Grad:  0.079, New P: 0.627
iter 8 loss: 0.042
Actual params: [-1.1129,  0.627 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.051, New P: -1.164
-Original Grad: 0.000, -lr * Pred Grad:  0.076, New P: 0.703
iter 9 loss: 0.042
Actual params: [-1.1635,  0.7029]
-Original Grad: -0.000, -lr * Pred Grad:  -0.046, New P: -1.209
-Original Grad: 0.000, -lr * Pred Grad:  0.073, New P: 0.776
iter 10 loss: 0.042
Actual params: [-1.2092,  0.7757]
-Original Grad: -0.000, -lr * Pred Grad:  -0.041, New P: -1.250
-Original Grad: 0.000, -lr * Pred Grad:  0.069, New P: 0.844
iter 11 loss: 0.042
Actual params: [-1.2503,  0.8444]
-Original Grad: -0.000, -lr * Pred Grad:  -0.037, New P: -1.287
-Original Grad: 0.000, -lr * Pred Grad:  0.063, New P: 0.907
iter 12 loss: 0.042
Actual params: [-1.2874,  0.9075]
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -1.321
-Original Grad: 0.000, -lr * Pred Grad:  0.058, New P: 0.966
iter 13 loss: 0.042
Actual params: [-1.3209,  0.9655]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -1.351
-Original Grad: 0.000, -lr * Pred Grad:  0.053, New P: 1.019
iter 14 loss: 0.042
Actual params: [-1.3512,  1.0188]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.379
-Original Grad: 0.000, -lr * Pred Grad:  0.049, New P: 1.067
iter 15 loss: 0.042
Actual params: [-1.3786,  1.0673]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.404
-Original Grad: 0.000, -lr * Pred Grad:  0.044, New P: 1.112
iter 16 loss: 0.042
Actual params: [-1.4035,  1.1116]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.426
-Original Grad: 0.000, -lr * Pred Grad:  0.040, New P: 1.152
iter 17 loss: 0.042
Actual params: [-1.4261,  1.1519]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.447
-Original Grad: 0.000, -lr * Pred Grad:  0.037, New P: 1.189
iter 18 loss: 0.042
Actual params: [-1.4466,  1.1886]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.465
-Original Grad: 0.000, -lr * Pred Grad:  0.033, New P: 1.222
iter 19 loss: 0.042
Actual params: [-1.4652,  1.2221]
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -1.482
-Original Grad: 0.000, -lr * Pred Grad:  0.031, New P: 1.253
iter 20 loss: 0.042
Actual params: [-1.4821,  1.2526]
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -1.498
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 1.280
iter 21 loss: 0.042
Actual params: [-1.4976,  1.2804]
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -1.512
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 1.306
iter 22 loss: 0.042
Actual params: [-1.5116,  1.3057]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -1.524
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 1.329
iter 23 loss: 0.042
Actual params: [-1.5244,  1.3288]
-Original Grad: 0.000, -lr * Pred Grad:  -0.012, New P: -1.536
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 1.350
iter 24 loss: 0.042
Actual params: [-1.536 ,  1.3499]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.547
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 1.369
iter 25 loss: 0.042
Actual params: [-1.5466,  1.3691]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.556
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 1.387
iter 26 loss: 0.042
Actual params: [-1.5562,  1.3866]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.565
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 1.403
iter 27 loss: 0.042
Actual params: [-1.565 ,  1.4025]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.573
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 1.417
iter 28 loss: 0.042
Actual params: [-1.573 ,  1.4171]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.580
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 1.430
iter 29 loss: 0.042
Actual params: [-1.5803,  1.4303]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.587
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 1.442
iter 30 loss: 0.042
Actual params: [-1.587 ,  1.4424]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.593
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 1.453
Target params: [1.3344, 1.5708]
iter 0 loss: 0.170
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.090, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.010, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.158
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.072, -lr * Pred Grad:  0.099, New P: -0.273
-Original Grad: 0.011, -lr * Pred Grad:  0.011, New P: -0.086
iter 2 loss: 0.145
Actual params: [-0.2735, -0.0855]
-Original Grad: 0.129, -lr * Pred Grad:  0.099, New P: -0.175
-Original Grad: 0.023, -lr * Pred Grad:  0.058, New P: -0.028
iter 3 loss: 0.130
Actual params: [-0.175 , -0.0276]
-Original Grad: 0.126, -lr * Pred Grad:  0.099, New P: -0.075
-Original Grad: 0.022, -lr * Pred Grad:  0.073, New P: 0.046
iter 4 loss: 0.114
Actual params: [-0.0755,  0.0458]
-Original Grad: 0.120, -lr * Pred Grad:  0.100, New P: 0.025
-Original Grad: 0.005, -lr * Pred Grad:  0.069, New P: 0.115
iter 5 loss: 0.097
Actual params: [0.0245, 0.1147]
-Original Grad: 0.114, -lr * Pred Grad:  0.100, New P: 0.125
-Original Grad: -0.008, -lr * Pred Grad:  0.047, New P: 0.162
iter 6 loss: 0.076
Actual params: [0.1247, 0.1618]
-Original Grad: 0.176, -lr * Pred Grad:  0.101, New P: 0.226
-Original Grad: 0.051, -lr * Pred Grad:  0.065, New P: 0.227
iter 7 loss: 0.047
Actual params: [0.2256, 0.2269]
-Original Grad: 0.169, -lr * Pred Grad:  0.102, New P: 0.327
-Original Grad: 0.055, -lr * Pred Grad:  0.076, New P: 0.303
iter 8 loss: 0.022
Actual params: [0.3275, 0.3027]
-Original Grad: 0.113, -lr * Pred Grad:  0.101, New P: 0.428
-Original Grad: 0.037, -lr * Pred Grad:  0.081, New P: 0.384
iter 9 loss: 0.007
Actual params: [0.4283, 0.3841]
-Original Grad: 0.033, -lr * Pred Grad:  0.094, New P: 0.522
-Original Grad: 0.003, -lr * Pred Grad:  0.074, New P: 0.458
iter 10 loss: 0.006
Actual params: [0.5221, 0.4583]
-Original Grad: -0.001, -lr * Pred Grad:  0.084, New P: 0.606
-Original Grad: -0.021, -lr * Pred Grad:  0.054, New P: 0.512
iter 11 loss: 0.013
Actual params: [0.606 , 0.5123]
-Original Grad: -0.054, -lr * Pred Grad:  0.068, New P: 0.674
-Original Grad: -0.066, -lr * Pred Grad:  0.012, New P: 0.524
iter 12 loss: 0.020
Actual params: [0.674, 0.524]
-Original Grad: -0.031, -lr * Pred Grad:  0.057, New P: 0.731
-Original Grad: -0.061, -lr * Pred Grad:  -0.014, New P: 0.510
iter 13 loss: 0.024
Actual params: [0.7312, 0.5104]
-Original Grad: -0.089, -lr * Pred Grad:  0.039, New P: 0.771
-Original Grad: -0.087, -lr * Pred Grad:  -0.037, New P: 0.473
iter 14 loss: 0.024
Actual params: [0.7707, 0.4732]
-Original Grad: -0.091, -lr * Pred Grad:  0.024, New P: 0.795
-Original Grad: -0.077, -lr * Pred Grad:  -0.052, New P: 0.422
iter 15 loss: 0.021
Actual params: [0.7947, 0.4215]
-Original Grad: -0.079, -lr * Pred Grad:  0.012, New P: 0.807
-Original Grad: -0.068, -lr * Pred Grad:  -0.062, New P: 0.360
iter 16 loss: 0.017
Actual params: [0.8067, 0.36  ]
-Original Grad: -0.059, -lr * Pred Grad:  0.004, New P: 0.811
-Original Grad: -0.077, -lr * Pred Grad:  -0.070, New P: 0.290
iter 17 loss: 0.013
Actual params: [0.8106, 0.2896]
-Original Grad: -0.067, -lr * Pred Grad:  -0.004, New P: 0.806
-Original Grad: -0.038, -lr * Pred Grad:  -0.072, New P: 0.218
iter 18 loss: 0.010
Actual params: [0.8062, 0.2177]
-Original Grad: -0.078, -lr * Pred Grad:  -0.013, New P: 0.793
-Original Grad: -0.026, -lr * Pred Grad:  -0.071, New P: 0.146
iter 19 loss: 0.008
Actual params: [0.7932, 0.1465]
-Original Grad: -0.033, -lr * Pred Grad:  -0.016, New P: 0.778
-Original Grad: -0.000, -lr * Pred Grad:  -0.065, New P: 0.082
iter 20 loss: 0.008
Actual params: [0.7776, 0.0817]
-Original Grad: -0.035, -lr * Pred Grad:  -0.018, New P: 0.759
-Original Grad: -0.009, -lr * Pred Grad:  -0.061, New P: 0.021
iter 21 loss: 0.009
Actual params: [0.7592, 0.0205]
-Original Grad: -0.016, -lr * Pred Grad:  -0.019, New P: 0.741
-Original Grad: 0.032, -lr * Pred Grad:  -0.047, New P: -0.027
iter 22 loss: 0.011
Actual params: [ 0.7407, -0.0266]
-Original Grad: -0.001, -lr * Pred Grad:  -0.017, New P: 0.724
-Original Grad: 0.069, -lr * Pred Grad:  -0.024, New P: -0.051
iter 23 loss: 0.013
Actual params: [ 0.7236, -0.0511]
-Original Grad: 0.011, -lr * Pred Grad:  -0.014, New P: 0.710
-Original Grad: 0.060, -lr * Pred Grad:  -0.008, New P: -0.059
iter 24 loss: 0.013
Actual params: [ 0.7095, -0.0587]
-Original Grad: 0.015, -lr * Pred Grad:  -0.011, New P: 0.699
-Original Grad: 0.064, -lr * Pred Grad:  0.008, New P: -0.051
iter 25 loss: 0.013
Actual params: [ 0.6985, -0.0508]
-Original Grad: 0.015, -lr * Pred Grad:  -0.008, New P: 0.690
-Original Grad: 0.041, -lr * Pred Grad:  0.016, New P: -0.034
iter 26 loss: 0.012
Actual params: [ 0.6904, -0.0344]
-Original Grad: 0.022, -lr * Pred Grad:  -0.005, New P: 0.686
-Original Grad: 0.057, -lr * Pred Grad:  0.027, New P: -0.007
iter 27 loss: 0.011
Actual params: [ 0.6858, -0.0072]
-Original Grad: 0.010, -lr * Pred Grad:  -0.003, New P: 0.683
-Original Grad: 0.038, -lr * Pred Grad:  0.033, New P: 0.026
iter 28 loss: 0.009
Actual params: [0.6829, 0.0257]
-Original Grad: 0.013, -lr * Pred Grad:  -0.001, New P: 0.682
-Original Grad: 0.038, -lr * Pred Grad:  0.038, New P: 0.064
iter 29 loss: 0.007
Actual params: [0.6819, 0.0638]
-Original Grad: 0.007, -lr * Pred Grad:  0.000, New P: 0.682
-Original Grad: 0.013, -lr * Pred Grad:  0.038, New P: 0.101
iter 30 loss: 0.006
Actual params: [0.6819, 0.1013]
-Original Grad: 0.007, -lr * Pred Grad:  0.001, New P: 0.683
-Original Grad: 0.017, -lr * Pred Grad:  0.038, New P: 0.139
Target params: [1.3344, 1.5708]
iter 0 loss: 0.591
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.043, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.584
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.040, -lr * Pred Grad:  0.100, New P: -0.273
-Original Grad: 0.005, -lr * Pred Grad:  0.053, New P: -0.044
iter 2 loss: 0.574
Actual params: [-0.2726, -0.0438]
-Original Grad: 0.051, -lr * Pred Grad:  0.100, New P: -0.173
-Original Grad: 0.003, -lr * Pred Grad:  0.068, New P: 0.024
iter 3 loss: 0.562
Actual params: [-0.1725,  0.0237]
-Original Grad: 0.062, -lr * Pred Grad:  0.100, New P: -0.072
-Original Grad: -0.002, -lr * Pred Grad:  0.032, New P: 0.055
iter 4 loss: 0.548
Actual params: [-0.0722,  0.0553]
-Original Grad: 0.112, -lr * Pred Grad:  0.097, New P: 0.025
-Original Grad: -0.007, -lr * Pred Grad:  -0.021, New P: 0.034
iter 5 loss: 0.529
Actual params: [0.0249, 0.0343]
-Original Grad: 0.099, -lr * Pred Grad:  0.099, New P: 0.124
-Original Grad: -0.006, -lr * Pred Grad:  -0.043, New P: -0.008
iter 6 loss: 0.507
Actual params: [ 0.1235, -0.0082]
-Original Grad: 0.221, -lr * Pred Grad:  0.094, New P: 0.217
-Original Grad: -0.006, -lr * Pred Grad:  -0.057, New P: -0.065
iter 7 loss: 0.478
Actual params: [ 0.2171, -0.0649]
-Original Grad: 0.167, -lr * Pred Grad:  0.096, New P: 0.313
-Original Grad: 0.015, -lr * Pred Grad:  0.007, New P: -0.057
iter 8 loss: 0.444
Actual params: [ 0.3133, -0.0575]
-Original Grad: 0.225, -lr * Pred Grad:  0.098, New P: 0.411
-Original Grad: 0.037, -lr * Pred Grad:  0.046, New P: -0.011
iter 9 loss: 0.405
Actual params: [ 0.4114, -0.011 ]
-Original Grad: 0.171, -lr * Pred Grad:  0.100, New P: 0.511
-Original Grad: 0.029, -lr * Pred Grad:  0.062, New P: 0.051
iter 10 loss: 0.361
Actual params: [0.511 , 0.0506]
-Original Grad: 0.193, -lr * Pred Grad:  0.101, New P: 0.612
-Original Grad: 0.042, -lr * Pred Grad:  0.073, New P: 0.124
iter 11 loss: 0.316
Actual params: [0.6121, 0.1239]
-Original Grad: 0.281, -lr * Pred Grad:  0.103, New P: 0.715
-Original Grad: 0.138, -lr * Pred Grad:  0.072, New P: 0.196
iter 12 loss: 0.275
Actual params: [0.7149, 0.1958]
-Original Grad: 0.158, -lr * Pred Grad:  0.102, New P: 0.817
-Original Grad: 0.045, -lr * Pred Grad:  0.076, New P: 0.272
iter 13 loss: 0.235
Actual params: [0.8172, 0.2716]
-Original Grad: 0.077, -lr * Pred Grad:  0.098, New P: 0.915
-Original Grad: 0.075, -lr * Pred Grad:  0.083, New P: 0.354
iter 14 loss: 0.201
Actual params: [0.9153, 0.3541]
-Original Grad: 0.114, -lr * Pred Grad:  0.096, New P: 1.012
-Original Grad: 0.070, -lr * Pred Grad:  0.087, New P: 0.442
iter 15 loss: 0.174
Actual params: [1.0117, 0.4416]
-Original Grad: 0.035, -lr * Pred Grad:  0.090, New P: 1.102
-Original Grad: 0.098, -lr * Pred Grad:  0.093, New P: 0.534
iter 16 loss: 0.153
Actual params: [1.1018, 0.5344]
-Original Grad: 0.023, -lr * Pred Grad:  0.084, New P: 1.185
-Original Grad: 0.073, -lr * Pred Grad:  0.096, New P: 0.630
iter 17 loss: 0.140
Actual params: [1.1855, 0.63  ]
-Original Grad: 0.030, -lr * Pred Grad:  0.078, New P: 1.264
-Original Grad: 0.055, -lr * Pred Grad:  0.096, New P: 0.726
iter 18 loss: 0.132
Actual params: [1.2639, 0.7262]
-Original Grad: -0.093, -lr * Pred Grad:  0.063, New P: 1.326
-Original Grad: 0.110, -lr * Pred Grad:  0.101, New P: 0.827
iter 19 loss: 0.127
Actual params: [1.3265, 0.8267]
-Original Grad: -0.121, -lr * Pred Grad:  0.046, New P: 1.372
-Original Grad: 0.081, -lr * Pred Grad:  0.102, New P: 0.929
iter 20 loss: 0.121
Actual params: [1.3722, 0.9292]
-Original Grad: -0.128, -lr * Pred Grad:  0.030, New P: 1.402
-Original Grad: 0.069, -lr * Pred Grad:  0.103, New P: 1.032
iter 21 loss: 0.113
Actual params: [1.4023, 1.0322]
-Original Grad: -0.068, -lr * Pred Grad:  0.022, New P: 1.424
-Original Grad: 0.041, -lr * Pred Grad:  0.100, New P: 1.132
iter 22 loss: 0.104
Actual params: [1.424 , 1.1325]
-Original Grad: -0.070, -lr * Pred Grad:  0.014, New P: 1.438
-Original Grad: 0.073, -lr * Pred Grad:  0.102, New P: 1.234
iter 23 loss: 0.100
Actual params: [1.4377, 1.234 ]
-Original Grad: -0.002, -lr * Pred Grad:  0.012, New P: 1.450
-Original Grad: -0.025, -lr * Pred Grad:  0.088, New P: 1.322
iter 24 loss: 0.104
Actual params: [1.4501, 1.3216]
-Original Grad: -0.004, -lr * Pred Grad:  0.011, New P: 1.461
-Original Grad: -0.054, -lr * Pred Grad:  0.069, New P: 1.390
iter 25 loss: 0.116
Actual params: [1.461 , 1.3902]
-Original Grad: -0.060, -lr * Pred Grad:  0.005, New P: 1.466
-Original Grad: -0.172, -lr * Pred Grad:  0.027, New P: 1.417
iter 26 loss: 0.123
Actual params: [1.4658, 1.4169]
-Original Grad: -0.078, -lr * Pred Grad:  -0.002, New P: 1.463
-Original Grad: -0.111, -lr * Pred Grad:  0.006, New P: 1.423
iter 27 loss: 0.125
Actual params: [1.4634, 1.423 ]
-Original Grad: -0.024, -lr * Pred Grad:  -0.004, New P: 1.459
-Original Grad: -0.206, -lr * Pred Grad:  -0.023, New P: 1.400
iter 28 loss: 0.118
Actual params: [1.4591, 1.4002]
-Original Grad: -0.038, -lr * Pred Grad:  -0.007, New P: 1.452
-Original Grad: -0.105, -lr * Pred Grad:  -0.034, New P: 1.366
iter 29 loss: 0.111
Actual params: [1.4519, 1.3662]
-Original Grad: -0.072, -lr * Pred Grad:  -0.013, New P: 1.439
-Original Grad: -0.048, -lr * Pred Grad:  -0.037, New P: 1.329
iter 30 loss: 0.105
Actual params: [1.439 , 1.3291]
-Original Grad: -0.038, -lr * Pred Grad:  -0.015, New P: 1.424
-Original Grad: -0.058, -lr * Pred Grad:  -0.041, New P: 1.288
Target params: [1.3344, 1.5708]
iter 0 loss: 0.746
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.002, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.000, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.746
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.004, -lr * Pred Grad:  0.095, New P: -0.278
-Original Grad: -0.000, -lr * Pred Grad:  -0.091, New P: -0.188
iter 2 loss: 0.745
Actual params: [-0.2776, -0.1876]
-Original Grad: 0.011, -lr * Pred Grad:  0.086, New P: -0.192
-Original Grad: 0.002, -lr * Pred Grad:  0.053, New P: -0.134
iter 3 loss: 0.744
Actual params: [-0.1915, -0.1341]
-Original Grad: 0.016, -lr * Pred Grad:  0.089, New P: -0.103
-Original Grad: 0.001, -lr * Pred Grad:  0.070, New P: -0.064
iter 4 loss: 0.742
Actual params: [-0.1028, -0.0637]
-Original Grad: 0.032, -lr * Pred Grad:  0.086, New P: -0.017
-Original Grad: 0.001, -lr * Pred Grad:  0.079, New P: 0.015
iter 5 loss: 0.738
Actual params: [-0.0171,  0.0152]
-Original Grad: 0.059, -lr * Pred Grad:  0.084, New P: 0.067
-Original Grad: -0.000, -lr * Pred Grad:  0.068, New P: 0.083
iter 6 loss: 0.733
Actual params: [0.0668, 0.083 ]
-Original Grad: 0.083, -lr * Pred Grad:  0.086, New P: 0.153
-Original Grad: -0.001, -lr * Pred Grad:  0.041, New P: 0.124
iter 7 loss: 0.724
Actual params: [0.1528, 0.1244]
-Original Grad: 0.121, -lr * Pred Grad:  0.087, New P: 0.240
-Original Grad: 0.012, -lr * Pred Grad:  0.057, New P: 0.181
iter 8 loss: 0.713
Actual params: [0.2402, 0.1811]
-Original Grad: 0.136, -lr * Pred Grad:  0.091, New P: 0.331
-Original Grad: 0.017, -lr * Pred Grad:  0.069, New P: 0.250
iter 9 loss: 0.696
Actual params: [0.3311, 0.2502]
-Original Grad: 0.208, -lr * Pred Grad:  0.092, New P: 0.423
-Original Grad: 0.035, -lr * Pred Grad:  0.073, New P: 0.323
iter 10 loss: 0.670
Actual params: [0.423 , 0.3233]
-Original Grad: 0.302, -lr * Pred Grad:  0.092, New P: 0.515
-Original Grad: 0.052, -lr * Pred Grad:  0.078, New P: 0.402
iter 11 loss: 0.627
Actual params: [0.5151, 0.4017]
-Original Grad: 0.546, -lr * Pred Grad:  0.089, New P: 0.604
-Original Grad: 0.064, -lr * Pred Grad:  0.084, New P: 0.486
iter 12 loss: 0.564
Actual params: [0.6038, 0.4856]
-Original Grad: 0.914, -lr * Pred Grad:  0.087, New P: 0.691
-Original Grad: 0.207, -lr * Pred Grad:  0.075, New P: 0.560
iter 13 loss: 0.492
Actual params: [0.6905, 0.5603]
-Original Grad: 0.567, -lr * Pred Grad:  0.092, New P: 0.782
-Original Grad: 0.205, -lr * Pred Grad:  0.082, New P: 0.643
iter 14 loss: 0.426
Actual params: [0.7822, 0.6428]
-Original Grad: 0.423, -lr * Pred Grad:  0.094, New P: 0.876
-Original Grad: 0.232, -lr * Pred Grad:  0.089, New P: 0.732
iter 15 loss: 0.371
Actual params: [0.8762, 0.7316]
-Original Grad: 0.554, -lr * Pred Grad:  0.097, New P: 0.974
-Original Grad: 0.276, -lr * Pred Grad:  0.094, New P: 0.826
iter 16 loss: 0.315
Actual params: [0.9736, 0.8255]
-Original Grad: 0.348, -lr * Pred Grad:  0.097, New P: 1.071
-Original Grad: 0.331, -lr * Pred Grad:  0.098, New P: 0.924
iter 17 loss: 0.260
Actual params: [1.0711, 0.9236]
-Original Grad: 0.297, -lr * Pred Grad:  0.096, New P: 1.168
-Original Grad: 0.202, -lr * Pred Grad:  0.100, New P: 1.024
iter 18 loss: 0.215
Actual params: [1.1675, 1.0241]
-Original Grad: 0.089, -lr * Pred Grad:  0.090, New P: 1.258
-Original Grad: 0.237, -lr * Pred Grad:  0.103, New P: 1.127
iter 19 loss: 0.189
Actual params: [1.258 , 1.1274]
-Original Grad: -0.057, -lr * Pred Grad:  0.080, New P: 1.338
-Original Grad: 0.198, -lr * Pred Grad:  0.105, New P: 1.232
iter 20 loss: 0.180
Actual params: [1.3383, 1.232 ]
-Original Grad: -0.177, -lr * Pred Grad:  0.067, New P: 1.405
-Original Grad: 0.179, -lr * Pred Grad:  0.105, New P: 1.337
iter 21 loss: 0.181
Actual params: [1.405, 1.337]
-Original Grad: -0.324, -lr * Pred Grad:  0.049, New P: 1.454
-Original Grad: 0.120, -lr * Pred Grad:  0.103, New P: 1.440
iter 22 loss: 0.186
Actual params: [1.4536, 1.4399]
-Original Grad: -0.329, -lr * Pred Grad:  0.033, New P: 1.486
-Original Grad: 0.015, -lr * Pred Grad:  0.095, New P: 1.535
iter 23 loss: 0.185
Actual params: [1.4862, 1.5348]
-Original Grad: -0.339, -lr * Pred Grad:  0.018, New P: 1.504
-Original Grad: 0.427, -lr * Pred Grad:  0.101, New P: 1.636
iter 24 loss: 0.146
Actual params: [1.5043, 1.6361]
-Original Grad: -0.331, -lr * Pred Grad:  0.006, New P: 1.510
-Original Grad: 0.477, -lr * Pred Grad:  0.107, New P: 1.743
iter 25 loss: 0.094
Actual params: [1.5098, 1.7427]
-Original Grad: -0.499, -lr * Pred Grad:  -0.011, New P: 1.499
-Original Grad: 0.470, -lr * Pred Grad:  0.111, New P: 1.854
iter 26 loss: 0.058
Actual params: [1.4991, 1.8538]
-Original Grad: -0.143, -lr * Pred Grad:  -0.014, New P: 1.485
-Original Grad: 0.202, -lr * Pred Grad:  0.110, New P: 1.963
iter 27 loss: 0.049
Actual params: [1.4849, 1.9635]
-Original Grad: -0.024, -lr * Pred Grad:  -0.014, New P: 1.471
-Original Grad: -0.023, -lr * Pred Grad:  0.099, New P: 2.062
iter 28 loss: 0.052
Actual params: [1.4712, 2.0622]
-Original Grad: 0.039, -lr * Pred Grad:  -0.011, New P: 1.460
-Original Grad: 0.002, -lr * Pred Grad:  0.090, New P: 2.152
iter 29 loss: 0.054
Actual params: [1.4599, 2.1522]
-Original Grad: -0.010, -lr * Pred Grad:  -0.011, New P: 1.449
-Original Grad: -0.013, -lr * Pred Grad:  0.081, New P: 2.234
iter 30 loss: 0.050
Actual params: [1.4494, 2.2335]
-Original Grad: 0.025, -lr * Pred Grad:  -0.009, New P: 1.441
-Original Grad: -0.004, -lr * Pred Grad:  0.074, New P: 2.307
Target params: [1.3344, 1.5708]
iter 0 loss: 0.065
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.059, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.009, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.057
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.075, -lr * Pred Grad:  0.100, New P: -0.272
-Original Grad: -0.005, -lr * Pred Grad:  -0.093, New P: -0.190
iter 2 loss: 0.048
Actual params: [-0.2724, -0.1898]
-Original Grad: 0.055, -lr * Pred Grad:  0.099, New P: -0.174
-Original Grad: 0.005, -lr * Pred Grad:  -0.040, New P: -0.230
iter 3 loss: 0.041
Actual params: [-0.1735, -0.2298]
-Original Grad: 0.047, -lr * Pred Grad:  0.097, New P: -0.076
-Original Grad: 0.011, -lr * Pred Grad:  0.016, New P: -0.213
iter 4 loss: 0.037
Actual params: [-0.0762, -0.2134]
-Original Grad: 0.002, -lr * Pred Grad:  0.083, New P: 0.007
-Original Grad: 0.004, -lr * Pred Grad:  0.027, New P: -0.187
iter 5 loss: 0.040
Actual params: [ 0.0071, -0.1866]
-Original Grad: -0.059, -lr * Pred Grad:  0.041, New P: 0.048
-Original Grad: -0.010, -lr * Pred Grad:  -0.008, New P: -0.195
iter 6 loss: 0.045
Actual params: [ 0.0485, -0.1949]
-Original Grad: -0.117, -lr * Pred Grad:  -0.006, New P: 0.042
-Original Grad: 0.003, -lr * Pred Grad:  0.001, New P: -0.194
iter 7 loss: 0.044
Actual params: [ 0.0422, -0.1939]
-Original Grad: -0.055, -lr * Pred Grad:  -0.020, New P: 0.022
-Original Grad: 0.005, -lr * Pred Grad:  0.013, New P: -0.180
iter 8 loss: 0.042
Actual params: [ 0.0223, -0.1804]
-Original Grad: -0.101, -lr * Pred Grad:  -0.039, New P: -0.017
-Original Grad: 0.006, -lr * Pred Grad:  0.026, New P: -0.155
iter 9 loss: 0.038
Actual params: [-0.0166, -0.1545]
-Original Grad: -0.106, -lr * Pred Grad:  -0.053, New P: -0.069
-Original Grad: 0.015, -lr * Pred Grad:  0.047, New P: -0.108
iter 10 loss: 0.036
Actual params: [-0.0695, -0.1078]
-Original Grad: 0.024, -lr * Pred Grad:  -0.042, New P: -0.112
-Original Grad: 0.015, -lr * Pred Grad:  0.061, New P: -0.047
iter 11 loss: 0.036
Actual params: [-0.1116, -0.0471]
-Original Grad: 0.050, -lr * Pred Grad:  -0.027, New P: -0.139
-Original Grad: 0.003, -lr * Pred Grad:  0.059, New P: 0.012
iter 12 loss: 0.038
Actual params: [-0.1386,  0.012 ]
-Original Grad: 0.057, -lr * Pred Grad:  -0.013, New P: -0.151
-Original Grad: 0.002, -lr * Pred Grad:  0.056, New P: 0.068
iter 13 loss: 0.039
Actual params: [-0.1513,  0.0681]
-Original Grad: 0.071, -lr * Pred Grad:  0.002, New P: -0.149
-Original Grad: -0.007, -lr * Pred Grad:  0.039, New P: 0.107
iter 14 loss: 0.039
Actual params: [-0.149 ,  0.1073]
-Original Grad: 0.088, -lr * Pred Grad:  0.018, New P: -0.131
-Original Grad: -0.015, -lr * Pred Grad:  0.011, New P: 0.118
iter 15 loss: 0.038
Actual params: [-0.1314,  0.118 ]
-Original Grad: 0.066, -lr * Pred Grad:  0.027, New P: -0.104
-Original Grad: -0.010, -lr * Pred Grad:  -0.005, New P: 0.113
iter 16 loss: 0.036
Actual params: [-0.1045,  0.1132]
-Original Grad: 0.076, -lr * Pred Grad:  0.037, New P: -0.068
-Original Grad: -0.011, -lr * Pred Grad:  -0.018, New P: 0.095
iter 17 loss: 0.034
Actual params: [-0.0679,  0.0952]
-Original Grad: 0.036, -lr * Pred Grad:  0.039, New P: -0.029
-Original Grad: -0.003, -lr * Pred Grad:  -0.020, New P: 0.075
iter 18 loss: 0.033
Actual params: [-0.0289,  0.0752]
-Original Grad: 0.023, -lr * Pred Grad:  0.039, New P: 0.010
-Original Grad: 0.006, -lr * Pred Grad:  -0.010, New P: 0.065
iter 19 loss: 0.033
Actual params: [0.0104, 0.0653]
-Original Grad: -0.010, -lr * Pred Grad:  0.034, New P: 0.044
-Original Grad: 0.020, -lr * Pred Grad:  0.016, New P: 0.081
iter 20 loss: 0.034
Actual params: [0.0445, 0.081 ]
-Original Grad: -0.068, -lr * Pred Grad:  0.019, New P: 0.063
-Original Grad: 0.037, -lr * Pred Grad:  0.044, New P: 0.125
iter 21 loss: 0.034
Actual params: [0.063 , 0.1253]
-Original Grad: -0.060, -lr * Pred Grad:  0.006, New P: 0.070
-Original Grad: 0.029, -lr * Pred Grad:  0.059, New P: 0.185
iter 22 loss: 0.032
Actual params: [0.0695, 0.1847]
-Original Grad: -0.058, -lr * Pred Grad:  -0.004, New P: 0.066
-Original Grad: 0.033, -lr * Pred Grad:  0.072, New P: 0.257
iter 23 loss: 0.030
Actual params: [0.0656, 0.2568]
-Original Grad: -0.008, -lr * Pred Grad:  -0.005, New P: 0.061
-Original Grad: 0.015, -lr * Pred Grad:  0.075, New P: 0.332
iter 24 loss: 0.030
Actual params: [0.0607, 0.3321]
-Original Grad: 0.039, -lr * Pred Grad:  0.002, New P: 0.063
-Original Grad: -0.001, -lr * Pred Grad:  0.068, New P: 0.400
iter 25 loss: 0.030
Actual params: [0.0629, 0.3997]
-Original Grad: 0.040, -lr * Pred Grad:  0.009, New P: 0.072
-Original Grad: -0.004, -lr * Pred Grad:  0.058, New P: 0.458
iter 26 loss: 0.030
Actual params: [0.0718, 0.4581]
-Original Grad: 0.050, -lr * Pred Grad:  0.016, New P: 0.088
-Original Grad: -0.010, -lr * Pred Grad:  0.045, New P: 0.503
iter 27 loss: 0.030
Actual params: [0.0882, 0.5033]
-Original Grad: 0.045, -lr * Pred Grad:  0.023, New P: 0.111
-Original Grad: -0.011, -lr * Pred Grad:  0.033, New P: 0.536
iter 28 loss: 0.029
Actual params: [0.1107, 0.536 ]
-Original Grad: 0.049, -lr * Pred Grad:  0.029, New P: 0.139
-Original Grad: -0.013, -lr * Pred Grad:  0.020, New P: 0.556
iter 29 loss: 0.028
Actual params: [0.1394, 0.5558]
-Original Grad: 0.047, -lr * Pred Grad:  0.034, New P: 0.173
-Original Grad: -0.012, -lr * Pred Grad:  0.009, New P: 0.565
iter 30 loss: 0.028
Actual params: [0.1733, 0.5646]
-Original Grad: 0.013, -lr * Pred Grad:  0.033, New P: 0.206
-Original Grad: 0.003, -lr * Pred Grad:  0.010, New P: 0.575
Target params: [1.3344, 1.5708]
iter 0 loss: 0.514
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.044, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.006, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.501
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.055, -lr * Pred Grad:  0.100, New P: -0.272
-Original Grad: 0.012, -lr * Pred Grad:  0.096, New P: 0.200
iter 2 loss: 0.482
Actual params: [-0.2723,  0.2   ]
-Original Grad: 0.139, -lr * Pred Grad:  0.092, New P: -0.180
-Original Grad: -0.003, -lr * Pred Grad:  0.058, New P: 0.258
iter 3 loss: 0.462
Actual params: [-0.1804,  0.2577]
-Original Grad: 0.152, -lr * Pred Grad:  0.094, New P: -0.086
-Original Grad: -0.001, -lr * Pred Grad:  0.044, New P: 0.302
iter 4 loss: 0.439
Actual params: [-0.086 ,  0.3019]
-Original Grad: 0.227, -lr * Pred Grad:  0.095, New P: 0.009
-Original Grad: 0.019, -lr * Pred Grad:  0.066, New P: 0.368
iter 5 loss: 0.402
Actual params: [0.0087, 0.368 ]
-Original Grad: 0.233, -lr * Pred Grad:  0.097, New P: 0.105
-Original Grad: -0.002, -lr * Pred Grad:  0.052, New P: 0.420
iter 6 loss: 0.347
Actual params: [0.1053, 0.4196]
-Original Grad: 0.487, -lr * Pred Grad:  0.092, New P: 0.198
-Original Grad: 0.127, -lr * Pred Grad:  0.058, New P: 0.478
iter 7 loss: 0.276
Actual params: [0.1976, 0.4778]
-Original Grad: 0.384, -lr * Pred Grad:  0.095, New P: 0.293
-Original Grad: 0.073, -lr * Pred Grad:  0.069, New P: 0.547
iter 8 loss: 0.211
Actual params: [0.2929, 0.5467]
-Original Grad: 0.238, -lr * Pred Grad:  0.096, New P: 0.388
-Original Grad: 0.137, -lr * Pred Grad:  0.078, New P: 0.625
iter 9 loss: 0.145
Actual params: [0.3885, 0.6249]
-Original Grad: 0.289, -lr * Pred Grad:  0.097, New P: 0.485
-Original Grad: 0.290, -lr * Pred Grad:  0.080, New P: 0.705
iter 10 loss: 0.091
Actual params: [0.4853, 0.7045]
-Original Grad: 0.139, -lr * Pred Grad:  0.094, New P: 0.579
-Original Grad: 0.118, -lr * Pred Grad:  0.083, New P: 0.787
iter 11 loss: 0.057
Actual params: [0.579 , 0.7875]
-Original Grad: 0.160, -lr * Pred Grad:  0.092, New P: 0.671
-Original Grad: 0.122, -lr * Pred Grad:  0.086, New P: 0.873
iter 12 loss: 0.045
Actual params: [0.6707, 0.8734]
-Original Grad: 0.002, -lr * Pred Grad:  0.083, New P: 0.753
-Original Grad: 0.046, -lr * Pred Grad:  0.082, New P: 0.956
iter 13 loss: 0.052
Actual params: [0.7535, 0.9559]
-Original Grad: -0.070, -lr * Pred Grad:  0.071, New P: 0.824
-Original Grad: -0.025, -lr * Pred Grad:  0.071, New P: 1.027
iter 14 loss: 0.076
Actual params: [0.8241, 1.0272]
-Original Grad: -0.136, -lr * Pred Grad:  0.055, New P: 0.879
-Original Grad: -0.035, -lr * Pred Grad:  0.060, New P: 1.087
iter 15 loss: 0.101
Actual params: [0.8794, 1.0873]
-Original Grad: -0.049, -lr * Pred Grad:  0.047, New P: 0.927
-Original Grad: -0.135, -lr * Pred Grad:  0.036, New P: 1.123
iter 16 loss: 0.117
Actual params: [0.9267, 1.123 ]
-Original Grad: -0.021, -lr * Pred Grad:  0.042, New P: 0.968
-Original Grad: -0.032, -lr * Pred Grad:  0.029, New P: 1.152
iter 17 loss: 0.120
Actual params: [0.9685, 1.1517]
-Original Grad: 0.105, -lr * Pred Grad:  0.044, New P: 1.012
-Original Grad: -0.084, -lr * Pred Grad:  0.016, New P: 1.167
iter 18 loss: 0.116
Actual params: [1.0121, 1.1673]
-Original Grad: 0.027, -lr * Pred Grad:  0.041, New P: 1.053
-Original Grad: -0.022, -lr * Pred Grad:  0.012, New P: 1.179
iter 19 loss: 0.109
Actual params: [1.0533, 1.179 ]
-Original Grad: 0.122, -lr * Pred Grad:  0.044, New P: 1.097
-Original Grad: -0.004, -lr * Pred Grad:  0.010, New P: 1.189
iter 20 loss: 0.097
Actual params: [1.0974, 1.1892]
-Original Grad: 0.209, -lr * Pred Grad:  0.051, New P: 1.148
-Original Grad: -0.111, -lr * Pred Grad:  -0.004, New P: 1.185
iter 21 loss: 0.081
Actual params: [1.1484, 1.1853]
-Original Grad: 0.218, -lr * Pred Grad:  0.057, New P: 1.206
-Original Grad: -0.015, -lr * Pred Grad:  -0.005, New P: 1.180
iter 22 loss: 0.064
Actual params: [1.2056, 1.18  ]
-Original Grad: 0.177, -lr * Pred Grad:  0.061, New P: 1.267
-Original Grad: -0.007, -lr * Pred Grad:  -0.006, New P: 1.174
iter 23 loss: 0.049
Actual params: [1.2667, 1.1743]
-Original Grad: 0.115, -lr * Pred Grad:  0.062, New P: 1.328
-Original Grad: 0.070, -lr * Pred Grad:  0.003, New P: 1.178
iter 24 loss: 0.036
Actual params: [1.3283, 1.1775]
-Original Grad: 0.107, -lr * Pred Grad:  0.062, New P: 1.390
-Original Grad: 0.040, -lr * Pred Grad:  0.008, New P: 1.185
iter 25 loss: 0.028
Actual params: [1.3901, 1.1852]
-Original Grad: 0.099, -lr * Pred Grad:  0.062, New P: 1.452
-Original Grad: 0.046, -lr * Pred Grad:  0.012, New P: 1.198
iter 26 loss: 0.022
Actual params: [1.4517, 1.1976]
-Original Grad: 0.041, -lr * Pred Grad:  0.058, New P: 1.510
-Original Grad: 0.088, -lr * Pred Grad:  0.022, New P: 1.219
iter 27 loss: 0.018
Actual params: [1.5101, 1.2193]
-Original Grad: 0.021, -lr * Pred Grad:  0.054, New P: 1.564
-Original Grad: 0.052, -lr * Pred Grad:  0.026, New P: 1.245
iter 28 loss: 0.015
Actual params: [1.5645, 1.2452]
-Original Grad: 0.023, -lr * Pred Grad:  0.051, New P: 1.615
-Original Grad: 0.048, -lr * Pred Grad:  0.029, New P: 1.274
iter 29 loss: 0.014
Actual params: [1.6154, 1.2744]
-Original Grad: 0.029, -lr * Pred Grad:  0.048, New P: 1.663
-Original Grad: -0.010, -lr * Pred Grad:  0.025, New P: 1.300
iter 30 loss: 0.013
Actual params: [1.6635, 1.2998]
-Original Grad: -0.026, -lr * Pred Grad:  0.042, New P: 1.706
-Original Grad: -0.005, -lr * Pred Grad:  0.023, New P: 1.322
Target params: [1.3344, 1.5708]
iter 0 loss: 0.274
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.067, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.237, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.236
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.081, -lr * Pred Grad:  0.100, New P: -0.272
-Original Grad: 0.326, -lr * Pred Grad:  0.100, New P: 0.203
iter 2 loss: 0.188
Actual params: [-0.2723,  0.2031]
-Original Grad: 0.235, -lr * Pred Grad:  0.090, New P: -0.182
-Original Grad: 0.205, -lr * Pred Grad:  0.098, New P: 0.301
iter 3 loss: 0.132
Actual params: [-0.1824,  0.3007]
-Original Grad: 0.491, -lr * Pred Grad:  0.086, New P: -0.097
-Original Grad: 0.104, -lr * Pred Grad:  0.091, New P: 0.392
iter 4 loss: 0.097
Actual params: [-0.0968,  0.3916]
-Original Grad: 0.209, -lr * Pred Grad:  0.087, New P: -0.010
-Original Grad: 0.068, -lr * Pred Grad:  0.084, New P: 0.476
iter 5 loss: 0.078
Actual params: [-0.0098,  0.4756]
-Original Grad: 0.139, -lr * Pred Grad:  0.085, New P: 0.075
-Original Grad: 0.097, -lr * Pred Grad:  0.081, New P: 0.557
iter 6 loss: 0.063
Actual params: [0.0751, 0.557 ]
-Original Grad: 0.040, -lr * Pred Grad:  0.077, New P: 0.152
-Original Grad: 0.097, -lr * Pred Grad:  0.080, New P: 0.637
iter 7 loss: 0.052
Actual params: [0.1524, 0.6368]
-Original Grad: -0.068, -lr * Pred Grad:  0.062, New P: 0.215
-Original Grad: 0.112, -lr * Pred Grad:  0.080, New P: 0.716
iter 8 loss: 0.046
Actual params: [0.2145, 0.7164]
-Original Grad: -0.101, -lr * Pred Grad:  0.046, New P: 0.261
-Original Grad: 0.071, -lr * Pred Grad:  0.077, New P: 0.793
iter 9 loss: 0.042
Actual params: [0.261 , 0.7933]
-Original Grad: 0.022, -lr * Pred Grad:  0.043, New P: 0.304
-Original Grad: 0.038, -lr * Pred Grad:  0.072, New P: 0.865
iter 10 loss: 0.038
Actual params: [0.3042, 0.8654]
-Original Grad: 0.032, -lr * Pred Grad:  0.041, New P: 0.345
-Original Grad: 0.038, -lr * Pred Grad:  0.068, New P: 0.933
iter 11 loss: 0.034
Actual params: [0.3453, 0.9334]
-Original Grad: 0.061, -lr * Pred Grad:  0.041, New P: 0.387
-Original Grad: 0.017, -lr * Pred Grad:  0.063, New P: 0.996
iter 12 loss: 0.030
Actual params: [0.3868, 0.9961]
-Original Grad: -0.051, -lr * Pred Grad:  0.033, New P: 0.420
-Original Grad: 0.090, -lr * Pred Grad:  0.064, New P: 1.060
iter 13 loss: 0.027
Actual params: [0.4201, 1.0603]
-Original Grad: -0.013, -lr * Pred Grad:  0.029, New P: 0.449
-Original Grad: 0.075, -lr * Pred Grad:  0.064, New P: 1.125
iter 14 loss: 0.024
Actual params: [0.4492, 1.1246]
-Original Grad: -0.011, -lr * Pred Grad:  0.026, New P: 0.475
-Original Grad: 0.057, -lr * Pred Grad:  0.063, New P: 1.188
iter 15 loss: 0.021
Actual params: [0.4747, 1.1877]
-Original Grad: 0.022, -lr * Pred Grad:  0.025, New P: 0.500
-Original Grad: 0.030, -lr * Pred Grad:  0.060, New P: 1.248
iter 16 loss: 0.019
Actual params: [0.4995, 1.2476]
-Original Grad: -0.003, -lr * Pred Grad:  0.022, New P: 0.522
-Original Grad: 0.038, -lr * Pred Grad:  0.058, New P: 1.305
iter 17 loss: 0.018
Actual params: [0.5217, 1.3054]
-Original Grad: 0.061, -lr * Pred Grad:  0.025, New P: 0.547
-Original Grad: 0.006, -lr * Pred Grad:  0.053, New P: 1.358
iter 18 loss: 0.017
Actual params: [0.5467, 1.3584]
-Original Grad: 0.027, -lr * Pred Grad:  0.025, New P: 0.572
-Original Grad: 0.028, -lr * Pred Grad:  0.051, New P: 1.409
iter 19 loss: 0.016
Actual params: [0.5715, 1.4091]
-Original Grad: 0.028, -lr * Pred Grad:  0.025, New P: 0.596
-Original Grad: -0.005, -lr * Pred Grad:  0.046, New P: 1.455
iter 20 loss: 0.016
Actual params: [0.5963, 1.4548]
-Original Grad: 0.018, -lr * Pred Grad:  0.024, New P: 0.620
-Original Grad: -0.002, -lr * Pred Grad:  0.041, New P: 1.496
iter 21 loss: 0.016
Actual params: [0.6204, 1.4962]
-Original Grad: 0.035, -lr * Pred Grad:  0.025, New P: 0.645
-Original Grad: -0.027, -lr * Pred Grad:  0.035, New P: 1.531
iter 22 loss: 0.017
Actual params: [0.6451, 1.5311]
-Original Grad: 0.029, -lr * Pred Grad:  0.025, New P: 0.670
-Original Grad: -0.025, -lr * Pred Grad:  0.029, New P: 1.560
iter 23 loss: 0.018
Actual params: [0.67  , 1.5604]
-Original Grad: 0.039, -lr * Pred Grad:  0.026, New P: 0.696
-Original Grad: -0.053, -lr * Pred Grad:  0.021, New P: 1.582
iter 24 loss: 0.018
Actual params: [0.6958, 1.5816]
-Original Grad: 0.027, -lr * Pred Grad:  0.026, New P: 0.722
-Original Grad: -0.057, -lr * Pred Grad:  0.013, New P: 1.595
iter 25 loss: 0.019
Actual params: [0.7217, 1.595 ]
-Original Grad: -0.018, -lr * Pred Grad:  0.022, New P: 0.744
-Original Grad: -0.036, -lr * Pred Grad:  0.009, New P: 1.604
iter 26 loss: 0.019
Actual params: [0.7437, 1.6035]
-Original Grad: 0.009, -lr * Pred Grad:  0.021, New P: 0.765
-Original Grad: -0.076, -lr * Pred Grad:  -0.000, New P: 1.603
iter 27 loss: 0.019
Actual params: [0.7646, 1.6034]
-Original Grad: -0.011, -lr * Pred Grad:  0.018, New P: 0.783
-Original Grad: -0.064, -lr * Pred Grad:  -0.007, New P: 1.597
iter 28 loss: 0.019
Actual params: [0.7825, 1.5967]
-Original Grad: 0.001, -lr * Pred Grad:  0.016, New P: 0.799
-Original Grad: -0.078, -lr * Pred Grad:  -0.014, New P: 1.583
iter 29 loss: 0.018
Actual params: [0.799 , 1.5827]
-Original Grad: 0.003, -lr * Pred Grad:  0.015, New P: 0.814
-Original Grad: -0.028, -lr * Pred Grad:  -0.016, New P: 1.567
iter 30 loss: 0.017
Actual params: [0.8143, 1.5671]
-Original Grad: -0.041, -lr * Pred Grad:  0.010, New P: 0.825
-Original Grad: -0.079, -lr * Pred Grad:  -0.022, New P: 1.545
Target params: [1.3344, 1.5708]
iter 0 loss: 0.187
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.187
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.002, -lr * Pred Grad:  0.094, New P: -0.278
-Original Grad: 0.000, -lr * Pred Grad:  0.093, New P: 0.197
iter 2 loss: 0.186
Actual params: [-0.2784,  0.1968]
-Original Grad: 0.009, -lr * Pred Grad:  0.079, New P: -0.199
-Original Grad: 0.002, -lr * Pred Grad:  0.078, New P: 0.275
iter 3 loss: 0.185
Actual params: [-0.199 ,  0.2752]
-Original Grad: 0.022, -lr * Pred Grad:  0.078, New P: -0.121
-Original Grad: 0.006, -lr * Pred Grad:  0.077, New P: 0.352
iter 4 loss: 0.181
Actual params: [-0.1209,  0.352 ]
-Original Grad: 0.037, -lr * Pred Grad:  0.082, New P: -0.039
-Original Grad: 0.011, -lr * Pred Grad:  0.080, New P: 0.432
iter 5 loss: 0.174
Actual params: [-0.0393,  0.4316]
-Original Grad: 0.154, -lr * Pred Grad:  0.069, New P: 0.030
-Original Grad: 0.048, -lr * Pred Grad:  0.068, New P: 0.500
iter 6 loss: 0.159
Actual params: [0.0302, 0.4998]
-Original Grad: 0.257, -lr * Pred Grad:  0.075, New P: 0.105
-Original Grad: 0.081, -lr * Pred Grad:  0.074, New P: 0.574
iter 7 loss: 0.133
Actual params: [0.1052, 0.5742]
-Original Grad: 0.207, -lr * Pred Grad:  0.083, New P: 0.188
-Original Grad: 0.067, -lr * Pred Grad:  0.082, New P: 0.656
iter 8 loss: 0.099
Actual params: [0.1879, 0.6563]
-Original Grad: 0.325, -lr * Pred Grad:  0.087, New P: 0.275
-Original Grad: 0.078, -lr * Pred Grad:  0.088, New P: 0.744
iter 9 loss: 0.066
Actual params: [0.2752, 0.7441]
-Original Grad: 0.321, -lr * Pred Grad:  0.092, New P: 0.367
-Original Grad: 0.052, -lr * Pred Grad:  0.090, New P: 0.834
iter 10 loss: 0.043
Actual params: [0.3669, 0.8345]
-Original Grad: 0.230, -lr * Pred Grad:  0.094, New P: 0.461
-Original Grad: 0.068, -lr * Pred Grad:  0.094, New P: 0.928
iter 11 loss: 0.040
Actual params: [0.461 , 0.9281]
-Original Grad: -0.007, -lr * Pred Grad:  0.084, New P: 0.545
-Original Grad: 0.018, -lr * Pred Grad:  0.089, New P: 1.017
iter 12 loss: 0.042
Actual params: [0.5451, 1.0171]
-Original Grad: -0.116, -lr * Pred Grad:  0.066, New P: 0.611
-Original Grad: 0.093, -lr * Pred Grad:  0.094, New P: 1.111
iter 13 loss: 0.042
Actual params: [0.6109, 1.1107]
-Original Grad: -0.045, -lr * Pred Grad:  0.056, New P: 0.667
-Original Grad: 0.049, -lr * Pred Grad:  0.094, New P: 1.205
iter 14 loss: 0.039
Actual params: [0.6668, 1.2047]
-Original Grad: -0.030, -lr * Pred Grad:  0.048, New P: 0.715
-Original Grad: 0.061, -lr * Pred Grad:  0.096, New P: 1.300
iter 15 loss: 0.033
Actual params: [0.715 , 1.3004]
-Original Grad: 0.058, -lr * Pred Grad:  0.048, New P: 0.763
-Original Grad: 0.027, -lr * Pred Grad:  0.092, New P: 1.393
iter 16 loss: 0.026
Actual params: [0.763 , 1.3929]
-Original Grad: 0.008, -lr * Pred Grad:  0.044, New P: 0.807
-Original Grad: 0.122, -lr * Pred Grad:  0.097, New P: 1.490
iter 17 loss: 0.020
Actual params: [0.8071, 1.4902]
-Original Grad: 0.020, -lr * Pred Grad:  0.042, New P: 0.849
-Original Grad: 0.037, -lr * Pred Grad:  0.095, New P: 1.585
iter 18 loss: 0.017
Actual params: [0.8486, 1.5851]
-Original Grad: 0.047, -lr * Pred Grad:  0.041, New P: 0.890
-Original Grad: -0.008, -lr * Pred Grad:  0.085, New P: 1.670
iter 19 loss: 0.017
Actual params: [0.89  , 1.6697]
-Original Grad: 0.019, -lr * Pred Grad:  0.039, New P: 0.929
-Original Grad: -0.017, -lr * Pred Grad:  0.073, New P: 1.743
iter 20 loss: 0.018
Actual params: [0.929, 1.743]
-Original Grad: -0.004, -lr * Pred Grad:  0.035, New P: 0.964
-Original Grad: -0.034, -lr * Pred Grad:  0.059, New P: 1.802
iter 21 loss: 0.020
Actual params: [0.9643, 1.802 ]
-Original Grad: 0.005, -lr * Pred Grad:  0.033, New P: 0.997
-Original Grad: -0.019, -lr * Pred Grad:  0.050, New P: 1.852
iter 22 loss: 0.023
Actual params: [0.9968, 1.8515]
-Original Grad: -0.005, -lr * Pred Grad:  0.029, New P: 1.026
-Original Grad: -0.050, -lr * Pred Grad:  0.034, New P: 1.885
iter 23 loss: 0.026
Actual params: [1.026 , 1.8854]
-Original Grad: -0.007, -lr * Pred Grad:  0.026, New P: 1.052
-Original Grad: -0.072, -lr * Pred Grad:  0.015, New P: 1.900
iter 24 loss: 0.027
Actual params: [1.052 , 1.9004]
-Original Grad: -0.011, -lr * Pred Grad:  0.023, New P: 1.075
-Original Grad: -0.082, -lr * Pred Grad:  -0.003, New P: 1.897
iter 25 loss: 0.027
Actual params: [1.0747, 1.8974]
-Original Grad: -0.010, -lr * Pred Grad:  0.020, New P: 1.095
-Original Grad: -0.051, -lr * Pred Grad:  -0.013, New P: 1.885
iter 26 loss: 0.027
Actual params: [1.0946, 1.8847]
-Original Grad: -0.017, -lr * Pred Grad:  0.017, New P: 1.111
-Original Grad: -0.078, -lr * Pred Grad:  -0.026, New P: 1.859
iter 27 loss: 0.025
Actual params: [1.1114, 1.8587]
-Original Grad: -0.021, -lr * Pred Grad:  0.013, New P: 1.125
-Original Grad: -0.073, -lr * Pred Grad:  -0.037, New P: 1.822
iter 28 loss: 0.023
Actual params: [1.1248, 1.8221]
-Original Grad: -0.019, -lr * Pred Grad:  0.011, New P: 1.135
-Original Grad: -0.068, -lr * Pred Grad:  -0.045, New P: 1.777
iter 29 loss: 0.021
Actual params: [1.1353, 1.7771]
-Original Grad: -0.018, -lr * Pred Grad:  0.008, New P: 1.143
-Original Grad: -0.039, -lr * Pred Grad:  -0.048, New P: 1.729
iter 30 loss: 0.020
Actual params: [1.1433, 1.7293]
-Original Grad: -0.004, -lr * Pred Grad:  0.007, New P: 1.150
-Original Grad: -0.004, -lr * Pred Grad:  -0.044, New P: 1.685
Target params: [1.3344, 1.5708]
iter 0 loss: 0.422
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.054, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.069, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.406
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.067, -lr * Pred Grad:  0.100, New P: -0.272
-Original Grad: 0.070, -lr * Pred Grad:  0.100, New P: 0.204
iter 2 loss: 0.387
Actual params: [-0.2723,  0.2036]
-Original Grad: 0.097, -lr * Pred Grad:  0.099, New P: -0.173
-Original Grad: 0.090, -lr * Pred Grad:  0.100, New P: 0.304
iter 3 loss: 0.363
Actual params: [-0.1732,  0.3037]
-Original Grad: 0.084, -lr * Pred Grad:  0.100, New P: -0.073
-Original Grad: 0.060, -lr * Pred Grad:  0.099, New P: 0.402
iter 4 loss: 0.335
Actual params: [-0.0735,  0.4024]
-Original Grad: 0.182, -lr * Pred Grad:  0.096, New P: 0.023
-Original Grad: 0.115, -lr * Pred Grad:  0.099, New P: 0.502
iter 5 loss: 0.305
Actual params: [0.0226, 0.5016]
-Original Grad: 0.192, -lr * Pred Grad:  0.097, New P: 0.120
-Original Grad: 0.100, -lr * Pred Grad:  0.100, New P: 0.602
iter 6 loss: 0.274
Actual params: [0.1198, 0.6017]
-Original Grad: 0.247, -lr * Pred Grad:  0.098, New P: 0.218
-Original Grad: 0.113, -lr * Pred Grad:  0.101, New P: 0.703
iter 7 loss: 0.240
Actual params: [0.2177, 0.7027]
-Original Grad: 0.254, -lr * Pred Grad:  0.099, New P: 0.317
-Original Grad: 0.092, -lr * Pred Grad:  0.101, New P: 0.804
iter 8 loss: 0.195
Actual params: [0.3171, 0.8035]
-Original Grad: 0.282, -lr * Pred Grad:  0.101, New P: 0.418
-Original Grad: 0.143, -lr * Pred Grad:  0.102, New P: 0.906
iter 9 loss: 0.147
Actual params: [0.4179, 0.9055]
-Original Grad: 0.310, -lr * Pred Grad:  0.102, New P: 0.520
-Original Grad: 0.189, -lr * Pred Grad:  0.103, New P: 1.008
iter 10 loss: 0.104
Actual params: [0.5201, 1.0083]
-Original Grad: 0.191, -lr * Pred Grad:  0.102, New P: 0.622
-Original Grad: 0.132, -lr * Pred Grad:  0.103, New P: 1.111
iter 11 loss: 0.071
Actual params: [0.6217, 1.1115]
-Original Grad: 0.128, -lr * Pred Grad:  0.099, New P: 0.721
-Original Grad: 0.090, -lr * Pred Grad:  0.101, New P: 1.213
iter 12 loss: 0.048
Actual params: [0.7206, 1.2129]
-Original Grad: 0.073, -lr * Pred Grad:  0.094, New P: 0.814
-Original Grad: 0.089, -lr * Pred Grad:  0.100, New P: 1.313
iter 13 loss: 0.036
Actual params: [0.8145, 1.3129]
-Original Grad: 0.058, -lr * Pred Grad:  0.089, New P: 0.903
-Original Grad: 0.022, -lr * Pred Grad:  0.093, New P: 1.406
iter 14 loss: 0.032
Actual params: [0.9032, 1.4058]
-Original Grad: -0.039, -lr * Pred Grad:  0.077, New P: 0.980
-Original Grad: -0.004, -lr * Pred Grad:  0.084, New P: 1.489
iter 15 loss: 0.035
Actual params: [0.9805, 1.4893]
-Original Grad: 0.032, -lr * Pred Grad:  0.072, New P: 1.053
-Original Grad: -0.008, -lr * Pred Grad:  0.075, New P: 1.564
iter 16 loss: 0.041
Actual params: [1.0528, 1.5639]
-Original Grad: -0.026, -lr * Pred Grad:  0.064, New P: 1.116
-Original Grad: -0.047, -lr * Pred Grad:  0.061, New P: 1.625
iter 17 loss: 0.047
Actual params: [1.1165, 1.6253]
-Original Grad: -0.100, -lr * Pred Grad:  0.050, New P: 1.166
-Original Grad: -0.050, -lr * Pred Grad:  0.049, New P: 1.674
iter 18 loss: 0.052
Actual params: [1.1664, 1.6745]
-Original Grad: -0.005, -lr * Pred Grad:  0.045, New P: 1.211
-Original Grad: -0.066, -lr * Pred Grad:  0.036, New P: 1.710
iter 19 loss: 0.055
Actual params: [1.2114, 1.7103]
-Original Grad: -0.015, -lr * Pred Grad:  0.040, New P: 1.251
-Original Grad: -0.071, -lr * Pred Grad:  0.023, New P: 1.734
iter 20 loss: 0.057
Actual params: [1.2512, 1.7336]
-Original Grad: -0.009, -lr * Pred Grad:  0.036, New P: 1.287
-Original Grad: -0.076, -lr * Pred Grad:  0.012, New P: 1.745
iter 21 loss: 0.057
Actual params: [1.2867, 1.7451]
-Original Grad: -0.008, -lr * Pred Grad:  0.032, New P: 1.318
-Original Grad: -0.056, -lr * Pred Grad:  0.004, New P: 1.749
iter 22 loss: 0.057
Actual params: [1.3184, 1.7486]
-Original Grad: -0.005, -lr * Pred Grad:  0.029, New P: 1.347
-Original Grad: -0.053, -lr * Pred Grad:  -0.003, New P: 1.745
iter 23 loss: 0.057
Actual params: [1.3469, 1.7452]
-Original Grad: 0.011, -lr * Pred Grad:  0.027, New P: 1.374
-Original Grad: -0.085, -lr * Pred Grad:  -0.014, New P: 1.732
iter 24 loss: 0.056
Actual params: [1.3737, 1.7317]
-Original Grad: 0.013, -lr * Pred Grad:  0.025, New P: 1.399
-Original Grad: -0.063, -lr * Pred Grad:  -0.020, New P: 1.712
iter 25 loss: 0.054
Actual params: [1.3991, 1.7118]
-Original Grad: -0.005, -lr * Pred Grad:  0.023, New P: 1.422
-Original Grad: -0.045, -lr * Pred Grad:  -0.024, New P: 1.688
iter 26 loss: 0.052
Actual params: [1.4219, 1.6882]
-Original Grad: -0.001, -lr * Pred Grad:  0.021, New P: 1.442
-Original Grad: -0.045, -lr * Pred Grad:  -0.027, New P: 1.661
iter 27 loss: 0.050
Actual params: [1.4425, 1.6612]
-Original Grad: 0.008, -lr * Pred Grad:  0.019, New P: 1.462
-Original Grad: -0.049, -lr * Pred Grad:  -0.031, New P: 1.631
iter 28 loss: 0.046
Actual params: [1.4619, 1.6306]
-Original Grad: 0.094, -lr * Pred Grad:  0.025, New P: 1.487
-Original Grad: -0.081, -lr * Pred Grad:  -0.037, New P: 1.593
iter 29 loss: 0.041
Actual params: [1.4872, 1.5932]
-Original Grad: 0.055, -lr * Pred Grad:  0.028, New P: 1.515
-Original Grad: -0.051, -lr * Pred Grad:  -0.040, New P: 1.553
iter 30 loss: 0.037
Actual params: [1.5147, 1.5529]
-Original Grad: 0.028, -lr * Pred Grad:  0.027, New P: 1.542
-Original Grad: -0.013, -lr * Pred Grad:  -0.038, New P: 1.515
Target params: [1.3344, 1.5708]
iter 0 loss: 0.565
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.565
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.004, -lr * Pred Grad:  0.088, New P: -0.284
-Original Grad: 0.003, -lr * Pred Grad:  0.089, New P: 0.193
iter 2 loss: 0.564
Actual params: [-0.2845,  0.1926]
-Original Grad: 0.014, -lr * Pred Grad:  0.080, New P: -0.204
-Original Grad: 0.011, -lr * Pred Grad:  0.082, New P: 0.274
iter 3 loss: 0.562
Actual params: [-0.204 ,  0.2741]
-Original Grad: 0.034, -lr * Pred Grad:  0.079, New P: -0.125
-Original Grad: 0.028, -lr * Pred Grad:  0.079, New P: 0.353
iter 4 loss: 0.557
Actual params: [-0.1248,  0.3529]
-Original Grad: 0.067, -lr * Pred Grad:  0.080, New P: -0.045
-Original Grad: 0.055, -lr * Pred Grad:  0.080, New P: 0.433
iter 5 loss: 0.547
Actual params: [-0.0445,  0.4327]
-Original Grad: 0.113, -lr * Pred Grad:  0.082, New P: 0.038
-Original Grad: 0.090, -lr * Pred Grad:  0.082, New P: 0.515
iter 6 loss: 0.534
Actual params: [0.0375, 0.5148]
-Original Grad: 0.137, -lr * Pred Grad:  0.086, New P: 0.124
-Original Grad: 0.113, -lr * Pred Grad:  0.086, New P: 0.601
iter 7 loss: 0.516
Actual params: [0.1239, 0.6011]
-Original Grad: 0.217, -lr * Pred Grad:  0.088, New P: 0.212
-Original Grad: 0.153, -lr * Pred Grad:  0.089, New P: 0.690
iter 8 loss: 0.493
Actual params: [0.2116, 0.6902]
-Original Grad: 0.285, -lr * Pred Grad:  0.090, New P: 0.301
-Original Grad: 0.196, -lr * Pred Grad:  0.092, New P: 0.782
iter 9 loss: 0.462
Actual params: [0.3014, 0.7817]
-Original Grad: 0.298, -lr * Pred Grad:  0.093, New P: 0.395
-Original Grad: 0.234, -lr * Pred Grad:  0.094, New P: 0.876
iter 10 loss: 0.420
Actual params: [0.3946, 0.8759]
-Original Grad: 0.516, -lr * Pred Grad:  0.093, New P: 0.487
-Original Grad: 0.313, -lr * Pred Grad:  0.096, New P: 0.972
iter 11 loss: 0.367
Actual params: [0.4875, 0.9715]
-Original Grad: 0.561, -lr * Pred Grad:  0.095, New P: 0.583
-Original Grad: 0.383, -lr * Pred Grad:  0.097, New P: 1.069
iter 12 loss: 0.304
Actual params: [0.5829, 1.0689]
-Original Grad: 0.634, -lr * Pred Grad:  0.098, New P: 0.681
-Original Grad: 0.404, -lr * Pred Grad:  0.100, New P: 1.169
iter 13 loss: 0.235
Actual params: [0.6811, 1.1688]
-Original Grad: 0.626, -lr * Pred Grad:  0.101, New P: 0.782
-Original Grad: 0.491, -lr * Pred Grad:  0.102, New P: 1.271
iter 14 loss: 0.171
Actual params: [0.7821, 1.2707]
-Original Grad: 0.352, -lr * Pred Grad:  0.101, New P: 0.883
-Original Grad: 0.329, -lr * Pred Grad:  0.103, New P: 1.374
iter 15 loss: 0.111
Actual params: [0.8832, 1.3741]
-Original Grad: 0.278, -lr * Pred Grad:  0.100, New P: 0.983
-Original Grad: 0.247, -lr * Pred Grad:  0.103, New P: 1.477
iter 16 loss: 0.073
Actual params: [0.9828, 1.4771]
-Original Grad: 0.124, -lr * Pred Grad:  0.095, New P: 1.077
-Original Grad: 0.166, -lr * Pred Grad:  0.100, New P: 1.578
iter 17 loss: 0.053
Actual params: [1.0774, 1.5775]
-Original Grad: 0.036, -lr * Pred Grad:  0.087, New P: 1.165
-Original Grad: 0.134, -lr * Pred Grad:  0.097, New P: 1.675
iter 18 loss: 0.042
Actual params: [1.1646, 1.6745]
-Original Grad: -0.049, -lr * Pred Grad:  0.077, New P: 1.242
-Original Grad: 0.109, -lr * Pred Grad:  0.093, New P: 1.768
iter 19 loss: 0.037
Actual params: [1.242 , 1.7677]
-Original Grad: -0.093, -lr * Pred Grad:  0.067, New P: 1.309
-Original Grad: 0.084, -lr * Pred Grad:  0.089, New P: 1.856
iter 20 loss: 0.036
Actual params: [1.3087, 1.8563]
-Original Grad: -0.124, -lr * Pred Grad:  0.056, New P: 1.364
-Original Grad: 0.098, -lr * Pred Grad:  0.085, New P: 1.941
iter 21 loss: 0.034
Actual params: [1.3645, 1.9415]
-Original Grad: -0.112, -lr * Pred Grad:  0.046, New P: 1.411
-Original Grad: 0.092, -lr * Pred Grad:  0.082, New P: 2.023
iter 22 loss: 0.032
Actual params: [1.4108, 2.0234]
-Original Grad: -0.085, -lr * Pred Grad:  0.039, New P: 1.450
-Original Grad: 0.084, -lr * Pred Grad:  0.079, New P: 2.102
iter 23 loss: 0.030
Actual params: [1.4497, 2.1021]
-Original Grad: -0.100, -lr * Pred Grad:  0.031, New P: 1.481
-Original Grad: 0.078, -lr * Pred Grad:  0.075, New P: 2.178
iter 24 loss: 0.028
Actual params: [1.4812, 2.1776]
-Original Grad: -0.076, -lr * Pred Grad:  0.026, New P: 1.507
-Original Grad: 0.064, -lr * Pred Grad:  0.072, New P: 2.250
iter 25 loss: 0.026
Actual params: [1.5068, 2.2495]
-Original Grad: -0.055, -lr * Pred Grad:  0.021, New P: 1.528
-Original Grad: -0.002, -lr * Pred Grad:  0.065, New P: 2.315
iter 26 loss: 0.026
Actual params: [1.528 , 2.3149]
-Original Grad: -0.032, -lr * Pred Grad:  0.018, New P: 1.546
-Original Grad: -0.033, -lr * Pred Grad:  0.058, New P: 2.373
iter 27 loss: 0.028
Actual params: [1.546 , 2.3727]
-Original Grad: 0.028, -lr * Pred Grad:  0.018, New P: 1.564
-Original Grad: -0.117, -lr * Pred Grad:  0.046, New P: 2.419
iter 28 loss: 0.031
Actual params: [1.5636, 2.4188]
-Original Grad: 0.093, -lr * Pred Grad:  0.020, New P: 1.583
-Original Grad: -0.200, -lr * Pred Grad:  0.030, New P: 2.449
iter 29 loss: 0.035
Actual params: [1.5834, 2.4492]
-Original Grad: 0.084, -lr * Pred Grad:  0.021, New P: 1.605
-Original Grad: -0.223, -lr * Pred Grad:  0.015, New P: 2.465
iter 30 loss: 0.038
Actual params: [1.6048, 2.4645]
-Original Grad: 0.108, -lr * Pred Grad:  0.024, New P: 1.629
-Original Grad: -0.224, -lr * Pred Grad:  0.002, New P: 2.466
Target params: [1.3344, 1.5708]
iter 0 loss: 0.548
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.002, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.548
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.088, New P: -0.660
-Original Grad: 0.000, -lr * Pred Grad:  0.098, New P: 0.201
iter 2 loss: 0.548
Actual params: [-0.66  ,  0.2014]
-Original Grad: -0.000, -lr * Pred Grad:  -0.076, New P: -0.736
-Original Grad: -0.000, -lr * Pred Grad:  0.055, New P: 0.257
iter 3 loss: 0.548
Actual params: [-0.7364,  0.2567]
-Original Grad: -0.000, -lr * Pred Grad:  -0.068, New P: -0.804
-Original Grad: -0.000, -lr * Pred Grad:  0.007, New P: 0.263
iter 4 loss: 0.548
Actual params: [-0.8041,  0.2632]
-Original Grad: -0.000, -lr * Pred Grad:  -0.059, New P: -0.863
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.274
iter 5 loss: 0.548
Actual params: [-0.8631,  0.2742]
-Original Grad: -0.000, -lr * Pred Grad:  -0.052, New P: -0.915
-Original Grad: -0.000, -lr * Pred Grad:  0.005, New P: 0.280
iter 6 loss: 0.548
Actual params: [-0.9152,  0.2797]
-Original Grad: -0.000, -lr * Pred Grad:  -0.046, New P: -0.962
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.287
iter 7 loss: 0.548
Actual params: [-0.9616,  0.2872]
-Original Grad: -0.000, -lr * Pred Grad:  -0.042, New P: -1.003
-Original Grad: -0.000, -lr * Pred Grad:  0.002, New P: 0.289
iter 8 loss: 0.548
Actual params: [-1.0032,  0.2891]
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -1.041
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 0.287
iter 9 loss: 0.548
Actual params: [-1.0408,  0.2874]
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -1.075
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 0.284
iter 10 loss: 0.548
Actual params: [-1.0748,  0.2836]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.106
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: 0.278
iter 11 loss: 0.548
Actual params: [-1.1057,  0.2778]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.134
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: 0.275
iter 12 loss: 0.548
Actual params: [-1.1336,  0.2749]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.159
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: 0.273
iter 13 loss: 0.548
Actual params: [-1.159 ,  0.2731]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.182
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 0.271
iter 14 loss: 0.548
Actual params: [-1.182 ,  0.2711]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.203
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 0.268
iter 15 loss: 0.548
Actual params: [-1.2031,  0.2681]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.223
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 0.264
iter 16 loss: 0.548
Actual params: [-1.2225,  0.2644]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.240
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 0.261
iter 17 loss: 0.548
Actual params: [-1.2402,  0.2606]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.256
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 0.257
iter 18 loss: 0.548
Actual params: [-1.2565,  0.2567]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.271
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: 0.255
iter 19 loss: 0.548
Actual params: [-1.2712,  0.2552]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.285
-Original Grad: 0.000, -lr * Pred Grad:  -0.000, New P: 0.255
iter 20 loss: 0.548
Actual params: [-1.2847,  0.2548]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.297
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.254
iter 21 loss: 0.548
Actual params: [-1.2971,  0.2538]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.308
-Original Grad: -0.000, -lr * Pred Grad:  -0.001, New P: 0.252
iter 22 loss: 0.548
Actual params: [-1.3085,  0.2524]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.319
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 0.251
iter 23 loss: 0.548
Actual params: [-1.3189,  0.2507]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.329
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 0.248
iter 24 loss: 0.548
Actual params: [-1.3286,  0.2484]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.337
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 0.246
iter 25 loss: 0.548
Actual params: [-1.3374,  0.2459]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.346
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 0.243
iter 26 loss: 0.548
Actual params: [-1.3456,  0.2429]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.353
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 0.240
iter 27 loss: 0.548
Actual params: [-1.3531,  0.24  ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.360
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 0.237
iter 28 loss: 0.548
Actual params: [-1.36  ,  0.2371]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.366
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 0.234
iter 29 loss: 0.548
Actual params: [-1.3664,  0.234 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.372
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: 0.231
iter 30 loss: 0.548
Actual params: [-1.3723,  0.2312]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.378
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: 0.229
Target params: [1.3344, 1.5708]
iter 0 loss: 0.288
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.037, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.062, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.278
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.025, -lr * Pred Grad:  -0.097, New P: -0.670
-Original Grad: 0.058, -lr * Pred Grad:  0.100, New P: 0.203
iter 2 loss: 0.271
Actual params: [-0.6697,  0.2033]
-Original Grad: -0.012, -lr * Pred Grad:  -0.089, New P: -0.759
-Original Grad: 0.046, -lr * Pred Grad:  0.098, New P: 0.302
iter 3 loss: 0.265
Actual params: [-0.759 ,  0.3016]
-Original Grad: -0.004, -lr * Pred Grad:  -0.078, New P: -0.837
-Original Grad: 0.040, -lr * Pred Grad:  0.097, New P: 0.398
iter 4 loss: 0.263
Actual params: [-0.8374,  0.3983]
-Original Grad: 0.005, -lr * Pred Grad:  -0.060, New P: -0.897
-Original Grad: 0.025, -lr * Pred Grad:  0.092, New P: 0.490
iter 5 loss: 0.261
Actual params: [-0.8971,  0.4904]
-Original Grad: 0.009, -lr * Pred Grad:  -0.040, New P: -0.937
-Original Grad: 0.012, -lr * Pred Grad:  0.085, New P: 0.575
iter 6 loss: 0.261
Actual params: [-0.9373,  0.5751]
-Original Grad: 0.010, -lr * Pred Grad:  -0.023, New P: -0.961
-Original Grad: 0.002, -lr * Pred Grad:  0.075, New P: 0.650
iter 7 loss: 0.261
Actual params: [-0.9606,  0.6498]
-Original Grad: 0.011, -lr * Pred Grad:  -0.009, New P: -0.969
-Original Grad: -0.000, -lr * Pred Grad:  0.066, New P: 0.716
iter 8 loss: 0.262
Actual params: [-0.9692,  0.7156]
-Original Grad: 0.012, -lr * Pred Grad:  0.004, New P: -0.966
-Original Grad: -0.003, -lr * Pred Grad:  0.057, New P: 0.773
iter 9 loss: 0.262
Actual params: [-0.9655,  0.7728]
-Original Grad: 0.008, -lr * Pred Grad:  0.011, New P: -0.955
-Original Grad: -0.003, -lr * Pred Grad:  0.050, New P: 0.822
iter 10 loss: 0.262
Actual params: [-0.9548,  0.8223]
-Original Grad: 0.008, -lr * Pred Grad:  0.016, New P: -0.938
-Original Grad: -0.003, -lr * Pred Grad:  0.043, New P: 0.865
iter 11 loss: 0.262
Actual params: [-0.9384,  0.8651]
-Original Grad: 0.011, -lr * Pred Grad:  0.024, New P: -0.914
-Original Grad: -0.005, -lr * Pred Grad:  0.036, New P: 0.901
iter 12 loss: 0.262
Actual params: [-0.9143,  0.9014]
-Original Grad: 0.009, -lr * Pred Grad:  0.030, New P: -0.885
-Original Grad: -0.005, -lr * Pred Grad:  0.031, New P: 0.932
iter 13 loss: 0.262
Actual params: [-0.8845,  0.9319]
-Original Grad: 0.012, -lr * Pred Grad:  0.036, New P: -0.848
-Original Grad: -0.006, -lr * Pred Grad:  0.025, New P: 0.957
iter 14 loss: 0.261
Actual params: [-0.8481,  0.9568]
-Original Grad: 0.013, -lr * Pred Grad:  0.043, New P: -0.805
-Original Grad: -0.007, -lr * Pred Grad:  0.019, New P: 0.976
iter 15 loss: 0.261
Actual params: [-0.8048,  0.9762]
-Original Grad: 0.014, -lr * Pred Grad:  0.050, New P: -0.755
-Original Grad: -0.008, -lr * Pred Grad:  0.014, New P: 0.990
iter 16 loss: 0.260
Actual params: [-0.7549,  0.9904]
-Original Grad: 0.016, -lr * Pred Grad:  0.057, New P: -0.698
-Original Grad: -0.009, -lr * Pred Grad:  0.009, New P: 0.999
iter 17 loss: 0.259
Actual params: [-0.6979,  0.9993]
-Original Grad: 0.020, -lr * Pred Grad:  0.064, New P: -0.633
-Original Grad: -0.010, -lr * Pred Grad:  0.003, New P: 1.003
iter 18 loss: 0.258
Actual params: [-0.6335,  1.0027]
-Original Grad: 0.020, -lr * Pred Grad:  0.071, New P: -0.562
-Original Grad: -0.009, -lr * Pred Grad:  -0.001, New P: 1.002
iter 19 loss: 0.256
Actual params: [-0.5625,  1.0018]
-Original Grad: 0.038, -lr * Pred Grad:  0.081, New P: -0.481
-Original Grad: -0.017, -lr * Pred Grad:  -0.009, New P: 0.993
iter 20 loss: 0.252
Actual params: [-0.4812,  0.9932]
-Original Grad: 0.048, -lr * Pred Grad:  0.090, New P: -0.391
-Original Grad: -0.021, -lr * Pred Grad:  -0.017, New P: 0.976
iter 21 loss: 0.246
Actual params: [-0.3914,  0.9759]
-Original Grad: 0.060, -lr * Pred Grad:  0.097, New P: -0.295
-Original Grad: -0.022, -lr * Pred Grad:  -0.025, New P: 0.951
iter 22 loss: 0.238
Actual params: [-0.2946,  0.9507]
-Original Grad: 0.096, -lr * Pred Grad:  0.101, New P: -0.194
-Original Grad: -0.009, -lr * Pred Grad:  -0.027, New P: 0.923
iter 23 loss: 0.227
Actual params: [-0.194 ,  0.9234]
-Original Grad: 0.107, -lr * Pred Grad:  0.105, New P: -0.089
-Original Grad: 0.014, -lr * Pred Grad:  -0.018, New P: 0.905
iter 24 loss: 0.217
Actual params: [-0.0888,  0.905 ]
-Original Grad: 0.115, -lr * Pred Grad:  0.110, New P: 0.021
-Original Grad: 0.064, -lr * Pred Grad:  0.011, New P: 0.916
iter 25 loss: 0.204
Actual params: [0.0208, 0.9161]
-Original Grad: 0.134, -lr * Pred Grad:  0.113, New P: 0.134
-Original Grad: 0.087, -lr * Pred Grad:  0.038, New P: 0.954
iter 26 loss: 0.185
Actual params: [0.1341, 0.9543]
-Original Grad: 0.145, -lr * Pred Grad:  0.117, New P: 0.251
-Original Grad: 0.101, -lr * Pred Grad:  0.059, New P: 1.013
iter 27 loss: 0.158
Actual params: [0.251 , 1.0132]
-Original Grad: 0.231, -lr * Pred Grad:  0.118, New P: 0.369
-Original Grad: 0.120, -lr * Pred Grad:  0.075, New P: 1.088
iter 28 loss: 0.123
Actual params: [0.3688, 1.0884]
-Original Grad: 0.184, -lr * Pred Grad:  0.121, New P: 0.490
-Original Grad: 0.118, -lr * Pred Grad:  0.087, New P: 1.175
iter 29 loss: 0.088
Actual params: [0.4899, 1.1751]
-Original Grad: 0.185, -lr * Pred Grad:  0.124, New P: 0.614
-Original Grad: 0.144, -lr * Pred Grad:  0.097, New P: 1.272
iter 30 loss: 0.056
Actual params: [0.6138, 1.2719]
-Original Grad: 0.130, -lr * Pred Grad:  0.124, New P: 0.738
-Original Grad: 0.154, -lr * Pred Grad:  0.105, New P: 1.377
Target params: [1.3344, 1.5708]
iter 0 loss: 0.879
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.216, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.111, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.860
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.153, -lr * Pred Grad:  -0.098, New P: -0.670
-Original Grad: 0.121, -lr * Pred Grad:  0.100, New P: 0.204
iter 2 loss: 0.846
Actual params: [-0.67  ,  0.2037]
-Original Grad: -0.101, -lr * Pred Grad:  -0.093, New P: -0.763
-Original Grad: 0.100, -lr * Pred Grad:  0.099, New P: 0.303
iter 3 loss: 0.836
Actual params: [-0.7634,  0.303 ]
-Original Grad: -0.049, -lr * Pred Grad:  -0.085, New P: -0.849
-Original Grad: 0.063, -lr * Pred Grad:  0.095, New P: 0.398
iter 4 loss: 0.831
Actual params: [-0.8485,  0.3984]
-Original Grad: -0.030, -lr * Pred Grad:  -0.077, New P: -0.926
-Original Grad: 0.049, -lr * Pred Grad:  0.091, New P: 0.490
iter 5 loss: 0.829
Actual params: [-0.9258,  0.4897]
-Original Grad: -0.010, -lr * Pred Grad:  -0.068, New P: -0.994
-Original Grad: 0.020, -lr * Pred Grad:  0.083, New P: 0.573
iter 6 loss: 0.828
Actual params: [-0.9942,  0.573 ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.060, New P: -1.054
-Original Grad: 0.008, -lr * Pred Grad:  0.075, New P: 0.648
iter 7 loss: 0.828
Actual params: [-1.0544,  0.6475]
-Original Grad: -0.000, -lr * Pred Grad:  -0.053, New P: -1.107
-Original Grad: 0.004, -lr * Pred Grad:  0.067, New P: 0.714
iter 8 loss: 0.827
Actual params: [-1.1075,  0.7141]
-Original Grad: 0.000, -lr * Pred Grad:  -0.047, New P: -1.155
-Original Grad: 0.001, -lr * Pred Grad:  0.059, New P: 0.774
iter 9 loss: 0.827
Actual params: [-1.1546,  0.7736]
-Original Grad: 0.001, -lr * Pred Grad:  -0.042, New P: -1.197
-Original Grad: 0.001, -lr * Pred Grad:  0.053, New P: 0.827
iter 10 loss: 0.827
Actual params: [-1.1965,  0.8268]
-Original Grad: 0.001, -lr * Pred Grad:  -0.037, New P: -1.234
-Original Grad: 0.000, -lr * Pred Grad:  0.048, New P: 0.874
iter 11 loss: 0.827
Actual params: [-1.234 ,  0.8745]
-Original Grad: 0.000, -lr * Pred Grad:  -0.034, New P: -1.268
-Original Grad: 0.000, -lr * Pred Grad:  0.043, New P: 0.917
iter 12 loss: 0.827
Actual params: [-1.2676,  0.9174]
-Original Grad: 0.000, -lr * Pred Grad:  -0.030, New P: -1.298
-Original Grad: 0.000, -lr * Pred Grad:  0.039, New P: 0.956
iter 13 loss: 0.827
Actual params: [-1.2979,  0.9561]
-Original Grad: 0.000, -lr * Pred Grad:  -0.027, New P: -1.325
-Original Grad: -0.000, -lr * Pred Grad:  0.035, New P: 0.991
iter 14 loss: 0.827
Actual params: [-1.3252,  0.9911]
-Original Grad: 0.000, -lr * Pred Grad:  -0.025, New P: -1.350
-Original Grad: -0.000, -lr * Pred Grad:  0.032, New P: 1.023
iter 15 loss: 0.827
Actual params: [-1.3498,  1.0227]
-Original Grad: 0.000, -lr * Pred Grad:  -0.022, New P: -1.372
-Original Grad: -0.000, -lr * Pred Grad:  0.029, New P: 1.051
iter 16 loss: 0.827
Actual params: [-1.372 ,  1.0514]
-Original Grad: 0.000, -lr * Pred Grad:  -0.020, New P: -1.392
-Original Grad: -0.000, -lr * Pred Grad:  0.026, New P: 1.077
iter 17 loss: 0.827
Actual params: [-1.3922,  1.0774]
-Original Grad: 0.000, -lr * Pred Grad:  -0.018, New P: -1.411
-Original Grad: -0.000, -lr * Pred Grad:  0.024, New P: 1.101
iter 18 loss: 0.827
Actual params: [-1.4105,  1.101 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -1.427
-Original Grad: -0.000, -lr * Pred Grad:  0.021, New P: 1.122
iter 19 loss: 0.827
Actual params: [-1.4271,  1.1224]
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -1.442
-Original Grad: -0.000, -lr * Pred Grad:  0.020, New P: 1.142
iter 20 loss: 0.827
Actual params: [-1.4422,  1.1419]
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -1.456
-Original Grad: -0.000, -lr * Pred Grad:  0.018, New P: 1.160
iter 21 loss: 0.827
Actual params: [-1.4559,  1.1597]
-Original Grad: 0.000, -lr * Pred Grad:  -0.012, New P: -1.468
-Original Grad: -0.000, -lr * Pred Grad:  0.016, New P: 1.176
iter 22 loss: 0.827
Actual params: [-1.4683,  1.1758]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.480
-Original Grad: -0.000, -lr * Pred Grad:  0.015, New P: 1.191
iter 23 loss: 0.827
Actual params: [-1.4796,  1.1905]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.490
-Original Grad: -0.000, -lr * Pred Grad:  0.013, New P: 1.204
iter 24 loss: 0.827
Actual params: [-1.4899,  1.2039]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.499
-Original Grad: -0.000, -lr * Pred Grad:  0.012, New P: 1.216
iter 25 loss: 0.827
Actual params: [-1.4993,  1.2161]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.508
-Original Grad: -0.000, -lr * Pred Grad:  0.011, New P: 1.227
iter 26 loss: 0.827
Actual params: [-1.5078,  1.2272]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.516
-Original Grad: -0.000, -lr * Pred Grad:  0.010, New P: 1.237
iter 27 loss: 0.827
Actual params: [-1.5155,  1.2373]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.523
-Original Grad: -0.000, -lr * Pred Grad:  0.009, New P: 1.247
iter 28 loss: 0.827
Actual params: [-1.5226,  1.2465]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.529
-Original Grad: -0.000, -lr * Pred Grad:  0.008, New P: 1.255
iter 29 loss: 0.827
Actual params: [-1.529 ,  1.2549]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.535
-Original Grad: -0.000, -lr * Pred Grad:  0.008, New P: 1.263
iter 30 loss: 0.827
Actual params: [-1.5348,  1.2625]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.540
-Original Grad: -0.000, -lr * Pred Grad:  0.007, New P: 1.269
Target params: [1.3344, 1.5708]
iter 0 loss: 0.118
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.083, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.112
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.039, -lr * Pred Grad:  -0.092, New P: -0.665
-Original Grad: -0.003, -lr * Pred Grad:  -0.091, New P: -0.187
iter 2 loss: 0.109
Actual params: [-0.6646, -0.1872]
-Original Grad: -0.025, -lr * Pred Grad:  -0.086, New P: -0.750
-Original Grad: -0.000, -lr * Pred Grad:  -0.072, New P: -0.260
iter 3 loss: 0.108
Actual params: [-0.7502, -0.2595]
-Original Grad: -0.014, -lr * Pred Grad:  -0.078, New P: -0.828
-Original Grad: -0.001, -lr * Pred Grad:  -0.071, New P: -0.330
iter 4 loss: 0.107
Actual params: [-0.8282, -0.3303]
-Original Grad: -0.007, -lr * Pred Grad:  -0.070, New P: -0.898
-Original Grad: -0.000, -lr * Pred Grad:  -0.061, New P: -0.391
iter 5 loss: 0.106
Actual params: [-0.8981, -0.391 ]
-Original Grad: -0.005, -lr * Pred Grad:  -0.063, New P: -0.961
-Original Grad: 0.000, -lr * Pred Grad:  -0.045, New P: -0.436
iter 6 loss: 0.106
Actual params: [-0.9612, -0.4363]
-Original Grad: -0.003, -lr * Pred Grad:  -0.057, New P: -1.018
-Original Grad: 0.001, -lr * Pred Grad:  -0.028, New P: -0.465
iter 7 loss: 0.106
Actual params: [-1.018 , -0.4645]
-Original Grad: -0.002, -lr * Pred Grad:  -0.051, New P: -1.069
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -0.482
iter 8 loss: 0.106
Actual params: [-1.0688, -0.4818]
-Original Grad: -0.001, -lr * Pred Grad:  -0.046, New P: -1.115
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -0.489
iter 9 loss: 0.106
Actual params: [-1.1147, -0.4886]
-Original Grad: -0.001, -lr * Pred Grad:  -0.042, New P: -1.156
-Original Grad: 0.000, -lr * Pred Grad:  0.002, New P: -0.487
iter 10 loss: 0.106
Actual params: [-1.1563, -0.4865]
-Original Grad: -0.001, -lr * Pred Grad:  -0.038, New P: -1.194
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: -0.478
iter 11 loss: 0.106
Actual params: [-1.1942, -0.4781]
-Original Grad: -0.001, -lr * Pred Grad:  -0.035, New P: -1.229
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: -0.466
iter 12 loss: 0.106
Actual params: [-1.2288, -0.4656]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.260
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: -0.451
iter 13 loss: 0.106
Actual params: [-1.2602, -0.4515]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.289
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: -0.436
iter 14 loss: 0.106
Actual params: [-1.2888, -0.4359]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.315
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: -0.420
iter 15 loss: 0.106
Actual params: [-1.3148, -0.4197]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.339
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: -0.403
iter 16 loss: 0.106
Actual params: [-1.3386, -0.4034]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.360
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: -0.387
iter 17 loss: 0.106
Actual params: [-1.3603, -0.3869]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.380
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: -0.371
iter 18 loss: 0.106
Actual params: [-1.3801, -0.3706]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.398
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: -0.355
iter 19 loss: 0.106
Actual params: [-1.3983, -0.3548]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.415
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: -0.340
iter 20 loss: 0.106
Actual params: [-1.4148, -0.3395]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.430
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: -0.325
iter 21 loss: 0.106
Actual params: [-1.43  , -0.3248]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.444
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: -0.311
iter 22 loss: 0.106
Actual params: [-1.4438, -0.3109]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.457
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: -0.298
iter 23 loss: 0.106
Actual params: [-1.4565, -0.2975]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.468
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: -0.285
iter 24 loss: 0.106
Actual params: [-1.4681, -0.2849]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.479
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: -0.273
iter 25 loss: 0.106
Actual params: [-1.4787, -0.2729]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.488
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: -0.262
iter 26 loss: 0.106
Actual params: [-1.4884, -0.2616]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.497
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: -0.251
iter 27 loss: 0.106
Actual params: [-1.4973, -0.2508]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.505
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: -0.241
iter 28 loss: 0.106
Actual params: [-1.5054, -0.2406]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.513
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: -0.231
iter 29 loss: 0.106
Actual params: [-1.5128, -0.231 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.520
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: -0.222
iter 30 loss: 0.106
Actual params: [-1.5196, -0.2219]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.526
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: -0.213
Target params: [1.3344, 1.5708]
iter 0 loss: 0.194
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.065, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.034, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.187
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.038, -lr * Pred Grad:  -0.096, New P: -0.668
-Original Grad: 0.026, -lr * Pred Grad:  0.098, New P: 0.202
iter 2 loss: 0.183
Actual params: [-0.6679,  0.202 ]
-Original Grad: -0.018, -lr * Pred Grad:  -0.087, New P: -0.755
-Original Grad: 0.014, -lr * Pred Grad:  0.093, New P: 0.295
iter 3 loss: 0.181
Actual params: [-0.7546,  0.2946]
-Original Grad: -0.013, -lr * Pred Grad:  -0.079, New P: -0.834
-Original Grad: 0.013, -lr * Pred Grad:  0.089, New P: 0.383
iter 4 loss: 0.179
Actual params: [-0.834 ,  0.3833]
-Original Grad: -0.005, -lr * Pred Grad:  -0.070, New P: -0.904
-Original Grad: 0.005, -lr * Pred Grad:  0.081, New P: 0.464
iter 5 loss: 0.179
Actual params: [-0.9044,  0.4638]
-Original Grad: -0.002, -lr * Pred Grad:  -0.062, New P: -0.967
-Original Grad: 0.003, -lr * Pred Grad:  0.072, New P: 0.536
iter 6 loss: 0.179
Actual params: [-0.9666,  0.5361]
-Original Grad: -0.002, -lr * Pred Grad:  -0.055, New P: -1.022
-Original Grad: 0.002, -lr * Pred Grad:  0.065, New P: 0.601
iter 7 loss: 0.178
Actual params: [-1.022 ,  0.6014]
-Original Grad: -0.002, -lr * Pred Grad:  -0.050, New P: -1.072
-Original Grad: 0.002, -lr * Pred Grad:  0.060, New P: 0.661
iter 8 loss: 0.178
Actual params: [-1.072 ,  0.6609]
-Original Grad: -0.001, -lr * Pred Grad:  -0.045, New P: -1.117
-Original Grad: 0.001, -lr * Pred Grad:  0.054, New P: 0.715
iter 9 loss: 0.178
Actual params: [-1.1168,  0.7146]
-Original Grad: -0.001, -lr * Pred Grad:  -0.040, New P: -1.157
-Original Grad: 0.001, -lr * Pred Grad:  0.049, New P: 0.763
iter 10 loss: 0.178
Actual params: [-1.1572,  0.7631]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -1.194
-Original Grad: 0.001, -lr * Pred Grad:  0.044, New P: 0.807
iter 11 loss: 0.178
Actual params: [-1.1936,  0.8072]
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -1.227
-Original Grad: 0.000, -lr * Pred Grad:  0.040, New P: 0.847
iter 12 loss: 0.178
Actual params: [-1.2265,  0.8472]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -1.256
-Original Grad: 0.000, -lr * Pred Grad:  0.036, New P: 0.883
iter 13 loss: 0.178
Actual params: [-1.2564,  0.8835]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.283
-Original Grad: 0.000, -lr * Pred Grad:  0.033, New P: 0.916
iter 14 loss: 0.178
Actual params: [-1.2835,  0.9165]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.308
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: 0.946
iter 15 loss: 0.178
Actual params: [-1.3081,  0.9465]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.331
-Original Grad: 0.000, -lr * Pred Grad:  0.027, New P: 0.974
iter 16 loss: 0.178
Actual params: [-1.3305,  0.9738]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.351
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.999
iter 17 loss: 0.178
Actual params: [-1.3509,  0.9987]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.370
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 1.021
iter 18 loss: 0.178
Actual params: [-1.3695,  1.0214]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.387
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 1.042
iter 19 loss: 0.178
Actual params: [-1.3865,  1.0422]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.402
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 1.061
iter 20 loss: 0.178
Actual params: [-1.4021,  1.0612]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.416
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 1.079
iter 21 loss: 0.178
Actual params: [-1.4162,  1.0785]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.429
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 1.094
iter 22 loss: 0.178
Actual params: [-1.4292,  1.0944]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.441
-Original Grad: 0.000, -lr * Pred Grad:  0.015, New P: 1.109
iter 23 loss: 0.178
Actual params: [-1.441 ,  1.1089]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.452
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 1.122
iter 24 loss: 0.178
Actual params: [-1.4519,  1.1221]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.462
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 1.134
iter 25 loss: 0.178
Actual params: [-1.4617,  1.1343]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.471
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 1.145
iter 26 loss: 0.178
Actual params: [-1.4708,  1.1454]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.479
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 1.156
iter 27 loss: 0.178
Actual params: [-1.479 ,  1.1555]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.487
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 1.165
iter 28 loss: 0.178
Actual params: [-1.4866,  1.1648]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.493
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 1.173
iter 29 loss: 0.178
Actual params: [-1.4935,  1.1733]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.500
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 1.181
iter 30 loss: 0.178
Actual params: [-1.4998,  1.1811]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.506
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 1.188
Target params: [1.3344, 1.5708]
iter 0 loss: 0.188
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.035, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: -0.009, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.184
Actual params: [-0.3723, -0.0965]
-Original Grad: 0.061, -lr * Pred Grad:  0.098, New P: -0.274
-Original Grad: -0.003, -lr * Pred Grad:  -0.088, New P: -0.185
iter 2 loss: 0.181
Actual params: [-0.2745, -0.1847]
-Original Grad: 0.092, -lr * Pred Grad:  0.097, New P: -0.178
-Original Grad: 0.015, -lr * Pred Grad:  0.017, New P: -0.168
iter 3 loss: 0.173
Actual params: [-0.1777, -0.1681]
-Original Grad: 0.101, -lr * Pred Grad:  0.098, New P: -0.080
-Original Grad: 0.077, -lr * Pred Grad:  0.060, New P: -0.108
iter 4 loss: 0.156
Actual params: [-0.0797, -0.1084]
-Original Grad: 0.143, -lr * Pred Grad:  0.098, New P: 0.018
-Original Grad: 0.111, -lr * Pred Grad:  0.074, New P: -0.035
iter 5 loss: 0.127
Actual params: [ 0.0181, -0.0347]
-Original Grad: 0.252, -lr * Pred Grad:  0.094, New P: 0.112
-Original Grad: 0.088, -lr * Pred Grad:  0.082, New P: 0.047
iter 6 loss: 0.100
Actual params: [0.1121, 0.047 ]
-Original Grad: 0.167, -lr * Pred Grad:  0.096, New P: 0.208
-Original Grad: 0.123, -lr * Pred Grad:  0.087, New P: 0.134
iter 7 loss: 0.120
Actual params: [0.2082, 0.1344]
-Original Grad: -0.822, -lr * Pred Grad:  -0.011, New P: 0.198
-Original Grad: 0.318, -lr * Pred Grad:  0.083, New P: 0.218
iter 8 loss: 0.093
Actual params: [0.1976, 0.2177]
-Original Grad: 0.018, -lr * Pred Grad:  -0.008, New P: 0.189
-Original Grad: 0.094, -lr * Pred Grad:  0.084, New P: 0.301
iter 9 loss: 0.078
Actual params: [0.1892, 0.3012]
-Original Grad: -0.105, -lr * Pred Grad:  -0.013, New P: 0.176
-Original Grad: 0.067, -lr * Pred Grad:  0.082, New P: 0.383
iter 10 loss: 0.069
Actual params: [0.1761, 0.3829]
-Original Grad: -0.092, -lr * Pred Grad:  -0.017, New P: 0.160
-Original Grad: 0.059, -lr * Pred Grad:  0.079, New P: 0.462
iter 11 loss: 0.066
Actual params: [0.1595, 0.4624]
-Original Grad: 0.105, -lr * Pred Grad:  -0.009, New P: 0.150
-Original Grad: 0.012, -lr * Pred Grad:  0.073, New P: 0.535
iter 12 loss: 0.067
Actual params: [0.1502, 0.5352]
-Original Grad: 0.178, -lr * Pred Grad:  0.001, New P: 0.151
-Original Grad: 0.003, -lr * Pred Grad:  0.066, New P: 0.601
iter 13 loss: 0.066
Actual params: [0.1512, 0.6013]
-Original Grad: 0.182, -lr * Pred Grad:  0.010, New P: 0.161
-Original Grad: -0.006, -lr * Pred Grad:  0.059, New P: 0.660
iter 14 loss: 0.063
Actual params: [0.1613, 0.6602]
-Original Grad: 0.235, -lr * Pred Grad:  0.021, New P: 0.182
-Original Grad: -0.012, -lr * Pred Grad:  0.052, New P: 0.712
iter 15 loss: 0.059
Actual params: [0.1819, 0.7121]
-Original Grad: 0.193, -lr * Pred Grad:  0.028, New P: 0.210
-Original Grad: -0.010, -lr * Pred Grad:  0.046, New P: 0.758
iter 16 loss: 0.054
Actual params: [0.2097, 0.7579]
-Original Grad: 0.233, -lr * Pred Grad:  0.036, New P: 0.246
-Original Grad: -0.004, -lr * Pred Grad:  0.041, New P: 0.799
iter 17 loss: 0.051
Actual params: [0.2455, 0.799 ]
-Original Grad: 0.043, -lr * Pred Grad:  0.035, New P: 0.280
-Original Grad: -0.006, -lr * Pred Grad:  0.037, New P: 0.836
iter 18 loss: 0.050
Actual params: [0.28  , 0.8356]
-Original Grad: -0.029, -lr * Pred Grad:  0.030, New P: 0.310
-Original Grad: 0.019, -lr * Pred Grad:  0.036, New P: 0.871
iter 19 loss: 0.053
Actual params: [0.31  , 0.8713]
-Original Grad: -0.186, -lr * Pred Grad:  0.018, New P: 0.328
-Original Grad: 0.042, -lr * Pred Grad:  0.038, New P: 0.909
iter 20 loss: 0.054
Actual params: [0.3277, 0.9088]
-Original Grad: -0.280, -lr * Pred Grad:  0.002, New P: 0.330
-Original Grad: 0.104, -lr * Pred Grad:  0.046, New P: 0.955
iter 21 loss: 0.051
Actual params: [0.33  , 0.9547]
-Original Grad: -0.068, -lr * Pred Grad:  -0.001, New P: 0.329
-Original Grad: 0.034, -lr * Pred Grad:  0.046, New P: 1.001
iter 22 loss: 0.049
Actual params: [0.3287, 1.0006]
-Original Grad: 0.057, -lr * Pred Grad:  0.002, New P: 0.330
-Original Grad: -0.009, -lr * Pred Grad:  0.041, New P: 1.041
iter 23 loss: 0.047
Actual params: [0.3304, 1.0413]
-Original Grad: -0.073, -lr * Pred Grad:  -0.002, New P: 0.328
-Original Grad: 0.035, -lr * Pred Grad:  0.041, New P: 1.083
iter 24 loss: 0.047
Actual params: [0.3283, 1.0827]
-Original Grad: 0.045, -lr * Pred Grad:  0.000, New P: 0.329
-Original Grad: -0.004, -lr * Pred Grad:  0.037, New P: 1.120
iter 25 loss: 0.046
Actual params: [0.3286, 1.1198]
-Original Grad: 0.112, -lr * Pred Grad:  0.006, New P: 0.335
-Original Grad: -0.009, -lr * Pred Grad:  0.033, New P: 1.152
iter 26 loss: 0.045
Actual params: [0.3345, 1.1525]
-Original Grad: 0.093, -lr * Pred Grad:  0.010, New P: 0.345
-Original Grad: -0.012, -lr * Pred Grad:  0.028, New P: 1.181
iter 27 loss: 0.044
Actual params: [0.3445, 1.1806]
-Original Grad: 0.074, -lr * Pred Grad:  0.013, New P: 0.357
-Original Grad: -0.009, -lr * Pred Grad:  0.024, New P: 1.205
iter 28 loss: 0.044
Actual params: [0.3574, 1.205 ]
-Original Grad: 0.010, -lr * Pred Grad:  0.012, New P: 0.370
-Original Grad: 0.002, -lr * Pred Grad:  0.022, New P: 1.227
iter 29 loss: 0.043
Actual params: [0.3696, 1.2274]
-Original Grad: 0.109, -lr * Pred Grad:  0.017, New P: 0.386
-Original Grad: -0.031, -lr * Pred Grad:  0.016, New P: 1.244
iter 30 loss: 0.042
Actual params: [0.3864, 1.2437]
-Original Grad: 0.115, -lr * Pred Grad:  0.021, New P: 0.408
-Original Grad: -0.028, -lr * Pred Grad:  0.011, New P: 1.254
Target params: [1.3344, 1.5708]
iter 0 loss: 0.548
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.548
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.098, New P: -0.670
-Original Grad: -0.000, -lr * Pred Grad:  0.034, New P: 0.138
iter 2 loss: 0.548
Actual params: [-0.6703,  0.1377]
-Original Grad: -0.000, -lr * Pred Grad:  -0.083, New P: -0.754
-Original Grad: -0.000, -lr * Pred Grad:  0.022, New P: 0.160
iter 3 loss: 0.548
Actual params: [-0.7537,  0.1601]
-Original Grad: -0.000, -lr * Pred Grad:  -0.072, New P: -0.826
-Original Grad: 0.000, -lr * Pred Grad:  0.020, New P: 0.180
iter 4 loss: 0.548
Actual params: [-0.8259,  0.1803]
-Original Grad: -0.000, -lr * Pred Grad:  -0.063, New P: -0.889
-Original Grad: -0.000, -lr * Pred Grad:  0.010, New P: 0.191
iter 5 loss: 0.548
Actual params: [-0.8892,  0.1907]
-Original Grad: -0.000, -lr * Pred Grad:  -0.057, New P: -0.946
-Original Grad: -0.000, -lr * Pred Grad:  0.001, New P: 0.192
iter 6 loss: 0.548
Actual params: [-0.9459,  0.1916]
-Original Grad: -0.000, -lr * Pred Grad:  -0.051, New P: -0.996
-Original Grad: -0.000, -lr * Pred Grad:  -0.002, New P: 0.190
iter 7 loss: 0.548
Actual params: [-0.9964,  0.1896]
-Original Grad: -0.000, -lr * Pred Grad:  -0.045, New P: -1.042
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: 0.186
iter 8 loss: 0.548
Actual params: [-1.0416,  0.1863]
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -1.082
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: 0.184
iter 9 loss: 0.548
Actual params: [-1.082 ,  0.1836]
-Original Grad: -0.000, -lr * Pred Grad:  -0.037, New P: -1.119
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: 0.179
iter 10 loss: 0.548
Actual params: [-1.1187,  0.1791]
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -1.152
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: 0.174
iter 11 loss: 0.548
Actual params: [-1.1518,  0.174 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -1.182
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: 0.169
iter 12 loss: 0.548
Actual params: [-1.1818,  0.1691]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.209
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: 0.163
iter 13 loss: 0.548
Actual params: [-1.2091,  0.1629]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.234
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: 0.157
iter 14 loss: 0.548
Actual params: [-1.234 ,  0.1566]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.257
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: 0.150
iter 15 loss: 0.548
Actual params: [-1.2567,  0.1503]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.277
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: 0.144
iter 16 loss: 0.548
Actual params: [-1.2773,  0.1441]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.296
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: 0.138
iter 17 loss: 0.548
Actual params: [-1.2962,  0.1383]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.314
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: 0.132
iter 18 loss: 0.548
Actual params: [-1.3135,  0.1322]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.329
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: 0.127
iter 19 loss: 0.548
Actual params: [-1.3293,  0.1266]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.344
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: 0.121
iter 20 loss: 0.548
Actual params: [-1.3437,  0.1214]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.357
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: 0.116
iter 21 loss: 0.548
Actual params: [-1.3569,  0.1161]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.369
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: 0.111
iter 22 loss: 0.548
Actual params: [-1.369 ,  0.1111]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.380
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: 0.106
iter 23 loss: 0.548
Actual params: [-1.3801,  0.1065]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.390
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 0.102
iter 24 loss: 0.548
Actual params: [-1.3902,  0.1022]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.399
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 0.098
iter 25 loss: 0.548
Actual params: [-1.3995,  0.0982]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.408
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 0.094
iter 26 loss: 0.548
Actual params: [-1.408 ,  0.0942]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.416
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 0.090
iter 27 loss: 0.548
Actual params: [-1.4158,  0.0903]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.423
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 0.087
iter 28 loss: 0.548
Actual params: [-1.423 ,  0.0867]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.430
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 0.083
iter 29 loss: 0.548
Actual params: [-1.4295,  0.0831]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.436
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 0.080
iter 30 loss: 0.548
Actual params: [-1.4356,  0.0796]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.441
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: 0.076
Target params: [1.3344, 1.5708]
iter 0 loss: 0.003
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.000, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.003
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.000, -lr * Pred Grad:  -0.068, New P: -0.640
-Original Grad: -0.000, -lr * Pred Grad:  -0.074, New P: -0.170
iter 2 loss: 0.003
Actual params: [-0.6405, -0.1701]
-Original Grad: -0.000, -lr * Pred Grad:  -0.053, New P: -0.694
-Original Grad: -0.000, -lr * Pred Grad:  -0.059, New P: -0.229
iter 3 loss: 0.003
Actual params: [-0.6937, -0.2295]
-Original Grad: -0.000, -lr * Pred Grad:  -0.045, New P: -0.739
-Original Grad: -0.000, -lr * Pred Grad:  -0.050, New P: -0.280
iter 4 loss: 0.003
Actual params: [-0.739 , -0.2799]
-Original Grad: 0.000, -lr * Pred Grad:  -0.038, New P: -0.777
-Original Grad: -0.000, -lr * Pred Grad:  -0.043, New P: -0.323
iter 5 loss: 0.003
Actual params: [-0.777 , -0.3228]
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -0.810
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -0.360
iter 6 loss: 0.003
Actual params: [-0.8104, -0.3603]
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -0.840
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -0.393
iter 7 loss: 0.003
Actual params: [-0.84  , -0.3935]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -0.866
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -0.423
iter 8 loss: 0.003
Actual params: [-0.8665, -0.423 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.023, New P: -0.890
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -0.449
iter 9 loss: 0.003
Actual params: [-0.8898, -0.4493]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -0.911
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -0.473
iter 10 loss: 0.003
Actual params: [-0.9108, -0.4728]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -0.930
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -0.494
iter 11 loss: 0.003
Actual params: [-0.9297, -0.494 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.947
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -0.513
iter 12 loss: 0.003
Actual params: [-0.9468, -0.5132]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.962
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.531
iter 13 loss: 0.003
Actual params: [-0.9624, -0.5305]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.977
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -0.546
iter 14 loss: 0.003
Actual params: [-0.9766, -0.5462]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.989
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.560
iter 15 loss: 0.003
Actual params: [-0.9894, -0.5605]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.001
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.573
iter 16 loss: 0.003
Actual params: [-1.0011, -0.5735]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.012
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.585
iter 17 loss: 0.003
Actual params: [-1.0119, -0.5853]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.022
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.596
iter 18 loss: 0.003
Actual params: [-1.0217, -0.596 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.031
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.606
iter 19 loss: 0.003
Actual params: [-1.0307, -0.6058]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.039
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.615
iter 20 loss: 0.003
Actual params: [-1.0391, -0.6148]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.047
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.623
iter 21 loss: 0.003
Actual params: [-1.0468, -0.623 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.054
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.631
iter 22 loss: 0.003
Actual params: [-1.0539, -0.6305]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.060
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.637
iter 23 loss: 0.003
Actual params: [-1.0603, -0.6374]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.066
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.644
iter 24 loss: 0.003
Actual params: [-1.0663, -0.6436]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.072
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.649
iter 25 loss: 0.003
Actual params: [-1.0718, -0.6493]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.077
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.655
iter 26 loss: 0.003
Actual params: [-1.0768, -0.6546]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.081
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.659
iter 27 loss: 0.003
Actual params: [-1.0815, -0.6594]
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -1.086
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.664
iter 28 loss: 0.003
Actual params: [-1.0857, -0.6637]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.090
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.668
iter 29 loss: 0.003
Actual params: [-1.0896, -0.6677]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.093
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.671
iter 30 loss: 0.003
Actual params: [-1.0933, -0.6714]
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -1.097
-Original Grad: -0.000, -lr * Pred Grad:  -0.003, New P: -0.675
Target params: [1.3344, 1.5708]
iter 0 loss: 0.879
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.000, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.879
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.000, -lr * Pred Grad:  -0.087, New P: -0.660
-Original Grad: -0.000, -lr * Pred Grad:  -0.081, New P: -0.178
iter 2 loss: 0.879
Actual params: [-0.6597, -0.1776]
-Original Grad: -0.000, -lr * Pred Grad:  -0.074, New P: -0.734
-Original Grad: -0.000, -lr * Pred Grad:  -0.066, New P: -0.244
iter 3 loss: 0.879
Actual params: [-0.7341, -0.2437]
-Original Grad: -0.000, -lr * Pred Grad:  -0.064, New P: -0.798
-Original Grad: -0.000, -lr * Pred Grad:  -0.054, New P: -0.298
iter 4 loss: 0.879
Actual params: [-0.7984, -0.2978]
-Original Grad: -0.000, -lr * Pred Grad:  -0.056, New P: -0.855
-Original Grad: 0.000, -lr * Pred Grad:  -0.045, New P: -0.343
iter 5 loss: 0.879
Actual params: [-0.8549, -0.3433]
-Original Grad: -0.000, -lr * Pred Grad:  -0.050, New P: -0.905
-Original Grad: 0.000, -lr * Pred Grad:  -0.039, New P: -0.382
iter 6 loss: 0.879
Actual params: [-0.9047, -0.3822]
-Original Grad: -0.000, -lr * Pred Grad:  -0.044, New P: -0.949
-Original Grad: 0.000, -lr * Pred Grad:  -0.034, New P: -0.416
iter 7 loss: 0.879
Actual params: [-0.9489, -0.4159]
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -0.989
-Original Grad: 0.000, -lr * Pred Grad:  -0.029, New P: -0.445
iter 8 loss: 0.879
Actual params: [-0.9887, -0.4449]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -1.024
-Original Grad: 0.000, -lr * Pred Grad:  -0.025, New P: -0.470
iter 9 loss: 0.879
Actual params: [-1.0244, -0.4704]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -1.057
-Original Grad: 0.000, -lr * Pred Grad:  -0.022, New P: -0.493
iter 10 loss: 0.879
Actual params: [-1.0567, -0.4928]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.086
-Original Grad: 0.000, -lr * Pred Grad:  -0.020, New P: -0.512
iter 11 loss: 0.879
Actual params: [-1.086 , -0.5124]
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -1.113
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -0.530
iter 12 loss: 0.879
Actual params: [-1.1125, -0.5297]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.137
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -0.545
iter 13 loss: 0.879
Actual params: [-1.1368, -0.545 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.159
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -0.559
iter 14 loss: 0.879
Actual params: [-1.1588, -0.5586]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.179
-Original Grad: 0.000, -lr * Pred Grad:  -0.012, New P: -0.571
iter 15 loss: 0.879
Actual params: [-1.1789, -0.5706]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.197
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -0.581
iter 16 loss: 0.879
Actual params: [-1.1973, -0.5813]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.214
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -0.591
iter 17 loss: 0.879
Actual params: [-1.2141, -0.5908]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.229
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -0.599
iter 18 loss: 0.879
Actual params: [-1.2294, -0.5993]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.243
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -0.607
iter 19 loss: 0.879
Actual params: [-1.2435, -0.6069]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.256
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -0.614
iter 20 loss: 0.879
Actual params: [-1.2563, -0.6136]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.268
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -0.620
iter 21 loss: 0.879
Actual params: [-1.2681, -0.6196]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.279
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -0.625
iter 22 loss: 0.879
Actual params: [-1.2789, -0.6249]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.289
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -0.630
iter 23 loss: 0.879
Actual params: [-1.2888, -0.6296]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.298
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -0.634
iter 24 loss: 0.879
Actual params: [-1.2979, -0.6338]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.306
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -0.637
iter 25 loss: 0.879
Actual params: [-1.3063, -0.6374]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.314
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -0.641
iter 26 loss: 0.879
Actual params: [-1.314 , -0.6406]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.321
-Original Grad: 0.000, -lr * Pred Grad:  -0.003, New P: -0.643
iter 27 loss: 0.879
Actual params: [-1.3211, -0.6433]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.328
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: -0.646
iter 28 loss: 0.879
Actual params: [-1.3276, -0.6458]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.334
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: -0.648
iter 29 loss: 0.879
Actual params: [-1.3336, -0.6479]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.339
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: -0.650
iter 30 loss: 0.879
Actual params: [-1.3391, -0.6497]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.344
-Original Grad: 0.000, -lr * Pred Grad:  -0.002, New P: -0.651
Target params: [1.3344, 1.5708]
iter 0 loss: 0.119
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.011, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.007, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.118
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.002, -lr * Pred Grad:  -0.080, New P: -0.653
-Original Grad: -0.001, -lr * Pred Grad:  -0.082, New P: -0.178
iter 2 loss: 0.118
Actual params: [-0.6526, -0.1781]
-Original Grad: -0.001, -lr * Pred Grad:  -0.069, New P: -0.721
-Original Grad: -0.001, -lr * Pred Grad:  -0.070, New P: -0.248
iter 3 loss: 0.118
Actual params: [-0.7213, -0.2482]
-Original Grad: -0.000, -lr * Pred Grad:  -0.058, New P: -0.780
-Original Grad: -0.000, -lr * Pred Grad:  -0.060, New P: -0.308
iter 4 loss: 0.118
Actual params: [-0.7796, -0.3079]
-Original Grad: -0.000, -lr * Pred Grad:  -0.050, New P: -0.830
-Original Grad: -0.000, -lr * Pred Grad:  -0.052, New P: -0.359
iter 5 loss: 0.118
Actual params: [-0.8299, -0.3594]
-Original Grad: -0.000, -lr * Pred Grad:  -0.044, New P: -0.874
-Original Grad: -0.000, -lr * Pred Grad:  -0.045, New P: -0.405
iter 6 loss: 0.118
Actual params: [-0.8741, -0.4048]
-Original Grad: -0.000, -lr * Pred Grad:  -0.039, New P: -0.913
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -0.445
iter 7 loss: 0.118
Actual params: [-0.9131, -0.4447]
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -0.948
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -0.480
iter 8 loss: 0.118
Actual params: [-0.9478, -0.4802]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -0.979
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -0.512
iter 9 loss: 0.118
Actual params: [-0.9788, -0.5118]
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -1.006
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -0.540
iter 10 loss: 0.118
Actual params: [-1.0065, -0.5402]
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -1.031
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -0.566
iter 11 loss: 0.118
Actual params: [-1.0315, -0.5657]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.054
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -0.589
iter 12 loss: 0.118
Actual params: [-1.054 , -0.5887]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.074
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -0.609
iter 13 loss: 0.118
Actual params: [-1.0744, -0.6094]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.093
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -0.628
iter 14 loss: 0.118
Actual params: [-1.0928, -0.6283]
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.110
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.645
iter 15 loss: 0.118
Actual params: [-1.1096, -0.6453]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.125
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.661
iter 16 loss: 0.118
Actual params: [-1.1248, -0.6608]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.139
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -0.675
iter 17 loss: 0.118
Actual params: [-1.1386, -0.6749]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.151
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.688
iter 18 loss: 0.118
Actual params: [-1.1512, -0.6877]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.163
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -0.699
iter 19 loss: 0.118
Actual params: [-1.1627, -0.6993]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.173
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.710
iter 20 loss: 0.118
Actual params: [-1.1732, -0.7099]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.183
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -0.720
iter 21 loss: 0.118
Actual params: [-1.1827, -0.7196]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.191
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.728
iter 22 loss: 0.118
Actual params: [-1.1914, -0.7284]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.199
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -0.736
iter 23 loss: 0.118
Actual params: [-1.1994, -0.7364]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.207
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.744
iter 24 loss: 0.118
Actual params: [-1.2067, -0.7438]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.213
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.750
iter 25 loss: 0.118
Actual params: [-1.2133, -0.7505]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.219
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.757
iter 26 loss: 0.118
Actual params: [-1.2194, -0.7566]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.225
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -0.762
iter 27 loss: 0.118
Actual params: [-1.2249, -0.7621]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.230
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.767
iter 28 loss: 0.118
Actual params: [-1.23  , -0.7672]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.235
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.772
iter 29 loss: 0.118
Actual params: [-1.2347, -0.7719]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.239
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.776
iter 30 loss: 0.118
Actual params: [-1.2389, -0.7761]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.243
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -0.780
Target params: [1.3344, 1.5708]
iter 0 loss: 1.009
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.051, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.018, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 1.003
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.091, -lr * Pred Grad:  0.098, New P: -0.275
-Original Grad: 0.024, -lr * Pred Grad:  0.100, New P: 0.203
iter 2 loss: 0.993
Actual params: [-0.2747,  0.2032]
-Original Grad: 0.121, -lr * Pred Grad:  0.098, New P: -0.177
-Original Grad: 0.029, -lr * Pred Grad:  0.100, New P: 0.303
iter 3 loss: 0.983
Actual params: [-0.177 ,  0.3029]
-Original Grad: 0.154, -lr * Pred Grad:  0.098, New P: -0.079
-Original Grad: 0.023, -lr * Pred Grad:  0.100, New P: 0.403
iter 4 loss: 0.971
Actual params: [-0.0791,  0.4027]
-Original Grad: 0.219, -lr * Pred Grad:  0.097, New P: 0.018
-Original Grad: 0.014, -lr * Pred Grad:  0.096, New P: 0.499
iter 5 loss: 0.955
Actual params: [0.0181, 0.4987]
-Original Grad: 0.355, -lr * Pred Grad:  0.094, New P: 0.113
-Original Grad: -0.023, -lr * Pred Grad:  0.053, New P: 0.552
iter 6 loss: 0.932
Actual params: [0.1126, 0.5522]
-Original Grad: 0.448, -lr * Pred Grad:  0.095, New P: 0.207
-Original Grad: -0.034, -lr * Pred Grad:  0.013, New P: 0.565
iter 7 loss: 0.900
Actual params: [0.2073, 0.5648]
-Original Grad: 0.416, -lr * Pred Grad:  0.097, New P: 0.304
-Original Grad: 0.021, -lr * Pred Grad:  0.026, New P: 0.591
iter 8 loss: 0.859
Actual params: [0.3044, 0.5906]
-Original Grad: 0.512, -lr * Pred Grad:  0.099, New P: 0.403
-Original Grad: 0.047, -lr * Pred Grad:  0.047, New P: 0.637
iter 9 loss: 0.800
Actual params: [0.4032, 0.6373]
-Original Grad: 0.677, -lr * Pred Grad:  0.100, New P: 0.503
-Original Grad: 0.120, -lr * Pred Grad:  0.064, New P: 0.701
iter 10 loss: 0.722
Actual params: [0.503 , 0.7008]
-Original Grad: 0.990, -lr * Pred Grad:  0.099, New P: 0.602
-Original Grad: 0.250, -lr * Pred Grad:  0.070, New P: 0.771
iter 11 loss: 0.633
Actual params: [0.6021, 0.7711]
-Original Grad: 1.210, -lr * Pred Grad:  0.100, New P: 0.702
-Original Grad: 0.383, -lr * Pred Grad:  0.077, New P: 0.848
iter 12 loss: 0.550
Actual params: [0.7019, 0.8476]
-Original Grad: 1.660, -lr * Pred Grad:  0.100, New P: 0.801
-Original Grad: 0.580, -lr * Pred Grad:  0.081, New P: 0.929
iter 13 loss: 0.462
Actual params: [0.8015, 0.9287]
-Original Grad: 1.532, -lr * Pred Grad:  0.102, New P: 0.904
-Original Grad: 0.687, -lr * Pred Grad:  0.087, New P: 1.015
iter 14 loss: 0.368
Actual params: [0.9035, 1.0154]
-Original Grad: 0.724, -lr * Pred Grad:  0.101, New P: 1.005
-Original Grad: 0.674, -lr * Pred Grad:  0.092, New P: 1.108
iter 15 loss: 0.275
Actual params: [1.0048, 1.1077]
-Original Grad: -0.559, -lr * Pred Grad:  0.081, New P: 1.086
-Original Grad: 0.239, -lr * Pred Grad:  0.091, New P: 1.199
iter 16 loss: 0.201
Actual params: [1.0862, 1.1991]
-Original Grad: -0.414, -lr * Pred Grad:  0.067, New P: 1.153
-Original Grad: 0.799, -lr * Pred Grad:  0.096, New P: 1.296
iter 17 loss: 0.165
Actual params: [1.153 , 1.2956]
-Original Grad: -0.333, -lr * Pred Grad:  0.055, New P: 1.208
-Original Grad: 0.774, -lr * Pred Grad:  0.101, New P: 1.396
iter 18 loss: 0.134
Actual params: [1.208 , 1.3963]
-Original Grad: -0.227, -lr * Pred Grad:  0.046, New P: 1.254
-Original Grad: 0.659, -lr * Pred Grad:  0.104, New P: 1.500
iter 19 loss: 0.120
Actual params: [1.2542, 1.4998]
-Original Grad: -0.358, -lr * Pred Grad:  0.036, New P: 1.290
-Original Grad: 0.663, -lr * Pred Grad:  0.106, New P: 1.606
iter 20 loss: 0.129
Actual params: [1.2903, 1.6057]
-Original Grad: -0.177, -lr * Pred Grad:  0.030, New P: 1.320
-Original Grad: 0.406, -lr * Pred Grad:  0.105, New P: 1.711
iter 21 loss: 0.149
Actual params: [1.3203, 1.7106]
-Original Grad: -0.159, -lr * Pred Grad:  0.025, New P: 1.345
-Original Grad: 0.227, -lr * Pred Grad:  0.101, New P: 1.811
iter 22 loss: 0.179
Actual params: [1.3449, 1.8114]
-Original Grad: 0.100, -lr * Pred Grad:  0.024, New P: 1.369
-Original Grad: 0.011, -lr * Pred Grad:  0.092, New P: 1.903
iter 23 loss: 0.218
Actual params: [1.369 , 1.9034]
-Original Grad: 0.117, -lr * Pred Grad:  0.024, New P: 1.393
-Original Grad: -0.192, -lr * Pred Grad:  0.078, New P: 1.982
iter 24 loss: 0.269
Actual params: [1.3928, 1.9818]
-Original Grad: 0.396, -lr * Pred Grad:  0.028, New P: 1.421
-Original Grad: -0.802, -lr * Pred Grad:  0.046, New P: 2.028
iter 25 loss: 0.305
Actual params: [1.421 , 2.0276]
-Original Grad: 0.141, -lr * Pred Grad:  0.028, New P: 1.449
-Original Grad: -0.745, -lr * Pred Grad:  0.022, New P: 2.049
iter 26 loss: 0.318
Actual params: [1.449 , 2.0491]
-Original Grad: 0.250, -lr * Pred Grad:  0.030, New P: 1.479
-Original Grad: -0.829, -lr * Pred Grad:  -0.001, New P: 2.049
iter 27 loss: 0.303
Actual params: [1.4787, 2.0486]
-Original Grad: 0.339, -lr * Pred Grad:  0.033, New P: 1.511
-Original Grad: -0.969, -lr * Pred Grad:  -0.021, New P: 2.027
iter 28 loss: 0.260
Actual params: [1.5115, 2.0274]
-Original Grad: 0.236, -lr * Pred Grad:  0.034, New P: 1.545
-Original Grad: -0.632, -lr * Pred Grad:  -0.032, New P: 1.995
iter 29 loss: 0.226
Actual params: [1.5454, 1.9953]
-Original Grad: 0.074, -lr * Pred Grad:  0.032, New P: 1.577
-Original Grad: -0.269, -lr * Pred Grad:  -0.035, New P: 1.960
iter 30 loss: 0.210
Actual params: [1.5775, 1.9604]
-Original Grad: -0.538, -lr * Pred Grad:  0.019, New P: 1.597
-Original Grad: 0.015, -lr * Pred Grad:  -0.031, New P: 1.929
Target params: [1.3344, 1.5708]
iter 0 loss: 0.466
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.466
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.000, -lr * Pred Grad:  -0.092, New P: -0.665
-Original Grad: 0.000, -lr * Pred Grad:  0.098, New P: 0.201
iter 2 loss: 0.466
Actual params: [-0.6647,  0.2013]
-Original Grad: -0.000, -lr * Pred Grad:  -0.080, New P: -0.744
-Original Grad: 0.000, -lr * Pred Grad:  0.094, New P: 0.295
iter 3 loss: 0.466
Actual params: [-0.7445,  0.2952]
-Original Grad: -0.000, -lr * Pred Grad:  -0.066, New P: -0.810
-Original Grad: 0.000, -lr * Pred Grad:  0.088, New P: 0.384
iter 4 loss: 0.466
Actual params: [-0.8103,  0.3835]
-Original Grad: 0.000, -lr * Pred Grad:  -0.053, New P: -0.863
-Original Grad: 0.000, -lr * Pred Grad:  0.088, New P: 0.471
iter 5 loss: 0.466
Actual params: [-0.8631,  0.4712]
-Original Grad: 0.000, -lr * Pred Grad:  -0.042, New P: -0.905
-Original Grad: 0.000, -lr * Pred Grad:  0.087, New P: 0.558
iter 6 loss: 0.466
Actual params: [-0.9049,  0.5584]
-Original Grad: 0.000, -lr * Pred Grad:  -0.031, New P: -0.936
-Original Grad: 0.000, -lr * Pred Grad:  0.089, New P: 0.647
iter 7 loss: 0.466
Actual params: [-0.9362,  0.6469]
-Original Grad: 0.000, -lr * Pred Grad:  -0.020, New P: -0.956
-Original Grad: 0.000, -lr * Pred Grad:  0.092, New P: 0.739
iter 8 loss: 0.466
Actual params: [-0.9559,  0.7388]
-Original Grad: 0.000, -lr * Pred Grad:  -0.001, New P: -0.957
-Original Grad: 0.000, -lr * Pred Grad:  0.093, New P: 0.831
iter 9 loss: 0.466
Actual params: [-0.9566,  0.8315]
-Original Grad: 0.001, -lr * Pred Grad:  0.031, New P: -0.925
-Original Grad: 0.001, -lr * Pred Grad:  0.083, New P: 0.914
iter 10 loss: 0.466
Actual params: [-0.9253,  0.9145]
-Original Grad: 0.001, -lr * Pred Grad:  0.056, New P: -0.870
-Original Grad: 0.001, -lr * Pred Grad:  0.078, New P: 0.993
iter 11 loss: 0.466
Actual params: [-0.8697,  0.9929]
-Original Grad: 0.003, -lr * Pred Grad:  0.066, New P: -0.804
-Original Grad: 0.003, -lr * Pred Grad:  0.076, New P: 1.069
iter 12 loss: 0.465
Actual params: [-0.8038,  1.0686]
-Original Grad: 0.008, -lr * Pred Grad:  0.066, New P: -0.738
-Original Grad: 0.009, -lr * Pred Grad:  0.069, New P: 1.138
iter 13 loss: 0.464
Actual params: [-0.7381,  1.1381]
-Original Grad: 0.023, -lr * Pred Grad:  0.066, New P: -0.672
-Original Grad: 0.024, -lr * Pred Grad:  0.068, New P: 1.206
iter 14 loss: 0.460
Actual params: [-0.672 ,  1.2062]
-Original Grad: 0.029, -lr * Pred Grad:  0.076, New P: -0.596
-Original Grad: 0.029, -lr * Pred Grad:  0.077, New P: 1.283
iter 15 loss: 0.452
Actual params: [-0.5962,  1.2834]
-Original Grad: 0.060, -lr * Pred Grad:  0.078, New P: -0.518
-Original Grad: 0.055, -lr * Pred Grad:  0.080, New P: 1.364
iter 16 loss: 0.435
Actual params: [-0.5179,  1.3637]
-Original Grad: 0.197, -lr * Pred Grad:  0.070, New P: -0.448
-Original Grad: 0.168, -lr * Pred Grad:  0.073, New P: 1.437
iter 17 loss: 0.409
Actual params: [-0.4476,  1.4366]
-Original Grad: 0.145, -lr * Pred Grad:  0.081, New P: -0.367
-Original Grad: 0.118, -lr * Pred Grad:  0.083, New P: 1.519
iter 18 loss: 0.379
Actual params: [-0.3667,  1.5192]
-Original Grad: 0.237, -lr * Pred Grad:  0.088, New P: -0.279
-Original Grad: 0.132, -lr * Pred Grad:  0.090, New P: 1.609
iter 19 loss: 0.347
Actual params: [-0.2788,  1.6094]
-Original Grad: 0.272, -lr * Pred Grad:  0.094, New P: -0.185
-Original Grad: 0.104, -lr * Pred Grad:  0.095, New P: 1.705
iter 20 loss: 0.313
Actual params: [-0.1846,  1.7046]
-Original Grad: 0.217, -lr * Pred Grad:  0.100, New P: -0.085
-Original Grad: 0.054, -lr * Pred Grad:  0.095, New P: 1.799
iter 21 loss: 0.278
Actual params: [-0.0851,  1.7995]
-Original Grad: 0.322, -lr * Pred Grad:  0.104, New P: 0.019
-Original Grad: -0.024, -lr * Pred Grad:  0.082, New P: 1.881
iter 22 loss: 0.244
Actual params: [0.0191, 1.8811]
-Original Grad: 0.344, -lr * Pred Grad:  0.108, New P: 0.127
-Original Grad: -0.052, -lr * Pred Grad:  0.064, New P: 1.945
iter 23 loss: 0.210
Actual params: [0.1273, 1.9447]
-Original Grad: 0.285, -lr * Pred Grad:  0.111, New P: 0.239
-Original Grad: -0.050, -lr * Pred Grad:  0.048, New P: 1.993
iter 24 loss: 0.175
Actual params: [0.2387, 1.9927]
-Original Grad: 0.238, -lr * Pred Grad:  0.113, New P: 0.352
-Original Grad: -0.066, -lr * Pred Grad:  0.031, New P: 2.023
iter 25 loss: 0.143
Actual params: [0.3517, 2.0233]
-Original Grad: 0.195, -lr * Pred Grad:  0.113, New P: 0.465
-Original Grad: -0.131, -lr * Pred Grad:  0.003, New P: 2.027
iter 26 loss: 0.115
Actual params: [0.4648, 2.0267]
-Original Grad: 0.229, -lr * Pred Grad:  0.114, New P: 0.579
-Original Grad: -0.175, -lr * Pred Grad:  -0.023, New P: 2.003
iter 27 loss: 0.087
Actual params: [0.579 , 2.0035]
-Original Grad: 0.185, -lr * Pred Grad:  0.114, New P: 0.693
-Original Grad: -0.193, -lr * Pred Grad:  -0.045, New P: 1.959
iter 28 loss: 0.060
Actual params: [0.6926, 1.9589]
-Original Grad: 0.116, -lr * Pred Grad:  0.110, New P: 0.803
-Original Grad: -0.147, -lr * Pred Grad:  -0.057, New P: 1.902
iter 29 loss: 0.041
Actual params: [0.8027, 1.9019]
-Original Grad: 0.060, -lr * Pred Grad:  0.104, New P: 0.907
-Original Grad: -0.109, -lr * Pred Grad:  -0.064, New P: 1.838
iter 30 loss: 0.030
Actual params: [0.9068, 1.8378]
-Original Grad: 0.034, -lr * Pred Grad:  0.097, New P: 1.004
-Original Grad: -0.081, -lr * Pred Grad:  -0.068, New P: 1.770
Target params: [1.3344, 1.5708]
iter 0 loss: 0.644
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.003, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.001, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.644
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.001, -lr * Pred Grad:  -0.084, New P: -0.656
-Original Grad: -0.000, -lr * Pred Grad:  -0.085, New P: -0.181
iter 2 loss: 0.644
Actual params: [-0.6564, -0.1814]
-Original Grad: -0.000, -lr * Pred Grad:  -0.072, New P: -0.729
-Original Grad: -0.000, -lr * Pred Grad:  -0.073, New P: -0.254
iter 3 loss: 0.644
Actual params: [-0.7285, -0.2541]
-Original Grad: -0.000, -lr * Pred Grad:  -0.060, New P: -0.789
-Original Grad: -0.000, -lr * Pred Grad:  -0.060, New P: -0.314
iter 4 loss: 0.644
Actual params: [-0.7888, -0.3143]
-Original Grad: -0.000, -lr * Pred Grad:  -0.052, New P: -0.841
-Original Grad: -0.000, -lr * Pred Grad:  -0.052, New P: -0.366
iter 5 loss: 0.644
Actual params: [-0.8413, -0.3663]
-Original Grad: -0.000, -lr * Pred Grad:  -0.046, New P: -0.887
-Original Grad: -0.000, -lr * Pred Grad:  -0.045, New P: -0.412
iter 6 loss: 0.644
Actual params: [-0.8875, -0.4118]
-Original Grad: -0.000, -lr * Pred Grad:  -0.041, New P: -0.928
-Original Grad: -0.000, -lr * Pred Grad:  -0.040, New P: -0.452
iter 7 loss: 0.644
Actual params: [-0.9284, -0.4518]
-Original Grad: -0.000, -lr * Pred Grad:  -0.036, New P: -0.965
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -0.487
iter 8 loss: 0.644
Actual params: [-0.9649, -0.4872]
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -0.998
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -0.519
iter 9 loss: 0.644
Actual params: [-0.9976, -0.5188]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.027
-Original Grad: -0.000, -lr * Pred Grad:  -0.028, New P: -0.547
iter 10 loss: 0.644
Actual params: [-1.0269, -0.547 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.053
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -0.572
iter 11 loss: 0.644
Actual params: [-1.0534, -0.5723]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.077
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -0.595
iter 12 loss: 0.644
Actual params: [-1.0773, -0.5951]
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -1.099
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -0.616
iter 13 loss: 0.644
Actual params: [-1.0991, -0.6157]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.119
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -0.634
iter 14 loss: 0.644
Actual params: [-1.1188, -0.6344]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.137
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -0.651
iter 15 loss: 0.644
Actual params: [-1.1368, -0.6512]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.153
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -0.667
iter 16 loss: 0.644
Actual params: [-1.1531, -0.6665]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.168
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -0.680
iter 17 loss: 0.644
Actual params: [-1.168 , -0.6804]
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.182
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -0.693
iter 18 loss: 0.644
Actual params: [-1.1816, -0.693 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.194
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -0.705
iter 19 loss: 0.644
Actual params: [-1.194 , -0.7045]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.205
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -0.715
iter 20 loss: 0.644
Actual params: [-1.2053, -0.7149]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.216
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -0.724
iter 21 loss: 0.644
Actual params: [-1.2157, -0.7244]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.225
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -0.733
iter 22 loss: 0.644
Actual params: [-1.2251, -0.7331]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.234
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -0.741
iter 23 loss: 0.644
Actual params: [-1.2338, -0.7409]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.242
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.748
iter 24 loss: 0.644
Actual params: [-1.2418, -0.7481]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.249
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -0.755
iter 25 loss: 0.644
Actual params: [-1.249 , -0.7546]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.256
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -0.761
iter 26 loss: 0.644
Actual params: [-1.2557, -0.7606]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.262
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -0.766
iter 27 loss: 0.644
Actual params: [-1.2618, -0.766 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.267
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -0.771
iter 28 loss: 0.644
Actual params: [-1.2674, -0.7709]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.272
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -0.775
iter 29 loss: 0.644
Actual params: [-1.2725, -0.7754]
-Original Grad: -0.000, -lr * Pred Grad:  -0.005, New P: -1.277
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -0.779
iter 30 loss: 0.644
Actual params: [-1.2772, -0.7795]
-Original Grad: -0.000, -lr * Pred Grad:  -0.004, New P: -1.282
-Original Grad: 0.000, -lr * Pred Grad:  -0.004, New P: -0.783
Target params: [1.3344, 1.5708]
iter 0 loss: 0.511
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.022, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.026, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.503
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.065, -lr * Pred Grad:  0.092, New P: -0.280
-Original Grad: 0.076, -lr * Pred Grad:  0.092, New P: 0.196
iter 2 loss: 0.488
Actual params: [-0.2804,  0.1958]
-Original Grad: 0.081, -lr * Pred Grad:  0.095, New P: -0.186
-Original Grad: 0.096, -lr * Pred Grad:  0.095, New P: 0.291
iter 3 loss: 0.472
Actual params: [-0.1857,  0.2905]
-Original Grad: 0.081, -lr * Pred Grad:  0.097, New P: -0.089
-Original Grad: 0.091, -lr * Pred Grad:  0.097, New P: 0.387
iter 4 loss: 0.450
Actual params: [-0.0888,  0.3875]
-Original Grad: 0.086, -lr * Pred Grad:  0.098, New P: 0.010
-Original Grad: 0.091, -lr * Pred Grad:  0.098, New P: 0.486
iter 5 loss: 0.422
Actual params: [0.0095, 0.4858]
-Original Grad: 0.138, -lr * Pred Grad:  0.098, New P: 0.108
-Original Grad: 0.133, -lr * Pred Grad:  0.099, New P: 0.585
iter 6 loss: 0.387
Actual params: [0.1078, 0.5849]
-Original Grad: 0.265, -lr * Pred Grad:  0.093, New P: 0.201
-Original Grad: 0.238, -lr * Pred Grad:  0.096, New P: 0.681
iter 7 loss: 0.344
Actual params: [0.2008, 0.681 ]
-Original Grad: 0.377, -lr * Pred Grad:  0.092, New P: 0.292
-Original Grad: 0.289, -lr * Pred Grad:  0.096, New P: 0.777
iter 8 loss: 0.292
Actual params: [0.2924, 0.7773]
-Original Grad: 0.309, -lr * Pred Grad:  0.095, New P: 0.387
-Original Grad: 0.213, -lr * Pred Grad:  0.098, New P: 0.876
iter 9 loss: 0.237
Actual params: [0.3874, 0.8756]
-Original Grad: 0.267, -lr * Pred Grad:  0.097, New P: 0.484
-Original Grad: 0.175, -lr * Pred Grad:  0.099, New P: 0.975
iter 10 loss: 0.190
Actual params: [0.4845, 0.9745]
-Original Grad: 0.260, -lr * Pred Grad:  0.099, New P: 0.583
-Original Grad: 0.151, -lr * Pred Grad:  0.099, New P: 1.073
iter 11 loss: 0.148
Actual params: [0.5832, 1.0732]
-Original Grad: 0.255, -lr * Pred Grad:  0.100, New P: 0.683
-Original Grad: 0.140, -lr * Pred Grad:  0.098, New P: 1.171
iter 12 loss: 0.114
Actual params: [0.6831, 1.1711]
-Original Grad: 0.173, -lr * Pred Grad:  0.099, New P: 0.782
-Original Grad: 0.107, -lr * Pred Grad:  0.096, New P: 1.267
iter 13 loss: 0.086
Actual params: [0.7817, 1.267 ]
-Original Grad: 0.139, -lr * Pred Grad:  0.096, New P: 0.878
-Original Grad: 0.081, -lr * Pred Grad:  0.093, New P: 1.360
iter 14 loss: 0.064
Actual params: [0.878 , 1.3597]
-Original Grad: 0.088, -lr * Pred Grad:  0.092, New P: 0.970
-Original Grad: 0.072, -lr * Pred Grad:  0.089, New P: 1.449
iter 15 loss: 0.050
Actual params: [0.9701, 1.449 ]
-Original Grad: 0.117, -lr * Pred Grad:  0.090, New P: 1.060
-Original Grad: 0.072, -lr * Pred Grad:  0.086, New P: 1.535
iter 16 loss: 0.045
Actual params: [1.0599, 1.5354]
-Original Grad: 0.044, -lr * Pred Grad:  0.084, New P: 1.144
-Original Grad: -0.015, -lr * Pred Grad:  0.077, New P: 1.612
iter 17 loss: 0.041
Actual params: [1.144 , 1.6125]
-Original Grad: 0.042, -lr * Pred Grad:  0.079, New P: 1.223
-Original Grad: -0.017, -lr * Pred Grad:  0.069, New P: 1.681
iter 18 loss: 0.039
Actual params: [1.2229, 1.681 ]
-Original Grad: 0.028, -lr * Pred Grad:  0.073, New P: 1.296
-Original Grad: -0.017, -lr * Pred Grad:  0.061, New P: 1.742
iter 19 loss: 0.036
Actual params: [1.2963, 1.7418]
-Original Grad: 0.056, -lr * Pred Grad:  0.070, New P: 1.367
-Original Grad: -0.036, -lr * Pred Grad:  0.052, New P: 1.794
iter 20 loss: 0.032
Actual params: [1.3666, 1.7938]
-Original Grad: 0.056, -lr * Pred Grad:  0.067, New P: 1.434
-Original Grad: 0.000, -lr * Pred Grad:  0.047, New P: 1.841
iter 21 loss: 0.029
Actual params: [1.4339, 1.8411]
-Original Grad: 0.071, -lr * Pred Grad:  0.066, New P: 1.500
-Original Grad: -0.049, -lr * Pred Grad:  0.039, New P: 1.880
iter 22 loss: 0.025
Actual params: [1.4996, 1.8797]
-Original Grad: 0.012, -lr * Pred Grad:  0.061, New P: 1.560
-Original Grad: 0.000, -lr * Pred Grad:  0.035, New P: 1.915
iter 23 loss: 0.023
Actual params: [1.5602, 1.9149]
-Original Grad: 0.011, -lr * Pred Grad:  0.056, New P: 1.616
-Original Grad: -0.011, -lr * Pred Grad:  0.031, New P: 1.946
iter 24 loss: 0.023
Actual params: [1.6161, 1.9458]
-Original Grad: -0.008, -lr * Pred Grad:  0.050, New P: 1.667
-Original Grad: -0.007, -lr * Pred Grad:  0.028, New P: 1.973
iter 25 loss: 0.024
Actual params: [1.6665, 1.9733]
-Original Grad: 0.012, -lr * Pred Grad:  0.047, New P: 1.713
-Original Grad: -0.009, -lr * Pred Grad:  0.024, New P: 1.998
iter 26 loss: 0.027
Actual params: [1.7132, 1.9976]
-Original Grad: -0.049, -lr * Pred Grad:  0.039, New P: 1.752
-Original Grad: 0.001, -lr * Pred Grad:  0.022, New P: 2.020
iter 27 loss: 0.030
Actual params: [1.7523, 2.0197]
-Original Grad: -0.056, -lr * Pred Grad:  0.032, New P: 1.784
-Original Grad: -0.011, -lr * Pred Grad:  0.019, New P: 2.039
iter 28 loss: 0.033
Actual params: [1.784 , 2.0389]
-Original Grad: -0.068, -lr * Pred Grad:  0.024, New P: 1.808
-Original Grad: 0.010, -lr * Pred Grad:  0.018, New P: 2.057
iter 29 loss: 0.035
Actual params: [1.808 , 2.0573]
-Original Grad: -0.088, -lr * Pred Grad:  0.016, New P: 1.824
-Original Grad: -0.004, -lr * Pred Grad:  0.016, New P: 2.074
iter 30 loss: 0.038
Actual params: [1.8235, 2.0737]
-Original Grad: -0.087, -lr * Pred Grad:  0.008, New P: 1.831
-Original Grad: 0.012, -lr * Pred Grad:  0.016, New P: 2.090
Target params: [1.3344, 1.5708]
iter 0 loss: 0.563
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.004, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.009, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.562
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.014, -lr * Pred Grad:  0.090, New P: -0.282
-Original Grad: 0.013, -lr * Pred Grad:  0.099, New P: 0.202
iter 2 loss: 0.559
Actual params: [-0.2823,  0.2024]
-Original Grad: 0.036, -lr * Pred Grad:  0.085, New P: -0.197
-Original Grad: 0.027, -lr * Pred Grad:  0.094, New P: 0.296
iter 3 loss: 0.555
Actual params: [-0.1971,  0.2965]
-Original Grad: 0.047, -lr * Pred Grad:  0.089, New P: -0.108
-Original Grad: 0.031, -lr * Pred Grad:  0.096, New P: 0.392
iter 4 loss: 0.548
Actual params: [-0.1079,  0.392 ]
-Original Grad: 0.079, -lr * Pred Grad:  0.089, New P: -0.019
-Original Grad: 0.055, -lr * Pred Grad:  0.093, New P: 0.485
iter 5 loss: 0.539
Actual params: [-0.0185,  0.4852]
-Original Grad: 0.103, -lr * Pred Grad:  0.091, New P: 0.073
-Original Grad: 0.070, -lr * Pred Grad:  0.094, New P: 0.579
iter 6 loss: 0.527
Actual params: [0.0726, 0.5789]
-Original Grad: 0.143, -lr * Pred Grad:  0.092, New P: 0.165
-Original Grad: 0.073, -lr * Pred Grad:  0.096, New P: 0.675
iter 7 loss: 0.511
Actual params: [0.1647, 0.6749]
-Original Grad: 0.214, -lr * Pred Grad:  0.092, New P: 0.256
-Original Grad: 0.099, -lr * Pred Grad:  0.097, New P: 0.772
iter 8 loss: 0.490
Actual params: [0.2565, 0.7719]
-Original Grad: 0.260, -lr * Pred Grad:  0.093, New P: 0.350
-Original Grad: 0.103, -lr * Pred Grad:  0.099, New P: 0.871
iter 9 loss: 0.464
Actual params: [0.3499, 0.8707]
-Original Grad: 0.376, -lr * Pred Grad:  0.094, New P: 0.444
-Original Grad: 0.120, -lr * Pred Grad:  0.100, New P: 0.971
iter 10 loss: 0.428
Actual params: [0.4436, 0.9711]
-Original Grad: 0.447, -lr * Pred Grad:  0.095, New P: 0.539
-Original Grad: 0.160, -lr * Pred Grad:  0.101, New P: 1.072
iter 11 loss: 0.382
Actual params: [0.5391, 1.0723]
-Original Grad: 0.647, -lr * Pred Grad:  0.096, New P: 0.635
-Original Grad: 0.222, -lr * Pred Grad:  0.101, New P: 1.173
iter 12 loss: 0.331
Actual params: [0.6349, 1.1734]
-Original Grad: 0.580, -lr * Pred Grad:  0.099, New P: 0.734
-Original Grad: 0.241, -lr * Pred Grad:  0.102, New P: 1.276
iter 13 loss: 0.280
Actual params: [0.7338, 1.2758]
-Original Grad: 0.453, -lr * Pred Grad:  0.101, New P: 0.835
-Original Grad: 0.229, -lr * Pred Grad:  0.104, New P: 1.380
iter 14 loss: 0.223
Actual params: [0.8347, 1.3802]
-Original Grad: 0.518, -lr * Pred Grad:  0.103, New P: 0.938
-Original Grad: 0.211, -lr * Pred Grad:  0.106, New P: 1.486
iter 15 loss: 0.168
Actual params: [0.9378, 1.4862]
-Original Grad: 0.384, -lr * Pred Grad:  0.103, New P: 1.041
-Original Grad: 0.189, -lr * Pred Grad:  0.107, New P: 1.593
iter 16 loss: 0.128
Actual params: [1.0412, 1.5931]
-Original Grad: 0.256, -lr * Pred Grad:  0.101, New P: 1.142
-Original Grad: 0.112, -lr * Pred Grad:  0.105, New P: 1.698
iter 17 loss: 0.117
Actual params: [1.1425, 1.698 ]
-Original Grad: -0.168, -lr * Pred Grad:  0.085, New P: 1.228
-Original Grad: -0.219, -lr * Pred Grad:  0.071, New P: 1.769
iter 18 loss: 0.120
Actual params: [1.2278, 1.769 ]
-Original Grad: -0.329, -lr * Pred Grad:  0.064, New P: 1.292
-Original Grad: 0.041, -lr * Pred Grad:  0.068, New P: 1.837
iter 19 loss: 0.131
Actual params: [1.2918, 1.8368]
-Original Grad: -0.512, -lr * Pred Grad:  0.038, New P: 1.330
-Original Grad: 0.319, -lr * Pred Grad:  0.078, New P: 1.915
iter 20 loss: 0.119
Actual params: [1.3296, 1.9149]
-Original Grad: -0.452, -lr * Pred Grad:  0.018, New P: 1.348
-Original Grad: 0.496, -lr * Pred Grad:  0.088, New P: 2.002
iter 21 loss: 0.093
Actual params: [1.348 , 2.0024]
-Original Grad: -0.378, -lr * Pred Grad:  0.004, New P: 1.352
-Original Grad: 0.456, -lr * Pred Grad:  0.095, New P: 2.097
iter 22 loss: 0.067
Actual params: [1.3522, 2.0971]
-Original Grad: -0.320, -lr * Pred Grad:  -0.006, New P: 1.346
-Original Grad: 0.364, -lr * Pred Grad:  0.099, New P: 2.196
iter 23 loss: 0.060
Actual params: [1.346 , 2.1963]
-Original Grad: 0.023, -lr * Pred Grad:  -0.005, New P: 1.341
-Original Grad: -0.039, -lr * Pred Grad:  0.088, New P: 2.284
iter 24 loss: 0.073
Actual params: [1.341 , 2.2845]
-Original Grad: 0.244, -lr * Pred Grad:  0.003, New P: 1.344
-Original Grad: -0.226, -lr * Pred Grad:  0.067, New P: 2.351
iter 25 loss: 0.086
Actual params: [1.3444, 2.3515]
-Original Grad: 0.299, -lr * Pred Grad:  0.012, New P: 1.357
-Original Grad: -0.328, -lr * Pred Grad:  0.042, New P: 2.394
iter 26 loss: 0.093
Actual params: [1.3569, 2.3937]
-Original Grad: 0.327, -lr * Pred Grad:  0.022, New P: 1.378
-Original Grad: -0.350, -lr * Pred Grad:  0.020, New P: 2.414
iter 27 loss: 0.092
Actual params: [1.3784, 2.4137]
-Original Grad: 0.261, -lr * Pred Grad:  0.028, New P: 1.406
-Original Grad: -0.354, -lr * Pred Grad:  0.001, New P: 2.415
iter 28 loss: 0.086
Actual params: [1.406 , 2.4149]
-Original Grad: 0.172, -lr * Pred Grad:  0.031, New P: 1.437
-Original Grad: -0.233, -lr * Pred Grad:  -0.010, New P: 2.405
iter 29 loss: 0.079
Actual params: [1.4366, 2.4054]
-Original Grad: 0.102, -lr * Pred Grad:  0.031, New P: 1.468
-Original Grad: -0.154, -lr * Pred Grad:  -0.016, New P: 2.390
iter 30 loss: 0.073
Actual params: [1.4676, 2.3897]
-Original Grad: 0.022, -lr * Pred Grad:  0.029, New P: 1.497
-Original Grad: -0.137, -lr * Pred Grad:  -0.020, New P: 2.369
Target params: [1.3344, 1.5708]
iter 0 loss: 0.024
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.040, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.022, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.020
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.019, -lr * Pred Grad:  -0.093, New P: -0.665
-Original Grad: 0.015, -lr * Pred Grad:  0.097, New P: 0.201
iter 2 loss: 0.017
Actual params: [-0.6649,  0.2008]
-Original Grad: -0.013, -lr * Pred Grad:  -0.087, New P: -0.752
-Original Grad: 0.017, -lr * Pred Grad:  0.098, New P: 0.299
iter 3 loss: 0.015
Actual params: [-0.7516,  0.2986]
-Original Grad: -0.005, -lr * Pred Grad:  -0.076, New P: -0.828
-Original Grad: 0.009, -lr * Pred Grad:  0.092, New P: 0.391
iter 4 loss: 0.014
Actual params: [-0.8281,  0.3911]
-Original Grad: -0.008, -lr * Pred Grad:  -0.073, New P: -0.901
-Original Grad: 0.009, -lr * Pred Grad:  0.090, New P: 0.481
iter 5 loss: 0.014
Actual params: [-0.9006,  0.481 ]
-Original Grad: 0.001, -lr * Pred Grad:  -0.061, New P: -0.962
-Original Grad: 0.003, -lr * Pred Grad:  0.082, New P: 0.563
iter 6 loss: 0.014
Actual params: [-0.9617,  0.5631]
-Original Grad: 0.004, -lr * Pred Grad:  -0.048, New P: -1.010
-Original Grad: -0.000, -lr * Pred Grad:  0.072, New P: 0.635
iter 7 loss: 0.014
Actual params: [-1.01  ,  0.6347]
-Original Grad: 0.006, -lr * Pred Grad:  -0.036, New P: -1.046
-Original Grad: -0.002, -lr * Pred Grad:  0.061, New P: 0.695
iter 8 loss: 0.014
Actual params: [-1.0457,  0.6953]
-Original Grad: 0.006, -lr * Pred Grad:  -0.025, New P: -1.070
-Original Grad: -0.003, -lr * Pred Grad:  0.050, New P: 0.745
iter 9 loss: 0.014
Actual params: [-1.0704,  0.745 ]
-Original Grad: 0.004, -lr * Pred Grad:  -0.018, New P: -1.089
-Original Grad: -0.002, -lr * Pred Grad:  0.041, New P: 0.786
iter 10 loss: 0.014
Actual params: [-1.0889,  0.786 ]
-Original Grad: 0.004, -lr * Pred Grad:  -0.013, New P: -1.101
-Original Grad: -0.003, -lr * Pred Grad:  0.033, New P: 0.819
iter 11 loss: 0.015
Actual params: [-1.1015,  0.8188]
-Original Grad: 0.002, -lr * Pred Grad:  -0.009, New P: -1.110
-Original Grad: -0.002, -lr * Pred Grad:  0.027, New P: 0.846
iter 12 loss: 0.015
Actual params: [-1.1103,  0.8462]
-Original Grad: 0.004, -lr * Pred Grad:  -0.004, New P: -1.114
-Original Grad: -0.003, -lr * Pred Grad:  0.021, New P: 0.867
iter 13 loss: 0.015
Actual params: [-1.1145,  0.8669]
-Original Grad: 0.003, -lr * Pred Grad:  -0.001, New P: -1.115
-Original Grad: -0.002, -lr * Pred Grad:  0.016, New P: 0.883
iter 14 loss: 0.015
Actual params: [-1.1155,  0.8832]
-Original Grad: 0.003, -lr * Pred Grad:  0.002, New P: -1.114
-Original Grad: -0.002, -lr * Pred Grad:  0.012, New P: 0.895
iter 15 loss: 0.015
Actual params: [-1.1137,  0.8953]
-Original Grad: 0.002, -lr * Pred Grad:  0.004, New P: -1.110
-Original Grad: -0.002, -lr * Pred Grad:  0.009, New P: 0.904
iter 16 loss: 0.015
Actual params: [-1.1097,  0.9039]
-Original Grad: 0.003, -lr * Pred Grad:  0.007, New P: -1.103
-Original Grad: -0.002, -lr * Pred Grad:  0.005, New P: 0.909
iter 17 loss: 0.015
Actual params: [-1.1032,  0.9086]
-Original Grad: 0.003, -lr * Pred Grad:  0.009, New P: -1.095
-Original Grad: -0.002, -lr * Pred Grad:  0.001, New P: 0.910
iter 18 loss: 0.015
Actual params: [-1.0947,  0.9098]
-Original Grad: 0.002, -lr * Pred Grad:  0.010, New P: -1.085
-Original Grad: -0.002, -lr * Pred Grad:  -0.001, New P: 0.908
iter 19 loss: 0.015
Actual params: [-1.0848,  0.9085]
-Original Grad: 0.003, -lr * Pred Grad:  0.012, New P: -1.073
-Original Grad: -0.002, -lr * Pred Grad:  -0.004, New P: 0.904
iter 20 loss: 0.015
Actual params: [-1.0731,  0.9045]
-Original Grad: 0.003, -lr * Pred Grad:  0.014, New P: -1.059
-Original Grad: -0.003, -lr * Pred Grad:  -0.007, New P: 0.897
iter 21 loss: 0.015
Actual params: [-1.0589,  0.8971]
-Original Grad: 0.003, -lr * Pred Grad:  0.016, New P: -1.043
-Original Grad: -0.002, -lr * Pred Grad:  -0.010, New P: 0.887
iter 22 loss: 0.015
Actual params: [-1.043,  0.887]
-Original Grad: 0.003, -lr * Pred Grad:  0.017, New P: -1.026
-Original Grad: -0.002, -lr * Pred Grad:  -0.012, New P: 0.875
iter 23 loss: 0.014
Actual params: [-1.0256,  0.8748]
-Original Grad: 0.003, -lr * Pred Grad:  0.020, New P: -1.006
-Original Grad: -0.002, -lr * Pred Grad:  -0.015, New P: 0.860
iter 24 loss: 0.014
Actual params: [-1.006,  0.86 ]
-Original Grad: 0.004, -lr * Pred Grad:  0.023, New P: -0.983
-Original Grad: -0.003, -lr * Pred Grad:  -0.018, New P: 0.842
iter 25 loss: 0.014
Actual params: [-0.9835,  0.8416]
-Original Grad: 0.005, -lr * Pred Grad:  0.026, New P: -0.958
-Original Grad: -0.004, -lr * Pred Grad:  -0.023, New P: 0.819
iter 26 loss: 0.014
Actual params: [-0.9578,  0.8189]
-Original Grad: 0.004, -lr * Pred Grad:  0.028, New P: -0.930
-Original Grad: -0.003, -lr * Pred Grad:  -0.025, New P: 0.794
iter 27 loss: 0.014
Actual params: [-0.9297,  0.7936]
-Original Grad: 0.005, -lr * Pred Grad:  0.031, New P: -0.899
-Original Grad: -0.003, -lr * Pred Grad:  -0.028, New P: 0.766
iter 28 loss: 0.014
Actual params: [-0.8991,  0.7658]
-Original Grad: 0.005, -lr * Pred Grad:  0.034, New P: -0.865
-Original Grad: -0.002, -lr * Pred Grad:  -0.029, New P: 0.737
iter 29 loss: 0.013
Actual params: [-0.8652,  0.7366]
-Original Grad: 0.005, -lr * Pred Grad:  0.036, New P: -0.829
-Original Grad: -0.002, -lr * Pred Grad:  -0.030, New P: 0.707
iter 30 loss: 0.013
Actual params: [-0.8288,  0.7069]
-Original Grad: 0.005, -lr * Pred Grad:  0.039, New P: -0.790
-Original Grad: 0.000, -lr * Pred Grad:  -0.026, New P: 0.680
Target params: [1.3344, 1.5708]
iter 0 loss: 0.370
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.109, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.002, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.352
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.122, -lr * Pred Grad:  -0.100, New P: -0.672
-Original Grad: -0.024, -lr * Pred Grad:  -0.081, New P: -0.177
iter 2 loss: 0.335
Actual params: [-0.6725, -0.1773]
-Original Grad: -0.096, -lr * Pred Grad:  -0.099, New P: -0.772
-Original Grad: -0.027, -lr * Pred Grad:  -0.089, New P: -0.267
iter 3 loss: 0.316
Actual params: [-0.7716, -0.2666]
-Original Grad: -0.106, -lr * Pred Grad:  -0.099, New P: -0.871
-Original Grad: -0.056, -lr * Pred Grad:  -0.089, New P: -0.355
iter 4 loss: 0.299
Actual params: [-0.8708, -0.3551]
-Original Grad: -0.068, -lr * Pred Grad:  -0.096, New P: -0.967
-Original Grad: -0.043, -lr * Pred Grad:  -0.093, New P: -0.448
iter 5 loss: 0.291
Actual params: [-0.9671, -0.4476]
-Original Grad: -0.033, -lr * Pred Grad:  -0.090, New P: -1.057
-Original Grad: -0.025, -lr * Pred Grad:  -0.092, New P: -0.539
iter 6 loss: 0.286
Actual params: [-1.0568, -0.5393]
-Original Grad: -0.015, -lr * Pred Grad:  -0.081, New P: -1.138
-Original Grad: -0.012, -lr * Pred Grad:  -0.087, New P: -0.626
iter 7 loss: 0.285
Actual params: [-1.1383, -0.6259]
-Original Grad: -0.005, -lr * Pred Grad:  -0.073, New P: -1.211
-Original Grad: -0.004, -lr * Pred Grad:  -0.078, New P: -0.704
iter 8 loss: 0.284
Actual params: [-1.2111, -0.7043]
-Original Grad: -0.002, -lr * Pred Grad:  -0.065, New P: -1.276
-Original Grad: -0.002, -lr * Pred Grad:  -0.071, New P: -0.775
iter 9 loss: 0.284
Actual params: [-1.2762, -0.775 ]
-Original Grad: -0.001, -lr * Pred Grad:  -0.058, New P: -1.334
-Original Grad: -0.001, -lr * Pred Grad:  -0.064, New P: -0.839
iter 10 loss: 0.284
Actual params: [-1.3344, -0.8386]
-Original Grad: -0.000, -lr * Pred Grad:  -0.052, New P: -1.387
-Original Grad: -0.000, -lr * Pred Grad:  -0.057, New P: -0.896
iter 11 loss: 0.284
Actual params: [-1.3867, -0.8958]
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -1.434
-Original Grad: -0.000, -lr * Pred Grad:  -0.052, New P: -0.947
iter 12 loss: 0.284
Actual params: [-1.4338, -0.9474]
-Original Grad: -0.000, -lr * Pred Grad:  -0.042, New P: -1.476
-Original Grad: -0.000, -lr * Pred Grad:  -0.047, New P: -0.994
iter 13 loss: 0.284
Actual params: [-1.4762, -0.994 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -1.515
-Original Grad: -0.000, -lr * Pred Grad:  -0.042, New P: -1.036
iter 14 loss: 0.284
Actual params: [-1.5146, -1.0361]
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -1.549
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -1.074
iter 15 loss: 0.284
Actual params: [-1.5494, -1.0742]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.581
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -1.109
iter 16 loss: 0.284
Actual params: [-1.5808, -1.1088]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.609
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.140
iter 17 loss: 0.284
Actual params: [-1.6094, -1.1402]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.635
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.169
iter 18 loss: 0.284
Actual params: [-1.6354, -1.1687]
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.659
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.195
iter 19 loss: 0.284
Actual params: [-1.6589, -1.1946]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.680
-Original Grad: -0.000, -lr * Pred Grad:  -0.024, New P: -1.218
iter 20 loss: 0.284
Actual params: [-1.6804, -1.2182]
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.700
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.240
iter 21 loss: 0.284
Actual params: [-1.6999, -1.2397]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.718
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.259
iter 22 loss: 0.284
Actual params: [-1.7177, -1.2592]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.734
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.277
iter 23 loss: 0.284
Actual params: [-1.7339, -1.277 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.749
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.293
iter 24 loss: 0.284
Actual params: [-1.7486, -1.2932]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.762
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.308
iter 25 loss: 0.284
Actual params: [-1.762 , -1.3079]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.774
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.321
iter 26 loss: 0.284
Actual params: [-1.7742, -1.3214]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.785
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.334
iter 27 loss: 0.284
Actual params: [-1.7854, -1.3336]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.796
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.345
iter 28 loss: 0.284
Actual params: [-1.7955, -1.3448]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.805
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.355
iter 29 loss: 0.284
Actual params: [-1.8048, -1.355 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.813
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.364
iter 30 loss: 0.284
Actual params: [-1.8132, -1.3642]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.821
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.373
Target params: [1.3344, 1.5708]
iter 0 loss: 0.137
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.130, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.112, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.109
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.164, -lr * Pred Grad:  0.100, New P: -0.272
-Original Grad: 0.097, -lr * Pred Grad:  0.099, New P: 0.203
iter 2 loss: 0.082
Actual params: [-0.2724,  0.2029]
-Original Grad: 0.199, -lr * Pred Grad:  0.100, New P: -0.172
-Original Grad: 0.070, -lr * Pred Grad:  0.097, New P: 0.300
iter 3 loss: 0.058
Actual params: [-0.1724,  0.2997]
-Original Grad: 0.173, -lr * Pred Grad:  0.100, New P: -0.072
-Original Grad: 0.041, -lr * Pred Grad:  0.091, New P: 0.391
iter 4 loss: 0.043
Actual params: [-0.0723,  0.3906]
-Original Grad: 0.069, -lr * Pred Grad:  0.094, New P: 0.022
-Original Grad: 0.032, -lr * Pred Grad:  0.086, New P: 0.476
iter 5 loss: 0.037
Actual params: [0.0216, 0.4764]
-Original Grad: 0.020, -lr * Pred Grad:  0.084, New P: 0.105
-Original Grad: 0.035, -lr * Pred Grad:  0.083, New P: 0.559
iter 6 loss: 0.037
Actual params: [0.1055, 0.5592]
-Original Grad: -0.074, -lr * Pred Grad:  0.061, New P: 0.166
-Original Grad: 0.072, -lr * Pred Grad:  0.086, New P: 0.645
iter 7 loss: 0.036
Actual params: [0.1664, 0.6453]
-Original Grad: -0.098, -lr * Pred Grad:  0.038, New P: 0.205
-Original Grad: 0.080, -lr * Pred Grad:  0.089, New P: 0.734
iter 8 loss: 0.033
Actual params: [0.2046, 0.7345]
-Original Grad: -0.072, -lr * Pred Grad:  0.024, New P: 0.228
-Original Grad: 0.057, -lr * Pred Grad:  0.089, New P: 0.824
iter 9 loss: 0.029
Actual params: [0.2285, 0.8239]
-Original Grad: -0.062, -lr * Pred Grad:  0.013, New P: 0.242
-Original Grad: 0.063, -lr * Pred Grad:  0.090, New P: 0.914
iter 10 loss: 0.027
Actual params: [0.2415, 0.9141]
-Original Grad: 0.005, -lr * Pred Grad:  0.012, New P: 0.254
-Original Grad: 0.007, -lr * Pred Grad:  0.082, New P: 0.996
iter 11 loss: 0.028
Actual params: [0.2539, 0.9964]
-Original Grad: -0.028, -lr * Pred Grad:  0.007, New P: 0.261
-Original Grad: 0.002, -lr * Pred Grad:  0.074, New P: 1.071
iter 12 loss: 0.030
Actual params: [0.2613, 1.0707]
-Original Grad: 0.066, -lr * Pred Grad:  0.015, New P: 0.276
-Original Grad: -0.046, -lr * Pred Grad:  0.056, New P: 1.127
iter 13 loss: 0.031
Actual params: [0.2762, 1.1267]
-Original Grad: 0.025, -lr * Pred Grad:  0.017, New P: 0.293
-Original Grad: -0.018, -lr * Pred Grad:  0.047, New P: 1.173
iter 14 loss: 0.032
Actual params: [0.2929, 1.1732]
-Original Grad: 0.085, -lr * Pred Grad:  0.025, New P: 0.318
-Original Grad: -0.048, -lr * Pred Grad:  0.031, New P: 1.204
iter 15 loss: 0.031
Actual params: [0.3182, 1.2043]
-Original Grad: 0.088, -lr * Pred Grad:  0.033, New P: 0.351
-Original Grad: -0.040, -lr * Pred Grad:  0.020, New P: 1.224
iter 16 loss: 0.030
Actual params: [0.3513, 1.2239]
-Original Grad: -0.014, -lr * Pred Grad:  0.028, New P: 0.380
-Original Grad: 0.012, -lr * Pred Grad:  0.020, New P: 1.244
iter 17 loss: 0.029
Actual params: [0.3795, 1.2443]
-Original Grad: 0.045, -lr * Pred Grad:  0.031, New P: 0.411
-Original Grad: -0.015, -lr * Pred Grad:  0.015, New P: 1.259
iter 18 loss: 0.028
Actual params: [0.4106, 1.2595]
-Original Grad: 0.040, -lr * Pred Grad:  0.033, New P: 0.444
-Original Grad: -0.002, -lr * Pred Grad:  0.013, New P: 1.273
iter 19 loss: 0.027
Actual params: [0.4436, 1.2729]
-Original Grad: 0.050, -lr * Pred Grad:  0.036, New P: 0.480
-Original Grad: 0.005, -lr * Pred Grad:  0.013, New P: 1.286
iter 20 loss: 0.026
Actual params: [0.4795, 1.2862]
-Original Grad: 0.028, -lr * Pred Grad:  0.036, New P: 0.516
-Original Grad: 0.004, -lr * Pred Grad:  0.013, New P: 1.299
iter 21 loss: 0.025
Actual params: [0.5156, 1.2991]
-Original Grad: 0.009, -lr * Pred Grad:  0.034, New P: 0.550
-Original Grad: 0.033, -lr * Pred Grad:  0.019, New P: 1.318
iter 22 loss: 0.024
Actual params: [0.5496, 1.3179]
-Original Grad: 0.025, -lr * Pred Grad:  0.034, New P: 0.584
-Original Grad: 0.017, -lr * Pred Grad:  0.021, New P: 1.339
iter 23 loss: 0.023
Actual params: [0.5837, 1.3388]
-Original Grad: 0.042, -lr * Pred Grad:  0.036, New P: 0.620
-Original Grad: 0.007, -lr * Pred Grad:  0.021, New P: 1.359
iter 24 loss: 0.022
Actual params: [0.62  , 1.3593]
-Original Grad: -0.001, -lr * Pred Grad:  0.033, New P: 0.653
-Original Grad: 0.033, -lr * Pred Grad:  0.026, New P: 1.385
iter 25 loss: 0.021
Actual params: [0.6529, 1.3851]
-Original Grad: 0.029, -lr * Pred Grad:  0.034, New P: 0.687
-Original Grad: 0.007, -lr * Pred Grad:  0.025, New P: 1.410
iter 26 loss: 0.020
Actual params: [0.6867, 1.4101]
-Original Grad: 0.029, -lr * Pred Grad:  0.034, New P: 0.721
-Original Grad: 0.016, -lr * Pred Grad:  0.026, New P: 1.436
iter 27 loss: 0.019
Actual params: [0.7211, 1.4365]
-Original Grad: 0.006, -lr * Pred Grad:  0.032, New P: 0.753
-Original Grad: 0.020, -lr * Pred Grad:  0.029, New P: 1.465
iter 28 loss: 0.018
Actual params: [0.7534, 1.4651]
-Original Grad: -0.022, -lr * Pred Grad:  0.026, New P: 0.780
-Original Grad: 0.029, -lr * Pred Grad:  0.033, New P: 1.498
iter 29 loss: 0.018
Actual params: [0.7799, 1.4977]
-Original Grad: 0.016, -lr * Pred Grad:  0.026, New P: 0.806
-Original Grad: 0.005, -lr * Pred Grad:  0.031, New P: 1.529
iter 30 loss: 0.017
Actual params: [0.8062, 1.5286]
-Original Grad: 0.045, -lr * Pred Grad:  0.030, New P: 0.836
-Original Grad: -0.018, -lr * Pred Grad:  0.024, New P: 1.552
Target params: [1.3344, 1.5708]
iter 0 loss: 0.363
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.001, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.000, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.363
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.002, -lr * Pred Grad:  0.094, New P: -0.278
-Original Grad: 0.000, -lr * Pred Grad:  0.095, New P: 0.198
iter 2 loss: 0.363
Actual params: [-0.2781,  0.1981]
-Original Grad: 0.005, -lr * Pred Grad:  0.084, New P: -0.194
-Original Grad: 0.001, -lr * Pred Grad:  0.084, New P: 0.282
iter 3 loss: 0.362
Actual params: [-0.1937,  0.2823]
-Original Grad: 0.012, -lr * Pred Grad:  0.081, New P: -0.113
-Original Grad: 0.003, -lr * Pred Grad:  0.080, New P: 0.362
iter 4 loss: 0.360
Actual params: [-0.1128,  0.3619]
-Original Grad: 0.026, -lr * Pred Grad:  0.079, New P: -0.033
-Original Grad: 0.007, -lr * Pred Grad:  0.077, New P: 0.439
iter 5 loss: 0.356
Actual params: [-0.0335,  0.4392]
-Original Grad: 0.075, -lr * Pred Grad:  0.073, New P: 0.040
-Original Grad: 0.022, -lr * Pred Grad:  0.071, New P: 0.511
iter 6 loss: 0.347
Actual params: [0.04  , 0.5106]
-Original Grad: 0.106, -lr * Pred Grad:  0.079, New P: 0.119
-Original Grad: 0.034, -lr * Pred Grad:  0.077, New P: 0.588
iter 7 loss: 0.328
Actual params: [0.1191, 0.588 ]
-Original Grad: 0.217, -lr * Pred Grad:  0.079, New P: 0.198
-Original Grad: 0.072, -lr * Pred Grad:  0.077, New P: 0.665
iter 8 loss: 0.298
Actual params: [0.1977, 0.6649]
-Original Grad: 0.401, -lr * Pred Grad:  0.079, New P: 0.276
-Original Grad: 0.126, -lr * Pred Grad:  0.078, New P: 0.743
iter 9 loss: 0.260
Actual params: [0.2764, 0.7432]
-Original Grad: 0.456, -lr * Pred Grad:  0.084, New P: 0.361
-Original Grad: 0.117, -lr * Pred Grad:  0.085, New P: 0.828
iter 10 loss: 0.206
Actual params: [0.3607, 0.8282]
-Original Grad: 0.623, -lr * Pred Grad:  0.088, New P: 0.449
-Original Grad: 0.122, -lr * Pred Grad:  0.090, New P: 0.918
iter 11 loss: 0.142
Actual params: [0.4486, 0.9182]
-Original Grad: 0.639, -lr * Pred Grad:  0.092, New P: 0.541
-Original Grad: 0.061, -lr * Pred Grad:  0.091, New P: 1.009
iter 12 loss: 0.100
Actual params: [0.541, 1.009]
-Original Grad: 0.250, -lr * Pred Grad:  0.092, New P: 0.633
-Original Grad: 0.001, -lr * Pred Grad:  0.082, New P: 1.091
iter 13 loss: 0.078
Actual params: [0.6328, 1.0909]
-Original Grad: 0.182, -lr * Pred Grad:  0.090, New P: 0.722
-Original Grad: -0.021, -lr * Pred Grad:  0.069, New P: 1.160
iter 14 loss: 0.061
Actual params: [0.7224, 1.1604]
-Original Grad: 0.268, -lr * Pred Grad:  0.090, New P: 0.812
-Original Grad: -0.032, -lr * Pred Grad:  0.056, New P: 1.216
iter 15 loss: 0.047
Actual params: [0.8124, 1.2161]
-Original Grad: 0.128, -lr * Pred Grad:  0.086, New P: 0.899
-Original Grad: 0.021, -lr * Pred Grad:  0.055, New P: 1.271
iter 16 loss: 0.034
Actual params: [0.8987, 1.2707]
-Original Grad: 0.133, -lr * Pred Grad:  0.083, New P: 0.982
-Original Grad: 0.031, -lr * Pred Grad:  0.055, New P: 1.326
iter 17 loss: 0.023
Actual params: [0.9821, 1.3261]
-Original Grad: 0.129, -lr * Pred Grad:  0.081, New P: 1.063
-Original Grad: -0.003, -lr * Pred Grad:  0.050, New P: 1.376
iter 18 loss: 0.019
Actual params: [1.0627, 1.3758]
-Original Grad: -0.007, -lr * Pred Grad:  0.073, New P: 1.136
-Original Grad: -0.031, -lr * Pred Grad:  0.038, New P: 1.414
iter 19 loss: 0.019
Actual params: [1.1356, 1.4141]
-Original Grad: 0.038, -lr * Pred Grad:  0.068, New P: 1.204
-Original Grad: -0.066, -lr * Pred Grad:  0.020, New P: 1.434
iter 20 loss: 0.020
Actual params: [1.2036, 1.4343]
-Original Grad: -0.010, -lr * Pred Grad:  0.061, New P: 1.265
-Original Grad: -0.101, -lr * Pred Grad:  -0.002, New P: 1.432
iter 21 loss: 0.020
Actual params: [1.2649, 1.4323]
-Original Grad: -0.005, -lr * Pred Grad:  0.056, New P: 1.321
-Original Grad: -0.117, -lr * Pred Grad:  -0.022, New P: 1.410
iter 22 loss: 0.019
Actual params: [1.3206, 1.41  ]
-Original Grad: -0.028, -lr * Pred Grad:  0.049, New P: 1.370
-Original Grad: -0.099, -lr * Pred Grad:  -0.036, New P: 1.374
iter 23 loss: 0.017
Actual params: [1.37  , 1.3741]
-Original Grad: -0.050, -lr * Pred Grad:  0.043, New P: 1.413
-Original Grad: -0.096, -lr * Pred Grad:  -0.047, New P: 1.327
iter 24 loss: 0.016
Actual params: [1.4127, 1.3273]
-Original Grad: -0.048, -lr * Pred Grad:  0.037, New P: 1.450
-Original Grad: -0.044, -lr * Pred Grad:  -0.050, New P: 1.278
iter 25 loss: 0.016
Actual params: [1.4495, 1.2778]
-Original Grad: -0.070, -lr * Pred Grad:  0.030, New P: 1.480
-Original Grad: -0.038, -lr * Pred Grad:  -0.051, New P: 1.227
iter 26 loss: 0.017
Actual params: [1.4798, 1.2267]
-Original Grad: -0.076, -lr * Pred Grad:  0.024, New P: 1.504
-Original Grad: -0.025, -lr * Pred Grad:  -0.051, New P: 1.176
iter 27 loss: 0.018
Actual params: [1.5039, 1.1762]
-Original Grad: -0.063, -lr * Pred Grad:  0.019, New P: 1.523
-Original Grad: -0.015, -lr * Pred Grad:  -0.049, New P: 1.128
iter 28 loss: 0.019
Actual params: [1.5228, 1.1276]
-Original Grad: -0.115, -lr * Pred Grad:  0.012, New P: 1.535
-Original Grad: 0.010, -lr * Pred Grad:  -0.043, New P: 1.085
iter 29 loss: 0.021
Actual params: [1.5347, 1.085 ]
-Original Grad: -0.116, -lr * Pred Grad:  0.005, New P: 1.540
-Original Grad: 0.009, -lr * Pred Grad:  -0.037, New P: 1.048
iter 30 loss: 0.022
Actual params: [1.54  , 1.0478]
-Original Grad: -0.097, -lr * Pred Grad:  0.000, New P: 1.540
-Original Grad: 0.006, -lr * Pred Grad:  -0.033, New P: 1.015
Target params: [1.3344, 1.5708]
iter 0 loss: 0.296
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.030, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.029, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.291
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.011, -lr * Pred Grad:  -0.089, New P: -0.661
-Original Grad: 0.034, -lr * Pred Grad:  0.100, New P: 0.204
iter 2 loss: 0.288
Actual params: [-0.6614,  0.2036]
-Original Grad: 0.001, -lr * Pred Grad:  -0.067, New P: -0.728
-Original Grad: 0.026, -lr * Pred Grad:  0.099, New P: 0.303
iter 3 loss: 0.285
Actual params: [-0.7283,  0.3025]
-Original Grad: 0.014, -lr * Pred Grad:  -0.026, New P: -0.755
-Original Grad: 0.036, -lr * Pred Grad:  0.100, New P: 0.402
iter 4 loss: 0.281
Actual params: [-0.7547,  0.4023]
-Original Grad: 0.017, -lr * Pred Grad:  0.004, New P: -0.751
-Original Grad: 0.036, -lr * Pred Grad:  0.100, New P: 0.503
iter 5 loss: 0.277
Actual params: [-0.7508,  0.5025]
-Original Grad: 0.027, -lr * Pred Grad:  0.032, New P: -0.718
-Original Grad: 0.039, -lr * Pred Grad:  0.101, New P: 0.603
iter 6 loss: 0.271
Actual params: [-0.7184,  0.6033]
-Original Grad: 0.050, -lr * Pred Grad:  0.056, New P: -0.662
-Original Grad: 0.058, -lr * Pred Grad:  0.101, New P: 0.704
iter 7 loss: 0.265
Actual params: [-0.6621,  0.7044]
-Original Grad: 0.035, -lr * Pred Grad:  0.067, New P: -0.595
-Original Grad: 0.033, -lr * Pred Grad:  0.100, New P: 0.805
iter 8 loss: 0.257
Actual params: [-0.5952,  0.8047]
-Original Grad: 0.049, -lr * Pred Grad:  0.076, New P: -0.519
-Original Grad: 0.038, -lr * Pred Grad:  0.100, New P: 0.905
iter 9 loss: 0.247
Actual params: [-0.5188,  0.9049]
-Original Grad: 0.076, -lr * Pred Grad:  0.083, New P: -0.436
-Original Grad: 0.045, -lr * Pred Grad:  0.101, New P: 1.006
iter 10 loss: 0.235
Actual params: [-0.4355,  1.006 ]
-Original Grad: 0.109, -lr * Pred Grad:  0.088, New P: -0.348
-Original Grad: 0.051, -lr * Pred Grad:  0.102, New P: 1.108
iter 11 loss: 0.221
Actual params: [-0.3478,  1.1083]
-Original Grad: 0.151, -lr * Pred Grad:  0.090, New P: -0.257
-Original Grad: 0.046, -lr * Pred Grad:  0.103, New P: 1.211
iter 12 loss: 0.206
Actual params: [-0.2574,  1.211 ]
-Original Grad: 0.157, -lr * Pred Grad:  0.094, New P: -0.163
-Original Grad: 0.014, -lr * Pred Grad:  0.097, New P: 1.308
iter 13 loss: 0.191
Actual params: [-0.1631,  1.3078]
-Original Grad: 0.160, -lr * Pred Grad:  0.098, New P: -0.065
-Original Grad: 0.005, -lr * Pred Grad:  0.089, New P: 1.397
iter 14 loss: 0.175
Actual params: [-0.0651,  1.3969]
-Original Grad: 0.185, -lr * Pred Grad:  0.101, New P: 0.036
-Original Grad: -0.014, -lr * Pred Grad:  0.075, New P: 1.472
iter 15 loss: 0.154
Actual params: [0.036 , 1.4722]
-Original Grad: 0.211, -lr * Pred Grad:  0.104, New P: 0.140
-Original Grad: 0.019, -lr * Pred Grad:  0.074, New P: 1.546
iter 16 loss: 0.123
Actual params: [0.1398, 1.5464]
-Original Grad: 0.274, -lr * Pred Grad:  0.106, New P: 0.246
-Original Grad: 0.069, -lr * Pred Grad:  0.082, New P: 1.629
iter 17 loss: 0.085
Actual params: [0.2456, 1.6285]
-Original Grad: 0.255, -lr * Pred Grad:  0.108, New P: 0.354
-Original Grad: 0.046, -lr * Pred Grad:  0.086, New P: 1.714
iter 18 loss: 0.050
Actual params: [0.3538, 1.7141]
-Original Grad: 0.246, -lr * Pred Grad:  0.110, New P: 0.464
-Original Grad: 0.050, -lr * Pred Grad:  0.089, New P: 1.803
iter 19 loss: 0.027
Actual params: [0.4639, 1.8032]
-Original Grad: 0.256, -lr * Pred Grad:  0.112, New P: 0.576
-Original Grad: 0.033, -lr * Pred Grad:  0.089, New P: 1.892
iter 20 loss: 0.016
Actual params: [0.5759, 1.8923]
-Original Grad: 0.068, -lr * Pred Grad:  0.107, New P: 0.683
-Original Grad: -0.038, -lr * Pred Grad:  0.068, New P: 1.961
iter 21 loss: 0.016
Actual params: [0.6825, 1.9607]
-Original Grad: -0.014, -lr * Pred Grad:  0.096, New P: 0.778
-Original Grad: -0.100, -lr * Pred Grad:  0.029, New P: 1.990
iter 22 loss: 0.020
Actual params: [0.7784, 1.9897]
-Original Grad: 0.009, -lr * Pred Grad:  0.088, New P: 0.866
-Original Grad: -0.104, -lr * Pred Grad:  -0.000, New P: 1.990
iter 23 loss: 0.024
Actual params: [0.8665, 1.9897]
-Original Grad: -0.060, -lr * Pred Grad:  0.075, New P: 0.942
-Original Grad: -0.102, -lr * Pred Grad:  -0.022, New P: 1.968
iter 24 loss: 0.025
Actual params: [0.9416, 1.968 ]
-Original Grad: -0.052, -lr * Pred Grad:  0.064, New P: 1.006
-Original Grad: -0.120, -lr * Pred Grad:  -0.041, New P: 1.927
iter 25 loss: 0.024
Actual params: [1.0057, 1.927 ]
-Original Grad: -0.020, -lr * Pred Grad:  0.057, New P: 1.062
-Original Grad: -0.089, -lr * Pred Grad:  -0.052, New P: 1.875
iter 26 loss: 0.021
Actual params: [1.0625, 1.875 ]
-Original Grad: -0.008, -lr * Pred Grad:  0.051, New P: 1.113
-Original Grad: -0.086, -lr * Pred Grad:  -0.061, New P: 1.814
iter 27 loss: 0.017
Actual params: [1.1135, 1.8141]
-Original Grad: -0.024, -lr * Pred Grad:  0.044, New P: 1.158
-Original Grad: -0.059, -lr * Pred Grad:  -0.065, New P: 1.749
iter 28 loss: 0.014
Actual params: [1.1579, 1.749 ]
-Original Grad: -0.035, -lr * Pred Grad:  0.038, New P: 1.195
-Original Grad: -0.051, -lr * Pred Grad:  -0.068, New P: 1.681
iter 29 loss: 0.012
Actual params: [1.1954, 1.6814]
-Original Grad: -0.004, -lr * Pred Grad:  0.034, New P: 1.229
-Original Grad: -0.038, -lr * Pred Grad:  -0.068, New P: 1.613
iter 30 loss: 0.010
Actual params: [1.2292, 1.6134]
-Original Grad: -0.004, -lr * Pred Grad:  0.030, New P: 1.260
-Original Grad: -0.016, -lr * Pred Grad:  -0.065, New P: 1.549
Target params: [1.3344, 1.5708]
iter 0 loss: 0.494
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.071, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: -0.043, -lr * Pred Grad:  -0.100, New P: -0.096
iter 1 loss: 0.470
Actual params: [-0.5723, -0.0965]
-Original Grad: -0.070, -lr * Pred Grad:  -0.100, New P: -0.672
-Original Grad: -0.047, -lr * Pred Grad:  -0.100, New P: -0.197
iter 2 loss: 0.449
Actual params: [-0.6723, -0.1966]
-Original Grad: -0.058, -lr * Pred Grad:  -0.099, New P: -0.771
-Original Grad: -0.045, -lr * Pred Grad:  -0.100, New P: -0.297
iter 3 loss: 0.439
Actual params: [-0.7712, -0.2967]
-Original Grad: -0.025, -lr * Pred Grad:  -0.091, New P: -0.863
-Original Grad: -0.021, -lr * Pred Grad:  -0.094, New P: -0.391
iter 4 loss: 0.435
Actual params: [-0.8626, -0.391 ]
-Original Grad: -0.007, -lr * Pred Grad:  -0.080, New P: -0.943
-Original Grad: -0.006, -lr * Pred Grad:  -0.083, New P: -0.474
iter 5 loss: 0.434
Actual params: [-0.9427, -0.4745]
-Original Grad: -0.003, -lr * Pred Grad:  -0.070, New P: -1.013
-Original Grad: -0.003, -lr * Pred Grad:  -0.074, New P: -0.548
iter 6 loss: 0.433
Actual params: [-1.013 , -0.5481]
-Original Grad: -0.001, -lr * Pred Grad:  -0.062, New P: -1.075
-Original Grad: -0.001, -lr * Pred Grad:  -0.065, New P: -0.613
iter 7 loss: 0.433
Actual params: [-1.0748, -0.6131]
-Original Grad: -0.000, -lr * Pred Grad:  -0.055, New P: -1.129
-Original Grad: -0.000, -lr * Pred Grad:  -0.058, New P: -0.671
iter 8 loss: 0.433
Actual params: [-1.1295, -0.6706]
-Original Grad: -0.000, -lr * Pred Grad:  -0.049, New P: -1.178
-Original Grad: -0.000, -lr * Pred Grad:  -0.051, New P: -0.722
iter 9 loss: 0.433
Actual params: [-1.1781, -0.7219]
-Original Grad: -0.000, -lr * Pred Grad:  -0.043, New P: -1.222
-Original Grad: -0.000, -lr * Pred Grad:  -0.046, New P: -0.768
iter 10 loss: 0.433
Actual params: [-1.2216, -0.7677]
-Original Grad: -0.000, -lr * Pred Grad:  -0.039, New P: -1.261
-Original Grad: -0.000, -lr * Pred Grad:  -0.041, New P: -0.809
iter 11 loss: 0.433
Actual params: [-1.2606, -0.8088]
-Original Grad: -0.000, -lr * Pred Grad:  -0.035, New P: -1.296
-Original Grad: -0.000, -lr * Pred Grad:  -0.037, New P: -0.846
iter 12 loss: 0.433
Actual params: [-1.2957, -0.8458]
-Original Grad: -0.000, -lr * Pred Grad:  -0.032, New P: -1.327
-Original Grad: -0.000, -lr * Pred Grad:  -0.033, New P: -0.879
iter 13 loss: 0.433
Actual params: [-1.3273, -0.8792]
-Original Grad: -0.000, -lr * Pred Grad:  -0.029, New P: -1.356
-Original Grad: -0.000, -lr * Pred Grad:  -0.030, New P: -0.909
iter 14 loss: 0.433
Actual params: [-1.3559, -0.9094]
-Original Grad: -0.000, -lr * Pred Grad:  -0.026, New P: -1.382
-Original Grad: -0.000, -lr * Pred Grad:  -0.027, New P: -0.937
iter 15 loss: 0.433
Actual params: [-1.3818, -0.9367]
-Original Grad: -0.000, -lr * Pred Grad:  -0.023, New P: -1.405
-Original Grad: -0.000, -lr * Pred Grad:  -0.025, New P: -0.961
iter 16 loss: 0.433
Actual params: [-1.4052, -0.9615]
-Original Grad: -0.000, -lr * Pred Grad:  -0.021, New P: -1.427
-Original Grad: -0.000, -lr * Pred Grad:  -0.022, New P: -0.984
iter 17 loss: 0.433
Actual params: [-1.4265, -0.9839]
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.446
-Original Grad: -0.000, -lr * Pred Grad:  -0.020, New P: -1.004
iter 18 loss: 0.433
Actual params: [-1.4459, -1.0043]
-Original Grad: -0.000, -lr * Pred Grad:  -0.018, New P: -1.463
-Original Grad: -0.000, -lr * Pred Grad:  -0.019, New P: -1.023
iter 19 loss: 0.433
Actual params: [-1.4634, -1.0229]
-Original Grad: -0.000, -lr * Pred Grad:  -0.016, New P: -1.479
-Original Grad: -0.000, -lr * Pred Grad:  -0.017, New P: -1.040
iter 20 loss: 0.433
Actual params: [-1.4794, -1.0398]
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.494
-Original Grad: -0.000, -lr * Pred Grad:  -0.015, New P: -1.055
iter 21 loss: 0.433
Actual params: [-1.4939, -1.0551]
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.507
-Original Grad: -0.000, -lr * Pred Grad:  -0.014, New P: -1.069
iter 22 loss: 0.433
Actual params: [-1.5072, -1.0691]
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.519
-Original Grad: -0.000, -lr * Pred Grad:  -0.013, New P: -1.082
iter 23 loss: 0.433
Actual params: [-1.5192, -1.0818]
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.530
-Original Grad: -0.000, -lr * Pred Grad:  -0.012, New P: -1.093
iter 24 loss: 0.433
Actual params: [-1.5302, -1.0934]
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.540
-Original Grad: -0.000, -lr * Pred Grad:  -0.011, New P: -1.104
iter 25 loss: 0.433
Actual params: [-1.5402, -1.104 ]
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.549
-Original Grad: -0.000, -lr * Pred Grad:  -0.010, New P: -1.114
iter 26 loss: 0.433
Actual params: [-1.5493, -1.1136]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.558
-Original Grad: -0.000, -lr * Pred Grad:  -0.009, New P: -1.122
iter 27 loss: 0.433
Actual params: [-1.5576, -1.1224]
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.565
-Original Grad: -0.000, -lr * Pred Grad:  -0.008, New P: -1.130
iter 28 loss: 0.433
Actual params: [-1.5652, -1.1304]
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.572
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.138
iter 29 loss: 0.433
Actual params: [-1.5721, -1.1377]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.578
-Original Grad: -0.000, -lr * Pred Grad:  -0.007, New P: -1.144
iter 30 loss: 0.433
Actual params: [-1.5784, -1.1443]
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.584
-Original Grad: -0.000, -lr * Pred Grad:  -0.006, New P: -1.150
Target params: [1.3344, 1.5708]
iter 0 loss: 0.746
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.029, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.015, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.741
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.050, -lr * Pred Grad:  0.098, New P: -0.274
-Original Grad: 0.021, -lr * Pred Grad:  0.099, New P: 0.203
iter 2 loss: 0.733
Actual params: [-0.2742,  0.2029]
-Original Grad: 0.050, -lr * Pred Grad:  0.099, New P: -0.175
-Original Grad: 0.016, -lr * Pred Grad:  0.099, New P: 0.302
iter 3 loss: 0.723
Actual params: [-0.1751,  0.3018]
-Original Grad: 0.068, -lr * Pred Grad:  0.099, New P: -0.076
-Original Grad: 0.019, -lr * Pred Grad:  0.100, New P: 0.401
iter 4 loss: 0.711
Actual params: [-0.0758,  0.4014]
-Original Grad: 0.107, -lr * Pred Grad:  0.097, New P: 0.022
-Original Grad: 0.019, -lr * Pred Grad:  0.100, New P: 0.501
iter 5 loss: 0.696
Actual params: [0.0217, 0.5013]
-Original Grad: 0.173, -lr * Pred Grad:  0.094, New P: 0.116
-Original Grad: 0.023, -lr * Pred Grad:  0.101, New P: 0.602
iter 6 loss: 0.675
Actual params: [0.116, 0.602]
-Original Grad: 0.219, -lr * Pred Grad:  0.094, New P: 0.210
-Original Grad: 0.010, -lr * Pred Grad:  0.096, New P: 0.698
iter 7 loss: 0.645
Actual params: [0.2104, 0.6985]
-Original Grad: 0.322, -lr * Pred Grad:  0.094, New P: 0.304
-Original Grad: 0.014, -lr * Pred Grad:  0.096, New P: 0.794
iter 8 loss: 0.607
Actual params: [0.304 , 0.7942]
-Original Grad: 0.421, -lr * Pred Grad:  0.094, New P: 0.398
-Original Grad: 0.036, -lr * Pred Grad:  0.098, New P: 0.892
iter 9 loss: 0.549
Actual params: [0.3982, 0.8917]
-Original Grad: 0.601, -lr * Pred Grad:  0.094, New P: 0.492
-Original Grad: 0.054, -lr * Pred Grad:  0.097, New P: 0.989
iter 10 loss: 0.474
Actual params: [0.4921, 0.9891]
-Original Grad: 0.688, -lr * Pred Grad:  0.096, New P: 0.588
-Original Grad: 0.123, -lr * Pred Grad:  0.088, New P: 1.078
iter 11 loss: 0.390
Actual params: [0.588 , 1.0775]
-Original Grad: 0.759, -lr * Pred Grad:  0.098, New P: 0.686
-Original Grad: 0.189, -lr * Pred Grad:  0.087, New P: 1.164
iter 12 loss: 0.314
Actual params: [0.6863, 1.1644]
-Original Grad: 0.518, -lr * Pred Grad:  0.100, New P: 0.786
-Original Grad: 0.137, -lr * Pred Grad:  0.092, New P: 1.256
iter 13 loss: 0.251
Actual params: [0.7864, 1.2564]
-Original Grad: 0.559, -lr * Pred Grad:  0.102, New P: 0.888
-Original Grad: 0.123, -lr * Pred Grad:  0.096, New P: 1.352
iter 14 loss: 0.196
Actual params: [0.8881, 1.3519]
-Original Grad: 0.673, -lr * Pred Grad:  0.104, New P: 0.992
-Original Grad: 0.068, -lr * Pred Grad:  0.095, New P: 1.447
iter 15 loss: 0.148
Actual params: [0.992, 1.447]
-Original Grad: 0.163, -lr * Pred Grad:  0.098, New P: 1.090
-Original Grad: 0.082, -lr * Pred Grad:  0.096, New P: 1.543
iter 16 loss: 0.114
Actual params: [1.0904, 1.5428]
-Original Grad: 0.177, -lr * Pred Grad:  0.094, New P: 1.184
-Original Grad: 0.001, -lr * Pred Grad:  0.087, New P: 1.630
iter 17 loss: 0.094
Actual params: [1.1844, 1.6299]
-Original Grad: 0.030, -lr * Pred Grad:  0.086, New P: 1.271
-Original Grad: 0.027, -lr * Pred Grad:  0.083, New P: 1.713
iter 18 loss: 0.082
Actual params: [1.2705, 1.7129]
-Original Grad: 0.025, -lr * Pred Grad:  0.079, New P: 1.350
-Original Grad: 0.038, -lr * Pred Grad:  0.081, New P: 1.794
iter 19 loss: 0.074
Actual params: [1.3496, 1.7937]
-Original Grad: 0.032, -lr * Pred Grad:  0.073, New P: 1.422
-Original Grad: 0.036, -lr * Pred Grad:  0.079, New P: 1.872
iter 20 loss: 0.069
Actual params: [1.4225, 1.8724]
-Original Grad: -0.059, -lr * Pred Grad:  0.064, New P: 1.487
-Original Grad: 0.034, -lr * Pred Grad:  0.077, New P: 1.949
iter 21 loss: 0.065
Actual params: [1.4869, 1.9489]
-Original Grad: -0.146, -lr * Pred Grad:  0.054, New P: 1.541
-Original Grad: 0.086, -lr * Pred Grad:  0.081, New P: 2.029
iter 22 loss: 0.065
Actual params: [1.5409, 2.0295]
-Original Grad: -0.063, -lr * Pred Grad:  0.047, New P: 1.588
-Original Grad: 0.025, -lr * Pred Grad:  0.077, New P: 2.106
iter 23 loss: 0.068
Actual params: [1.5881, 2.1065]
-Original Grad: -0.297, -lr * Pred Grad:  0.033, New P: 1.621
-Original Grad: 0.067, -lr * Pred Grad:  0.079, New P: 2.186
iter 24 loss: 0.073
Actual params: [1.6214, 2.1855]
-Original Grad: -0.109, -lr * Pred Grad:  0.027, New P: 1.648
-Original Grad: 0.039, -lr * Pred Grad:  0.078, New P: 2.263
iter 25 loss: 0.080
Actual params: [1.6482, 2.2632]
-Original Grad: -0.449, -lr * Pred Grad:  0.010, New P: 1.658
-Original Grad: 0.000, -lr * Pred Grad:  0.071, New P: 2.334
iter 26 loss: 0.088
Actual params: [1.6582, 2.3339]
-Original Grad: -0.507, -lr * Pred Grad:  -0.006, New P: 1.652
-Original Grad: -0.022, -lr * Pred Grad:  0.061, New P: 2.395
iter 27 loss: 0.091
Actual params: [1.6519, 2.3947]
-Original Grad: -0.531, -lr * Pred Grad:  -0.021, New P: 1.631
-Original Grad: -0.169, -lr * Pred Grad:  0.025, New P: 2.420
iter 28 loss: 0.087
Actual params: [1.6311, 2.4202]
-Original Grad: -0.275, -lr * Pred Grad:  -0.027, New P: 1.604
-Original Grad: -0.159, -lr * Pred Grad:  0.000, New P: 2.420
iter 29 loss: 0.080
Actual params: [1.6044, 2.4203]
-Original Grad: 0.047, -lr * Pred Grad:  -0.023, New P: 1.581
-Original Grad: -0.079, -lr * Pred Grad:  -0.011, New P: 2.410
iter 30 loss: 0.073
Actual params: [1.5815, 2.4097]
-Original Grad: -0.137, -lr * Pred Grad:  -0.025, New P: 1.557
-Original Grad: -0.085, -lr * Pred Grad:  -0.021, New P: 2.389
Target params: [1.3344, 1.5708]
iter 0 loss: 0.103
Actual params: [-0.4723,  0.0035]
-Original Grad: -0.025, -lr * Pred Grad:  -0.100, New P: -0.572
-Original Grad: 0.016, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.099
Actual params: [-0.5723,  0.1035]
-Original Grad: -0.013, -lr * Pred Grad:  -0.093, New P: -0.665
-Original Grad: 0.007, -lr * Pred Grad:  0.092, New P: 0.195
iter 2 loss: 0.098
Actual params: [-0.6655,  0.1951]
-Original Grad: -0.008, -lr * Pred Grad:  -0.086, New P: -0.752
-Original Grad: 0.003, -lr * Pred Grad:  0.081, New P: 0.276
iter 3 loss: 0.098
Actual params: [-0.7517,  0.2764]
-Original Grad: -0.003, -lr * Pred Grad:  -0.076, New P: -0.828
-Original Grad: 0.002, -lr * Pred Grad:  0.073, New P: 0.349
iter 4 loss: 0.097
Actual params: [-0.8282,  0.349 ]
-Original Grad: -0.002, -lr * Pred Grad:  -0.068, New P: -0.896
-Original Grad: 0.001, -lr * Pred Grad:  0.064, New P: 0.413
iter 5 loss: 0.097
Actual params: [-0.8964,  0.4134]
-Original Grad: -0.001, -lr * Pred Grad:  -0.060, New P: -0.957
-Original Grad: 0.001, -lr * Pred Grad:  0.057, New P: 0.471
iter 6 loss: 0.097
Actual params: [-0.9568,  0.4707]
-Original Grad: -0.000, -lr * Pred Grad:  -0.054, New P: -1.010
-Original Grad: 0.000, -lr * Pred Grad:  0.051, New P: 0.522
iter 7 loss: 0.097
Actual params: [-1.0104,  0.5217]
-Original Grad: -0.000, -lr * Pred Grad:  -0.048, New P: -1.058
-Original Grad: 0.000, -lr * Pred Grad:  0.046, New P: 0.567
iter 8 loss: 0.097
Actual params: [-1.0581,  0.5674]
-Original Grad: -0.000, -lr * Pred Grad:  -0.043, New P: -1.101
-Original Grad: 0.000, -lr * Pred Grad:  0.041, New P: 0.609
iter 9 loss: 0.097
Actual params: [-1.1008,  0.6086]
-Original Grad: -0.000, -lr * Pred Grad:  -0.038, New P: -1.139
-Original Grad: 0.000, -lr * Pred Grad:  0.037, New P: 0.646
iter 10 loss: 0.097
Actual params: [-1.139 ,  0.6457]
-Original Grad: -0.000, -lr * Pred Grad:  -0.034, New P: -1.173
-Original Grad: 0.000, -lr * Pred Grad:  0.034, New P: 0.679
iter 11 loss: 0.097
Actual params: [-1.1734,  0.6793]
-Original Grad: -0.000, -lr * Pred Grad:  -0.031, New P: -1.204
-Original Grad: 0.000, -lr * Pred Grad:  0.030, New P: 0.710
iter 12 loss: 0.097
Actual params: [-1.2042,  0.7096]
-Original Grad: 0.000, -lr * Pred Grad:  -0.028, New P: -1.232
-Original Grad: 0.000, -lr * Pred Grad:  0.028, New P: 0.737
iter 13 loss: 0.097
Actual params: [-1.2321,  0.7372]
-Original Grad: 0.000, -lr * Pred Grad:  -0.025, New P: -1.257
-Original Grad: 0.000, -lr * Pred Grad:  0.025, New P: 0.762
iter 14 loss: 0.097
Actual params: [-1.2572,  0.7621]
-Original Grad: 0.000, -lr * Pred Grad:  -0.023, New P: -1.280
-Original Grad: 0.000, -lr * Pred Grad:  0.023, New P: 0.785
iter 15 loss: 0.097
Actual params: [-1.2799,  0.7848]
-Original Grad: 0.000, -lr * Pred Grad:  -0.021, New P: -1.300
-Original Grad: 0.000, -lr * Pred Grad:  0.021, New P: 0.805
iter 16 loss: 0.097
Actual params: [-1.3004,  0.8055]
-Original Grad: 0.000, -lr * Pred Grad:  -0.019, New P: -1.319
-Original Grad: 0.000, -lr * Pred Grad:  0.019, New P: 0.824
iter 17 loss: 0.097
Actual params: [-1.319 ,  0.8243]
-Original Grad: 0.000, -lr * Pred Grad:  -0.017, New P: -1.336
-Original Grad: 0.000, -lr * Pred Grad:  0.017, New P: 0.841
iter 18 loss: 0.097
Actual params: [-1.3358,  0.8414]
-Original Grad: 0.000, -lr * Pred Grad:  -0.015, New P: -1.351
-Original Grad: 0.000, -lr * Pred Grad:  0.016, New P: 0.857
iter 19 loss: 0.097
Actual params: [-1.3511,  0.857 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.014, New P: -1.365
-Original Grad: 0.000, -lr * Pred Grad:  0.014, New P: 0.871
iter 20 loss: 0.097
Actual params: [-1.3649,  0.8713]
-Original Grad: 0.000, -lr * Pred Grad:  -0.013, New P: -1.377
-Original Grad: 0.000, -lr * Pred Grad:  0.013, New P: 0.884
iter 21 loss: 0.097
Actual params: [-1.3775,  0.8842]
-Original Grad: 0.000, -lr * Pred Grad:  -0.011, New P: -1.389
-Original Grad: 0.000, -lr * Pred Grad:  0.012, New P: 0.896
iter 22 loss: 0.097
Actual params: [-1.3889,  0.8961]
-Original Grad: 0.000, -lr * Pred Grad:  -0.010, New P: -1.399
-Original Grad: 0.000, -lr * Pred Grad:  0.011, New P: 0.907
iter 23 loss: 0.097
Actual params: [-1.3992,  0.9069]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.409
-Original Grad: 0.000, -lr * Pred Grad:  0.010, New P: 0.917
iter 24 loss: 0.097
Actual params: [-1.4086,  0.9167]
-Original Grad: 0.000, -lr * Pred Grad:  -0.009, New P: -1.417
-Original Grad: 0.000, -lr * Pred Grad:  0.009, New P: 0.926
iter 25 loss: 0.097
Actual params: [-1.4171,  0.9258]
-Original Grad: 0.000, -lr * Pred Grad:  -0.008, New P: -1.425
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.934
iter 26 loss: 0.097
Actual params: [-1.4248,  0.934 ]
-Original Grad: 0.000, -lr * Pred Grad:  -0.007, New P: -1.432
-Original Grad: 0.000, -lr * Pred Grad:  0.008, New P: 0.941
iter 27 loss: 0.097
Actual params: [-1.4318,  0.9415]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.438
-Original Grad: 0.000, -lr * Pred Grad:  0.007, New P: 0.948
iter 28 loss: 0.097
Actual params: [-1.4381,  0.9483]
-Original Grad: 0.000, -lr * Pred Grad:  -0.006, New P: -1.444
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.955
iter 29 loss: 0.097
Actual params: [-1.4439,  0.9546]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.449
-Original Grad: 0.000, -lr * Pred Grad:  0.006, New P: 0.960
iter 30 loss: 0.097
Actual params: [-1.4491,  0.9603]
-Original Grad: 0.000, -lr * Pred Grad:  -0.005, New P: -1.454
-Original Grad: 0.000, -lr * Pred Grad:  0.005, New P: 0.966
Target params: [1.3344, 1.5708]
iter 0 loss: 0.185
Actual params: [-0.4723,  0.0035]
-Original Grad: 0.050, -lr * Pred Grad:  0.100, New P: -0.372
-Original Grad: 0.019, -lr * Pred Grad:  0.100, New P: 0.104
iter 1 loss: 0.179
Actual params: [-0.3723,  0.1035]
-Original Grad: 0.047, -lr * Pred Grad:  0.100, New P: -0.273
-Original Grad: 0.017, -lr * Pred Grad:  0.100, New P: 0.203
iter 2 loss: 0.167
Actual params: [-0.2725,  0.2031]
-Original Grad: 0.133, -lr * Pred Grad:  0.092, New P: -0.180
-Original Grad: 0.042, -lr * Pred Grad:  0.095, New P: 0.298
iter 3 loss: 0.146
Actual params: [-0.1804,  0.2976]
-Original Grad: 0.278, -lr * Pred Grad:  0.087, New P: -0.093
-Original Grad: 0.089, -lr * Pred Grad:  0.088, New P: 0.386
iter 4 loss: 0.120
Actual params: [-0.0934,  0.3857]
-Original Grad: 0.236, -lr * Pred Grad:  0.091, New P: -0.002
-Original Grad: 0.042, -lr * Pred Grad:  0.090, New P: 0.475
iter 5 loss: 0.094
Actual params: [-0.0019,  0.4755]
-Original Grad: 0.233, -lr * Pred Grad:  0.094, New P: 0.093
-Original Grad: 0.024, -lr * Pred Grad:  0.087, New P: 0.562
iter 6 loss: 0.068
Actual params: [0.0925, 0.5622]
-Original Grad: 0.204, -lr * Pred Grad:  0.096, New P: 0.188
-Original Grad: 0.014, -lr * Pred Grad:  0.082, New P: 0.644
iter 7 loss: 0.049
Actual params: [0.1885, 0.6438]
-Original Grad: 0.113, -lr * Pred Grad:  0.093, New P: 0.282
-Original Grad: -0.008, -lr * Pred Grad:  0.068, New P: 0.712
iter 8 loss: 0.037
Actual params: [0.2819, 0.7118]
-Original Grad: 0.135, -lr * Pred Grad:  0.093, New P: 0.375
-Original Grad: -0.002, -lr * Pred Grad:  0.060, New P: 0.771
iter 9 loss: 0.029
Actual params: [0.3746, 0.7714]
-Original Grad: 0.027, -lr * Pred Grad:  0.085, New P: 0.460
-Original Grad: 0.026, -lr * Pred Grad:  0.063, New P: 0.834
iter 10 loss: 0.025
Actual params: [0.4596, 0.8341]
-Original Grad: 0.020, -lr * Pred Grad:  0.078, New P: 0.538
-Original Grad: 0.026, -lr * Pred Grad:  0.065, New P: 0.899
iter 11 loss: 0.023
Actual params: [0.5376, 0.8993]
-Original Grad: -0.014, -lr * Pred Grad:  0.069, New P: 0.606
-Original Grad: 0.042, -lr * Pred Grad:  0.071, New P: 0.971
iter 12 loss: 0.021
Actual params: [0.6064, 0.9707]
-Original Grad: -0.029, -lr * Pred Grad:  0.059, New P: 0.666
-Original Grad: 0.047, -lr * Pred Grad:  0.077, New P: 1.048
iter 13 loss: 0.019
Actual params: [0.6657, 1.0479]
-Original Grad: -0.048, -lr * Pred Grad:  0.049, New P: 0.715
-Original Grad: 0.066, -lr * Pred Grad:  0.084, New P: 1.132
iter 14 loss: 0.015
Actual params: [0.7148, 1.1317]
-Original Grad: -0.042, -lr * Pred Grad:  0.040, New P: 0.755
-Original Grad: 0.081, -lr * Pred Grad:  0.090, New P: 1.222
iter 15 loss: 0.011
Actual params: [0.7552, 1.2216]
-Original Grad: -0.006, -lr * Pred Grad:  0.036, New P: 0.791
-Original Grad: 0.041, -lr * Pred Grad:  0.091, New P: 1.312
iter 16 loss: 0.008
Actual params: [0.7913, 1.3123]
-Original Grad: 0.023, -lr * Pred Grad:  0.035, New P: 0.826
-Original Grad: 0.033, -lr * Pred Grad:  0.090, New P: 1.402
iter 17 loss: 0.005
Actual params: [0.8261, 1.4024]
-Original Grad: 0.024, -lr * Pred Grad:  0.034, New P: 0.860
-Original Grad: 0.024, -lr * Pred Grad:  0.088, New P: 1.490
iter 18 loss: 0.004
Actual params: [0.86  , 1.4901]
-Original Grad: 0.015, -lr * Pred Grad:  0.032, New P: 0.892
-Original Grad: 0.001, -lr * Pred Grad:  0.080, New P: 1.570
iter 19 loss: 0.004
Actual params: [0.8923, 1.5702]
-Original Grad: 0.034, -lr * Pred Grad:  0.032, New P: 0.925
-Original Grad: -0.024, -lr * Pred Grad:  0.065, New P: 1.636
iter 20 loss: 0.005
Actual params: [0.9247, 1.6356]
-Original Grad: 0.007, -lr * Pred Grad:  0.030, New P: 0.955
-Original Grad: -0.029, -lr * Pred Grad:  0.051, New P: 1.686
iter 21 loss: 0.007
Actual params: [0.9549, 1.6863]
-Original Grad: -0.006, -lr * Pred Grad:  0.027, New P: 0.982
-Original Grad: -0.054, -lr * Pred Grad:  0.030, New P: 1.716
iter 22 loss: 0.009
Actual params: [0.9818, 1.7159]
-Original Grad: -0.041, -lr * Pred Grad:  0.020, New P: 1.002
-Original Grad: -0.049, -lr * Pred Grad:  0.013, New P: 1.729
iter 23 loss: 0.011
Actual params: [1.0021, 1.7289]
-Original Grad: -0.021, -lr * Pred Grad:  0.016, New P: 1.019
-Original Grad: -0.036, -lr * Pred Grad:  0.002, New P: 1.731
iter 24 loss: 0.011
Actual params: [1.0186, 1.7311]
-Original Grad: -0.045, -lr * Pred Grad:  0.011, New P: 1.029
-Original Grad: -0.067, -lr * Pred Grad:  -0.015, New P: 1.716
iter 25 loss: 0.011
Actual params: [1.0291, 1.7161]
-Original Grad: -0.023, -lr * Pred Grad:  0.007, New P: 1.036
-Original Grad: -0.037, -lr * Pred Grad:  -0.023, New P: 1.693
iter 26 loss: 0.010
Actual params: [1.0364, 1.6932]
-Original Grad: -0.032, -lr * Pred Grad:  0.003, New P: 1.040
-Original Grad: -0.040, -lr * Pred Grad:  -0.031, New P: 1.663
iter 27 loss: 0.009
Actual params: [1.0397, 1.6626]
-Original Grad: -0.036, -lr * Pred Grad:  -0.001, New P: 1.039
-Original Grad: -0.051, -lr * Pred Grad:  -0.040, New P: 1.623
iter 28 loss: 0.007
Actual params: [1.039 , 1.6226]
-Original Grad: -0.034, -lr * Pred Grad:  -0.004, New P: 1.035
-Original Grad: -0.051, -lr * Pred Grad:  -0.048, New P: 1.575
iter 29 loss: 0.005
Actual params: [1.0349, 1.5747]
-Original Grad: -0.022, -lr * Pred Grad:  -0.006, New P: 1.029
-Original Grad: -0.022, -lr * Pred Grad:  -0.049, New P: 1.526
iter 30 loss: 0.003
Actual params: [1.0288, 1.5257]
-Original Grad: -0.022, -lr * Pred Grad:  -0.008, New P: 1.021
-Original Grad: -0.026, -lr * Pred Grad:  -0.051, New P: 1.475
