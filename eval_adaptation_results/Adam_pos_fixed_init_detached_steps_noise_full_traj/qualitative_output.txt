Target params: [1.1812, 0.2779]
iter 0 loss: 0.779
Actual params: [0.5941, 0.5941]
-Original Grad: 0.119, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.099, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.731
Actual params: [0.6941, 0.4941]
-Original Grad: 0.368, -lr * Pred Grad:  0.091, New P: 0.785
-Original Grad: -0.041, -lr * Pred Grad:  -0.090, New P: 0.404
iter 2 loss: 0.648
Actual params: [0.7855, 0.4036]
-Original Grad: 0.115, -lr * Pred Grad:  0.086, New P: 0.871
-Original Grad: -0.266, -lr * Pred Grad:  -0.085, New P: 0.318
iter 3 loss: 0.520
Actual params: [0.8714, 0.3183]
-Original Grad: -0.230, -lr * Pred Grad:  0.032, New P: 0.904
-Original Grad: -0.025, -lr * Pred Grad:  -0.075, New P: 0.244
iter 4 loss: 0.433
Actual params: [0.9038, 0.2436]
-Original Grad: -0.031, -lr * Pred Grad:  0.024, New P: 0.928
-Original Grad: -0.168, -lr * Pred Grad:  -0.082, New P: 0.162
iter 5 loss: 0.373
Actual params: [0.9275, 0.1615]
-Original Grad: -0.129, -lr * Pred Grad:  0.006, New P: 0.933
-Original Grad: -0.175, -lr * Pred Grad:  -0.087, New P: 0.075
iter 6 loss: 0.358
Actual params: [0.9332, 0.0746]
-Original Grad: 0.031, -lr * Pred Grad:  0.008, New P: 0.941
-Original Grad: -0.096, -lr * Pred Grad:  -0.086, New P: -0.011
iter 7 loss: 0.360
Actual params: [ 0.9415, -0.0115]
-Original Grad: 0.026, -lr * Pred Grad:  0.010, New P: 0.951
-Original Grad: 0.068, -lr * Pred Grad:  -0.066, New P: -0.078
iter 8 loss: 0.368
Actual params: [ 0.9514, -0.0777]
-Original Grad: -0.018, -lr * Pred Grad:  0.007, New P: 0.958
-Original Grad: 0.214, -lr * Pred Grad:  -0.028, New P: -0.106
iter 9 loss: 0.372
Actual params: [ 0.9584, -0.106 ]
-Original Grad: 0.211, -lr * Pred Grad:  0.025, New P: 0.983
-Original Grad: -0.321, -lr * Pred Grad:  -0.049, New P: -0.155
iter 10 loss: 1.177
Actual params: [ 0.9835, -0.1547]
-Original Grad: -0.441, -lr * Pred Grad:  -0.014, New P: 0.970
-Original Grad: 0.076, -lr * Pred Grad:  -0.037, New P: -0.191
iter 11 loss: 1.596
Actual params: [ 0.9697, -0.1915]
-Original Grad: -0.305, -lr * Pred Grad:  -0.031, New P: 0.939
-Original Grad: 0.236, -lr * Pred Grad:  -0.012, New P: -0.203
iter 12 loss: 1.632
Actual params: [ 0.9389, -0.203 ]
-Original Grad: -0.349, -lr * Pred Grad:  -0.046, New P: 0.893
-Original Grad: 0.181, -lr * Pred Grad:  0.004, New P: -0.199
iter 13 loss: 1.601
Actual params: [ 0.8934, -0.1992]
-Original Grad: 0.261, -lr * Pred Grad:  -0.025, New P: 0.869
-Original Grad: -0.078, -lr * Pred Grad:  -0.002, New P: -0.202
iter 14 loss: 0.481
Actual params: [ 0.8688, -0.2017]
-Original Grad: 0.286, -lr * Pred Grad:  -0.006, New P: 0.863
-Original Grad: 0.256, -lr * Pred Grad:  0.016, New P: -0.186
iter 15 loss: 0.467
Actual params: [ 0.8628, -0.1856]
-Original Grad: -0.179, -lr * Pred Grad:  -0.015, New P: 0.848
-Original Grad: 0.011, -lr * Pred Grad:  0.015, New P: -0.170
iter 16 loss: 0.469
Actual params: [ 0.8481, -0.1702]
-Original Grad: 0.240, -lr * Pred Grad:  -0.001, New P: 0.847
-Original Grad: 0.343, -lr * Pred Grad:  0.035, New P: -0.136
iter 17 loss: 0.459
Actual params: [ 0.8475, -0.1356]
-Original Grad: -0.017, -lr * Pred Grad:  -0.001, New P: 0.846
-Original Grad: 0.119, -lr * Pred Grad:  0.039, New P: -0.097
iter 18 loss: 0.455
Actual params: [ 0.846, -0.097]
-Original Grad: 0.201, -lr * Pred Grad:  0.009, New P: 0.855
-Original Grad: 0.094, -lr * Pred Grad:  0.041, New P: -0.056
iter 19 loss: 0.443
Actual params: [ 0.855 , -0.0561]
-Original Grad: -0.061, -lr * Pred Grad:  0.005, New P: 0.860
-Original Grad: 0.073, -lr * Pred Grad:  0.042, New P: -0.014
iter 20 loss: 0.436
Actual params: [ 0.86  , -0.0142]
-Original Grad: 0.161, -lr * Pred Grad:  0.013, New P: 0.873
-Original Grad: -0.029, -lr * Pred Grad:  0.036, New P: 0.022
Target params: [1.1812, 0.2779]
iter 0 loss: 0.340
Actual params: [0.5941, 0.5941]
-Original Grad: -0.066, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: 0.077, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.370
Actual params: [0.4941, 0.6941]
-Original Grad: -0.092, -lr * Pred Grad:  -0.099, New P: 0.395
-Original Grad: 0.106, -lr * Pred Grad:  0.100, New P: 0.794
iter 2 loss: 0.421
Actual params: [0.3946, 0.7936]
-Original Grad: 0.048, -lr * Pred Grad:  -0.046, New P: 0.349
-Original Grad: -0.028, -lr * Pred Grad:  0.062, New P: 0.856
iter 3 loss: 0.447
Actual params: [0.3488, 0.8557]
-Original Grad: -0.116, -lr * Pred Grad:  -0.067, New P: 0.282
-Original Grad: 0.227, -lr * Pred Grad:  0.076, New P: 0.932
iter 4 loss: 0.499
Actual params: [0.2817, 0.9315]
-Original Grad: -0.021, -lr * Pred Grad:  -0.063, New P: 0.219
-Original Grad: 0.073, -lr * Pred Grad:  0.076, New P: 1.008
iter 5 loss: 0.539
Actual params: [0.2185, 1.0079]
-Original Grad: 0.041, -lr * Pred Grad:  -0.041, New P: 0.178
-Original Grad: -0.066, -lr * Pred Grad:  0.052, New P: 1.060
iter 6 loss: 0.548
Actual params: [0.1777, 1.0595]
-Original Grad: 0.033, -lr * Pred Grad:  -0.026, New P: 0.152
-Original Grad: 0.045, -lr * Pred Grad:  0.053, New P: 1.112
iter 7 loss: 0.555
Actual params: [0.1522, 1.1122]
-Original Grad: 0.018, -lr * Pred Grad:  -0.017, New P: 0.135
-Original Grad: 0.011, -lr * Pred Grad:  0.048, New P: 1.160
iter 8 loss: 0.559
Actual params: [0.1348, 1.1605]
-Original Grad: 0.089, -lr * Pred Grad:  0.008, New P: 0.143
-Original Grad: 0.024, -lr * Pred Grad:  0.047, New P: 1.207
iter 9 loss: 0.556
Actual params: [0.1428, 1.2073]
-Original Grad: -0.062, -lr * Pred Grad:  -0.007, New P: 0.135
-Original Grad: -0.051, -lr * Pred Grad:  0.033, New P: 1.240
iter 10 loss: 0.557
Actual params: [0.1354, 1.24  ]
-Original Grad: 0.111, -lr * Pred Grad:  0.017, New P: 0.152
-Original Grad: -0.020, -lr * Pred Grad:  0.026, New P: 1.266
iter 11 loss: 0.553
Actual params: [0.152 , 1.2659]
-Original Grad: -0.048, -lr * Pred Grad:  0.005, New P: 0.157
-Original Grad: 0.013, -lr * Pred Grad:  0.025, New P: 1.291
iter 12 loss: 0.551
Actual params: [0.1572, 1.2912]
-Original Grad: -0.133, -lr * Pred Grad:  -0.019, New P: 0.138
-Original Grad: -0.002, -lr * Pred Grad:  0.022, New P: 1.314
iter 13 loss: 0.554
Actual params: [0.1379, 1.3137]
-Original Grad: 0.074, -lr * Pred Grad:  -0.004, New P: 0.134
-Original Grad: 0.041, -lr * Pred Grad:  0.027, New P: 1.341
iter 14 loss: 0.554
Actual params: [0.1337, 1.3405]
-Original Grad: 0.039, -lr * Pred Grad:  0.003, New P: 0.136
-Original Grad: 0.006, -lr * Pred Grad:  0.025, New P: 1.366
iter 15 loss: 0.553
Actual params: [0.1365, 1.3659]
-Original Grad: 0.002, -lr * Pred Grad:  0.003, New P: 0.139
-Original Grad: -0.003, -lr * Pred Grad:  0.022, New P: 1.388
iter 16 loss: 0.551
Actual params: [0.1394, 1.3883]
-Original Grad: -0.021, -lr * Pred Grad:  -0.001, New P: 0.138
-Original Grad: 0.036, -lr * Pred Grad:  0.026, New P: 1.415
iter 17 loss: 0.550
Actual params: [0.1384, 1.4146]
-Original Grad: -0.070, -lr * Pred Grad:  -0.013, New P: 0.126
-Original Grad: 0.050, -lr * Pred Grad:  0.032, New P: 1.446
iter 18 loss: 0.552
Actual params: [0.1258, 1.4465]
-Original Grad: -0.049, -lr * Pred Grad:  -0.020, New P: 0.106
-Original Grad: 0.012, -lr * Pred Grad:  0.031, New P: 1.477
iter 19 loss: 0.555
Actual params: [0.1063, 1.4774]
-Original Grad: -0.009, -lr * Pred Grad:  -0.019, New P: 0.087
-Original Grad: 0.033, -lr * Pred Grad:  0.034, New P: 1.511
iter 20 loss: 0.557
Actual params: [0.0869, 1.511 ]
-Original Grad: -0.039, -lr * Pred Grad:  -0.024, New P: 0.063
-Original Grad: 0.032, -lr * Pred Grad:  0.036, New P: 1.547
Target params: [1.1812, 0.2779]
iter 0 loss: 0.415
Actual params: [0.5941, 0.5941]
-Original Grad: 0.130, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.378, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.304
Actual params: [0.6941, 0.4941]
-Original Grad: 0.059, -lr * Pred Grad:  0.092, New P: 0.786
-Original Grad: -0.646, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.298
Actual params: [0.7858, 0.396 ]
-Original Grad: -0.012, -lr * Pred Grad:  0.065, New P: 0.851
-Original Grad: -0.148, -lr * Pred Grad:  -0.087, New P: 0.309
iter 3 loss: 0.313
Actual params: [0.8512, 0.3093]
-Original Grad: -0.028, -lr * Pred Grad:  0.041, New P: 0.892
-Original Grad: -0.019, -lr * Pred Grad:  -0.072, New P: 0.237
iter 4 loss: 0.323
Actual params: [0.8925, 0.2368]
-Original Grad: -0.024, -lr * Pred Grad:  0.025, New P: 0.918
-Original Grad: -0.099, -lr * Pred Grad:  -0.068, New P: 0.169
iter 5 loss: 0.328
Actual params: [0.9179, 0.169 ]
-Original Grad: -0.033, -lr * Pred Grad:  0.010, New P: 0.928
-Original Grad: -0.065, -lr * Pred Grad:  -0.063, New P: 0.106
iter 6 loss: 0.329
Actual params: [0.9279, 0.1063]
-Original Grad: -0.060, -lr * Pred Grad:  -0.010, New P: 0.917
-Original Grad: -0.074, -lr * Pred Grad:  -0.059, New P: 0.047
iter 7 loss: 0.330
Actual params: [0.9174, 0.047 ]
-Original Grad: -0.042, -lr * Pred Grad:  -0.021, New P: 0.896
-Original Grad: -0.118, -lr * Pred Grad:  -0.059, New P: -0.012
iter 8 loss: 0.333
Actual params: [ 0.8961, -0.0122]
-Original Grad: -0.015, -lr * Pred Grad:  -0.023, New P: 0.873
-Original Grad: -0.003, -lr * Pred Grad:  -0.053, New P: -0.065
iter 9 loss: 0.337
Actual params: [ 0.8728, -0.0649]
-Original Grad: -0.063, -lr * Pred Grad:  -0.036, New P: 0.836
-Original Grad: -0.054, -lr * Pred Grad:  -0.050, New P: -0.115
iter 10 loss: 0.341
Actual params: [ 0.8364, -0.1152]
-Original Grad: -0.021, -lr * Pred Grad:  -0.038, New P: 0.798
-Original Grad: -0.085, -lr * Pred Grad:  -0.050, New P: -0.165
iter 11 loss: 0.346
Actual params: [ 0.7984, -0.1652]
-Original Grad: -0.015, -lr * Pred Grad:  -0.038, New P: 0.760
-Original Grad: -0.038, -lr * Pred Grad:  -0.047, New P: -0.212
iter 12 loss: 0.352
Actual params: [ 0.7604, -0.2125]
-Original Grad: -0.024, -lr * Pred Grad:  -0.040, New P: 0.720
-Original Grad: 0.007, -lr * Pred Grad:  -0.042, New P: -0.255
iter 13 loss: 0.359
Actual params: [ 0.7202, -0.2547]
-Original Grad: 0.038, -lr * Pred Grad:  -0.026, New P: 0.695
-Original Grad: -0.028, -lr * Pred Grad:  -0.040, New P: -0.294
iter 14 loss: 0.366
Actual params: [ 0.6946, -0.2944]
-Original Grad: 0.035, -lr * Pred Grad:  -0.014, New P: 0.681
-Original Grad: 0.043, -lr * Pred Grad:  -0.033, New P: -0.328
iter 15 loss: 0.370
Actual params: [ 0.6807, -0.3277]
-Original Grad: 0.043, -lr * Pred Grad:  -0.001, New P: 0.679
-Original Grad: 0.012, -lr * Pred Grad:  -0.029, New P: -0.357
iter 16 loss: 0.373
Actual params: [ 0.6793, -0.3572]
-Original Grad: 0.037, -lr * Pred Grad:  0.008, New P: 0.687
-Original Grad: 0.040, -lr * Pred Grad:  -0.024, New P: -0.381
iter 17 loss: 0.372
Actual params: [ 0.6872, -0.3813]
-Original Grad: -0.009, -lr * Pred Grad:  0.005, New P: 0.692
-Original Grad: 0.023, -lr * Pred Grad:  -0.020, New P: -0.402
iter 18 loss: 0.373
Actual params: [ 0.6919, -0.4018]
-Original Grad: 0.043, -lr * Pred Grad:  0.015, New P: 0.707
-Original Grad: 0.170, -lr * Pred Grad:  -0.008, New P: -0.409
iter 19 loss: 0.371
Actual params: [ 0.7068, -0.4094]
-Original Grad: 0.007, -lr * Pred Grad:  0.015, New P: 0.722
-Original Grad: 0.046, -lr * Pred Grad:  -0.004, New P: -0.414
iter 20 loss: 0.369
Actual params: [ 0.7223, -0.4135]
-Original Grad: -0.000, -lr * Pred Grad:  0.014, New P: 0.736
-Original Grad: 0.035, -lr * Pred Grad:  -0.001, New P: -0.415
Target params: [1.1812, 0.2779]
iter 0 loss: 1.381
Actual params: [0.5941, 0.5941]
-Original Grad: 0.168, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.078, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 1.349
Actual params: [0.6941, 0.4941]
-Original Grad: -0.023, -lr * Pred Grad:  0.056, New P: 0.750
-Original Grad: -0.055, -lr * Pred Grad:  -0.098, New P: 0.397
iter 2 loss: 1.316
Actual params: [0.7502, 0.3965]
-Original Grad: 0.176, -lr * Pred Grad:  0.076, New P: 0.826
-Original Grad: -0.161, -lr * Pred Grad:  -0.093, New P: 0.303
iter 3 loss: 1.211
Actual params: [0.8262, 0.3031]
-Original Grad: 0.158, -lr * Pred Grad:  0.084, New P: 0.910
-Original Grad: -0.058, -lr * Pred Grad:  -0.090, New P: 0.213
iter 4 loss: 0.993
Actual params: [0.9101, 0.2128]
-Original Grad: 0.249, -lr * Pred Grad:  0.089, New P: 0.999
-Original Grad: -0.054, -lr * Pred Grad:  -0.088, New P: 0.125
iter 5 loss: 0.801
Actual params: [0.9994, 0.1247]
-Original Grad: -0.055, -lr * Pred Grad:  0.069, New P: 1.068
-Original Grad: 0.050, -lr * Pred Grad:  -0.061, New P: 0.063
iter 6 loss: 0.738
Actual params: [1.0682, 0.0632]
-Original Grad: 0.014, -lr * Pred Grad:  0.062, New P: 1.130
-Original Grad: -0.021, -lr * Pred Grad:  -0.058, New P: 0.005
iter 7 loss: 0.722
Actual params: [1.13  , 0.0048]
-Original Grad: 0.078, -lr * Pred Grad:  0.063, New P: 1.193
-Original Grad: -0.058, -lr * Pred Grad:  -0.063, New P: -0.058
iter 8 loss: 0.740
Actual params: [ 1.1932, -0.0581]
-Original Grad: 0.021, -lr * Pred Grad:  0.059, New P: 1.252
-Original Grad: 0.078, -lr * Pred Grad:  -0.036, New P: -0.094
iter 9 loss: 0.762
Actual params: [ 1.2518, -0.0941]
-Original Grad: -0.107, -lr * Pred Grad:  0.038, New P: 1.290
-Original Grad: -0.023, -lr * Pred Grad:  -0.037, New P: -0.131
iter 10 loss: 0.792
Actual params: [ 1.2896, -0.1307]
-Original Grad: -0.037, -lr * Pred Grad:  0.029, New P: 1.319
-Original Grad: 0.041, -lr * Pred Grad:  -0.024, New P: -0.155
iter 11 loss: 0.819
Actual params: [ 1.3191, -0.1548]
-Original Grad: -0.207, -lr * Pred Grad:  0.002, New P: 1.321
-Original Grad: 0.023, -lr * Pred Grad:  -0.017, New P: -0.172
iter 12 loss: 0.824
Actual params: [ 1.321 , -0.1717]
-Original Grad: -0.103, -lr * Pred Grad:  -0.009, New P: 1.312
-Original Grad: 0.003, -lr * Pred Grad:  -0.015, New P: -0.186
iter 13 loss: 0.816
Actual params: [ 1.3121, -0.1862]
-Original Grad: -0.209, -lr * Pred Grad:  -0.027, New P: 1.285
-Original Grad: 0.011, -lr * Pred Grad:  -0.011, New P: -0.197
iter 14 loss: 0.796
Actual params: [ 1.2851, -0.197 ]
-Original Grad: -0.131, -lr * Pred Grad:  -0.036, New P: 1.249
-Original Grad: -0.017, -lr * Pred Grad:  -0.013, New P: -0.210
iter 15 loss: 0.773
Actual params: [ 1.2494, -0.2102]
-Original Grad: -0.065, -lr * Pred Grad:  -0.038, New P: 1.211
-Original Grad: -0.005, -lr * Pred Grad:  -0.013, New P: -0.223
iter 16 loss: 0.753
Actual params: [ 1.2114, -0.2231]
-Original Grad: 0.006, -lr * Pred Grad:  -0.034, New P: 1.177
-Original Grad: 0.015, -lr * Pred Grad:  -0.009, New P: -0.232
iter 17 loss: 0.735
Actual params: [ 1.1774, -0.2317]
-Original Grad: 0.034, -lr * Pred Grad:  -0.028, New P: 1.150
-Original Grad: -0.041, -lr * Pred Grad:  -0.016, New P: -0.248
iter 18 loss: 0.735
Actual params: [ 1.1497, -0.2478]
-Original Grad: 0.050, -lr * Pred Grad:  -0.020, New P: 1.129
-Original Grad: -0.022, -lr * Pred Grad:  -0.019, New P: -0.267
iter 19 loss: 0.734
Actual params: [ 1.1295, -0.2671]
-Original Grad: 0.047, -lr * Pred Grad:  -0.014, New P: 1.115
-Original Grad: -0.015, -lr * Pred Grad:  -0.021, New P: -0.288
iter 20 loss: 0.736
Actual params: [ 1.1155, -0.2878]
-Original Grad: 0.043, -lr * Pred Grad:  -0.009, New P: 1.107
-Original Grad: -0.049, -lr * Pred Grad:  -0.029, New P: -0.316
Target params: [1.1812, 0.2779]
iter 0 loss: 0.916
Actual params: [0.5941, 0.5941]
-Original Grad: 0.104, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.180, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.870
Actual params: [0.6941, 0.4941]
-Original Grad: 0.119, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: 0.051, -lr * Pred Grad:  -0.044, New P: 0.450
iter 2 loss: 0.806
Actual params: [0.7942, 0.4496]
-Original Grad: 0.199, -lr * Pred Grad:  0.098, New P: 0.892
-Original Grad: -0.024, -lr * Pred Grad:  -0.042, New P: 0.407
iter 3 loss: 0.717
Actual params: [0.8924, 0.4074]
-Original Grad: 0.175, -lr * Pred Grad:  0.099, New P: 0.992
-Original Grad: -0.018, -lr * Pred Grad:  -0.040, New P: 0.367
iter 4 loss: 0.632
Actual params: [0.9916, 0.3675]
-Original Grad: 0.042, -lr * Pred Grad:  0.090, New P: 1.082
-Original Grad: 0.121, -lr * Pred Grad:  0.001, New P: 0.368
iter 5 loss: 0.583
Actual params: [1.0821, 0.3684]
-Original Grad: 0.027, -lr * Pred Grad:  0.082, New P: 1.164
-Original Grad: 0.104, -lr * Pred Grad:  0.023, New P: 0.391
iter 6 loss: 0.587
Actual params: [1.1643, 0.3911]
-Original Grad: -0.020, -lr * Pred Grad:  0.068, New P: 1.233
-Original Grad: 0.075, -lr * Pred Grad:  0.034, New P: 0.425
iter 7 loss: 0.612
Actual params: [1.2327, 0.4248]
-Original Grad: -0.033, -lr * Pred Grad:  0.055, New P: 1.287
-Original Grad: 0.103, -lr * Pred Grad:  0.046, New P: 0.471
iter 8 loss: 0.646
Actual params: [1.2874, 0.4708]
-Original Grad: -0.005, -lr * Pred Grad:  0.048, New P: 1.335
-Original Grad: 0.106, -lr * Pred Grad:  0.056, New P: 0.526
iter 9 loss: 0.679
Actual params: [1.3353, 0.5263]
-Original Grad: -0.113, -lr * Pred Grad:  0.024, New P: 1.359
-Original Grad: 0.064, -lr * Pred Grad:  0.059, New P: 0.585
iter 10 loss: 0.709
Actual params: [1.359, 0.585]
-Original Grad: -0.216, -lr * Pred Grad:  -0.008, New P: 1.351
-Original Grad: -0.281, -lr * Pred Grad:  0.006, New P: 0.591
iter 11 loss: 0.705
Actual params: [1.3506, 0.5909]
-Original Grad: -0.244, -lr * Pred Grad:  -0.032, New P: 1.319
-Original Grad: -0.367, -lr * Pred Grad:  -0.028, New P: 0.563
iter 12 loss: 0.681
Actual params: [1.319 , 0.5629]
-Original Grad: -0.126, -lr * Pred Grad:  -0.040, New P: 1.279
-Original Grad: -0.004, -lr * Pred Grad:  -0.026, New P: 0.537
iter 13 loss: 0.659
Actual params: [1.2789, 0.5374]
-Original Grad: -0.005, -lr * Pred Grad:  -0.037, New P: 1.242
-Original Grad: -0.096, -lr * Pred Grad:  -0.031, New P: 0.506
iter 14 loss: 0.636
Actual params: [1.2422, 0.5063]
-Original Grad: -0.098, -lr * Pred Grad:  -0.042, New P: 1.200
-Original Grad: 0.160, -lr * Pred Grad:  -0.014, New P: 0.493
iter 15 loss: 0.613
Actual params: [1.1998, 0.4926]
-Original Grad: -0.028, -lr * Pred Grad:  -0.041, New P: 1.159
-Original Grad: 0.095, -lr * Pred Grad:  -0.004, New P: 0.488
iter 16 loss: 0.603
Actual params: [1.1587, 0.4882]
-Original Grad: -0.027, -lr * Pred Grad:  -0.040, New P: 1.119
-Original Grad: -0.022, -lr * Pred Grad:  -0.006, New P: 0.482
iter 17 loss: 0.590
Actual params: [1.1188, 0.4824]
-Original Grad: 0.009, -lr * Pred Grad:  -0.035, New P: 1.083
-Original Grad: 0.062, -lr * Pred Grad:  -0.000, New P: 0.482
iter 18 loss: 0.592
Actual params: [1.0835, 0.4824]
-Original Grad: -0.008, -lr * Pred Grad:  -0.033, New P: 1.051
-Original Grad: 0.012, -lr * Pred Grad:  0.001, New P: 0.483
iter 19 loss: 0.611
Actual params: [1.0506, 0.4833]
-Original Grad: 0.077, -lr * Pred Grad:  -0.022, New P: 1.029
-Original Grad: 0.206, -lr * Pred Grad:  0.017, New P: 0.501
iter 20 loss: 0.627
Actual params: [1.0288, 0.5008]
-Original Grad: 0.076, -lr * Pred Grad:  -0.012, New P: 1.017
-Original Grad: 0.090, -lr * Pred Grad:  0.023, New P: 0.524
Target params: [1.1812, 0.2779]
iter 0 loss: 0.651
Actual params: [0.5941, 0.5941]
-Original Grad: -0.009, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.063, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.647
Actual params: [0.4941, 0.4941]
-Original Grad: 0.008, -lr * Pred Grad:  0.001, New P: 0.495
-Original Grad: -0.121, -lr * Pred Grad:  -0.097, New P: 0.397
iter 2 loss: 0.627
Actual params: [0.4946, 0.3971]
-Original Grad: 0.033, -lr * Pred Grad:  0.061, New P: 0.555
-Original Grad: -0.089, -lr * Pred Grad:  -0.098, New P: 0.299
iter 3 loss: 0.599
Actual params: [0.5552, 0.2994]
-Original Grad: -0.077, -lr * Pred Grad:  -0.032, New P: 0.523
-Original Grad: -0.063, -lr * Pred Grad:  -0.096, New P: 0.204
iter 4 loss: 0.603
Actual params: [0.5229, 0.2038]
-Original Grad: 0.097, -lr * Pred Grad:  0.023, New P: 0.546
-Original Grad: -0.045, -lr * Pred Grad:  -0.092, New P: 0.112
iter 5 loss: 0.586
Actual params: [0.546 , 0.1119]
-Original Grad: 0.147, -lr * Pred Grad:  0.052, New P: 0.598
-Original Grad: -0.065, -lr * Pred Grad:  -0.092, New P: 0.020
iter 6 loss: 0.558
Actual params: [0.5984, 0.0197]
-Original Grad: 0.079, -lr * Pred Grad:  0.061, New P: 0.660
-Original Grad: -0.040, -lr * Pred Grad:  -0.089, New P: -0.069
iter 7 loss: 0.528
Actual params: [ 0.6597, -0.0694]
-Original Grad: -0.003, -lr * Pred Grad:  0.053, New P: 0.713
-Original Grad: -0.099, -lr * Pred Grad:  -0.093, New P: -0.162
iter 8 loss: 0.500
Actual params: [ 0.7131, -0.162 ]
-Original Grad: 0.066, -lr * Pred Grad:  0.060, New P: 0.773
-Original Grad: -0.034, -lr * Pred Grad:  -0.089, New P: -0.251
iter 9 loss: 0.458
Actual params: [ 0.773 , -0.2506]
-Original Grad: 0.069, -lr * Pred Grad:  0.065, New P: 0.838
-Original Grad: -0.028, -lr * Pred Grad:  -0.084, New P: -0.335
iter 10 loss: 0.403
Actual params: [ 0.8385, -0.3351]
-Original Grad: 0.103, -lr * Pred Grad:  0.073, New P: 0.912
-Original Grad: 0.001, -lr * Pred Grad:  -0.075, New P: -0.410
iter 11 loss: 0.360
Actual params: [ 0.9117, -0.4105]
-Original Grad: 0.106, -lr * Pred Grad:  0.079, New P: 0.991
-Original Grad: -0.003, -lr * Pred Grad:  -0.068, New P: -0.479
iter 12 loss: 0.341
Actual params: [ 0.9911, -0.4788]
-Original Grad: 0.085, -lr * Pred Grad:  0.083, New P: 1.074
-Original Grad: 0.098, -lr * Pred Grad:  -0.037, New P: -0.516
iter 13 loss: 0.337
Actual params: [ 1.0737, -0.5158]
-Original Grad: 0.052, -lr * Pred Grad:  0.082, New P: 1.156
-Original Grad: -0.029, -lr * Pred Grad:  -0.039, New P: -0.555
iter 14 loss: 0.343
Actual params: [ 1.1559, -0.5549]
-Original Grad: -0.016, -lr * Pred Grad:  0.072, New P: 1.227
-Original Grad: -0.017, -lr * Pred Grad:  -0.039, New P: -0.594
iter 15 loss: 0.351
Actual params: [ 1.2274, -0.5935]
-Original Grad: -0.018, -lr * Pred Grad:  0.062, New P: 1.289
-Original Grad: 0.091, -lr * Pred Grad:  -0.016, New P: -0.609
iter 16 loss: 0.359
Actual params: [ 1.2891, -0.6094]
-Original Grad: -0.055, -lr * Pred Grad:  0.046, New P: 1.335
-Original Grad: -0.022, -lr * Pred Grad:  -0.018, New P: -0.628
iter 17 loss: 0.369
Actual params: [ 1.335 , -0.6277]
-Original Grad: -0.099, -lr * Pred Grad:  0.024, New P: 1.359
-Original Grad: 0.021, -lr * Pred Grad:  -0.013, New P: -0.640
iter 18 loss: 0.374
Actual params: [ 1.3587, -0.6404]
-Original Grad: -0.075, -lr * Pred Grad:  0.009, New P: 1.368
-Original Grad: 0.009, -lr * Pred Grad:  -0.010, New P: -0.650
iter 19 loss: 0.375
Actual params: [ 1.3681, -0.6503]
-Original Grad: -0.044, -lr * Pred Grad:  0.002, New P: 1.370
-Original Grad: 0.005, -lr * Pred Grad:  -0.008, New P: -0.658
iter 20 loss: 0.376
Actual params: [ 1.3696, -0.6582]
-Original Grad: 0.008, -lr * Pred Grad:  0.003, New P: 1.372
-Original Grad: -0.022, -lr * Pred Grad:  -0.012, New P: -0.670
Target params: [1.1812, 0.2779]
iter 0 loss: 0.458
Actual params: [0.5941, 0.5941]
-Original Grad: 0.011, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.261, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.408
Actual params: [0.6941, 0.4941]
-Original Grad: 0.052, -lr * Pred Grad:  0.087, New P: 0.781
-Original Grad: -0.107, -lr * Pred Grad:  -0.090, New P: 0.404
iter 2 loss: 0.368
Actual params: [0.7808, 0.4039]
-Original Grad: 0.025, -lr * Pred Grad:  0.088, New P: 0.869
-Original Grad: -0.065, -lr * Pred Grad:  -0.082, New P: 0.322
iter 3 loss: 0.347
Actual params: [0.8688, 0.3217]
-Original Grad: 0.007, -lr * Pred Grad:  0.078, New P: 0.947
-Original Grad: -0.016, -lr * Pred Grad:  -0.070, New P: 0.251
iter 4 loss: 0.345
Actual params: [0.9471, 0.2513]
-Original Grad: 0.031, -lr * Pred Grad:  0.084, New P: 1.031
-Original Grad: -0.029, -lr * Pred Grad:  -0.065, New P: 0.187
iter 5 loss: 0.341
Actual params: [1.0311, 0.1866]
-Original Grad: 0.005, -lr * Pred Grad:  0.076, New P: 1.107
-Original Grad: 0.009, -lr * Pred Grad:  -0.054, New P: 0.132
iter 6 loss: 0.340
Actual params: [1.1074, 0.1325]
-Original Grad: 0.003, -lr * Pred Grad:  0.069, New P: 1.176
-Original Grad: -0.027, -lr * Pred Grad:  -0.052, New P: 0.081
iter 7 loss: 0.343
Actual params: [1.1762, 0.0808]
-Original Grad: 0.023, -lr * Pred Grad:  0.074, New P: 1.250
-Original Grad: -0.075, -lr * Pred Grad:  -0.056, New P: 0.024
iter 8 loss: 0.348
Actual params: [1.2498, 0.0243]
-Original Grad: 0.034, -lr * Pred Grad:  0.080, New P: 1.330
-Original Grad: 0.058, -lr * Pred Grad:  -0.040, New P: -0.016
iter 9 loss: 0.349
Actual params: [ 1.3298, -0.0157]
-Original Grad: 0.011, -lr * Pred Grad:  0.078, New P: 1.407
-Original Grad: -0.005, -lr * Pred Grad:  -0.036, New P: -0.052
iter 10 loss: 0.351
Actual params: [ 1.4074, -0.0522]
-Original Grad: 0.002, -lr * Pred Grad:  0.071, New P: 1.478
-Original Grad: -0.016, -lr * Pred Grad:  -0.035, New P: -0.087
iter 11 loss: 0.352
Actual params: [ 1.4782, -0.0873]
-Original Grad: -0.012, -lr * Pred Grad:  0.055, New P: 1.534
-Original Grad: 0.012, -lr * Pred Grad:  -0.030, New P: -0.117
iter 12 loss: 0.354
Actual params: [ 1.5337, -0.117 ]
-Original Grad: 0.012, -lr * Pred Grad:  0.057, New P: 1.590
-Original Grad: 0.013, -lr * Pred Grad:  -0.025, New P: -0.142
iter 13 loss: 0.356
Actual params: [ 1.5904, -0.1417]
-Original Grad: -0.005, -lr * Pred Grad:  0.048, New P: 1.639
-Original Grad: 0.004, -lr * Pred Grad:  -0.022, New P: -0.163
iter 14 loss: 0.357
Actual params: [ 1.6387, -0.1634]
-Original Grad: -0.002, -lr * Pred Grad:  0.043, New P: 1.682
-Original Grad: 0.005, -lr * Pred Grad:  -0.019, New P: -0.182
iter 15 loss: 0.359
Actual params: [ 1.6815, -0.1822]
-Original Grad: 0.005, -lr * Pred Grad:  0.042, New P: 1.723
-Original Grad: 0.013, -lr * Pred Grad:  -0.015, New P: -0.197
iter 16 loss: 0.361
Actual params: [ 1.7232, -0.1972]
-Original Grad: -0.007, -lr * Pred Grad:  0.033, New P: 1.756
-Original Grad: -0.004, -lr * Pred Grad:  -0.014, New P: -0.211
iter 17 loss: 0.362
Actual params: [ 1.7563, -0.2114]
-Original Grad: 0.001, -lr * Pred Grad:  0.031, New P: 1.787
-Original Grad: 0.019, -lr * Pred Grad:  -0.010, New P: -0.221
iter 18 loss: 0.364
Actual params: [ 1.7871, -0.2212]
-Original Grad: 0.007, -lr * Pred Grad:  0.032, New P: 1.819
-Original Grad: 0.039, -lr * Pred Grad:  -0.002, New P: -0.224
iter 19 loss: 0.366
Actual params: [ 1.8191, -0.2236]
-Original Grad: -0.004, -lr * Pred Grad:  0.026, New P: 1.845
-Original Grad: -0.010, -lr * Pred Grad:  -0.004, New P: -0.227
iter 20 loss: 0.368
Actual params: [ 1.8454, -0.2274]
-Original Grad: 0.006, -lr * Pred Grad:  0.027, New P: 1.873
-Original Grad: 0.009, -lr * Pred Grad:  -0.002, New P: -0.229
Target params: [1.1812, 0.2779]
iter 0 loss: 0.809
Actual params: [0.5941, 0.5941]
-Original Grad: -0.231, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.145, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.797
Actual params: [0.4941, 0.4941]
-Original Grad: -0.011, -lr * Pred Grad:  -0.070, New P: 0.424
-Original Grad: 0.019, -lr * Pred Grad:  -0.057, New P: 0.437
iter 2 loss: 0.796
Actual params: [0.4237, 0.4373]
-Original Grad: 0.075, -lr * Pred Grad:  -0.032, New P: 0.392
-Original Grad: 0.001, -lr * Pred Grad:  -0.043, New P: 0.394
iter 3 loss: 0.794
Actual params: [0.3917, 0.394 ]
-Original Grad: 0.044, -lr * Pred Grad:  -0.015, New P: 0.376
-Original Grad: -0.197, -lr * Pred Grad:  -0.068, New P: 0.326
iter 4 loss: 0.788
Actual params: [0.3763, 0.3262]
-Original Grad: -0.006, -lr * Pred Grad:  -0.014, New P: 0.362
-Original Grad: 0.000, -lr * Pred Grad:  -0.057, New P: 0.269
iter 5 loss: 0.784
Actual params: [0.362 , 0.2689]
-Original Grad: 0.027, -lr * Pred Grad:  -0.006, New P: 0.355
-Original Grad: 0.008, -lr * Pred Grad:  -0.048, New P: 0.221
iter 6 loss: 0.780
Actual params: [0.3555, 0.2212]
-Original Grad: 0.044, -lr * Pred Grad:  0.003, New P: 0.359
-Original Grad: 0.030, -lr * Pred Grad:  -0.035, New P: 0.186
iter 7 loss: 0.776
Actual params: [0.3587, 0.1861]
-Original Grad: 0.059, -lr * Pred Grad:  0.014, New P: 0.373
-Original Grad: 0.026, -lr * Pred Grad:  -0.026, New P: 0.161
iter 8 loss: 0.771
Actual params: [0.3726, 0.1605]
-Original Grad: 0.062, -lr * Pred Grad:  0.024, New P: 0.396
-Original Grad: 0.021, -lr * Pred Grad:  -0.018, New P: 0.142
iter 9 loss: 0.765
Actual params: [0.3961, 0.1421]
-Original Grad: 0.026, -lr * Pred Grad:  0.026, New P: 0.422
-Original Grad: -0.098, -lr * Pred Grad:  -0.033, New P: 0.109
iter 10 loss: 0.758
Actual params: [0.4217, 0.109 ]
-Original Grad: 0.042, -lr * Pred Grad:  0.030, New P: 0.452
-Original Grad: -0.016, -lr * Pred Grad:  -0.033, New P: 0.077
iter 11 loss: 0.749
Actual params: [0.4517, 0.0765]
-Original Grad: 0.033, -lr * Pred Grad:  0.033, New P: 0.484
-Original Grad: 0.020, -lr * Pred Grad:  -0.026, New P: 0.051
iter 12 loss: 0.740
Actual params: [0.4843, 0.051 ]
-Original Grad: 0.094, -lr * Pred Grad:  0.044, New P: 0.528
-Original Grad: -0.061, -lr * Pred Grad:  -0.033, New P: 0.018
iter 13 loss: 0.728
Actual params: [0.5279, 0.0179]
-Original Grad: 0.075, -lr * Pred Grad:  0.050, New P: 0.578
-Original Grad: -0.043, -lr * Pred Grad:  -0.037, New P: -0.019
iter 14 loss: 0.714
Actual params: [ 0.5782, -0.0191]
-Original Grad: 0.052, -lr * Pred Grad:  0.053, New P: 0.631
-Original Grad: -0.022, -lr * Pred Grad:  -0.037, New P: -0.056
iter 15 loss: 0.697
Actual params: [ 0.6314, -0.0563]
-Original Grad: 0.307, -lr * Pred Grad:  0.069, New P: 0.700
-Original Grad: -0.020, -lr * Pred Grad:  -0.037, New P: -0.093
iter 16 loss: 0.670
Actual params: [ 0.7001, -0.0934]
-Original Grad: -0.065, -lr * Pred Grad:  0.054, New P: 0.754
-Original Grad: -0.074, -lr * Pred Grad:  -0.045, New P: -0.139
iter 17 loss: 0.638
Actual params: [ 0.7543, -0.1385]
-Original Grad: -0.166, -lr * Pred Grad:  0.028, New P: 0.783
-Original Grad: -0.079, -lr * Pred Grad:  -0.053, New P: -0.191
iter 18 loss: 0.625
Actual params: [ 0.7826, -0.1912]
-Original Grad: 0.129, -lr * Pred Grad:  0.038, New P: 0.821
-Original Grad: -0.013, -lr * Pred Grad:  -0.050, New P: -0.241
iter 19 loss: 0.595
Actual params: [ 0.8208, -0.2411]
-Original Grad: -0.076, -lr * Pred Grad:  0.026, New P: 0.847
-Original Grad: -0.046, -lr * Pred Grad:  -0.053, New P: -0.294
iter 20 loss: 0.569
Actual params: [ 0.8471, -0.2937]
-Original Grad: 0.013, -lr * Pred Grad:  0.025, New P: 0.872
-Original Grad: -0.067, -lr * Pred Grad:  -0.058, New P: -0.352
Target params: [1.1812, 0.2779]
iter 0 loss: 0.789
Actual params: [0.5941, 0.5941]
-Original Grad: 0.150, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.052, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.730
Actual params: [0.6941, 0.4941]
-Original Grad: 0.042, -lr * Pred Grad:  0.085, New P: 0.779
-Original Grad: -0.060, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 0.650
Actual params: [0.7788, 0.3939]
-Original Grad: -0.018, -lr * Pred Grad:  0.058, New P: 0.836
-Original Grad: -0.140, -lr * Pred Grad:  -0.094, New P: 0.300
iter 3 loss: 0.548
Actual params: [0.8364, 0.3002]
-Original Grad: -0.649, -lr * Pred Grad:  -0.045, New P: 0.791
-Original Grad: -0.134, -lr * Pred Grad:  -0.096, New P: 0.204
iter 4 loss: 0.609
Actual params: [0.791 , 0.2041]
-Original Grad: -0.011, -lr * Pred Grad:  -0.039, New P: 0.752
-Original Grad: -0.062, -lr * Pred Grad:  -0.093, New P: 0.111
iter 5 loss: 0.666
Actual params: [0.7517, 0.1107]
-Original Grad: 0.380, -lr * Pred Grad:  -0.004, New P: 0.748
-Original Grad: 0.072, -lr * Pred Grad:  -0.060, New P: 0.051
iter 6 loss: 0.674
Actual params: [0.7481, 0.0506]
-Original Grad: 0.193, -lr * Pred Grad:  0.009, New P: 0.757
-Original Grad: 0.053, -lr * Pred Grad:  -0.040, New P: 0.011
iter 7 loss: 0.667
Actual params: [0.7574, 0.0109]
-Original Grad: 0.080, -lr * Pred Grad:  0.013, New P: 0.770
-Original Grad: 0.042, -lr * Pred Grad:  -0.026, New P: -0.015
iter 8 loss: 0.647
Actual params: [ 0.7705, -0.0148]
-Original Grad: 0.043, -lr * Pred Grad:  0.014, New P: 0.785
-Original Grad: 0.096, -lr * Pred Grad:  -0.003, New P: -0.018
iter 9 loss: 0.619
Actual params: [ 0.7848, -0.0177]
-Original Grad: 0.070, -lr * Pred Grad:  0.017, New P: 0.802
-Original Grad: 0.114, -lr * Pred Grad:  0.017, New P: -0.001
iter 10 loss: 0.584
Actual params: [ 8.0167e-01, -5.1597e-04]
-Original Grad: 0.113, -lr * Pred Grad:  0.022, New P: 0.823
-Original Grad: 0.005, -lr * Pred Grad:  0.016, New P: 0.016
iter 11 loss: 0.538
Actual params: [0.8234, 0.0157]
-Original Grad: 0.291, -lr * Pred Grad:  0.035, New P: 0.858
-Original Grad: 0.106, -lr * Pred Grad:  0.031, New P: 0.046
iter 12 loss: 0.468
Actual params: [0.8582, 0.0462]
-Original Grad: 0.084, -lr * Pred Grad:  0.036, New P: 0.894
-Original Grad: 0.060, -lr * Pred Grad:  0.036, New P: 0.083
iter 13 loss: 0.411
Actual params: [0.8941, 0.0827]
-Original Grad: -0.203, -lr * Pred Grad:  0.020, New P: 0.915
-Original Grad: -0.034, -lr * Pred Grad:  0.027, New P: 0.110
iter 14 loss: 0.391
Actual params: [0.9145, 0.1102]
-Original Grad: -0.301, -lr * Pred Grad:  0.002, New P: 0.916
-Original Grad: -0.132, -lr * Pred Grad:  0.004, New P: 0.114
iter 15 loss: 0.390
Actual params: [0.9164, 0.1138]
-Original Grad: 0.170, -lr * Pred Grad:  0.010, New P: 0.927
-Original Grad: -0.257, -lr * Pred Grad:  -0.027, New P: 0.087
iter 16 loss: 0.409
Actual params: [0.9268, 0.0867]
-Original Grad: 0.197, -lr * Pred Grad:  0.019, New P: 0.946
-Original Grad: -0.137, -lr * Pred Grad:  -0.039, New P: 0.048
iter 17 loss: 0.448
Actual params: [0.9461, 0.048 ]
-Original Grad: -0.268, -lr * Pred Grad:  0.004, New P: 0.950
-Original Grad: 0.026, -lr * Pred Grad:  -0.032, New P: 0.016
iter 18 loss: 0.492
Actual params: [0.9497, 0.0158]
-Original Grad: 0.022, -lr * Pred Grad:  0.004, New P: 0.954
-Original Grad: 0.004, -lr * Pred Grad:  -0.029, New P: -0.013
iter 19 loss: 1.623
Actual params: [ 0.954, -0.013]
-Original Grad: -0.141, -lr * Pred Grad:  -0.003, New P: 0.951
-Original Grad: 0.042, -lr * Pred Grad:  -0.021, New P: -0.034
iter 20 loss: 1.626
Actual params: [ 0.9508, -0.0343]
-Original Grad: 0.049, -lr * Pred Grad:  -0.000, New P: 0.950
-Original Grad: 0.009, -lr * Pred Grad:  -0.018, New P: -0.053
Target params: [1.1812, 0.2779]
iter 0 loss: 0.859
Actual params: [0.5941, 0.5941]
-Original Grad: 0.021, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.130, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.638
Actual params: [0.6941, 0.4941]
-Original Grad: -0.011, -lr * Pred Grad:  0.025, New P: 0.719
-Original Grad: -0.056, -lr * Pred Grad:  -0.091, New P: 0.403
iter 2 loss: 0.475
Actual params: [0.719, 0.403]
-Original Grad: 0.074, -lr * Pred Grad:  0.067, New P: 0.786
-Original Grad: -0.160, -lr * Pred Grad:  -0.094, New P: 0.309
iter 3 loss: 0.395
Actual params: [0.7857, 0.3086]
-Original Grad: 0.009, -lr * Pred Grad:  0.061, New P: 0.847
-Original Grad: -0.023, -lr * Pred Grad:  -0.083, New P: 0.225
iter 4 loss: 0.364
Actual params: [0.8469, 0.2254]
-Original Grad: -0.003, -lr * Pred Grad:  0.050, New P: 0.897
-Original Grad: -0.061, -lr * Pred Grad:  -0.083, New P: 0.143
iter 5 loss: 0.349
Actual params: [0.8967, 0.1428]
-Original Grad: 0.016, -lr * Pred Grad:  0.053, New P: 0.949
-Original Grad: 0.012, -lr * Pred Grad:  -0.068, New P: 0.074
iter 6 loss: 0.340
Actual params: [0.9493, 0.0745]
-Original Grad: 0.024, -lr * Pred Grad:  0.059, New P: 1.008
-Original Grad: -0.038, -lr * Pred Grad:  -0.067, New P: 0.007
iter 7 loss: 0.334
Actual params: [1.0081, 0.0071]
-Original Grad: -0.014, -lr * Pred Grad:  0.043, New P: 1.051
-Original Grad: -0.000, -lr * Pred Grad:  -0.059, New P: -0.052
iter 8 loss: 0.330
Actual params: [ 1.0508, -0.0523]
-Original Grad: 0.012, -lr * Pred Grad:  0.044, New P: 1.095
-Original Grad: -0.017, -lr * Pred Grad:  -0.056, New P: -0.109
iter 9 loss: 0.323
Actual params: [ 1.0951, -0.1086]
-Original Grad: -0.005, -lr * Pred Grad:  0.037, New P: 1.132
-Original Grad: -0.036, -lr * Pred Grad:  -0.057, New P: -0.166
iter 10 loss: 0.312
Actual params: [ 1.1319, -0.1657]
-Original Grad: 0.008, -lr * Pred Grad:  0.037, New P: 1.169
-Original Grad: -0.092, -lr * Pred Grad:  -0.065, New P: -0.231
iter 11 loss: 0.307
Actual params: [ 1.1691, -0.2312]
-Original Grad: 0.019, -lr * Pred Grad:  0.043, New P: 1.212
-Original Grad: -0.073, -lr * Pred Grad:  -0.070, New P: -0.301
iter 12 loss: 0.304
Actual params: [ 1.2124, -0.3013]
-Original Grad: 0.029, -lr * Pred Grad:  0.052, New P: 1.265
-Original Grad: -0.007, -lr * Pred Grad:  -0.064, New P: -0.366
iter 13 loss: 0.304
Actual params: [ 1.2646, -0.3657]
-Original Grad: 0.015, -lr * Pred Grad:  0.054, New P: 1.319
-Original Grad: 0.028, -lr * Pred Grad:  -0.053, New P: -0.418
iter 14 loss: 0.303
Actual params: [ 1.3189, -0.4182]
-Original Grad: 0.012, -lr * Pred Grad:  0.055, New P: 1.374
-Original Grad: -0.017, -lr * Pred Grad:  -0.051, New P: -0.469
iter 15 loss: 0.303
Actual params: [ 1.3737, -0.4689]
-Original Grad: 0.023, -lr * Pred Grad:  0.060, New P: 1.434
-Original Grad: -0.047, -lr * Pred Grad:  -0.054, New P: -0.523
iter 16 loss: 0.303
Actual params: [ 1.4335, -0.5229]
-Original Grad: 0.020, -lr * Pred Grad:  0.063, New P: 1.497
-Original Grad: 0.106, -lr * Pred Grad:  -0.027, New P: -0.550
iter 17 loss: 0.302
Actual params: [ 1.4968, -0.5499]
-Original Grad: 0.004, -lr * Pred Grad:  0.059, New P: 1.556
-Original Grad: 0.047, -lr * Pred Grad:  -0.016, New P: -0.566
iter 18 loss: 0.302
Actual params: [ 1.556 , -0.5659]
-Original Grad: 0.017, -lr * Pred Grad:  0.061, New P: 1.617
-Original Grad: 0.107, -lr * Pred Grad:  0.004, New P: -0.562
iter 19 loss: 0.301
Actual params: [ 1.6174, -0.5621]
-Original Grad: -0.009, -lr * Pred Grad:  0.051, New P: 1.668
-Original Grad: -0.003, -lr * Pred Grad:  0.003, New P: -0.559
iter 20 loss: 0.300
Actual params: [ 1.6684, -0.5592]
-Original Grad: -0.005, -lr * Pred Grad:  0.044, New P: 1.712
-Original Grad: 0.015, -lr * Pred Grad:  0.005, New P: -0.554
Target params: [1.1812, 0.2779]
iter 0 loss: 0.601
Actual params: [0.5941, 0.5941]
-Original Grad: -0.086, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.091, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.711
Actual params: [0.4941, 0.4941]
-Original Grad: -0.013, -lr * Pred Grad:  -0.077, New P: 0.417
-Original Grad: 0.075, -lr * Pred Grad:  -0.004, New P: 0.490
iter 2 loss: 0.734
Actual params: [0.4167, 0.4898]
-Original Grad: 0.036, -lr * Pred Grad:  -0.031, New P: 0.386
-Original Grad: 0.018, -lr * Pred Grad:  0.007, New P: 0.496
iter 3 loss: 0.738
Actual params: [0.3857, 0.4963]
-Original Grad: 0.122, -lr * Pred Grad:  0.031, New P: 0.416
-Original Grad: 0.101, -lr * Pred Grad:  0.042, New P: 0.538
iter 4 loss: 0.717
Actual params: [0.4163, 0.5379]
-Original Grad: -0.124, -lr * Pred Grad:  -0.014, New P: 0.402
-Original Grad: -0.033, -lr * Pred Grad:  0.023, New P: 0.561
iter 5 loss: 0.712
Actual params: [0.4022, 0.5611]
-Original Grad: -0.166, -lr * Pred Grad:  -0.043, New P: 0.359
-Original Grad: -0.062, -lr * Pred Grad:  -0.000, New P: 0.561
iter 6 loss: 0.723
Actual params: [0.3593, 0.5609]
-Original Grad: -0.083, -lr * Pred Grad:  -0.051, New P: 0.308
-Original Grad: -0.005, -lr * Pred Grad:  -0.002, New P: 0.559
iter 7 loss: 0.734
Actual params: [0.3082, 0.5593]
-Original Grad: -0.028, -lr * Pred Grad:  -0.050, New P: 0.258
-Original Grad: 0.031, -lr * Pred Grad:  0.007, New P: 0.567
iter 8 loss: 0.743
Actual params: [0.2583, 0.5668]
-Original Grad: 0.004, -lr * Pred Grad:  -0.044, New P: 0.215
-Original Grad: 0.078, -lr * Pred Grad:  0.026, New P: 0.593
iter 9 loss: 0.746
Actual params: [0.2148, 0.5929]
-Original Grad: 0.026, -lr * Pred Grad:  -0.034, New P: 0.181
-Original Grad: -0.013, -lr * Pred Grad:  0.020, New P: 0.613
iter 10 loss: 0.745
Actual params: [0.1808, 0.6129]
-Original Grad: -0.120, -lr * Pred Grad:  -0.047, New P: 0.133
-Original Grad: -0.042, -lr * Pred Grad:  0.007, New P: 0.620
iter 11 loss: 0.750
Actual params: [0.1335, 0.62  ]
-Original Grad: -0.003, -lr * Pred Grad:  -0.043, New P: 0.091
-Original Grad: -0.008, -lr * Pred Grad:  0.004, New P: 0.624
iter 12 loss: 0.754
Actual params: [0.0905, 0.6244]
-Original Grad: 0.052, -lr * Pred Grad:  -0.030, New P: 0.061
-Original Grad: 0.038, -lr * Pred Grad:  0.013, New P: 0.637
iter 13 loss: 0.754
Actual params: [0.0606, 0.6374]
-Original Grad: 0.060, -lr * Pred Grad:  -0.017, New P: 0.043
-Original Grad: 0.065, -lr * Pred Grad:  0.026, New P: 0.664
iter 14 loss: 0.751
Actual params: [0.0434, 0.6637]
-Original Grad: 0.059, -lr * Pred Grad:  -0.006, New P: 0.037
-Original Grad: 0.030, -lr * Pred Grad:  0.030, New P: 0.694
iter 15 loss: 0.747
Actual params: [0.0372, 0.694 ]
-Original Grad: 0.049, -lr * Pred Grad:  0.002, New P: 0.039
-Original Grad: 0.030, -lr * Pred Grad:  0.034, New P: 0.728
iter 16 loss: 0.743
Actual params: [0.0393, 0.7281]
-Original Grad: -0.038, -lr * Pred Grad:  -0.004, New P: 0.035
-Original Grad: -0.045, -lr * Pred Grad:  0.020, New P: 0.748
iter 17 loss: 0.742
Actual params: [0.0353, 0.7482]
-Original Grad: 0.037, -lr * Pred Grad:  0.002, New P: 0.037
-Original Grad: 0.006, -lr * Pred Grad:  0.020, New P: 0.768
iter 18 loss: 0.740
Actual params: [0.0375, 0.7679]
-Original Grad: -0.008, -lr * Pred Grad:  0.001, New P: 0.038
-Original Grad: -0.082, -lr * Pred Grad:  -0.001, New P: 0.767
iter 19 loss: 0.740
Actual params: [0.0382, 0.767 ]
-Original Grad: -0.028, -lr * Pred Grad:  -0.004, New P: 0.034
-Original Grad: -0.055, -lr * Pred Grad:  -0.012, New P: 0.755
iter 20 loss: 0.741
Actual params: [0.0345, 0.7546]
-Original Grad: -0.035, -lr * Pred Grad:  -0.009, New P: 0.026
-Original Grad: -0.047, -lr * Pred Grad:  -0.021, New P: 0.734
Target params: [1.1812, 0.2779]
iter 0 loss: 0.756
Actual params: [0.5941, 0.5941]
-Original Grad: -0.011, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.042, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.694
Actual params: [0.4941, 0.4941]
-Original Grad: -0.068, -lr * Pred Grad:  -0.084, New P: 0.410
-Original Grad: -0.050, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 0.653
Actual params: [0.4099, 0.394 ]
-Original Grad: 0.084, -lr * Pred Grad:  0.008, New P: 0.418
-Original Grad: 0.011, -lr * Pred Grad:  -0.066, New P: 0.328
iter 3 loss: 0.622
Actual params: [0.4183, 0.3284]
-Original Grad: 0.001, -lr * Pred Grad:  0.007, New P: 0.426
-Original Grad: -0.044, -lr * Pred Grad:  -0.077, New P: 0.252
iter 4 loss: 0.586
Actual params: [0.4256, 0.2516]
-Original Grad: 0.038, -lr * Pred Grad:  0.024, New P: 0.449
-Original Grad: -0.117, -lr * Pred Grad:  -0.082, New P: 0.170
iter 5 loss: 0.549
Actual params: [0.4495, 0.1699]
-Original Grad: -0.010, -lr * Pred Grad:  0.016, New P: 0.465
-Original Grad: -0.120, -lr * Pred Grad:  -0.087, New P: 0.082
iter 6 loss: 0.520
Actual params: [0.4653, 0.0825]
-Original Grad: -0.001, -lr * Pred Grad:  0.013, New P: 0.478
-Original Grad: -0.013, -lr * Pred Grad:  -0.080, New P: 0.003
iter 7 loss: 0.508
Actual params: [0.4784, 0.0026]
-Original Grad: 0.127, -lr * Pred Grad:  0.044, New P: 0.523
-Original Grad: 0.014, -lr * Pred Grad:  -0.066, New P: -0.064
iter 8 loss: 0.490
Actual params: [ 0.5229, -0.0638]
-Original Grad: 0.145, -lr * Pred Grad:  0.062, New P: 0.585
-Original Grad: -0.055, -lr * Pred Grad:  -0.070, New P: -0.134
iter 9 loss: 0.457
Actual params: [ 0.5846, -0.1341]
-Original Grad: -0.079, -lr * Pred Grad:  0.036, New P: 0.620
-Original Grad: -0.051, -lr * Pred Grad:  -0.073, New P: -0.207
iter 10 loss: 0.435
Actual params: [ 0.6204, -0.2072]
-Original Grad: 0.172, -lr * Pred Grad:  0.054, New P: 0.675
-Original Grad: -0.009, -lr * Pred Grad:  -0.067, New P: -0.275
iter 11 loss: 0.405
Actual params: [ 0.6748, -0.2746]
-Original Grad: 0.108, -lr * Pred Grad:  0.062, New P: 0.737
-Original Grad: -0.052, -lr * Pred Grad:  -0.071, New P: -0.345
iter 12 loss: 0.368
Actual params: [ 0.7372, -0.3453]
-Original Grad: -0.326, -lr * Pred Grad:  0.004, New P: 0.741
-Original Grad: -0.033, -lr * Pred Grad:  -0.070, New P: -0.416
iter 13 loss: 0.360
Actual params: [ 0.7414, -0.4158]
-Original Grad: 0.295, -lr * Pred Grad:  0.030, New P: 0.771
-Original Grad: -0.116, -lr * Pred Grad:  -0.079, New P: -0.495
iter 14 loss: 0.338
Actual params: [ 0.7709, -0.4949]
-Original Grad: 0.216, -lr * Pred Grad:  0.043, New P: 0.814
-Original Grad: -0.138, -lr * Pred Grad:  -0.086, New P: -0.581
iter 15 loss: 0.308
Actual params: [ 0.8138, -0.5811]
-Original Grad: -0.457, -lr * Pred Grad:  0.000, New P: 0.814
-Original Grad: -0.114, -lr * Pred Grad:  -0.091, New P: -0.672
iter 16 loss: 0.444
Actual params: [ 0.8141, -0.6721]
-Original Grad: -0.724, -lr * Pred Grad:  -0.034, New P: 0.780
-Original Grad: 0.250, -lr * Pred Grad:  -0.032, New P: -0.704
iter 17 loss: 0.439
Actual params: [ 0.7798, -0.7037]
-Original Grad: -0.681, -lr * Pred Grad:  -0.053, New P: 0.726
-Original Grad: 0.190, -lr * Pred Grad:  -0.004, New P: -0.708
iter 18 loss: 0.315
Actual params: [ 0.7264, -0.7076]
-Original Grad: -1.078, -lr * Pred Grad:  -0.070, New P: 0.657
-Original Grad: 0.792, -lr * Pred Grad:  0.042, New P: -0.665
iter 19 loss: 0.368
Actual params: [ 0.6569, -0.6653]
-Original Grad: 0.412, -lr * Pred Grad:  -0.049, New P: 0.608
-Original Grad: 0.063, -lr * Pred Grad:  0.042, New P: -0.623
iter 20 loss: 0.395
Actual params: [ 0.6079, -0.6234]
-Original Grad: -0.100, -lr * Pred Grad:  -0.048, New P: 0.560
-Original Grad: -0.093, -lr * Pred Grad:  0.033, New P: -0.591
Target params: [1.1812, 0.2779]
iter 0 loss: 0.720
Actual params: [0.5941, 0.5941]
-Original Grad: -0.043, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.012, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.783
Actual params: [0.4941, 0.4941]
-Original Grad: -0.068, -lr * Pred Grad:  -0.099, New P: 0.395
-Original Grad: -0.018, -lr * Pred Grad:  -0.099, New P: 0.395
iter 2 loss: 0.820
Actual params: [0.3953, 0.395 ]
-Original Grad: 0.015, -lr * Pred Grad:  -0.063, New P: 0.332
-Original Grad: -0.013, -lr * Pred Grad:  -0.099, New P: 0.296
iter 3 loss: 0.837
Actual params: [0.3324, 0.2963]
-Original Grad: 0.035, -lr * Pred Grad:  -0.024, New P: 0.308
-Original Grad: -0.001, -lr * Pred Grad:  -0.082, New P: 0.214
iter 4 loss: 0.845
Actual params: [0.3082, 0.2143]
-Original Grad: -0.006, -lr * Pred Grad:  -0.024, New P: 0.284
-Original Grad: -0.022, -lr * Pred Grad:  -0.088, New P: 0.126
iter 5 loss: 0.855
Actual params: [0.284 , 0.1262]
-Original Grad: 0.008, -lr * Pred Grad:  -0.016, New P: 0.268
-Original Grad: 0.020, -lr * Pred Grad:  -0.040, New P: 0.087
iter 6 loss: 0.860
Actual params: [0.268 , 0.0866]
-Original Grad: 0.026, -lr * Pred Grad:  0.001, New P: 0.269
-Original Grad: -0.025, -lr * Pred Grad:  -0.056, New P: 0.030
iter 7 loss: 0.862
Actual params: [0.2686, 0.0303]
-Original Grad: -0.043, -lr * Pred Grad:  -0.020, New P: 0.248
-Original Grad: -0.003, -lr * Pred Grad:  -0.053, New P: -0.022
iter 8 loss: 0.865
Actual params: [ 0.2485, -0.0222]
-Original Grad: 0.025, -lr * Pred Grad:  -0.006, New P: 0.243
-Original Grad: 0.017, -lr * Pred Grad:  -0.027, New P: -0.049
iter 9 loss: 0.866
Actual params: [ 0.2428, -0.0489]
-Original Grad: 0.033, -lr * Pred Grad:  0.010, New P: 0.252
-Original Grad: 0.010, -lr * Pred Grad:  -0.014, New P: -0.062
iter 10 loss: 0.866
Actual params: [ 0.2524, -0.0624]
-Original Grad: 0.016, -lr * Pred Grad:  0.016, New P: 0.268
-Original Grad: -0.030, -lr * Pred Grad:  -0.035, New P: -0.097
iter 11 loss: 0.865
Actual params: [ 0.2679, -0.0975]
-Original Grad: -0.005, -lr * Pred Grad:  0.012, New P: 0.280
-Original Grad: -0.003, -lr * Pred Grad:  -0.034, New P: -0.131
iter 12 loss: 0.865
Actual params: [ 0.2797, -0.1311]
-Original Grad: 0.028, -lr * Pred Grad:  0.022, New P: 0.302
-Original Grad: 0.024, -lr * Pred Grad:  -0.010, New P: -0.141
iter 13 loss: 0.862
Actual params: [ 0.3017, -0.1408]
-Original Grad: 0.021, -lr * Pred Grad:  0.028, New P: 0.330
-Original Grad: 0.030, -lr * Pred Grad:  0.012, New P: -0.128
iter 14 loss: 0.858
Actual params: [ 0.33  , -0.1284]
-Original Grad: 0.016, -lr * Pred Grad:  0.032, New P: 0.362
-Original Grad: 0.006, -lr * Pred Grad:  0.015, New P: -0.113
iter 15 loss: 0.852
Actual params: [ 0.362 , -0.1133]
-Original Grad: -0.094, -lr * Pred Grad:  -0.008, New P: 0.354
-Original Grad: -0.000, -lr * Pred Grad:  0.013, New P: -0.100
iter 16 loss: 0.853
Actual params: [ 0.354, -0.1  ]
-Original Grad: 0.149, -lr * Pred Grad:  0.030, New P: 0.384
-Original Grad: 0.112, -lr * Pred Grad:  0.048, New P: -0.052
iter 17 loss: 0.845
Actual params: [ 0.3836, -0.0518]
-Original Grad: 0.093, -lr * Pred Grad:  0.044, New P: 0.428
-Original Grad: 0.027, -lr * Pred Grad:  0.053, New P: 0.001
iter 18 loss: 0.835
Actual params: [0.4281, 0.0011]
-Original Grad: -0.021, -lr * Pred Grad:  0.036, New P: 0.464
-Original Grad: -0.054, -lr * Pred Grad:  0.026, New P: 0.027
iter 19 loss: 0.829
Actual params: [0.4639, 0.0272]
-Original Grad: -0.010, -lr * Pred Grad:  0.030, New P: 0.494
-Original Grad: -0.004, -lr * Pred Grad:  0.022, New P: 0.050
iter 20 loss: 0.822
Actual params: [0.4942, 0.0495]
-Original Grad: 0.033, -lr * Pred Grad:  0.035, New P: 0.529
-Original Grad: -0.035, -lr * Pred Grad:  0.008, New P: 0.057
Target params: [1.1812, 0.2779]
iter 0 loss: 0.694
Actual params: [0.5941, 0.5941]
-Original Grad: -0.039, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: 0.103, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.750
Actual params: [0.4941, 0.6941]
-Original Grad: 0.029, -lr * Pred Grad:  -0.008, New P: 0.486
-Original Grad: 0.146, -lr * Pred Grad:  0.099, New P: 0.793
iter 2 loss: 0.895
Actual params: [0.4856, 0.7935]
-Original Grad: 0.050, -lr * Pred Grad:  0.041, New P: 0.527
-Original Grad: 0.026, -lr * Pred Grad:  0.085, New P: 0.879
iter 3 loss: 1.094
Actual params: [0.5268, 0.8787]
-Original Grad: -0.090, -lr * Pred Grad:  -0.025, New P: 0.502
-Original Grad: -0.100, -lr * Pred Grad:  0.033, New P: 0.912
iter 4 loss: 1.115
Actual params: [0.5016, 0.9116]
-Original Grad: -0.026, -lr * Pred Grad:  -0.033, New P: 0.469
-Original Grad: -0.119, -lr * Pred Grad:  -0.003, New P: 0.908
iter 5 loss: 1.068
Actual params: [0.4689, 0.9084]
-Original Grad: -0.084, -lr * Pred Grad:  -0.053, New P: 0.415
-Original Grad: -0.024, -lr * Pred Grad:  -0.008, New P: 0.900
iter 6 loss: 0.985
Actual params: [0.4154, 0.9003]
-Original Grad: -0.118, -lr * Pred Grad:  -0.068, New P: 0.347
-Original Grad: -0.164, -lr * Pred Grad:  -0.034, New P: 0.866
iter 7 loss: 0.849
Actual params: [0.3471, 0.8658]
-Original Grad: -0.062, -lr * Pred Grad:  -0.073, New P: 0.274
-Original Grad: -0.180, -lr * Pred Grad:  -0.052, New P: 0.814
iter 8 loss: 0.703
Actual params: [0.2744, 0.8138]
-Original Grad: 0.037, -lr * Pred Grad:  -0.054, New P: 0.220
-Original Grad: 0.165, -lr * Pred Grad:  -0.020, New P: 0.794
iter 9 loss: 0.650
Actual params: [0.2201, 0.7936]
-Original Grad: -0.024, -lr * Pred Grad:  -0.054, New P: 0.166
-Original Grad: -0.153, -lr * Pred Grad:  -0.035, New P: 0.759
iter 10 loss: 0.578
Actual params: [0.1662, 0.7587]
-Original Grad: 0.067, -lr * Pred Grad:  -0.031, New P: 0.135
-Original Grad: -0.289, -lr * Pred Grad:  -0.053, New P: 0.705
iter 11 loss: 0.523
Actual params: [0.1355, 0.7053]
-Original Grad: 0.104, -lr * Pred Grad:  -0.003, New P: 0.132
-Original Grad: -0.129, -lr * Pred Grad:  -0.059, New P: 0.647
iter 12 loss: 0.488
Actual params: [0.1321, 0.6467]
-Original Grad: 0.037, -lr * Pred Grad:  0.005, New P: 0.137
-Original Grad: 0.011, -lr * Pred Grad:  -0.052, New P: 0.595
iter 13 loss: 0.463
Actual params: [0.1366, 0.595 ]
-Original Grad: -0.019, -lr * Pred Grad:  0.000, New P: 0.137
-Original Grad: 0.082, -lr * Pred Grad:  -0.039, New P: 0.556
iter 14 loss: 0.448
Actual params: [0.1368, 0.5563]
-Original Grad: -0.052, -lr * Pred Grad:  -0.010, New P: 0.127
-Original Grad: 0.001, -lr * Pred Grad:  -0.035, New P: 0.521
iter 15 loss: 0.435
Actual params: [0.1266, 0.5214]
-Original Grad: 0.010, -lr * Pred Grad:  -0.007, New P: 0.119
-Original Grad: 0.031, -lr * Pred Grad:  -0.029, New P: 0.493
iter 16 loss: 0.431
Actual params: [0.1194, 0.4928]
-Original Grad: 0.051, -lr * Pred Grad:  0.004, New P: 0.123
-Original Grad: -0.028, -lr * Pred Grad:  -0.029, New P: 0.464
iter 17 loss: 0.426
Actual params: [0.123 , 0.4641]
-Original Grad: -0.057, -lr * Pred Grad:  -0.008, New P: 0.115
-Original Grad: -0.043, -lr * Pred Grad:  -0.030, New P: 0.434
iter 18 loss: 0.418
Actual params: [0.1151, 0.4342]
-Original Grad: 0.038, -lr * Pred Grad:  0.000, New P: 0.115
-Original Grad: 0.015, -lr * Pred Grad:  -0.026, New P: 0.408
iter 19 loss: 0.414
Actual params: [0.1153, 0.4084]
-Original Grad: -0.020, -lr * Pred Grad:  -0.004, New P: 0.112
-Original Grad: -0.024, -lr * Pred Grad:  -0.026, New P: 0.383
iter 20 loss: 0.411
Actual params: [0.1115, 0.3826]
-Original Grad: -0.015, -lr * Pred Grad:  -0.006, New P: 0.105
-Original Grad: -0.055, -lr * Pred Grad:  -0.029, New P: 0.354
Target params: [1.1812, 0.2779]
iter 0 loss: 0.534
Actual params: [0.5941, 0.5941]
-Original Grad: 0.036, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.243, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.521
Actual params: [0.6941, 0.4941]
-Original Grad: 0.058, -lr * Pred Grad:  0.098, New P: 0.792
-Original Grad: -0.113, -lr * Pred Grad:  -0.092, New P: 0.402
iter 2 loss: 0.498
Actual params: [0.7925, 0.4019]
-Original Grad: -0.001, -lr * Pred Grad:  0.075, New P: 0.868
-Original Grad: -0.155, -lr * Pred Grad:  -0.094, New P: 0.308
iter 3 loss: 0.490
Actual params: [0.8677, 0.3083]
-Original Grad: -0.002, -lr * Pred Grad:  0.060, New P: 0.928
-Original Grad: 0.003, -lr * Pred Grad:  -0.076, New P: 0.232
iter 4 loss: 0.487
Actual params: [0.9276, 0.2322]
-Original Grad: -0.150, -lr * Pred Grad:  -0.029, New P: 0.899
-Original Grad: -0.070, -lr * Pred Grad:  -0.075, New P: 0.158
iter 5 loss: 0.499
Actual params: [0.899 , 0.1575]
-Original Grad: -0.012, -lr * Pred Grad:  -0.028, New P: 0.871
-Original Grad: -0.027, -lr * Pred Grad:  -0.069, New P: 0.089
iter 6 loss: 0.510
Actual params: [0.8706, 0.0889]
-Original Grad: -0.021, -lr * Pred Grad:  -0.031, New P: 0.840
-Original Grad: -0.068, -lr * Pred Grad:  -0.069, New P: 0.020
iter 7 loss: 0.523
Actual params: [0.8395, 0.0198]
-Original Grad: 0.047, -lr * Pred Grad:  -0.013, New P: 0.827
-Original Grad: -0.007, -lr * Pred Grad:  -0.062, New P: -0.042
iter 8 loss: 0.532
Actual params: [ 0.8266, -0.0423]
-Original Grad: -0.005, -lr * Pred Grad:  -0.013, New P: 0.814
-Original Grad: 0.004, -lr * Pred Grad:  -0.055, New P: -0.097
iter 9 loss: 0.541
Actual params: [ 0.8139, -0.0968]
-Original Grad: 0.196, -lr * Pred Grad:  0.029, New P: 0.843
-Original Grad: -0.049, -lr * Pred Grad:  -0.055, New P: -0.152
iter 10 loss: 0.536
Actual params: [ 0.8428, -0.1522]
-Original Grad: 0.104, -lr * Pred Grad:  0.042, New P: 0.885
-Original Grad: -0.006, -lr * Pred Grad:  -0.050, New P: -0.203
iter 11 loss: 0.534
Actual params: [ 0.8847, -0.2026]
-Original Grad: 0.012, -lr * Pred Grad:  0.040, New P: 0.924
-Original Grad: -0.062, -lr * Pred Grad:  -0.053, New P: -0.256
iter 12 loss: 0.535
Actual params: [ 0.9243, -0.256 ]
-Original Grad: -0.021, -lr * Pred Grad:  0.032, New P: 0.956
-Original Grad: -0.003, -lr * Pred Grad:  -0.049, New P: -0.305
iter 13 loss: 0.538
Actual params: [ 0.9564, -0.3047]
-Original Grad: 0.020, -lr * Pred Grad:  0.032, New P: 0.989
-Original Grad: 0.008, -lr * Pred Grad:  -0.043, New P: -0.347
iter 14 loss: 0.540
Actual params: [ 0.9888, -0.3474]
-Original Grad: -0.003, -lr * Pred Grad:  0.029, New P: 1.018
-Original Grad: -0.031, -lr * Pred Grad:  -0.043, New P: -0.390
iter 15 loss: 0.541
Actual params: [ 1.0175, -0.3905]
-Original Grad: -0.030, -lr * Pred Grad:  0.021, New P: 1.038
-Original Grad: 0.051, -lr * Pred Grad:  -0.031, New P: -0.422
iter 16 loss: 0.547
Actual params: [ 1.0384, -0.4216]
-Original Grad: -0.053, -lr * Pred Grad:  0.010, New P: 1.048
-Original Grad: 0.135, -lr * Pred Grad:  -0.008, New P: -0.430
iter 17 loss: 0.557
Actual params: [ 1.0479, -0.4296]
-Original Grad: -0.024, -lr * Pred Grad:  0.005, New P: 1.052
-Original Grad: 0.026, -lr * Pred Grad:  -0.004, New P: -0.433
iter 18 loss: 0.559
Actual params: [ 1.0524, -0.4333]
-Original Grad: -0.045, -lr * Pred Grad:  -0.004, New P: 1.049
-Original Grad: 0.091, -lr * Pred Grad:  0.009, New P: -0.425
iter 19 loss: 0.556
Actual params: [ 1.0487, -0.4245]
-Original Grad: -0.025, -lr * Pred Grad:  -0.008, New P: 1.041
-Original Grad: 0.025, -lr * Pred Grad:  0.011, New P: -0.413
iter 20 loss: 0.545
Actual params: [ 1.0411, -0.4131]
-Original Grad: -0.050, -lr * Pred Grad:  -0.015, New P: 1.026
-Original Grad: 0.087, -lr * Pred Grad:  0.022, New P: -0.391
Target params: [1.1812, 0.2779]
iter 0 loss: 0.589
Actual params: [0.5941, 0.5941]
-Original Grad: 0.119, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.046, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.603
Actual params: [0.6941, 0.4941]
-Original Grad: 0.078, -lr * Pred Grad:  0.097, New P: 0.791
-Original Grad: -0.077, -lr * Pred Grad:  -0.098, New P: 0.396
iter 2 loss: 0.606
Actual params: [0.7908, 0.3958]
-Original Grad: 0.066, -lr * Pred Grad:  0.095, New P: 0.886
-Original Grad: 0.008, -lr * Pred Grad:  -0.070, New P: 0.326
iter 3 loss: 0.595
Actual params: [0.8857, 0.3262]
-Original Grad: -0.161, -lr * Pred Grad:  0.012, New P: 0.898
-Original Grad: -0.053, -lr * Pred Grad:  -0.079, New P: 0.248
iter 4 loss: 0.564
Actual params: [0.8981, 0.2476]
-Original Grad: 0.029, -lr * Pred Grad:  0.017, New P: 0.916
-Original Grad: 0.064, -lr * Pred Grad:  -0.028, New P: 0.219
iter 5 loss: 0.557
Actual params: [0.9155, 0.2192]
-Original Grad: -0.154, -lr * Pred Grad:  -0.017, New P: 0.899
-Original Grad: -0.029, -lr * Pred Grad:  -0.036, New P: 0.184
iter 6 loss: 0.547
Actual params: [0.8986, 0.1836]
-Original Grad: 0.001, -lr * Pred Grad:  -0.015, New P: 0.884
-Original Grad: -0.003, -lr * Pred Grad:  -0.032, New P: 0.151
iter 7 loss: 0.538
Actual params: [0.8841, 0.1512]
-Original Grad: 0.044, -lr * Pred Grad:  -0.005, New P: 0.879
-Original Grad: 0.006, -lr * Pred Grad:  -0.026, New P: 0.125
iter 8 loss: 0.533
Actual params: [0.8792, 0.125 ]
-Original Grad: 0.057, -lr * Pred Grad:  0.006, New P: 0.885
-Original Grad: 0.001, -lr * Pred Grad:  -0.023, New P: 0.102
iter 9 loss: 0.531
Actual params: [0.8847, 0.1024]
-Original Grad: 0.072, -lr * Pred Grad:  0.017, New P: 0.901
-Original Grad: -0.009, -lr * Pred Grad:  -0.023, New P: 0.079
iter 10 loss: 0.529
Actual params: [0.9015, 0.0789]
-Original Grad: -0.017, -lr * Pred Grad:  0.012, New P: 0.914
-Original Grad: -0.002, -lr * Pred Grad:  -0.022, New P: 0.057
iter 11 loss: 0.526
Actual params: [0.9137, 0.057 ]
-Original Grad: -0.053, -lr * Pred Grad:  0.002, New P: 0.916
-Original Grad: 0.031, -lr * Pred Grad:  -0.008, New P: 0.049
iter 12 loss: 0.525
Actual params: [0.9158, 0.0493]
-Original Grad: -0.114, -lr * Pred Grad:  -0.015, New P: 0.900
-Original Grad: 0.014, -lr * Pred Grad:  -0.002, New P: 0.048
iter 13 loss: 0.524
Actual params: [0.9004, 0.0475]
-Original Grad: -0.033, -lr * Pred Grad:  -0.019, New P: 0.881
-Original Grad: -0.013, -lr * Pred Grad:  -0.006, New P: 0.041
iter 14 loss: 0.522
Actual params: [0.8814, 0.0413]
-Original Grad: -0.099, -lr * Pred Grad:  -0.031, New P: 0.851
-Original Grad: -0.018, -lr * Pred Grad:  -0.012, New P: 0.029
iter 15 loss: 0.518
Actual params: [0.8506, 0.0291]
-Original Grad: 0.008, -lr * Pred Grad:  -0.027, New P: 0.824
-Original Grad: -0.008, -lr * Pred Grad:  -0.014, New P: 0.015
iter 16 loss: 0.514
Actual params: [0.8239, 0.0149]
-Original Grad: 0.072, -lr * Pred Grad:  -0.013, New P: 0.811
-Original Grad: -0.029, -lr * Pred Grad:  -0.023, New P: -0.008
iter 17 loss: 0.511
Actual params: [ 0.8106, -0.0083]
-Original Grad: 0.027, -lr * Pred Grad:  -0.008, New P: 0.802
-Original Grad: 0.008, -lr * Pred Grad:  -0.018, New P: -0.026
iter 18 loss: 0.510
Actual params: [ 0.8024, -0.0264]
-Original Grad: 0.132, -lr * Pred Grad:  0.011, New P: 0.814
-Original Grad: -0.023, -lr * Pred Grad:  -0.025, New P: -0.051
iter 19 loss: 0.511
Actual params: [ 0.8136, -0.0511]
-Original Grad: 0.117, -lr * Pred Grad:  0.025, New P: 0.839
-Original Grad: 0.002, -lr * Pred Grad:  -0.022, New P: -0.073
iter 20 loss: 0.513
Actual params: [ 0.8387, -0.0727]
-Original Grad: -0.043, -lr * Pred Grad:  0.017, New P: 0.856
-Original Grad: 0.010, -lr * Pred Grad:  -0.016, New P: -0.089
Target params: [1.1812, 0.2779]
iter 0 loss: 0.423
Actual params: [0.5941, 0.5941]
-Original Grad: 0.164, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.117, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.349
Actual params: [0.6941, 0.4941]
-Original Grad: 0.190, -lr * Pred Grad:  0.100, New P: 0.794
-Original Grad: -0.154, -lr * Pred Grad:  -0.100, New P: 0.394
iter 2 loss: 0.321
Actual params: [0.7942, 0.3943]
-Original Grad: 0.079, -lr * Pred Grad:  0.093, New P: 0.887
-Original Grad: -0.059, -lr * Pred Grad:  -0.092, New P: 0.302
iter 3 loss: 0.319
Actual params: [0.8871, 0.3018]
-Original Grad: 0.090, -lr * Pred Grad:  0.091, New P: 0.978
-Original Grad: -0.057, -lr * Pred Grad:  -0.089, New P: 0.213
iter 4 loss: 0.317
Actual params: [0.978 , 0.2131]
-Original Grad: -0.054, -lr * Pred Grad:  0.065, New P: 1.043
-Original Grad: -0.033, -lr * Pred Grad:  -0.082, New P: 0.131
iter 5 loss: 0.328
Actual params: [1.043 , 0.1306]
-Original Grad: 0.001, -lr * Pred Grad:  0.056, New P: 1.099
-Original Grad: -0.020, -lr * Pred Grad:  -0.076, New P: 0.055
iter 6 loss: 0.336
Actual params: [1.0992, 0.055 ]
-Original Grad: 0.008, -lr * Pred Grad:  0.050, New P: 1.150
-Original Grad: -0.039, -lr * Pred Grad:  -0.074, New P: -0.019
iter 7 loss: 0.341
Actual params: [ 1.1497, -0.019 ]
-Original Grad: -0.058, -lr * Pred Grad:  0.034, New P: 1.183
-Original Grad: 0.024, -lr * Pred Grad:  -0.059, New P: -0.078
iter 8 loss: 0.341
Actual params: [ 1.1834, -0.0784]
-Original Grad: -0.010, -lr * Pred Grad:  0.028, New P: 1.212
-Original Grad: -0.037, -lr * Pred Grad:  -0.060, New P: -0.139
iter 9 loss: 0.345
Actual params: [ 1.2115, -0.1385]
-Original Grad: -0.033, -lr * Pred Grad:  0.019, New P: 1.231
-Original Grad: -0.039, -lr * Pred Grad:  -0.061, New P: -0.200
iter 10 loss: 0.361
Actual params: [ 1.2309, -0.1999]
-Original Grad: -0.053, -lr * Pred Grad:  0.008, New P: 1.239
-Original Grad: -0.010, -lr * Pred Grad:  -0.057, New P: -0.257
iter 11 loss: 0.373
Actual params: [ 1.2394, -0.257 ]
-Original Grad: -0.039, -lr * Pred Grad:  0.001, New P: 1.241
-Original Grad: 0.006, -lr * Pred Grad:  -0.050, New P: -0.307
iter 12 loss: 0.380
Actual params: [ 1.2407, -0.3071]
-Original Grad: -0.011, -lr * Pred Grad:  -0.001, New P: 1.240
-Original Grad: -0.058, -lr * Pred Grad:  -0.056, New P: -0.363
iter 13 loss: 0.383
Actual params: [ 1.2401, -0.3628]
-Original Grad: -0.070, -lr * Pred Grad:  -0.012, New P: 1.228
-Original Grad: -0.024, -lr * Pred Grad:  -0.055, New P: -0.418
iter 14 loss: 0.386
Actual params: [ 1.2284, -0.4178]
-Original Grad: -0.080, -lr * Pred Grad:  -0.022, New P: 1.206
-Original Grad: 0.005, -lr * Pred Grad:  -0.049, New P: -0.467
iter 15 loss: 0.387
Actual params: [ 1.206 , -0.4665]
-Original Grad: -0.007, -lr * Pred Grad:  -0.021, New P: 1.184
-Original Grad: -0.028, -lr * Pred Grad:  -0.050, New P: -0.516
iter 16 loss: 0.392
Actual params: [ 1.1845, -0.5163]
-Original Grad: -0.027, -lr * Pred Grad:  -0.024, New P: 1.161
-Original Grad: -0.009, -lr * Pred Grad:  -0.047, New P: -0.563
iter 17 loss: 0.397
Actual params: [ 1.1609, -0.5632]
-Original Grad: 0.007, -lr * Pred Grad:  -0.020, New P: 1.141
-Original Grad: -0.087, -lr * Pred Grad:  -0.057, New P: -0.621
iter 18 loss: 0.402
Actual params: [ 1.1407, -0.6205]
-Original Grad: -0.080, -lr * Pred Grad:  -0.030, New P: 1.110
-Original Grad: -0.014, -lr * Pred Grad:  -0.055, New P: -0.675
iter 19 loss: 0.408
Actual params: [ 1.1105, -0.6754]
-Original Grad: -0.018, -lr * Pred Grad:  -0.030, New P: 1.080
-Original Grad: -0.017, -lr * Pred Grad:  -0.053, New P: -0.729
iter 20 loss: 0.414
Actual params: [ 1.0802, -0.7285]
-Original Grad: -0.153, -lr * Pred Grad:  -0.047, New P: 1.034
-Original Grad: 0.122, -lr * Pred Grad:  -0.021, New P: -0.750
Target params: [1.1812, 0.2779]
iter 0 loss: 0.561
Actual params: [0.5941, 0.5941]
-Original Grad: 0.097, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.120, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.492
Actual params: [0.6941, 0.4941]
-Original Grad: 0.198, -lr * Pred Grad:  0.096, New P: 0.790
-Original Grad: -0.071, -lr * Pred Grad:  -0.095, New P: 0.399
iter 2 loss: 0.439
Actual params: [0.7903, 0.3986]
-Original Grad: 0.029, -lr * Pred Grad:  0.082, New P: 0.872
-Original Grad: -0.121, -lr * Pred Grad:  -0.098, New P: 0.301
iter 3 loss: 0.384
Actual params: [0.8725, 0.301 ]
-Original Grad: -0.014, -lr * Pred Grad:  0.063, New P: 0.936
-Original Grad: -0.071, -lr * Pred Grad:  -0.095, New P: 0.206
iter 4 loss: 0.357
Actual params: [0.9359, 0.2055]
-Original Grad: 0.083, -lr * Pred Grad:  0.069, New P: 1.005
-Original Grad: 0.020, -lr * Pred Grad:  -0.075, New P: 0.131
iter 5 loss: 0.344
Actual params: [1.0053, 0.1306]
-Original Grad: 0.071, -lr * Pred Grad:  0.072, New P: 1.077
-Original Grad: -0.006, -lr * Pred Grad:  -0.066, New P: 0.064
iter 6 loss: 0.331
Actual params: [1.0774, 0.0644]
-Original Grad: 0.060, -lr * Pred Grad:  0.073, New P: 1.151
-Original Grad: -0.017, -lr * Pred Grad:  -0.062, New P: 0.003
iter 7 loss: 0.319
Actual params: [1.1506, 0.0025]
-Original Grad: 0.094, -lr * Pred Grad:  0.078, New P: 1.228
-Original Grad: -0.018, -lr * Pred Grad:  -0.059, New P: -0.056
iter 8 loss: 0.311
Actual params: [ 1.2282, -0.0563]
-Original Grad: 0.135, -lr * Pred Grad:  0.084, New P: 1.312
-Original Grad: 0.026, -lr * Pred Grad:  -0.045, New P: -0.102
iter 9 loss: 0.301
Actual params: [ 1.3117, -0.1018]
-Original Grad: 0.041, -lr * Pred Grad:  0.080, New P: 1.392
-Original Grad: -0.024, -lr * Pred Grad:  -0.046, New P: -0.148
iter 10 loss: 0.301
Actual params: [ 1.392 , -0.1479]
-Original Grad: 0.048, -lr * Pred Grad:  0.079, New P: 1.471
-Original Grad: -0.054, -lr * Pred Grad:  -0.052, New P: -0.200
iter 11 loss: 0.303
Actual params: [ 1.4707, -0.2003]
-Original Grad: 0.082, -lr * Pred Grad:  0.081, New P: 1.551
-Original Grad: -0.070, -lr * Pred Grad:  -0.060, New P: -0.260
iter 12 loss: 0.307
Actual params: [ 1.5513, -0.2601]
-Original Grad: -0.054, -lr * Pred Grad:  0.064, New P: 1.615
-Original Grad: 0.022, -lr * Pred Grad:  -0.049, New P: -0.309
iter 13 loss: 0.312
Actual params: [ 1.615, -0.309]
-Original Grad: -0.088, -lr * Pred Grad:  0.043, New P: 1.658
-Original Grad: -0.005, -lr * Pred Grad:  -0.045, New P: -0.354
iter 14 loss: 0.315
Actual params: [ 1.6579, -0.3543]
-Original Grad: 0.006, -lr * Pred Grad:  0.040, New P: 1.697
-Original Grad: 0.015, -lr * Pred Grad:  -0.037, New P: -0.392
iter 15 loss: 0.317
Actual params: [ 1.6975, -0.3918]
-Original Grad: -0.014, -lr * Pred Grad:  0.034, New P: 1.731
-Original Grad: 0.006, -lr * Pred Grad:  -0.033, New P: -0.425
iter 16 loss: 0.319
Actual params: [ 1.7312, -0.4245]
-Original Grad: -0.016, -lr * Pred Grad:  0.028, New P: 1.759
-Original Grad: 0.009, -lr * Pred Grad:  -0.028, New P: -0.452
iter 17 loss: 0.320
Actual params: [ 1.7594, -0.4521]
-Original Grad: -0.019, -lr * Pred Grad:  0.023, New P: 1.782
-Original Grad: -0.002, -lr * Pred Grad:  -0.025, New P: -0.478
iter 18 loss: 0.321
Actual params: [ 1.7822, -0.4776]
-Original Grad: -0.003, -lr * Pred Grad:  0.020, New P: 1.802
-Original Grad: 0.017, -lr * Pred Grad:  -0.019, New P: -0.497
iter 19 loss: 0.322
Actual params: [ 1.8024, -0.4969]
-Original Grad: 0.011, -lr * Pred Grad:  0.020, New P: 1.822
-Original Grad: -0.049, -lr * Pred Grad:  -0.028, New P: -0.525
iter 20 loss: 0.323
Actual params: [ 1.8225, -0.5248]
-Original Grad: -0.025, -lr * Pred Grad:  0.014, New P: 1.837
-Original Grad: 0.021, -lr * Pred Grad:  -0.021, New P: -0.546
Target params: [1.1812, 0.2779]
iter 0 loss: 0.763
Actual params: [0.5941, 0.5941]
-Original Grad: -0.095, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.207, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.560
Actual params: [0.4941, 0.4941]
-Original Grad: -0.039, -lr * Pred Grad:  -0.090, New P: 0.404
-Original Grad: -0.053, -lr * Pred Grad:  -0.083, New P: 0.411
iter 2 loss: 0.486
Actual params: [0.4038, 0.4107]
-Original Grad: 0.032, -lr * Pred Grad:  -0.048, New P: 0.356
-Original Grad: -0.103, -lr * Pred Grad:  -0.086, New P: 0.325
iter 3 loss: 0.456
Actual params: [0.3559, 0.325 ]
-Original Grad: -0.046, -lr * Pred Grad:  -0.059, New P: 0.297
-Original Grad: 0.063, -lr * Pred Grad:  -0.053, New P: 0.272
iter 4 loss: 0.451
Actual params: [0.297, 0.272]
-Original Grad: -0.039, -lr * Pred Grad:  -0.064, New P: 0.233
-Original Grad: -0.097, -lr * Pred Grad:  -0.062, New P: 0.210
iter 5 loss: 0.447
Actual params: [0.2325, 0.2103]
-Original Grad: 0.002, -lr * Pred Grad:  -0.055, New P: 0.178
-Original Grad: -0.082, -lr * Pred Grad:  -0.066, New P: 0.144
iter 6 loss: 0.441
Actual params: [0.1778, 0.144 ]
-Original Grad: -0.005, -lr * Pred Grad:  -0.050, New P: 0.128
-Original Grad: -0.068, -lr * Pred Grad:  -0.068, New P: 0.076
iter 7 loss: 0.434
Actual params: [0.1279, 0.0757]
-Original Grad: -0.015, -lr * Pred Grad:  -0.050, New P: 0.078
-Original Grad: -0.054, -lr * Pred Grad:  -0.068, New P: 0.007
iter 8 loss: 0.429
Actual params: [0.0784, 0.0072]
-Original Grad: 0.009, -lr * Pred Grad:  -0.040, New P: 0.038
-Original Grad: -0.124, -lr * Pred Grad:  -0.075, New P: -0.068
iter 9 loss: 0.421
Actual params: [ 0.0382, -0.068 ]
-Original Grad: -0.071, -lr * Pred Grad:  -0.055, New P: -0.017
-Original Grad: -0.004, -lr * Pred Grad:  -0.068, New P: -0.136
iter 10 loss: 0.416
Actual params: [-0.017 , -0.1357]
-Original Grad: 0.022, -lr * Pred Grad:  -0.042, New P: -0.059
-Original Grad: -0.169, -lr * Pred Grad:  -0.076, New P: -0.212
iter 11 loss: 0.398
Actual params: [-0.0587, -0.2119]
-Original Grad: -0.055, -lr * Pred Grad:  -0.052, New P: -0.111
-Original Grad: -0.111, -lr * Pred Grad:  -0.080, New P: -0.292
iter 12 loss: 0.383
Actual params: [-0.1109, -0.2917]
-Original Grad: -0.060, -lr * Pred Grad:  -0.061, New P: -0.172
-Original Grad: -0.032, -lr * Pred Grad:  -0.076, New P: -0.367
iter 13 loss: 0.371
Actual params: [-0.1722, -0.3675]
-Original Grad: -0.052, -lr * Pred Grad:  -0.067, New P: -0.240
-Original Grad: -0.056, -lr * Pred Grad:  -0.075, New P: -0.442
iter 14 loss: 0.364
Actual params: [-0.2397, -0.4424]
-Original Grad: -0.023, -lr * Pred Grad:  -0.067, New P: -0.307
-Original Grad: -0.076, -lr * Pred Grad:  -0.076, New P: -0.518
iter 15 loss: 0.364
Actual params: [-0.3065, -0.5184]
-Original Grad: -0.026, -lr * Pred Grad:  -0.067, New P: -0.374
-Original Grad: -0.041, -lr * Pred Grad:  -0.074, New P: -0.592
iter 16 loss: 0.368
Actual params: [-0.3737, -0.592 ]
-Original Grad: -0.021, -lr * Pred Grad:  -0.066, New P: -0.440
-Original Grad: 0.130, -lr * Pred Grad:  -0.048, New P: -0.640
iter 17 loss: 0.370
Actual params: [-0.4401, -0.6397]
-Original Grad: -0.008, -lr * Pred Grad:  -0.062, New P: -0.502
-Original Grad: -0.050, -lr * Pred Grad:  -0.049, New P: -0.689
iter 18 loss: 0.370
Actual params: [-0.5024, -0.6888]
-Original Grad: -0.043, -lr * Pred Grad:  -0.067, New P: -0.569
-Original Grad: 0.143, -lr * Pred Grad:  -0.026, New P: -0.714
iter 19 loss: 0.370
Actual params: [-0.5693, -0.7145]
-Original Grad: -0.035, -lr * Pred Grad:  -0.069, New P: -0.638
-Original Grad: 0.015, -lr * Pred Grad:  -0.022, New P: -0.736
iter 20 loss: 0.370
Actual params: [-0.6385, -0.736 ]
-Original Grad: 0.012, -lr * Pred Grad:  -0.059, New P: -0.698
-Original Grad: 0.245, -lr * Pred Grad:  0.008, New P: -0.728
Target params: [1.1812, 0.2779]
iter 0 loss: 0.546
Actual params: [0.5941, 0.5941]
-Original Grad: -0.208, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.512, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.562
Actual params: [0.4941, 0.4941]
-Original Grad: 0.059, -lr * Pred Grad:  -0.044, New P: 0.450
-Original Grad: -0.230, -lr * Pred Grad:  -0.092, New P: 0.402
iter 2 loss: 0.564
Actual params: [0.4497, 0.4024]
-Original Grad: 0.047, -lr * Pred Grad:  -0.020, New P: 0.430
-Original Grad: -0.107, -lr * Pred Grad:  -0.082, New P: 0.321
iter 3 loss: 0.560
Actual params: [0.4298, 0.3209]
-Original Grad: 0.093, -lr * Pred Grad:  0.008, New P: 0.437
-Original Grad: -0.087, -lr * Pred Grad:  -0.075, New P: 0.246
iter 4 loss: 0.546
Actual params: [0.4374, 0.2462]
-Original Grad: 0.076, -lr * Pred Grad:  0.023, New P: 0.460
-Original Grad: -0.060, -lr * Pred Grad:  -0.068, New P: 0.178
iter 5 loss: 0.528
Actual params: [0.4599, 0.1777]
-Original Grad: 0.154, -lr * Pred Grad:  0.044, New P: 0.504
-Original Grad: -0.151, -lr * Pred Grad:  -0.070, New P: 0.107
iter 6 loss: 0.509
Actual params: [0.5037, 0.1074]
-Original Grad: 0.101, -lr * Pred Grad:  0.053, New P: 0.556
-Original Grad: -0.091, -lr * Pred Grad:  -0.068, New P: 0.039
iter 7 loss: 0.488
Actual params: [0.5564, 0.0392]
-Original Grad: 0.072, -lr * Pred Grad:  0.056, New P: 0.613
-Original Grad: -0.121, -lr * Pred Grad:  -0.069, New P: -0.030
iter 8 loss: 0.477
Actual params: [ 0.6127, -0.0295]
-Original Grad: -0.040, -lr * Pred Grad:  0.044, New P: 0.656
-Original Grad: -0.134, -lr * Pred Grad:  -0.070, New P: -0.099
iter 9 loss: 0.467
Actual params: [ 0.6563, -0.0995]
-Original Grad: -0.040, -lr * Pred Grad:  0.033, New P: 0.689
-Original Grad: -0.070, -lr * Pred Grad:  -0.067, New P: -0.167
iter 10 loss: 0.463
Actual params: [ 0.6889, -0.1669]
-Original Grad: -0.001, -lr * Pred Grad:  0.029, New P: 0.718
-Original Grad: -0.092, -lr * Pred Grad:  -0.067, New P: -0.234
iter 11 loss: 0.460
Actual params: [ 0.7181, -0.2337]
-Original Grad: -0.034, -lr * Pred Grad:  0.021, New P: 0.739
-Original Grad: -0.033, -lr * Pred Grad:  -0.062, New P: -0.296
iter 12 loss: 0.458
Actual params: [ 0.7391, -0.2961]
-Original Grad: -0.030, -lr * Pred Grad:  0.014, New P: 0.753
-Original Grad: 0.005, -lr * Pred Grad:  -0.056, New P: -0.352
iter 13 loss: 0.457
Actual params: [ 0.7535, -0.352 ]
-Original Grad: -0.039, -lr * Pred Grad:  0.007, New P: 0.761
-Original Grad: -0.000, -lr * Pred Grad:  -0.051, New P: -0.403
iter 14 loss: 0.457
Actual params: [ 0.7606, -0.4025]
-Original Grad: -0.064, -lr * Pred Grad:  -0.003, New P: 0.758
-Original Grad: 0.050, -lr * Pred Grad:  -0.042, New P: -0.444
iter 15 loss: 0.457
Actual params: [ 0.7576, -0.4444]
-Original Grad: -0.051, -lr * Pred Grad:  -0.010, New P: 0.748
-Original Grad: 0.122, -lr * Pred Grad:  -0.028, New P: -0.472
iter 16 loss: 0.456
Actual params: [ 0.7475, -0.4725]
-Original Grad: -0.076, -lr * Pred Grad:  -0.020, New P: 0.728
-Original Grad: -0.012, -lr * Pred Grad:  -0.026, New P: -0.499
iter 17 loss: 0.456
Actual params: [ 0.7278, -0.4989]
-Original Grad: -0.036, -lr * Pred Grad:  -0.023, New P: 0.705
-Original Grad: -0.022, -lr * Pred Grad:  -0.026, New P: -0.525
iter 18 loss: 0.454
Actual params: [ 0.7047, -0.5246]
-Original Grad: -0.009, -lr * Pred Grad:  -0.022, New P: 0.683
-Original Grad: 0.091, -lr * Pred Grad:  -0.016, New P: -0.541
iter 19 loss: 0.453
Actual params: [ 0.6826, -0.5408]
-Original Grad: -0.005, -lr * Pred Grad:  -0.021, New P: 0.662
-Original Grad: 0.013, -lr * Pred Grad:  -0.014, New P: -0.554
iter 20 loss: 0.450
Actual params: [ 0.6617, -0.5545]
-Original Grad: 0.063, -lr * Pred Grad:  -0.010, New P: 0.652
-Original Grad: 0.140, -lr * Pred Grad:  -0.002, New P: -0.556
Target params: [1.1812, 0.2779]
iter 0 loss: 1.000
Actual params: [0.5941, 0.5941]
-Original Grad: 0.006, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.045, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.939
Actual params: [0.6941, 0.4941]
-Original Grad: -0.015, -lr * Pred Grad:  -0.042, New P: 0.652
-Original Grad: -0.038, -lr * Pred Grad:  -0.099, New P: 0.395
iter 2 loss: 0.925
Actual params: [0.6516, 0.3949]
-Original Grad: 0.119, -lr * Pred Grad:  0.059, New P: 0.710
-Original Grad: -0.166, -lr * Pred Grad:  -0.086, New P: 0.309
iter 3 loss: 0.862
Actual params: [0.7105, 0.3091]
-Original Grad: 0.060, -lr * Pred Grad:  0.069, New P: 0.780
-Original Grad: -0.251, -lr * Pred Grad:  -0.088, New P: 0.221
iter 4 loss: 0.817
Actual params: [0.7797, 0.2212]
-Original Grad: -0.026, -lr * Pred Grad:  0.047, New P: 0.827
-Original Grad: -0.131, -lr * Pred Grad:  -0.090, New P: 0.131
iter 5 loss: 0.792
Actual params: [0.827 , 0.1315]
-Original Grad: -0.035, -lr * Pred Grad:  0.027, New P: 0.854
-Original Grad: -0.030, -lr * Pred Grad:  -0.082, New P: 0.050
iter 6 loss: 0.780
Actual params: [0.8536, 0.0497]
-Original Grad: -0.040, -lr * Pred Grad:  0.008, New P: 0.862
-Original Grad: -0.023, -lr * Pred Grad:  -0.075, New P: -0.025
iter 7 loss: 0.769
Actual params: [ 0.8619, -0.025 ]
-Original Grad: -0.037, -lr * Pred Grad:  -0.005, New P: 0.857
-Original Grad: -0.072, -lr * Pred Grad:  -0.075, New P: -0.100
iter 8 loss: 0.751
Actual params: [ 0.857 , -0.0998]
-Original Grad: -0.123, -lr * Pred Grad:  -0.034, New P: 0.823
-Original Grad: -0.064, -lr * Pred Grad:  -0.074, New P: -0.174
iter 9 loss: 0.737
Actual params: [ 0.8228, -0.1741]
-Original Grad: -0.087, -lr * Pred Grad:  -0.048, New P: 0.775
-Original Grad: -0.103, -lr * Pred Grad:  -0.077, New P: -0.251
iter 10 loss: 0.725
Actual params: [ 0.7752, -0.2513]
-Original Grad: -0.058, -lr * Pred Grad:  -0.054, New P: 0.721
-Original Grad: -0.056, -lr * Pred Grad:  -0.076, New P: -0.327
iter 11 loss: 0.709
Actual params: [ 0.7213, -0.3272]
-Original Grad: -0.106, -lr * Pred Grad:  -0.065, New P: 0.657
-Original Grad: -0.060, -lr * Pred Grad:  -0.075, New P: -0.402
iter 12 loss: 0.691
Actual params: [ 0.6567, -0.4022]
-Original Grad: -0.146, -lr * Pred Grad:  -0.075, New P: 0.582
-Original Grad: 0.037, -lr * Pred Grad:  -0.062, New P: -0.465
iter 13 loss: 0.666
Actual params: [ 0.5821, -0.4646]
-Original Grad: 0.070, -lr * Pred Grad:  -0.054, New P: 0.528
-Original Grad: 0.013, -lr * Pred Grad:  -0.055, New P: -0.519
iter 14 loss: 0.676
Actual params: [ 0.5281, -0.5193]
-Original Grad: 0.126, -lr * Pred Grad:  -0.026, New P: 0.502
-Original Grad: 0.054, -lr * Pred Grad:  -0.042, New P: -0.561
iter 15 loss: 0.670
Actual params: [ 0.5024, -0.5614]
-Original Grad: 0.066, -lr * Pred Grad:  -0.013, New P: 0.489
-Original Grad: 0.178, -lr * Pred Grad:  -0.014, New P: -0.575
iter 16 loss: 0.665
Actual params: [ 0.4895, -0.575 ]
-Original Grad: 0.156, -lr * Pred Grad:  0.011, New P: 0.500
-Original Grad: -0.129, -lr * Pred Grad:  -0.026, New P: -0.601
iter 17 loss: 0.665
Actual params: [ 0.5002, -0.6013]
-Original Grad: 0.148, -lr * Pred Grad:  0.028, New P: 0.528
-Original Grad: 0.153, -lr * Pred Grad:  -0.006, New P: -0.607
iter 18 loss: 0.661
Actual params: [ 0.528 , -0.6074]
-Original Grad: -0.148, -lr * Pred Grad:  0.006, New P: 0.534
-Original Grad: 0.116, -lr * Pred Grad:  0.007, New P: -0.601
iter 19 loss: 0.660
Actual params: [ 0.5338, -0.6005]
-Original Grad: 0.196, -lr * Pred Grad:  0.026, New P: 0.560
-Original Grad: 0.044, -lr * Pred Grad:  0.011, New P: -0.590
iter 20 loss: 0.654
Actual params: [ 0.5601, -0.5896]
-Original Grad: -0.021, -lr * Pred Grad:  0.022, New P: 0.582
-Original Grad: -0.189, -lr * Pred Grad:  -0.010, New P: -0.599
Target params: [1.1812, 0.2779]
iter 0 loss: 0.749
Actual params: [0.5941, 0.5941]
-Original Grad: 0.128, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: 0.046, -lr * Pred Grad:  0.100, New P: 0.694
iter 1 loss: 0.752
Actual params: [0.6941, 0.6941]
-Original Grad: 0.085, -lr * Pred Grad:  0.097, New P: 0.791
-Original Grad: -0.011, -lr * Pred Grad:  0.047, New P: 0.741
iter 2 loss: 0.732
Actual params: [0.7911, 0.7415]
-Original Grad: 0.100, -lr * Pred Grad:  0.098, New P: 0.889
-Original Grad: 0.443, -lr * Pred Grad:  0.067, New P: 0.809
iter 3 loss: 0.740
Actual params: [0.8888, 0.8089]
-Original Grad: 0.237, -lr * Pred Grad:  0.095, New P: 0.984
-Original Grad: 0.094, -lr * Pred Grad:  0.066, New P: 0.875
iter 4 loss: 0.766
Actual params: [0.9837, 0.875 ]
-Original Grad: 0.084, -lr * Pred Grad:  0.092, New P: 1.076
-Original Grad: -0.034, -lr * Pred Grad:  0.052, New P: 0.927
iter 5 loss: 0.801
Actual params: [1.0757, 0.9266]
-Original Grad: -0.005, -lr * Pred Grad:  0.079, New P: 1.154
-Original Grad: -0.104, -lr * Pred Grad:  0.032, New P: 0.958
iter 6 loss: 0.807
Actual params: [1.1542, 0.9583]
-Original Grad: -0.062, -lr * Pred Grad:  0.057, New P: 1.212
-Original Grad: -0.016, -lr * Pred Grad:  0.026, New P: 0.984
iter 7 loss: 0.801
Actual params: [1.2115, 0.9842]
-Original Grad: 0.056, -lr * Pred Grad:  0.058, New P: 1.270
-Original Grad: -0.042, -lr * Pred Grad:  0.018, New P: 1.003
iter 8 loss: 0.792
Actual params: [1.27  , 1.0026]
-Original Grad: 0.043, -lr * Pred Grad:  0.058, New P: 1.328
-Original Grad: -0.048, -lr * Pred Grad:  0.011, New P: 1.014
iter 9 loss: 0.781
Actual params: [1.3279, 1.0139]
-Original Grad: 0.043, -lr * Pred Grad:  0.058, New P: 1.386
-Original Grad: -0.024, -lr * Pred Grad:  0.008, New P: 1.022
iter 10 loss: 0.771
Actual params: [1.3855, 1.0216]
-Original Grad: 0.056, -lr * Pred Grad:  0.059, New P: 1.444
-Original Grad: 0.008, -lr * Pred Grad:  0.008, New P: 1.029
iter 11 loss: 0.765
Actual params: [1.4445, 1.0292]
-Original Grad: 0.071, -lr * Pred Grad:  0.062, New P: 1.506
-Original Grad: -0.058, -lr * Pred Grad:  0.001, New P: 1.030
iter 12 loss: 0.762
Actual params: [1.5064, 1.0301]
-Original Grad: 0.097, -lr * Pred Grad:  0.067, New P: 1.573
-Original Grad: -0.057, -lr * Pred Grad:  -0.005, New P: 1.025
iter 13 loss: 0.758
Actual params: [1.5733, 1.0252]
-Original Grad: 0.022, -lr * Pred Grad:  0.063, New P: 1.637
-Original Grad: 0.076, -lr * Pred Grad:  0.003, New P: 1.028
iter 14 loss: 0.759
Actual params: [1.6367, 1.0283]
-Original Grad: 0.034, -lr * Pred Grad:  0.062, New P: 1.698
-Original Grad: 0.061, -lr * Pred Grad:  0.009, New P: 1.037
iter 15 loss: 0.759
Actual params: [1.6984, 1.0372]
-Original Grad: 0.010, -lr * Pred Grad:  0.057, New P: 1.756
-Original Grad: -0.024, -lr * Pred Grad:  0.006, New P: 1.043
iter 16 loss: 0.759
Actual params: [1.7556, 1.0428]
-Original Grad: 0.030, -lr * Pred Grad:  0.056, New P: 1.812
-Original Grad: -0.014, -lr * Pred Grad:  0.004, New P: 1.046
iter 17 loss: 0.759
Actual params: [1.8116, 1.0465]
-Original Grad: -0.003, -lr * Pred Grad:  0.050, New P: 1.862
-Original Grad: 0.058, -lr * Pred Grad:  0.009, New P: 1.056
iter 18 loss: 0.759
Actual params: [1.8619, 1.0556]
-Original Grad: 0.023, -lr * Pred Grad:  0.049, New P: 1.911
-Original Grad: -0.005, -lr * Pred Grad:  0.008, New P: 1.063
iter 19 loss: 0.761
Actual params: [1.9109, 1.0634]
-Original Grad: 0.020, -lr * Pred Grad:  0.047, New P: 1.958
-Original Grad: 0.034, -lr * Pred Grad:  0.011, New P: 1.074
iter 20 loss: 0.761
Actual params: [1.958 , 1.0741]
-Original Grad: -0.020, -lr * Pred Grad:  0.040, New P: 1.998
-Original Grad: 0.050, -lr * Pred Grad:  0.015, New P: 1.089
Target params: [1.1812, 0.2779]
iter 0 loss: 1.042
Actual params: [0.5941, 0.5941]
-Original Grad: -0.165, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.345, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.979
Actual params: [0.4941, 0.4941]
-Original Grad: -0.059, -lr * Pred Grad:  -0.088, New P: 0.406
-Original Grad: -0.140, -lr * Pred Grad:  -0.090, New P: 0.404
iter 2 loss: 0.962
Actual params: [0.4059, 0.404 ]
-Original Grad: 0.018, -lr * Pred Grad:  -0.061, New P: 0.345
-Original Grad: -0.040, -lr * Pred Grad:  -0.076, New P: 0.328
iter 3 loss: 0.957
Actual params: [0.3447, 0.3279]
-Original Grad: 0.088, -lr * Pred Grad:  -0.019, New P: 0.326
-Original Grad: -0.043, -lr * Pred Grad:  -0.069, New P: 0.259
iter 4 loss: 0.952
Actual params: [0.3259, 0.2594]
-Original Grad: -0.084, -lr * Pred Grad:  -0.036, New P: 0.290
-Original Grad: -0.093, -lr * Pred Grad:  -0.069, New P: 0.190
iter 5 loss: 0.948
Actual params: [0.2898, 0.19  ]
-Original Grad: -0.029, -lr * Pred Grad:  -0.038, New P: 0.252
-Original Grad: -0.037, -lr * Pred Grad:  -0.065, New P: 0.125
iter 6 loss: 0.944
Actual params: [0.252 , 0.1255]
-Original Grad: -0.035, -lr * Pred Grad:  -0.041, New P: 0.211
-Original Grad: -0.088, -lr * Pred Grad:  -0.066, New P: 0.059
iter 7 loss: 0.940
Actual params: [0.2113, 0.0594]
-Original Grad: 0.012, -lr * Pred Grad:  -0.033, New P: 0.178
-Original Grad: -0.064, -lr * Pred Grad:  -0.065, New P: -0.006
iter 8 loss: 0.937
Actual params: [ 0.1781, -0.006 ]
-Original Grad: -0.005, -lr * Pred Grad:  -0.031, New P: 0.148
-Original Grad: -0.003, -lr * Pred Grad:  -0.058, New P: -0.064
iter 9 loss: 0.936
Actual params: [ 0.1475, -0.0645]
-Original Grad: -0.019, -lr * Pred Grad:  -0.031, New P: 0.116
-Original Grad: 0.032, -lr * Pred Grad:  -0.048, New P: -0.113
iter 10 loss: 0.936
Actual params: [ 0.1161, -0.1126]
-Original Grad: 0.005, -lr * Pred Grad:  -0.027, New P: 0.089
-Original Grad: -0.009, -lr * Pred Grad:  -0.044, New P: -0.157
iter 11 loss: 0.936
Actual params: [ 0.0892, -0.1568]
-Original Grad: -0.012, -lr * Pred Grad:  -0.027, New P: 0.062
-Original Grad: 0.014, -lr * Pred Grad:  -0.038, New P: -0.195
iter 12 loss: 0.936
Actual params: [ 0.0622, -0.1948]
-Original Grad: -0.004, -lr * Pred Grad:  -0.025, New P: 0.037
-Original Grad: 0.007, -lr * Pred Grad:  -0.033, New P: -0.228
iter 13 loss: 0.935
Actual params: [ 0.037 , -0.2282]
-Original Grad: -0.008, -lr * Pred Grad:  -0.024, New P: 0.013
-Original Grad: 0.028, -lr * Pred Grad:  -0.027, New P: -0.255
iter 14 loss: 0.934
Actual params: [ 0.0126, -0.2551]
-Original Grad: -0.004, -lr * Pred Grad:  -0.023, New P: -0.011
-Original Grad: -0.015, -lr * Pred Grad:  -0.026, New P: -0.281
iter 15 loss: 0.933
Actual params: [-0.0106, -0.2811]
-Original Grad: -0.001, -lr * Pred Grad:  -0.021, New P: -0.032
-Original Grad: 0.026, -lr * Pred Grad:  -0.020, New P: -0.302
iter 16 loss: 0.933
Actual params: [-0.0318, -0.3016]
-Original Grad: -0.002, -lr * Pred Grad:  -0.020, New P: -0.051
-Original Grad: 0.022, -lr * Pred Grad:  -0.016, New P: -0.317
iter 17 loss: 0.932
Actual params: [-0.0514, -0.3174]
-Original Grad: 0.013, -lr * Pred Grad:  -0.015, New P: -0.066
-Original Grad: 0.011, -lr * Pred Grad:  -0.013, New P: -0.330
iter 18 loss: 0.932
Actual params: [-0.0662, -0.3304]
-Original Grad: -0.004, -lr * Pred Grad:  -0.014, New P: -0.080
-Original Grad: 0.048, -lr * Pred Grad:  -0.006, New P: -0.336
iter 19 loss: 0.932
Actual params: [-0.0805, -0.3363]
-Original Grad: 0.007, -lr * Pred Grad:  -0.011, New P: -0.092
-Original Grad: 0.029, -lr * Pred Grad:  -0.002, New P: -0.338
iter 20 loss: 0.932
Actual params: [-0.0918, -0.3381]
-Original Grad: -0.001, -lr * Pred Grad:  -0.010, New P: -0.102
-Original Grad: -0.029, -lr * Pred Grad:  -0.005, New P: -0.343
Target params: [1.1812, 0.2779]
iter 0 loss: 0.434
Actual params: [0.5941, 0.5941]
-Original Grad: 0.259, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.204, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.390
Actual params: [0.6941, 0.4941]
-Original Grad: -0.026, -lr * Pred Grad:  0.059, New P: 0.753
-Original Grad: -0.091, -lr * Pred Grad:  -0.091, New P: 0.403
iter 2 loss: 0.379
Actual params: [0.7532, 0.4026]
-Original Grad: 0.042, -lr * Pred Grad:  0.055, New P: 0.809
-Original Grad: -0.064, -lr * Pred Grad:  -0.086, New P: 0.317
iter 3 loss: 0.374
Actual params: [0.8086, 0.317 ]
-Original Grad: 0.161, -lr * Pred Grad:  0.069, New P: 0.878
-Original Grad: -0.168, -lr * Pred Grad:  -0.091, New P: 0.226
iter 4 loss: 0.373
Actual params: [0.8776, 0.2261]
-Original Grad: 0.023, -lr * Pred Grad:  0.062, New P: 0.940
-Original Grad: -0.166, -lr * Pred Grad:  -0.094, New P: 0.132
iter 5 loss: 0.372
Actual params: [0.9398, 0.1323]
-Original Grad: -0.068, -lr * Pred Grad:  0.041, New P: 0.981
-Original Grad: -0.064, -lr * Pred Grad:  -0.089, New P: 0.043
iter 6 loss: 0.370
Actual params: [0.981, 0.043]
-Original Grad: 0.033, -lr * Pred Grad:  0.041, New P: 1.022
-Original Grad: -0.048, -lr * Pred Grad:  -0.084, New P: -0.041
iter 7 loss: 0.369
Actual params: [ 1.022 , -0.0414]
-Original Grad: 0.022, -lr * Pred Grad:  0.040, New P: 1.062
-Original Grad: -0.047, -lr * Pred Grad:  -0.081, New P: -0.122
iter 8 loss: 0.368
Actual params: [ 1.0615, -0.122 ]
-Original Grad: 0.048, -lr * Pred Grad:  0.042, New P: 1.104
-Original Grad: -0.069, -lr * Pred Grad:  -0.080, New P: -0.202
iter 9 loss: 0.369
Actual params: [ 1.1035, -0.2017]
-Original Grad: 0.021, -lr * Pred Grad:  0.041, New P: 1.144
-Original Grad: -0.062, -lr * Pred Grad:  -0.079, New P: -0.280
iter 10 loss: 0.367
Actual params: [ 1.1441, -0.2804]
-Original Grad: 0.021, -lr * Pred Grad:  0.039, New P: 1.183
-Original Grad: -0.100, -lr * Pred Grad:  -0.081, New P: -0.361
iter 11 loss: 0.365
Actual params: [ 1.1835, -0.3612]
-Original Grad: -0.044, -lr * Pred Grad:  0.029, New P: 1.212
-Original Grad: -0.070, -lr * Pred Grad:  -0.080, New P: -0.442
iter 12 loss: 0.361
Actual params: [ 1.2121, -0.4417]
-Original Grad: -0.040, -lr * Pred Grad:  0.020, New P: 1.232
-Original Grad: -0.091, -lr * Pred Grad:  -0.082, New P: -0.523
iter 13 loss: 0.356
Actual params: [ 1.2318, -0.5235]
-Original Grad: -0.039, -lr * Pred Grad:  0.012, New P: 1.244
-Original Grad: -0.024, -lr * Pred Grad:  -0.077, New P: -0.600
iter 14 loss: 0.354
Actual params: [ 1.2438, -0.6003]
-Original Grad: -0.027, -lr * Pred Grad:  0.007, New P: 1.251
-Original Grad: -0.104, -lr * Pred Grad:  -0.080, New P: -0.680
iter 15 loss: 0.352
Actual params: [ 1.2506, -0.68  ]
-Original Grad: 0.002, -lr * Pred Grad:  0.006, New P: 1.257
-Original Grad: -0.078, -lr * Pred Grad:  -0.080, New P: -0.760
iter 16 loss: 0.340
Actual params: [ 1.2571, -0.7603]
-Original Grad: -0.011, -lr * Pred Grad:  0.004, New P: 1.261
-Original Grad: 0.009, -lr * Pred Grad:  -0.072, New P: -0.832
iter 17 loss: 0.312
Actual params: [ 1.2613, -0.8321]
-Original Grad: 0.049, -lr * Pred Grad:  0.011, New P: 1.272
-Original Grad: 0.405, -lr * Pred Grad:  -0.011, New P: -0.843
iter 18 loss: 0.312
Actual params: [ 1.2724, -0.8432]
-Original Grad: 0.038, -lr * Pred Grad:  0.016, New P: 1.288
-Original Grad: 0.137, -lr * Pred Grad:  0.002, New P: -0.841
iter 19 loss: 0.311
Actual params: [ 1.2882, -0.8414]
-Original Grad: 0.007, -lr * Pred Grad:  0.015, New P: 1.303
-Original Grad: -0.112, -lr * Pred Grad:  -0.008, New P: -0.849
iter 20 loss: 0.313
Actual params: [ 1.3035, -0.8493]
-Original Grad: 0.041, -lr * Pred Grad:  0.020, New P: 1.324
-Original Grad: 0.375, -lr * Pred Grad:  0.021, New P: -0.828
Target params: [1.1812, 0.2779]
iter 0 loss: 0.406
Actual params: [0.5941, 0.5941]
-Original Grad: 0.223, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.160, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.365
Actual params: [0.6941, 0.4941]
-Original Grad: -0.083, -lr * Pred Grad:  0.037, New P: 0.731
-Original Grad: 0.004, -lr * Pred Grad:  -0.065, New P: 0.429
iter 2 loss: 0.362
Actual params: [0.7309, 0.4289]
-Original Grad: -0.193, -lr * Pred Grad:  -0.018, New P: 0.713
-Original Grad: -0.050, -lr * Pred Grad:  -0.067, New P: 0.362
iter 3 loss: 0.362
Actual params: [0.7128, 0.3618]
-Original Grad: -0.119, -lr * Pred Grad:  -0.035, New P: 0.678
-Original Grad: -0.060, -lr * Pred Grad:  -0.071, New P: 0.290
iter 4 loss: 0.364
Actual params: [0.678 , 0.2905]
-Original Grad: 0.049, -lr * Pred Grad:  -0.021, New P: 0.657
-Original Grad: -0.045, -lr * Pred Grad:  -0.072, New P: 0.219
iter 5 loss: 0.366
Actual params: [0.657 , 0.2187]
-Original Grad: 0.109, -lr * Pred Grad:  -0.001, New P: 0.656
-Original Grad: -0.076, -lr * Pred Grad:  -0.077, New P: 0.142
iter 6 loss: 0.365
Actual params: [0.656 , 0.1416]
-Original Grad: -0.120, -lr * Pred Grad:  -0.017, New P: 0.639
-Original Grad: -0.013, -lr * Pred Grad:  -0.071, New P: 0.071
iter 7 loss: 0.364
Actual params: [0.6387, 0.071 ]
-Original Grad: -0.034, -lr * Pred Grad:  -0.020, New P: 0.619
-Original Grad: -0.018, -lr * Pred Grad:  -0.066, New P: 0.005
iter 8 loss: 0.363
Actual params: [0.619 , 0.0047]
-Original Grad: 0.086, -lr * Pred Grad:  -0.006, New P: 0.613
-Original Grad: -0.057, -lr * Pred Grad:  -0.070, New P: -0.065
iter 9 loss: 0.363
Actual params: [ 0.613 , -0.0654]
-Original Grad: 0.069, -lr * Pred Grad:  0.003, New P: 0.616
-Original Grad: -0.046, -lr * Pred Grad:  -0.072, New P: -0.137
iter 10 loss: 0.363
Actual params: [ 0.6164, -0.137 ]
-Original Grad: -0.090, -lr * Pred Grad:  -0.008, New P: 0.608
-Original Grad: -0.003, -lr * Pred Grad:  -0.065, New P: -0.202
iter 11 loss: 0.363
Actual params: [ 0.6084, -0.2018]
-Original Grad: 0.049, -lr * Pred Grad:  -0.001, New P: 0.607
-Original Grad: -0.072, -lr * Pred Grad:  -0.071, New P: -0.272
iter 12 loss: 0.364
Actual params: [ 0.6071, -0.2725]
-Original Grad: 0.044, -lr * Pred Grad:  0.004, New P: 0.611
-Original Grad: -0.047, -lr * Pred Grad:  -0.072, New P: -0.345
iter 13 loss: 0.365
Actual params: [ 0.6113, -0.3446]
-Original Grad: 0.015, -lr * Pred Grad:  0.006, New P: 0.617
-Original Grad: -0.024, -lr * Pred Grad:  -0.070, New P: -0.415
iter 14 loss: 0.367
Actual params: [ 0.6169, -0.4146]
-Original Grad: -0.069, -lr * Pred Grad:  -0.003, New P: 0.614
-Original Grad: -0.016, -lr * Pred Grad:  -0.067, New P: -0.481
iter 15 loss: 0.369
Actual params: [ 0.6137, -0.4811]
-Original Grad: 0.150, -lr * Pred Grad:  0.014, New P: 0.628
-Original Grad: -0.046, -lr * Pred Grad:  -0.069, New P: -0.550
iter 16 loss: 0.371
Actual params: [ 0.6278, -0.5498]
-Original Grad: -0.130, -lr * Pred Grad:  -0.002, New P: 0.626
-Original Grad: -0.005, -lr * Pred Grad:  -0.063, New P: -0.613
iter 17 loss: 0.372
Actual params: [ 0.6261, -0.6131]
-Original Grad: -0.098, -lr * Pred Grad:  -0.012, New P: 0.614
-Original Grad: 0.008, -lr * Pred Grad:  -0.056, New P: -0.669
iter 18 loss: 0.373
Actual params: [ 0.6139, -0.6689]
-Original Grad: -0.162, -lr * Pred Grad:  -0.027, New P: 0.587
-Original Grad: 0.063, -lr * Pred Grad:  -0.036, New P: -0.705
iter 19 loss: 0.374
Actual params: [ 0.5869, -0.7048]
-Original Grad: 0.155, -lr * Pred Grad:  -0.008, New P: 0.579
-Original Grad: -0.113, -lr * Pred Grad:  -0.051, New P: -0.756
iter 20 loss: 0.374
Actual params: [ 0.5788, -0.7559]
-Original Grad: 0.102, -lr * Pred Grad:  0.003, New P: 0.582
-Original Grad: -0.154, -lr * Pred Grad:  -0.066, New P: -0.822
Target params: [1.1812, 0.2779]
iter 0 loss: 0.788
Actual params: [0.5941, 0.5941]
-Original Grad: -0.017, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.126, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.825
Actual params: [0.4941, 0.4941]
-Original Grad: -0.115, -lr * Pred Grad:  -0.084, New P: 0.410
-Original Grad: 0.046, -lr * Pred Grad:  -0.037, New P: 0.457
iter 2 loss: 0.848
Actual params: [0.4104, 0.4566]
-Original Grad: -0.061, -lr * Pred Grad:  -0.087, New P: 0.323
-Original Grad: -0.148, -lr * Pred Grad:  -0.067, New P: 0.390
iter 3 loss: 0.843
Actual params: [0.3235, 0.3898]
-Original Grad: 0.053, -lr * Pred Grad:  -0.044, New P: 0.279
-Original Grad: -0.177, -lr * Pred Grad:  -0.079, New P: 0.310
iter 4 loss: 0.818
Actual params: [0.279 , 0.3103]
-Original Grad: -0.021, -lr * Pred Grad:  -0.045, New P: 0.234
-Original Grad: -0.102, -lr * Pred Grad:  -0.082, New P: 0.228
iter 5 loss: 0.800
Actual params: [0.2339, 0.228 ]
-Original Grad: 0.021, -lr * Pred Grad:  -0.031, New P: 0.203
-Original Grad: -0.075, -lr * Pred Grad:  -0.082, New P: 0.146
iter 6 loss: 0.790
Actual params: [0.203 , 0.1461]
-Original Grad: 0.023, -lr * Pred Grad:  -0.019, New P: 0.184
-Original Grad: -0.090, -lr * Pred Grad:  -0.083, New P: 0.063
iter 7 loss: 0.788
Actual params: [0.1844, 0.063 ]
-Original Grad: 0.042, -lr * Pred Grad:  -0.002, New P: 0.182
-Original Grad: -0.064, -lr * Pred Grad:  -0.082, New P: -0.019
iter 8 loss: 0.788
Actual params: [ 0.1824, -0.0188]
-Original Grad: 0.191, -lr * Pred Grad:  0.037, New P: 0.220
-Original Grad: 0.012, -lr * Pred Grad:  -0.071, New P: -0.090
iter 9 loss: 0.784
Actual params: [ 0.2196, -0.0897]
-Original Grad: 0.168, -lr * Pred Grad:  0.055, New P: 0.274
-Original Grad: -0.002, -lr * Pred Grad:  -0.063, New P: -0.153
iter 10 loss: 0.780
Actual params: [ 0.2743, -0.1532]
-Original Grad: 0.013, -lr * Pred Grad:  0.051, New P: 0.325
-Original Grad: -0.005, -lr * Pred Grad:  -0.058, New P: -0.211
iter 11 loss: 0.771
Actual params: [ 0.3255, -0.2108]
-Original Grad: -0.005, -lr * Pred Grad:  0.045, New P: 0.371
-Original Grad: -0.012, -lr * Pred Grad:  -0.054, New P: -0.264
iter 12 loss: 0.759
Actual params: [ 0.3707, -0.2645]
-Original Grad: -0.080, -lr * Pred Grad:  0.027, New P: 0.397
-Original Grad: -0.077, -lr * Pred Grad:  -0.058, New P: -0.323
iter 13 loss: 0.752
Actual params: [ 0.3973, -0.3229]
-Original Grad: -0.033, -lr * Pred Grad:  0.019, New P: 0.416
-Original Grad: -0.044, -lr * Pred Grad:  -0.059, New P: -0.382
iter 14 loss: 0.745
Actual params: [ 0.4161, -0.3818]
-Original Grad: 0.036, -lr * Pred Grad:  0.023, New P: 0.439
-Original Grad: 0.031, -lr * Pred Grad:  -0.048, New P: -0.430
iter 15 loss: 0.734
Actual params: [ 0.4387, -0.4301]
-Original Grad: 0.167, -lr * Pred Grad:  0.041, New P: 0.480
-Original Grad: -0.037, -lr * Pred Grad:  -0.049, New P: -0.479
iter 16 loss: 0.704
Actual params: [ 0.4799, -0.4792]
-Original Grad: -0.074, -lr * Pred Grad:  0.026, New P: 0.506
-Original Grad: 0.020, -lr * Pred Grad:  -0.041, New P: -0.521
iter 17 loss: 0.676
Actual params: [ 0.5063, -0.5206]
-Original Grad: 0.204, -lr * Pred Grad:  0.045, New P: 0.552
-Original Grad: 0.075, -lr * Pred Grad:  -0.026, New P: -0.546
iter 18 loss: 0.610
Actual params: [ 0.5517, -0.5463]
-Original Grad: 0.284, -lr * Pred Grad:  0.062, New P: 0.614
-Original Grad: 0.037, -lr * Pred Grad:  -0.018, New P: -0.564
iter 19 loss: 0.519
Actual params: [ 0.6141, -0.5641]
-Original Grad: 0.043, -lr * Pred Grad:  0.061, New P: 0.675
-Original Grad: -0.041, -lr * Pred Grad:  -0.022, New P: -0.586
iter 20 loss: 0.440
Actual params: [ 0.675 , -0.5862]
-Original Grad: -0.034, -lr * Pred Grad:  0.052, New P: 0.727
-Original Grad: -0.005, -lr * Pred Grad:  -0.021, New P: -0.607
Target params: [1.1812, 0.2779]
iter 0 loss: 0.819
Actual params: [0.5941, 0.5941]
-Original Grad: 0.023, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.103, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.713
Actual params: [0.6941, 0.4941]
-Original Grad: 0.052, -lr * Pred Grad:  0.095, New P: 0.789
-Original Grad: -0.358, -lr * Pred Grad:  -0.090, New P: 0.404
iter 2 loss: 0.580
Actual params: [0.7892, 0.404 ]
-Original Grad: -0.003, -lr * Pred Grad:  0.070, New P: 0.859
-Original Grad: 0.000, -lr * Pred Grad:  -0.070, New P: 0.334
iter 3 loss: 0.498
Actual params: [0.8593, 0.3344]
-Original Grad: -0.056, -lr * Pred Grad:  -0.000, New P: 0.859
-Original Grad: -0.199, -lr * Pred Grad:  -0.078, New P: 0.257
iter 4 loss: 0.465
Actual params: [0.859 , 0.2568]
-Original Grad: 0.015, -lr * Pred Grad:  0.010, New P: 0.869
-Original Grad: -0.131, -lr * Pred Grad:  -0.079, New P: 0.178
iter 5 loss: 0.431
Actual params: [0.8688, 0.1779]
-Original Grad: -0.036, -lr * Pred Grad:  -0.013, New P: 0.855
-Original Grad: -0.054, -lr * Pred Grad:  -0.074, New P: 0.104
iter 6 loss: 0.411
Actual params: [0.8553, 0.1041]
-Original Grad: 0.138, -lr * Pred Grad:  0.036, New P: 0.892
-Original Grad: -0.087, -lr * Pred Grad:  -0.073, New P: 0.031
iter 7 loss: 0.382
Actual params: [0.8916, 0.0311]
-Original Grad: -0.001, -lr * Pred Grad:  0.032, New P: 0.923
-Original Grad: -0.037, -lr * Pred Grad:  -0.068, New P: -0.037
iter 8 loss: 0.357
Actual params: [ 0.9233, -0.0371]
-Original Grad: 0.026, -lr * Pred Grad:  0.036, New P: 0.959
-Original Grad: -0.094, -lr * Pred Grad:  -0.069, New P: -0.106
iter 9 loss: 0.335
Actual params: [ 0.9588, -0.1062]
-Original Grad: -0.009, -lr * Pred Grad:  0.029, New P: 0.988
-Original Grad: -0.061, -lr * Pred Grad:  -0.067, New P: -0.174
iter 10 loss: 0.316
Actual params: [ 0.9877, -0.1737]
-Original Grad: -0.001, -lr * Pred Grad:  0.026, New P: 1.013
-Original Grad: -0.085, -lr * Pred Grad:  -0.068, New P: -0.242
iter 11 loss: 0.307
Actual params: [ 1.0134, -0.2418]
-Original Grad: 0.012, -lr * Pred Grad:  0.027, New P: 1.040
-Original Grad: 0.002, -lr * Pred Grad:  -0.061, New P: -0.303
iter 12 loss: 0.296
Actual params: [ 1.04  , -0.3029]
-Original Grad: -0.000, -lr * Pred Grad:  0.024, New P: 1.064
-Original Grad: -0.022, -lr * Pred Grad:  -0.057, New P: -0.360
iter 13 loss: 0.289
Actual params: [ 1.0638, -0.3601]
-Original Grad: -0.003, -lr * Pred Grad:  0.021, New P: 1.085
-Original Grad: 0.013, -lr * Pred Grad:  -0.050, New P: -0.411
iter 14 loss: 0.286
Actual params: [ 1.0845, -0.4105]
-Original Grad: 0.008, -lr * Pred Grad:  0.021, New P: 1.106
-Original Grad: -0.016, -lr * Pred Grad:  -0.047, New P: -0.458
iter 15 loss: 0.280
Actual params: [ 1.1056, -0.4577]
-Original Grad: -0.004, -lr * Pred Grad:  0.018, New P: 1.124
-Original Grad: -0.013, -lr * Pred Grad:  -0.044, New P: -0.502
iter 16 loss: 0.275
Actual params: [ 1.1235, -0.5019]
-Original Grad: 0.027, -lr * Pred Grad:  0.024, New P: 1.147
-Original Grad: 0.012, -lr * Pred Grad:  -0.039, New P: -0.541
iter 17 loss: 0.271
Actual params: [ 1.1475, -0.5407]
-Original Grad: -0.002, -lr * Pred Grad:  0.021, New P: 1.169
-Original Grad: -0.005, -lr * Pred Grad:  -0.036, New P: -0.576
iter 18 loss: 0.268
Actual params: [ 1.1685, -0.5764]
-Original Grad: 0.028, -lr * Pred Grad:  0.027, New P: 1.196
-Original Grad: 0.016, -lr * Pred Grad:  -0.031, New P: -0.607
iter 19 loss: 0.269
Actual params: [ 1.1957, -0.6072]
-Original Grad: 0.015, -lr * Pred Grad:  0.029, New P: 1.225
-Original Grad: -0.041, -lr * Pred Grad:  -0.032, New P: -0.639
iter 20 loss: 0.263
Actual params: [ 1.225 , -0.6395]
-Original Grad: 0.009, -lr * Pred Grad:  0.029, New P: 1.254
-Original Grad: -0.026, -lr * Pred Grad:  -0.032, New P: -0.672
Target params: [1.1812, 0.2779]
iter 0 loss: 0.590
Actual params: [0.5941, 0.5941]
-Original Grad: -0.079, -lr * Pred Grad:  -0.100, New P: 0.494
-Original Grad: -0.064, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.587
Actual params: [0.4941, 0.4941]
-Original Grad: -0.044, -lr * Pred Grad:  -0.095, New P: 0.399
-Original Grad: -0.012, -lr * Pred Grad:  -0.079, New P: 0.415
iter 2 loss: 0.568
Actual params: [0.3993, 0.4147]
-Original Grad: 0.007, -lr * Pred Grad:  -0.068, New P: 0.331
-Original Grad: -0.084, -lr * Pred Grad:  -0.088, New P: 0.327
iter 3 loss: 0.544
Actual params: [0.331 , 0.3267]
-Original Grad: 0.124, -lr * Pred Grad:  0.014, New P: 0.345
-Original Grad: 0.052, -lr * Pred Grad:  -0.039, New P: 0.288
iter 4 loss: 0.530
Actual params: [0.3451, 0.2876]
-Original Grad: 0.099, -lr * Pred Grad:  0.040, New P: 0.385
-Original Grad: 0.007, -lr * Pred Grad:  -0.030, New P: 0.258
iter 5 loss: 0.518
Actual params: [0.3846, 0.2577]
-Original Grad: 0.018, -lr * Pred Grad:  0.039, New P: 0.424
-Original Grad: 0.056, -lr * Pred Grad:  -0.001, New P: 0.256
iter 6 loss: 0.514
Actual params: [0.4237, 0.2565]
-Original Grad: -0.077, -lr * Pred Grad:  0.012, New P: 0.435
-Original Grad: 0.133, -lr * Pred Grad:  0.035, New P: 0.292
iter 7 loss: 0.524
Actual params: [0.4355, 0.2919]
-Original Grad: 0.070, -lr * Pred Grad:  0.026, New P: 0.462
-Original Grad: -0.153, -lr * Pred Grad:  -0.007, New P: 0.285
iter 8 loss: 0.519
Actual params: [0.4617, 0.2846]
-Original Grad: 0.006, -lr * Pred Grad:  0.025, New P: 0.486
-Original Grad: 0.145, -lr * Pred Grad:  0.020, New P: 0.304
iter 9 loss: 0.522
Actual params: [0.4864, 0.3043]
-Original Grad: -0.042, -lr * Pred Grad:  0.012, New P: 0.498
-Original Grad: -0.046, -lr * Pred Grad:  0.009, New P: 0.314
iter 10 loss: 0.524
Actual params: [0.4985, 0.3137]
-Original Grad: -0.106, -lr * Pred Grad:  -0.012, New P: 0.487
-Original Grad: -0.064, -lr * Pred Grad:  -0.002, New P: 0.311
iter 11 loss: 0.525
Actual params: [0.4869, 0.3115]
-Original Grad: -0.081, -lr * Pred Grad:  -0.025, New P: 0.462
-Original Grad: -0.026, -lr * Pred Grad:  -0.006, New P: 0.305
iter 12 loss: 0.526
Actual params: [0.4615, 0.3052]
-Original Grad: 0.063, -lr * Pred Grad:  -0.010, New P: 0.451
-Original Grad: -0.046, -lr * Pred Grad:  -0.013, New P: 0.292
iter 13 loss: 0.523
Actual params: [0.4512, 0.2922]
-Original Grad: 0.147, -lr * Pred Grad:  0.016, New P: 0.467
-Original Grad: -0.116, -lr * Pred Grad:  -0.029, New P: 0.264
iter 14 loss: 0.511
Actual params: [0.4668, 0.2636]
-Original Grad: -0.140, -lr * Pred Grad:  -0.008, New P: 0.459
-Original Grad: -0.069, -lr * Pred Grad:  -0.036, New P: 0.228
iter 15 loss: 0.499
Actual params: [0.4591, 0.228 ]
-Original Grad: -0.033, -lr * Pred Grad:  -0.012, New P: 0.447
-Original Grad: -0.127, -lr * Pred Grad:  -0.048, New P: 0.180
iter 16 loss: 0.485
Actual params: [0.4471, 0.1802]
-Original Grad: -0.014, -lr * Pred Grad:  -0.013, New P: 0.434
-Original Grad: 0.118, -lr * Pred Grad:  -0.025, New P: 0.155
iter 17 loss: 0.479
Actual params: [0.4342, 0.1549]
-Original Grad: 0.055, -lr * Pred Grad:  -0.003, New P: 0.431
-Original Grad: -0.022, -lr * Pred Grad:  -0.026, New P: 0.129
iter 18 loss: 0.471
Actual params: [0.4308, 0.129 ]
-Original Grad: 0.090, -lr * Pred Grad:  0.010, New P: 0.441
-Original Grad: -0.057, -lr * Pred Grad:  -0.031, New P: 0.098
iter 19 loss: 0.460
Actual params: [0.4409, 0.0981]
-Original Grad: 0.023, -lr * Pred Grad:  0.012, New P: 0.453
-Original Grad: 0.101, -lr * Pred Grad:  -0.014, New P: 0.084
iter 20 loss: 0.454
Actual params: [0.4534, 0.0842]
-Original Grad: 0.010, -lr * Pred Grad:  0.013, New P: 0.466
-Original Grad: 0.046, -lr * Pred Grad:  -0.006, New P: 0.078
Target params: [1.1812, 0.2779]
iter 0 loss: 1.198
Actual params: [0.5941, 0.5941]
-Original Grad: 0.061, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.187, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 1.093
Actual params: [0.6941, 0.4941]
-Original Grad: 0.187, -lr * Pred Grad:  0.091, New P: 0.786
-Original Grad: 0.012, -lr * Pred Grad:  -0.062, New P: 0.432
iter 2 loss: 1.004
Actual params: [0.7855, 0.432 ]
-Original Grad: 0.184, -lr * Pred Grad:  0.095, New P: 0.881
-Original Grad: -0.064, -lr * Pred Grad:  -0.066, New P: 0.366
iter 3 loss: 0.887
Actual params: [0.8808, 0.3659]
-Original Grad: 0.302, -lr * Pred Grad:  0.095, New P: 0.976
-Original Grad: 0.012, -lr * Pred Grad:  -0.050, New P: 0.316
iter 4 loss: 0.759
Actual params: [0.9761, 0.3155]
-Original Grad: 0.230, -lr * Pred Grad:  0.097, New P: 1.073
-Original Grad: 0.027, -lr * Pred Grad:  -0.035, New P: 0.281
iter 5 loss: 0.620
Actual params: [1.0731, 0.2808]
-Original Grad: -0.106, -lr * Pred Grad:  0.070, New P: 1.143
-Original Grad: -0.049, -lr * Pred Grad:  -0.041, New P: 0.239
iter 6 loss: 0.547
Actual params: [1.143 , 0.2393]
-Original Grad: 0.011, -lr * Pred Grad:  0.062, New P: 1.205
-Original Grad: -0.029, -lr * Pred Grad:  -0.043, New P: 0.196
iter 7 loss: 0.524
Actual params: [1.2052, 0.1965]
-Original Grad: 0.005, -lr * Pred Grad:  0.055, New P: 1.261
-Original Grad: -0.059, -lr * Pred Grad:  -0.050, New P: 0.147
iter 8 loss: 0.522
Actual params: [1.2605, 0.1466]
-Original Grad: 0.087, -lr * Pred Grad:  0.057, New P: 1.318
-Original Grad: -0.049, -lr * Pred Grad:  -0.054, New P: 0.093
iter 9 loss: 0.529
Actual params: [1.3177, 0.0926]
-Original Grad: 0.039, -lr * Pred Grad:  0.055, New P: 1.372
-Original Grad: 0.003, -lr * Pred Grad:  -0.048, New P: 0.045
iter 10 loss: 0.546
Actual params: [1.3724, 0.045 ]
-Original Grad: 0.054, -lr * Pred Grad:  0.054, New P: 1.426
-Original Grad: -0.050, -lr * Pred Grad:  -0.052, New P: -0.007
iter 11 loss: 0.563
Actual params: [ 1.4265, -0.0071]
-Original Grad: 0.026, -lr * Pred Grad:  0.051, New P: 1.478
-Original Grad: -0.077, -lr * Pred Grad:  -0.060, New P: -0.067
iter 12 loss: 0.572
Actual params: [ 1.4775, -0.0669]
-Original Grad: 0.005, -lr * Pred Grad:  0.047, New P: 1.524
-Original Grad: 0.112, -lr * Pred Grad:  -0.028, New P: -0.095
iter 13 loss: 0.578
Actual params: [ 1.5241, -0.0953]
-Original Grad: 0.075, -lr * Pred Grad:  0.049, New P: 1.573
-Original Grad: -0.060, -lr * Pred Grad:  -0.036, New P: -0.131
iter 14 loss: 0.583
Actual params: [ 1.573, -0.131]
-Original Grad: 0.073, -lr * Pred Grad:  0.051, New P: 1.624
-Original Grad: 0.006, -lr * Pred Grad:  -0.031, New P: -0.162
iter 15 loss: 0.589
Actual params: [ 1.6238, -0.1622]
-Original Grad: 0.044, -lr * Pred Grad:  0.050, New P: 1.674
-Original Grad: -0.046, -lr * Pred Grad:  -0.036, New P: -0.198
iter 16 loss: 0.594
Actual params: [ 1.674 , -0.1982]
-Original Grad: 0.022, -lr * Pred Grad:  0.048, New P: 1.722
-Original Grad: -0.115, -lr * Pred Grad:  -0.049, New P: -0.247
iter 17 loss: 0.598
Actual params: [ 1.7217, -0.2475]
-Original Grad: -0.002, -lr * Pred Grad:  0.043, New P: 1.765
-Original Grad: -0.009, -lr * Pred Grad:  -0.046, New P: -0.294
iter 18 loss: 0.600
Actual params: [ 1.7648, -0.2937]
-Original Grad: -0.006, -lr * Pred Grad:  0.038, New P: 1.803
-Original Grad: 0.034, -lr * Pred Grad:  -0.036, New P: -0.330
iter 19 loss: 0.600
Actual params: [ 1.8032, -0.3297]
-Original Grad: 0.060, -lr * Pred Grad:  0.041, New P: 1.844
-Original Grad: -0.028, -lr * Pred Grad:  -0.037, New P: -0.367
iter 20 loss: 0.601
Actual params: [ 1.844, -0.367]
-Original Grad: -0.001, -lr * Pred Grad:  0.037, New P: 1.881
-Original Grad: -0.049, -lr * Pred Grad:  -0.042, New P: -0.409
Target params: [1.1812, 0.2779]
iter 0 loss: 0.485
Actual params: [0.5941, 0.5941]
-Original Grad: 0.057, -lr * Pred Grad:  0.100, New P: 0.694
-Original Grad: -0.150, -lr * Pred Grad:  -0.100, New P: 0.494
iter 1 loss: 0.442
Actual params: [0.6941, 0.4941]
-Original Grad: 0.023, -lr * Pred Grad:  0.090, New P: 0.784
-Original Grad: -0.116, -lr * Pred Grad:  -0.099, New P: 0.396
iter 2 loss: 0.409
Actual params: [0.784 , 0.3955]
-Original Grad: 0.008, -lr * Pred Grad:  0.077, New P: 0.861
-Original Grad: -0.058, -lr * Pred Grad:  -0.092, New P: 0.304
iter 3 loss: 0.394
Actual params: [0.8611, 0.304 ]
-Original Grad: -0.001, -lr * Pred Grad:  0.062, New P: 0.923
-Original Grad: -0.020, -lr * Pred Grad:  -0.080, New P: 0.224
iter 4 loss: 0.384
Actual params: [0.9229, 0.2237]
-Original Grad: -0.004, -lr * Pred Grad:  0.048, New P: 0.971
-Original Grad: -0.027, -lr * Pred Grad:  -0.074, New P: 0.149
iter 5 loss: 0.380
Actual params: [0.971 , 0.1492]
-Original Grad: 0.015, -lr * Pred Grad:  0.052, New P: 1.024
-Original Grad: -0.009, -lr * Pred Grad:  -0.067, New P: 0.083
iter 6 loss: 0.377
Actual params: [1.0235, 0.0827]
-Original Grad: 0.025, -lr * Pred Grad:  0.061, New P: 1.085
-Original Grad: 0.031, -lr * Pred Grad:  -0.050, New P: 0.033
iter 7 loss: 0.374
Actual params: [1.0848, 0.033 ]
-Original Grad: 0.048, -lr * Pred Grad:  0.073, New P: 1.157
-Original Grad: -0.001, -lr * Pred Grad:  -0.044, New P: -0.011
iter 8 loss: 0.370
Actual params: [ 1.1574, -0.0109]
-Original Grad: 0.015, -lr * Pred Grad:  0.072, New P: 1.229
-Original Grad: 0.038, -lr * Pred Grad:  -0.029, New P: -0.040
iter 9 loss: 0.366
Actual params: [ 1.2294, -0.0403]
-Original Grad: 0.012, -lr * Pred Grad:  0.070, New P: 1.300
-Original Grad: 0.015, -lr * Pred Grad:  -0.023, New P: -0.063
iter 10 loss: 0.363
Actual params: [ 1.2997, -0.063 ]
-Original Grad: 0.009, -lr * Pred Grad:  0.068, New P: 1.367
-Original Grad: -0.022, -lr * Pred Grad:  -0.025, New P: -0.088
iter 11 loss: 0.359
Actual params: [ 1.3673, -0.0884]
-Original Grad: 0.023, -lr * Pred Grad:  0.071, New P: 1.438
-Original Grad: -0.016, -lr * Pred Grad:  -0.026, New P: -0.115
iter 12 loss: 0.354
Actual params: [ 1.4385, -0.1149]
-Original Grad: 0.017, -lr * Pred Grad:  0.072, New P: 1.511
-Original Grad: 0.033, -lr * Pred Grad:  -0.016, New P: -0.131
iter 13 loss: 0.349
Actual params: [ 1.5107, -0.1309]
-Original Grad: 0.027, -lr * Pred Grad:  0.077, New P: 1.587
-Original Grad: -0.013, -lr * Pred Grad:  -0.017, New P: -0.148
iter 14 loss: 0.343
Actual params: [ 1.5872, -0.1483]
-Original Grad: 0.020, -lr * Pred Grad:  0.078, New P: 1.665
-Original Grad: 0.019, -lr * Pred Grad:  -0.011, New P: -0.160
iter 15 loss: 0.338
Actual params: [ 1.6652, -0.1595]
-Original Grad: 0.031, -lr * Pred Grad:  0.082, New P: 1.747
-Original Grad: 0.011, -lr * Pred Grad:  -0.008, New P: -0.167
iter 16 loss: 0.334
Actual params: [ 1.7474, -0.1671]
-Original Grad: 0.037, -lr * Pred Grad:  0.087, New P: 1.834
-Original Grad: 0.010, -lr * Pred Grad:  -0.005, New P: -0.172
iter 17 loss: 0.331
Actual params: [ 1.8345, -0.1718]
-Original Grad: 0.005, -lr * Pred Grad:  0.081, New P: 1.916
-Original Grad: 0.045, -lr * Pred Grad:  0.006, New P: -0.166
iter 18 loss: 0.329
Actual params: [ 1.9159, -0.1656]
-Original Grad: 0.013, -lr * Pred Grad:  0.080, New P: 1.995
-Original Grad: -0.008, -lr * Pred Grad:  0.004, New P: -0.162
iter 19 loss: 0.327
Actual params: [ 1.9955, -0.1619]
-Original Grad: 0.021, -lr * Pred Grad:  0.081, New P: 2.076
-Original Grad: 0.043, -lr * Pred Grad:  0.013, New P: -0.149
iter 20 loss: 0.327
Actual params: [ 2.0761, -0.1489]
-Original Grad: 0.029, -lr * Pred Grad:  0.084, New P: 2.160
-Original Grad: -0.014, -lr * Pred Grad:  0.009, New P: -0.140
